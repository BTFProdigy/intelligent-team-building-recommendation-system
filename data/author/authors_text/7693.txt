Proceedings of the 12th Conference of the European Chapter of the ACL, pages 33?41,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Personalizing PageRank for Word Sense Disambiguation
Eneko Agirre and Aitor Soroa
IXA NLP Group
University of the Basque Country
Donostia, Basque Contry
{e.agirre,a.soroa}@ehu.es
Abstract
In this paper we propose a new graph-
based method that uses the knowledge in
a LKB (based on WordNet) in order to
perform unsupervised Word Sense Disam-
biguation. Our algorithm uses the full
graph of the LKB efficiently, performing
better than previous approaches in English
all-words datasets. We also show that the
algorithm can be easily ported to other lan-
guages with good results, with the only re-
quirement of having a wordnet. In addi-
tion, we make an analysis of the perfor-
mance of the algorithm, showing that it is
efficient and that it could be tuned to be
faster.
1 Introduction
Word Sense Disambiguation (WSD) is a key
enabling-technology that automatically chooses
the intended sense of a word in context. Super-
vised WSD systems are the best performing in
public evaluations (Palmer et al, 2001; Snyder
and Palmer, 2004; Pradhan et al, 2007) but they
need large amounts of hand-tagged data, which is
typically very expensive to build. Given the rela-
tively small amount of training data available, cur-
rent state-of-the-art systems only beat the simple
most frequent sense (MFS) baseline1 by a small
margin. As an alternative to supervised systems,
knowledge-based WSD systems exploit the infor-
mation present in a lexical knowledge base (LKB)
to perform WSD, without using any further corpus
evidence.
1This baseline consists of tagging all occurrences in the
test data with the sense of the word that occurs more often in
the training data
Traditional knowledge-based WSD systems as-
sign a sense to an ambiguous word by comparing
each of its senses with those of the surrounding
context. Typically, some semantic similarity met-
ric is used for calculating the relatedness among
senses (Lesk, 1986; McCarthy et al, 2004). One
of the major drawbacks of these approaches stems
from the fact that senses are compared in a pair-
wise fashion and thus the number of computa-
tions can grow exponentially with the number of
words. Although alternatives like simulated an-
nealing (Cowie et al, 1992) and conceptual den-
sity (Agirre and Rigau, 1996) were tried, most of
past knowledge based WSD was done in a subop-
timal word-by-word process, i.e., disambiguating
words one at a time.
Recently, graph-based methods for knowledge-
based WSD have gained much attention in the
NLP community (Sinha and Mihalcea, 2007; Nav-
igli and Lapata, 2007; Mihalcea, 2005; Agirre
and Soroa, 2008). These methods use well-known
graph-based techniques to find and exploit the
structural properties of the graph underlying a par-
ticular LKB. Because the graph is analyzed as a
whole, these techniques have the remarkable prop-
erty of being able to find globally optimal solu-
tions, given the relations between entities. Graph-
based WSD methods are particularly suited for
disambiguating word sequences, and they man-
age to exploit the interrelations among the senses
in the given context. In this sense, they provide
a principled solution to the exponential explosion
problem, with excellent performance.
Graph-based WSD is performed over a graph
composed by senses (nodes) and relations between
pairs of senses (edges). The relations may be of
several types (lexico-semantic, coocurrence rela-
tions, etc.) and may have some weight attached to
33
them. The disambiguation is typically performed
by applying a ranking algorithm over the graph,
and then assigning the concepts with highest rank
to the corresponding words. Given the compu-
tational cost of using large graphs like WordNet,
many researchers use smaller subgraphs built on-
line for each target context.
In this paper we present a novel graph-based
WSD algorithm which uses the full graph of
WordNet efficiently, performing significantly bet-
ter that previously published approaches in En-
glish all-words datasets. We also show that the
algorithm can be easily ported to other languages
with good results, with the only requirement of
having a wordnet. The algorithm is publicly avail-
able2 and can be applied easily to sense invento-
ries and knowledge bases different from WordNet.
Our analysis shows that our algorithm is efficient
compared to previously proposed alternatives, and
that a good choice of WordNet versions and rela-
tions is fundamental for good performance.
The paper is structured as follows. We first de-
scribe the PageRank and Personalized PageRank
algorithms. Section 3 introduces the graph based
methods used for WSD. Section 4 shows the ex-
perimental setting and the main results, and Sec-
tion 5 compares our methods with related exper-
iments on graph-based WSD systems. Section 6
shows the results of the method when applied to
a Spanish dataset. Section 7 analyzes the perfor-
mance of the algorithm. Finally, we draw some
conclusions in Section 8.
2 PageRank and Personalized PageRank
The celebrated PageRank algorithm (Brin and
Page, 1998) is a method for ranking the vertices
in a graph according to their relative structural
importance. The main idea of PageRank is that
whenever a link from vi to vj exists in a graph, a
vote from node i to node j is produced, and hence
the rank of node j increases. Besides, the strength
of the vote from i to j also depends on the rank
of node i: the more important node i is, the more
strength its votes will have. Alternatively, PageR-
ank can also be viewed as the result of a random
walk process, where the final rank of node i rep-
resents the probability of a random walk over the
graph ending on node i, at a sufficiently large time.
Let G be a graph with N vertices v1, . . . , vN
and di be the outdegree of node i; let M be a
2http://ixa2.si.ehu.es/ukb
N?N transition probability matrix, where Mji =
1
di
if a link from i to j exists, and zero otherwise.
Then, the calculation of the PageRank vector Pr
over G is equivalent to resolving Equation (1).
Pr = cMPr + (1 ? c)v (1)
In the equation, v is a N ? 1 vector whose ele-
ments are 1N and c is the so called damping factor,
a scalar value between 0 and 1. The first term of
the sum on the equation models the voting scheme
described in the beginning of the section. The sec-
ond term represents, loosely speaking, the proba-
bility of a surfer randomly jumping to any node,
e.g. without following any paths on the graph.
The damping factor, usually set in the [0.85..0.95]
range, models the way in which these two terms
are combined at each step.
The second term on Eq. (1) can also be seen as
a smoothing factor that makes any graph fulfill the
property of being aperiodic and irreducible, and
thus guarantees that PageRank calculation con-
verges to a unique stationary distribution.
In the traditional PageRank formulation the vec-
tor v is a stochastic normalized vector whose ele-
ment values are all 1N , thus assigning equal proba-
bilities to all nodes in the graph in case of random
jumps. However, as pointed out by (Haveliwala,
2002), the vector v can be non-uniform and assign
stronger probabilities to certain kinds of nodes, ef-
fectively biasing the resulting PageRank vector to
prefer these nodes. For example, if we concen-
trate all the probability mass on a unique node i,
all random jumps on the walk will return to i and
thus its rank will be high; moreover, the high rank
of i will make all the nodes in its vicinity also re-
ceive a high rank. Thus, the importance of node i
given by the initial distribution of v spreads along
the graph on successive iterations of the algorithm.
In this paper, we will use traditional PageRank
to refer to the case when a uniform v vector is used
in Eq. (1); and whenever a modified v is used, we
will call it Personalized PageRank. The next sec-
tion shows how we define a modified v.
PageRank is actually calculated by applying an
iterative algorithm which computes Eq. (1) suc-
cessively until convergence below a given thresh-
old is achieved, or, more typically, until a fixed
number of iterations are executed.
Regarding PageRank implementation details,
we chose a damping value of 0.85 and finish the
calculation after 30 iterations. We did not try other
34
damping factors. Some preliminary experiments
with higher iteration counts showed that although
sometimes the node ranks varied, the relative order
among particular word synsets remained stable af-
ter the initial iterations (cf. Section 7 for further
details). Note that, in order to discard the effect
of dangling nodes (i.e. nodes without outlinks) we
slightly modified Eq. (1). For the sake of brevity
we omit the details, which the interested reader
can check in (Langville and Meyer, 2003).
3 Using PageRank for WSD
In this section we present the application of
PageRank to WSD. If we were to apply the tra-
ditional PageRank over the whole WordNet we
would get a context-independent ranking of word
senses, which is not what we want. Given an input
piece of text (typically one sentence, or a small set
of contiguous sentences), we want to disambiguate
all open-class words in the input taken the rest as
context. In this framework, we need to rank the
senses of the target words according to the other
words in the context. Theare two main alternatives
to achieve this:
? To create a subgraph of WordNet which con-
nects the senses of the words in the input text,
and then apply traditional PageRank over the
subgraph.
? To use Personalized PageRank, initializing v
with the senses of the words in the input text
The first method has been explored in the lit-
erature (cf. Section 5), and we also presented a
variant in (Agirre and Soroa, 2008) but the second
method is novel in WSD. In both cases, the algo-
rithms return a list of ranked senses for each target
word in the context. We will see each of them in
turn, but first we will present some notation and a
preliminary step.
3.1 Preliminary step
A LKB is formed by a set of concepts and relations
among them, and a dictionary, i.e., a list of words
(typically, word lemmas) each of them linked to
at least one concept of the LKB. Given any such
LKB, we build an undirected graph G = (V, E)
where nodes represent LKB concepts (vi), and
each relation between concepts vi and vj is rep-
resented by an undirected edge ei,j .
In our experiments we have tried our algorithms
using three different LKBs:
? MCR16 + Xwn: The Multilingual Central
Repository (Atserias et al, 2004b) is a lexical
knowledge base built within the MEANING
project3. This LKB comprises the original
WordNet 1.6 synsets and relations, plus some
relations from other WordNet versions auto-
matically mapped4 into version 1.6: WordNet
2.0 relations and eXtended WordNet relations
(Mihalcea and Moldovan, 2001) (gold, silver
and normal relations). The resulting graph
has 99, 632 vertices and 637, 290 relations.
? WNet17 + Xwn: WordNet 1.7 synset and
relations and eXtended WordNet relations.
The graph has 109, 359 vertices and 620, 396
edges
? WNet30 + gloss: WordNet 3.0 synset and
relations, including manually disambiguated
glosses . The graph has 117, 522 vertices and
525, 356 relations.
Given an input text, we extract the list Wi i =
1 . . .m of content words (i.e. nouns, verbs, ad-
jectives and adverbs) which have an entry in the
dictionary, and thus can be related to LKB con-
cepts. Let Concepts i = {v1, . . . , vim} be the
im associated concepts of word Wi in the LKB
graph. Note that monosemous words will be re-
lated to just one concept, whereas polysemous
words may be attached to several. As a result
of the disambiguation process, every concept in
Concepts i, i = 1, . . . , m receives a score. Then,
for each target word to be disambiguated, we just
choose its associated concept in G with maximal
score.
In our experiments we build a context of at least
20 content words for each sentence to be disam-
biguated, taking the sentences immediately before
and after it in the case that the original sentence
was too short.
3.2 Traditional PageRank over Subgraph
(Spr)
We follow the algorithm presented in (Agirre and
Soroa, 2008), which we explain here for complete-
ness. The main idea of the subgraph method is to
extract the subgraph of GKB whose vertices and
relations are particularly relevant for a given input
3http://nipadio.lsi.upc.es/nlp/meaning
4We use the freely available WordNet mappings from
http://www.lsi.upc.es/?nlp/tools/download-map.php
35
context. Such a subgraph is called a ?disambigua-
tion subgraph? GD, and it is built in the following
way. For each word Wi in the input context and
each concept vi ? Concepts i, a standard breath-
first search (BFS) over GKB is performed, start-
ing at node vi. Each run of the BFS calculates the
minimum distance paths between vi and the rest of
concepts of GKB . In particular, we are interested
in the minimum distance paths between vi and the
concepts associated to the rest of the words in the
context, vj ?
?
j 6=i Conceptsj . Let mdpvi be the
set of these shortest paths.
This BFS computation is repeated for every
concept of every word in the input context, stor-
ing mdpvi accordingly. At the end, we obtain a
set of minimum length paths each of them hav-
ing a different concept as a source. The disam-
biguation graph GD is then just the union of the
vertices and edges of the shortest paths, GD =
?m
i=1{mdpvj/vj ? Concepts i}.
The disambiguation graph GD is thus a sub-
graph of the original GKB graph obtained by com-
puting the shortest paths between the concepts of
the words co-occurring in the context. Thus, we
hypothesize that it captures the most relevant con-
cepts and relations in the knowledge base for the
particular input context.
Once the GD graph is built, we compute the tra-
ditional PageRank algorithm over it. The intuition
behind this step is that the vertices representing
the correct concepts will be more relevant in GD
than the rest of the possible concepts of the context
words, which should have less relations on average
and be more isolated.
As usual, the disambiguation step is performed
by assigning to each word Wi the associated con-
cept in Concepts i which has maximum rank. In
case of ties we assign all the concepts with maxi-
mum rank. Note that the standard evaluation script
provided in the Senseval competitions treats mul-
tiple senses as if one was chosen at random, i.e.
for evaluation purposes our method is equivalent
to breaking ties at random.
3.3 Personalized PageRank (Ppr and
Ppr w2w)
As mentioned before, personalized PageRank al-
lows us to use the full LKB. We first insert the
context words into the graph G as nodes, and link
them with directed edges to their respective con-
cepts. Then, we compute the personalized PageR-
ank of the graph G by concentrating the initial
probability mass uniformly over the newly intro-
duced word nodes. As the words are linked to
the concepts by directed edges, they act as source
nodes injecting mass into the concepts they are as-
sociated with, which thus become relevant nodes,
and spread their mass over the LKB graph. There-
fore, the resulting personalized PageRank vector
can be seen as a measure of the structural rele-
vance of LKB concepts in the presence of the input
context.
One problem with Personalized PageRank is
that if one of the target words has two senses
which are related by semantic relations, those
senses reinforce each other, and could thus
dampen the effect of the other senses in the con-
text. With this observation in mind we devised
a variant (dubbed Ppr w2w), where we build the
graph for each target word in the context: for each
target word Wi, we concentrate the initial proba-
bility mass in the senses of the words surrounding
Wi, but not in the senses of the target word itself,
so that context words increase its relative impor-
tance in the graph. The main idea of this approach
is to avoid biasing the initial score of concepts as-
sociated to target word Wi, and let the surround-
ing words decide which concept associated to Wi
has more relevance. Contrary to the other two ap-
proaches, Ppr w2w does not disambiguate all tar-
get words of the context in a single run, which
makes it less efficient (cf. Section 7).
4 Evaluation framework and results
In this paper we will use two datasets for com-
paring graph-based WSD methods, namely, the
Senseval-2 (S2AW) and Senseval-3 (S3AW) all
words datasets (Snyder and Palmer, 2004; Palmer
et al, 2001), which are both labeled with WordNet
1.7 tags. We did not use the Semeval dataset, for
the sake of comparing our results to related work,
none of which used Semeval data. Table 1 shows
the results as recall of the graph-based WSD sys-
tem over these datasets on the different LKBs. We
detail overall results, as well as results per PoS,
and the confidence interval for the overall results.
The interval was computed using bootstrap resam-
pling with 95% confidence.
The table shows that Ppr w2w is consistently
the best method in both datasets and for all LKBs.
Ppr and Spr obtain comparable results, which is
remarkable, given the simplicity of the Ppr algo-
36
Senseval-2 All Words dataset
LKB Method All N V Adj. Adv. Conf. interval
MCR16 + Xwn Ppr 51.1 64.9 38.1 57.4 47.5 [49.3, 52.6]
MCR16 + Xwn Ppr w2w 53.3 64.5 38.6 58.3 48.1 [52.0, 55.0]
MCR16 + Xwn Spr 52.7 64.8 35.3 56.8 50.2 [51.3, 54.4]
WNet17 + Xwn Ppr 56.8 71.1 33.4 55.9 67.1 [55.0, 58.7]
WNet17 + Xwn Ppr w2w 58.6 70.4 38.9 58.3 70.1 [56.7, 60.3]
WNet17 + Xwn Spr 56.7 66.8 37.7 57.6 70.8 [55.0, 58.2]
WNet30 + gloss Ppr 53.5 70.0 28.6 53.9 55.1 [51.8, 55.2]
WNet30 + gloss Ppr w2w 55.8 71.9 34.4 53.8 57.5 [54.1, 57.8]
WNet30 + gloss Spr 54.8 68.9 35.1 55.2 56.5 [53.2, 56.3]
MFS 60.1 71.2 39.0 61.1 75.4 [58.6, 61.9]
SMUaw 68.6 78.0 52.9 69.9 81.7
Senseval-3 All Words dataset
LKB Method All N V Adj. Adv.
MCR16 + Xwn Ppr 54.3 60.9 45.4 56.5 92.9 [52.3, 56.1]
MCR16 + Xwn Ppr w2w 55.8 63.2 46.2 57.5 92.9 [53.7, 57.7]
MCR16 + Xwn Static 53.7 59.5 45.0 57.8 92.9 [51.8, 55.7]
WNet17 + Xwn Ppr 56.1 62.6 46.0 60.8 92.9 [54.0, 58.1]
WNet17 + Xwn Ppr w2w 57.4 64.1 46.9 62.6 92.9 [55.5, 59.3]
WNet17 + Xwn Spr 56.20 61.6 47.3 61.8 92.9 [54.8, 58.2]
WNet30 + gloss Ppr 48.5 52.2 41.5 54.2 78.6 [46.7, 50.6]
WNet30 + gloss Ppr w2w 51.6 59.0 40.2 57.2 78.6 [49.9, 53.3]
WNet30 + gloss Spr 45.4 54.1 31.4 52.5 78.6 [43.7, 47.4]
MFS 62.3 69.3 53.6 63.7 92.9 [60.2, 64.0]
GAMBL 65.2 70.8 59.3 65.3 100
Table 1: Results (as recall) on Senseval-2 and Senseval-3 all words tasks. We also include the MFS
baseline and the best results of supervised systems at competition time (SMUaw,GAMBL).
rithm, compared to the more elaborate algorithm
to construct the graph. The differences between
methods are not statistically significant, which is a
common problem on this relatively small datasets
(Snyder and Palmer, 2004; Palmer et al, 2001).
Regarding LKBs, the best results are obtained
using WordNet 1.7 and eXtended WordNet. Here
the differences are in many cases significant.
These results are surprising, as we would ex-
pect that the manually disambiguated gloss re-
lations from WordNet 3.0 would lead to bet-
ter results, compared to the automatically disam-
biguated gloss relations from the eXtended Word-
Net (linked to version 1.7). The lower perfor-
mance of WNet30+gloss can be due to the fact
that the Senseval all words data set is tagged using
WordNet 1.7 synsets. When using a different LKB
for WSD, a mapping to WordNet 1.7 is required.
Although the mapping is cited as having a correct-
ness on the high 90s (Daude et al, 2000), it could
have introduced sufficient noise to counteract the
benefits of the hand-disambiguated glosses.
Table 1 also shows the most frequent sense
(MFS), as well as the best supervised sys-
tems (Snyder and Palmer, 2004; Palmer et
al., 2001) that participated in each competition
(SMUaw and GAMBL, respectively). The MFS is
a baseline for supervised systems, but it is consid-
ered a difficult competitor for unsupervised sys-
tems, which rarely come close to it. In this case
the MFS baseline was computed using previously
availabel training data like SemCor. Our best re-
sults are close to the MFS in both Senseval-2 and
Senseval-3 datasets. The results for the supervised
system are given for reference, and we can see that
the gap is relatively small, specially for Senseval-
3.
5 Comparison to Related work
In this section we will briefly describe some
graph-based methods for knowledge-based WSD.
The methods here presented cope with the prob-
lem of sequence-labeling, i.e., they disambiguate
all the words coocurring in a sequence (typically,
all content words of a sentence). All the meth-
ods rely on the information represented on some
LKB, which typically is some version of Word-
Net, sometimes enriched with proprietary rela-
tions. The results on our datasets, when available,
are shown in Table 2. The table also shows the
performance of supervised systems.
The TexRank algorithm (Mihalcea, 2005) for
WSD creates a complete weighted graph (e.g. a
graph where every pair of distinct vertices is con-
nected by a weighted edge) formed by the synsets
of the words in the input context. The weight
37
Senseval-2 All Words dataset
System All N V Adj. Adv.
Mih05 54.2 57.5 36.5 56.7 70.9
Sihna07 56.4 65.6 32.3 61.4 60.2
Tsatsa07 49.2 ? ? ? ?
Spr 56.6 66.7 37.5 57.6 70.8
Ppr 56.8 71.1 33.4 55.9 67.1
Ppr w2w 58.6 70.4 38.9 58.3 70.1
MFS 60.1 71.2 39.0 61.1 75.4
Senseval-3 All Words dataset
System All N V Adj. Adv.
Mih05 52.2 - - - -
Sihna07 52.4 60.5 40.6 54.1 100.0
Nav07 - 61.9 36.1 62.8 -
Spr 56.2 61.6 47.3 61.8 92.9
Ppr 56.1 62.6 46.0 60.8 92.9
Ppr w2w 57.4 64.1 46.9 62.6 92.9
MFS 62.3 69.3 53.6 63.7 92.9
Nav05 60.4 - - - -
Table 2: Comparison with related work. Note that
Nav05 uses the MFS.
of the links joining two synsets is calculated by
executing Lesk?s algorithm (Lesk, 1986) between
them, i.e., by calculating the overlap between the
words in the glosses of the correspongind senses.
Once the complete graph is built, the PageRank al-
gorithm is executed over it and words are assigned
to the most relevant synset. In this sense, PageR-
ank is used an alternative to simulated annealing
to find the optimal pairwise combinations. The
method was evaluated on the Senseval-3 dataset,
as shown in row Mih05 on Table 2.
(Sinha and Mihalcea, 2007) extends their pre-
vious work by using a collection of semantic sim-
ilarity measures when assigning a weight to the
links across synsets. They also compare differ-
ent graph-based centrality algorithms to rank the
vertices of the complete graph. They use differ-
ent similarity metrics for different POS types and
a voting scheme among the centrality algorithm
ranks. Here, the Senseval-3 corpus was used as
a development data set, and we can thus see those
results as the upper-bound of their method.
We can see in Table 2 that the methods pre-
sented in this paper clearly outperform both Mih05
and Sin07. This result suggests that analyzing the
LKB structure as a whole is preferable than com-
puting pairwise similarity measures over synsets.
The results of various in-house made experiments
replicating (Mihalcea, 2005) also confirm this ob-
servation. Note also that our methods are simpler
than the combination strategy used in (Sinha and
Mihalcea, 2007), and that we did not perform any
parameter tuning as they did.
In (Navigli and Velardi, 2005) the authors de-
velop a knowledge-based WSD method based on
lexical chains called structural semantic intercon-
nections (SSI). Although the system was first de-
signed to find the meaning of the words in Word-
Net glosses, the authors also apply the method for
labeling text sequences. Given a text sequence,
SSI first identifies monosemous words and assigns
the corresponding synset to them. Then, it iter-
atively disambiguates the rest of terms by select-
ing the senses that get the strongest interconnec-
tion with the synsets selected so far. The inter-
connection is calculated by searching for paths on
the LKB, constrained by some hand-made rules of
possible semantic patterns. The method was eval-
uated on the Senseval-3 dataset, as shown in row
Nav05 on Table 2. Note that the method labels
an instance with the most frequent sense of the
word if the algorithm produces no output for that
instance, which makes comparison to our system
unfair, specially given the fact that the MFS per-
forms better than SSI. In fact it is not possible to
separate the effect of SSI from that of the MFS.
For this reason we place this method close to the
MFS baseline in Table 2.
In (Navigli and Lapata, 2007), the authors per-
form a two-stage process for WSD. Given an input
context, the method first explores the whole LKB
in order to find a subgraph which is particularly
relevant for the words of the context. Then, they
study different graph-based centrality algorithms
for deciding the relevance of the nodes on the sub-
graph. As a result, every word of the context is
attached to the highest ranking concept among its
possible senses. The Spr method is very similar
to (Navigli and Lapata, 2007), the main differ-
ence lying on the initial method for extracting the
context subgraph. Whereas (Navigli and Lapata,
2007) apply a depth-first search algorithm over the
LKB graph ?and restrict the depth of the subtree
to a value of 3?, Spr relies on shortest paths be-
tween word synsets. Navigli and Lapata don?t re-
port overall results and therefore, we can?t directly
compare our results with theirs. However, we can
see that on a PoS-basis evaluation our results are
consistently better for nouns and verbs (especially
the Ppr w2w method) and rather similar for adjec-
tives.
(Tsatsaronis et al, 2007) is another example of
a two-stage process, the first one consisting on
finding a relevant subgraph by performing a BFS
38
Spanish Semeval07
LKB Method Acc.
Spanish Wnet + Xnet? Ppr 78.4
Spanish Wnet + Xnet? Ppr w2w 79.3
? MFS 84.6
? Supervised 85.10
Table 3: Results (accuracy) on Spanish Semeval07
dataset, including MFS and the best supervised
system in the competition.
search over the LKB. The authors apply a spread-
ing activation algorithm over the subgraph for
node ranking. Edges of the subgraph are weighted
according to its type, following a tf.idf like ap-
proach. The results show that our methods clearly
outperform Tsatsa07. The fact that the Spr method
works better suggests that the traditional PageR-
ank algorithm is a superior method for ranking the
subgraph nodes.
As stated before, all methods presented here
use some LKB for performing WSD. (Mihalcea,
2005) and (Sinha and Mihalcea, 2007) use Word-
Net relations as a knowledge source, but neither
of them specify which particular version did they
use. (Tsatsaronis et al, 2007) uses WordNet 1.7
enriched with eXtended WordNet relations, just
as we do. Both (Navigli and Velardi, 2005; Nav-
igli and Lapata, 2007) use WordNet 2.0 as the un-
derlying LKB, albeit enriched with several new
relations, which are manually created. Unfor-
tunately, those manual relations are not publicly
available, so we can?t directly compare their re-
sults with the rest of the methods. In (Agirre and
Soroa, 2008) we experiment with different LKBs
formed by combining relations of different MCR
versions along with relations extracted from Sem-
Cor, which we call supervised and unsupervised
relations, respectively. The unsupervised relations
that yielded bests results are also used in this paper
(c.f Section 3.1).
6 Experiments on Spanish
Our WSD algorithm can be applied over non-
english texts, provided that a LKB for this partic-
ular language exists. We have tested the graph-
algorithms proposed in this paper on a Spanish
dataset, using the Spanish WordNet as knowledge
source (Atserias et al, 2004a).
We used the Semeval-2007 Task 09 dataset as
evaluation gold standard (Ma`rquez et al, 2007).
The dataset contains examples of the 150 most
frequent nouns in the CESS-ECE corpus, manu-
Method Time
Ppr 26m46
Spr 119m7
Ppr w2w 164m4
Table 4: Elapsed time (in minutes) of the algo-
rithms when applied to the Senseval-2 dataset.
ally annotated with Spanish WordNet synsets. It
is split into a train and test part, and has an ?all
words? shape i.e. input consists on sentences,
each one having at least one occurrence of a tar-
get noun. We ran the experiment over the test part
(792 instances), and used the train part for cal-
culating the MFS baseline. We used the Span-
ish WordNet as LKB, enriched with eXtended
WordNet relations. It contains 105, 501 nodes and
623, 316 relations. The results in Table 3 are con-
sistent with those for English, with our algorithm
approaching MFS performance. Note that for this
dataset the supervised algorithm could barely im-
prove over the MFS, suggesting that for this par-
ticular dataset MFS is particularly strong.
7 Performance analysis
Table 4 shows the time spent by the different al-
gorithms when applied to the Senseval-2 all words
dataset, using the WNet17 + Xwn as LKB. The
dataset consists on 2473 word instances appear-
ing on 476 different sentences. The experiments
were done on a computer with four 2.66 Ghz pro-
cessors and 16 Gb memory. The table shows that
the time elapsed by the algorithms varies between
30 minutes for the Ppr method (which thus dis-
ambiguates circa 82 instances per minute) to al-
most 3 hours spent by the Ppr w2w method (circa
15 instances per minute). The Spr method lies
in between, requiring 2 hours for completing the
task, but its overall performance is well below the
PageRank based Ppr w2w method. Note that the
algorithm is coded in C++ for greater efficiency,
and uses the Boost Graph Library.
Regarding PageRank calculation, we have tried
different numbers of iterations, and analyze the
rate of convergence of the algorithm. Figure 1 de-
picts the performance of the Ppr w2w method for
different iterations of the algorithm. As before, the
algorithm is applied over the MCR17 + Xwn LKB,
and evaluated on the Senseval-2 all words dataset.
The algorithm converges very quickly: one sole it-
eration suffices for achieving a relatively high per-
39
57
57.2
57.4
57.6
57.8
58
58.2
58.4
58.6
0 5 10 15 20 25 30
R
ec
al
l
Iterations
Rate of convergence
3
3
3
3 3
3 3 3
Figure 1: Rate of convergence of PageRank algo-
rithm over the MCR17 + Xwn LKB.
formance, and 20 iterations are enough for achiev-
ing convergence. The figure shows that, depend-
ing on the LKB complexity, the user can tune the
algorithm and lower the number of iterations, thus
considerably reducing the time required for disam-
biguation.
8 Conclusions
In this paper we propose a new graph-based
method that uses the knowledge in a LKB (based
on WordNet) in order to perform unsupervised
Word Sense Disambuation. Our algorithm uses the
full graph of the LKB efficiently, performing bet-
ter than previous approaches in English all-words
datasets. We also show that the algorithm can be
easily ported to other languages with good results,
with the only requirement of having a wordnet.
Both for Spanish and English the algorithm attains
performances close to the MFS.
The algorithm is publicly available5 and can be
applied easily to sense inventories and knowledge
bases different from WordNet. Our analysis shows
that our algorithm is efficient compared to previ-
ously proposed alternatives, and that a good choice
of WordNet versions and relations is fundamental
for good performance.
Acknowledgments
This work has been partially funded by the EU Commission
(project KYOTO ICT-2007-211423) and Spanish Research
Department (project KNOW TIN2006-15049-C03-01).
References
E. Agirre and G. Rigau. 1996. Word sense disam-
biguation using conceptual density. In In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics, pages 16?22.
5http://ixa2.si.ehu.es/ukb
E. Agirre and A. Soroa. 2008. Using the multilin-
gual central repository for graph-based word sense
disambiguation. In Proceedings of LREC ?08, Mar-
rakesh, Morocco.
J. Atserias, G. Rigau, and L. Villarejo. 2004a. Span-
ish wordnet 1.6: Porting the spanish wordnet across
princeton versions. In In Proceedings of LREC ?04.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004b. The meaning
multilingual central repository. In In Proceedings of
GWC, Brno, Czech Republic.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
J. Cowie, J. Guthrie, and L. Guthrie. 1992. Lexical
disambiguation using simulated annealing. In HLT
?91: Proceedings of the workshop on Speech and
Natural Language, pages 238?242, Morristown, NJ,
USA.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping
WordNets using structural information. In Proceed-
ings of ACL?2000, Hong Kong.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02: Proceedings of the 11th international
conference on World Wide Web, pages 517?526,
New York, NY, USA. ACM.
A. N. Langville and C. D. Meyer. 2003. Deeper inside
pagerank. Internet Mathematics, 1(3):335?380.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference
on Systems documentation, pages 24?26, New York,
NY, USA. ACM.
L. Ma`rquez, L. Villarejo, M. A. Mart??, and M. Taule?.
2007. Semeval-2007 task 09: Multilevel semantic
annotation of catalan and spanish. In Proceedings
of SemEval-2007, pages 42?47, Prague, Czech Re-
public, June.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in untagged
text. In ACL ?04: Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, page 279, Morristown, NJ, USA. Association
for Computational Linguistics.
R. Mihalcea and D. I. Moldovan. 2001. eXtended
WordNet: Progress report. In in Proceedings of
NAACL Workshop on WordNet and Other Lexical
Resources, pages 95?100.
R. Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
HLT05, Morristown, NJ, USA.
40
R. Navigli and M. Lapata. 2007. Graph connectivity
measures for unsupervised word sense disambigua-
tion. In IJCAI.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: A knowledge-based approach
to word sense disambiguation. IEEE Trans. Pattern
Anal. Mach. Intell., 27(7):1075?1086.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H.T.
Dang. 2001. English tasks: All-words and verb
lexical sample. In Proc. of SENSEVAL-2: Second
International Workshop on Evaluating Word Sense
Disambiguation Systems, Tolouse, France, July.
S. Pradhan, E. Loper, D. Dligach, and M.Palmer. 2007.
Semeval-2007 task-17: English lexical sample srl
and all words. In Proceedings of SemEval-2007,
pages 87?92, Prague, Czech Republic, June.
R. Sinha and R. Mihalcea. 2007. Unsupervised graph-
based word sense disambiguation using measures
of word semantic similarity. In Proceedings of the
IEEE International Conference on Semantic Com-
puting (ICSC 2007), Irvine, CA, USA.
B. Snyder and M. Palmer. 2004. The English all-words
task. In ACL 2004 Senseval-3 Workshop, Barcelona,
Spain, July.
G. Tsatsaronis, M. Vazirgiannis, and I. Androutsopou-
los. 2007. Word sense disambiguation with spread-
ing activation networks generated from thesauri. In
IJCAI.
41
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Study on Similarity and Relatedness
Using Distributional and WordNet-based Approaches
Eneko Agirre? Enrique Alfonseca? Keith Hall? Jana Kravalova?? Marius Pas?ca? Aitor Soroa?
? IXA NLP Group, University of the Basque Country
? Google Inc.
? Institute of Formal and Applied Linguistics, Charles University in Prague
{e.agirre,a.soroa}@ehu.es {ealfonseca,kbhall,mars}@google.com
kravalova@ufal.mff.cuni.cz
Abstract
This paper presents and compares WordNet-
based and distributional similarity approaches.
The strengths and weaknesses of each ap-
proach regarding similarity and relatedness
tasks are discussed, and a combination is pre-
sented. Each of our methods independently
provide the best results in their class on the
RG and WordSim353 datasets, and a super-
vised combination of them yields the best pub-
lished results on all datasets. Finally, we pio-
neer cross-lingual similarity, showing that our
methods are easily adapted for a cross-lingual
task with minor losses.
1 Introduction
Measuring semantic similarity and relatedness be-
tween terms is an important problem in lexical se-
mantics. It has applications in many natural lan-
guage processing tasks, such as Textual Entailment,
Word Sense Disambiguation or Information Extrac-
tion, and other related areas like Information Re-
trieval. The techniques used to solve this problem
can be roughly classified into two main categories:
those relying on pre-existing knowledge resources
(thesauri, semantic networks, taxonomies or ency-
clopedias) (Alvarez and Lim, 2007; Yang and Pow-
ers, 2005; Hughes and Ramage, 2007) and those in-
ducing distributional properties of words from cor-
pora (Sahami and Heilman, 2006; Chen et al, 2006;
Bollegala et al, 2007).
In this paper, we explore both families. For the
first one we apply graph based algorithms to Word-
Net, and for the second we induce distributional
similarities collected from a 1.6 Terabyte Web cor-
pus. Previous work suggests that distributional sim-
ilarities suffer from certain limitations, which make
them less useful than knowledge resources for se-
mantic similarity. For example, Lin (1998b) finds
similar phrases like captive-westerner which made
sense only in the context of the corpus used, and
Budanitsky and Hirst (2006) highlight other prob-
lems that stem from the imbalance and sparseness of
the corpora. Comparatively, the experiments in this
paper demonstrate that distributional similarities can
perform as well as the knowledge-based approaches,
and a combination of the two can exceed the per-
formance of results previously reported on the same
datasets. An application to cross-lingual (CL) sim-
ilarity identification is also described, with applica-
tions such as CL Information Retrieval or CL spon-
sored search. A discussion on the differences be-
tween learning similarity and relatedness scores is
provided.
The paper is structured as follows. We first
present the WordNet-based method, followed by the
distributional methods. Section 4 is devoted to the
evaluation and results on the monolingual and cross-
lingual tasks. Section 5 presents some analysis, in-
cluding learning curves for distributional methods,
the use of distributional similarity to improve Word-
Net similarity, the contrast between similarity and
relatedness, and the combination of methods. Sec-
tion 6 presents related work, and finally, Section 7
draws the conclusions and mentions future work.
2 WordNet-based method
WordNet (Fellbaum, 1998) is a lexical database of
English, which groups nouns, verbs, adjectives and
adverbs into sets of synonyms (synsets), each ex-
pressing a distinct concept. Synsets are interlinked
with conceptual-semantic and lexical relations, in-
cluding hypernymy, meronymy, causality, etc.
Given a pair of words and a graph-based repre-
sentation of WordNet, our method has basically two
19
steps: We first compute the personalized PageR-
ank over WordNet separately for each of the words,
producing a probability distribution over WordNet
synsets. We then compare how similar these two dis-
crete probability distributions are by encoding them
as vectors and computing the cosine between the
vectors.
We represent WordNet as a graph G = (V,E) as
follows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets associated
to them by directed edges.
For each word in the pair we first compute a per-
sonalized PageRank vector of graph G (Haveliwala,
2002). Basically, personalized PageRank is com-
puted by modifying the random jump distribution
vector in the traditional PageRank equation. In our
case, we concentrate all probability mass in the tar-
get word.
Regarding PageRank implementation details, we
chose a damping value of 0.85 and finish the calcula-
tion after 30 iterations. These are default values, and
we did not optimize them. Our similarity method is
similar, but simpler, to that used by (Hughes and Ra-
mage, 2007), which report very good results on sim-
ilarity datasets. More details of our algorithm can be
found in (Agirre and Soroa, 2009). The algorithm
and needed resouces are publicly available1.
2.1 WordNet relations and versions
The WordNet versions that we use in this work are
the Multilingual Central Repository or MCR (At-
serias et al, 2004) (which includes English Word-
Net version 1.6 and wordnets for several other lan-
guages like Spanish, Italian, Catalan and Basque),
and WordNet version 3.02. We used all the rela-
tions in MCR (except cooccurrence relations and se-
lectional preference relations) and in WordNet 3.0.
Given the recent availability of the disambiguated
gloss relations for WordNet 3.03, we also used a
version which incorporates these relations. We will
refer to the three versions as MCR16, WN30 and
WN30g, respectively. Our choice was mainly moti-
vated by the fact that MCR contains tightly aligned
1http://http://ixa2.si.ehu.es/ukb/
2Available from http://http://wordnet.princeton.edu/
3http://wordnet.princeton.edu/glosstag
wordnets of several languages (see below).
2.2 Cross-linguality
MCR follows the EuroWordNet design (Vossen,
1998), which specifies an InterLingual Index (ILI)
that links the concepts across wordnets of differ-
ent languages. The wordnets for other languages in
MCR use the English WordNet synset numbers as
ILIs. This design allows a decoupling of the rela-
tions between concepts (which can be taken to be
language independent) and the links from each con-
tent word to its corresponding concepts (which is
language dependent).
As our WordNet-based method uses the graph of
the concepts and relations, we can easily compute
the similarity between words from different lan-
guages. For example, consider a English-Spanish
pair like car ? coche. Given that the Spanish Word-
Net is included in MCR we can use MCR as the
common knowledge-base for the relations. We can
then compute the personalized PageRank for each
of car and coche on the same underlying graph, and
then compare the similarity between both probabil-
ity distributions.
As an alternative, we also tried to use pub-
licly available mappings for wordnets (Daude et al,
2000)4 in order to create a 3.0 version of the Span-
ish WordNet. The mapping was used to link Spanish
variants to 3.0 synsets. We used the English Word-
Net 3.0, including glosses, to construct the graph.
The two Spanish WordNet versions are referred to
as MCR16 and WN30g.
3 Context-based methods
In this section, we describe the distributional meth-
ods used for calculating similarities between words,
and profiting from the use of a large Web-based cor-
pus.
This work is motivated by previous studies that
make use of search engines in order to collect co-
occurrence statistics between words. Turney (2001)
uses the number of hits returned by a Web search
engine to calculate the Pointwise Mutual Informa-
tion (PMI) between terms, as an indicator of syn-
onymy. Bollegala et al (2007) calculate a number
of popular relatedness metrics based on page counts,
4http://www.lsi.upc.es/?nlp/tools/download-map.php.
20
like PMI, the Jaccard coefficient, the Simpson co-
efficient and the Dice coefficient, which are com-
bined with lexico-syntactic patterns as model fea-
tures. The model parameters are trained using Sup-
port Vector Machines (SVM) in order to later rank
pairs of words. A different approach is the one taken
by Sahami and Heilman (2006), who collect snip-
pets from the results of a search engine and repre-
sent each snippet as a vector, weighted with the tf?idf
score. The semantic similarity between two queries
is calculated as the inner product between the cen-
troids of the respective sets of vectors.
To calculate the similarity of two words w1 and
w2, Ruiz-Casado et al (2005) collect snippets con-
taining w1 from a Web search engine, extract a con-
text around it, replace it with w2 and check for the
existence of that modified context in the Web.
Using a search engine to calculate similarities be-
tween words has the drawback that the data used will
always be truncated. So, for example, the numbers
of hits returned by search engines nowadays are al-
ways approximate and rounded up. The systems that
rely on collecting snippets are also limited by the
maximum number of documents returned per query,
typically around a thousand. We hypothesize that
by crawling a large corpus from the Web and doing
standard corpus analysis to collect precise statistics
for the terms we should improve over other unsu-
pervised systems that are based on search engine
results, and should yield results that are competi-
tive even when compared to knowledge-based ap-
proaches.
In order to calculate the semantic similarity be-
tween the words in a set, we have used a vector space
model, with the following three variations:
In the bag-of-words approach, for each word w
in the dataset we collect every term t that appears in
a window centered in w, and add them to the vector
together with its frequency.
In the context window approach, for each word
w in the dataset we collect every window W cen-
tered in w (removing the central word), and add it
to the vector together with its frequency (the total
number of times we saw windowW around w in the
whole corpus). In this case, all punctuation symbols
are replaced with a special token, to unify patterns
like , the <term> said to and ? the <term> said to.
Throughout the paper, when we mention a context
window of size N it means N words at each side of
the phrase of interest.
In the syntactic dependency approach, we parse
the entire corpus using an implementation of an In-
ductive Dependency parser as described in Nivre
(2006). For each word w we collect a template of
the syntactic context. We consider sequences of gov-
erning words (e.g. the parent, grand-parent, etc.) as
well as collections of descendants (e.g., immediate
children, grandchildren, etc.). This information is
then encoded as a contextual template. For example,
the context template cooks <term> delicious could
be contexts for nouns such as food, meals, pasta, etc.
This captures both syntactic preferences as well as
selectional preferences. Contrary to Pado and Lap-
ata (2007), we do not use the labels of the syntactic
dependencies.
Once the vectors have been obtained, the fre-
quency for each dimension in every vector is
weighted using the other vectors as contrast set, with
the ?2 test, and finally the cosine similarity between
vectors is used to calculate the similarity between
each pair of terms.
Except for the syntactic dependency approach,
where closed-class words are needed by the parser,
in the other cases we have removed stopwords (pro-
nouns, prepositions, determiners and modal and
auxiliary verbs).
3.1 Corpus used
We have used a corpus of four billion documents,
crawled from the Web in August 2008. An HTML
parser is used to extract text, the language of each
document is identified, and non-English documents
are discarded. The final corpus remaining at the end
of this process contains roughly 1.6 Terawords. All
calculations are done in parallel sharding by dimen-
sion, and it is possible to calculate all pairwise sim-
ilarities of the words in the test sets very quickly
on this corpus using the MapReduce infrastructure.
A complete run takes around 15 minutes on 2,000
cores.
3.2 Cross-linguality
In order to calculate similarities in a cross-lingual
setting, where some of the words are in a language l
other than English, the following algorithm is used:
21
Method Window size RG dataset WordSim353 dataset
MCR16 0.83 [0.73, 0.89] 0.53 (0.56) [0.45, 0.60]
WN30 0.79 [0.67, 0.86] 0.56 (0.58) [0.48, 0.63]
WN30g 0.83 [0.73, 0.89] 0.66 (0.69) [0.59, 0.71]
CW 1 0.83 [0.73, 0.89] 0.63 [0.57, 0.69]
2 0.83 [0.74, 0.90] 0.60 [0.53, 0.66]
3 0.85 [0.76, 0.91] 0.59 [0.52, 0.65]
4 0.89 [0.82, 0.93] 0.60 [0.53, 0.66]
5 0.80 [0.70, 0.88] 0.58 [0.51, 0.65]
6 0.75 [0.62, 0.84] 0.58 [0.50, 0.64]
7 0.72 [0.58, 0.82] 0.57 [0.49, 0.63]
BoW 1 0.81 [0.70, 0.88] 0.64 [0.57, 0.70]
2 0.80 [0.69, 0.87] 0.64 [0.58, 0.70]
3 0.79 [0.67, 0.86] 0.64 [0.58, 0.70]
4 0.78 [0.66, 0.86] 0.65 [0.58, 0.70]
5 0.77 [0.64, 0.85] 0.64 [0.58, 0.70]
6 0.76 [0.63, 0.85] 0.65 [0.58, 0.70]
7 0.75 [0.62, 0.84] 0.64 [0.58, 0.70]
Syn G1,D0 0.81 [0.70, 0.88] 0.62 [0.55, 0.68]
G2,D0 0.82 [0.72, 0.89] 0.55 [0.48, 0.62]
G3,D0 0.81 [0.71, 0.88] 0.62 [0.56, 0.68]
G1,D1 0.82 [0.72, 0.89] 0.62 [0.55, 0.68]
G2,D1 0.82 [0.73, 0.89] 0.62 [0.55, 0.68]
G3,D1 0.82 [0.72, 0.88] 0.62 [0.55, 0.68]
CW+ 4; G1,D0 0.88 [0.81, 0.93] 0.66 [0.59, 0.71]
Syn 4; G2,D0 0.87 [0.80, 0.92] 0.64 [0.57, 0.70]
4; G3,D0 0.86 [0.77, 0.91] 0.63 [0.56, 0.69]
4; G1,D1 0.83 [0.73, 0.89] 0.48 [0.40, 0.56]
4; G2,D1 0.83 [0.73, 0.89] 0.49 [0.40, 0.56]
4; G3,D1 0.82 [0.72, 0.89] 0.48 [0.40, 0.56]
Table 1: Spearman correlation results for the various WordNet-based
models and distributional models. CW=Context Windows, BoW=bag
of words, Syn=syntactic vectors. For Syn, the window size is actually
the tree-depth for the governors and descendants. For examples, G1
indicates that the contexts include the parents and D2 indicates that both
the children and grandchildren make up the contexts. The final grouping
includes both contextual windows (at width 4) and syntactic contexts in
the template vectors. Max scores are bolded.
1. Replace each non-English word in the dataset
with its 5-best translations into English using
state-of-the-art machine translation technology.
2. The vector corresponding to each Spanish word
is calculated by collecting features from all the
contexts of any of its translations.
3. Once the vectors are generated, the similarities
are calculated in the same way as before.
4 Experimental results
4.1 Gold-standard datasets
We have used two standard datasets. The first
one, RG, consists of 65 pairs of words collected by
Rubenstein and Goodenough (1965), who had them
judged by 51 human subjects in a scale from 0.0 to
4.0 according to their similarity, but ignoring any
other possible semantic relationships that might ap-
pear between the terms. The second dataset, Word-
Sim3535 (Finkelstein et al, 2002) contains 353 word
pairs, each associated with an average of 13 to 16 hu-
man judgements. In this case, both similarity and re-
5Available at http://www.cs.technion.ac.il/
?gabr/resources/data/wordsim353/wordsim353.html
Context RG terms and frequencies
ll never forget the * on his face when grin,2,smile,10
he had a giant * on his face and grin,3,smile,2
room with a huge * on her face and grin,2,smile,6
the state of every * will be updated every automobile,2,car,3
repair or replace the * if it is stolen automobile,2,car,2
located on the north * of the Bay of shore,14,coast,2
areas on the eastern * of the Adriatic Sea shore,3,coast,2
Thesaurus of Current English * The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2
wizard,4,glass,4,crane,5,smile,5
implement,5,oracle,2,lad,2
food,3,car,2,madhouse,3,jewel,3
asylum,4,tool,8,journey,6,etc.
be understood that the * 10 may be designed crane,3,tool,3
a fight between a * and a snake and bird,3,crane,5
Table 2: Sample of context windows for the terms in the RG dataset.
latedness are annotated without any distinction. Sev-
eral studies indicate that the human scores consis-
tently have very high correlations with each other
(Miller and Charles, 1991; Resnik, 1995), thus val-
idating the use of these datasets for evaluating se-
mantic similarity.
For the cross-lingual evaluation, the two datasets
were modified by translating the second word in
each pair into Spanish. Two humans translated
simultaneously both datasets, with an inter-tagger
agreement of 72% for RG and 84% for Word-
Sim353.
4.2 Results
Table 1 shows the Spearman correlation obtained on
the RG and WordSim353 datasets, including the in-
terval at 0.95 of confidence6.
Overall the distributional context-window ap-
proach performs best in the RG, reaching 0.89 corre-
lation, and both WN30g and the combination of con-
text windows and syntactic context perform best on
WordSim353. Note that the confidence intervals are
quite large in both RG and WordSim353, and few of
the pairwise differences are statistically significant.
Regarding WordNet-based approaches, the use of
the glosses and WordNet 3.0 (WN30g) yields the
best results in both datasets. While MCR16 is close
to WN30g for the RG dataset, it lags well behind
on WordSim353. This discrepancy is further ana-
lyzed is Section 5.3. Note that the performance of
WordNet in the WordSim353 dataset suffers from
unknown words. In fact, there are nine pairs which
returned null similarity for this reason. The num-
6To calculate the Spearman correlations values are trans-
formed into ranks, and we calculate the Pearson correlation on
them. The confidence intervals refer to the Pearson correlations
of the rank vectors.
22
Figure 1: Effect of the size of the training corpus, for the best distributional similarity model in each dataset. Left: WordSim353 with bag-of-words,
Right: RG with context windows.
Dataset Method overall ? interval
RG MCR16 0.78 -0.05 [0.66, 0.86]
WN30g 0.74 -0.09 [0.61, 0.84]
Bag of words 0.68 -0.23 [0.53, 0.79]
Context windows 0.83 -0.05 [0.73, 0.89]
WS353 MCR16 0.42 (0.53) -0.11 (-0.03) [0.34, 0.51]
WN30g 0.58 (0.67) -0.07 (-0.02) [0.51, 0.64]
Bag of words 0.53 -0.12 [0.45, 0.61]
Context windows 0.52 -0.11 [0.44, 0.59]
Table 3: Results obtained by the different methods on the Span-
ish/English cross-lingual datasets. The ? column shows the perfor-
mance difference with respect to the results on the original dataset.
ber in parenthesis in Table 1 for WordSim353 shows
the results for the 344 remaining pairs. Section 5.2
shows a proposal to overcome this limitation.
The bag-of-words approach tends to group to-
gether terms that can have a similar distribution of
contextual terms. Therefore, terms that are topically
related can appear in the same textual passages and
will get high values using this model. We see this
as an explanation why this model performed better
than the context window approach for WordSim353,
where annotators were instructed to provide high
ratings to related terms. On the contrary, the con-
text window approach tends to group together words
that are exchangeable in exactly the same context,
preserving order. Table 2 illustrates a few exam-
ples of context collected. Therefore, true synonyms
and hyponyms/hyperonyms will receive high simi-
larities, whereas terms related topically or based on
any other semantic relation (e.g. movie and star) will
have lower scores. This explains why this method
performed better for the RG dataset. Section 5.3
confirms these observations.
4.3 Cross-lingual similarity
Table 3 shows the results for the English-Spanish
cross-lingual datasets. For RG, MCR16 and the
context windows methods drop only 5 percentage
points, showing that cross-lingual similarity is feasi-
ble, and that both cross-lingual strategies are robust.
The results for WordSim353 show that WN30g is
the best for this dataset, with the rest of the meth-
ods falling over 10 percentage points relative to the
monolingual experiment. A closer look at the Word-
Net results showed that most of the drop in perfor-
mance was caused by out-of-vocabulary words, due
to the smaller vocabulary of the Spanish WordNet.
Though not totally comparable, if we compute the
correlation over pairs covered in WordNet alne, the
correlation would drop only 2 percentage points. In
the case of the distributional approaches, the fall in
performance was caused by the translations, as only
61% of the words were translated into the original
word in the English datasets.
5 Detailed analysis and system
combination
In this section we present some analysis, including
learning curves for distributional methods, the use
of distributional similarity to improve WordNet sim-
ilarity, the contrast between similarity and related-
ness, and the combination of methods.
5.1 Learning curves for distributional methods
Figure 1 shows that the correlation improves with
the size of the corpus, as expected. For the re-
sults using the WordSim353 corpus, we show the
results of the bag-of-words approach with context
size 10. Results improve from 0.5 Spearman correla-
tion up to 0.65 when increasing the corpus size three
orders of magnitude, although the effect decays at
the end, which indicates that we might not get fur-
23
Method Without similar words With similar words
WN30 0.56 (0.58) [0.48, 0.63] 0.58 [0.51, 0.65]
WN30g 0.66 (0.69) [0.59, 0.71] 0.68 [0.62, 0.73]
Table 4: Results obtained replacing unknown words with their most
similar three words (WordSim353 dataset).
Method overall Similarity Relatedness
MCR16 0.53 [0.45, 0.60] 0.65 [0.56, 0.72] 0.33 [0.21, 0.43]
WN30 0.56 [0.48, 0.63] 0.73 [0.65, 0.79] 0.38 [0.27, 0.48]
WN30g 0.66 [0.59, 0.71] 0.72 [0.64, 0.78] 0.56 [0.46, 0.64]
BoW 0.65 [0.59, 0.71] 0.70 [0.63, 0.77] 0.62 [0.53, 0.69]
CW 0.60 [0.53, 0.66] 0.77 [0.71, 0.82] 0.46 [0.36, 0.55]
Table 5: Results obtained on the WordSim353 dataset and on the two
similarity and relatedness subsets.
ther gains going beyond the current size of the cor-
pus. With respect to results for the RG dataset, we
used a context-window approach with context radius
4. Here, results improve even more with data size,
probably due to the sparse data problem collecting
8-word context windows if the corpus is not large
enough. Correlation improves linearly right to the
end, where results stabilize around 0.89.
5.2 Combining both approaches: dealing with
unknown words in WordNet
Although the vocabulary of WordNet is very ex-
tensive, applications are bound to need the similar-
ity between words which are not included in Word-
Net. This is exemplified in the WordSim353 dataset,
where 9 pairs contain words which are unknown to
WordNet. In order to overcome this shortcoming,
we could use similar words instead, as provided by
the distributional thesaurus. We used the distribu-
tional thesaurus defined in Section 3, using context
windows of width 4, to provide three similar words
for each of the unknown words in WordNet. Results
improve for both WN30 and WN30g, as shown in
Table 4, attaining our best results for WordSim353.
5.3 Similarity vs. relatedness
We mentioned above that the annotation guidelines
of WordSim353 did not distinguish between simi-
lar and related pairs. As the results in Section 4
show, different techniques are more appropriate to
calculate either similarity or relatedness. In order to
study this effect, ideally, we would have two ver-
sions of the dataset, where annotators were given
precise instructions to distinguish similarity in one
case, and relatedness in the other. Given the lack
of such datasets, we devised a simpler approach in
order to reuse the existing human judgements. We
manually split the dataset in two parts, as follows.
First, two humans classified all pairs as be-
ing synonyms of each other, antonyms, iden-
tical, hyperonym-hyponym, hyponym-hyperonym,
holonym-meronym, meronym-holonym, and none-
of-the-above. The inter-tagger agreement rate was
0.80, with a Kappa score of 0.77. This anno-
tation was used to group the pairs in three cate-
gories: similar pairs (those classified as synonyms,
antonyms, identical, or hyponym-hyperonym), re-
lated pairs (those classified as meronym-holonym,
and pairs classified as none-of-the-above, with a hu-
man average similarity greater than 5), and unrelated
pairs (those classified as none-of-the-above that had
average similarity less than or equal to 5). We then
created two new gold-standard datasets: similarity
(the union of similar and unrelated pairs), and relat-
edness (the union of related and unrelated)7.
Table 5 shows the results on the relatedness and
similarity subsets of WordSim353 for the different
methods. Regarding WordNet methods, both WN30
and WN30g perform similarly on the similarity sub-
set, but WN30g obtains the best results by far on
the relatedness data. These results are congruent
with our expectations: two words are similar if their
synsets are in close places in the WordNet hierarchy,
and two words are related if there is a connection
between them. Most of the relations in WordNet
are of hierarchical nature, and although other rela-
tions exist, they are far less numerous, thus explain-
ing the good results for both WN30 and WN30g on
similarity, but the bad results of WN30 on related-
ness. The disambiguated glosses help find connec-
tions among related concepts, and allow our method
to better model relatedness with respect to WN30.
The low results for MCR16 also deserve some
comments. Given the fact that MCR16 performed
very well on the RG dataset, it comes as a surprise
that it performs so poorly for the similarity subset
of WordSim353. In an additional evaluation, we at-
tested that MCR16 does indeed perform as well as
MCR30g on the similar pairs subset. We believe
that this deviation could be due to the method used to
construct the similarity dataset, which includes some
pairs of loosely related pairs labeled as unrelated.
7Available at http://alfonseca.org/eng/research/wordsim353.html
24
Methods combined in the SVM RG dataset WordSim353 dataset WordSim353 similarity WordSim353 relatedness
WN30g, bag of words 0.88 [0.82, 0.93] 0.78 [0.73, 0.81] 0.81 [0.76, 0.86] 0.72 [0.65, 0.77]
WN30g, context windows 0.90 [0.84, 0.94] 0.73 [0.68, 0.79] 0.83 [0.78, 0.87] 0.64 [0.56, 0.71]
WN30g, syntax 0.89 [0.83, 0.93] 0.75 [0.70, 0.79] 0.83 [0.78, 0.87] 0.67 [0.60, 0.74]
WN30g, bag of words, context windows, syntax 0.96 [0.93, 0.97] 0.78 [0.73, 0.82] 0.83 [0.78, 0.87] 0.71 [0.65, 0.77]
Table 6: Results using a supervised combination of several systems. Max values are bolded for each dataset.
Concerning the techniques based on distributional
similarities, the method based on context windows
provides the best results for similarity, and the bag-
of-words representation outperforms most of the
other techniques for relatedness.
5.4 Supervised combination
In order to gain an insight on which would be the up-
per bound that we could obtain when combining our
methods, we took the output of three systems (bag
of words with window size 10, context window with
size 4, and the WN30g run). Each of these outputs is
a ranking of word pairs, and we implemented an or-
acle that chooses, for each pair, the rank that is most
similar to the rank of the pair in the gold-standard.
The outputs of the oracle have a Spearman correla-
tion of 0.97 for RG and 0.92 for WordSim353, which
gives as an indication of the correlations that could
be achieved by choosing for each pair the rank out-
put by the best classifier for that pair.
The previous results motivated the use of a su-
pervised approach to combine the output of the
different systems. We created a training cor-
pus containing pairs of pairs of words from the
datasets, having as features the similarity and rank
of each pair involved as given by the differ-
ent unsupervised systems. A classifier is trained
to decide whether the first pair is more simi-
lar than the second one. For example, a train-
ing instance using two unsupervised classifiers is
0.001364, 31, 0.327515, 64, 0.084805, 57, 0.109061, 59, negative
meaning that the similarities given by the first clas-
sifier to the two pairs were 0.001364 and 0.327515
respectively, which ranked them in positions 31 and
64. The second classifier gave them similarities of
0.084805 and 0.109061 respectively, which ranked
them in positions 57 and 59. The class negative in-
dicates that in the gold-standard the first pair has a
lower score than the second pair.
We have trained a SVM to classify pairs of pairs,
and use its output to rank the entries in both datasets.
It uses a polynomial kernel with degree 4. We did
Method Source Spearman (MC) Pearson (MC)
(Sahami et al, 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78]
(Chen et al, 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85]
(Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89]
(Leacock et al, 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91]
(Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90]
(Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92]
(Bollegala et al, 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92]
(Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93]
(Jarmasz, 2003) Roget?s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94]
(Patwardhan et al, 2006) WordNet n/a 0.91
(Alvarez and Lim, 2007) WordNet n/a 0.91
(Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96]
(Hughes et al, 2007) WordNet 0.90 n/a
Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a
Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93]
Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95]
Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87]
SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97]
Table 7: Comparison with previous approaches for MC.
not have a held-out set, so we used the standard set-
tings of Weka, without trying to modify parameters,
e.g. C. Each word pair is scored with the number
of pairs that were considered to have less similar-
ity using the SVM. The results using 10-fold cross-
validation are shown in Table 6. A combination of
all methods produces the best results reported so far
for both datasets, statistically significant for RG.
6 Related work
Contrary to the WordSim353 dataset, common prac-
tice with the RG dataset has been to perform the
evaluation with Pearson correlation. In our believe
Pearson is less informative, as the Pearson correla-
tion suffers much when the scores of two systems are
not linearly correlated, something which happens
often given due to the different nature of the tech-
niques applied. Some authors, e.g. Alvarez and Lim
(2007), use a non-linear function to map the system
outputs into new values distributed more similarly
to the values in the gold-standard. In their case, the
mapping function was exp (?x4 ), which was chosenempirically. Finding such a function is dependent
on the dataset used, and involves an extra step in the
similarity calculations. Alternatively, the Spearman
correlation provides an evaluation metric that is in-
dependent of such data-dependent transformations.
Most similarity researchers have published their
25
Word pair M&C SVM Word pair M&C SVM
automobile, car 3.92 62 crane, implement 1.68 26
journey, voyage 3.84 54 brother, lad 1.66 39
gem, jewel 3.84 61 car, journey 1.16 37
boy, lad 3.76 57 monk, oracle 1.1 32
coast, shore 3.7 53 food, rooster 0.89 3
asylum, madhouse 3.61 45 coast, hill 0.87 34
magician, wizard 3.5 49 forest, graveyard 0.84 27
midday, noon 3.42 61 monk, slave 0.55 17
furnace, stove 3.11 50 lad, wizard 0.42 13
food, fruit 3.08 47 coast, forest 0.42 18
bird, cock 3.05 46 cord, smile 0.13 5
bird, crane 2.97 38 glass, magician 0.11 10
implement, tool 2.95 55 rooster, voyage 0.08 1
brother, monk 2.82 42 noon, string 0.08 5
Table 8: Our best results for the MC dataset.
Method Source Spearman
(Strube and Ponzetto, 2006) Wikipedia 0.19?0.48
(Jarmasz, 2003) WordNet 0.33?0.35
(Jarmasz, 2003) Roget?s 0.55
(Hughes and Ramage, 2007) WordNet 0.55
(Finkelstein et al, 2002) Web corpus, WN 0.56
(Gabrilovich and Markovitch, 2007) ODP 0.65
(Gabrilovich and Markovitch, 2007) Wikipedia 0.75
SVM Web corpus, WN 0.78
Table 9: Comparison with previous work for WordSim353.
complete results on a smaller subset of the RG
dataset containing 30 word pairs (Miller and
Charles, 1991), usually referred to as MC, making it
possible to compare different systems using differ-
ent correlation. Table 7 shows the results of related
work on MC that was available to us, including our
own. For the authors that did not provide the de-
tailed data we include only the Pearson correlation
with no confidence intervals.
Among the unsupervised methods introduced in
this paper, the context window produced the best re-
ported Spearman correlation, although the 0.95 con-
fidence intervals are too large to allow us to accept
the hypothesis that it is better than all others meth-
ods. The supervised combination produces the best
results reported so far. For the benefit of future re-
search, our results for the MC subset are displayed
in Table 8.
Comparison on the WordSim353 dataset is eas-
ier, as all researchers have used Spearman. The
figures in Table 9) show that our WordNet-based
method outperforms all previously published Word-
Net methods. We want to note that our WordNet-
based method outperforms that of Hughes and Ram-
age (2007), which uses a similar method. Although
there are some differences in the method, we think
that the main performance gain comes from the use
of the disambiguated glosses, which they did not
use. Our distributional methods also outperform all
other corpus-based methods. The most similar ap-
proach to our distributional technique is Finkelstein
et al (2002), who combined distributional similar-
ities from Web documents with a similarity from
WordNet. Their results are probably worse due to
the smaller data size (they used 270,000 documents)
and the differences in the calculation of the simi-
larities. The only method which outperforms our
non-supervised methods is that of (Gabrilovich and
Markovitch, 2007) when based on Wikipedia, prob-
ably because of the dense, manually distilled knowl-
edge contained in Wikipedia. All in all, our super-
vised combination gets the best published results on
this dataset.
7 Conclusions and future work
This paper has presented two state-of-the-art dis-
tributional and WordNet-based similarity measures,
with a study of several parameters, including per-
formance on similarity and relatedness data. We
show that the use of disambiguated glosses allows
for the best published results for WordNet-based
systems on the WordSim353 dataset, mainly due to
the better modeling of relatedness (as opposed to
similarity). Distributional similarities have proven
to be competitive when compared to knowledge-
based methods, with context windows being better
for similarity and bag of words for relatedness. Dis-
tributional similarity was effectively used to cover
out-of-vocabulary items in the WordNet-based mea-
sure providing our best unsupervised results. The
complementarity of our methods was exploited by
a supervised learner, producing the best results so
far for RG and WordSim353. Our results include
confidence values, which, surprisingly, were not in-
cluded in most previous work, and show that many
results over RG and WordSim353 are indistinguish-
able. The algorithm for WordNet-base similarity
and the necessary resources are publicly available8.
This work pioneers cross-lingual extension and
evaluation of both distributional and WordNet-based
measures. We have shown that closely aligned
wordnets provide a natural and effective way to
compute cross-lingual similarity with minor losses.
A simple translation strategy also yields good results
for distributional methods.
8http://ixa2.si.ehu.es/ukb/
26
References
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proc. of EACL
2009, Athens, Greece.
M.A. Alvarez and S.J. Lim. 2007. A Graph Modeling
of Semantic Similarity between Words. Proc. of the
Conference on Semantic Computing, pages 355?362.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The meaning multi-
lingual central repository. In Proc. of Global WordNet
Conference, Brno, Czech Republic.
D. Bollegala, Matsuo Y., and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proceedings of WWW?2007.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness.
Computational Linguistics, 32(1):13?47.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proceedings of COCLING/ACL 2006.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets using structural information. In Proceedings of
ACL?2000, Hong Kong.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press,
Cambridge, Mass.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Transactions on Information Systems, 20(1):116?131.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proc of IJCAI, pages 6?12.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02: Proceedings of the 11th international con-
ference on World Wide Web, pages 517?526.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581?589.
M. Jarmasz. 2003. Roget?s Thesuarus as a lexical re-
source for Natural Language Processing.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, volume 33. Taiwan.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. WordNet: An Electronic Lexical Database,
49(2):265?283.
D. Lin. 1998a. An information-theoretic definition of
similarity. In Proc. of ICML, pages 296?304, Wiscon-
sin, USA.
D. Lin. 1998b. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of ACL-98.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
J. Nivre. 2006. Inductive Dependency Parsing, vol-
ume 34 of Text, Speech and Language Technology.
Springer.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Re-
latedness of Concepts. In Proceedings of the EACL
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Pycholinguistics Together,
pages 1?8, Trento, Italy.
P. Resnik. 1995. Using Information Content to Evaluate
Semantic Similarity in a Taxonomy. Proc. of IJCAI,
14:448?453.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
M Ruiz-Casado, E. Alfonseca, and P. Castells. 2005.
Using context-window overlapping in Synonym Dis-
covery and Ontology Extension. In Proceedings of
RANLP-2005, Borovets, Bulgaria,.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. Proc. of WWW, pages 377?386.
M. Strube and S.P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the AAAI-2006, pages 1419?1424.
P.D. Turney. 2001. Mining the Web for Synonyms: PMI-
IR versus LSA on TOEFL. Lecture Notes in Computer
Science, 2167:491?502.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proc. of ACL, pages 133?138, Las
Cruces, New Mexico.
D. Yang and D.M.W. Powers. 2005. Measuring semantic
similarity in the taxonomy of WordNet. Proceedings
of the Australasian conference on Computer Science.
27
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 585?593,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Two graph-based algorithms for state-of-the-art WSD
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle and Aitor Soroa
IXA NLP Group
University of the Basque Country
Donostia, Basque Contry
a.soroa@si.ehu.es
Abstract
This paper explores the use of two graph
algorithms for unsupervised induction and
tagging of nominal word senses based on
corpora. Our main contribution is the op-
timization of the free parameters of those
algorithms and its evaluation against pub-
licly available gold standards. We present
a thorough evaluation comprising super-
vised and unsupervised modes, and both
lexical-sample and all-words tasks. The
results show that, in spite of the infor-
mation loss inherent to mapping the in-
duced senses to the gold-standard, the
optimization of parameters based on a
small sample of nouns carries over to all
nouns, performing close to supervised sys-
tems in the lexical sample task and yield-
ing the second-best WSD systems for the
Senseval-3 all-words task.
1 Introduction
Word sense disambiguation (WSD) is a key
enabling-technology. Supervised WSD tech-
niques are the best performing in public evalu-
ations, but need large amounts of hand-tagged
data. Existing hand-annotated corpora like Sem-
Cor (Miller et al, 1993), which is annotated with
WordNet senses (Fellbaum, 1998) allow for a
small improvement over the simple most frequent
sense heuristic, as attested in the all-words track of
the last Senseval competition (Snyder and Palmer,
2004). In theory, larger amounts of training data
(SemCor has approx. 700K words) would improve
the performance of supervised WSD, but no cur-
rent project exists to provide such an expensive re-
source.
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a tar-
get word are a closed list coming from a dic-
tionary or lexicon. Lexicographers and seman-
ticists have long warned about the problems of
such an approach, where senses are listed sepa-
rately as discrete entities, and have argued in fa-
vor of more complex representations, where, for
instance, senses are dense regions in a contin-
uum (Cruse, 2000).
Unsupervised WSD has followed this line of
thinking, and tries to induce word senses directly
from the corpus. Typical unsupervised WSD sys-
tems involve clustering techniques, which group
together similar examples. Given a set of induced
clusters (which represent word uses or senses1),
each new occurrence of the target word will be
compared to the clusters and the most similar clus-
ter will be selected as its sense.
Most of the unsupervised WSD work has been
based on the vector space model, where each
example is represented by a vector of features
(e.g. the words occurring in the context), and
the induced senses are either clusters of ex-
amples (Schu?tze, 1998; Purandare and Peder-
sen, 2004) or clusters of words (Pantel and Lin,
2002). Recently, Ve?ronis (Ve?ronis, 2004) has pro-
posed HyperLex, an application of graph models
to WSD based on the small-world properties of
cooccurrence graphs. Graph-based methods have
gained attention in several areas of NLP, including
knowledge-based WSD (Mihalcea, 2005; Navigli
and Velardi, 2005) and summarization (Erkan and
Radev, 2004; Mihalcea and Tarau, 2004).
The HyperLex algorithm presented in (Ve?ronis,
2004) is entirely corpus-based. It builds a cooccur-
rence graph for all pairs of words cooccurring in
the context of the target word. Ve?ronis shows that
this kind of graph fulfills the properties of small
world graphs, and thus possesses highly connected
1Unsupervised WSD approaches prefer the term ?word
uses? to ?word senses?. In this paper we use them inter-
changeably to refer to both the induced clusters, and to the
word senses from some reference lexicon.
585
components (hubs) in the graph. These hubs even-
tually identify the main word uses (senses) of the
target word, and can be used to perform word
sense disambiguation. These hubs are used as a
representation of the senses induced by the sys-
tem, the same way that clusters of examples are
used to represent senses in clustering approaches
to WSD (Purandare and Pedersen, 2004).
One of the problems of unsupervised systems
is that of managing to do a fair evaluation.
Most of current unsupervised systems are evalu-
ated in-house, with a brief comparison to a re-
implementation of a former system, leading to a
proliferation of unsupervised systems with little
ground to compare among them.
In preliminary work (Agirre et al, 2006), we
have shown that HyperLex compares favorably
to other unsupervised systems. We defined a
semi-supervised setting for optimizing the free-
parameters of HyperLex on the Senseval-2 En-
glish Lexical Sample task (S2LS), which con-
sisted on mapping the induced senses onto the
official sense inventory using the training part of
S2LS. The best parameters were then used on the
Senseval-3 English Lexical Sample task (S3LS),
where a similar semi-supervised method was used
to output the official sense inventory.
This paper extends the previous work in sev-
eral aspects. First of all, we adapted the PageR-
ank graph-based method (Brin and Page, 1998)
for WSD and compared it with HyperLex.
We also extend the previous evaluation scheme,
using measures in the clustering community which
only require a gold standard clustering and no
mapping step. This allows for having a purely
unsupervised WSD system, and at the same time
comparing supervised and unsupervised systems
according to clustering criteria.
We also include the Senseval-3 English All-
words testbed (S3AW), where, in principle, unsu-
pervised and semi-supervised systems have an ad-
vantage over purely supervised systems due to the
scarcity of training data. We show that our sys-
tem is competitive with supervised systems, rank-
ing second.
This paper is structured as follows. We first
present two graph-based algorithms, HyperLex
and PageRank. Section 3 presents the two evalu-
ation frameworks. Section 4 introduces parameter
optimization. Section 5 shows the experimental
setting and results. Section 6 analyzes the results
and presents related work. Finally, we draw the
conclusions and advance future work.
2 A graph algorithm for corpus-based
WSD
The basic steps for our implementation of Hyper-
Lex and its variant using PageRank are common.
We first build the cooccurrence graph, then we se-
lect the hubs that are going to represent the senses
using two different strategies inspired by Hyper-
Lex and PageRank. We are then ready to use the
induced senses to do word sense disambiguation.
2.1 Building cooccurrence graphs
For each word to be disambiguated, a text corpus
is collected, consisting of the paragraphs where
the word occurs. From this corpus, a cooccur-
rence graph for the target word is built. Vertices
in the graph correspond to words2 in the text (ex-
cept the target word itself). Two words appear-
ing in the same paragraph are said to cooccur, and
are connected with edges. Each edge is assigned
a weight which measures the relative frequency of
the two words cooccurring. Specifically, let wij be
the weight of the edge3 connecting nodes i and j,
then wij = 1 ? max[P (i | j), P (j | i)], where
P (i | j) = freqijfreqj and P (j | i) =
freqij
freqi .The weight of an edge measures how tightly
connected the two words are. Words which always
occur together receive a weight of 0. Words rarely
cooccurring receive weights close to 1.
2.2 Selecting hubs: HyperLex vs. PageRank
Once the cooccurrence graph is built, Ve?ronis pro-
poses a simple iterative algorithm to obtain its
hubs. At each step, the algorithm finds the ver-
tex with highest relative frequency4 in the graph,
and, if it meets some criteria, it is selected as a hub.
These criteria are determined by a set of heuristic
parameters, that will be explained later in Section
4. After a vertex is selected to be a hub, its neigh-
bors are no longer eligible as hub candidates. At
any time, if the next vertex candidate has a relative
frequency below a certain threshold, the algorithm
stops.
Another alternative is to use the PageRank algo-
rithm (Brin and Page, 1998) for finding hubs in the
2Following Ve?ronis, we only work on nouns.
3The cooccurrence graph is undirected, i.e. wij = wji
4In cooccurrence graphs, the relative frequency of a vertex
and its degree are linearly related, and it is therefore possible
to avoid the costly computation of the degree.
586
coocurrence graph. PageRank is an iterative algo-
rithm that ranks all the vertices according to their
relative importance within the graph following a
random-walk model. In this model, a link between
vertices v1 and v2 means that v1 recommends v2.
The more vertices recommend v2, the higher the
rank of v2 will be. Furthermore, the rank of a ver-
tex depends not only on how many vertices point
to it, but on the rank of these vertices as well.
Although PageRank was initially designed to
work with directed graphs, and with no weights
in links, the algorithm can be easily extended
to model undirected graphs whose edges are
weighted. Specifically, let G = (V, E) be an undi-
rected graph with the set of vertices V and set of
edges E. For a given vertex vi, let In(vi) be the set
of vertices pointing to it5. The rank of vi is defined
as:
P (vi) = (1? d) + d
?
j?In(vi)
wji
?
k?In(vj) wjk
P (vj)
where wij is the weight of the link between ver-
tices vi and vj , and 0 ? d ? 1. d is called the
damping factor and models the probability of a
web surfer standing at a vertex to follow a link
from this vertex (probability d) or to jump to a ran-
dom vertex in the graph (probability 1 ? d). The
factor is usually set at 0.85.
The algorithm initializes the ranks of the ver-
tices with a fixed value (usually 1N for a graph with
N vertices) and iterates until convergence below a
given threshold is achieved, or, more typically, un-
til a fixed number of iterations are executed. Note
that the convergence of the algorithms doesn?t de-
pend on the initial value of the ranks.
After running the algorithm, the vertices of the
graph are ordered in decreasing order according to
its rank, and a number of them are chosen as the
main hubs of the word. The hubs finally selected
depend again of some heuristics and will be de-
scribed in section 4.
2.3 Using hubs for WSD
Once the hubs that represent the senses of the word
are selected (following any of the methods pre-
sented in the last section), each of them is linked
to the target word with edges weighting 0, and
the Minimum Spanning Tree (MST) of the whole
graph is calculated and stored.
5As G is undirected, the in-degree of a vertex v is equal
to its out-degree.
The MST is then used to perform word sense
disambiguation, in the following way. For every
instance of the target word, the words surrounding
it are examined and looked up in the MST. By con-
struction of the MST, words in it are placed under
exactly one hub. Each word in the context receives
a set of scores s, with one score per hub, where all
scores are 0 except the one corresponding to the
hub where it is placed. If the scores are organized
in a score vector, all values are 0, except, say, the
i-th component, which receives a score d(hi, v),
which is the distance between the hub hi and the
node representing the word v. Thus, d(hi, v) as-
signs a score of 1 to hubs and the score decreases
as the nodes move away from the hub in the tree.
For a given occurrence of the target word, the
score vectors of all the words in the context are
added, and the hub that receives the maximum
score is chosen.
3 Evaluating unsupervised WSD systems
All unsupervised WSD algorithms need some ad-
dition in order to be evaluated. One alternative, as
in (Ve?ronis, 2004), is to manually decide the cor-
rectness of the hubs assigned to each occurrence
of the words. This approach has two main disad-
vantages. First, it is expensive to manually verify
each occurrence of the word, and different runs of
the algorithm need to be evaluated in turn. Sec-
ond, it is not an easy task to manually decide if
an occurrence of a word effectively corresponds
with the use of the word the assigned hub refers
to, specially considering that the person is given a
short list of words linked to the hub. Besides, it is
widely acknowledged that people are leaned not to
contradict the proposed answer.
A second alternative is to evaluate the system
according to some performance in an application,
e.g. information retrieval (Schu?tze, 1998). This is
a very attractive idea, but requires expensive sys-
tem development and it is sometimes difficult to
separate the reasons for the good (or bad) perfor-
mance.
A third alternative would be to devise a method
to map the hubs (clusters) returned by the system
to the senses in a lexicon. Pantel and Lin (2002)
automatically mapped the senses to WordNet, and
then measured the quality of the mapping. More
recently, tagged corpora have been used to map
the induced senses, and then compare the sys-
tems over publicly available benchmarks (Puran-
587
dare and Pedersen, 2004; Niu et al, 2005; Agirre
et al, 2006), which offers the advantage of com-
paring to other systems, but converts the whole
system into semi-supervised. See Section 5 for
more details on these systems. Note that the map-
ping introduces noise and information loss, which
is a disadvantage when comparing to other sys-
tems that rely on the gold-standard senses.
Yet another possibility is to evaluate the induced
senses against a gold standard as a clustering task.
Induced senses are clusters, gold standard senses
are classes, and measures from the clustering lit-
erature like entropy or purity can be used. In this
case the manually tagged corpus is taken to be the
gold standard, where a class is the set of examples
tagged with a sense.
We decided to adopt the last two alternatives,
since they allow for comparison over publicly
available systems of any kind.
3.1 Evaluation of clustering: hubs as clusters
In this setting the selected hubs are treated as
clusters of examples and gold standard senses are
classes. In order to compare the clusters with the
classes, hand annotated corpora are needed (for in-
stance Senseval). The test set is first tagged with
the induced senses. A perfect clustering solution
will be the one where each cluster has exactly the
same examples as one of the classes, and vice
versa. The evaluation is completely unsupervised.
Following standard cluster evaluation prac-
tice (Zhao and Karypis, 2005), we consider three
measures: entropy, purity and Fscore. The entropy
measure considers how the various classes of ob-
jects are distributed within each cluster. In gen-
eral, the smaller the entropy value, the better the
clustering algorithm performs. The purity mea-
sure considers the extent to which each cluster
contained objects from primarily one class. The
larger the values of purity, the better the cluster-
ing algorithm performs. The Fscore is used in a
similar fashion to Information Retrieval exercises,
with precision and recall defined as the percent-
age of correctly ?retrieved? examples for a clus-
ter (divided by total cluster size), and recall as the
percentage of correctly ?retrieved? examples for a
cluster (divided by total class size). For a formal
definition refer to (Zhao and Karypis, 2005). If the
clustering is identical to the original classes in the
datasets, FScore will be equal to one which means
that the higher the FScore, the better the clustering
is.
3.2 Evaluation as supervised WSD: mapping
hubs to senses
(Agirre et al, 2006) presents a straightforward
framework that uses hand-tagged material in or-
der to map the induced senses into the senses used
in a gold standard . The WSD system first tags the
training part of some hand-annotated corpus with
the induced hubs. The hand labels are then used
to construct a matrix relating assigned hubs to ex-
isting senses, simply counting the times an occur-
rence with sense sj has been assigned hub hi. In
the testing step we apply the WSD algorithm over
the test corpus, using the hubs-to-senses matrix to
select the sense with highest weights. See (Agirre
et al, 2006) for further details.
4 Tuning the parameters
The behavior of the original HyperLex algorithm
was influenced by a set of heuristic parameters,
which were set by Ve?ronis following his intuition.
In (Agirre et al, 2006) we tuned the parameters us-
ing the mapping strategy for evaluation. We set a
range for each of the parameters, and evaluated the
algorithm for each combination of the parameters
on a fixed set of words (S2LS), which was differ-
ent from the final test sets (S3LS and S3AW). This
ensures that the chosen parameter set can be used
for any noun, and is not overfitted to a small set of
nouns.
In this paper, we perform the parameter tuning
according to four different criteria, i.e., best su-
pervised performance and best unsupervised en-
tropy/purity/FScore performance. At the end, we
have four sets of parameters (those that obtained
the best results in S2LS for each criterion), and
each set is then selected to be run against the S3LS
and S3AW datasets.
The parameters of the graph-based algorithm
can be divided in two sets: those that affect how
the cooccurrence graph is built (p1?p4 below), and
those that control the way the hubs are extracted
from it (p5?p8 below).
p1 Minimum frequency of edges (occurrences)
p2 Minimum frequency of vertices (words)
p3 Edges with weights above this value are removed
p4 Context containing fewer words are not processed
p5 Minimum number of adjacent vertices in a hub
p6 Max. mean weight of the adjacent vertices of a hub
p7 Minimum frequency of hubs
p8 Number of selected hubs
588
Vr opt Pr fr (p7) and Pr fx (p8)
Vr Range Best Range Best
p1 5 1-3 1 1-3 2
p2 10 2-4 3 2-4 3
p3 .9 .3-.7 .4 .4-.5 .5
p4 4 4 4 4 4
p5 6 1-7 1 ? ?
p6 .8 .6-.95 .95 ? ?
p7 .001 .0009-.003 .001 .0015-.0025 .0016
p8 ? ? ? 50-65 55
Table 1: Parameters of the HyperLex algorithm
Both strategies to select hubs from the coocur-
rence graph (cf. Sect. 2.2) share parameters p1?
p4. The algorithm proposed by Ve?ronis uses p5?
p6 as requirements for hubs, and p7 as the thresh-
old to stop looking for more hubs: candidates with
frequency below p7 are not eligible to be hubs.
Regarding PageRank the original formulation
does not have any provision for determining which
are hubs and which not, it just returns a weighted
list of vertices. We have experimented with two
methods: a threshold for the frequency of the hubs
(as before, p7), and a fixed number of hubs for ev-
ery target word (p8). For a shorthand we use Vr for
Veronis? original formulation with default param-
eters, Vr opt for optimized parameters, and Pr fr
and Pr fx respectively for the two ways of using
PageRank.
Table 1 lists the parameters of the HyperLex al-
gorithm, with the default values proposed for them
in the original work (second column), the ranges
that we explored, and the optimal values according
to the supervised recall evaluation (cf. Sect. 3.1).
For Vr opt we tried 6700 combinations. PageRank
has less parameters, and we also used the previous
optimization of Vr opt to limit the range of p4, so
Pr fr and Pr fx get respectively 180 and 288 com-
binations.
5 Experiment setting and results
To evaluate the HyperLex algorithm in a standard
benchmark, we will first focus on a more exten-
sive evaluation of S3LS and then see the results
in S3AW (cf. Sec. 5.4). Following the design
for evaluation explained in Section 3, we use the
standard train-test split for the supervised evalua-
tion, while the unsupervised evaluation only uses
the test part.
Table 2 shows the results of the 4 variants of
our algorithm. Vr stands for the original Vero-
nis algorithm with default parameters, Vr opt to
our optimized version, and Pr fr and Pr fx to the
Sup. Unsupervised
Rec. Entr. Pur. FS
Vr 59.9 50.3 58.2 44.1
Vr opt 64.6 18.3 78.5 35.0
Pr fr 64.5 18.7 77.2 34.3
Pr fx 62.2 25.4 72.2 33.3
1ex-1hub 40.1 0.0 100.0 14.5
MFS 54.5 53.2 52.8 28.3
S3LS-best 72.9 19.9 67.3 63.8
kNN-all 70.6 21.2 64.0 60.6
kNN-BoW 63.5 22.6 61.1 57.1
Cymfony (10%-S3LS) 57.9 25.0 55.7 52.0
Prob0 (MFS-S3) 54.2 28.8 49.3 46.0
clr04 (MFS-Sc) 48.8 25.8 52.5 46.2
Ciaosenso (MFS-Sc) 48.7 28.0 50.3 48.8
duluth-senserelate 47.5 27.2 51.1 44.9
Table 2: Results for the nouns in S3LS using the 4 meth-
ods (Vr, Vr opt, Pr fr and Pr fx). Each of the methods was
optimized in S2LS using the 4 evaluation criteria (Supervised
recall, Entropy, Purity and Fscore) and evaluated on S3LS ac-
cording to the respective evaluation criteria (in the columns).
Two baselines, plus 3 supervised and 5 unsupervised systems
are also shown. Bold is used for best results in each category.
two variants of PageRank. In the columns we find
the evaluation results according to our 4 criteria.
For supervised evaluation we indicate only recall,
which in our case equals precision, as the cover-
age is 100% in all cases (values returned by the
official Senseval scorer). We also include 2 base-
lines, a system returning a single cluster (that of
the most frequent sense, MFS), and another re-
turning one cluster for each example (1ex-1hub).
The last rows list the results for 3 supervised and
5 unsupervised systems (see Sect. 5.1). We will
comment on the result of this table from different
perspectives.
5.1 Supervised evaluation
In this subsection we will focus in the first four
evaluation rows in Table 2. All variants of the al-
gorithm outperform by an ample margin the MFS
and the 1ex-1hub baselines when evaluated on
S3LS recall. This means that the method is able
to learn useful hubs. Note that we perform this su-
pervised evaluation just for comparison with other
systems, and to prove that we are able to provide
high performance WSD.
The default parameter setting (Vr) gets the
worst results, followed by the fixed-hub imple-
mentation of PageRank (Pr fx). Pagerank with
frequency threshold (Pr fr) and the optimized
Veronis (Vr opt) obtain a 10 point improvement
over the MFS baseline with very similar results
(the difference is not statistically significant ac-
cording to McNemar?s test at 95% confidence
589
level).
Table 2 also shows the results of three super-
vised systems. These results (and those of the
other unsupervised systems in the table) where ob-
tained from the Senseval website, and the only
processing we did was to filter nouns. S3LS-best
stands for the the winner of S3LS (Mihalcea et al,
2004), which is 8.3 points over our method. We
also include the results of two of our in-house sys-
tems. kNN-all is a state-of-the-art system (Agirre
et al, 2005) using wide range of local and top-
ical features, and only 2.3 points below the best
S3LS system. kNN-BoW which is the same super-
vised system, but restricted to bag-of-words fea-
tures only, which are the ones used by our graph-
based systems. The table shows that Vr opt and
Pr fr are one single point from kNN-BoW, which
is an impressive result if we take into account the
information loss of the mapping step and that we
tuned our parameters on a different set of words.
The last 5 rows of Table 2 show several un-
supervised systems, all of which except Cym-
fony (Niu et al, 2005) and (Purandare and Ped-
ersen, 2004) participated in S3LS (check (Mihal-
cea et al, 2004) for further details on the systems).
We classify them according to the amount of ?su-
pervision? they have: some have access to most-
frequent information (MFS-S3 if counted over
S3LS, MFS-Sc if counted over SemCor), some use
10% of the S3LS training part for mapping (10%-
S3LS). Only one system (Duluth) did not use in
any way hand-tagged corpora.
The table shows that Vr opt and Pr fr are more
than 6 points above the other unsupervised sys-
tems, but given the different typology of unsuper-
vised systems, it?s unfair to draw definitive con-
clusions from a raw comparison of results. The
system coming closer to ours is that described in
(Niu et al, 2005). They use hand tagged corpora
which does not need to include the target word to
tune the parameters of a rather complex clustering
method which does use local features. They do use
the S3LS training corpus for mapping. For every
sense of the target word, three of its contexts in
the train corpus are gathered (around 10% of the
training data) and tagged. Each cluster is then re-
lated with its most frequent sense. The mapping
method is similar to ours, but we use all the avail-
able training data and allow for different hubs to
be assigned to the same sense.
Another system similar to ours is (Purandare
and Pedersen, 2004), which unfortunately was
evaluated on Senseval 2 data and is not included
in the table. The authors use first and second or-
der bag-of-word context features to represent each
instance of the corpus. They apply several cluster-
ing algorithms based on the vector space model,
limiting the number of clusters to 7. They also
use all available training data for mapping, but
given their small number of clusters they opt for a
one-to-one mapping which maximizes the assign-
ment and discards the less frequent clusters. They
also discard some difficult cases, like senses and
words with low frequencies (10% of total occur-
rences and 90, respectively). The different test set
and mapping system make the comparison diffi-
cult, but the fact that the best of their combina-
tions beats MFS by 1 point on average (47.6% vs.
46.4%) for the selected nouns and senses make us
think that our results are more robust (nearly 10%
over MFS).
5.2 Clustering evaluation
The three columns corresponding to fully unsu-
pervised evaluation in Table 2 show that all our
3 optimized variants easily outperform the MFS
baseline. The best results are in this case for the
optimized Veronis, followed closely by Pagerank
with frequency threshold.
The comparison with the supervised and unsu-
pervised systems shows that our system gets better
entropy and purity values, but worse FScore. This
can be explained by the bias of entropy and purity
towards smaller and more numerous clusters. In
fact the 1ex-1hub baseline obtains the best entropy
and purity scores. Our graph-based system tends
to induce a large number of senses (with averages
of 60 to 70 senses). On the other hand FScore pe-
nalizes the systems inducing a different number of
clusters. As the supervised and unsupervised sys-
tems were designed to return the same (or similar)
number of senses as in the gold standard, they at-
tain higher FScores. This motivated us to compare
the results of the best parameters across evaluation
methods.
5.3 Comparison across evaluation methods
Table 3 shows all 16 evaluation possibilities for
each variant of the algorithm, depending of the
evaluation criteria used in S2LS (in the rows)
and the evaluation criteria used in S3LS (in the
columns). This table shows that the best results (in
bold for each variant) tend to be in the diagonal,
590
that is, when the same evaluation criterion is used
for optimization and test, but it is not decisive. If
we take the first row (supervised evaluation) as the
most credible criterion, we can see that optimiz-
ing according to entropy and purity get similar and
sometimes better result (Pr fr and Pr fx). On the
contrary the Fscore yields worse results by far.
This indicates that a purely unsupervised sys-
tem evaluated according to the gold standard
(based on entropy or purity) yields optimal param-
eters similar to the supervised (mapped) version.
This is an important result, as it shows that the
quality in performance does not come from the
mapping step, but from the algorithm and opti-
mal parameter setting. The table shows that op-
timization on purity and entropy criteria do corre-
late with good performance in the supervised eval-
uation.
The failure of FScore based optimization, in our
opinion, indicates that our clustering algorithm
prefers smaller and more numerous clusters, com-
pared to the gold standard. FScore prefers cluster-
ing solutions that have a similar number of clusters
to that of the gold standard, but it is unable to drive
the optimization or our algorithm towards good re-
sults in the supervised evaluation.
All in all, the best results are attained with
smaller and more numerous hubs, a kind of micro-
senses. This effect is the same for all three vari-
ants tried and all evaluation criteria, with Fscore
yielding less clusters. At first we were uncom-
fortable with this behavior, so we checked whether
HyperLex was degenerating into a trivial solution.
This was the main reason to include the 1ex-1hub
baseline, which simulates a clustering algorithm
returning one hub per example, and its precision
was 40.1, well below the MFS baseline. We also
realized that our results are in accordance with
some theories of word meaning, e.g. the ?indef-
initely large set of prototypes-within-prototypes?
envisioned in (Cruse, 2000). Ted Pedersen has
also observed a similar behaviour in his vector-
space model clustering experiments (PC). We now
think that the idea of having many micro-senses
is attractive for further exploration, specially if we
are able to organize them into coarser hubs in fu-
ture work.
5.4 S3AW task
In the Senseval-3 all-words task (Snyder and
Palmer, 2004) all words in three document ex-
Sup. Unsupervised
Alg. Opt. Rec. Entr. Pur. FS
Vr Sup 64.6 18.4 77.9 30.0
Ent 64.6 18.3 78.3 29.1
Pur 63.7 19.0 78.5 30.8
Fsc 60.4 38.2 63.5 35.0
Pr fr Sup 64.5 20.8 76.1 28.6
Ent 64.6 18.7 77.7 27.2
Pur 64.7 19.3 77.2 27.6
Fsc 61.2 36.0 65.2 34.3
Pr fx Sup 62.2 28.2 69.3 29.5
Ent 63.1 25.4 72.2 28.4
Pur 63.1 25.4 72.2 28.4
Fsc 54.5 32.9 66.5 33.3
Table 3: Cross-evaluation comparison. In the rows the eval-
uation method for optmizing over S2LS is shown, and in the
columns the result over S3LS according to the different eval-
uation methods.
recall
kuaw 70.9
Pr fr 70.7
Vr opt 70.1
GAMBL 70.1
MFS 69.9
LCCaw 68.6
Table 4: Results for the nouns in S3AW, compared to the
most frequent baseline and the top three supervised systems
cerpts need to be disambiguated. Given the
scarce amount of training data available in Sem-
cor (Miller et al, 1993), supervised systems barely
improve upon the simple most frequent heuris-
tic. In this setting the unsupervised evaluation
schemes are not feasible, as many of the target
words occur only once, so we used the map-
ping strategy with Semcor to produce the required
WordNet senses in the output.
Table 4 shows the results for our systems with
the best parameters according to the supervised
criterion on S2LS, plus the top three S3AW super-
vised systems and the most frequent sense heuris-
tic. In order to focus the comparison, we only kept
noun occurrences of all systems and filtered out
multiwords, target words with two different lem-
mas and unknown tags, leaving a total of 857 oc-
currences of nouns. We can see that Pr fr is only
0.2 from the S3AW winning system, demonstrat-
ing that our unsupervised graph-based systems
that use Semcor for mapping are nearly equivalent
to the most powerful supervised systems to date.
In fact, the differences in performance for the sys-
tems are not statistically significant (McNemar?s
test at 95% significance level).
591
6 Conclusions and further work
This paper has explored the use of two graph algo-
rithms for corpus-based disambiguation of nomi-
nal senses. We have shown that the parameter op-
timization learnt over a small set of nouns signifi-
cantly improves the performance for all nouns, and
produces a system which (1) in a lexical-sample
setting (Senseval 3 dataset) is 10 points over the
Most-Frequent-Sense baseline, 1 point over a su-
pervised system using the same kind of informa-
tion (i.e. bag-of-words features), and 8 points be-
low the best supervised system, and (2) in the all-
words setting is a` la par the best supervised sys-
tem. The performance of PageRank is statistically
the same as that of HyperLex, with the advantage
of PageRank of using less parameters.
In order to compete on the same test set as su-
pervised systems, we do use hand-tagged data, but
only to do the mapping from the induced senses
into the gold standard senses. In fact, we believe
that using our WSD system as a purely unsuper-
vised system (i.e. returning just hubs), the per-
fomance would be higher, as we would avoid the
information loss in the mapping. We would like
to test this on Information Retrieval, perhaps on a
setting similar to that of (Schu?tze, 1998), which
would allow for an indirect evaluation of the qual-
ity and a comparison with supervised WSD system
on the same grounds.
We have also shown that the optimization ac-
cording to purity and entropy values (which does
not need the supervised mapping step) yields very
good parameters, comparable to those obtained in
the supervised optimization strategy. This indi-
cates that we are able to optimize the algorithm
in a completely unsupervised fashion for a small
number of words, and then carry over to tag new
text with the induced senses.
Regarding efficiency, our implementation of
HyperLex is extremely fast. Trying the 6700 com-
binations of parameters takes 5 hours in a 2 AMD
Opteron processors at 2GHz and 3Gb RAM. A
single run (building the MST, mapping and tag-
ging the test sentences) takes only 16 sec. For this
reason, even if an on-line version would be in prin-
ciple desirable, we think that this batch version is
readily usable as a standalone word sense disam-
biguation system.
Both graph-based methods and vector-based
clustering methods rely on local information, typ-
ically obtained by the occurrences of neighbor
words in context. The advantage of graph-
based techniques over over vector-based cluster-
ing might come from the fact that the former are
able to measure the relative importance of a vertex
in the whole graph, and thus combine both local
and global cooccurrence information.
For the future, we would like to look more
closely the micro-senses induced by HyperLex,
and see if we can group them into coarser clus-
ters. We would also like to integrate different
kinds of information, specially the local or syn-
tactic features so successfully used by supervised
systems, but also more heterogeneous information
from knowledge bases.
Graph models have been very successful in
some settings (e.g. the PageRank algorithm of
Google), and have been rediscovered recently
for natural language tasks like knowledge-based
WSD, textual entailment, summarization and de-
pendency parsing. Now that we have set a ro-
bust optimization and evaluation framework we
would like to test other such algorithms (e.g.
HITS (Kleinberg, 1999)) in the same conditions.
Acknowledgements
Oier Lopez de Lacalle enjoys a PhD grant from the
Basque Government. We thank the comments of
the three anonymous reviewers.
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2005.
Exploring feature spaces with svd and unlabeled
data for word sense disambiguation. In Proc. of
RANLP.
E. Agirre, O. Lopez de Lacalle, D. Martinez, and
A. Soroa. 2006. Evaluating and optimizing the pa-
rameters of an unsupervised graph-based wsd algo-
rithm. In Proc. of the NAACL Texgraphs workshop.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
D. A. Cruse, 2000. Polysemy: Theoretical and Compu-
tational Approaches, chapter Aspects of the Micro-
structure of Word Meanings, pages 31?51. OUP.
G Erkan and D. R. Radev. 2004. Lexrank: Graph-
based centrality as salience in text summarization.
Journal of Artificial Intelligence Research (JAIR).
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
592
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632.
R. Mihalcea and P Tarau. 2004. Textrank: Bringing
order into texts. In Proc. of EMNLP2004.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The senseval-3 english lexical sample task. In
R. Mihalcea and P. Edmonds, editors, Senseval-3
proccedings, pages 25?28. ACL, July.
R. Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proc. of
EMNLP2005.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker.
1993. A semantic concordance. In Proc. of the
ARPA HLT workshop.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: a knowledge-based approach
to word sense disambiguation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
7(27):1063?1074, June.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word in-
dependent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proc. of CoNLL-2004, pages 41?
48.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
J. Ve?ronis. 2004. Hyperlex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252.
Y Zhao and G Karypis. 2005. Hierarchical clustering
algorithms for document datasets. Data Mining and
Knowledge Discovery, 10(2):141?168.
593
Workshop on TextGraphs, at HLT-NAACL 2006, pages 89?96,
New York City, June 2006. c?2006 Association for Computational Linguistics
Evaluating and optimizing the parameters
of an unsupervised graph-based WSD algorithm
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle and Aitor Soroa
IXA NLP Group
University of Basque Country
Donostia, Basque Contry
a.soroa@ehu.es
Abstract
Ve?ronis (2004) has recently proposed
an innovative unsupervised algorithm for
word sense disambiguation based on
small-world graphs called HyperLex. This
paper explores two sides of the algorithm.
First, we extend Ve?ronis? work by opti-
mizing the free parameters (on a set of
words which is different to the target set).
Second, given that the empirical compar-
ison among unsupervised systems (and
with respect to supervised systems) is sel-
dom made, we used hand-tagged corpora
to map the induced senses to a standard
lexicon (WordNet) and a publicly avail-
able gold standard (Senseval 3 English
Lexical Sample). Our results for nouns
show that thanks to the optimization of
parameters and the mapping method, Hy-
perLex obtains results close to supervised
systems using the same kind of bag-of-
words features. Given the information
loss inherent in any mapping step and the
fact that the parameters were tuned for an-
other set of words, these are very interest-
ing results.
1 Introduction
Word sense disambiguation (WSD) is a key en-
abling technology. Supervised WSD techniques are
the best performing in public evaluations, but need
large amounts of hand-tagging data. Existing hand-
annotated corpora like SemCor (Miller et al, 1993),
which is annotated with WordNet senses (Fellbaum,
1998) allow for a small improvement over the simple
most frequent sense heuristic, as attested in the all-
words track of the last Senseval competition (Sny-
der and Palmer, 2004). In theory, larger amounts
of training data (SemCor has approx. 500M words)
would improve the performance of supervised WSD,
but no current project exists to provide such an ex-
pensive resource.
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a target word
are a closed list coming from a dictionary or lex-
icon. Lexicographers and semanticists have long
warned about the problems of such an approach,
where senses are listed separately as discrete enti-
ties, and have argued in favor of more complex rep-
resentations, where, for instance, senses are dense
regions in a continuum (Cruse, 2000).
Unsupervised WSD has followed this line of
thinking, and tries to induce word senses directly
from the corpus. Typical unsupervised WSD sys-
tems involve clustering techniques, which group to-
gether similar examples. Given a set of induced
clusters (which represent word uses or senses1),
each new occurrence of the target word will be com-
pared to the clusters and the most similar cluster will
be selected as its sense.
Most of the unsupervised WSD work has been
based on the vector space model (Schu?tze, 1998;
Pantel and Lin, 2002; Purandare and Pedersen,
2004), where each example is represented by a vec-
tor of features (e.g. the words occurring in the
context). Recently, Ve?ronis (Ve?ronis, 2004) has
1Unsupervised WSD approaches prefer the term ?word uses?
to ?word senses?. In this paper we use them interchangeably to
refer to both the induced clusters, and to the word senses from
some reference lexicon.
89
proposed HyperLex, an application of graph mod-
els to WSD based on the small-world properties
of cooccurrence graphs. Hand inspection of the
clusters (called hubs in this setting) by the author
was very positive, with hubs capturing the main
senses of the words. Besides, hand inspection of the
disambiguated occurrences yielded precisions over
95% (compared to a most frequent baseline of 73%)
which is an outstanding figure for WSD systems.
We noticed that HyperLex had some free param-
eters and had not been evaluated against a public
gold standard. Besides, we were struck by the few
works where supervised and unsupervised systems
were evaluated on the same test data. In this pa-
per we use an automatic method to map the induced
senses to WordNet using hand-tagged corpora, en-
abling the automatic evaluation against available
gold standards (Senseval 3 English Lexical Sam-
ple S3LS (Mihalcea et al, 2004)) and the automatic
optimization of the free parameters of the method.
The use of hand-tagged corpora for tagging makes
this algorithm a mixture of unsupervised and super-
vised: the method to induce senses in completely
unsupervised, but the mapping is supervised (albeit
very straightforward).
This paper is structured as follows. We first
present the graph-based algorithm as proposed by
Ve?ronis, reviewing briefly the features of small-
world graphs. Section 3 presents our framework for
mapping and evaluating the induced hubs. Section 4
introduces parameter optimization. Section 5 shows
the experiment setting and results. Section 6 ana-
lyzes the results and presents related work. Finally,
we draw the conclusions and advance future work.
2 HyperLex
Before presenting the HyperLex algorithm itself, we
briefly introduce small-world graphs.
2.1 Small world graphs
The small-world nature of a graph can be explained
in terms of its clustering coefficient and characteris-
tic path length. The clustering coefficient of a graph
shows the extent to which nodes tend to form con-
nected groups that have many edges connecting each
other in the group, and few edges leading out of
the group. On the other side, the characteristic path
length represents ?closeness? in a graph. See (Watts
and Strogatz, 1998) for further details on these char-
acteristics.
Randomly built graphs exhibit low clustering co-
efficients and are believed to represent something
very close to the minimal possible average path
length, at least in expectation. Perfectly ordered
graphs, on the other side, show high clustering coef-
ficients but also high average path length. According
to Watts and Strogatz (1998), small-world graphs lie
between these two extremes: they exhibit high clus-
tering coefficients, but short average path lengths.
Barabasi and Albert (1999) use the term ?scale-
free? to graphs whose degree probability follow a
power-law2. Specifically, scale free graphs follow
the property that the probability P (k) that a vertex
in the graph interacts with k other vertices decays as
a power-law, following P (k) ? k??. It turns out
that in this kind of graphs there exist nodes centrally
located and highly connected, called hubs.
2.2 The HyperLex algorithm for WSD
The HyperLex algorithm builds a cooccurrence
graph for all pairs of words cooccurring in the con-
text of the target word. Ve?ronis shows that this kind
of graph fulfills the properties of small world graphs,
and thus possess highly connected components in
the graph. The centers or prototypes of these com-
ponents, called hubs, eventually identify the main
word uses (senses) of the target word.
We will briefly introduce the algorithm here,
check (Ve?ronis, 2004) for further details. For each
word to be disambiguated, a text corpus is collected,
consisting of the paragraphs where the word occurs.
From this corpus, a cooccurrence graph for the tar-
get word is built. Nodes in the graph correspond to
the words3 in the text (except the target word itself).
Two words appearing in the same paragraph are said
to cooccur, and are connected with edges. Each edge
is assigned with a weight which measures the rela-
tive frequency of the two words cooccurring. Specif-
ically, let wij be the weight of the edge4 connecting
2Although scale-free graphs are not necessarily small
worlds, a lot of real world networks are both scale-free and
small worlds.
3Following Ve?ronis, we only work on nouns for the time
being.
4Note that the cooccurrence graph is undirected, i.e. wij =
wji
90
nodes i and j, then
wij = 1? max[P (i | j), P (j | i)]
P (i | j) =
freqij
freqj
and P (j | i) =
freqij
freqi
The weight of an edge measures how tightly con-
nected the two words are. Words which always oc-
cur together receive a weight of 0. Words rarely
cooccurring receive weights close to 1.
Once the cooccurrence graph is built, a simple it-
erative algorithm is executed to obtain its hubs. At
each step, the algorithm finds the vertex with high-
est relative frequency5 in the graph, and, if it meets
some criteria, it is selected as a hub. These criteria
are determined by a set of heuristic parameters, that
will be explained later in Section 4. After a vertex is
selected to be a hub, its neighbors are no longer eli-
gible as hub candidates. At any time, if the next ver-
tex candidate has a relative frequency below a cer-
tain threshold, the algorithm stops.
Once the hubs are selected, each of them is linked
to the target word with edges weighting 0, and the
Minimum Spanning Tree (MST) of the whole graph
is calculated and stored.
The MST is then used to perform word sense dis-
ambiguation, in the following way. For every in-
stance of the target word, the words surrounding it
are examined and confronted with the MST. By con-
struction of the MST, words in it are placed under
exactly one hub. Each word in the context receives
a set of scores s, with one score per hub, where all
scores are 0 except the one corresponding to the hub
where it is placed. If the scores are organized in a
score vector, all values are 0, except, say, the i-th
component, which receives a score d(hi, v), which
is the distance between the hub hi and the node rep-
resenting the word v. Thus, d(hi, v) assigns a score
of 1 to hubs and the score decreases as the nodes
move away from the hub in the tree.
For a given occurrence of the target word, the
score vectors of all the words in the context are
added, and the hub that receives the maximum score
is chosen.
5In cooccurrence graphs, the relative frequency of a vertex
and its degree are linearly related, and it is therefore possible to
avoid the costly computation of the degree.
Base
corpus
hyperLex_wsd hyperLex_wsd
hyperLex
Evaluator
Tagged
corpus
Test
corpus
Mapping
corpus
MST
matrix
Mapping
Figure 1: Design for the automatic mapping and evaluation
of HyperLex algorithm against a gold standard (test corpora).
3 Evaluating unsupervised WSD systems
All unsupervised WSD algorithms need some addi-
tion in order to be evaluated. One alternative, as in
(Ve?ronis, 2004), is to manually decide the correct-
ness of the hubs assigned to each occurrence of the
words. This approach has two main disadvantages.
First, it is expensive to manually verify each occur-
rence of the word, and different runs of the algo-
rithm need to be evaluated in turn. Second, it is not
an easy task to manually decide if an occurrence of
a word effectively corresponds with the use of the
word the assigned hub refers to, especially consid-
ering that the person is given a short list of words
linked to the hub. We also think that instead of judg-
ing whether the hub returned by the algorithm is cor-
rect, the person should have independently tagged
the occurrence with hubs, which should have been
then compared to the hub returned by the system.
A second alternative is to evaluate the system ac-
cording to some performance in an application, e.g.
information retrieval (Schu?tze, 1998). This is a very
attractive idea, but requires expensive system devel-
opment and it is sometimes difficult to separate the
reasons for the good (or bad) performance.
A third alternative would be to devise a method
to map the hubs (clusters) returned by the system
to the senses in a lexicon. Pantel and Lin (2002)
automatically map the senses to WordNet, and then
measure the quality of the mapping. More recently,
the mapping has been used to test the system on
publicly available benchmarks (Purandare and Ped-
91
Default p180 p1800 p6700
value Range Best Range Best Range Best
p1 5 2-3 2 1-3 2 1-3 1
p2 10 3-4 3 2-4 3 2-4 3
p3 0.9 0.7-0.9 0.7 0.5-0.7 0.5 0.3-0.7 0.4
p4 4 4 4 4 4 4 4
p5 6 6-7 6 3-7 3 1-7 1
p6 0.8 0.5-0.8 0.6 0.4-0.8 0.7 0.6-0.95 0.95
p7 0.001 0.0005-0.001 0.0009 0.0005-0.001 0.0009 0.0009-0.003 0.001
Table 1: Parameters of the HyperLex algorithm
ersen, 2004; Niu et al, 2005). See Section 6 for
more details on these systems.
Yet another possibility is to evaluate the induced
senses against a gold standard as a clustering task.
Induced senses are clusters, gold standard senses are
classes, and measures from the clustering literature
like entropy or purity can be used. As we wanted to
focus on the comparison against a standard data-set,
we decided to leave aside this otherwise interesting
option.
In this section we present a framework for au-
tomatically evaluating unsupervised WSD systems
against publicly available hand-tagged corpora. The
framework uses three data sets, called Base corpus,
Mapping corpus and Test corpus:
? The Base Corpus: a collection of examples of
the target word. The corpus is not annotated.
? The Mapping Corpus: a collection of examples
of the target word, where each corpus has been
manually annotated with its sense.
? The Test Corpus: a separate collection, also an-
notated with senses.
The evaluation framework is depicted in Figure 1.
The first step is to execute the HyperLex algorithm
over the Base corpus in order to obtain the hubs of
a target word, and the generated MST is stored. As
stated before, the Base Corpus is not tagged, so the
building of the MST is completely unsupervised.
In a second step (left part in Figure 1), we assign a
hub score vector to each of the occurrences of target
word in the Mapping corpus, using the MST calcu-
lated in the previous step (following the WSD al-
gorithm in Section 2.2). Using the hand-annotated
sense information, we can compute a mapping ma-
trix M that relates hubs and senses in the following
way. Suppose there are m hubs and n senses for the
target word. Then, M = {mij} 1 ? i ? m, 1 ?
j ? n, and each mij = P (sj |hi), that is, mij is the
probability of a word having sense j given that it has
been assigned hub i. This probability can be com-
puted counting the times an occurrence with sense
sj has been assigned hub hi.
This mapping matrix will be used to transform
any hub score vector h? = (h1, . . . , hm) returned
by the WSD algorithm into a sense score vector
s? = (s1, . . . , sn). It suffices to multiply the score
vector by M , i.e., s? = h?M .
In the last step (right part in Figure 1), we apply
the WSD algorithm over the Test corpus, using again
the MST generated in the first step, and returning a
hub score vector for each occurrence of the target
word in the test corpus. We then run the Evaluator,
which uses the M mapping matrix in order to con-
vert the hub score vector into a sense score vector.
The Evaluator then compares the sense with high-
est weight in the sense score vector to the sense that
was manually assigned, and outputs the precision
figures.
Preliminary experiments showed that, similar to
other unsupervised systems, HyperLex performs
better if it sees the test examples when building the
graph. We therefore decided to include a copy of the
training and test corpora in the base corpus (discard-
ing all hand-tagged sense information, of course).
Given the high efficiency of the algorithm this poses
no practical problem (see efficiency figures in Sec-
tion 6).
4 Tuning the parameters
As stated before, the behavior of the HyperLex algo-
rithm is influenced by a set of heuristic parameters,
that affect the way the cooccurrence graph is built,
the number of induced hubs, and the way they are
extracted from the graph. There are 7 parameters in
total:
p1 Minimum frequency of edges (occurrences)
p2 Minimum frequency of vertices (words)
p3 Edges with weights above this value are removed
p4 Context containing fewer words are not processed
92
word train test MFS default p180 p1800 p6700
argument 221 111 51.4 51.4 51.4 51.4 51.4
arm 266 133 82.0 82.0 80.5 82.0 82.7
atmosphere 161 81 66.7 67.9 70.4 70.4 67.9
audience 200 100 67.0 69.0 71.0 74.0 77.0
bank 262 132 67.4 69.7 75.0 76.5 75.0
degree 256 128 60.9 60.9 60.9 62.5 63.3
difference 226 114 40.4 40.4 41.2 46.5 49.1
difficulty 46 23 17.4 30.4 30.4 39.1 26.1
disc 200 100 38.0 66.0 75.0 70.0 76.0
image 146 74 36.5 63.5 62.2 67.6 64.9
interest 185 93 41.9 49.5 41.9 47.3 51.6
judgment 62 32 28.1 28.1 28.1 53.1 50.0
organization 112 56 73.2 73.2 73.2 71.4 73.2
paper 232 117 25.6 42.7 39.3 47.9 53.8
party 230 116 62.1 67.2 64.7 65.5 67.2
performance 172 87 32.2 44.8 46.0 54.0 59.8
plan 166 84 82.1 81.0 79.8 81.0 83.3
shelter 196 98 44.9 45.9 49.0 48.0 54.1
sort 190 96 65.6 64.6 64.6 65.6 64.6
source 64 32 65.6 59.4 56.2 62.5 62.5
Average: 54.5 59.9 60.3 63.0 64.6
(Over S2LS) 51.9 56.2 57.5 58.7 60.0
Table 2: Precision figures for nouns over the test corpus (S3LS). The second and third columns show the number of occurrences
in the train and test splits. The MFS column corresponds to the most frequent sense. The rest of columns correspond to different
parameter settings: default for the default setting, p180 for the best combination over 180, etc.. The last rows show the micro-
average over the S3LS run, and we also add the results on the S2LS dataset (different sets of nouns) to confirm that the same trends
hold in both datasets.
p5 Minimum number of adjacent vertices a hub must have
p6 Max. mean weight of the adjacent vertices of a hub
p7 Minimum frequency of hubs
Table 1 lists the parameters of the HyperLex al-
gorithm, and the default values proposed for them in
the original work (second column).
Given that we have devised a method to efficiently
evaluate the performance of HyperLex, we are able
to tune the parameters against the gold standard. We
first set a range for each of the parameters, and eval-
uated the algorithm for each combination of the pa-
rameters on a collection of examples of different
words (Senseval 2 English lexical-sample, S2LS).
This ensures that the chosen parameter set is valid
for any noun, and is not overfitted to a small set of
nouns.6 The set of parameters that obtained the best
results in the S2LS run is then selected to be run
against the S3LS dataset.
We first devised ranges for parameters amounting
to 180 possible combinations (p180 column in Ta-
ble 2), and then extended the ranges to amount to
1800 and 6700 combinations (columns p1800 and
p6700).
6In fact, previous experiments showed that optimizing the
parameters for each word did not yield better results.
5 Experiment setting and results
To evaluate the HyperLex algorithm in a standard
benchmark, we applied it to the 20 nouns in S3LS.
We use the standard training-test split. Following
the design in Section 3, we used both the training
and test sets as the Base Corpus (ignoring the sense
tags, of course). The Mapping Corpus comprised
the training split only, and the Test corpus the test
split only. The parameter tuning was done in a simi-
lar fashion, but on the S2LS dataset.
In Table 2 we can see the number of examples
of each word in the different corpus and the results
of the algorithm. We indicate only precision, as the
coverage is 100% in all cases. The left column,
named MFS, shows the precision when always as-
signing the most frequent sense (relative to the train
split). This is the baseline of our algorithm as our
algorithm does see the tags in the mapping step (see
Section 6 for further comments on this issue).
The default column shows the results for the Hy-
perLex algorithm with the default parameters as set
by Ve?ronis, except for the minimum frequency of
the vertices (p2 in Table 1), which according to some
preliminary experiments we set to 3. As we can see,
the algorithm with the default settings outperforms
93
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 0.5  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1
Pr
ec
is
io
n
Similarity
Parameter space
Best fitting line
Figure 2: Dispersion plot of the parameter space for 6700
combinations. The horizontal axis shows the similarity of a pa-
rameter set w.r.t. the best parameter set using the cosine. The
vertical axis shows the precision in S2LS. The best fitting line
is also depicted.
the MFS baseline by 5.4 points average, and in al-
most all words (except plan, sort and source).
The results for the best of 180 combinations of the
parameters improve the default setting (0.4 overall),
Extending the parameter space to 1800 and 6700 im-
proves the precision up to 63.0 and 64.6, 10.1 over
the MFS (MFS only outperforms HyperLex in the
best setting for two words). The same trend can be
seen on the S2LS dataset, where the gain was more
modest (note that the parameters were optimized for
S2LS).
6 Discussion and related work
We first comment the results, doing some analysis,
and then compare our results to those of Ve?ronis. Fi-
nally we overview some relevant work and review
the results of unsupervised systems on the S3LS
benchmark.
6.1 Comments on the results
The results show clearly that our exploration of the
parameter space was successful, with the widest pa-
rameter space showing the best results.
In order to analyze whether the search in the pa-
rameter space was making any sense, we drew a dis-
persion plot (see Figure 2). In the top right-hand cor-
ner we have the point corresponding to the best per-
forming parameter set. If the parameters were not
conditioning the good results, then we would have
expected a random cloud of points. On the contrary,
we can see that there is a clear tendency for those
default p180 p1800 p6700
hubs defined 9.2 ?3.8 15.3 ?5.7 38.6 ?11.8 77.7?18.7
used 8.4 ?3.5 14.4 ?5.3 30.4 ?9.3 45.2?13.3
senses defined 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5
used 2.6 ?1.2 2.5 ?1 3.1 ?1.1 3.2?1.2
senses in test 5.1 ?1.3 - - -
Table 3: Average number of hubs and senses (along with the
standard deviation) for three parameter settings. Defined means
the number of hubs induced, and used means the ones actually
returned by HyperLex when disambiguating the test set. The
same applies for senses, that is, defined means total number of
senses (equal for all columns), and used means the senses that
were actually used by HyperLex in the test set. The last row
shows the actual number of senses used by the hand-annotators
in the test set.
parameter sets most similar to the best one to obtain
better results, and in fact the best fitting line shows a
clearly ascending slope.
Regarding efficiency, our implementation of Hy-
perLex is extremely fast. Doing the 1800 combina-
tions takes 2 hours in a 2 AMD Opteron processors
at 2GHz and 3Gb RAM. A single run (building the
MST, mapping and tagging the test sentences) takes
only 16 sec. For this reason, even if an on-line ver-
sion would be in principle desirable, we think that
this batch version is readily usable.
6.2 Comparison to (Ve?ronis, 2004)
Compared to Ve?ronis we are inducing larger num-
bers of hubs (with different parameters), using less
examples to build the graphs and obtaining more
modest results (far from the 90?s). Regarding the lat-
ter, our results are in the range of other S3LS WSD
systems (see below), and the discrepancy can be ex-
plained by the way Ve?ronis performed his evaluation
(see Section 3).
Table 3 shows the average number of hubs for
the four parameter settings. The average number
of hubs for the default setting is larger than that of
Ve?ronis (which ranges between 4 and 9 per word),
but quite close to the average number of senses. The
exploration of the parameter space prefers parame-
ter settings with even larger number of hubs, and the
figures shows that most of them are actually used
for disambiguation. The table also shows that, after
the mapping, less than half of the senses are actu-
ally used, which seems to indicate that the mapping
tends to favor the most frequent senses.
Regarding the actual values of the parameters
used (c.f. Table 1), we had to reduce the value
94
of some parameters (e.g. the minimum frequency
of vertices) due to the smaller number of of exam-
ples (Ve?ronis used from 1900 to 8700 examples per
word). In theory, we could explore larger parame-
ter spaces, but Table 1 shoes that the best setting for
the 6700 combinations has no parameter in a range
boundary (except p5, which cannot be further re-
duced).
All in all, the best results are attained with smaller
and more numerous hubs, a kind of micro-senses.
A possible explanation for this discrepancy with
Ve?ronis could be that he was inspecting by hand
the hubs that he got, and perhaps was biased by the
fact that he wanted the hubs to look more like stan-
dard senses. At first we were uncomfortable with
this behavior, so we checked whether HyperLex was
degenerating into a trivial solution. We simulated
a clustering algorithm returning one hub per exam-
ple, and its precision was 40.1, well below the MFS
baseline. We also realized that our results are in
accordance with some theories of word meaning,
e.g. the ?indefinitely large set of prototypes-within-
prototypes? envisioned in (Cruse, 2000). We now
think that the idea of having many micro-senses is
very attractive for further exploration, especially if
we are able to organize them into coarser hubs.
6.3 Comparison to related work
Table 4 shows the performance of different systems
on the nouns of the S3LS benchmark. When not re-
ported separately, we obtained the results for nouns
running the official scorer program on the filtered
results, as available in the S3LS web page. The sec-
ond column shows the type of system (supervised,
unsupervised).
We include three supervised systems, the winner
of S3LS (Mihalcea et al, 2004), an in-house system
(kNN-all, CITATION OMITTED) which uses opti-
mized kNN, and the same in-house system restricted
to bag-of-words features only (kNN-bow), i.e. dis-
carding other local features like bigrams or trigrams
(which is what most unsupervised systems do). The
table shows that we are one point from the bag-of-
words classifier kNN-bow, which is an impressive
result if we take into account the information loss of
the mapping step and that we tuned our parameters
on a different set of words. The full kNN system is
state-of-the-art, only 4 points below the S3LS win-
System Type Prec. Cov.
S3LS-best Sup. 74.9 0.99
kNN-all Sup. 70.3 1.0
kNN-bow Sup. 65.7 1.0
HyperLex Unsup(S3LS) 64.6 1.0
Cymfony Unsup(10%-S3LS) 57.9 1.0
Prob0 Unsup. (MFS-S3) 55.0 0.98
MFS - 51.5 1.0
Ciaosenso Unsup (MFS-Sc) 53.95 0.90
clr04 Unsup (MFS-Sc) 48.86 1.0
duluth-senserelate Unsup 47.48 1.0
(Purandare and
Pedersen, 2004)
Unsup (S2LS) - -
Table 4: Comparison of HyperLex and MFS baseline to S3LS
systems for nouns. The last system was evaluated on S2LS.
ner.
Table 4 also shows several unsupervised systems,
all of which except Cymfony and (Purandare and
Pedersen, 2004) participated in S3LS (check (Mi-
halcea et al, 2004) for further details on the sys-
tems). We classify them according to the amount of
?supervision? they have: some have have access to
most-frequent information (MFS-S3 if counted over
S3LS, MFS-Sc if counted over SemCor), some use
10% of the S3LS training part for mapping (10%-
S3LS), and some use the full amount of S3LS train-
ing for mapping (S3LS). Only one system (Duluth)
did not use in any way hand-tagged corpora.
Given the different typology of unsupervised sys-
tems, it?s unfair to draw definitive conclusions from
a raw comparison of results. The system coming
closer to ours is that described in (Niu et al, 2005).
They use hand tagged corpora which does not need
to include the target word to tune the parameters of
a rather complex clustering method which does use
local information (an exception to the rule of unsu-
pervised systems). They do use the S3LS training
corpus for mapping. For every sense the target word,
three of its contexts in the train corpus are gathered
(around 10% of the training data) and tagged. Each
cluster is then related with its most frequent sense.
Only one cluster may be related to a specific sense,
so if two or more clusters map to the same sense,
only the largest of them is retained. The mapping
method is similar to ours, but we use all the avail-
able training data and allow for different hubs to be
assigned to the same sense.
Another system similar to ours is (Purandare and
Pedersen, 2004), which unfortunately was evaluated
on Senseval 2 data. The authors use first and second
95
order bag-of-word context features to represent each
instance of the corpus. They apply several clustering
algorithms based on the vector space model, limiting
the number of clusters to 7. They also use all avail-
able training data for mapping, but given their small
number of clusters they opt for a one-to-one map-
ping which maximizes the assignment and discards
the less frequent clusters. They also discard some
difficult cases, like senses and words with low fre-
quencies (10% of total occurrences and 90, respec-
tively). The different test set and mapping system
make the comparison difficult, but the fact that the
best of their combinations beats MFS by 1 point on
average (47.6% vs. 46.4%) for the selected nouns
and senses make us think that our results are more
robust (nearly 10% over MFS).
7 Conclusions and further work
This paper has explored two sides of HyperLex: the
optimization of the free parameters, and the empir-
ical comparison on a standard benchmark against
other WSD systems. We use hand-tagged corpora
to map the induced senses to WordNet senses.
Regarding the optimization of parameters, we
used a another testbed (S2LS) comprising different
words to select the best parameter. We consistently
improve the results of the parameters by Ve?ronis,
which is not perhaps so surprising, but the method
allows to fine-tune the parameters automatically to a
given corpus given a small test set.
Comparing unsupervised systems against super-
vised systems is seldom done. Our results indicate
that HyperLex with the supervised mapping is on
par with a state-of-the-art system which uses bag-
of-words features only. Given the information loss
inherent to any mapping, this is an impressive re-
sult. The comparison to other unsupervised systems
is difficult, as each one uses a different mapping
strategy and a different amount of supervision.
For the future, we would like to look more closely
the micro-senses induced by HyperLex, and see if
we can group them into coarser clusters. We also
plan to apply the parameters to the Senseval 3 all-
words task, which seems well fit for HyperLex: the
best supervised system only outperforms MFS by
a few points in this setting, and the training cor-
pora used (Semcor) is not related to the test corpora
(mainly Wall Street Journal texts).
Graph models have been very successful in some
settings (e.g. the PageRank algorithm of Google),
and have been rediscovered recently for natural lan-
guage tasks like knowledge-based WSD, textual en-
tailment, summarization and dependency parsing.
We would like to test other such algorithms in the
same conditions, and explore their potential to inte-
grate different kinds of information, especially the
local or syntactic features so successfully used by
supervised systems, but also more heterogeneous in-
formation from knowledge bases.
References
A. L. Barabasi and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 286(5439):509?512,
October.
D. A. Cruse, 2000. Polysemy: Theoretical and Com-
putational Approaches, chapter Aspects of the Micro-
structure of Word Meanings. OUP.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
senseval-3 english lexical sample task. In R. Mihal-
cea and P. Edmonds, editors, Senseval-3 proceedings,
pages 25?28. ACL, July.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A semantic concordance. In Proc. of the ARPA HLT
workshop.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word
independent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Proc. of CoNLL-2004.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
J. Ve?ronis. 2004. HyperLex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252.
D. J. Watts and S. H. Strogatz. 1998. Collec-
tive dynamics of ?small-world? networks. Nature,
393(6684):440?442, June.
96
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 7?12,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semeval-2007 Task 02:
Evaluating Word Sense Induction and Discrimination Systems
Eneko Agirre
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
e.agirre@ehu.es
Aitor Soroa
IXA NLP Group
Univ. of the Basque Country
Donostia, Basque Country
a.soroa@ehu.es
Abstract
The goal of this task is to allow for com-
parison across sense-induction and discrim-
ination systems, and also to compare these
systems to other supervised and knowledge-
based systems. In total there were 6 partic-
ipating systems. We reused the SemEval-
2007 English lexical sample subtask of task
17, and set up both clustering-style unsuper-
vised evaluation (using OntoNotes senses as
gold-standard) and a supervised evaluation
(using the part of the dataset for mapping).
We provide a comparison to the results of
the systems participating in the lexical sam-
ple subtask of task 17.
1 Introduction
Word Sense Disambiguation (WSD) is a key
enabling-technology. Supervised WSD techniques
are the best performing in public evaluations, but
need large amounts of hand-tagging data. Exist-
ing hand-annotated corpora like SemCor (Miller
et al, 1993), which is annotated with WordNet
senses (Fellbaum, 1998) allow for a small improve-
ment over the simple most frequent sense heuristic,
as attested in the all-words track of the last Sense-
val competition (Snyder and Palmer, 2004). In the-
ory, larger amounts of training data (SemCor has
approx. 500M words) would improve the perfor-
mance of supervised WSD, but no current project
exists to provide such an expensive resource. An-
other problem of the supervised approach is that the
inventory and distribution of senses changes dra-
matically from one domain to the other, requiring
additional hand-tagging of corpora (Mart??nez and
Agirre, 2000; Koeling et al, 2005).
Supervised WSD is based on the ?fixed-list of
senses? paradigm, where the senses for a target word
are a closed list coming from a dictionary or lex-
icon. Lexicographers and semanticists have long
warned about the problems of such an approach,
where senses are listed separately as discrete enti-
ties, and have argued in favor of more complex rep-
resentations, where, for instance, senses are dense
regions in a continuum (Cruse, 2000).
Unsupervised Word Sense Induction and Dis-
crimination (WSID, also known as corpus-based un-
supervised systems) has followed this line of think-
ing, and tries to induce word senses directly from
the corpus. Typical WSID systems involve cluster-
ing techniques, which group together similar exam-
ples. Given a set of induced clusters (which repre-
sent word uses or senses1), each new occurrence of
the target word will be compared to the clusters and
the most similar cluster will be selected as its sense.
One of the problems of unsupervised systems is
that of managing to do a fair evaluation. Most of cur-
rent unsupervised systems are evaluated in-house,
with a brief comparison to a re-implementation of a
former system, leading to a proliferation of unsuper-
vised systems with little ground to compare among
them. The goal of this task is to allow for compar-
ison across sense-induction and discrimination sys-
tems, and also to compare these systems to other su-
pervised and knowledge-based systems.
The paper is organized as follows. Section 2
presents the evaluation framework used in this task.
Section 3 presents the systems that participated in
1WSID approaches prefer the term ?word uses? to ?word
senses?. In this paper we use them interchangeably to refer to
both the induced clusters, and to the word senses from some
reference lexicon.
7
the task, and the official results. Finally, Section 5
draws the conclusions.
2 Evaluating WSID systems
All WSID algorithms need some addition in order
to be evaluated. One alternative is to manually de-
cide the correctness of the clusters assigned to each
occurrence of the words. This approach has two
main disadvantages. First, it is expensive to man-
ually verify each occurrence of the word, and dif-
ferent runs of the algorithm need to be evaluated
in turn. Second, it is not an easy task to manu-
ally decide if an occurrence of a word effectively
corresponds with the use of the word the assigned
cluster refers to, especially considering that the per-
son is given a short list of words linked to the clus-
ter. We also think that instead of judging whether
the cluster returned by the algorithm is correct, the
person should have independently tagged the occur-
rence with his own senses, which should have been
then compared to the cluster returned by the system.
This is paramount to compare a corpus which has
been hand-tagged with some reference senses (also
known as the gold-standard) with the clustering re-
sult. The gold standard tags are taken to be the def-
inition of the classes, and standard measures from
the clustering literature can be used to evaluate the
clusters against the classes.
A second alternative would be to devise a method
to map the clusters returned by the systems to the
senses in a lexicon. Pantel and Lin (2002) automat-
ically map the senses to WordNet, and then mea-
sure the quality of the mapping. More recently, the
mapping has been used to test the system on pub-
licly available benchmarks (Purandare and Peder-
sen, 2004; Niu et al, 2005).
A third alternative is to evaluate the systems ac-
cording to some performance in an application, e.g.
information retrieval (Schu?tze, 1998). This is a very
attractive idea, but requires expensive system devel-
opment and it is sometimes difficult to separate the
reasons for the good (or bad) performance.
In this task we decided to adopt the first two alter-
natives, since they allow for comparison over pub-
licly available systems of any kind. With this goal on
mind we gave all the participants an unlabeled cor-
pus, and asked them to induce the senses and create
a clustering solution on it. We evaluate the results
according to the following types of evaluation:
1. Evaluate the induced senses as clusters of ex-
amples. The induced clusters are compared to
the sets of examples tagged with the given gold
standard word senses (classes), and evaluated
using the FScore measure for clusters. We will
call this evaluation unsupervised.
2. Map the induced senses to gold standard
senses, and use the mapping to tag the test cor-
pus with gold standard tags. The mapping is
automatically produced by the organizers, and
the resulting results evaluated according to the
usual precision and recall measures for super-
vised word sense disambiguation systems. We
call this evaluation supervised.
We will see each of them in turn.
2.1 Unsupervised evaluation
In this setting the results of the systems are treated
as clusters of examples and gold standard senses are
classes. In order to compare the clusters with the
classes, hand annotated corpora is needed. The test
set is first tagged with the induced senses. A per-
fect clustering solution will be the one where each
cluster has exactly the same examples as one of the
classes, and vice versa.
Following standard cluster evaluation prac-
tice (Zhao and Karypis, 2005), we consider the FS-
core measure for measuring the performance of the
systems. The FScore is used in a similar fashion
to Information Retrieval exercises, with precision
and recall defined as the percentage of correctly ?re-
trieved? examples for a cluster (divided by total clus-
ter size), and recall as the percentage of correctly
?retrieved? examples for a cluster (divided by total
class size).
Given a particular class sr of size nr and a cluster
hi of size ni, suppose nir examples in the class sr
belong to hi. The F value of this class and cluster is
defined to be:
f(sr, hi) =
2P (sr, hi)R(sr, hi)
P (sr, hi) + R(sr, hi)
where P (sr, hi) =
nir
nr
is the precision value and
R(sr, hi) =
nir
ni
is the recall value defined for class
sr and cluster hi. The FScore of class sr is the max-
imum F value attained at any cluster, that is,
8
F (sr) = max
hi
f(sr, hi)
and the FScore of the entire clustering solution is:
FScore =
c?
r=1
nr
n
F (sr)
where q is the number of classes and n is the size
of the clustering solution. If the clustering is the
identical to the original classes in the datasets, FS-
core will be equal to one which means that the higher
the FScore, the better the clustering is.
For the sake of completeness we also include the
standard entropy and purity measures in the unsu-
pervised evaluation. The entropy measure consid-
ers how the various classes of objects are distributed
within each cluster. In general, the smaller the en-
tropy value, the better the clustering algorithm per-
forms. The purity measure considers the extent to
which each cluster contained objects from primarily
one class. The larger the values of purity, the bet-
ter the clustering algorithm performs. For a formal
definition refer to (Zhao and Karypis, 2005).
2.2 Supervised evaluation
We have followed the supervised evaluation frame-
work for evaluating WSID systems as described in
(Agirre et al, 2006). First, we split the corpus into
a train/test part. Using the hand-annotated sense in-
formation in the train part, we compute a mapping
matrix M that relates clusters and senses in the fol-
lowing way. Suppose there are m clusters and n
senses for the target word. Then, M = {mij} 1 ?
i ? m, 1 ? j ? n, and each mij = P (sj |hi), that
is, mij is the probability of a word having sense j
given that it has been assigned cluster i. This proba-
bility can be computed counting the times an occur-
rence with sense sj has been assigned cluster hi in
the train corpus.
The mapping matrix is used to transform any
cluster score vector h? = (h1, . . . , hm) returned by
the WSID algorithm into a sense score vector s? =
(s1, . . . , sn). It suffices to multiply the score vector
by M , i.e., s? = h?M .
We use the M mapping matrix in order to convert
the cluster score vector of each test corpus instance
into a sense score vector, and assign the sense with
All Nouns Verbs
train 22281 14746 9773
test 4851 2903 2427
all 27132 17649 12200
Table 1: Number of occurrences for the 100 target words in
the corpus following the train/test split.
maximum score to that instance. Finally, the result-
ing test corpus is evaluated according to the usual
precision and recall measures for supervised word
sense disambiguation systems.
3 Results
In this section we will introduce the gold standard
and corpus used, the description of the systems and
the results obtained. Finally we provide some mate-
rial for discussion.
Gold Standard
The data used for the actual evaluation was bor-
rowed from the SemEval-2007 ?English lexical
sample subtask? of task 17. The texts come from the
Wall Street Journal corpus, and were hand-annotated
with OntoNotes senses (Hovy et al, 2006). Note
that OntoNotes senses are coarser than WordNet
senses, and thus the number of senses to be induced
is smaller in this case.
Participants were provided with information
about 100 target words (65 verbs and 35 nouns),
each target word having a set of contexts where the
word appears. After removing the sense tags from
the train corpus, the train and test parts were joined
into the official corpus and given to the participants.
Participants had to tag with the induced senses all
the examples in this corpus. Table 1 summarizes the
size of the corpus.
Participant systems
In total there were 6 participant systems. One of
them (UoFL) was not a sense induction system, but
rather a knowledge-based WSD system. We include
their data in the results section below for coherence
with the official results submitted to participants, but
we will not mention it here.
I2R: This team used a cluster validation method
to estimate the number of senses of a target word in
untagged data, and then grouped the instances of this
target word into the estimated number of clusters us-
ing the sequential Information Bottleneck algorithm.
9
UBC-AS: A two stage graph-based clustering
where a co-occurrence graph is used to compute
similarities against contexts. The context similarity
matrix is pruned and the resulting associated graph
is clustered by means of a random-walk type al-
gorithm. The parameters of the system are tuned
against the Senseval-3 lexical sample dataset, and
some manual tuning is performed in order to reduce
the overall number of induced senses. Note that this
system was submitted by the organizers. The orga-
nizers took great care in order to participate under
the same conditions as the rest of participants.
UMND2: A system which clusters the second or-
der co-occurrence vectors associated with each word
in a context. Clustering is done using k-means and
the number of clusters was automatically discovered
using the Adapted Gap Statistic. No parameter tun-
ing is performed.
upv si: A self-term expansion method based on
co-ocurrence, where the terms of the corpus are ex-
panded by its best co-ocurrence terms in the same
corpus. The clustering is done using one implemen-
tation of the KStar method where the stop criterion
has been modified. The trial data was used for de-
termining the corpus structure. No further tuning is
performed.
UOY: A graph based system which creates a co-
occurrence hypergraph model. The hypergraph is
filtered and weighted according to some associa-
tion rules. The clustering is performed by selecting
the nodes of higher degree until a stop criterion is
reached. WSD is performed by assigning to each in-
duced cluster a score equal to the sum of weights of
hyperedges found in the local context of the target
word. The system was tested and tuned on 10 nouns
of Senseval-3 lexical-sample.
Official Results
Participants were required to induce the senses of
the target words and cluster all target word contexts
accordingly2. Table 2 summarizes the average num-
ber of induced senses as well as the real senses in
the gold standard.
2They were allowed to label each context with a weighted
score vector, assigning a weight to each induced sense. In the
unsupervised evaluation only the sense with maximum weight
was considered, but for the supervised one the whole score vec-
tor was used. However, none of the participating systems la-
beled any instance with more than one sense.
system All nouns verbs
I2R 3.08 3.11 3.06
UBC-AS? 1.32 1.63 1.15
UMND2 1.36 1.71 1.17
upv si 5.57 7.2 4.69
UOY 9.28 11.28 8.2
Gold standard
test 2.87 2.86 2.86
train 3.6 3.91 3.43
all 3.68 3.94 3.54
Table 2: Average number of clusters as returned by the par-
ticipants, and number of classes in the gold standard. Note that
UBC-AS? is the system submitted by the organizers of the task.
System R. All Nouns Verbs
FSc. Pur. Entr. FSc. FSc.
1c1word 1 78.9 79.8 45.4 80.7 76.8
UBC-AS? 2 78.7 80.5 43.8 80.8 76.3
upv si 3 66.3 83.8 33.2 69.9 62.2
UMND2 4 66.1 81.7 40.5 67.1 65.0
I2R 5 63.9 84.0 32.8 68.0 59.3
UofL?? 6 61.5 82.2 37.8 62.3 60.5
UOY 7 56.1 86.1 27.1 65.8 45.1
Random 8 37.9 86.1 27.7 38.1 37.7
1c1inst 9 9.5 100 0 6.6 12.7
Table 3: Unsupervised evaluation on the test corpus (FScore),
including 3 baselines. Purity and entropy are also provided.
UBC-AS? was submitted by the organizers. UofL?? is not a
sense induction system.
System Rank Supervised evaluation
All Nouns Verbs
I2R 1 81.6 86.8 75.7
UMND2 2 80.6 84.5 76.2
upv si 3 79.1 82.5 75.3
MFS 4 78.7 80.9 76.2
UBC-AS? 5 78.5 80.7 76.0
UOY 6 77.7 81.6 73.3
UofL?? 7 77.1 80.5 73.3
Table 4: Supervised evaluation as recall. UBC-AS? was sub-
mitted by the organizers. UofL?? is not a sense induction sys-
tem.
Table 3 shows the unsupervised evaluation of
the systems on the test corpus. We also include
three baselines: the ?one cluster per word? baseline
(1c1word), which groups all instances of a word into
a single cluster, the ?one cluster per instance? base-
line (1c1inst), where each instance is a distinct clus-
ter, and a random baseline, where the induced word
senses and their associated weights have been ran-
domly produced. The random baseline figures in this
paper are averages over 10 runs.
As shown in Table 3, no system outperforms the
1c1word baseline, which indicates that this baseline
10
is quite strong, perhaps due the relatively small num-
ber of classes in the gold standard. However, all
systems outperform by far the random and 1c1inst
baselines, meaning that the systems are able to in-
duce correct senses. Note that the purity and entropy
measures are not very indicative in this setting. For
completeness, we also computed the FScore using
the complete corpus (both train and test). The re-
sults are similar and the ranking is the same. We
omit them for brevity.
The results of the supervised evaluation can be
seen in Table 4. The evaluation is also performed
over the test corpus. Apart from participants, we
also show the most frequent sense (MFS), which
tags every test instance with the sense that occurred
most often in the training part. Note that the su-
pervised evaluation combines the information in the
clustering solution implicitly with the MFS infor-
mation via the mapping in the training part. Pre-
vious Senseval evaluation exercises have shown that
the MFS baseline is very hard to beat by unsuper-
vised systems. In fact, only three of the participant
systems are above the MFS baseline, which shows
that the clustering information carries over the map-
ping successfully for these systems. Note that the
1c1word baseline is equivalent to MFS in this set-
ting. We will review the random baseline in the dis-
cussion section below.
Further Results
Table 5 shows the results of the best systems from
the lexical sample subtask of task 17. The best sense
induction system is only 6.9 percentage points below
the best supervised, and 3.5 percentage points be-
low the best (and only) semi-supervised system. If
the sense induction system had participated, it would
be deemed as semi-supervised, as it uses, albeit in a
shallow way, the training data for mapping the clus-
ters into senses. In this sense, our supervised evalu-
ation does not seek to optimize the available training
data.
After the official evaluation, we realized that con-
trary to previous lexical sample evaluation exercises
task 17 organizers did not follow a random train/test
split. We decided to produce a random train/test
split following the same 82/18 proportion as the of-
ficial split, and re-evaluated the systems. The results
are presented in Table 6, where we can see that all
System Supervised evaluation
best supervised 88.7
best semi-supervised 85.1
best induction (semi-sup.) 81.6
MFS 78.7
best unsupervised 53.8
Table 5: Comparing the best induction system in this task with
those of task 17.
System Supervised evaluation
I2R 82.2
UOY 81.3
UMND2 80.1
upv si 79.9
UBC-AS 79.0
MFS 78.4
Table 6: Supervised evaluation as recall using a random
train/test split.
participants are above the MFS baseline, showing
that all of them learned useful clustering informa-
tion. Note that UOY was specially affected by the
original split. The distribution of senses in this split
did not vary (cf. Table 2).
Finally, we also studied the supervised evalua-
tion of several random clustering algorithms, which
can attain performances close to MFS, thanks to the
mapping information. This is due to the fact that the
random clusters would be mapped to the most fre-
quent senses. Table 7 shows the results of random
solutions using varying numbers of clusters (e.g.
random2 is a random choice between two clusters).
Random2 is only 0.1 below MFS, but as the number
of clusters increases some clusters don?t get mapped,
and the recall of the random baselines decrease.
4 Discussion
The evaluation of clustering solutions is not straight-
forward. All measures have some bias towards cer-
tain clustering strategy, and this is one of the reasons
of adding the supervised evaluation as a complemen-
tary information to the more standard unsupervised
evaluation.
In our case, we noticed that the FScore penal-
ized the systems with a high number of clusters,
and favored those that induce less senses. Given
the fact that FScore tries to balance precision (higher
for large numbers of clusters) and recall (higher for
small numbers of clusters), this was not expected.
We were also surprised to see that no system could
11
System Supervised evaluation
random2 78.6
random10 77.6
ramdom100 64.2
random1000 31.8
Table 7: Supervised evaluation of several random baselines.
beat the ?one cluster one word? baseline. An expla-
nation might lay in that the gold-standard was based
on the coarse-grained OntoNotes senses. We also
noticed that some words had hundreds of instances
and only a single sense. We suspect that the partic-
ipating systems would have beaten all baselines if a
fine-grained sense inventory like WordNet had been
used, as was customary in previous WSD evaluation
exercises.
Supervised evaluation seems to be more neutral
regarding the number of clusters, as the ranking of
systems according to this measure include diverse
cluster averages. Each of the induced clusters is
mapped into a weighted vector of senses, and thus
inducing a number of clusters similar to the number
of senses is not a requirement for good results. With
this measure some of the systems3 are able to beat
all baselines.
5 Conclusions
We have presented the design and results of the
SemEval-2007 task 02 on evaluating word sense in-
duction and discrimination systems. 6 systems par-
ticipated, but one of them was not a sense induc-
tion system. We reused the data from the SemEval-
2007 English lexical sample subtask of task 17, and
set up both clustering-style unsupervised evaluation
(using OntoNotes senses as gold-standard) and a su-
pervised evaluation (using the training part of the
dataset for mapping). We also provide a compari-
son to the results of the systems participating in the
lexical sample subtask of task 17.
Evaluating clustering solutions is not straightfor-
ward. The unsupervised evaluation seems to be
sensitive to the number of senses in the gold stan-
dard, and the coarse grained sense inventory used
in the gold standard had a great impact in the re-
sults. The supervised evaluation introduces a map-
ping step which interacts with the clustering solu-
tion. In fact, the ranking of the participating systems
3All systems in the case of a random train/test split
varies according to the evaluation method used. We
think the two evaluation results should be taken to be
complementary regarding the information learned
by the clustering systems, and that the evaluation
of word sense induction and discrimination systems
needs further developments, perhaps linked to a cer-
tain application or purpose.
Acknowledgments
We want too thank the organizers of SemEval-2007 task 17 for
kindly letting us use their corpus. We are also grateful to Ted
Pedersen for his comments on the evaluation results. This work
has been partially funded by the Spanish education ministry
(project KNOW) and by the regional government of Gipuzkoa
(project DAHAD).
References
E. Agirre, D. Mart??nez, O. Lo?pez de Lacalle, and
A. Soroa. 2006. Evaluating and optimizing the param-
eters of an unsupervised graph-based wsd algorithm.
In Proceedings of the NAACL TextGraphs workshop,
pages 89?96, New York City, June.
D. A. Cruse, 2000. Polysemy: Theoretical and Com-
putational Approaches, chapter Aspects of the Micro-
structure of Word Meanings, pages 31?51. OUP.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solution.
In Proceedings of HLT/NAACL.
R. Koeling, D. McCarthy, and J.D. Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition.
D. Mart??nez and E. Agirre. 2000. One sense per colloca-
tion and genre/topic variations.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A semantic concordance. In Proc. of the ARPA HLT
workshop.
C. Niu, W. Li, R. K. Srihari, and H. Li. 2005. Word
independent context pair classification model for word
sense disambiguation. In Proc. of CoNLL-2005.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proc. of KDD02.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Proc. of CoNLL-2004, pages 41?48.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Proc. of SENSEVAL.
Y Zhao and G Karypis. 2005. Hierarchical clustering
algorithms for document datasets. Data Mining and
Knowledge Discovery, 10(2):141?168.
12
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 346?349,
Prague, June 2007. c?2007 Association for Computational Linguistics
UBC-AS: A Graph Based Unsupervised System
for Induction and Classification
Eneko Agirre and Aitor Soroa
IXA NLP Group
UBC
Donostia, Basque Contry
{e.agirre,a.soroa}@si.ehu.es
Abstract
This paper describes a graph-based unsu-
pervised system for induction and clas-
sification. The system performs a two
stage graph based clustering where a co-
occurrence graph is first clustered to com-
pute similarities against contexts. The con-
text similarity matrix is pruned and the re-
sulting associated graph is clustered again
by means of a random-walk type algorithm.
The system relies on a set of parameters that
have been tuned to fit the corpus data. The
system has participated in tasks 2 and 13
of the SemEval-2007 competition, on word
sense induction and Web people search, re-
spectively, with mixed results.
1 Introduction
This paper describes a graph-based unsupervised
system for induction and classification. Given a set
of data to be classified, the system first induces the
possible clusters and then clusters the data accord-
ingly. The paper is organized as follows. Section 2
gives an description of the general framework of our
system. Sections 3 and 4 presents in more detail the
implementation of the framework for the Semeval-
2007 WEPS task (Artiles et al, 2007) and Semeval-
2007 sense induction task (Agirre and Soroa, 2007),
respectively. Section 5 presents the results obtained
in both tasks, and Section 6 draws some conclusions.
2 A graph based system for unsupervised
classification
The system performs a two stage graph based clus-
tering where a co-occurrence graph is first clustered
to compute similarities against contexts. The context
similarity matrix is pruned and the resulting associ-
ated graph is clustered again by means of a random-
walk type algorithm. We will see both steps in turn.
First step: calculating hub score vectors
In a first step, and for each entity to be clustered, a
graph consisting on context word co-occurrences is
built. Vertices in the co-occurrence graph are words
and two vertices share an edge whenever they co-
occur in the same context. Besides, each edge re-
ceives a weight, which indicates how strong the in-
cident vertices relate each other.
As shown in (Ve?ronis, 2004), co-occurrence
graphs exhibit the so called small world structure
(Watts and Strogatz, 1998) and, thus, they contain
highly dense subgraphs which will represent the dif-
ferent clusters the entity may have. For identifying
these clusters we have implemented two algorithms
based on the notion of centrality of the vertices,
where some highly dense vertices, called ?hubs?, are
chosen as representatives of each cluster. The algo-
rithms are the HyperLex algorithm (Ve?ronis, 2004)
and the HITS algorithm (Kleinberg, 1999).
Once the hubs are identified, the minimum span-
ning tree (MST) of the co-occurrence graph is com-
puted. The root elements of the MST are precisely
the induced hubs and each vertex of the original
graph ?and, thus, each word of the corpus? is at-
tached to exactly one of these hubs, at a certain dis-
tance. Note that the MST can be considered as a
single link clustering over the co-occurrence graph.
The original contexts are then taken one by one
and scored according to the MST in the following
way: each word in the context receives a set of score
vectors, with one score per hub, where all scores are
346
0 except for the one corresponding to the hub where
it is placed1, which will receive a socre d(hi, v),
which is the distance between the hub hi and the
node representing the word v in the MST. Thus,
d(hi, v) assigns a score of 1 to hubs and the score
decreases as the nodes move away from the hub in
the MST. As a consequence, each context receives a
hub score vector, which is just the sum of the score
vectors of all the words in the context.
At this point we can use the hub score vectors
to create clusters of contexts, just assigning to each
context the hub with maximum score. This process
is thoroughly explained in (Agirre et al, 2006b).
One of the problems of such an approach comes
from the tendency of the system to produce a high
number of hubs, somehow favouring small micro-
clusters over coarse ones. Knowing in advance that
the number of clusters in the tasks we will partici-
pate in would not be very high, we decided to per-
form a second stage and re-cluster again the results
obtained in the first step, using a different graph-
based technique. Re-clustering also gives us the op-
portunity to feed the system with additional data, as
will be explained below.
Second step: clustering via MCL
In this second stage, we compute a square ma-
trix with as many rows/columns as contexts, and
where each element represents the relatedness be-
tween two contexts, just computing the cosine dis-
tance of its (normalized) hub score vectors obtained
in the first step. We prune each row in the matrix
and keep only the element with maximum values, so
that the percentage of the kept elements? sum respect
the total is below a given threshold. The resulting
matrix M represents the adjacency matrix of a di-
rected weighted graph, where vertices are contexts
and edges represent the similarity between them. We
can feed the matrixM with external information just
by calculating another dissimilarity matrix between
contexts and lineally interpolating the matrices with
a factor.
Finally, we apply the Markov Clustering (MCL)
algorithm (van Dongen, 2000) over the graph M
for calculating the final clusters. MCL is a graph-
clustering algorithm based on simulation of stochas-
1Note that each word will be attached to exactly one hub in
the MST.
tic flows in graphs, its main idea being that random
walks within the graph will tend to stay in the same
cluster rather than jump between clusters. MCL has
the remarkable property that there is no need to a-
priori decide how many clusters it must find. How-
ever, it has some parameters which will influence the
granularity of the clusters.
In fact, the behavior of the whole process relies
on a number of parameters, which can be divided in
several groups:
? Parameters for calculating the hubs
? Parameters for merging the hubs information
with external information in the matrix M (?)
? The threshold for pruning the graph (?)
? Parameters of the MCL algorithm (I , inflation
parameter)
In sections 3 and 4 we describe the parameters
we actually used for the final experiments, as well
as how the tuning of these parameters has been per-
formed for the two tasks.
3 Web People Search task
In this section we will explain in more detail how
we implemented the general schema described in
the previous section to the ?Web People Search?
task (Artiles et al, 2007). The task consist on dis-
ambiguating person names in a web searching sce-
nario. The input consists on web pages retrieved
from a web searching engine using person names as
a query. The aim is to determine how many ref-
erents (people with the same name) exist for that
person name, and classify each document with its
corresponding referent. There is a train set con-
sisting on 49 names and 100 documents per name.
The test setting consist on 30 unrelated names, with
100 document per name. The evaluation is per-
formed following the ?purity? and ?inverse purity?
measures. Roughly speaking, purity measures how
many classes they are in each cluster (like the pre-
cision measure). If a cluster fits into one class, the
purity equals to 1. On the other side, inverse purity
measures how many clusters they are in each class
(recall). The final figure is obtained by combining
purity and inverse purity by means of the standard
F-Measure with ? = 0.5.
The parameters of the system were tuned using
the train part of the corpus as a development set. As
usual, the parameters that yielded best results were
used on the test part.
347
We first apply a home-made wrapper over the
html files for retrieving the text chunks of the pages,
which is usually mixed with html tags, javascript
code, etc. The text is split into sentences and parsed
using the FreeLing parser (Atserias et al, 2006).
Only the lemmas of nouns are retained. We filter the
nouns and keep only back those words whose fre-
quency, according to the British National Corpus, is
greater than 4. Next, we search for the person name
across the sentences, and when such a sentence is
found we build a context consisting on its four pre-
decessor and four successors, i.e., contexts consists
on 9 sentences. At the end, each document is rep-
resented as a set of contexts containing the person
name. Finally, the person names are removed from
the contexts.
For inducing the hubs we apply the HyperLex al-
gorithm (Ve?ronis, 2004). Then, the MST is calcu-
lated and every context is assigned with a hub score
vector. We calculate the hub score vector of the
whole document by averaging the score vectors of
its contexts. The M matrix of pairwise similarities
between documents is then computed and pruned
with a threshold of 0.2, as described in section 2.
We feed the system with additional data about
the topology of the pages over the web. For each
document di to be classified we retrieve the set of
documents Pi which link to di. We use the pub-
licly available API for Microsoft Search. Then, for
each pair of documents di and dj we calculate the
number of overlapping documents linking to them,
i.e., lij = #{Pi ? Pj} with the intuition that, the
more pages point to the two documents, the more
probably is that they both refer to the same per-
son. The resulting matrix, ML is combined with
the original matrix M to give a final matrix M ?, by
means of a linear interpolation with factor of 0.2, i.e.
M ? = 0.2M + 0.8ML. Finally, the MCL algorithm
is run over M ? with an inflation parameter of 5.
4 Word Sense Induction and
Discrimination task
The goal of this task is to allow for comparison
across sense-induction and discrimination systems,
and also to compare these systems to other super-
vised and knowledge-based systems. The input con-
sist on 100 target words (65 verbs and 35 nouns),
each target word having a set of contexts where the
word appears. The goal is to automatically induce
the senses each word has, and cluster the contexts
accordingly. Two evaluation measures are provided:
and unsupervised evaluation (FScore measure) and
a supervised evaluation, where the organizers auto-
matically map the induced clusters onto senses. See
(Agirre and Soroa, 2007) for more details.
In order to improve the overall performance, we
have clustered the 35 nouns and the 65 verbs sepa-
rately. In the case of nouns, we have filtered the orig-
inal contexts and kept only noun lemmas, whereas
for verbs lemmas of nouns, verbs and adjectives
were hold.
The algorithm for inducing the hubs is also dif-
ferent among nouns and verbs. Nouns hubs are in-
duced with the usual HyperLex algorithm (just like
in section 3) but for identifying verb hubs we used
the HITS algorithm (Kleinberg, 1999), based on pre-
liminary experiments.
The co-occurrence relatedness is also measured
differently for verbs: instead of using the original
conditional probabilities, the ?2 measure between
words is used. The reason behind is that condi-
tional probabilities, as used in (Ve?ronis, 2004), per-
form poorly in presence of words which occur in
nearly all contexts, giving them an extraordinary
high weight in the graph. Very few nouns hap-
pen to occur in many contexts, but they are verbs
which certainly do (be, use, etc). On the other
hand, ?2 measures to what extent the observed co-
occurrences diverge from those expected by chance,
so weights of edges incident with very common,
non-informant words will be low.
Parameter tuning for both nouns and verbs was
performed over the senseval-3 testbed, and the best
parameter combination were applied over the sense
induction corpus. However, there is a factor we have
taken into account in tuning directly over the sense
induction corpus, i.e., that the granularity?and thus
the number of classes? of senses in OntoNotes (the
inventory used in the gold standard) is considerably
coarser than in senseval-3. Therefore, we have man-
ually tuned the inflation parameter of the MCL al-
gorithm in order to achieve numbers of clusters be-
tween 1 and 4.
A threshold of 0.6was used when pruning the dis-
similarity matrix M for both nouns and verbs. We
have tried to feed the system with additional data
348
System All Nouns Verbs
Best 78.7 80.8 76.3
Worst 56.1 62.3 45.1
Average 65.4 69.0 61.4
UBC-AS 78.7 80.8 76.3
Table 1: Results of Semeval-2007 Task 2. Unsuper-
vised evaluation (FScore).
System All Nouns Verbs
Best 81.6 86.8 76.2
Worst 77.1 80.5 73.3
Average 79.1 82.8 75.0
UBC-AS 78.5 80.7 76.0
Table 2: Results of Semeval-2007 Task 2. Super-
vised evaluation as recall.
(mostly local and domain features of the context
words) but, although the system performed slightly
better, we decided that the little gain (which prob-
ably was not statistically significant) was no worth
the effort.
5 Results
Table 1 shows the results of the unsupervised evalu-
ation in task 2, where our system got the best results
in this setting. Table 2 shows the supervised evalua-
tion on the same task, where our system got a rank-
ing of 4, performing slightly worse than the average
of the systems.
In Table 3 we can see the results of Semeval-2007
Task 13. As can be seen, our system didn?t manage
to capture the structure of the corpus, and it got the
worst result, far below the average of the systems.
6 Conclusions
We have presented graph-based unsupervised sys-
tem for induction and classification. The system per-
forms a two stage graph based clustering where a co-
occurrence graph is first clustered to compute simi-
larities against contexts. The context similarity ma-
trix is pruned and the resulting associated graph is
clustered again by means of a random-walk type al-
gorithm. The system has participated in tasks 2 and
13 of the SemEval-2007 competition, on word sense
induction and Web people search, respectively, with
mixed results. We did not have time to perform
an in-depth analysis of the reasons causing such a
different performance. One of the reasons for the
failure in the WePS task could be the fact that we
System F?=0.5
Best 78.0
Worst 40.0
Average 60.0
UBC-AS 40.0
Table 3: Results of Semeval-2007 Task 13
were first-comers, with very little time to develop
the system, and we used a very basic and coarse pre-
processing of the HTML files. Another factor could
be that we intentionally made our clustering algo-
rithm return few clusters. We were mislead by the
training data provided, as the final test data had more
classes on average.
Acknowledgements
This work has been partially funded by the Spanish
education ministry (project KNOW) and by the re-
gional government of Gipuzkoa (project DAHAD).
References
E. Agirre and A. Soroa. 2007. Semeval-2007 task 2:evaluating
word sense induction and discrimination systems. In Pro-
ceedings of Semeval 2007, Association for Computational
Linguistics.
E. Agirre, D. Mart??nez, O. Lo?pez de Lacalle, and A. Soroa.
2006a. Evaluating and optimizing the parameters of an un-
supervised graph-based wsd algorithm. In Proceedings of
TextGraphsWorkshop. NAACL06., pages 89?96. Association
for Computational Linguistics, June.
E. Agirre, D. Mart??nez, O. Lo?pez de Lacalle, and A. Soroa.
2006b. Two graph-based algorithms for state-of-the-art wsd.
In Proceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 585?593. Asso-
ciation for Computational Linguistics, July.
J. Artiles, J. Gonzalo, and S. Sekine. 2007. Establishing a
benchmark for the web people search task: The semeval
2007 weps track. In Proceedings of Semeval 2007, Asso-
ciation for Computational Linguistics.
J. Atserias, B. Casas, E. Comelles, M. Gonza?lez, L. Padro?, and
M. Padro?. 2006. Freeling 1.3: Syntactic and semantic ser-
vices in an open-source NLP library. In Proceedings of the
5th International Conference on Language Resources and
Evaluation (LREC?06), pages 48?55.
Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked
environment. Journal of the ACM, 46(5):604?632.
Stijn van Dongen. 2000. A cluster algorithm for graphs.
Technical Report INS-R0010, National Research Institute
for Mathematics and Computer Science in the Netherlands,
Amsterdam, May.
J. Ve?ronis. 2004. Hyperlex: lexical cartography for informa-
tion retrieval. Computer Speech & Language, 18(3):223?
252.
D. J. Watts and S. H. Strogatz. 1998. Collective dynamics of
?small-world? networks. Nature, 393(6684):440?442, June.
349
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 41?49,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
WikiWalk: Random walks on Wikipedia for Semantic Relatedness
Eric Yeh, Daniel Ramage,
Christopher D. Manning
Computer Science Department,
Stanford University
Stanford, CA, USA
{yeh1,dramage,manning}@cs.stanford.edu
Eneko Agirre, Aitor Soroa
Ixa Taldea
University of the Basque Country
Donostia, Basque Country
{e.agirre,a.soroa}@ehu.es
Abstract
Computing semantic relatedness of natural
language texts is a key component of tasks
such as information retrieval and sum-
marization, and often depends on knowl-
edge of a broad range of real-world con-
cepts and relationships. We address this
knowledge integration issue by comput-
ing semantic relatedness using person-
alized PageRank (random walks) on a
graph derived from Wikipedia. This pa-
per evaluates methods for building the
graph, including link selection strategies,
and two methods for representing input
texts as distributions over the graph nodes:
one based on a dictionary lookup, the
other based on Explicit Semantic Analy-
sis. We evaluate our techniques on stan-
dard word relatedness and text similarity
datasets, finding that they capture similar-
ity information complementary to existing
Wikipedia-based relatedness measures, re-
sulting in small improvements on a state-
of-the-art measure.
1 Introduction
Many problems in NLP call for numerical mea-
sures of semantic relatedness, including document
summarization, information retrieval, and textual
entailment. Often, measuring the relatedness of
words or text passages requires world knowledge
about entities and concepts that are beyond the
scope of any single word in the document. Con-
sider, for instance, the following pair:
1. Emancipation Proclamation
2. Gettysburg Address
To correctly assess that these examples are re-
lated requires knowledge of the United States Civil
War found neither in the examples themselves nor
in traditional lexical resources such as WordNet
(Fellbaum, 1998). Fortunately, a massive collabo-
ratively constructed knowledge resource is avail-
able that has specific articles dedicated to both.
Wikipedia is an online encyclopedia containing
around one million articles on a wide variety of
topics maintained by over one hundred thousand
volunteer editors with quality comparable to that
of traditional encyclopedias.
Recent work has shown that Wikipedia can be
used as the basis of successful measures of se-
mantic relatedness between words or text pas-
sages (Strube and Ponzetto, 2006; Gabrilovich and
Markovitch, 2007; Milne and Witten, 2008). The
most successful measure, Explicit Semantic Anal-
ysis (ESA) (Gabrilovich and Markovitch, 2007),
treats each article as its own dimension in a vec-
tor space. Texts are compared by first projecting
them into the space of Wikipedia articles and then
comparing the resulting vectors.
In addition to article text, Wikipedia stores a
great deal of information about the relationships
between the articles in the form of hyperlinks, info
boxes, and category pages. Despite a long his-
tory of research demonstrating the effectiveness
of incorporating link information into relatedness
measures based on the WordNet graph (Budanit-
sky and Hirst, 2006), previous work on Wikipedia
has made limited use of this relationship infor-
mation, using only category links (Bunescu and
Pasca, 2006) or just the actual links in a page
(Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008).
In this work, we combine previous approaches
by converting Wikipedia into a graph, mapping in-
put texts into the graph, and performing random
walks based on Personalized PageRank (Haveli-
wala, 2002) to obtain stationary distributions that
characterize each text. Semantic relatedness be-
tween two texts is computed by comparing their
distributions. In contrast to previous work, we
explore the use of all these link types when con-
41
structing the Wikipedia graph, the intuition being
these links, or some combination of them, con-
tain additional information that would allow a gain
over methods that use only just the article text. We
also discuss two methods for performing the initial
mapping of input texts to the graph, using tech-
niques from previous studies that utilized Word-
Net graphs and Wikipedia article text.
We find that performance is signficantly af-
fected by the strategy used to initialize the graph
walk, as well as the links selected when con-
structing the Wikipedia graph. Our best system
combines an ESA-initialized vector with random
walks, improving on state-of-the-art results over
the (Lee et al, 2005) dataset. An analysis of
the output demonstrates that, while the gains are
small, the random walk adds complementary re-
latedness information not present in the page text.
2 Preliminaries
A wide range of different methods, from corpus-
based distributional similarity methods, such as
Latent Semantic Analysis (Landauer et al, 1998),
to knowledge-based ones that employ structured
sources such as WordNet,
1
have been developed
to score semantic relatedness and similarity. We
now review two leading techniques which we use
as starting points for our approach: those that per-
form random walks over WordNet?s graph struc-
ture, and those that utilize Wikipedia as an under-
lying data source.
2.1 Random Graph Walks for Semantic
Relatedness
Some of the best performing WordNet-based al-
gorithms for computing semantic relatedness are
based on the popular Personalized PageRank al-
gorithm (Hughes and Ramage, 2007; Agirre and
Soroa, 2009). These approaches start by taking
WordNet as a graph of concepts G = (V,E) with
a set of vertices V derived from WordNet synsets
and a set of edges E representing relations be-
tween synsets. Both algorithms can be viewed
as random walk processes that postulate the ex-
istence of a particle that randomly traverses the
graph, but at any time may jump, or teleport, to
a new vertex with a given teleport probability. In
standard PageRank (Brin and Page, 1998), this tar-
get is chosen uniformly, whereas for Personalized
1
See (Budanitsky and Hirst, 2006) for a survey.
PageRank it is chosen from a nonuniform distribu-
tion of nodes, specified by a teleport vector.
The final weight of node i represents the propor-
tion of time the random particle spends visiting it
after a sufficiently long time, and corresponds to
that node?s structural importance in the graph. Be-
cause the resulting vector is the stationary distri-
bution of a Markov chain, it is unique for a par-
ticular walk formulation. As the teleport vector
is nonuniform, the stationary distribution will be
biased towards specific parts of the graph. In the
case of (Hughes and Ramage, 2007) and (Agirre
and Soroa, 2009), the teleport vector is used to re-
flect the input texts to be compared, by biasing the
stationary distribution towards the neighborhood
of each word?s mapping.
The computation of relatedness for a word pair
can be summarized in three steps: First, each input
word is mapped with to its respective synsets in
the graph, creating its teleport vector. In the case
words with multiple synsets (senses), the synsets
are weighted uniformly. Personalized PageRank is
then executed to compute the stationary distribu-
tion for each word, using their respective teleport
vectors. Finally, the stationary distributions for
each word pair are scored with a measure of vector
similarity, such as cosine similarity. The method
to compute relatedness for text pairs is analogous,
with the only difference being in the first step all
words are considered, and thus the stationary dis-
tribution is biased towards all synsets of the words
in the text.
2.2 Wikipedia as a Semantic Resource
Recent Wikipedia-based lexical semantic related-
ness approaches have been found to outperform
measures based on the WordNet graph. Two such
methods stand out: Wikipedia Link-based Mea-
sure (WLM) (Milne and Witten, 2008), and Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007).
WLM uses the anchors found in the body of
Wikipedia articles, treating them as links to other
articles. Each article is represented by a list of
its incoming and outgoing links. For word relat-
edness, the set of articles are first identified by
matching the word to the text in the anchors, and
the score is derived using several weighting strate-
gies applied to the overlap score of the articles?
links. WLM does not make further use of the link
graph, nor does it attempt to differentiate the links.
42
In contrast to WLM, Explicit Semantic Analy-
sis (ESA) is a vector space comparison algorithm
that does not use the link structure, relying solely
on the Wikipedia article text. Unlike Latent Se-
mantic Analysis (LSA), the underlying concept
space is not computationally derived, but is instead
based on Wikipedia articles. For a candidate text,
each dimension in its ESA vector corresponds to
a Wikipedia article, with the score being the sim-
ilarity of the text with the article text, subject to
TF-IDF weighting. The relatedness of two texts
is computed as the cosine similarity of their ESA
vectors.
Although ESA reports the best results to date
on both the WordSim-353 dataset as well as the
Lee sentence similarity dataset, it does not utilize
the link structure, which motivated a combined ap-
proach as follows.
2.3 A Combined Approach
In this work, we base our random walk algorithms
after the ones described in (Hughes and Ramage,
2007) and (Agirre et al, 2009), but use Wikipedia-
based methods to construct the graph. As in previ-
ous studies, we obtain a relatedness score between
a pair of texts by performing random walks over
a graph to compute a stationary distribution for
each text. For our evaluations, the score is simply
the cosine similarity between the distributions. In
the following sections, we describe how we built
graphs from Wikipedia, and how input texts are
initially mapped into these structures.
3 Building a Wikipedia Graph
In order to obtain the graph structure of Wikipedia,
we simply treat the articles as vertices, and
the links between articles as the edges. There
are several sources of pre-processed Wikipedia
dumps which could be used to extract the arti-
cles and links between articles, including DBpe-
dia (Auer et al, 2008), which provides a rela-
tional database representation of Wikipedia, and
Wikipedia-Miner
2
, which produces similar infor-
mation from Wikipedia dumps directly. In this
work we used a combination of Wikipedia-Miner
and custom processing scripts. The dump used in
this work is from mid 2008.
As in (Milne and Witten, 2008), anchors in
Wikipedia articles are used to define links between
2
http://wikipedia-miner.sourceforge.net
articles. Because of different distributional proper-
ties, we explicitly distinguish three types of links,
in order to explore their impact on the graph walk.
Infobox links are anchors found in the infobox
section of Wikipedia articles. Article in-
foboxes, when present, often enumerate
defining attributes and characteristics for that
article?s topic.
Categorical links reference articles whose titles
belong in the Wiki namespace ?Category,?
as well as those with titles beginning with
?List of.? These pages are often just lists of
anchors to other articles, which may be use-
ful for capturing categorical information that
roughly contains a mixture of hyponymy and
meronymy relations between articles.
Content links are those that are not already clas-
sified as infobox nor categorical, and are in-
tended to represent the set of miscellaneous
anchors found solely in the article body.
These may include links already found in the
categorical and infobox categories.
Links can be further factored out according to
generality, a concept introduced in (Gabrilovich
and Markovitch, 2009). We say that one article
is more general than another when the number of
inlinks is larger. Although only a rough heuris-
tic, the intuition is that articles on general top-
ics will receive many links, whereas specific ar-
ticles will receive fewer. We will use +k notation
for links which point to more general articles, i.e.,
where the difference in generality between source
s and target t is #inlink(t)/#inlink(s) ? k.
We will use ?k for links to less general articles,
i.e., #inlink(s)/#inlink(t) ? k. Finally we
use =k when the generality is in the same order
of magnitude, i.e., when the link is neither +k
nor ?k. The original notion of generality from
(Gabrilovich and Markovitch, 2009) restricts con-
sideration to only more general articles by one or-
der of magnitude (+10), without reference to the
link types introduced above.
Given the size of the Wikipedia graph, we ex-
plored further methods inspired by (Gabrilovich
and Markovitch, 2009) to make the graph smaller.
We discarded articles with fewer than 2,000 non-
stop words and articles with fewer than 5 outgoing
and incoming links. We will refer to the complete
43
graph as full and to this reduced graph as reduced.
3
4 Initializing a Wikipedia Graph Walk
In order to apply Personalized PageRank to a
given passage of text or word, we need to con-
struct a custom teleport vector, representing the
initial distribution of mass over the article nodes.
In this section we introduce two such methods,
one based on constructing a direct mapping from
individual words to Wikipedia articles (which we
call dictionary-based initialization), and the other
based directly on the results of ESA. We will see
each technique in turn.
4.1 Dictionary based initialization
Given a target word, we would like to define
its teleport vector using the set of articles in
Wikipedia to which the word refers. This is analo-
gous to a dictionary, where an entry lists the set of
meanings pertaining to the entry.
We explored several methods for building such
a dictionary. The first method constructed the dic-
tionary using the article title directly, while also
including redirection pages and disambiguation
pages for additional ways to refer to the article. In
addition, we can use the anchor text to refer to arti-
cles, and we turned to Wikipedia-Miner to extract
this information. Anchors are indeed a rich source
of information, as they help to relate similar words
to Wikipedia articles. For instance, links to page
Monk are created by using textual anchors such as
lama, brothers, monastery, etc. As a result, the
dictionary entries for those words will have a link
to the Monk page. This information turned out to
be very valuable, so all experiments have been car-
ried out using anchors.
An additional difficulty was that any of these
methods yielded dictionaries where the entries
could refer to tens, even hundreds of articles. In
most of the cases we could see that relevant arti-
cles were followed by a long tail of loosely related
articles. We tried two methods to prune the dic-
tionary. The first, coarse, method was to eliminate
all articles whose title contains a space. The mo-
tivation was that our lexical semantic relatedness
datasets (cf. Section 5) do not contain multiword
entries (e.g., United States). In the second method,
we pruned articles from the dictionary which ac-
3
In order to keep category and infobox links, the 2,000
non-stop word filter was not applied to categories and lists of
pages.
Graphs
Graph # Vertices # Edges
Full 2,483,041 49,602,752
Reduced 1,002,411 30,939,288
Dictionaries
Dictionary # Entries Avg. Articles
all 6,660,315 1.31
1% 6,660,306 1.12
1% noent 1,058,471 1.04
Table 1: Graph and dictionary sizes. Avg. Articles
column details the average number of articles per
entry.
counted for less than 1% or 10% of the occur-
rences of that anchor word, as suggested by (Milne
and Witten, 2008).
In short, for this method of initialization, we ex-
plored the use of the following variants: all, all ar-
ticles are introduced in the dictionary; noent, arti-
cles with space characters are omitted; 1% (10%),
anchors that account for less than 1% (10%) of the
total number of anchors for that entry are omitted.
We did not use stemming. If a target word has no
matching Wikipedia article in the dictionary, then
it is ignored.
Table 1 shows the numbers for some graph and
dictionary versions. Although the average number
of articles per entry in the dictionary might seem
low, it is actually quite high for the words in the
datasets: for MC it?s 5.92, and for wordsim353 it?s
42.14. If we keep the articles accounting for 10%
of all occurrences, the numbers drops drastically
to 1.85 and 1.64 respectively.
As we will see in the results section, smaller
graphs and dictionaries are able to attain higher
results, but at the cost of losing information for
some words. That is, we observed that some fac-
tored, smaller graphs contained less noise, but that
meant that some articles and words are isolated in
the graph, and therefore we are not able to com-
pute relatedness for them. As a solution, we de-
vised an alternative way to initialize the random
walk. Instead of initializing it according to the ar-
ticles in the dictionary, we initialized it with the
vector weights returned by ESA, as explained in
the next section.
44
4.2 Initialization with ESA
In addition to the dictionary based approach, we
also explored the use of ESA to construct the tele-
port vector. In contrast to dictionary initialization,
ESA uses the text of the article body instead of an-
chor text or the article titles. Because ESA maps
query text to a weighted vector of Wikipedia arti-
cles, it can be naturally adapted as a teleport vector
for a random walk with a simple L
1
normaliza-
tion. We used Apache Lucene
4
to implement both
ESA?s repository of Wikipedia articles, and to re-
turn vectors for queries. Each article is indexed as
its own document, with page text preprocessed to
strip out Wiki markup.
Although we followed the steps outlined in
(Gabrilovich and Markovitch, 2007), we had to
add an extension to the algorithm: for a return
vector from ESA, we order the articles by score,
and retain only the scores for the top-n articles,
setting the scores of the remaining articles to 0.
Without this modification, our performance results
were will below the reported numbers, but with a
cutoff at 625 (determined by a basic grid search),
we obtained a correlation of 0.76 on the Lee sen-
tence similarity dataset, over the previously pub-
lished score of 0.72.
4.3 Teleport Probability
For this work, we used a value of 0.15 as the prob-
ability of returning to the teleport distribution at
any given step. The walk terminates when the vec-
tor converges with an L
1
error of 0.0001 (circa 30
iterations). Some preliminary experiments on a re-
lated Word Sense Disambiguation task indicated
that in this context, our algorithm is quite robust to
these values, and we did not optimize them. How-
ever, we will discuss using different return param-
eters in Section 6.1.
5 Experiments
In this section, we compare the two methods of
initialization as well as several types of edges. For
a set of pairs, system performance is evaluated by
how well the generated scores correlate with the
gold scores. Gold scores for each pair are the av-
erage of human judgments for that pair. In order to
compare against previous results obtained on the
datasets, we use the Spearman correlation coeffi-
cient on the Miller Charles (MC) and WordSim-
353 word-pair datasets, and the Pearson correla-
4
http://lucene.apache.org
Dictionary Graph MC
all full 0.369
1% full 0.610
1%, noent full 0.565 (0.824)
1% reduced 0.563
1% reduced +2 0.530
1% reduced +4 0.601
1% reduced +8 0.512
1% reduced +10 0.491 (0.522)
10% full 0.604 (0.750)
10% reduced 0.605 (0.751)
10% reduced +2 0.491 (0.540)
10% reduced +4 0.476 (0.519)
10% reduced +8 0.474 (0.506)
10% reduced +10 0.430 (0.484)
WordNet 0.90 / 0.89
WLM 0.70
ESA 0.72
Table 2: Spearman correlation on the MC dataset
with dictionary-based initialization. Refer to Sec-
tion 3 for explanation of dictionary and graph
building methods. Between parenthesis, results
excluding pairs which had a word with an empty
dictionary entry.
tion coefficient on the (Lee et al, 2005) document-
pair dataset.
5.1 Dictionary-based Initialization
Given the smaller size of the MC dataset, we
explored the effect of the different variants to
build the graph and dictionary on this dataset.
Some selected results are shown in Table 2, along-
side those of related work, where we used Word-
Net for (Hughes and Ramage, 2007) and (Agirre
et al, 2009) (separated by ?/? in the results),
WLM for (Milne and Witten, 2008) and ESA for
(Gabrilovich and Markovitch, 2007).
We can observe that using the full graph and
dictionaries yields very low results. Reducing the
dictionary (removing articles with less than 1% or
10% of the total occurrences) produces higher re-
sults, but reducing the graph does not provide any
improvement. On a closer look, we realized that
pruning the dictionary to 10% or removing multi-
words (noent) caused some words to not get any
link to articles (e.g., magician). If we evaluate
only over pairs where both words get a Personal-
ized PageRank vector, the results raise up to 0.751
and 0.824, respectively, placing our method close
45
Dictionary Graph WordSim-353
1% full 0.449
1%, noent full 0.440 (0.634)
1% reduced 0.485
WordNet 0.55 / 0.66
WLM 0.69
ESA 0.75
WikiRelate 0.50
Table 3: Spearman correlation on the WordSim-
353 dataset with dictionary-based initialization.
Refer to Section 3 for explanation of dictionary
and graph building methods. Between parenthe-
sis, results excluding pairs which had a word with
an empty dictionary entry.
Dictionary Graph (Lee et al, 2005)
1%, noent Full 0.308
1% Reduced +4 0.269
ESA 0.72
Table 4: Pearson correlation on (Lee et al, 2005)
with dictionary-based initialization. Refer to Sec-
tion 3 for explanation of dictionary and graph
building methods.
to the best results on the MC dataset. This came
at the cost of not being able to judge the related-
ness of 3 and 5 pairs, respectively. We think that
removing multiwords (noent) is probably too dras-
tic, but the positive effect is congruent with (Milne
and Witten, 2008), who suggested that the cover-
age of certain words in Wikipedia is not adequate.
The results in Table 3 show the Spearman cor-
relation for some selected runs over the WordSim-
353 dataset. Again we see that a restrictive dic-
tionary allows for better results on the pairs which
do get a dictionary entry, up to 0.63. WikiRelate
refers to the results in (Strube and Ponzetto, 2006).
We only tested a few combinations over (Lee et
al., 2005), with results given in Table 4. These are
well below state-of-the-art, and show that initial-
izing the random walk with all words in the doc-
ument does not characterize the documents well,
resulting in low correlation.
5.2 ESA-based initialization
While the results using a dictionary based ap-
proach were encouraging, they did not come close
to the state-of-the-art results achieved by ESA.
Here, we explore combining ESA and random
Method Text Sim
ESA@625 0.766
ESA@625+Walk All 0.556
ESA@625+Walk Categories 0.410
ESA@625+Walk Content 0.536
ESA@625+Walk Infobox 0.710
Table 5: Pearson correlation on the (Lee et al,
2005) dataset when walking on various types of
links. Note that walking tends to hurt performance
overall, with Infobox links by far the least harm-
ful.
walks, by using ESA to initialize the teleport vec-
tor. Following section 4.2, we used a top-n cutoff
of 625.
Table 5 displays the results of our ESA im-
plementation followed by a walk from that ESA
distribution. Walking on any link type actually
depresses performance below the baseline ESA
value, although the Infobox links seem the least
harmful.
However, as mentioned in Section 3, links be-
tween articles represent many different types of
relationships beyond the few well-defined links
present in lexical resources like WordNet. This
also extends to where the link is found, and the ar-
ticle it is pointing to. As such, not all links are cre-
ated equal, and we expect that some types of links
at different levels of generality will perform bet-
ter or worse than others. Table 6 presents a sam-
ple grid search across the category links choosing
more general, less general, or similar generality at
several factors of k, showing that there is a consis-
tent pattern across multiple link types. Note that
the best value indeed improves upon the score of
the ESA distribution, albeit modestly.
We performed a similar analysis across all link
types and found that the best link types were Cat-
egory links at +6 and Infobox links at =2. Intu-
itively, these link types make sense: for seman-
tic relatedness, it seem reasonable to expect more
general pages within the same category to help.
And for Infobox links, much rarer and much more
common pages can both introduce their own kind
of noise. While the improvement from each type
of edge walk is small, they are additive?the best
results on the sentence similarity dataset was from
walking across both link types. Our final Pearson
correlation coefficient of .772 is to our knowledge
the highest number reported in the literature, al-
46
Generality of Category links
+k -k =k
k = 2 0.760 0.685 0.462
k = 4 0.766 0.699 0.356
k = 6 0.771 0.729 0.334
k = 8 0.768 0.729 0.352
k = 10 0.768 0.720 0.352
Table 6: Pearson correlation on the (Lee et al,
2005) with random walks over only a subset of
the edges in the Category link information (scores
.410 when taking all edges). Note that factoring
the graph by link generality can be very helpful to
the walk.
Method Text Sim
ESA@625 0.766
ESA@625+Walk Cat@+6 0.770
ESA@625+Walk Cat@+6 Inf@=2 0.772
Bag of words (Lee et al, 2005) 0.5
LDA (Lee et al, 2005) 0.60
ESA* 0.72
Table 7: Pearson correlation on the (Lee et al,
2005) dataset for our best sytems compared to pre-
viously reported numbers. ESA* is the score for
raw ESA as reported number in (Gabrilovich and
Markovitch, 2007).
beit only a small improvement over our ESA@625
score.
Despite the results obtained for text similarity,
the best settings found for the Lee dataset did not
translate to consistent improvements over the ESA
baseline for Spearman rank correlation on the lex-
ical similarity datasets. While our scores on the
MC dataset of 30 word pairs did improve with the
walk in roughly the same way as in Lee, no such
improvements were found on the larger WordSim-
353 data. On WordSim-353, our implementa-
tion of ESA scored 0.709 (versus Gabrilovich?s
reported ESA score of 0.75), and our walk on
Cat@+6 showing no gain or loss. In contrast to
the text similarity dataset, Infobox links were no
longer helpful, bringing the correlation down to
.699. We believe this is because Infobox links
helped the most with entities, which are very rare
in the WordSim-353 data, but are more common
in the Lee dataset.
6 Discussion
Our results suggest that even with a simple
dictionary-based approach, the graph of Wikipedia
links can act as an effective resource for comput-
ing semantic relatedness. However, the dictio-
nary approach alone was unable to reach the re-
sults of state-of-the-art models using Wikipedia
(Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008) or using the same technique on
WordNet (Hughes and Ramage, 2007; Agirre
et al, 2009). Thus, it seems that the text of
Wikipedia provides a stronger signal than the link
structure. However, a pruned dictionary can im-
prove the results of the dictionary based initial-
ization, which indicates that some links are in-
formative for semantic relatedness while others
are not. The careful pruning, disambiguation and
weighting functions presented in (Milne and Wit-
ten, 2008) are directions for future work.
The use of WordNet as a graph provided ex-
cellent results (Hughes and Ramage, 2007), close
to those of ESA. In contrast with our dictionary-
based initialization on Wikipedia, no pruning of
dictionary or graph seem necessary to obtain high
results with WordNet. One straightforward expla-
nation is that Wikipedia is a noisy source of link
information. In fact, both ESA and (Milne and
Witten, 2008) use ad-hoc pruning strategies in or-
der to obtain good results.
6.1 ESA and Walk Comparison
By using ESA to generate the teleport distribu-
tion, we were able to introduce small gains us-
ing the random walk. Because these gains were
small, it is plausible that the walk introduces only
modest changes from the initial ESA teleport dis-
tributions. To evaluate this, we examined the dif-
ferences between the vector returned by ESA and
distribution over the equivalent nodes in the graph
after performing a random walk starting with that
ESA vector.
For this analysis, we took all of the text entries
used in this study, and generated two distributions
over the Wikipedia graph, one using ESA@625,
the other the result of performing a random walk
starting at ESA@625. We generated a list of the
concept nodes for both distributions, sorted in de-
creasing order by their associated scores. Start-
ing from the beginning of both lists, we then
counted the number of matched nodes until they
disagreed on ordering, giving a simple view of
47
Walk Type Avg Std Max
MC Cat@+6 12.1 7.73 35
Cat@+6 Inf@=2 5.39 5.81 20
WordSim Cat@+6 12.0 10.6 70
Cat@+6 Inf@=2 5.74 7.78 54
Lee Cat@+6 28.3 89.7 625
Cat@+6 Inf@=2 4.24 14.8 103
Table 8: Statistics for first concept match length,
by run and walk type.
how the walk perturbed the strongest factors in the
graphs. We performed this for both the best per-
forming walk models (ESA@625+Walk Cat@+6
and ESA@625+Walk Cat@+6 Inf@=2) against
ESA@625. Results are given in Table 8.
As expected, adding edges to the random walk
increases the amount of change from the graph,
as initialized by ESA. A cursory examination of
the distributions also revealed a number of outliers
with extremely high match lengths: these were
likely due to the fact that the selected edge types
were already extremely specialized. Thus for a
number of concept nodes, it is likely they did not
have any outbound edges at all.
Having established that the random walk does
indeed have an impact on the ESA vectors, the
next question is if changes via graph walk are
consistently helpful. To answer this, we com-
pared the performance of the walk on the (Lee et
al., 2005) dataset for probabilities at selected val-
ues, using the best link pruned Wikipedia graph
(ESA@625+Walk Cat@+6 Inf@=2), and using all
of the available edges in the graph for compari-
son. Here, a lower probability means the distribu-
tion spreads out further into the graph, compared
to higher values, where the distribution varies only
slightly from the ESA vector. Results are given in
Table 9. Performance for the pruned graph im-
proves as the return probability decreases, with
larger changes introduced by the graph walk re-
sulting in better scores, whereas using all available
links decreases performance. This reinforces the
notion that Wikipedia links are indeed noisy, but
that within a selected edge subset, making use of
all information via the random walk indeed results
in gains.
7 Conclusion
This paper has demonstrated that performing ran-
dom walks with Personalized PageRank over the
Prob Corr (Pruned) Corr (All)
0.01 0.772 0.246
0.10 0.773 0.500
0.15 0.772 0.556
0.30 0.771 0.682
0.45 0.769 0.737
0.60 0.767 0.758
0.90 0.766 0.766
0.99 0.766 0.766
Table 9: Return probability vs. correlation, on tex-
tual similarity data (Lee et al, 2005).
Wikipedia graph is a feasible and potentially fruit-
ful means of computing semantic relatedness for
words and texts. We have explored two methods of
initializing the teleport vector: a dictionary-based
method and a method based on ESA, the cur-
rent state-of-the-art technique. Our results show
the importance of pruning the dictionary, and for
Wikipedia link structure, the importance of both
categorizing by anchor type and comparative gen-
erality. We report small improvements over the
state-of-the-art on (Lee et al, 2005) using ESA as
a teleport vector and a limited set of links from
Wikipedia category pages and infoboxes.
In future work, we plan to explore new ways
to construct nodes, edges, and dictionary entries
when constructing the Wikipedia graph and dic-
tionary. We believe that finer grained methods of
graph construction promise to improve the value
of the Wikipedia link structure. We also plan to
further investigate the differences between Word-
Net and Wikipedia and how these may be com-
bined, from the perspective of graph and random
walk techniques. A public distribution of software
used for these experiments will also be made avail-
able.
5
Acknowledgements
The authors would like to thank Michael D. Lee
and Brandon Pincombe for access to their textual
similarity dataset, and the reviewers for their help-
ful comments. Eneko Agirre performed part of
the work while visiting Stanford, thanks to a grant
from the Science Ministry of Spain.
5
Please see http://nlp.stanford.edu/
software and http://ixa2.si.ehu.es/ukb
48
References
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proceedings
of 14th Conference of the European Chapter of the
Association for Computational Linguistics, Athens,
Greece.
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pasc?a, and A. Soroa. 2009. A study on similarity
and relatedness using distributional and WordNet-
based approaches. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies,
Boulder, USA.
S Auer, C Bizer, G Kobilarov, J Lehmann, R Cyganiak,
and Z Ives. 2008. Dbpedia: A nucleus for a web
of open data. In Proceedings of 6th International
Semantic Web Conference, 2nd Asian Semantic Web
Conference (ISWC+ASWC 2007), pages 722?735.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30(1-7).
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13?47.
R. C. Bunescu and M. Pasca. 2006. Using encyclo-
pedic knowledge for named entity disambiguation.
In Proceedings of 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. MIT Press.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-07).
E. Gabrilovich and S. Markovitch. 2009. Wikipedia-
based semantic interpretation. Journal of Artificial
Intelligence Research, 34:443?498.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02, pages 517?526, New York, NY, USA.
ACM.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings
of EMNLP-CoNLL, pages 581?589.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25(2-3):259?284.
M. D. Lee, B. Pincombe, and M. Welsh. 2005. An em-
pirical evaluation of models of text document sim-
ilarity. In Proceedings of the 27th Annual Confer-
ence of the Cognitive Science Society, pages 1254?
1259, Mahwah, NJ. Erlbaum.
D. Milne and I.H. Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
Wikipedia links. In Proceedings of the first AAAI
Workshop on Wikipedia and Artifical Intellegence
(WIKIAI?08), Chicago, I.L.
M. Strube and S.P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedness using Wikipedia. In
Proceedings of the 21st National Conference on Ar-
tificial Intelligence, pages 1419?1424.
49
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2260?2269, Dublin, Ireland, August 23-29 2014.
?One Entity per Discourse? and ?One Entity per Collocation?
Improve Named-Entity Disambiguation
Ander Barrena*, Eneko Agirre*, Bernardo Cabaleiro**, Anselmo Pe
?
nas**, Aitor Soroa*
*IXA NLP Group / University of the Basque Country, Basque Country
abarrena014@ikasle.ehu.es, e.agirre@ehu.es, a.soroa@ehu.es
**UNED NLP & IR Group, Madrid
anselmo@lsi.uned.es, bcabaleiro@lsi.uned.es
Abstract
The ?one sense per discourse? (OSPD) and ?one sense per collocation? (OSPC) hypotheses have
been very influential in Word Sense Disambiguation. The goal of this paper is twofold: (i) to
explore whether these hypotheses hold for entities, that is, whether several mentions in the same
discourse (or the same collocation) tend to refer to the same entity or not, and (ii) test their impact
in Named-Entity Disambiguation (NED). Our experiments show consistent results on different
collections and three state-of-the-art NED system. OSPD hypothesis holds in around 96%-98%
of documents whereas OSPC hypothesis holds in 91%-98% of collocations. Furthermore, a
simple NED post-processing in which the majority entity is promoted, produces a gain in perfor-
mance in all cases, reaching up to 8 absolute points of improvement in F-measure. These results
show that NED systems would benefit of considering these hypotheses into their implementation.
1 Introduction
The ?one sense per discourse? (OSPD) hypothesis was introduced by Gale et al. (1992), and stated that a
word tends to preserve its meaning when occurring multiple times in a discourse. They estimated that the
probability of two occurrences of the same polysemous noun drawn from one document having the same
sense to be around 94% for documents from Grolier encyclopedia, and 96% for documents from Brown,
based on word senses from the Oxford Advanced Learner?s Dictionary and a handful of examples. A few
years later, Krovetz (1998) reported 66% on larger corpora (SemCor and DSO) annotated with WordNet
senses by third parties, but, unfortunately, he only reported how many polysemous nouns occurred with
a single sense in all documents, not in each document. In the context of statistical machine translation,
Carpuat (2009) reported that, 80% of the time, words occurring multiple times in a source document are
translated into a single word in the target language.
In the case of entities, OSPD is closely related to coreference, where the task is to find whether two
different mentions (perhaps using different surface strings like ?John? and ?he?) in a document refer
to the same entity or not. For instance, the coreference system presented by (Lee et al., 2013), uses a
heuristic which links mentions in a document that share the same surface string: ?This sieve [heuristic]
accounts for approximately 16 CoNLL F1 points improvement, which proves that a signicant percentage
of mentions in text are indeed repetitions of previously seen concepts?. Our paper actually quantifies the
amount of those repetitions for entities, providing additional evidence for the heuristic.
The ?one sense per collocation? (OSPC) hypothesis was introduced by Yarowsky (1993), stating that
a word tends to preserve its meaning when occurring with the same collocate. Yarowsky tested his
hypothesis for several definitions of collocate, including positional collocates (word to left or right)
and syntactic collocations (governing verb of object, governing verb of subject, modifying adjective).
He reported entropy on train data, as well as disambiguation performance on unseen data, with the
precision ranging between 90% and 99% for a handful of words with two distinct homograph senses,
like, e.g. ?bass? or ?colon?. In larger-scale research, Martinez and Agirre (2000) measured the precision
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2260
Abbott Beefs Up Litigation Reserves NORTH CHICAGO, Ill. (AP) Abbott Laboratories Inc., bracing
for a costly settlement in a federal investigation involving the prostate-cancer drug Lupron, said Friday
it was increasing litigation reserves by $344 million. As part of the announcement, Abbott said it had
restated its quarterly results and is now reporting a loss of $319.9 million for the first three months
of this year rather than a profit. The move comes amid long-running negotiations between the U.S.
Department of Justice and TAP Pharmaceutical Products, the 50-50 joint venture between Abbott and
Takeda Chemical Industries of Japan that made Lupron. Abbott said in January ...
Figure 1: Example of OSPD for entities. All occurrences of ?Abbott? refer to ?Abbott Laboratories?.
of similar collocations on corpora (Semcor and DSO) annotated by third parties with finer-grained senses
from WordNet, reporting lower figures around 70%.
In this paper, we take a collocation to be a word (or multiword term) that co-occurs with the target
named-entity more often than would be expected by chance. In our case we use syntactic dependencies
to extract co-occurring terms.
These two hypotheses have been very influential, and have inspired multiple heuristics and methods
in Word Sense Disambiguation research (Agirre and Edmonds, 2007, Chapters 5,7,10,11). In this work
we are going to show that both hypotheses hold for named-entities as well, and that the hypotheses can
be used to post-process the output of any Named-Entity Disambiguation system (NED) to improve its
performance. NED, also known as Entity Linking, takes as input a named-entity mention in context and
assigns it a specific entity from a given entity repository (Hachey et al., 2012; Daiber et al., 2013).
In the first part of this work we are going to test whether the two hypotheses hold for entity mentions
with respect to a repository of entities extracted from Wikipedia. For instance, do all occurrences of
mention ?Abbott? in a document refer to the same entity? Do all occurrences of mention ?CPI? as
subject of verb ?rise? refer to the same entity? Do all occurrences of ?CDU? in relation to ?Merkel? refer
to the same entity? The examples in Figures 1 and 2 show evidence that this is indeed the case. The
experiments aim at quantifying in which degree OSPD and OSPC hypotheses hold for entities
1
.
In the second part of the paper, we will explore a simple method to incorporate OSPD and OSPC
hypotheses to any existing NED system, showing their potential. After running the NED system, we take
its output and observe, for each mention string, which is the entity returned most often for a given docu-
ment (or collocation), assigning to all occurrences the majority entity. We tested the improvements with a
freely available NED system (Daiber et al., 2013), a reimplementation of a strong Bayesian NED system
(Han and Sun, 2011) and an in-house graph-based system. We got statistically significant improvements
for all systems and ?one sense? hypotheses that we tested, with a couple exceptions.
In order to check the OSPD and OSPC hypotheses for entities, we first looked into existing datasets.
AIDA (Hoffart et al., 2011)
2
is a publicly available hand-tagged corpus based on the CoNLL named-
entity recognition and disambiguation task dataset. AIDA contains links of all entity mentions in full
documents, so it is a natural fit for OSPD. We estimated OSPD based on more than 4,000 mentions that
occur multiple times in a document. For completeness, we also estimated OSPD at the collection level.
OSPD and OSPC are independent of each other, as one is applied at the document level and the other
at the corpus level, focusing on the entities that occur with a specific collocation. Multiple occurrences
of a target string in a document usually occur with different collocations, and conversely, multiple occur-
rences of a target string with a specific collocation typically occur in different documents. Note also that
singletons (entities that are only mentioned once in a document) are not affected by OSPD, but could be
affected by OSPC.
In order to estimate OSPC, no available corpus existed, so we decided to base our dataset on the TAC
KBP 2009 Entity Linking dataset
3
(TAC2009 for short) (Ji et al., 2010). The TAC2009 dataset involves
138 mention strings, which have been annotated in several documents drawn primarily from Gigaword
4
.
1
For the sake of clarity we will also refer to OSPD and OSPC for entities as OSPD and OSPC.
2
http://www.mpi-inf.mpg.de/yago-naga/aida/downloads.html
3
http://www.nist.gov/tac/2013/KBP/EntityLinking/index.html
4
http://catalog.ldc.upenn.edu/LDC2003T05
2261
CPI subject-of rise:
China?s consumer price index, or CPI, rose 2.8 percent last December.
In the 10 months to October, the CPI rose 1.35 percent, the core price index grew 1.13 percent ...
Measured on a month-on-month basis, March CPI rose 2.3 percent from February, ...
... still lower than in China, Hong Kong and Singapore, whose CPIs have rised 8.0 percent, ...
The core CPI rose 0.2 percent, in line with Wall Street expectations.
Angela Merkel has CDU:
... who share power with Merkel?s CDU nationally in an uneasy ? grand coalition ? ...
Economy Minister Michael Glos, also from the CSU, the sister party to Merkel?s CDU ...
In the past Merkel?s CDU had been able to rely on the CSU?s strength in Bavaria ...
... but while her conservative CDU wanted new legal tools to do so, ...
The new development has put a further strain on Merkel?s CDU ...
Figure 2: Examples of OSPC for entities, showing five examples for a syntactic collocation (top row)
and fie examples for a more specific proposition (bottom row). ?CPI? might refer to ?Comunist Party
of India? or ?Consumer Price Index?, among others, but refers to the second in all cases. ?CDU? can
refer to the German ?Christian Democratic Union? or ?Catholic Distance University?, among others, but
refers to the first in all cases.
We extracted several syntactic collocations for those 138 mention strings from Gigaword, and hand-
annotated them, yielding an estimate for the OSPC. Note that TAC2009 only provides the annotation for
a specific mention in a document, so we had to annotate by hand the rest of occurrences in the documents.
For instance, we analyzed examples of ?CPI? as subject of the verb ?rise? (cf. Figure 2). Some of the
syntactic collocations like the subjects of verb ?has? seemed very uninformative, so we decided to also
check the OSPC hypothesis on more specific collocations, involving more complete argument structures.
For instance, we checked ?ABC? occurring as subject of ?has? with object ?radio?. We call this more
specific collocations propositions (Pe?nas and Hovy, 2010).
The paper is structured as follows. We will first present the resources used in this study. Section 3
presents the results of OSPD. Section 3.1 extends OSPD when, instead of documents, we take the com-
plete collection. Section 4 presents the study of OSPC both for syntactic dependencies and propositions.
Section 5 presents the experiments where OSPD and OSPC are used to improve the performance of
existing systems. Finally, we draw the conclusions and future work.
2 Resources used
AIDA is based on the corpus used in the CONLL named-entity recognition and classification task, where
all entities in full documents had been linked to the referred Wikipedia articles (using the 2010 Wikipedia
dump). We use the full AIDA dataset, with 1,393 documents, 34,140 disambiguated entity mentions,
where 27,240 are linked to a Wikipedia article. All in all there are 6,877 distinct mention strings (types)
which are linked at least once to a Wikipedia article. The rest refer to articles not in Wikipedia (NIL
instances), and were discarded. This corpus covers news from a sample of a few days spanning from
1996-05-28 to 1996-12-07.
In order to prepare our dataset for OSPC, we chose the dataset of the TAC KBP 2009 Entity Linking
competition, as this dataset have been extensively used in Entity Linking evaluation. In addition, the cor-
pus used in the task was very large, allowing us to mine relevant collocations (see below). We manually
annotated the occurrences in the extracted collocations, producing two datasets, one for each kind of col-
location (cf. Section 4). Note that the TAC KBP organizers only annotated one specific mention in each
target document. For completeness, we also tagged the rest of the occurrences of the target mentions in
the documents, thus allowing us to provide OSPD estimated based on TAC2009 data as well. This is
the third dataset that we annotated by hand. The hand-annotation was performed by a single person, and
later reviewed by the rest of the authors. The three annotation datasets are publicly available
5
. Hand-
5
http://ixa2.si.ehu.es/OEPDC
2262
NHasN ?U.S. dollar?
NPN ?condition of anonymity?
NVN ?official tells AFP?
NVNPN ?article maintains interest within layout?
NVPN ?others steal from input?
VNPN ?includes link to website?
Table 1: List of the six patterns used to extract propositions, with some examples.
tagging is costly, so we tagged around 250 examples of syntactic collocations and around 250 examples
of propositions.
Note that both AIDA and TAC2009 contain mentions that were not linked to a Wikipedia article
because the mention referred to an entity which was not listed in the entity inventory. We ignored all
those cases (called NIL cases), as we would need to investigate, for each NIL, which actual entity they
refer to.
The collocations were extracted from the TAC KBP collection (Ji et al., 2010), comprising 1.7 mil-
lion documents, 1.3 millions from newswire and 0.5 millions from the web. We have parsed them with
the Stanford CoreNLP software (Klein and Manning, 2003), obtaining around 650 million dependen-
cies (De Marneffe and Manning, 2008). We selected subject, object, prepositional complements and
adjectival modifiers as the source for syntactic collocations. In order to provide more specific collo-
cations, we implemented the syntactic patterns proposed in (Pe?nas and Hovy, 2010), which produce
so-called propositions. The result is a database with 16 million distinct propositions. Table 1 shows the
six patterns used in this work, together with some examples.
In order to know whether a mention is ambiguous, we built a dictionary based on Wikipedia which
lists, for each string mention, which entities it can refer to. We followed the construction method of
(Spitkovsky and Chang, 2012), which checked article titles, redirects, disambiguation pages and hyper-
links to find mention strings that can be used to refer to entities. Contrary to them, we could not access
hyperlinks in the web, so we could use only those in Wikipedia. According to our dictionary, the am-
biguity of the mentions that we are studying is very high, 26.4 entities on average for the mentions in
AIDA, and 62.6 entities on average for the mentions in TAC2009.
3 One entity per discourse
In order to estimate OSPD we divided the number of times a mention string referred to different entities
in the document with the number of times a mention string occurred multiple times in the document. In
the denominator and numerator we count each mention-document pair once.
Regarding AIDA, we found 12,084 occurrences of mentions which occurred more than once in a
document, making 4,265 unique mention-document pairs
6
(cf. Table 2). In the vast majority of the
cases those mentions refer to a single entity in the document, and only in 170 cases the mentions in the
document refer to several entities. The last row in Table 2 shows the ratio between those values, 96.01%,
showing that OSPD is strong in this dataset.
We also checked OSPD in the TAC2009 dataset. Out of the 138 distinct mention strings used in the
task, we discarded those only linked to NIL (that is, no corresponding Wikipedia article existed) and
those which were not ambiguous (that is, they had only one entity in the dictionary, cf. Section 2). That
leaves 105 mention strings, occurring 1,776 times in 918 different documents, which we annotated by
hand. The 105 strings occurred 1,776 times in 918 documents. Removing the cases where the mention
occurred only once, we were left with 1,173 occurrences, which make 334 unique mention-document
pairs, of which only 6 occurred with more than one sense (rightmost row in Table 2). This yields an
estimate for OSPD of 98.2%.
6
By unique mention-document pairs we mean that we only count once for a mention occurring multiple times in a document.
For instance if mention Smith occurs 10 times in the whole corpus, 8 times in document A and 2 times in document B, we count
two unique mention-document pairs.
2263
AIDA TAC2009
Mention-document pairs 4,265 334
Ambiguous pairs 170 6
OSPD 96.0% 98.2%
Table 2: One entity per discourse: per document statistics in AIDA and TAC2009 datasets. Pairs stand
for the number of unique mention-document pairs. The 4,265 pairs in AIDA correspond to 12,084
occurrences of mentions, and the 334 pairs in TAC2009 correspond to 1,173 occurrences.
All mentions First mention
AIDA TAC2009 AIDA TAC2009
Mention types 3,363 105 2,731 105
Ambiguous types 475 26 454 25
OSPD (collections) 85.9% 75.2% 83.4% 76.2%
Table 3: One entity per collection: statistics in AIDA and TAC2009. In the first two columns (?All
mentions?) we consider all mention types (3, 363 types in AIDA correspond to 23, 726 occurrences of
mentions, and 105 types in TAC2009 correspond to 1, 776 occurrences). In the second two columns
(?First mention?) we leave only the first mention of each document (in this case, there are 2, 731 mention
types in AIDA which correspond to 15, 275 occurrences, and 105 types in TAC2009 corresponding to
941 occurrences).
Finally, we also thought about measuring OSPD on the Wikipedia articles, where many mentions
have been manually linked to their respective article. Unfortunately, we noted that Wikipedia guidelines
explicitly prevent authors linking a mention multiple times: Generally, a link should appear only once
in an article, but if helpful for readers, links may be repeated in infoboxes, tables, image captions,
footnotes, and at the first occurrence after the lead
7
. The fact that Wikipedia editors did not explicitly
state exceptions to the above rule (e.g. for cases where the word or phrase is used to refer to two different
articles, thus breaking the OSPD hypothesis) is remarkable, and might indicate that Wikipedia editors
had not felt the need to challenge the OSPD hypothesis.
3.1 One entity per collection
We took the opportunity to also explore ?one entity per collection?, which gives an idea of what is
the spread of entities for whole document collections. In this case, there is no need to count mention-
document pairs, as there is one single document, the collection, so we estimate the hypothesis according
to mention types. The first two columns in table 3 shows that, overall, mentions which occurred more
than once in the collection tend to refer to the same entity 85.9% of the time in AIDA, and 75.2% of the
time in TAC2009.
As we know that multiple mentions in a document tend to refer to one entity, the second two columns
in table 3 offers the statistics when factoring out multiple occurrences of mention in a document, that is,
leaving the first mention in each document. The statistics are very similar, with minor variations.
We think that the lower estimate for TAC2009 is an artifact of how the TAC KBP organizers set up the
dataset, as they were explicitly looking for cases where the target string would refer to different entities,
making the task more challenging for NED systems. This fact does not affect OSPD for documents, as
those strings still tend to refer to a single entity per document, but given the need to find occurrences
for different entities, the organizers (Ji et al., 2010) did focus on strings occurring with different entities
across the document collection. This is in contrast with AIDA, where they tagged all named-entities
occurring in the target documents. Had the organizers of TAC2009 focused on a random choice of
strings and documents, the one entity per collection would also hold to the high degree exhibited in
AIDA, as the genre of most of the documents is also news (as in AIDA).
7
http://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking#What_generally_
should_be_linked
2264
Syn. coll. Propositions
Mention-collocation pairs 58 61
Ambiguous pairs 5 1
OSPC 91.4% 98.4%
Table 4: One entity per collocation: statistics for syntactic collocations and propositions. The 58
mention-collocation pairs correspond to 262 occurrences, and the 61 mention-proposition pairs to 279.
4 One entity per collocation
In order to estimate OSPC for syntactic collocations, we manually annotated several occurrences of the
138 mention strings of the TAC2009 dataset. Hand-tagging mention entities is a costly process, so we
chose (at random) one syntactic dependency relation for each of the 138 mention strings that occurred
more than five times in the corpus. We then hand-tagged at random five occurrences of each collocation
(cf. Figure 2). This method would provide a maximum of 5 examples for each of the 138 mentions, but
after checking the minimum frequency of the collocations, the quality of the context, repeated sentences,
mentions that are not ambiguous in the dictionary, and whether the mention could be attached to an
entity in the database, the actual number was lower. All in all we found 58 mention-collocation pairs
(262 occurrences) for syntactic collocations (cf. middle column in Table 4). Only 5 mentions referred to
more than one entity per collocation, yielding that OSPC for syntactic collocation is around 91.4%.
To gather the dataset for propositions, we followed the same method as for the syntactic collocations,
that is, we chose (at random) one propositions involving one of the 138 mention strings that occurred
more than five times in the corpus, and hand-tagged at random five occurrences of each proposition
(cf. Figure 2. As with syntactic collocations, we also found a limited number of mentions filling the
desired properties. That left 61 mention-collocation pairs (279 occurrences) for propositions (cf. right
column in Table 4). Only 1 mention referred to more than one entity per proposition, yielding OSPC
for propositions around 98.4%. This shows that the more specific the context is, the stronger is the link
between mention and entity.
5 Improving performance
In order to check whether any of the ?one sense? hypothesis above could improve the performance of
a NED system, we followed a simple procedure: After running the NED system, we take its output
and observe, for each mention string, which is the entity returned most often for a given document (or
collocation), assigning to all occurrences the majority entity. In case of ties, we return the entity with the
highest support from the NED system. We tested the improvements on three NED systems: the freely
available DBpedia Spotlight, a reimplementation of a strong Bayesian NED system and a graph-based
system.
DBpedia Spotlight is a freely available NED system (Daiber et al., 2013), based on a generative proba-
bilistic model (Han and Sun, 2011). Nowadays it is one of the most widely used NED systems and attains
performances close to state-of-the-art (Daiber et al., 2013)We used the default values of the parameters
for all the experiments in this paper.
We also tested an in-house reimplementation of the generative probabilistic model presented in (Han
and Sun, 2011). This is a state-of-the-art system which got the same accuracy as the best participant
(72.0) when evaluated in the non-NIL subset of TAC2013.
UKB is a freely-available system for performing Word Sense Disambiguation and Similarity based
on random walks on graphs (Agirre and Soroa, 2009). Instead of using it on WordNet, we represented
Wikipedia as a graph, where vertices are the wikipedia articles and edges represents bidirectional hy-
perlinks among Wikipedia pages, effectively implementing a NED system. We used a Wikipedia dump
from 2013 in our experiments. UKB is a competitive, state-of-the-art system which attained a score of
69.0 when evaluated in the non-NIL subset of the TAC2013 dataset.
The input of the systems is the context of each mention to be disambiguated, in the form of a 100
token window centered in the target mention. In NED, the identification of the correct mention to be
2265
Mention in context Entity
Abbott Beefs Up Litigation ... Abbot Kinney
Abbott Laboratories Inc., bracing ... Abbott Laboratories
Abbott said it had restated ... Abbott Laboratories
venture between Abbott and Takeda ... Abbott Laboratories
Abbott said in January ... Abbott Laboratories
Figure 3: Applying OSPD: Each of the five occurrences of Abbott in the document in Figure 1 has been
tagged independently by a NED systems, which return the correct entity in all but one case (precision
80%). Applying OSPD would return the correct entity (Abbott Laboratories) in all cases, improving
precision to 100%.
AIDA TAC 2009
Prec. Recall F1 Prec. Recall F1
Spotlight 83.24 63.90 72.30 64.48 46.44 53.99
+ OSPD Discourse 84.17 70.01 76.44 64.65 48.50 55.42
+ OSPD Collection 84.02 74.64 79.05 56.24 47.98 51.78
UKB 70.09 69.03 69.55 67.70 67.64 67.67
+ OSPD Discourse 71.30 70.23 70.76 70.21 70.21 70.21
+ OSPD Collection 75.79 74.64 75.21 68.84 68.84 68.84
(Han and Sun, 2011) 65.71 65.11 65.41 65.49 65.49 65.49
+ OSPD Discourse 67.77 67.37 67.57 66.27 66.27 66.27
+ OSPD Collection 74.29 73.89 74.09 68.24 68.24 68.24
Table 5: Applying OSPD: NED performance on AIDA and TAC2009 OSPD datasets, including each of
the three NED systems, and the results after applying OSPD at the document and collections levels. Bold
marks best result for each system.
disambiguated is part of the problem. AIDA does provide gold mentions, but TAC2009 only provides a
query string which might be just a substring of the real mention in the document. We treated both corpus
in the same way. In the case of DBpedia Spotlight we use the built-in mention spotter. In the case of our
in-house implementations, we use the longest string that matches a valid entity mention in the system, as
given by the dictionary (cf. Section 3).
Some of the NED systems do not return an entity for all mentions, so we evaluate precision, recall and
the harmonic mean (F1 measure). Statistical significance has been estimated using Wilcoxon. We reused
the same corpora as in the previous sections for the evaluation, and also removed all NIL mentions (i.e.
mentions which refer to an entity not in Wikipedia).
5.1 One entity per discourse
We report the improvements using OSPD for both document and collection levels. At the document
level, we relabel mentions that occur multiple times in a document using the entity returned most times
by the NED system in that document. Figure 3 illustrates the idea for a NED system on the same sample
document as in Figure 1. At the collection level, we relabel mentions using the entity returned most
times by the NED systems in the whole collection.
Table 5 reports the results of the performance as evaluated on mentions occurring multiple times in
the AIDA and TAC2009 datasets. The numbers in the left part of the table correspond to the perfor-
mance as evaluated on mentions occurring multiple times in AIDA documents. Note that the number of
occurrences where OSPD at the collection level can be applied is larger (a superset of those for OSPD
at the document level), as, for instance, a mention string occurring once in three different documents
won?t be affected by OSPD at the document level, but it could be relabeled at the collection level. We
were especially interested in making the numbers between OSPD at the document and collection levels
2266
CPI subject-of rise Angela Merkel has CDU:
Consumer price index Christian Democratic Union (Germany)
Consumer price index Catholic Distance University
Communist Party of India Christian Democratic Union (Germany)
Communist Party of India Christian Democratic Union (Germany)
Consumer price index Christian Democratic Union (Germany)
Figure 4: Applying OSPC: A NED system system tagged each example in Figure 2 independently. For
CPI, the precision is 60%, but after relabeling with OSPC it would be 100%. For CDU, the improvement
is from 80% to 100%.
Syntactic collocations Propositions
prec. recall F1 prec. recall F1
Spotlight 82.46 66.41 73.57 74.67 60.22 66.67
+ OSPC 82.63 67.18 74.11 74.79 62.72 68.23
UKB 75.86 75.57 75.72 67.87 67.38 67.63
+ OSPC 78.54 78.24 78.39 68.59 68.10 68.35
(Han and Sun, 2011) 75.57 75.57 75.57 71.33 71.33 71.33
+ OSPC 78.24 78.24 78.24 73.12 73.12 73.12
Table 6: Applying OSPC: NED performance on TAC2009, including each of the three NED systems,
and the results after applying OSPC for syntactic collocations and propositions. Bold is used for best
results for each system.
directly comparable, and therefore report the results on the same occurrences, that is, the occurrences
where OSPD at the document level can be applied.
The results show a small but consistent improvement for OSPD at the document level in precision,
recall and F1 for the three NED systems, around 1 or 2 absolute points. The improvements when applying
OSPD at the collection level are also consistent, but remarkably larger, between 5 and 9 absolute points.
All improvements are statistically significant (p-value below 0.01).
Table 5 also reports the results after applying OSPD to TAC2009 instances which occurred more
than once in a document. Results for OSPD at document level and collection level follow the same
methodology as for AIDA. The improvement at the collection level is not so consistent, with a loss in
performance for Spotlight, a small improvement for UKB, and a larger improvement for (Han and Sun,
2011). All differences across the table are statistically significant (p-value below 0.01).
While the OSPD at the document level is strong in both corpora, Section 3.1 showed that the OSPD
at the collection level is only strong in AIDA, with a much lower estimate in TAC2009. This fact
would explain why the improvement with OSPD at the collection level is not consistent. Following
the rationale in Section 3.1, we think that had the organizers of the task chosen strings and documents
at random, the improvement in TAC 2009 at the collection level would be also as high as in AIDA. The
high improvement in AIDA at the collection level compared to the more modest improvement at the
document level, despite having a lower OSPD estimate (cf. Section 3.1), could be caused by the fact that
there are more occurrences and evidence in favor of the majority entity.
5.2 One entity per collocation
Figure 4 shows the application of OSPC to the output of a NED system to two sample collocations in our
dataset. In this case, the application of OSPC would increase precision to 100%. The actual result on the
datasets produced in Section 4 for syntactic collocations and propositions is reported on table 6.
Regarding syntactic collocations, table 6 shows that the improvement is small but consistent for the
three systems on precision, recall and F1, ranging from 0.5 to 2.5 absolute points in F1 score. The results
for propositions also show the same trend, with consistent improvements across the table. All differences
2267
in the two tables are statistically significant (p-value < 0.01), except for UKB.
6 Conclusions and future work
Our study shows that OSPD holds for 96%-98% (in the AIDA and TAC2009 datasets, respectively)
of the mentions that occur multiple times in documents. We also measured OSPD at the collection
level (86% and 75%, respectively). OSPC holds for 91% of the mentions that occur multiple times in
the syntactic collocations that we studied, and 98% of the mentions that occur multiple times in more
specific collocations. We reused the publicly available AIDA dataset for estimating OSPD. In addition,
we created a dataset to study OSPC based on the TAC KBP Entity Linking 2009 task dataset, which is
publicly available
8
.
We carefully chose to estimate both OPSD and OSPC on TAC2009, in order to make the numbers
between OSPD and OSPC comparable. The OSPD numbers for AIDA are very similar to those obtained
on TAC2009, providing complementary evidence. Although the high estimate of OSPD for entities was
somehow expected, the high estimate of OSPC for the syntactic collocations, especially the propositions,
was somehow unexpected, given the high ambiguity rate of the discussed strings, and the fact that the
ambiguity included similar entities, like for instance ?ABC? which can refer, among other 190 entities,
to the American Broadcasting Company or the Australian Broadcasting Corporation.
Our results also show that a simple application of the OSPD and OSPC hypotheses to the output of
three different NED systems improves the results in all cases. Remarkably, the highest performance gain,
8 absolute points, was for OSPD at the collection level in the AIDA corpus.
The results presented here could be largely dependent on the domain and genre of the documents,
as well as the definition of collocation. Our work is a strong basis for claiming that OSPD and OSPC
hold for entities, but the evidence could be further extended exploring alternative operationalization of
collocations and a larger breadth of genres and domains.
For the future we would like to check whether these hypotheses can be further used to improve current
NED systems. The OSPD hypothesis can be used to jointly disambiguate all occurrences of a mention
in a document. The OSPC hypothesis could be used to acquire important disambiguation features, or to
perform large-scale joint entity linking. The OSPD for whole collections could be useful for documents
on specific domains, and for domain adaptation scenarios.
Acknowledgements
This work was partially funded by MINECO (CHIST-ERA READERS project ? PCIN-2013-002- C02-
01) and the European Commission (QTLEAP ? FP7-ICT-2013.4.1-610516, OPENER ? FP7-ICT-2011-
SME-DCL-296451). Ander Barrena is supported by a PhD grant from the University of the Basque
Country.
References
Eneko Agirre and Philip Edmonds. 2007. Word Sense Disambiguation: Algorithms and Applications. Springer
Publishing Company, Incorporated, 1st edition.
Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings
of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ?09,
pages 33?41.
Marine Carpuat. 2009. One translation per discourse. In Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, DEW ?09, pages 19?27, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo N. Mendes. 2013. Improving efficiency and accuracy
in multilingual entity extraction. In Proceedings of the 9th International Conference on Semantic Systems,
I-SEMANTICS ?13, pages 121?124, New York, NY, USA. ACM.
8
http://ixa2.si.ehu.es/OEPDC
2268
Marie-Catherine De Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual. URL
http://nlp. stanford. edu/software/dependencies manual. pdf.
William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of
the workshop on Speech and Natural Language, HLT ?91, page 233237, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2012. Evaluating Entity
Linking with Wikipedia. Artif. Intell., 194:130?150, January.
Xianpei Han and Le Sun. 2011. A Generative Entity-mention Model for Linking Entities with Knowledge Base.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, volume 1, pages 945?954.
Johannes Hoffart, Mohamed A. Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, Bilyana
Taneva, Stephan Thater, and Gerdhard Weikum. 2011. Robust Disambiguation of Named Entities in Text.
In Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, United Kingdom
2011, pages 782?792.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Griffitt, and Joe Ellis. 2010. Overview of the tac 2010 knowledge
base population track. In Third Text Analysis Conference (TAC 2010).
Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
Robert Krovetz. 1998. More than one sense per discourse. In NEC Princeton NJ Labs., Research Memorandum.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics,
39(4).
David Martinez and Eneko Agirre. 2000. One sense per collocation and genre/topic variations. In Proceedings
of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large
corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics -
Volume 13, EMNLP ?00, page 207215, Stroudsburg, PA, USA. Association for Computational Linguistics.
Anselmo Pe?nas and Eduard Hovy. 2010. Filling knowledge gaps in text for machine reading. In Proceedings
of the 23rd International Conference on Computational Linguistics: Posters, pages 979?987. Association for
Computational Linguistics.
Valentin I. Spitkovsky and Angel X. Chang. 2012. A Cross-lingual Dictionary for English Wikipedia Concepts.
Eighth International Conference on Language Resources and Evaluation (LREC 2012).
David Yarowsky. 1993. One sense per collocation. In Proceedings of the workshop on Human Language Tech-
nology, HLT ?93, page 266271, Stroudsburg, PA, USA. Association for Computational Linguistics.
2269
Random Walks for Knowledge-Based
Word Sense Disambiguation
Eneko Agirre?
IXA NLP group
University of the Basque Country
Oier Lo?pez de Lacalle??
University of Edinburgh
IKERBASQUE
Basque Foundation for Science
Aitor Soroa?
IXA NLP group
University of the Basque Country
Word Sense Disambiguation (WSD) systems automatically choose the intended meaning of a
word in context. In this article we present a WSD algorithm based on random walks over large
Lexical Knowledge Bases (LKB). We show that our algorithm performs better than other graph-
based methods when run on a graph built from WordNet and eXtended WordNet. Our algorithm
and LKB combination compares favorably to other knowledge-based approaches in the literature
that use similar knowledge on a variety of English data sets and a data set on Spanish. We include
a detailed analysis of the factors that affect the algorithm. The algorithm and the LKBs used are
publicly available, and the results easily reproducible.
1. Introduction
Word Sense Disambiguation (WSD) is a key enabling technology that automatically
chooses the intended sense of a word in context. It has been the focus of intensive
research since the beginning of Natural Language Processing (NLP), and more recently
it has been shown to be useful in several tasks such as parsing (Agirre, Baldwin, and
Martinez 2008; Agirre et al. 2011), machine translation (Carpuat and Wu 2007; Chan,
Ng, and Chiang 2007), information retrieval (Pe?rez-Agu?era and Zaragoza 2008; Zhong
and Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and
? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: e.agirre@ehu.es.
?? IKERBASQUE, Basque Foundation for Science, 48011, Bilbao, Basque Country.
E-mail: oier.lopezdelacalle@gmail.com.
? Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: a.soroa@ehu.es.
Submission received: 30 July 2011; Revised submission received: 15 November 2012; Accepted for publication:
17 February 2013.
doi:10.1162/COLI a 00164
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
summarization (Barzilay and Elhadad 1997). WSD is considered to be a key step in
order to approach language understanding beyond keyword matching.
The best performing WSD systems are currently those based on supervised learn-
ing, as attested in public evaluation exercises (Snyder and Palmer 2004; Pradhan et al.
2007), but they need large amounts of hand-tagged data, which is typically very ex-
pensive to produce. Contrary to lexical-sample exercises (where plenty of training and
testing examples for a handful of words are provided), all-words exercises (which
comprise all words occurring in a running text, and where training data is more scarce)
show that only a few systems beat the most frequent sense (MFS) heuristic, with small
differences. For instance, the best system in SensEval-3 scored 65.2 F1, compared to 62.4
(Snyder and Palmer 2004). The best current state-of-the-art WSD system (Zhong and Ng
2010), outperforms the MFS heuristic by 5% to 8% in absolute F1 scores on the SensEval
and SemEval fine-grained English all words tasks.
The causes of the small improvement over the MFS heuristic can be found in the
relatively small amount of training data available (sparseness) and the problems that
arise when the supervised systems are applied to different corpora from that used to
train the system (corpus mismatch) (Ng 1997; Escudero, Ma?rquez, and Rigau 2000).
Note that most of the supervised systems for English are trained over SemCor (Miller
et al. 1993), a half-a-million word subset of the Brown Corpus made available from the
WordNet team, and DSO (Ng and Lee 1996), comprising 192,800 word occurrences from
the Brown and WSJ corpora corresponding to the 191 most frequent nouns and verbs.
Several researchers have explored solutions to sparseness. For instance, Chan and Ng
(2005) present an unsupervised method to obtain training examples from bilingual data,
which was used together with SemCor and DSO to train one of the best performing
supervised systems to date (Zhong and Ng 2010).
In view of the problems of supervised systems, knowledge-based WSD is emerging
as a powerful alternative. Knowledge-based WSD systems exploit the information in
a lexical knowledge base (LKB) to perform WSD. They currently perform below su-
pervised systems on general domain data, but are attaining performance close or above
MFS without access to hand-tagged data (Ponzetto and Navigli 2010). In this sense, they
provide a complementary strand of research which could be combined with supervised
methods, as shown for instance in Navigli (2008). In addition, Agirre, Lo?pez de Lacalle,
and Soroa (2009) show that knowledge-based WSD systems can outperform supervised
systems in a domain-specific data set, where MFS from general domains also fails. In
this article, we will focus our attention on knowledge-based methods.
Early work for knowledge-based WSD was based on measures of similarity
between pairs of concepts. In order to maximize pairwise similarity for a sequence of
n words where each has up to k senses, the algorithms had to consider up to kn sense
sequences. Greedy methods were often used to avoid the combinatorial explosion
(Patwardhan, Banerjee, and Pedersen 2003). As an alternative, graph-based methods
are able to exploit the structural properties of the graph underlying a particular LKB.
These methods are able to consider all possible combinations of occurring senses on a
particular context, and thus offer a way to analyze efficiently the inter-relations among
them, gaining much attention in the NLP community (Mihalcea 2005; Navigli and
Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata 2010).
The nodes in the graph represent the concepts (word senses) in the LKB, and edges
in the graph represent relations between them, such as subclass and part-of. Network
analysis techniques based on random walks like PageRank (Brin and Page 1998) can
then be used to choose the senses that are most relevant in the graph, and thus output
those senses.
58
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
In order to deal with large knowledge bases containing more than 100,000 con-
cepts (Fellbaum 1998), previous algorithms had to extract subsets of the LKB (Navigli
and Lapata 2007, 2010) or construct ad hoc graphs for each context to be dis-
ambiguated (Mihalcea 2005; Sinha and Mihalcea 2007). An additional reason for the
use of custom-built subsets of ad hoc graphs for each context is that if we were using
a centrality algorithm like PageRank over the whole graph, it would choose the most
important senses in the LKB regardless of context, limiting the applicability of the
algorithm. For instance, the word coach is ambiguous at least between the ?sports
coach? and the ?transport service? meanings, as shown in the following examples:
(1) Nadal is sharing a house with his uncle and coach, Toni, and his physical trainer,
Rafael Maymo.
(2) Our fleet comprises coaches from 35 to 58 seats.
If we were to run a centrality algorithm over the whole LKB, with no context, then
we would always assign coach to the same concept, and we would thus fail to correctly
disambiguate either one of the given examples.
The contributions of this article are the following: (1) A WSD method based on ran-
dom walks over large LKBs. The algorithm outperforms other graph-based algorithms
when using a LKB built from WordNet and eXtended WordNet. The algorithm and
LKB combination compares favorably to the state-of-the-art in knowledge-based WSD
on a wide variety of data sets, including four English and one Spanish data set. (2) A
detailed analysis of the factors that affect the algorithm. (3) The algorithm together with
the corresponding graphs are publicly available1 and can be applied easily to sense
inventories and knowledge bases different from WordNet.
The algorithm for WSD was first presented in Agirre and Soroa (2009). In this article,
we present further evaluation on two more recent data sets, analyze the parameters and
options of the system, compare it to the state of the art, and discuss the relation of our
algorithm with PageRank and the MFS heuristic.
2. Related Work
Traditional knowledge-based WSD systems assign a sense to an ambiguous word by
comparing each of its senses with those of the surrounding context. Typically, some se-
mantic similarity metric is used for calculating the relatedness among senses (Lesk 1986;
Patwardhan, Banerjee, and Pedersen 2003). The metric varies between counting word
overlaps between definitions of the words (Lesk 1986) to finding distances between
concepts following the structure of the LKB (Patwardhan, Banerjee, and Pedersen 2003).
Usually the distances are calculated using only hierarchical relations on the LKB (Sussna
1993; Agirre and Rigau 1996). Combining both intuitions, Jiang and Conrath (1997)
present a metric that combines statistics from corpus and a lexical taxonomy structure.
One of the major drawbacks of these approaches stems from the fact that senses are
compared in a pairwise fashion and thus the number of computations grows exponen-
tially with the number of words?that is, for a sequence of n words where each has
up to k senses they need to consider up to kn sense sequences. Although alternatives
like simulated annealing (Cowie, Guthrie, and Guthrie 1992) and conceptual density
1 http://ixa2.si.ehu.es/ukb.
59
Computational Linguistics Volume 40, Number 1
(Agirre and Rigau 1996) were tried, most of the knowledge-based WSD at the time was
done in a suboptimal word-by-word greedy process, namely, disambiguating words
one at a time (Patwardhan, Banerjee, and Pedersen 2003). Still, some recent work on
finding predominant senses in domains has applied such similarity-based techniques
with success (McCarthy et al. 2007).
Recently, graph-based methods for knowledge-based WSD have gained much at-
tention in the NLP community (Mihalcea 2005; Navigli and Velardi 2005; Navigli and
Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata
2010). These methods use well-known graph-based techniques to find and exploit the
structural properties of the graph underlying a particular LKB. Graph-based techniques
consider all the sense combinations of the words occurring on a particular context at
once, and thus offer a way to analyze the relations among them with respect to the
whole graph. They are particularly suited for disambiguating words in the sequence,
and they manage to exploit the interrelations among the senses in the given context.
In this sense, they provide a principled solution to the exponential explosion problem
mentioned before, with excellent performance.
Graph-based WSD is performed over a graph composed of senses (nodes) and
relations between pairs of senses (edges). The relations may be of several types (lexico-
semantic, cooccurrence relations, etc.) and may have some weight attached to them. All
the methods reviewed in this section use some version of WordNet as a LKB. Apart
from relations in WordNet, some authors have used semi-automatic and fully auto-
matic methods to enrich WordNet with additional relations. Mihalcea and Moldovan
(2001) disambiguated WordNet glosses in a resource called eXtended WordNet. The
disambiguated glosses have been shown to improve results of a graph-based system
(Agirre and Soroa 2008), and we have also used them in our experiments. Navigli and
Velardi (2005) enriched WordNet with cooccurrence relations semi-automatically and
showed that those relations are effective in a number of graph-based WSD systems
(Navigli and Velardi 2005; Navigli and Lapata 2007, 2010). More recently, Cuadros and
Rigau (2006, 2007, 2008) learned automatically so-called KnowNets, and showed that
the new provided relations improved WSD performance when plugged into a simple
vector-based WSD system. Finally, Ponzetto and Navigli (2010) have acquired relations
automatically from Wikipedia, released as WordNet++, and have shown that they are
beneficial in a graph-based WSD algorithm. All of these relations are publicly available
with the exception of Navigli and Velardi (2005), but note that the system is available
on-line.2
Disambiguation is typically performed by applying a ranking algorithm over the
graph, and then assigning the concepts with highest rank to the corresponding words.
Given the computational cost of using large graphs like WordNet, most researchers use
smaller subgraphs built on-line for each target context. The main idea of the subgraph
method is to extract the subgraph whose vertices and relations are particularly relevant
for the set of senses from a given input context. The subgraph is then analyzed and the
most relevant vertices are chosen as the correct senses of the words.
The TextRank algorithm for WSD (Mihalcea 2005) creates a complete weighted
graph (e.g., a graph in which every pair of distinct vertices is connected by a weighted
edge) formed by the synsets of the words in the input context. The weight of the links
joining two synsets is calculated by executing Lesk?s algorithm (Lesk 1986) between
them?that is, by calculating the overlap between the words in the glosses of the
2 http://lcl.uniroma1.it/ssi.
60
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
corresponding senses. Once the complete graph is built, a random walk algorithm
(PageRank) is executed over it and words are assigned to the most relevant synset.
In this sense, PageRank is used as an alternative to simulated annealing to find the
optimal pairwise combinations. This work is extended in Sinha and Mihalcea (2007),
using a collection of semantic similarity measures when assigning a weight to the links
across synsets. They also compare different graph-based centrality algorithms to rank
the vertices of the complete graph. They use different similarity metrics for different
POS types and a voting scheme among the centrality algorithm ranks.
In Navigli and Velardi (2005), the authors develop a knowledge-based WSD method
based on lexical chains called structural semantic interconnections (SSI). Although the
system was first designed to find the meaning of the words in WordNet glosses, the
authors also apply the method for labeling each word in a text sequence. Given a
text sequence, SSI first identifies monosemous words and assigns the corresponding
synset to them. Then, it iteratively disambiguates the rest of the terms by selecting
the senses that get the strongest interconnection with the synsets selected so far. The
interconnection is calculated by searching for paths on the LKB, constrained by some
hand-made rules of possible semantic patterns.
In Navigli and Lapata (2007, 2010), the authors perform a two-stage process for
WSD. Given an input context, the method first explores the whole LKB in order to find
a subgraph that is particularly relevant for the words of the context. The subgraph is
calculated by applying a depth-first search algorithm over the LKB graph for every
word sense occurring in a context. Then, they study different graph-based centrality
algorithms for deciding the relevance of the nodes on the subgraph. As a result, every
word of the context is attached to the highest ranking concept among its possible senses.
The best results were obtained by a simple algorithm like choosing the concept for each
word with the largest degree (number of edges) and by PageRank (Brin and Page 1998).
We reimplemented their best methods in order to compare our algorithm with theirs on
the same setting (cf. Section 6.3). In later work (Ponzetto and Navigli 2010) the authors
apply a subset of their methods to an enriched WordNet with additional relations from
Wikipedia, improving their results for nouns.
Tsatsaronis, Vazirgiannis, and Androutsopoulos (2007) and Agirre and Soroa (2008)
also use such a two-stage process. They build the graph as before, but using breadth-
first search. The first authors apply a spreading activation algorithm over the subgraph
for node ranking, while the second use PageRank. In later work (Tsatsaronis, Varlamis,
and N?rva?g 2010) spreading activation is compared with PageRank and other centrality
measures like HITS (Kleinberg 1998), obtaining better results than in their previous
work.
This work departs from earlier work in its use of the full graph, and its ability to
infuse context information when computing the importance of nodes in the graph. For
this, we resort to an extension of the PageRank algorithm (Brin and Page 1998), called
Personalized PageRank (Haveliwala 2002), which tries to bias PageRank using a set of
representative topics and thus capture more accurately the notion of importance with
respect to a particular topic. In our case, we initialize the random walk with the words
in the context of the target word, and thus we obtain a context-dependent PageRank.
We will show that this method is indeed effective for WSD. Note that in order to use
other centrality algorithms (e.g., HITS [Kleinberg 1998]), previous authors had to build
a subgraph first. In principle, those algorithms could be made context-dependent when
using the full graph and altering their formulae, but we are not aware of such variations.
Random walks over WordNet using Personalized PageRank have been also used
to measure semantic similarity between two words (Hughes and Ramage 2007; Agirre
61
Computational Linguistics Volume 40, Number 1
et al. 2009). In those papers, the random walks are initialized with a single word,
whereas we use all content words in the context. The results obtained by the authors,
especially in the latter paper, are well above other WordNet-based methods.
Most previous work on knowledge-based WSD has presented results on one or two
general domain corpora for English. We present our results on four general domain
data sets for English and a Spanish data set (Ma`rquez et al. 2007). Alternatively, some
researchers have applied knowledge-based WSD to specific domains, using different
methods to adapt the method to the particular test domain. In Agirre, Lo?pez de Lacalle,
and Soroa (2009) and Navigli et al. (2011), the authors apply our Personalized PageRank
method to a domain-specific corpus with good results. Ponzetto and Navigli (2010) also
apply graph-based algorithms to the same domain-specific corpus.
3. WordNet
Most WSD work uses WordNet as the sense inventory of choice. WordNet (Fellbaum
1998) is a freely available3 lexical database of English, which groups nouns, verbs,
adjectives, and adverbs into sets of synonyms, each expressing a distinct concept (called
synset in WordNet parlance). For instance, coach has five nominal senses and two verbal
senses, which correspond to the following synsets:
<coach#n1, manager#n2, handler#n3>
<coach#n2, private instructor#n1, tutor#n1>
<coach#n3, passenger car#n1, carriage#n1>
<coach#n4, four-in-hand#n2, coach-and-four#n1>
<coach#n5, bus#n1, autobus#n1, charabanc#n1,double-decker#n1,jitney#n1 . . .>
<coach#v1, train#v7>
<coach#v2>
In these synsets coach#n1 corresponds to the first nominal sense of coach, coach#v1 corre-
sponds to the first verbal sense, and so on. Each of the senses of coach corresponds to a
different synset, and each synset contains several words with different sense numbers.
For instance, the first nominal sense of coach has two synonyms: manager in its second
sense and handler in its third sense. As a synset can be identified by any of its words
in a particular sense number, we will use a word and sense number to represent the
full concept. Each synset has a descriptive gloss (e.g., a carriage pulled by four horses with
one driver for coach#n4, or drive a coach for coach#v2). The examples correspond to the
current version of WordNet (3.1), but the sense differences have varied across different
versions. There exist automatic mappings across versions (Daude, Padro, and Rigau
2000), but they contain small errors. In this article we will focus on WordNet versions
1.7 and 2.1, which have been used to tag the evaluation data sets used in this article
(cf. Section 6).
The synsets in WordNet are interlinked with conceptual-semantic and lexical rela-
tions. Examples of conceptual-semantic relations are hypernymy, which corresponds to
the superclass or is-a relation, and holonymy, the part-of relation. Figure 1 shows two
small regions of the graph around three synsets of the word coach, including several
conceptual-semantic relations and lexical relations. For example, the figure shows that
concept trainer#n1 is a coach#n1 (hypernymy relation), and that seat#n1 is a part of
coach#n5 (holonymy relation). The figure only shows a small subset of the relations
3 http://wordnet.princeton.edu.
62
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
Figure 1
Example showing three senses of coach, with links to related concepts.
for three synsets of coach. If we were to show the relations of the rest of the synsets
in WordNet we would end up with a densely connected graph, where one can go from
one synset to another following the semantic relations. In addition to purely conceptual-
semantic relations which hold between synsets, there are also lexical relations which
hold between specific senses. For instance, angry#a2 is the antonym of calm#a2 and a
derivation relation exists between handler#n3 and handle#v6, meaning that handler is a
derived form of handle and that the third nominal sense of handler is related to the sixth
verbal sense of handle. Although lexical relations hold only between two senses, we gen-
eralize to the whole synset. This generalization captures the notion that if handler#n3 is
related by derivation to handle#v6, then coach#n1 is also semantically related to handle#v6
(as shown in Figure 1).
In addition to these relations, we also use the relation between each synset and
the words in the glosses. Most of the words in the glosses have been manually asso-
ciated with their corresponding senses, and we can thus produce a link between the
synset being glossed, and the synsets of each of the words in the gloss. For instance,
following one of the given glosses, a gloss relation would be added between coach#v2
and drive#v2. The gloss relations were not available prior to WordNet 3.0, and we
thus used automatically disambiguated glosses for WordNet 1.7 and WordNet 2.1, as
made freely available in the eXtended WordNet (Mihalcea and Moldovan 2001). Note
also that the eXtended WordNet provided about 550,000 relations, whereas the disam-
biguated glosses made available with WordNet 3.0 provide around 339,000 relations.
We compare the performance of XWN relations and WordNet 3.0 gloss relations in
Section 6.4.4.
Table 1 summarizes the most relevant relations (with less frequent relations
grouped as ?other?). The table also lists how we grouped the relations, and the overall
counts. Note that inverse relations are not counted, as their numbers equal those of the
original relation. In Section 6.4.5 we report the impact of the relations in the behavior
of the system. Overall, the graph for WordNet 1.7 has 109, 359 vertices (concepts) and
620, 396 edges (relations between concepts). Note that there is some overlap between
XWN and other types of relations. For instance, the hypernym of coach#n4 is carriage#n2,
which is also present in its gloss. Note that most of the relation types relate concepts
from the same part of speech, with the exception of derivation and XWN.
63
Computational Linguistics Volume 40, Number 1
Table 1
Relations and their inverses in WordNet 1.7, how we grouped them, and overall counts. XWN
refers to relations from the disambiguated glosses in eXtended WordNet.
relation inverse group counts
hypernymy hyponymy TAX 89,078
derivation derivation REL 28,866
holonymy meronymy MER 21,260
antonymy antonymy ANT 7,558
other other REL 3,134
xwn xwn?1 XWN 551,551
Finally, we have also used the Spanish WordNet (Atserias, Rigau, and Villarejo
2004). In addition to the native relations, we also added relations from the eXtended
WordNet. All in all, it contains 105, 501 vertices and 623, 316 relations.
3.1 Representing WordNet as a Graph
An LKB such as WordNet can be seen as a set of concepts and relations among them,
plus a dictionary, which contains the list of words (typically word lemmas) linked to
the corresponding concepts (senses). WordNet can be thus represented as a graph G =
(V, E). V is the set of nodes, where each node represents one concept (vi ? V), and E
is the set of edges. Each relation between concepts vi and vj is represented by an edge
ei,j ? E. We ignore the relation type of the edges. If two WordNet relations exist between
two nodes, we only represent one edge, and ignore the type of the relation. We chose to
use undirected relations between concepts, because most of the relations are symmetric
and have their inverse counterpart (cf. Section 3), and in preliminary work we failed to
see any effect using directed relations.
In addition, we also add vertices for the dictionary words, which are linked to
their corresponding concepts by directed edges (cf. Figure 1). Note that monosemous
words will be related to just one concept, whereas polysemous words may be attached
to several. Section 5.2 explains the reason for using directed edges, and also mentions
an alternative to avoid introducing these vertices.
4. PageRank and Personalized PageRank
The PageRank random walk algorithm (Brin and Page 1998) is a method for ranking
the vertices in a graph according to their relative structural importance. The main idea
of PageRank is that whenever a link from vi to vj exists in a graph, a vote from node i
to node j is produced, and hence the rank of node j increases. In addition, the strength
of the vote from i to j also depends on the rank of node i: The more important node
i is, the more strength its votes will have. Alternatively, PageRank can also be viewed
as the result of a random walk process, where the final rank of node i represents the
probability of a random walk over the graph ending on node i, at a sufficiently large
time.
Let G be a graph with N vertices v1, . . . , vN and di be the outdegree of node i; let M
be a N ? N transition probability matrix, where Mji = 1di if a link from i to j exists, and
64
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
zero otherwise. Then, the calculation of the PageRank Vector P over G is equivalent to
resolving Equation (1).
P = cMP+ (1 ? c)v (1)
In the equation, v is a N ? 1 stochastic vector and c is the so-called damping
factor, a scalar value between 0 and 1. The first term of the sum on the equation
models the voting scheme described in the beginning of the section. The second term
represents, loosely speaking, the probability of a surfer randomly jumping to any node
(e.g., without following any paths on the graph). The damping factor, usually set in
the [0.85..0.95] range, models the way in which these two terms are combined at
each step.
The second term in Equation (1) can also be seen as a smoothing factor that makes
any graph fulfill the property of being aperiodic and irreducible, and thus guarantees
that the PageRank calculation converges to a unique stationary distribution.
In the traditional PageRank formulation the vector v is a stochastic normalized
vector whose element values are all 1N , thus assigning equal probabilities to all nodes
in the graph in the case of random jumps. However, as pointed out by Haveliwala
(2002), the vector v can be non-uniform and assign stronger probabilities to certain
kinds of nodes, effectively biasing the resulting PageRank vector to prefer these nodes.
For example, if we concentrate all the probability mass on a unique node i, all random
jumps on the walk will return to i and thus its rank will be high; moreover, the high rank
of i will make all the nodes in its vicinity also receive a high rank. Thus, the importance
of node i given by the initial distribution of v spreads along the graph on successive
iterations of the algorithm. As a consequence, the P vector can be seen as representing
the relevance of every node in the graph from the perspective of node i.
In this article, we will use Static PageRank to refer to the case when a uniform
v vector is used in Equation (1); and whenever a modified v is used, we will call it
Personalized PageRank. The next section shows how we define a modified v.
PageRank is actually calculated by applying an iterative algorithm that computes
Equation (1) successively until convergence below a given threshold is achieved, or
until a fixed number of iterations are executed. Following usual practice, we used a
damping value of 0.85 and finish the calculations after 30 iterations (Haveliwala 2002;
Langville and Meyer 2003; Mihalcea 2005). Some preliminary experiments with higher
iteration counts showed that although sometimes the node ranks varied, the relative
order among particular word synsets remained stable after the initial iterations (cf.
Section 6.4 for further details). Note that, in order to discard the effect of dangling
nodes (i.e., nodes without outlinks) one would need to slightly modify Equation (1)
following Langville and Meyer (2003).4 This modification is not necessary for WordNet,
as it does not have dangling nodes.
5. Random Walks for WSD
We tested two different methods to apply random walks to WSD.
4 The equation becomes P = cMP + (ca + (1 ? c)e)v, where ai = 1 if node i is a dangling node,
and 0 otherwise, and e is a vector of all ones.
65
Computational Linguistics Volume 40, Number 1
5.1 Static PageRank, No Context
If we apply traditional PageRank over the whole WordNet, we get a context-
independent ranking of word senses. All concepts in WordNet get ranked according
to their PageRank value. Given a target word, it suffices to check which is the relative
ranking of its senses, and the WSD system would output the one ranking highest. We
call this application of PageRank to WSD Static PageRank STATIC for short, as it does
not change with the context, and we use it as a baseline.
As the PageRank measure over undirected graphs for a node is closely related
to the degree of the node, the Static PageRank returns the most predominant sense
according to the number of relations the senses have. We think that this is closely related
to the Most Frequent Sense attested in general corpora, as the lexicon builders would
tend to assign more relations to the most predominant sense. In fact, our results (cf.
Section 6.4.5) show that this is indeed the case for the English WordNet.
5.2 Personalized PageRank, Using Context
Static PageRank is independent of context, but this is not what we want in a WSD
system. Given an input piece of text we want to disambiguate all content words in
the input according to the relationships among them. For this we can use Personalized
PageRank (PPR for short) over the whole WordNet graph.
Given an input text (e.g., a sentence), we extract the list Wi i = 1 . . .m of content
words (i.e., nouns, verbs, adjectives, and adverbs) that have an entry in the dictionary,
and thus can be related to LKB concepts. As a result of the disambiguation process,
every LKB concept receives a score. Then, for each target word to be disambiguated, we
just choose its associated concept in G with maximum score.
In order to apply Personalized PageRank over the LKB graph, the context words
are first inserted into the graph G as nodes, and linked with directed edges to their
respective concepts. Then, the Personalized PageRank of the graph G is computed by
concentrating the initial probability mass uniformly over the newly introduced word
nodes. As the words are linked to the concepts by directed edges, they act as source
nodes injecting mass into the concepts they are associated with, which thus become
relevant nodes, and spread their mass over the LKB graph. Therefore, the resulting
Personalized PageRank vector can be seen as a measure of the structural relevance of
LKB concepts in the presence of the input context.
Making the edges from words to concepts directed is important, as the use of
undirected edges will move part of the probability mass in the concepts to the word
nodes. Note the contrast with the edges representing relations between concepts, which
are undirected (cf. Section 3.1).
Alternatively, we could do without the word nodes, concentrating the initial prob-
ability mass on the senses of the words under consideration. Such an initialization over
the graph with undirected edges between synset nodes is equivalent to initializing
the walk on the words in a graph with undirected edges between synset nodes and
directed nodes from words to synsets. We experimentally checked that the results
of both alternatives are indistinguishable. Although the alternative without nodes is
marginally more efficient, we keep the word nodes as they provide a more intuitive and
appealing formalization.
One problem with Personalized PageRank is that if one of the target words has
two senses that are related by semantic relations, those senses reinforce each other, and
could thus dampen the effect of the other senses in the context. Although one could
66
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
remove direct edges between competing senses from the graph, it is quite rare that those
senses are directly linked, and usually a path with several edges is involved. With this
observation in mind we devised a variant called word-to-word heuristic (PPRw2w for
short), where we run Personalized PageRank separately for each target word in the
context, that is, for each target word Wi, we concentrate the initial probability mass
in the senses of the rest of the words in the context of Wi, but not in the senses of
the target word itself, so that context words increase their relative importance in the
graph. The main idea of this approach is to avoid biasing the initial score of concepts
associated with target word Wi, and let the surrounding words decide which concept
associated with Wi has more relevance. Contrary to the previous approach, PPRw2w does
not disambiguate all target words of the context in a single run, which makes it less
efficient (cf. Section 6.4).
Figure 2 illustrates the disambiguation of a sample sentence. The STATIC method
(not shown in the figure) would choose the synset coach#n1 for the word coach because
it is related to more concepts than other senses, and because those senses are related
to concepts that have a high degree (for instance, sport#1). The PPR method (left side
of Figure 2) concentrates the initial mass on the content words in the example. After
running the iterative algorithm, the system would return coach#n1 as the result for the
target word coach. Although the words in the sentence clearly indicate that the correct
synset in this sentence corresponds to coach#n5, the fact that teacher#n1 is related to
trainer#n1 in WordNet causes both coach#n2 and coach#n1 to reinforce each other, and
make their pagerank higher. The right side of Figure 2 depicts the PPRw2w method,
where the word coach is not activated. Thus, there is no reinforcement between coach
senses, and the method would correctly choose coach#n5 as the proper synset.
6. Evaluation
WSD literature has used several measures for evaluation. Precision is the percentage of
correctly disambiguated instances divided by the number of instances disambiguated.
Some systems don?t disambiguate all instances, and thus the precision can be high
even if the system disambiguates a handful of instances. In our case, when a word has
Figure 2
Portion of WordNet to illustrate the disambiguation of coach in the sentence Our fleet comprises
coaches from 35 to 58 seats. Each word in the sentence (shown partially) is linked to all its synsets.
The path between trainer#n1 and teacher#1 is omitted for brevity (see Figure 1). The left part
shows the PPR method, and the right part shows the PPRw2w method.
67
Computational Linguistics Volume 40, Number 1
two senses with the same PageRank value, our algorithm does not return anything,
because it abstains from returning a sense in the case of ties. In contrast, recall measures
the percentage of correctly disambiguated instances divided by the total number of
instances to be disambiguated. This measure penalizes systems that are unable to return
a solution for all instances. Finally, the harmonic mean between precision and recall
(F1) combines both measures. F1 is our main measure of evaluation, as it provides a
balanced measure between the two extremes. Note that a system that returns a solution
for all instances would have equal precision, recall, and F1 measures.
In our experiments we build a context of at least 20 content words for each sentence
to be disambiguated, taking the sentences immediately before and after it in the case that
the original sentence was too short. The parameters for the PageRank algorithm were
set to 0.85 and 30 iterations following standard practice (Haveliwala 2002; Langville
and Meyer 2003; Mihalcea 2005). The post hoc impact of those and other parameters
has been studied in Section 6.4.
The general domain data sets used in this work are the SensEval-2 (S2AW) (Snyder
and Palmer 2004), SensEval-3 (S3AW) (Palmer et al. 2001), and SemEval-2007 fine-
grained (S07AW) (Palmer et al. 2001; Snyder and Palmer 2004; Pradhan et al. 2007)
and coarse grained all-words data sets (S07CG) (Navigli, Litkowski, and Hargraves
2007). All data sets have been produced similarly: A few documents were selected for
tagging, at least two annotators tagged nouns, verbs, adjectives, and adverbs, inter-
tagger agreement was measured, and the discrepancies between taggers were solved.
The first two data sets are labeled with WordNet 1.7 tags, the third uses WordNet
2.1 tags, and the last one uses coarse-grained senses that group WordNet 2.1 senses.
We run our system using WordNet 1.7 relations and senses for the first two data sets,
and WordNet 2.1 for the other two. Section 6.4.3 explores the use of WordNet 3.0 and
compares the performance with the use of other versions.
Regarding the coarse senses in S07CG, we used the mapping from WordNet 2.1
senses made available by the authors of the data set. In order to return coarse grained-
senses, we run our algorithm on fine-grained senses, and aggregate the scores for
all senses that map to the same coarse-grained sense. We finally choose the coarse-
grained sense with the highest score.
The data sets used in this article contain polysemous and monosemous words,
as customary; the percentage of monosemous word occurrences in the S2AW, S3AW,
S07AW, and S07CG data sets are 20.7%, 16.9%, 14.4%, and 29.9%, respectively.
6.1 Results
Table 2 shows the results as F1 of our random walk WSD systems over these data
sets. We detail overall results, as well as results per part of speech, and whether there
is any statistical difference with respect to the best result on each column. Statistical
significance is obtained using the paired bootstrap resampling method (Noreen 1989),
p < 0.01.
The table shows that PPRw2w is consistently the best method in three data sets. All
in all the differences are small, and in one data set STATIC obtains the best results. The
differences with respect to the best system overall are always statistically significant.
In fact, it is remarkable that a simple non-contextual measure like STATIC performs so
well, without the need for building subgraphs or any other manipulation. Section 6.4.6
will show that in some circumstances the performance of STATIC is much lower and
analyzes the reasons for this drop. Regarding the use of the word-to-word heuristic, it
consistently provides slightly better results than PPR in all four data sets. An analysis of
68
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
Table 2
Results on English data sets (F1). Best results in each column in bold. ? Statistically significant
with respect to the best result in each column.
S2AW - SensEval-2 All-Words
Method All N V Adj. Adv.
PPR 58.7? 71.8 35.0 58.9 69.8
PPRw2w 59.7 70.3 40.3 59.8 72.9
STATIC 58.0? 66.5 40.2 59.8 72.5
S3AW - SensEval-3 All-Words
Method All N V Adj. Adv.
PPR 57.3? 63.7 47.5 61.3 96.3
PPRw2w 57.9 65.3 47.2 63.6 96.3
STATIC 56.5? 62.5 47.1 62.8 96.3
S07AW - SemEval 2007 All-Words
Method All N V Adj. Adv.
PPR 39.7? 51.6 34.6 ? ?
PPRw2w 41.7? 56.0 35.3 ? ?
STATIC 43.0 56.0 37.3 ? ?
S07CG - SemEval 2007 Coarse-grained All-Words
Method All N V Adj. Adv.
PPR 78.1? 78.3 73.8 84.0 78.4
PPRw2w 80.1 83.6 71.1 83.1 82.3
STATIC 79.2? 81.0 72.4 82.9 82.8
the performance according to the POS shows that PPRw2w performs better particularly
on nouns, but there does not seem to be a clear pattern for the rest. In the rest of the
article, we will only show the overall results, omitting those for all POS, in order not to
clutter the result tables.
Our algorithms do not always return an answer, and thus the precision is higher
than the F1 measure. For instance, in S2AW the percentage of instances that get an
answer ranges between 95.4% and 95.6% for PPR, PPRw2w, and STATIC. The precision
for PPRw2w in S2AW is 61.1%, the recall is 58.4%, and F1 is 59.7%. This pattern of slightly
higher values for precisions, lower values for recall, and F1 in between is repeated for
all data sets, POS, and data sets. The percentage of instances that get an answer for the
other data sets is higher, ranging between 98.1% in S3AW and 99.9% in S07CG.
6.2 Comparison to State-of-the-Art Systems
In this section we compare our results with the WSD systems described in Section 2, as
well as the top performing supervised systems at competition time and other unsuper-
vised systems that improved on them. Note that we do not mention all unsupervised
systems participating in the competitions, but we do select the top performing ones. All
results in Table 3 are given as overall F1 for all Parts of Speech, but we also report F1 for
nouns in the case of S07CG, where Ponz10 (Ponzetto and Navigli 2010) reported very
69
Computational Linguistics Volume 40, Number 1
high results, but only for nouns. Note that the systems reported here and our system
might use different context sizes.
For easier reference, Table 3 uses a shorthand for each system, whereas the text in
this section includes the shorthand and the full reference the first time the shorthand is
used. The shorthand uses the first letters of the first author followed by the year of the
paper, except for systems which participated in SensEval and SemEval, where we use
their acronym. Most systems in the table have been presented in Section 2, with a few
exceptions that will be presented this section.
The results in Table 3 confirm that our system (PPRw2w) performs on the state-of-the-
art of knowledge-based and unsupervised systems, with two exceptions:
(1) Nav10 (Navigli and Lapata 2010) obtained better results on S07AW.
We will compare both systems in more detail below, and also include
a reimplementation in the next subsection which shows that, when
using the same LKB, our method obtains better results.
(2) Although not reported in the table, an unsupervised system using
automatically acquired training examples from bilingual data (Chan and
Ng 2005) obtained very good results on S2AW nouns (77.2 F1, compared
with our 70.3 F1 in Table 2). The automatically acquired training examples
are used in addition to hand-annotated data in Zhong10 (Zhong and Ng
2010), also reported in the table (see below).
We report the best unsupervised systems in S07AW and S07CG on the same row.
JU-SKNSB (Naskar and Bandyopadhyay 2007) is a system based on an extended version
Table 3
Comparison with state-of-the-art results (F1). The top rows report knowledge-based and
unsupervised systems, followed by our system (PPRw2w). Below we report systems that use
annotated data to some degree: (1) MFS or counts from hand-annotated corpora, (2) fully
supervised systems, including the best supervised participants in each exercise. Best result
among unsupervised systems in each column is shown in bold. Please see text for references
of each system.
System S2AW S3AW S07AW S07CG (N)
Mih05 54.2 52.2
Sinha07 57.6 53.6
Tsatsa10 58.8 57.4
Agirre08 56.8
Nav10 52.9 43.1
JU-SKNSB / TKB-UO 40.2 70.2 (70.8)
Ponz10 (79.4)
PPRw2w 59.7 57.9 41.7 80.1 (83.6)
MFS(1) 60.1 62.3 51.4 78.9 (77.4)
IRST-DDD-00(1) 58.3
Nav05(1) / UOR-SSI(1) 60.4 83.2 (84.1)
BESTsup (2) 68.6 65.2 59.1 82.5 (82.3)
Zhong10(2) 68.2 67.6 58.3 82.6
70
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
of the Lesk algorithm (Lesk 1986), evaluated on S07AW. TKB-UO (Anaya-Sa?nchez,
Pons-Porrata, and Berlanga-Llavori 2007), which was evaluated in S07CG, clusters
WordNet senses and uses so-called topic signatures based on WordNet information for
disambiguation. IRST-DDD-00 (Strapparava, Gliozzo, and Giuliano 2004) is a system
based on WordNet domains which leverages on large unannotated corpora. They
obtained excellent results, but their calculation of scores takes into account synset
probabilities from SemCor, and the system can thus be considered to use some degree
of supervision. We consider that systems which make use of information derived from
hand-annotated corpora need to be singled out as having some degree of supervision.
This includes systems using the MFS heuristic, as it is derived from hand-annotated
corpora. In the case of the English WordNet, the use of the first sense also falls in this
category, as the order of senses in WordNet is based on sense counts in hand-annotated
corpora. Note that for wordnets in other languages, hand-annotated corpus is scarce,
and thus our main results do not use this information. Section 6.4.7 analyzes the results
of our system when combined with this information.
Among supervised systems, the best supervised systems at competition time are
reported in a single row (Mihalcea 2002; Decadt et al. 2004; Chan, Ng, and Zhong
2007; Tratz et al. 2007). We also report Zhong10 (Zhong and Ng 2010), which is a freely
available supervised system giving some of the strongest results in WSD.
We will now discuss in detail the systems that are most similar to our own. We first
review the WordNet versions and relations used by each system. Mih05 (Mihalcea 2005)
and Sinha07 (Sinha and Mihalcea 2007) apply several similarity methods, which use
WordNet information from versions 1.7.1 and 2.0, respectively, including all relations
and the text in the glosses.5 Tsatsa10 (Tsatsaronis, Varlamis, and N?rva?g 2010) uses
WordNet 2.0. Agirre08 (Agirre and Soroa 2008) experimented with several LKBs formed
by combining relations from different sources and versions, including WordNet 1.7 and
eXtended WordNet. Nav05 and Nav10 (Navigli and Velardi 2005; Navigli and Lapata
2010) use WordNet 2.0, enriched with manually added co-occurrence relations which
are not publicly available.
We can see in Table 3 that the combination of Personalized PageRank and LKB
presented in this article outperforms both Mih05 and Sinha07. In order to factor
out the difference in the WordNet version, we performed experiments using WN2.1
and eXtended WordNet, yielding 58.7 and 56.5 F1 for S2AW and S3AW, respectively.
Although a head-to-head comparison is not possible, the systems use similar informa-
tion: Although they use glosses, our algorithm cannot directly use the glosses, and thus
we use disambiguated glosses as delivered in eXtended WordNet. All in all the results
suggest that analyzing the LKB structure as a graph is preferable to computing pairwise
similarity measures over synsets to build a custom graph and then applying graph
measures. The results of various in-house experiments replicating Mih05 also confirmed
this observation. Note also that our methods are simpler than the combination strategy
used in Sinha07.
Nav05 (Navigli and Velardi 2005) uses a knowledge-based WSD method based on
lexical chains called structural semantic interconnections (SSI). The SSI method was
evaluated on the SensEval-3 data set, as shown in row Nav05 in Table 3. Note that
the method labels an instance with the MFS of the word if the algorithm produces
no output for that instance, which makes comparison to our system unfair, especially
given the fact that the MFS performs better than SSI. In fact, it is not possible to separate
5 Personal communication.
71
Computational Linguistics Volume 40, Number 1
the effect of SSI from that of the MFS, and we thus report it as using some degree of
supervision in the table. A variant of the algorithm called UOR-SSI (Navigli, Litkowski,
and Hargraves 2007) (reported in the same row) used a manually added set of 70,000
relations and obtained the best results in S07CG out-of-competition,6 even better than
the best supervised method. Reimplementing SSI is not trivial, so we did not check the
performance of a variant of SSI that does not use MFS and that uses the same LKB as
our method. Section 6.4.7 analyzes the results of our system when combined with MFS
information.
Agirre08 (Agirre and Soroa 2008) uses breadth-first search to extract subgraphs of
the WordNet graph for each context to be disambiguated, and then applies PageRank.
Our better results seem to indicate that using the full graph instead of those subgraphs
would perform better. In order to check whether the better results are due to differences
in the information used, the next subsection presents the results of our reimplementa-
tion of the systems using the same information as our full-graph algorithm.
Tsatsa10 (Tsatsaronis, Vazirgiannis, and Androutsopoulos 2007; Tsatsaronis,
Varlamis, and N?rva?g 2010) also builds the graph using breadth-first search, but
weighting each type of edge differently, and using graph-based measures that take into
account those weights. This is in contrast to the experiments performed in this article
where edges have no weight, and is an interesting avenue for future work.
Nav10 (Navigli and Lapata 2010) first builds a subgraph of WordNet composed of
paths between synsets using depth-first search and then applies a set of graph centrality
algorithms. The best results are obtained using the degree of the nodes, and they present
two variants, depending on how they treat ties: Either they return a sense at random, or
they return the most frequent sense. For fair comparison to our system (which does not
use MFS as a back-off), Table 3 reports the former variant as Nav10. This system is better
than ours in one data set and worse in another. They use 60,000 relations that are not
publicly available, but they do not use eXtended WordNet relations. In order to check
whether the difference in performance is due to the relations used or the algorithm,
the next subsection presents a reimplementation of their best graph-based algorithms
using the same LKB as we do. In earlier work (Navigli and Lapata 2007) they test a
similar system on S3AW, but report results only for nouns, verbs, and adjectives (F1 of
61.9, 36.1, and 62.8, respectively), all of which are below the results of our system (cf.
Table 2).
In Ponz10 (Ponzetto and Navigli 2010) the authors apply the same techniques as in
Nav10 to a new resource called WordNet++. They report results for nouns using degree
on subgraphs for the S07CG data set, as shown in Table 3. Their F1 on nouns is 79.4,
lower than our results using our LKB.
6.3 Comparison with Related Algorithms
The previous section shows that our algorithm when applied to a LKB built from
WordNet and eXtended WordNet outperforms other knowledge-based systems in all
cases but one system in one data set. In this section we factor out algorithm and LKB,
and present the results of other graph-based methods for WSD using the same WordNet
versions and relations as in the previous section. As we mentioned in Section 2, ours
is the only method using the full WordNet graph. Navigli and Lapata (2010) and
Ponzetto and Navigli (2010) build a custom graph based on the relations in WordNet
6 The task was co-organized by the authors.
72
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
as follows: For each sense si of each word in the context, a depth-first search (DFS for
short) is conducted through the WordNet graph starting in si until another sense sj of a
word in the context is found or maximum distance is reached. The maximum distance
was set by the authors to 6. All nodes and edges between si and sj, inclusive, are added to
the subgraph. Graph-based measures are then used to select the output senses for each
target word, with degree and PageRank yielding the best results. In closely related work,
Agirre and Soroa (2008) and Tsatsaronis, Varlamis, and N?rva?g (2010) use breadth-first
search (BFS) over the whole graph, and keep all paths connecting senses. Note that
unlike the dfs approach, bfs does not require any threshold. The subgraphs obtained
by each of these methods are slightly different.
We reimplemented both strategies, namely, DFS with threshold 6 and BFS with no
threshold. Table 4 shows the overall results of degree and PageRank for both kinds of
subgraphs. DFS yields slightly better results than BFS but PPRw2w is best in all four data
sets, with statistical significance.
In addition, we run PPR and PPRw2w on DFS and BFS subgraphs, and obtained better
results than degree and PageRank in all data sets. DFS with PPR and DFS with PPRw2w
are best in S3AW and S07AW, respectively, although the differences with PPRw2w are not
statistically significant. PPRw2w on the full graph is best in two data sets, with statistical
significance.
From these results we can conclude that PPR and PPRw2w yield the best results
also for subgraphs. Regarding the use of the full graph with respect to DFS or BFS,
the performances for PPRw2w are very similar, but using the full graph gives a small
advantage. Section 6.4.5 provides an analysis of efficiency.
6.4 Analysis of Performance Factors
The behavior of the WSD system is influenced by a set of parameters that can yield
different results. In our main experiments we did not perform any parameter tuning;
we just used some default values which were found to be useful according to previous
work. In this section we perform a post hoc analysis of several parameters on the general
performance of the system, reporting F1 on a single data set, S2AW.
Table 4
Results for subgraph methods compared with our method (F1). In the Reference column we
mention the reference system that we reimplemented. Best results in each column in bold. ?
Statistically significant with respect to the best result in each column. 0 No significant difference.
Reference S2AW S3AW S07AW S07CG
DFSdegree Nav10, Ponz10 58.4? 56.4? 40.3? 79.4?
BFSdegree 57.9? 56.5? 39.9? 79.2?
DFSPageRank Nav10 58.2? 56.4? 39.9? 79.6?
BFSPageRank Agirre08 57.7? 56.7? 39.7? 79.4?
DFSPPR 59.3? 58.2 41.40 78.1?
BFSPPR 58.80 57.50 41.20 78.8?
DFSPPRw2w 58.7
0 58.00 41.2? 79.7?
BFSPPRw2w 58.1
? 57.90 41.9 79.5?
PPRw2w 59.7 57.90 41.70 80.1
73
Computational Linguistics Volume 40, Number 1
55
56
57
58
59
60
0 5 10 15 20 25 30
Iterations



  
Figure 3
Convergence according to number of PageRank iterations (F1 on S2AW).
6.4.1 PageRank Parameters. The PageRank algorithm has two main parameters, the
so-called damping factor and the number of iterations (or, conversely, the convergence
threshold), which we set as 0.85 and 30, respectively (cf. Section 4). Figure 3 depicts
the effect of varying the number of iterations. It shows that the algorithm converges
very quickly: One sole iteration yields relatively high performance, and 20 iterations
are enough to achieve convergence. Note also that the performance is in the [58.0, 58.5]
range for iterations over 5. Note that we use the same range of F1 for the y axis of Figures
3, 4, and 5 for easier comparison.
Figure 4 shows the effect of varying the damping factor. Note that a damping
factor of zero means that the PageRank value coincides with the initial probability
distribution. Given the way we initialize the distribution (c.f. Section 5.2), it would mean
that the algorithm is not able to disambiguate the target words. Thus, the initial value
on Figure 4 corresponds to a damping factor of 0.001. On the other hand, a damping
factor of 1 yields to the same results as the STATIC method (c.f. Section 5.1). The best
value is attained with 0.90, with similar values around it (less than 0.5 absolute points in
55
56
57
58
59
60
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Damping factor









Figure 4
Varying the damping factor (F1 on S2AW).
74
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
55
56
57
58
59
60
5 10 15 20 25 30 35 40 45 50
Context size

 


 

Figure 5
Varying the context size (F1 on S2AW).
variation), in agreement with previous results which preferred values in the 0.85...0.95
interval (Haveliwala 2002; Langville and Meyer 2003; Mihalcea 2005).
6.4.2 Size of Context Window. Figure 5 shows the performance of the system when trying
different context windows for the target words. The best context size is for windows
of 20 content words, with less than 0.5 absolute point losses for windows in the [5, 25]
range.
6.4.3 Using Different WordNet Versions. There has been little research on the best strat-
egy to use when dealing with data sets and resources attached to different WordNet
versions. Table 5 shows the results for the four data sets used in this study when using
different WordNet versions. Two of the data sets (S2AW and S3AW) were tagged with
senses from version 1.7, S07AW with senses from version 2.1, and S07CG with coarse
senses built on version 2.1 senses.
Given the fact that WordNet 3.0 is a more recent version that includes more rela-
tions, one would hope that using it would provide the best results (Cuadros and Rigau
2008; Navigli and Lapata 2010). We built a graph analogous to the ones for versions
1.7 and 2.1, but using the hand-disambiguated glosses instead of eXtended WordNet
glosses. We used freely available mappings (Daude, Padro, and Rigau 2000)7 to convert
our eXtended WordNet relations to 3.0, and then the WordNet 3.0 sense results to the
corresponding version. In addition, we also tested WN1.7 on S07AW and S07CG, and
WN2.1 on S2AW and S3AW, also using the mappings from Daude, Padro, and Rigau
(2000).
Table 5 shows that the best results are obtained using our algorithm on the same
WordNet version as used in the respective data set. When testing on data sets tagged
with WordNet 1.7, similar results are obtained using 2.1 or 3.0. When testing on data sets
based on 2.1, 3.0 has a small lead over 1.7. In any case, the differences are small ranging
from 1.4 absolute points to 0.5 points. All in all, it seems that the changes introduced
7 http://nlp.lsi.upc.edu/tools/download-map.php.
75
Computational Linguistics Volume 40, Number 1
Table 5
Comparing WordNet versions. Best result in each row in bold.
Data set version 1.7 + xwn 2.1 + xwn 3.0 + xwn
S2AW 1.7 59.7 58.7 58.4
S3AW 1.7 57.9 56.5 56.8
S07AW 2.1 40.7 41.7 40.9
S07CG 2.1 coarse 79.6 80.1 79.6
by different versions slightly deteriorate the results, and the best strategy is to use the
same WordNet version as was used for tagging.
6.4.4 Using xwn vs. WN3.0 Gloss Relations. WordNet 3.0 was released with an accom-
panying data set comprising glosses where some of the words had been manually
disambiguated. In Table 6 we present the results of using these glosses with the WN3.0
graph, showing that the results are lower than using XWN relations. We also checked
the use of WN3.0 gloss relation with other WordNet versions, and the results using
XWN were always slightly better. We hypothesize that the better results for XWN are
due to the amount of relations, with XWN holding 61% more relations than WN3.0
glosses. Still, the best relations are obtained with the combination of both kinds of gloss
relations.
6.4.5 Analysis of Relations. Previous results were obtained using all the relations
of WordNet and taking eXtended WordNet relations into account. In this section
we analyze the effect of the relation types on the whole process, following the
relation groups presented in Table 1. Table 7 shows the results when using different
combinations over relation types. The eXtended WordNet XWN relations appear the
most valuable when performing random walk WSD, as their performance is as good
as when using the whole graph, and they produce a large drop when ablated from the
graph. Ignoring antonymy relations produces a small improvement, but the differences
between using all the relations, eliminating antonyms, and using XWN relations only
are too small to draw any further conclusions. It seems that given the XWN relations
(the most numerous, cf. Section 3.1), our algorithm is fairly robust to the addition or
deletion of other kinds of relations (less numerous).
6.4.6 Behavior with Respect to STATIC and MFS. The high results of the very simple STATIC
method (PageRank with no context) seems to imply that there is no need to use context
for disambiguation. Our intuition was that the synsets which correspond to the most
Table 6
Comparing XWN and WN3.0 gloss relations, separately and combined. Best result in each row in
bold.
Data set 3.0 + XWN 3.0 + gloss 3.0 + XWN + gloss
S2AW 58.4 58.1 58.8
S3AW 56.8 51.7 56.1
S07AW 40.9 38.8 42.2
S07CG 79.6 78.9 80.2
76
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
Table 7
Analysis of relation types. The first column shows the performance using just that relation type.
The second shows the combination of TAX and each type. The last column shows all relations
except the corresponding type.
relation single + TAX ablation
TAX 37.4 ? 59.9
ANT 19.1 42.1 59.9
MER 23.4 36.4 59.6
REL 35.4 46.1 59.6
XWN 59.9 59.8 47.1
Reference system (all relations) 59.7
frequent senses would get more relations. We thus computed the correlation between
systems, gold tags, and MFS. In order to make the correlation results comparable to
the figures used on evaluation, we use the number of times both sets of results agree,
divided by the number of results returned by the first system. Table 8 shows such a
matrix of pairwise correlations. If we take the row of gold tags, the results reflect the
performance of each system (the precision). In the case of MFS, the column shows that
STATIC has a slightly larger correlation with the MFS than the other two methods. The
matrix also shows that all our three methods agree more than 80% of the time, with PPR
and STATIC having a relatively smaller agreement.
In contrast, related work using the same techniques over domain-specific words
(Agirre, Lo?pez de Lacalle, and Soroa 2009) shows that the results of our Personalized
PageRank models departs significantly from MFS and STATIC. Table 9 shows the results
of the three techniques on the three subcorpora that constitute the evaluation data set
published in Koeling, McCarthy, and Carroll (2005). This data set consists of examples
retrieved from the Sports and Finance sections of the Reuters corpus, and also from the
balanced British National Corpus (BNC), which is used as a general domain contrast
corpus.
Applying PageRank over the entire WordNet graph yields low results, very similar
to those of MFS, and below those of Personalized PageRank. This confirms that STATIC
PageRank is closely related to MFS, as we hypothesized in Section 5.1 and showed in
Table 8 for the other general domain data sets. Whereas the results of PPRw2w are very
similar in the general-domain BNC, PPRw2w departs from STATIC and MFS with 30 and
20 points of difference in the domain-specific Sports and Finance corpora. These results
are highly relevant, because they show that PPR is able to effectively use contextual
information, and depart from the MFS and STATIC baselines.
Table 8
Correlation between systems, gold tags, and MFS.
Gold MFS PPR PPRw2w STATIC
Gold 100.0 61.3 58.6 59.7 57.8
MFS 60.1 100.0 79.8 79.0 81.3
PPR 57.4 79.8 100.0 86.8 82.8
PPRw2w 58.4 79.0 86.8 100.0 86.4
STATIC 56.7 81.4 82.8 86.4 100.0
77
Computational Linguistics Volume 40, Number 1
Table 9
Results on three subcorpora as reported in Agirre, Lo?pez de Lacalle, and Soroa (2009),
where Sports and Finance are domain-specific. Best results on each column in bold.
System BNC Sports Finance
MFS 34.9 19.6 37.1
STATIC 36.6 20.1 39.6
PPRw2w 37.7 51.5 59.3
Table 10
Combination with MFS (F1). The first two rows correspond to our system with and without
information from MFS. Below that we report systems that also use MFS. Best results in each
column in bold.
System S2AW S3AW S07AW S07CG (N)
PPRw2w 59.7 57.9 41.7 80.1 (83.6)
PPRw2w MFS 62.6 63.0 48.6 81.4 (82.1)
MFS 60.1 62.3 51.4 78.9 (77.4)
IRST-DDD-00 58.3
Nav05 / UOR-SSI 60.4 83.2 (84.1)
Ponz10 81.7 (85.5)
6.4.7 Combination with MFS. As mentioned in Section 6.2, we have avoided using any
information regarding sense frequencies from annotated corpora, as this information
is not always available for all wordnets. In this section we report the results of our
algorithm when taking into account prior probabilities of senses taken from sense
counts. We used the sense counts provided with WordNet in the index.sense file.8 In
this setting, the edges linking words and their respective senses are weighted according
to the prior probabilities of those senses, instead of uniform weights as in Section 5.2.
Table 10 shows that results when using priors from MFS improve over the results
of the original PPRw2w in all data sets. The improvement varies across parts of speech,
and, for instance, the results for nouns in S07CG are worse (shown in rightmost column
of Table 10). In addition, the results for PPRw2w when using MFS information improve
over MFS in all cases except for S07AW.
The table also reports the best systems that do use MFS (see Section 6.3 for detailed
explanations). For S2AW and S07AW we do not have references to related systems.
For S3AW we can see that our system performs best. In the case of S07CG, UOR-SSI
reports better results than our system. Finally, the final row reports their system when
combined with MFS information as back-off (Ponzetto and Navigli 2010), which also
attains better results than our system. We tried to use a combination method similar to
theirs, but did not manage to improve results.
6.4.8 Efficiency of Full Graphs vs. Subgraphs. Given the very close results of our algorithm
when using full graphs and subgraphs (cf. Section 6.3), we studied the efficiency of each.
We benchmarked several graph-based methods on the S2AW data set, which comprises
8 http://wordnet.princeton.edu/wordnet/man/senseidx.5WN.html.
78
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
2,473 instances to be disambiguated. All tests were done on a multicore computer with
16 GB of memory using a single 2.66 GHz processor. When using the full graph PPR
disambiguates full sentences in one go at 684 instances per minute, whereas PPRw2w
disambiguates one word at a time, 70 instances per minute. The DFS subgraphs provide
better performance than PPRw2w, 228 instances per minute when using degree, with
marginally slower performance when using PPRw2w (210 instances per minute). The
BFS subgraph is slowest, with around 20 instances per minute. The memory footprint of
using the full graph algorithm is small, just 270 MB, so several processes can be run on
a multiprocessor machine easily.
All in all, there is a tradeoff in performance and speed, with PPRw2w on the full
graph providing better results at the cost of some speed, and PPR on the full graph
providing the best speed at the cost of worse performance. Using DFS with PPRw2w
lays in between and is also a good alternative, and its speed can be improved using
pre-indexed paths.
6.5 Experiments on Spanish
Our WSD algorithm can be applied over non-English texts, provided that a LKB for this
particular language exists. We have applied our random walk algorithms to the Spanish
WordNet (Atserias, Rigau, and Villarejo 2004), using the SemEval-2007 Task 09 data set
as evaluation gold standard (Ma`rquez et al. 2007). The data set contains examples of the
150 most frequent nouns in the CESS-ECE corpus, manually annotated with Spanish
WordNet synsets. It is split into a train and test part, and has an ?all words? shape
(i.e., input consists of sentences, each one having at least one occurrence of a target
noun). We ran the experiment over the test part (792 instances), and used the train part
for calculating the MFS heuristic. The results in Table 11 are consistent with those for
English, with our algorithms approaching MFS performance, and PPRw2w yielding the
best results. Note that for this data set the supervised algorithm could barely improve
over the MFS, which performs very well, suggesting that in this particular data set the
sense distributions are highly skewed.
Finally, we also show results for the first sense in the Spanish WordNet. In the
Spanish WordNet the order of the senses of a word has been assigned directly by
the lexicographer (Atserias, Rigau, and Villarejo 2004), as there is no information
of sense frequency from hand-annotated corpora. This is in contrast to the English
WordNet, where the senses are ordered according to their frequency in annotated
Table 11
Results as F1 on the Spanish SemEval07 data set, including first sense, MFS, and the best
supervised system in the competition. ? Statistically significant difference with respect to
the best of our results (in bold).
Method Acc.
PPR 78.4?
PPRw2w 79.3
STATIC 76.5?
First sense 66.4?
MFS 84.6?
BEST 85.1?
79
Computational Linguistics Volume 40, Number 1
corpora (Fellbaum 1998), and reflects the status on most other wordnets. In this case,
our algorithm clearly improves over the first sense in the dictionary.
7. Conclusions
In this article we present a method for knowledge-based Word Sense Disambiguation
based on random walks over relations in a LKB. Our algorithm uses the full graph
of WordNet efficiently, and performs better than PageRank or degree on subgraphs
(Navigli and Lapata 2007; Agirre and Soroa 2008; Navigli and Lapata 2010; Ponzetto
and Navigli 2010). We also show that our combination of method and LKB built from
WordNet and eXtended WordNet compares favorably to other knowledge-based
systems using similar information sources (Mihalcea 2005; Sinha and Mihalcea
2007; Tsatsaronis, Vazirgiannis, and Androutsopoulos 2007; Tsatsaronis, Varlamis, and
N?rva?g 2010). Our analysis shows that Personalized PageRank yields similar results
when using subgraphs and the full graph, with a trade-off between speed and perfor-
mance, where Personalized PageRank over the full graph is fastest, its word-to-word
variant slowest, and Personalized PageRank over the subgraph lies in between.
We also show that the algorithm can be easily ported to other languages with good
results, with the only requirement of having a wordnet. Our results improve over the
first sense of the Spanish dictionary. This is particularly relevant for wordnets other
than English. For the English WordNet the senses of a word are ordered according to the
frequency of the senses in hand-annotated corpora, and thus the first sense is equivalent
to the Most Frequent Sense, but this information is not always available for languages
that lack large-scale hand-annotated corpora.
We have performed an extensive analysis, showing the behavior according to the
parameters of PageRank, and studying the impact of different relations and WordNet
versions. We have also analyzed the relation between our PPR algorithm, MFS, and
STATIC PageRank. In general domain corpora they get similar results, close to the
performance of the MFS learned from SemCor, but the results reported on domain-
specific data sets (Agirre, Lo?pez de Lacalle, and Soroa 2009) show that PPR is able to
move away from the MFS and STATIC and improve over them, indicating that PPR is able
to effectively use contextual information, and depart from MFS and STATIC PageRank.
The experiments in this study are readily reproducible, as the algorithm and the
LKBs are publicly available.9 The system can be applied easily to sense inventories and
knowledge bases different from WordNet.
In the future we would like to explore methods to incorporate global weights of the
edges in the random walk calculations (Tsatsaronis, Varlamis, and N?rva?g 2010). Given
the complementarity of the WordNet++ resource (Ponzetto and Navigli 2010) and our
algorithm, it would be very interesting to explore the combination of both, as well as
the contribution of other WordNet related resources (Cuadros and Rigau 2008).
Acknowledgments
We are deeply indebted to the reviewers,
who greatly helped to improve the
article. Our thanks to Rob Koeling and
Diana McCarthy for kindly providing
the data set, thesauri, and assistance,
and to Roberto Navigli and Simone
Ponzetto for clarifying the method to
map to coarse-grained senses. This
work has been partially funded by the
Education Ministry (project KNOW2
TIN2009-15049-C03-01).
9 http://ixa2.si.ehu.es/ukb.
80
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
References
Agirre, E., E. Alfonseca, K. Hall, J. Kravalova,
M. Pasca, and A. Soroa. 2009. A study
on similarity and relatedness using
distributional and WordNet-based
approaches. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics - Human Language
Technologies conference (NAACL/HLT?09),
pages 19?27, Boulder, CO.
Agirre, E., T. Baldwin, and D. Martinez.
2008. Improving parsing and PP
attachment performance with sense
information. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics (ACL/HLT?08),
pages 317?325, Columbus, OH.
Agirre, E., K. Bengoetxea, K. Gojenola,
and J. Nivre. 2011. Improving
dependency parsing with semantic
classes. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies
(ACL/HLT?11), pages 699?703,
Portland, OR.
Agirre, E., O. Lo?pez de Lacalle, and A. Soroa.
2009. Knowledge-based WSD on specific
domains: Performing better than generic
supervised WSD. In Proceedings of the
19th International Joint Conference on
Artificial Intelligence (IJCAI?09),
pages 1,501?1,506, Pasadena, CA.
Agirre, E. and G. Rigau. 1996. Word sense
disambiguation using conceptual density.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 16?22, Copenhagen.
Agirre, E. and A. Soroa. 2008. Using the
multilingual central repository for
graph-based word sense disambiguation.
In Proceedings of the 6th Conference on
Language Resources and Evaluation
(LREC ?08), pages 1,388?1,392, Marrakesh.
Agirre, E. and A. Soroa. 2009. Personalizing
PageRank for word sense disambiguation.
In Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL?09),
pages 33?41, Athens.
Anaya-Sa?nchez, H., A. Pons-Porrata, and
R. Berlanga-Llavori. 2007. TKB-UO: Using
sense clustering for WSD. In Proceedings of
the 4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 322?325, Prague.
Atserias, J., G. Rigau, and L. Villarejo. 2004.
Spanish wordnet 1.6: Porting the Spanish
wordnet across Princeton versions.
In Proceedings of the 4th Conference on
Language Resources and Evaluation
(LREC?04), pages 161?164, Lisbon.
Barzilay, R. and M. Elhadad. 1997. Using
lexical chains for text summarization.
In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization,
pages 10?17, New York, NY.
Brin, S. and L. Page. 1998. The anatomy
of a large-scale hypertextual Web search
engine. Computer Networks and ISDN
Systems, 30(1-7):107?117.
Carpuat, M. and D. Wu. 2007. Improving
statistical machine translation using word
sense disambiguation. In Proceedings of
the Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP/CoNLL?07), pages 61?72, Prague.
Chan, Y. S. and H. T. Ng. 2005. Scaling
up word sense disambiguation via
parallel texts. In Proceedings of the
20th National Conference on Artificial
Intelligence (AAAI?05), pages 1,037?1,042,
Pittsburgh, PA.
Chan, Y. S., H. T. Ng, and D. Chiang.
2007. Word sense disambiguation
improves statistical machine translation.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics
(ACL?07), pages 33?40, Prague.
Chan, Y. S., H. T. Ng, and Z. Zhong. 2007.
NUS-PT: Exploiting parallel texts for
word sense disambiguation in the English
all-words tasks. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 253?256, Prague.
Cowie, J., J. Guthrie, and L. Guthrie. 1992.
Lexical disambiguation using simulated
annealing. In Proceedings of the Workshop
on Speech and Natural Language (HLT?91),
pages 238?242, Morristown, NJ.
Cuadros, M. and G. Rigau. 2006. Quality
assessment of large scale knowledge
resources. In Proceedings of Joint SIGDAT
Conference on Empirical Methods in Natural
Language Processing (EMNLP?06),
pages 534?541, Sydney.
Cuadros, M. and G. Rigau. 2007.
Semeval-2007 task 16: Evaluation of
wide coverage knowledge resources.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007) in conjunction with ACL,
pages 81?86, Prague.
Cuadros, M. and G. Rigau. 2008. Knownet:
Using topic signatures acquired from the
Web for building automatically highly
dense knowledge bases. In Proceedings
81
Computational Linguistics Volume 40, Number 1
of the 22nd International Conference on
Computational Linguistics (COLING?08),
pages 71?84, Manchester.
Daude, J., L. Padro, and G. Rigau. 2000.
Mapping WordNets using structural
information. In Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics (ACL?00),
pages 504?511, Hong Kong.
Decadt, B., V. Hoste, W. Daelemans, and
A. Van Den Bosch. 2004. GAMBL, genetic
algorithm optimization of memory-based
WSD. In Proceedings of SENSEVAL-3 Third
International Workshop on Evaluation of
Systems for the Semantic Analysis of Text,
pages 108?112, Barcelona.
Escudero, G., L. Ma?rquez, and G. Rigau.
2000. An empirical study of the domain
dependence of supervised word sense
disambiguation systems. Proceedings of
the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing
and Very Large Corpora (EMNLP/VLC?00),
pages 172?180, Hong Kong.
Fellbaum, C., editor. 1998. WordNet: An
Electronic Lexical Database and Some of Its
Applications. MIT Press, Cambridge, MA.
Haveliwala, T. H. 2002. Topic-sensitive
PageRank. In Proceedings of the
11th International Conference on World
Wide Web (WWW?02), pages 517?526,
New York, NY.
Hughes, T. and D. Ramage. 2007. Lexical
semantic relatedness with random graph
walks. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP/CoNLL?07),
pages 581?589, Prague.
Jiang, J. J. and D. W. Conrath. 1997. Semantic
similarity based on corpus statistics and
lexical taxonomy. In Proceedings of the
International Conference on Research in
Computational Linguistics, pages 19?33,
Taiwan.
Kleinberg., J. M. 1998. Authoritative
sources in a hyperlinked environment.
In Proceedings of the Ninth Annual
ACM-SIAM Symposium on Discrete
Algorithms (SODA?98), pages 668?677,
Philadelphia, PA.
Koeling, R., D. McCarthy, and J. Carroll.
2005. Domain-specific sense distributions
and predominant sense acquisition.
In Proceedings of the Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing (HLT/EMNLP?05),
pages 419?426, Ann Arbor, MI.
Langville, A. N. and C. D. Meyer. 2003.
Deeper inside PageRank. Internet
Mathematics, 1(3):335?380.
Lesk, M. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of the
5th Annual International Conference on
Systems Documentation (SIGDOC?86),
pages 24?26, New York, NY.
Ma`rquez, L., M. A. Villarejo, T. Mart??, and
M. Taule?. 2007. SemEval-2007 Task 09:
Multilevel semantic annotation of
Catalan and Spanish. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 42?47, Prague.
McCarthy, D., R. Koeling, J. Weeds,
and J. Carroll. 2007. Unsupervised
acquisition of predominant word senses.
Computational Linguistics, 33(4):553?590.
Mihalcea, R. 2002. Word sense
disambiguation with pattern learning
and automatic feature selection. Natural
Language Engineering, 8:343?358.
Mihalcea, R. 2005. Unsupervised
Large-vocabulary word sense
disambiguation with graph-based
algorithms for sequence data labeling.
In Proceedings of the Conference on
Human Language Technology and
Empirical Methods in Natural Language
Processing (HLT?05), pages 411?418,
Morristown, NJ.
Mihalcea, R. and D. I. Moldovan. 2001.
eXtended WordNet: Progress report.
In Proceedings of the NAACL Workshop
on WordNet and Other Lexical Resources,
pages 95?100, Pittsburgh, PA.
Miller, G. A., C. Leacock, R. Tengi, and R.
Bunker. 1993. A semantic concordance.
In Proceedings of the Workshop on
Human Language Technology (HLT?93),
pages 303?308, Plainsboro, NJ.
Naskar, S. K. and S. Bandyopadhyay.
2007. JU-SKNSB: Extended WordNet
based WSD on the English all-words
task at SemEval-1. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 203?206, Prague.
Navigli, R. 2008. A structural approach
to the automatic adjudication of word
sense disagreements. Natural Language
Engineering, 14(4):547?573.
Navigli, R. and M. Lapata. 2007. Graph
connectivity measures for unsupervised
word sense disambiguation. In Proceedings
of the 17th International Joint Conference
82
Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD
on Artificial Intelligence (IJCAI?07),
pages 1,683?1,688, Hyderabad.
Navigli, R. and M. Lapata. 2010.
An experimental study of graph
connectivity for unsupervised word
sense disambiguation. IEEE Transactions
on Pattern Analysis and Machine
Intelligence, 32(4):678?692.
Navigli, R., K. C. Litkowski, and
O. Hargraves. 2007. SemEval-2007 Task 07:
Coarse-grained English all-words task.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007) in conjunction with ACL,
pages 30?35, Prague.
Navigli, R. and P. Velardi. 2005. Structural
semantic interconnections: A knowledge-
based approach to word sense
disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
27(7):1,075?1,086.
Navigli, Roberto, Stefano Faralli, Aitor Soroa,
Oier Lo?pez de Lacalle, and Eneko Agirre.
2011. Two birds with one stone: Learning
semantic models for text categorization
and word sense disambiguation.
In Proceedings of CIKM, pages 2,317?2,320,
Glasgow.
Ng, H. T. and H. B. Lee. 1996. Integrating
multiple knowledge sources to
disambiguate word sense: An exemplar-
based approach. In Proceedings of the
34th Annual Meeting of the Association for
Computational Linguistics, ACL ?96,
pages 40?47, Stroudsburg, PA.
Ng, T. H. 1997. Getting serious about
word sense disambiguation.
In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical
Semantics: Why, What, and How?,
pages 1?7, Washington, DC.
Noreen, E. W. 1989. Computer-Intensive
Methods for Testing Hypotheses.
John Wiley & Sons.
Palmer, M., C. Fellbaum, S. Cotton, L. Delfs,
and H. T. Dang. 2001. English tasks:
All-words and verb lexical sample.
In Proceedings of SENSEVAL-2: Second
International Workshop on Evaluating
Word Sense Disambiguation Systems,
pages 21?24, Toulouse.
Patwardhan, S., S. Banerjee, and
T. Pedersen. 2003. Using measures of
semantic relatedness for word sense
disambiguation. In Proceedings of the
Fourth International Conference on
Intelligent Text Processing and
Computational Linguistics (CICLING-03),
pages 241?257, Mexico City.
Pe?rez-Agu?era, J. R. and H. Zaragoza. 2008.
UCM-Y!R at CLEF 2008 Robust and WSD
tasks. In Proceedings of the 9th Cross
Language Evaluation Forum Workshop
(CLEF?08), pages 138?145, Aarhus.
Ponzetto, S. P. and R. Navigli. 2010.
Knowledge-rich word sense
disambiguation rivaling supervised
system. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics (ACL?10), pages 1,522?1,531,
Uppsala.
Pradhan, S., E. Loper, D. Dligach, and
M. Palmer. 2007. SemEval-2007 Task-17:
English lexical sample SRL and all words.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007) in conjunction with ACL,
pages 87?92, Prague.
Sinha, R. and R. Mihalcea. 2007.
Unsupervised graph-based word sense
disambiguation using measures of word
semantic similarity. In Proceedings of the
IEEE International Conference on Semantic
Computing (ICSC 2007), pages 363?369,
Irvine, CA.
Snyder, B. and M. Palmer. 2004. The
English all-words task. In Proceedings of
SENSEVAL-3 Third International Workshop
on Evaluation of Systems for the Semantic
Analysis of Text, pages 41?43, Barcelona.
Strapparava, C., A. Gliozzo, and C. Giuliano.
2004. Pattern abstraction and term
similarity for word sense disambiguation:
IRST at SENSEVAL-3. In Proceedings of
SENSEVAL-3 Third International Workshop
on Evaluation of Systems for the Semantic
Analysis of Text, pages 229?234, Barcelona.
Surdeanu, M., M. Ciaramita, and
H. Zaragoza. 2008. Learning to rank
answers on large online QA collections.
In Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL/HLT?08), pages 719?727,
Columbus, OH.
Sussna, M. 1993. Word sense disambiguation
for free-text indexing using a massive
semantic network. In Proceedings of the
Second International Conference on
Information and Knowledge Management
(CIKM?93), pages 67?74, New York, NY.
Tratz, S., A. Sanfilippo, M. Gregory,
A. Chappell, C. Posse, and P. Whitney.
2007. PNNL: A supervised maximum
entropy approach to word sense
disambiguation. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007) in conjunction
with ACL, pages 264?267, Prague.
83
Computational Linguistics Volume 40, Number 1
Tsatsaronis, G., I. Varlamis, and K. N?rva?g.
2010. An experimental study on
unsupervised graph-based word
sense disambiguation. In Proceedings
of the 11th International Conference on
Computational Linguistics and Intelligent
Text Processing (CICLing?10),
pages 184?198, Iasi.
Tsatsaronis, G., M. Vazirgiannis, and
I. Androutsopoulos. 2007. Word sense
disambiguation with spreading activation
networks generated from thesauri.
In Proceedings of the 17th International
Joint Conference on Artificial Intelligence
(IJCAI?07), pages 1,725?1,730, Hyderabad.
Zhong, Z. and H. T. Ng. 2010. It makes
sense: A wide-coverage word sense
disambiguation system for free text.
In Proceedings of the ACL 2010 System
Demonstrations, pages 78?83, Uppsala.
Zhong, Z. and H. T. Ng. 2012. Word sense
disambiguation improves information
retrieval. In Proceedings of the 50th Annual
Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers),
pages 273?282, Jeju Island.
84
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 151?156,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PATHS: A System for Accessing Cultural Heritage Collections
Eneko Agirre?, Nikolaos Aletras?, Paul Clough?, Samuel Fernando?,
Paula Goodale?, Mark Hall?, Aitor Soroa? and Mark Stevenson?
(?) IXA NLP Group, University of the Basque Country
Manuel Lardizabal, 1, 20.018 Donostia, Basque Country
(?) Department of Computer Science, Sheffield University
211 Portobello, Sheffield S1 4DP, United Kingdom
Abstract
This paper describes a system for navigat-
ing large collections of information about
cultural heritage which is applied to Eu-
ropeana, the European Library. Euro-
peana contains over 20 million artefacts
with meta-data in a wide range of Euro-
pean languages. The system currently pro-
vides access to Europeana content with
meta-data in English and Spanish. The pa-
per describes how Natural Language Pro-
cessing is used to enrich and organise this
meta-data to assist navigation through Eu-
ropeana and shows how this information is
used within the system.
1 Introduction
Significant amounts of information about cultural
heritage has been digitised in recent years and is
now easily available through online portals. How-
ever, this vast amount of material can also be over-
whelming for many users since they are provided
with little or no guidance on how to find and inter-
pret this information. Potentially useful and rel-
evant content is hidden from the users who are
typically offered simple keyword-based searching
functionality as the entry point into a cultural her-
itage collection. The situation is very different
within traditional mechanisms for viewing cultural
heritage (e.g. museums) where artefacts are or-
ganised thematically and users guided through the
collection.
This paper describes a system that allows users
to explore large cultural heritage collections. Nav-
igation is based around the metaphor of pathways
(or trails) through the collection, an approach that
has been widely explored as an alternative to stan-
dard keyword-based search (Furuta et al, 1997;
Reich et al, 1999; Shipman et al, 2000; White and
Huang, 2010). Pathways are sets of artefacts or-
ganised around a theme which form access points
to the collection.
Pathways are a useful way to access informa-
tion about cultural heritage. Users accessing these
collections are often unfamiliar with their content,
making keyword-based search unsuitable since
they are unable to formulate appropriate queries
(Wilson et al, 2010). Non-keyword-based search
interfaces have been shown to be suitable for ex-
ploratory search (Marchionini, 2006). Pathways
support this exploration by echoing the organised
galleries and guided tours found in museums.
2 Related Work
Heitzman et al (1997) describe the ILEX system
which acts as a guide through the jewellery col-
lection of the National Museum of Scotland. The
user explores the collection through a set of web
pages which provide descriptions of each artefact
that are personalised for each user. The system
makes use of information about the artefacts the
user has viewed to build up a model of their in-
terests and uses this to customise the descriptions
of each artefact and provide recommendations for
further artefacts in which they may be interested.
Grieser et al (2007) also explore providing rec-
ommendations based on the artefacts a user has
viewed so far. They make use of a range of tech-
niques including language modelling, geospatial
modelling and analysis of previous visitors? be-
haviour to provide recommendations to visitors to
the Melbourne Museum.
Grieser et al (2011) explore methods for de-
termining the similarity between museum arte-
facts, commenting that this is useful for navigation
through these collections and important for per-
sonalisation (Bowen and Filippini-Fantoni, 2004;
O?Donnell et al, 2001), recommendation (Bohn-
ert et al, 2009; Trant, 2009) and automatic tour
generation (Finkelstein et al, 2002; Roes et al,
2009). They also use exhibits from Melbourne
151
Museum and apply a range of approaches to deter-
mine the similarity between them, including com-
paring descriptions and measuring physical dis-
tance between them in the museum.
These approaches, like many of the systems
that have been developed for online access to cul-
tural heritage (e.g. (Hage et al, 2010)), are based
around virtual access to a concrete physical space
(i.e. a museum). They often provide tours which
are constrained by the physical layout of the mu-
seum, such as virtual museum visits. However,
these approaches are less suitable for unstructured
collections such as databases of cultural heritage
artefacts collected from multiple institutions or
artefacts not connected with existing physical pre-
sentation (e.g. in a museum). The PATHS sys-
tem is designed for these types of collections and
makes use of natural language analysis to sup-
port navigation. In particular, similarity between
artefacts is computed automatically (see Section
4.1), background information automatically added
to artefact descriptions (see Section 4.2) and a hi-
erarchy of artefacts generated (see Section 4.3).
3 Cultural Heritage Data
The PATHS system has been applied to data from
Europeana1. This is a web-portal to collections
of cultural heritage artefacts provided by a wide
range of European institutions. Europeana cur-
rently provides access to over 20 million artefacts
including paintings, films, books, archival records
and museum objects. The artefacts are provided
by around 1,500 institutions which range from
major institutions, including the Rijksmuseum in
Amsterdam, the British Library and the Louvre,
to smaller organisations such as local museums.
It therefore contains an aggregation of digital con-
tent from several sources and is not connected with
any one physical museum.
The PATHS system makes use of three collec-
tions from Europeana. The first of these con-
tains artefacts from content providers in the United
Kingdom which has meta-data in English. The
artefacts in the remaining two collections come
from institutions in Spain and have meta-data in
Spanish.
CultureGrid Culture Grid2 is a digital content
provider service from the Collection Trust3.
1http://www.europeana.eu
2http://www.culturegrid.org.uk
3http://www.collectionstrust.org.uk
It contains information about over one mil-
lion artefacts from 40 different UK content
providers such as national and regional mu-
seums and libraries.
Cervantes Biblioteca Virtual Miguel De Cer-
vantes4 contains digitalised Spanish text in
various formats. In total, the online library
contains about 75,000 works from a range of
periods in Spanish history.
Hispana The Biblioteca Nacional de Espan?a5
contains information about a diverse set of
content including text and drawings. The ma-
terial is collected from different providers in
Spain including museums and libraries.
Europeana stores metadata for each artefact in
an XML-based format which includes information
such as its title, the digital format, the collection,
the year of creation and also a short description of
each artefact. However, this meta-data is created
by the content providers and varies significantly
across artefacts. Many of the artefacts have only
limited information associated with them, for ex-
ample a single word title. In addition, the content
providers that contribute to Europeana use differ-
ent hierarchical structures to organise their collec-
tions (e.g. Library of Congress Subject Headings6
and the Art and Architecture Thesaurus7), or do
not organise their content into any structure. Con-
sequently the various hierarchies that are used in
Europeana only cover some of the artefacts and
are not compatible with each other.
3.1 Filtering Data
Analysis of the artefacts in these three collections
revealed that many have short and uninformative
titles or lack a description. This forms a challenge
to language processing techniques since the arte-
fact?s meta-data does not contain enough informa-
tion to model it accurately.
The collections were filtered by removing any
artefacts that have no description and have either
fewer than four words in their title or have a title
that is repeated more than 100 times in the col-
lection. Table 1 shows the number of artefacts
in each of the Europeana collections before and
4http://www.cervantesvirtual.com
5http://www.bne.es
6http://authorities.loc.gov/
7http://www.getty.edu/research/tools/
vocabularies/aat/
152
after this filter has been applied. Applying the
heuristic leads to the removal of around 31% of the
artefacts, although the number varies significantly
across the collections with 61% of the artefacts in
CultureGrid being removed and only 1% of those
in Hispana.
Collection Lang. Total Filtered
CultureGrid Eng. 1,207,781 466,958
Hispana Sp. 1,235,133 1,219,731
Cervantes Sp. 19,278 14,983
2,462,192 1,701,672
Table 1: Number of artefacts in Europeana collec-
tions before and after filtering
4 Data Processing
A range of pre-preprocessing steps were carried
out on these collections to provide additional in-
formation to support navigation in the PATHS sys-
tem.
4.1 Artefact Similarity
We begin by computing the similarity between
the various artefacts in the Europeana collections.
This information is useful for navigation and rec-
ommendation but is not available in the Europeana
collections since they are drawn from a diverse
range of sources.
Similarity is computed using an approach de-
scribed by Aletras et al (2012). in which the top-
ics generated from each artefact?s metadata using
a topic model are compared. Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) is a widely used
type of topic model in which documents can be
viewed as probability distributions over topics, ?.
The similarity between a pair of documents can be
estimated by comparing their topic distributions.
This is achieved by viewing each distribution as
a vector of probabilities and then computing the
cosine of the angle between them:
sim(a, b) =
~?a.~?b
|~?a| ? | ~?b|
(1)
where ~?a is the vector created from the probability
distribution generated by LDA for document a.
This approach is evaluated using a set of 295
pairs of artefacts for which human judgements
of similarity were obtained using crowdsourcing
(Aletras et al, 2012). Pearson correlation between
the similarity scores and human judgements was
0.53.
The similarity between all the artefacts in the
collection is computed in a pairwise fashion. The
25 artefacts with the highest score are retained for
each artefact.
4.2 Background Links
The metadata associated with Europeana artefacts
is often very limited. Consequently links to rele-
vant articles in Wikipedia were added to each the
meta-data of each artefact using Wikipedia Miner
(Milne and Witten, 2008) to provide background
information. In addition to the link, Wikipedia
Miner returns a confidence value between 0 and
1 for each link based on the context of the item.
The accuracy of the links added by Wikipedia
Miner were evaluated using the meta-data associ-
ated with 21 randomly selected artefacts. Three
annotators analysed the links added and found that
a confidence value of 0.5 represented a good bal-
ance between accuracy and coverage. See Fer-
nando and Stevenson (2012) for further details.
4.3 Hierarchies
The range of hierarchies used by the various col-
lections that comprise the Europeana collection
make navigation difficult (see Section 3). Con-
sequently, the Wikipedia links added to the arte-
fact meta-data were used to automatically gener-
ate hierarchies that the cover the entire collection.
These hierarchies are used by the PATHS system
to assist browsing and exploration.
Two approaches are used to generate hierarchies
of Europeana artefacts (WikiFreq and WikiTax).
These are combined to generate the WikiMerge hi-
erarchy which is used in the PATHS system.
WikiFreq uses link frequencies across the en-
tire collection to organise the artefacts. The first
stage in the hierarchy generation process is to
compute the frequency with which each linked
Wikipedia article appears in the collection. The
links in each artefact are these analysed to con-
struct a hierarchy consisting of Wikipedia articles.
The links in the meta-data associated with each
artefact are ordered based on their frequency in the
entire collection and that set of links then inserted
into the hierarchy. For example, if the set of or-
dered links for an artefact is a1, a2, a3 ? ? ? an then
the artefact is then inserted into the hierarchy un-
der the branch a1 ? a2 ? a3 ? ? ? ? an, with
a1 at the top level in the tree and the artefact ap-
pearing under the node an. If this branch does not
already exist in the tree then it is created.
153
The hierarchy is pruned to removing nodes with
fewer than 20 artefacts in them. In addition, if a
node has more than 20 child nodes, only the 20
most frequent are used.
WikiTax uses the Wikipedia Taxonomy
(Ponzetto and Strube, 2011), a taxonomy derived
from Wikipedia categories. Europeana artefacts
are inserted into this taxonomy using the links
added by Wikipedia Miner with each artefact
being added to the taxonomy for all categories
listed in the links. This leads to a taxonomy in
which artefacts can occur in multiple locations.
Each approach was used to generate hierarchies
from the Europeana collections. The resulting hi-
erarchies were evaluated via online surveys, see
Fernando et al (2012) for further details. It was
found that WikiFreq performed well at placing
items into the correct location in the taxonomy and
grouping together similar items under the same
node. However, the overall structure of WikiTax
was judged to be more coherent and comprehensi-
ble.
WikiMerge combines combines WikiFreq and
WikiTax. WikiFreq is used to link each artefact
to Wikipedia articles a1 . . . an, but only the link
to the most specific article, an, is retained. The
an articles are linked to their parent WikiTax top-
ics based on the Wikipedia categories the articles
belong to. The resulting hierarchy is pruned re-
moving all WikiTax topics that do not have a Wik-
iFreq child or have only one child topic. Finally
top-level topics in the combined hierarchy are then
linked to their respective Wikipedia root node.
The resulting WikiMerge hierarchy has Wik-
iFreq topics as its leaves and WikiTax topics as
its interior and root nodes. Experiments showed
that this approach was successful in combining
the strengths of the two methods (Fernando et al,
2012).
5 The PATHS System
The PATHS system provides access to the Euro-
peana collections described in Section 3 by mak-
ing use of the additional information generated us-
ing the approaches described in Section 4. The in-
terface of the PATHS system has three main areas:
Paths enables users to navigate via pathways (see
Section 5.1).
Search supports discovery of both collection arte-
facts and pathways through keyword search
(see Section 5.2).
Explore enables users to explore the collections
using a variety of types of overview (see Sec-
tion 5.3).
5.1 Paths Area
This area provides users with access to Europeana
through pathways or trails. These are manually
generated sets of artefacts organised into a tree
structure which are designed to showcase the con-
tent available to the user in an organised way.
These can be created by users and can be pub-
lished for others to follow. An example path-
way on the topic ?railways? is shown in Figure
1. A short description of the pathway?s content is
shown towards the top of the figure and a graphical
overview of its contents at the bottom.
Figure 1: Example pathway on the topic ?rail-
ways?
Figure 2 shows as example artefact as displayed
in the system. The example artefact is a portrait
of Catherine the Great. The left side of the figure
shows information extracted directly from the Eu-
ropeana meta-data for this artefact. The title and
textual description are shown towards the top left
together with a thumbnail image of the artefact.
Other information from the meta-data is shown be-
neath the ?About this item? heading. The right
side of the figure shows additional information
Figure 2: Example artefact displayed in system in-
terface. Related artefacts and background links are
displayed on right hand side
154
Figure 3: Example visualisations of hierarchy: thesaurus view (top left), tag cloud (top right), map views
(bottom)
about the artefact generated using the approaches
described in Sections 4.1 and 4.2. Related arte-
facts are shown to the user one at a time, click-
ing on the thumbnail image leads to the equivalent
page for the related artefact. Below this are links
to the Wikipedia articles that are identified in the
text of the article?s title and description.
5.2 Search Area
This area allows users to search for artefacts and
pathways using standard keyword search imple-
mented using Lucene (McCandless et al, 2010).
5.3 Explore Area
The system provides a variety of ways to view
the hierarchies generated using the approach de-
scribed in Section 4.3. Figure 3 shows how these
are displayed for a section of the hierarchy with
the label ?Society?. The simplest view (shown in
the top left of Figure 3) is a thesaurus type view
in which levels of the hierarchy are represented by
indentation. The system also allows levels of the
hierarchy to be viewed as a tag cloud (top right of
Figure 3). The final representation of the hierar-
chy is as a map, shown in the bottom of Figure 3.
In this visualisation categories in the hierarchy are
represented as ?islands? on the map. Zooming in
on the map provides more detail about that area of
the hierarchy.
6 Summary and Future Developments
This paper describes a system for navigating Eu-
ropeana, an aggregation of collections of cultural
heritage artefacts. NLP analysis is used to organ-
ise the collection and provide additional informa-
tion. The results of this analysis are provided to
the user through an online interface which pro-
vides access to English and Spanish content in Eu-
ropeana.
Planned future development of this system in-
cludes providing recommendations and more per-
sonalised access. Similarity information (Sec-
tion 4.1) can be used to provide information from
which the recommendations can be made. Person-
alised access will make use of information about
individual users (e.g. from their browsing be-
haviour or information they provide about their
preferences) to generate individual views of Eu-
ropeana.
155
Online Demo
The PATHS system is available at
http://explorer.paths-project.eu/
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082
References
N. Aletras, M. Stevenson, and P. Clough. 2012. Com-
puting similarity between items in a digital library
of cultural heritage. Journal of Computing and Cul-
tural Heritage, 5(4):no. 16.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
F. Bohnert, D. Schmidt, and I. Zuckerman. 2009. Spa-
tial Process for Recommender Systems. In Proc. of
IJCAI 2009, pages 2022?2027, Pasadena, CA.
J. Bowen and S. Filippini-Fantoni. 2004. Personaliza-
tion and the Web from a Museum Perspective. In
Proc. of Museums and the Web 2004, pages 63?78.
Samuel Fernando and Mark Stevenson. 2012. Adapt-
ing Wikification to Cultural Heritage. In Proceed-
ings of the 6th Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Human-
ities, pages 101?106, Avignon, France.
Samuel Fernando, Mark Hall, Eneko Agirre, Aitor
Soroa, Paul Clough, and Mark Stevenson. 2012.
Comparing taxonomies for organising collections of
documents. In Proc. of COLING 2012, pages 879?
894, Mumbai, India.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Trans. on Information Systems, 20(1):116?131.
R. Furuta, F.. Shipman, C. Marshall, D. Brenner, and
H. Hsieh. 1997. Hypertext paths and the World-
Wide Web: experiences with Walden?s Paths. In
Proc. of the Eighth ACM conference on Hypertext,
pages 167?176, New York, NY.
K. Grieser, T. Baldwin, and S. Bird. 2007. Dynamic
Path Prediction and Recommendation in a Museum
Environment. In Proc. of the Workshop on Lan-
guage Technology for Cultural Heritage Data (LaT-
eCH 2007), pages 49?56, Prague, Czech Republic.
K. Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.
2011. Using Ontological and Document Similarity
to Estimate Museum Exhibit Relatedness. Journal
of Computing and Cultural Heritage, 3(3):1?20.
W.R. van Hage, N. Stash, Y. Wang, and L.M. Aroyo.
2010. Finding your way through the Rijksmuseum
with an adaptive mobile museum guide. In Proc. of
ESWC 2010, pages 46?59.
J. Heitzman, C. Mellish, and J. Oberlander. 1997. Dy-
namic Generation of Museum Web Pages: The In-
telligent Labelling Explorer. Archives and Museum
Informatics, 11(2):117?125.
G. Marchionini. 2006. Exploratory Search: from Find-
ing to Understanding. Comm. ACM, 49(1):41?46.
M. McCandless, E. Hatcher, and O. Gospodnetic.
2010. Lucene in Action. Manning Publications.
D. Milne and I. Witten. 2008. Learning to Link with
Wikipedia. In Proc. of CIKM 2008, Napa Valley,
California.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: An architecture for a dynamic hy-
pertext generation system. Natural Language En-
gineering, 7:225?250.
S.P. Ponzetto and M. Strube. 2011. Taxonomy in-
duction based on a collaboratively built knowledge
repository. Artificial Intelligence, 175(9-10):1737?
1756.
S. Reich, L. Carr, D. De Roure, and W. Hall. 1999.
Where have you been from here? Trails in hypertext
systems. ACM Computing Surveys, 31.
I. Roes, N. Stash, Y. Wang, and L. Aroyo. 2009. A
personalized walk through the museum: the CHIP
interactive tour guide. In Proc. of the 27th Interna-
tional Conference on Human Factors in Computing
Systems, pages 3317?3322, Boston, MA.
F. Shipman, R. Furuta, D. Brenner, C. Chung, and
H. Hsieh. 2000. Guided paths through web-based
collections: Design, experiences, and adaptations.
Journal of the American Society for Information Sci-
ence, 51(3):260?272.
J. Trant. 2009. Tagging, folksonomies and art mu-
seums: Early experiments and ongoing research.
Journal of Digital Information, 10(1).
R. White and J. Huang. 2010. Assessing the scenic
route: measuring the value of search trails in web
logs. In Proc. of SIGIR 2010, pages 587?594.
M. Wilson, Kulesm B., M. Schraefel, and B. Schnei-
derman. 2010. From keyword search to explo-
ration: Designing future search interfaces for the
web. Foundations and Trends in Web Science,
2(1):1?97.
156
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 417?420,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
Kyoto: An Integrated System for Specific Domain WSD
Aitor Soroa, Eneko Agirre, Oier Lopez de Lacalle
University of the Basque Country
a.soroa@ehu.es
Monica Monachini
Istituto di Linguistica Computazionale
monica.monachini@ilc.cnr.it
Jessie Lo, Shu-Kai Hsieh
National Taiwan Normal University
shukai@ntnu.edu.tw
Wauter Bosma, Piek Vossen
Vrije Universiteit
{p.vossen,w.bosma}@let.vu.nl
Abstract
This document describes the prelimi-
nary release of the integrated Kyoto sys-
tem for specific domain WSD. The sys-
tem uses concept miners (Tybots) to ex-
tract domain-related terms and produces
a domain-related thesaurus, followed by
knowledge-based WSD based on word-
net graphs (UKB). The resulting system
can be applied to any language with a
lexical knowledge base, and is based on
publicly available software and resources.
Our participation in Semeval task #17 fo-
cused on producing running systems for
all languages in the task, and we attained
good results in all except Chinese. Due
to the pressure of the time-constraints in
the competition, the system is still under
development, and we expect results to im-
prove in the near future.
1 Introduction
In this paper we describe the participation of the
integrated Kyoto system on the ?SemEval-2010
task #17: All-words Word Sense Disambigua-
tion on a Specific Domain? task (Agirre et al,
2010). The goal of our participation was to eval-
uate the preliminary release of the integrated sys-
tem for specific domain WSD developed for the
Kyoto project
1
. Besides, we wanted to test the
performance of our domain specific WSD system
(Agirre et al, 2009) on this test set, and to inte-
grate the thesaurus construction software (Tybots)
developed for the project. The system can be run
for any language and domain if provided with a
lexical knowledge base and some background doc-
uments on the domain.
We will first present the components of our sys-
tem, followed by the experimental design and the
1
http://www.kyoto-project.eu
results. Finally, the conclusions are presented.
2 The Kyoto System for Domain Specific
WSD
We will present in turn UKB, the Tybots, and the
lexical knowledge-bases used.
2.1 UKB
UKB is a knowledge-based unsupervised WSD
system which exploits the structure of an under-
lying Language Knowledge Base (LKB) and finds
the most relevant concepts given an input con-
text (Agirre and Soroa, 2009). UKB starts by tak-
ing the LKB as a graph of concepts G = (V,E)
with a set of vertices V derived from LKB con-
cepts and a set of edges E representing relations
among them. Giving an input context, UKB ap-
plies the so called Personalized PageRank (Haveli-
wala, 2002) over it to obtain the most representa-
tive senses for the context.
PageRank (Brin and Page, 1998) is a method
for scoring the vertices V of a graph according
to each node?s structural importance. The algo-
rithm can be viewed as random walk process that
postulate the existence of a particle that randomly
traverses the graph, but at any time may jump to
a new vertex with a given damping factor (also
called teleport probability). After PageRank cal-
culation, the final weight of node i represents the
proportion of time that a random particle spends
visiting node i after a sufficiently long time. In
standard PageRank, the teleport vector is chosen
uniformly, whereas for Personalized PageRank it
is chosen from a nonuniform distribution of nodes,
specified by a teleport vector.
UKB concentrates the initial probability mass
of the teleport vector in the words occurring in
the context of the target word, causing all random
jumps on the walk to return to these words and
thus assigning a higher rank to the senses linked to
these words. Moreover, the high rank of the words
417
spreads through the links in the graph and make
all the nodes in its vicinity also receive high ranks.
Given a target word, the system checks which is
the relative ranking of its senses, and the WSD
system would output the one ranking highest.
UKB is very flexible and can be use to perform
WSD on different settings, depending on the con-
text used for disambiguating a word instance. In
this paper we use it to perform general and do-
main specific WSD, as shown in section 3. PageR-
ank is calculated by applying an iterative algo-
rithm until convergence below a given threshold
is achieved. Following usual practice, we used a
damping value of 0.85 and set the threshold value
at 0.001. We did not optimize these parameters.
2.2 Tybots
Tybots (Term Yielding Robots) are text mining
software that mine domain terms from corpus
(e.g. web pages), organizing them in a hierar-
chical structure, connecting them to wordnets and
ontologies to create a semantic model for the do-
main (Bosma and Vossen, 2010). The software is
freely available using Subversion
2
. Tybots try to
establish a view on the terminology of the domain
which is as complete as possible, discovering rela-
tions between terms and ranking terms by domain
relevance.
Preceding term extraction, we perform tok-
enization, part-of-speech tagging and lemmatiza-
tion, which is stored in Kyoto Annotation For-
mat (KAF) (Bosma et al, 2009). Tybots work
through KAF documents, acquire domain relevant
terms based on the syntactic features, gather co-
occurrence statistics to decide which terms are sig-
nificant in the domain and produce a thesaurus
with sets of related words. Section 3.3 describes
the specific settings that we used.
2.3 Lexical Knowledge bases
We used the following wordnets, as suggested by
the organizers:
WN30g: English WordNet 3.0 with gloss relations
(Fellbaum, 1998).
Dutch: The Dutch LKB is part of the Cor-
netto database version 1.3 (Vossen et al, 2008).
The Cornetto database can be obtained from
the Dutch/Flanders Taalunie
3
. Cornetto com-
prises taxonomic relations and equivalence rela-
2
http://kyoto.let.vu.nl/svn/kyoto/trunk
3
http://www.inl.nl/nl/lexica/780
#entries #synsets #rels. #WN30g
Monolingual
Chinese 8,186 14,243 20,433 20,584
Dutch 83,812 70,024 224,493 83,669
Italian 46,724 49,513 65,567 52,524
WN30g 147,306 117,522 525,351 n/a
Bilingual
Chinese-eng 8,186 141,561 566,368
Dutch-eng 83,812 188,511 833,513
Italian-eng 46,724 167,094 643,442
Table 1: Wordnets and their sizes (entries, synsets,
relations and links to WN30g).
tions from both WordNet 2.0 and 3.0. Cornetto
concepts are mapped to English WordNet 3.0.
Italian: Italwordnet (Roventini et al, 2003) was
created in the framework of the EuroWordNet,
employs the same set of semantic relations used
in EuroWordNet, and includes links to WordNet
3.0 synsets.
Chinese: The Chinese WordNet (Version 1.6) is
now partially open to the public
4
(Tsai et al,
2001). The Chinese WordNet is also mapped to
WordNet 3.0.
Table 1 shows the sizes of the graphs created
using each LKB as a source. The upper part shows
the number of lexical entries, synsets and relations
of each LKB. It also depicts the number of links to
English WordNet 3.0 synsets.
In addition, we also created bilingual graphs for
Dutch, Italian and Chinese, comprising the orig-
inal monolingual LKB, the links to WordNet 3.0
and WordNet 3.0 itself. We expected this richer
graphs to perform better performance. The sizes
of the bilingual graphs are shown in the lower side
of Table 1.
3 Experimental setting
All test documents were lemmatized and PoS-
tagged using the linguistic processors available
within the Kyoto project. In this section we de-
scribe the submitted runs.
3.1 UKB parameters
We use UKB with the default parameters. In par-
ticular, we don?t use dictionary weights, which in
the case of English come from annotated corpora.
This is done in order to make the system fully un-
supervised. It?s also worth mentioning that in the
default setting parts of speech were not used.
4
http://cwn.ling.sinica.edu.tw
418
RANK RUN P R R-NOUN R-VERB
Chinese
- 1sense 0.562 0.562 0.589 0.518
1 Best 0.559 0.559 - -
- Random 0.321 0.321 0.326 0.312
4 kyoto-3 0.322 0.296 0.257 0.360
3 kyoto-2 0.342 0.285 0.251 0.342
5 kyoto-1 0.310 0.258 0.256 0.261
Dutch
1 kyoto-3 0.526 0.526 0.575 0.450
2 kyoto-2 0.519 0.519 0.561 0.454
- 1sense 0.480 0.480 0.600 0.291
3 kyoto-1 0.465 0.465 0.505 0.403
- Random 0.328 0.328 0.350 0.293
English
1 Best 0.570 0.555 - -
- 1sense 0.505 0.505 0.519 0.454
10 kyoto-2 0.481 0.481 0.487 0.462
22 kyoto-1 0.384 0.384 0.382 0.391
- Random 0.232 0.232 0.253 0.172
Italian
1 kyoto-3 0.529 0.529 0.530 0.528
2 kyoto-2 0.521 0.521 0.522 0.519
3 kyoto-1 0.496 0.496 0.507 0.468
- 1sense 0.462 0.462 0.472 0.437
- Random 0.294 0.294 0.308 0.257
Table 2: Overall results of our runs, including pre-
cision (P) and recall (R), overall and for each PoS.
We include the First Sense (1sense) and random
baselines, as well as the best run, as provided by
the organizers.
3.2 Run1: UKB using context
The first run is an application of the UKB tool in
the standard setting, as described in (Agirre and
Soroa, 2009). Given the input text, we split it in
sentences, and we disambiguate each sentence at a
time. We extract the lemmas which have an entry
in the LKB and then apply Personalized PageR-
ank over all of them, obtaining a score for every
concept of the LKB. To disambiguate the words in
the sentence we just choose its associated concept
(sense) with maximum score.
In our experiments we build a context of at least
20 content words for each sentence to be disam-
biguated, taking the sentences immediately before
when necessary. UKB allows two main methods
of disambiguation, namely ppr and ppr w2w. We
used the latter method, as it has been shown to per-
form best.
In this setting we used the monolingual graphs
for each language (cf. section 2.3). Note that
in this run there is no domain adaptation, it thus
serves us as a baseline for assessing the benefits of
applying domain adaptation techniques.
3.3 Run2: UKB using related words
Instead of disambiguating words using their con-
text of occurrence, we follow the method de-
scribed in (Agirre et al, 2009). The idea is to first
obtain a list of related words for each of the tar-
get words, as collected from a domain corpus. On
a second step each target word is disambiguated
using the N most related words as context (see
below). For instance, in order to disambiguate
the word environment, we would not take into
account the context of occurrence (as in Section
3.2), but we would use the list of most related
words in the thesaurus (e.g. ?biodiversity, agri-
culture, ecosystem, nature, life, climate, . . .?). Us-
ing UKB over these contexts we obtain the most
predominant sense for each target word in the do-
main(McCarthy et al, 2007), which is used to la-
bel all occurrences of the target word in the test
dataset.
In order to build the thesaurus with the lists of
related words, we used Tybots (c.f. section 2.2),
one for each corpus of the evaluation dataset, i.e.
Chinese, Dutch, English, and Italian. We used the
background documents provided by the organiz-
ers, which we processed using the linguistic pro-
cessors of the project to obtain the documents in
KAF. We used the Tybots with the following set-
tings. We discarded co-occurring words with fre-
quencies below 10
5
. Distributional similarity was
computed using (Lin, 1998). Finally, we used up
to 50 related words for each target word.
As in run1, we used the monolingual graphs for
the LKBs in each language.
3.4 Run3: UKB using related words and
bilingual graphs
The third run is exactly the same as run2, except
that we used bilingual graphs instead of monolin-
gual ones for all languages other than English (cf.
section 2.3). There is no run3 for English.
4 Results
Table 2 shows the results of our system on the
different languages. We will analyze different as-
pects of the results in turn.
Domain adaptation: Using Personalized Pager-
ank over related words (run2 and run3) con-
sistently outperforms the standard setting (run1)
in all languages. This result is consistent with
5
In the case of Dutch we did not use any threshold due to
the small size of the background corpus.
419
our previous work on English (Agirre et al,
2009), and shows that domain adaptation works
for knowledge-based systems.
Monolingual vs. Bilingual graphs: As ex-
pected, we obtained better results using the bilin-
gual graphs (run3) than with monolingual graphs
(run2), showing that the English WordNet has a
richer set of relations, and that those relations can
be successfully ported to other languages. This
confirms that aligning different wordnets at the
synset level is highly beneficial.
Overall results: the results of our runs are highly
satisfactory. In two languages (Dutch and Ital-
ian) our best runs perform better than the first
sense baseline, which is typically hard to beat for
knowledge-based systems. In English, our system
performs close but below the first sense baseline,
and in Chinese our method performed below the
random baseline.
The poor results obtained for Chinese can be
due the LKB topology; an analysis over the graph
shows that it is formed by a large number of
small components, unrelated with each other. This
?flat? structure heavily penalizes the graph based
method, which is many times unable to discrimi-
nate among the concepts of a word. We are cur-
rently inspecting the results, and we don?t discard
bugs, due to the preliminary status of our software.
In particular, we need to re-examine the output of
the Tybot for Chinese.
5 Conclusions
This paper describes the results of the prelimi-
nary release of he integrated Kyoto system for do-
main specific WSD. It comprises Tybots to con-
struct a domain-related thesaurus, and UKB for
knowledge-based WSD based on wordnet graphs.
We applied our system to all languages in the
dataset, obtaining good results. In fact, our sys-
tem can be applied to any language with a lexical
knowledge base, and is based on publicly available
software and resources. We used the wordnets and
background texts provided by the organizers of the
task.
Our results show that we were succesful in
adapting our system to the domain, as we man-
aged to beat the first sense baseline in two lan-
guages. Our results also show that adding the En-
glish WordNet to the other language wordnets via
the available links is beneficial.
Our participation focused on producing running
systems for all languages in the task, and we at-
tained good results in all except Chinese. Due to
the pressure and the time-constraints in the com-
petition, the system is still under development. We
are currently revising our system for bugs and fine-
tuning it.
Acknowledgments
This work task is partially funded by the Eu-
ropean Commission (KYOTO ICT-2007-211423),
the Spanish Research Department (KNOW-2
TIN2009-14715-C04-01) and the Basque Govern-
ment (BERBATEK IE09-262).
References
E. Agirre and A. Soroa. 2009. Personalizing pagerank for
word sense disambiguation. In Proceedings of EACL09,
pages 33?41. Association for Computational Linguistics.
E. Agirre, O. L?opez de Lacalle, and A. Soroa. 2009.
Knowledge-based wsd on specific domains: Performing
better than generic supervised wsd. In Proceedigns of IJ-
CAI. pp. 1501-1506.?.
E. Agirre, O. L?opez de Lacalle, C. Fellbaum, S.K. Hsieh,
M. Tesconi, M. Monachini, P. Vossen, and R. Segers.
2010. Semeval-2010 task 17: All-words word sense dis-
ambiguation on a specific domain. In Same volume.
W. E. Bosma and P. Vossen. 2010. Bootstrapping language
neutral term extraction. In Proceedings of LREC2010,
May.
W. E. Bosma, P. Vossen, A. Soroa, G. Rigau, M. Tesconi,
A. Marchetti, M. Monachini, and C. Aliprandi. 2009.
KAF: a generic semantic annotation format. In Proceed-
ings of the GL2009 Workshop on Semantic Annotation.
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual web search engine. Computer Networks and
ISDN Systems, 30(1-7).
C. Fellbaum. 1998. WordNet: An Electronical Lexical
Database. The MIT Press, Cambridge, MA.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In WWW
?02: Proceedings of the 11th international conference on
WWW, pages 517?526, New York, NY, USA. ACM.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of ACL98, Montreal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2007.
Unsupervised acquisition of predominant word senses.
Computational Linguistics, 33(4).
A. Roventini, A. Alonge, F. Bertagna, N. Calzolari, J. Can-
cila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza,
and A. Zampolli. 2003. Italwordnet: building a large
semantic database for the automatic treatment of Italian.
Linguistica Computazionale, Special Issue (XVIII-XIX),
pages 745?791.
B.S. Tsai, C.R. Huang, S.c. Tseng, J.Y. Lin, K.J. Chen, and
Y.S. Chuang. 2001. Definition and tests for lexical se-
mantic relations in Chinese. In Proceedings CLSW 2001.
P. Vossen, I. Maks, R. Segers, H. van der Vliet, and H. van
Zutphen. 2008. The cornetto database: the architecture
and alignment issues. In Proceedings GWC 2008, pages
485?506.
420
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 1?10,
Beijing, August 2010
KYOTO: an open platform for mining facts
Piek Vossen
VU University Amsterdam
p.vossen@let.vu.nl
German Rigau
Eneko Agirre
Aitor Soroa
University of the Basque 
Country
german.rigau/e.a-
girre/a.soroa@ehu.es
Monica Monachini
Roberto Bartolini
Istituto di Linguistica 
Computazionale, CNR
monica.monachini/r
oberto.bartolin-
i@ilc.cnr.it
Abstract
This  document  describes  an  open 
text-mining  system  that  was  developed 
for the Asian-European project KYOTO. 
The  KYOTO system uses  an  open text 
representation format and a central onto-
logy to  enable  extraction  of  knowledge 
and facts  from large volumes of text  in 
many different languages. We implemen-
ted a semantic tagging approach that per-
forms off-line reasoning. Mining of facts 
and  knowledge  is  achieved  through  a 
flexible pattern matching module that can 
work in much the same way for different 
languages,  can  handle  efficiently  large 
volumes of documents and is not restric-
ted to a specific domain. We applied the 
system to an English database on estuar-
ies.
1 Introduction
Traditionally, Information Extraction (IE) is the 
task of filling template information from previ-
ously unseen text which belongs to a predefined 
domain (Peshkin & Pfeffer 2003). Most systems 
in  the  Message  Understanding  Conferences 
(MUC,  1987-1998)  and the  Automatic  Content 
Extraction  program  (ACE)1 use  a  pipeline  of 
tools to achieve this, ranging from sophisticated 
NLP tools (like deep parsing) to shallower text-
processing (e.g. FASTUS (Appelt 1995)).
Standard  IE  systems  are  based  on  lan-
guage-specific  pattern  matching  (Kaiser  & 
1http://www.itl.nist.gov/iad/mig//tests/ace  
Miksch 2005), where each pattern consists of a 
regular  expression  and  an  associated  mapping 
from syntactic to logical form. In general, the ap-
proaches can be categorized into two groups: (1) 
the Knowledge Engineering approach (Appelt et 
al.1995), and (2) the learning approach, such as 
AutoSlog  (Appelt  et  al.  1993),  SRV  (Freitag 
1998), or RAPIER (Califf & R. Mooney 1999). 
Another  important  system  is  GATE (Cunning-
ham et al2002), which is a platform for creating 
IE systems. It uses regular expressions, but it can 
also  use  ontologies  to  perform semantic  infer-
ences  to  constrain  linguistic  patterns  semantic-
ally. The use of ontologies in IE is an emerging 
field (Bontcheva & Wilks 2004): linking text in-
stances with elements belonging to the ontology, 
instead of consulting flat gazetteers.
The major disadvantage of traditional IE sys-
tems is that they focus on satisfying precise, nar-
row, pre-specified requests from small homogen-
eous corpora (e.g., extract information about ter-
rorist events). Likewise, they are not flexible, are 
limited to specific types of knowledge and need 
to be built by knowledge engineers for each spe-
cific application and language. In fact most text 
mining  systems are  developed for  a  single  do-
main and a single language, and are not able to 
handle  knowledge  expressed  in  different  lan-
guages  or  expressed  and conceptualized  differ-
ently across cultures.
In this paper we describe an open platform for 
text-mining  or  IE that  can  be applied  to many 
different  languages  in  the  same  way  using  an 
open text representation system and a central on-
1
tology that  is  shared across  languages.  Ontolo-
gical implications are inserted in the text through 
off-line  reasoning and ontological  tagging.  The 
events and facts are extracted from large amounts 
of text using a flexible pattern-matching module, 
as specified by profiles  which comprise  ontolo-
gical and shallow linguistic patterns. The system 
is  developed  in  the  Asian-European  project 
KYOTO2.
In the next section,  we describe the general 
architecture of the KYOTO system. In section 3, 
we specify the knowledge structure that is used. 
Section  4,  describes  the  off-line  reasoning  and 
ontological tagging. In section 5, we describe the 
module for mining knowledge from the text that 
is enriched with ontological  statements.  Finally 
in section 6, we describe the first results of ap-
plying the system to databases on Estuaries.
2 KYOTO overview
The  KYOTO  project  allows  communities  to 
model terms and concepts in their domain and to 
use this knowledge to apply text mining on docu-
ments. The knowledge cycle in the KYOTO sys-
tem starts  with a set  of  source  documents pro-
duced by the community, such as PDFs and web-
sites.  Linguistic  processors  apply  tokenization, 
segmentation, morpho-syntactic analysis and  se-
mantic  processing  to  the  text  in  different  lan-
guages. The semantic processing involves the de-
tection of named-entities (persons, organizations, 
places,  time-expressions)  and  determining  the 
meaning of  words  in  the  text  according to  the 
given wordnet.  
The  output  of  the  linguistic  processors is 
stored in an XML annotation format that  is the 
same for  all  the languages,  called  the KYOTO 
Annotation  Format  (KAF,  Bosma  et  al  2009). 
This format incorporates standardized proposals 
for the linguistic annotation of text and represents 
them in an easy-to-use layered structure, which is 
compatible with the Linguistic Annotation Frame-
work  (LAF,  Ide  and  Romary  2003).  In  KAF, 
words, terms, constituents and syntactic depend-
encies  are  stored  in  separate  layers  with  refer-
ences across the structures. This makes it easier 
to harmonize the output of  linguistic processors 
2 Http://www.kyoto-project.eu
for different languages and to add new semantic 
layers to the basic output, when needed (Bosma 
et al 2009, Vossen et al 2010). All modules in 
KYOTO draw their input from these structures. 
In fact, the word-sense disambiguation process is 
carried out to the same KAF annotation in differ-
ent languages and is therefore the same for all the 
languages (Agirre et al 2009). In the current sys-
tem,  there  are  processors  for  English,  Dutch, 
Italian, Spanish, Basque, Chinese and Japanese.
The KYOTO system proceeds in 2 cycles (see 
Figure 1). In the 1st cycle, the Tybot (Term Yield-
ing Robot) extracts the most relevant terms from 
the documents. The Tybot is another generic pro-
gram that  can  do  this  for  all  the  different  lan-
guages in much the same way. The terms are or-
ganized as a structured hierarchy and, wherever 
possible,  related  to  generic  semantic  databases, 
i.e. wordnets for each language. In the left part of 
Figure 1, we show those terms in the input docu-
ment and their classification in wordnet. Terms in 
italics are present in the original wordnet, while 
underlined terms correspond to terms which were 
not in the original wordnet but were automatic-
ally discovered and linked to wordnet by Tybots. 
Straight  terms  correspond  to hyperonyms  in 
wordnet that do not necessarily occur in the text 
but are linked to ontological classes. The result of 
this  1st cycle  is a domain wordnet  for the target 
language.
The 2nd cycle of the system involves the actu-
al extraction of factual knowledge from the docu-
ments by the Kybots  (Knowledge Yielding Ro-
bots). Kybots use a collection of profiles that rep-
resent patterns of information of interest. In the 
profile, conceptual relations are expressed using 
ontological  and morpho-syntactic linguistic pat-
terns. Since the semantics is defined through the 
ontology,  it  is  possible  to  detect  similar  data 
across documents in different languages, even if 
expressed differently. In Figure 1, we give an ex-
ample of a conceptual pattern that relates organ-
isms that live in habitats. The Kybot can combine 
morpho-syntactic and semantic patterns. When a 
match is detected, the instantiation of the pattern 
is saved in a formal representation, either in KAF 
or in RDF. Since the wordnets in different lan-
guages are mapped to the same ontology and the 
text in these languages is represented in the same 
KAF,  similar  patterns  can  easily  be  applied  to 
multiple languages.
2
3 Ontological  and  lexical  background 
knowledge
As a semantic background model, we defined a 
3-layered  knowledge  architecture  following the 
principle  of  the  division  of  labour  (Putnam 
1975). In this model, the ontology does not need 
to be the central hub for all terms in a domain in 
all  languages.  Following the division  of labour 
principle, we can state that a computer does not 
need  to  distinguish  between  instances  of  a 
European Tree Frog and a Glass Tree frog. We 
assume  that  rigid  concepts  (as  defined  by 
Guarino and Welty 2002) are known to the do-
main experts and do not need to be defined form-
ally in the ontology but can remain in the avail-
able  background  resources,  such  as   databases 
with millions of species.  Terms in the documents 
are mostly non-rigid, e.g.  endangered frogs,  in-
vasive  frogs.  Such  non-rigid  terms  refer  to  in-
stances  of  species  in  contextual  circumstances. 
The processes and states are the important pieces 
of  information  that  matter  to the users  and are 
useful for mining text. The model therefore dis-
tinguishes between background vocabularies, do-
main terms,  wordnets and the central  ontology. 
The  background  vocabularies  are  automatically 
aligned  to  wordnet,  where  we  assume  that 
hyponymy relations to rigid synsets in wordnet 
declare those subconcepts as rigid subtypes too, 
without the necessity to include them in the onto-
logy.  For  non-rigid  terms,  we  defined  a  set  of 
mapping relations to the ontology through which 
we express their non-rigid involvement in these 
processes and states. Likewise, the ontology has 
been extended with processes and states for the 
domain  and  verbs  and  adjectives  have  been 
mapped to be able to detect expressions in text.
The  3-layered  knowledge  model  combines  the 
efforts from 3 different communities:
1.Domain  experts  in  social  communities  that 
continuously build background vocabularies;
2.Wordnet  specialists  that  define  the  basic  se-
mantic model for general concepts for a lan-
guage
3.Semantic Web specialists that define top-level 
and domain-specific ontologies that capture 
formal definitions of concepts;
We formalized the relations between these repos-
itories so that they can developed separately but 
combined within KYOTO to form a coherent and 
formal model.
3.1 Ontology
The KYOTO ontology currently consists of 1149 
classes divided over three layers. The top layer is 
based  on  DOLCE  (DOLCE-Lite-Plus  version 
3.9.7,  Masolo  et  al  2003)  and  OntoWordNet. 
This layer of the ontology has been modified for 
our purposes (Herold et. al. 2009).  The second 
layer consists of so-called Base Concepts (BCs) 
derived  from various  wordnets  (Vossen  1998, 
Izquierdo  et  al. 2007).  Examples  of  BCs  are: 
building,  vehicle,  animal,  plant,  change,  move,  
size, weight. The BCs are those synsets in Word-
Net 3.0 that have the most relations with other 
synsets in the wordnet hierarchies and are selec-
ted in a way that ensures complete coverage of 
the nominal and verbal part of WordNet. This has 
been  completed  for  the  nouns  (about  500 
synsets).  The ontology has also been adapted to 
include important concepts in the domain. Spe-
cial attention has been paid to represents the pro-
cesses  (perdurants)  in  which  objects  (endur-
ants)  of  the domain are  involved and qualities 
they may have. This is typically the information 
that is found in documents on the environment. 
We thus added 40 new event classes for repres-
enting  important  verbs  (e.g.  pollute, absorb, 
damage, drain) and 115 new qualities and qual-
ity-regions for representing important adjectives 
(e.g. airborne, acid, (un)healthy, clear). The full 
Figure 1: Two Cycles of processing in KYOTO
3
ontology can be downloaded from the KYOTO 
website, free for use. A considerable set of gener-
al verbs and adjectives (relevant for for the do-
main)  have  then  been  mapped  to  ontological 
classes: 189  verbal  synsets  and  222  adjectival 
synsets.
The  500  nominal  BCs  are  connected  to  the 
complete  WordNet  hierarchy,  whereas  the  189 
verbs represent 5,978 more specific verbal syn-
sets and the 222 adjectives represent  1,081 ad-
jectival synsets through the wordnet relations.
This basic ontology and the mapping to Word-
Net  are  used  to  model  the  shared  and  lan-
guage-neutral  concepts  and  relations  in the do-
main. Instances are excluded from the ontology. 
Instances will be detected in the documents and 
will be mapped to the ontology through instance 
to ontology relations (see below).  Likewise, we 
make a clear separation between the ontological 
model and the instantiation of the model as de-
scribed in the text.
3.2 Wordnet to ontology mappings
In addition to the ontology, we have wordnets for 
each language in the domain. In addition to the 
regular synset to synset relations in the wordnet, 
we will have a specific set of relations for map-
ping the synsets  to the ontology,  which are  all 
prefixed with sc_ standing for synset-to-concept. 
We differentiate between rigid and non-rigid con-
cepts in the wordnets through the mapping rela-
tions:
? sc_equivalenceOf: the synset is fully equi-
valent to the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_  subclassOf: the synset is a proper sub-
class of the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_domainOf: the synset is not a proper sub-
class  of  the  ontology  Type  &  is  not  disjoint 
(therefore orthogonal) with other synsets that are 
mapped to the same Type either through sc_sub-
classOf or sc_domainOf; the synset is non-Rigid 
but still inherits all properties of the target onto-
logy Type;  the synset  is  also related to a Role 
with a sc_playRole relation
? sc_playRole:  the  synset  denotes  instances 
for  which  the  context  of  the  Role  applies  for 
some period of time but this is not essential for 
the existence of the instances, i.e. if the context 
ceases to exist then the instances may still exist 
(Mizoguchi et al 2007).3
? sc_participantOf:  instances of the concept 
(denoted by the synset) participate in some en-
durant, where the specific role relation is indic-
ated by the playRole mapping. 
? sc_hasState: instances of the concept are in 
a particular state which is not essential and can 
be changed. There is no need to represent the role 
for a stative perdurant.
This model  extends  existing  WordNet  to  onto-
logy mappings.  For  instance,  in  the  SUMO to 
Wordnet mapping (Niles and Pease 2003), only 
the  sc_equivalenceOf and  sc_subclassOf rela-
tions  are  used,  represented  by  the symbols  ?=? 
and ?+? respectively. The SUMO-Wordnet map-
ping likewise does not systematically distinguish 
rigid from non-rigid  synsets.  In our  model,  we 
separate the linguistically and culturally specific 
vocabularies from the shared ontology while us-
ing the ontology  to interface  the concepts used 
by the various communities.
Using these mapping relations, we can express 
that the synset for  duck (which has a hypernym 
relation to the synset  bird, which, in its turn, has 
an  equivalence  relation  to  the  ontology  class 
bird) is  thus  a  proper  subclassOf  the  ontology 
class bird:
wn:duck hypernym wn:bird
wn:bird  sc_equivalenceOf ont:bird
For a concept such as migratory bird, which is 
also  a  hyponym of  bird in  wordnet  but  not  a 
proper subclass as a non-rigid concept, we thus 
create the following mapping:
wn:migratory bird 
? sc_domainOf ont:bird
? sc_playRole ont:done-by
? sc_participantOf ont:migration
This mapping indicates that the synset is used to 
refer to instances of endurants (not subclasses!), 
where the domain is restricted to birds. Further-
more, these instances participate in the process of 
3 Some terms involve more than one role,  e.g.  gas-
powered-vehicle.  Secondary  participants  are  related 
through  sc_hasCoParticipant and sc_playCoRole 
mappings.
4
migration in the role of  done-by. The properties 
of  the  process  migration are  further  defined  in 
the  ontology,  which  indicates  that  it  is  a  act-
ive-change-of-location  done-by  some  endurant, 
going from a source, via a path to some destina-
tion. The mapping relations from the wordnet to 
the ontology, need to satisfy the constraints of the 
ontology, i.e. only roles can be expressed that are 
compatible with the role-schema of the process 
in which they participate.
For  implied  non-essential  states,  we  use  the 
sc_hasState relation to express that a synset such 
as wild dog refers to instances of dogs that life in 
the wild but can stop being wild:
wn:wild dog ? sc_domainOf ont:dog
wn:wild dog ? sc_hasState ont:wild
Ideally, all processes and states that can be ap-
plied to endurants should be defined in the onto-
logy. This may hold for most verbs and adject-
ives in languages, which do not tend to extend in 
specific  domains  and  are  part  of  the  general 
vocabulary  (e.g.  to  pollute,  to  reduce,  wild). 
However, domain specific text contain many new 
nominal terms that refer to domain-specific pro-
cesses and states, e.g. air pollution, nitrogen pol-
lution,  nitrogen  reduction.  These  terms  are 
equally relevant as their counter-parts that refer 
to endurants involved in similar  processes, e.g. 
polluted air, polluting nitrogen or reduced nitro-
gen. We therefore use the reverse participant and 
role mappings to be able to define such terms for 
processes  as  subclasses  of  more  general  pro-
cesses  involving  specific  participants  in  a  spe-
cified role:
wn:air pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:air
? sc_hasRole ont:patient
wn:nitrogen pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:nitrogen
? sc_hasRole ont:done-by
 
Further  mapping  relations  are  described  in  the 
documentation on the KYOTO website. Through 
the mapping relations, we can keep the ontology 
relatively small and compact whereas we can still 
define  the  richness  of  the  vocabularies  of  lan-
guages in a precise way. The classes in the onto-
logy can be defined using rich axioms that model 
precise implications for inferencing. The wordnet 
to synset mappings can be used to define rather 
basic relations relative to the given ontology that 
still  captures  the  semantics  of  the  terms. The 
term definitions capture both relevance and per-
spective  (those  relations  that  matter  from  the 
point of the view of the term), on the one hand, 
and some semantics with respect to the concepts 
that are involved and their (role) relation on the 
other  hand.  Likewise,  the  KYOTO  system can 
model the linguistic and cultural diversity of lan-
guages in a domain but at the same time keep a 
firm anchoring to a basic and compact ontology.
3.3 Domain wordnet
We selected 3 representative documents on estu-
aries to extract relevant terms for the domain us-
ing the Tybot module. The terms have been re-
lated  through  structural  relations,  e.g.  nitrogen 
pollution is a hyponym of pollution, and through 
WordNet synsets that are assigned through WSD 
of the text.  We extracted 3950 candidate  terms 
form the KAF representations of the documents. 
Most of these are nouns (2818 terms). The nom-
inal  terms matched for 40% with wordnet syn-
sets, the verbs and adjectives for 98% and 85% 
respectively. For the domain wordnet, we restric-
ted ourselves to the nouns. From the new nomin-
al  terms,  environmentalists selected  390  terms 
that they deem to be important. These terms are 
connected to parent terms, which ultimately are 
connected to wordnet synsets.  The final domain 
wordnet contains 659 synsets: 197 synsets from 
the generic wordnet and 462 new synsets connec-
ted to the former.  The domain wordnet synsets 
got 990 mappings to the ontology, using the rela-
tions described in the previous section. There are 
86 synsets that have a sc_domainOf mapping, in-
dicating  that  they  are  non-rigid.  Note  that 
hyponyms of these synsets are also non-rigid by 
definition. These non-rigid synsets have complex 
mappings to processes and states in which  they 
are involved. The domain wordnet can be down-
loaded from the KYOTO website, free for use.
5
4 Off-line reasoning and ontological tag-
ging 
The ontological tagging represents the last phase 
in the KYOTO Linguistic  Processor  annotation 
pipeline.  It  consists  of  a three-step module  de-
vised to enrich the KAF documents with know-
ledge derived from the ontology. For each synset 
connected to a term, the first step   adds the Base 
Concepts to which the synset is related through 
the wordnet taxonomical relations. Then, through 
the synset to ontology mapping, it  adds the cor-
responding ontology type with appropriate rela-
tions. Once each synset is specified as to its onto-
logy type,  the  last  ontotagging  step  inserts  the 
full  set  of  ontological  implications  that  follow 
from the explicit ontology. The explicit ontology 
is a new data  structure consisting of a table with 
all  ontology nodes and all  ontological  implica-
tions expressed. The main purpose is to optimize 
<term lemma="pollution" pos="N" tid="t13444" type="open">
  <externalReferences>
   <externalRef reference="eng-30-00191142-n" reftype="baseConcept" resource="wn30g"/>
   <externalRef reference="Kyoto#change-eng-3.0-00191142-n" reftype="sc_subClassOf" resource="ontology">
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#contamination_pollution"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#accomplishment" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#event" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#part" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#specific-constant-constituent" reference="DOLCE-Lite.owl#perdurant" 
status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-quality" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#spatio-temporal-particular" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#participant" reference="DOLCE-Lite.owl#endurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-location_q" status="im-
plied"/>
    <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#particular" status="implied"/>
    </externalRef>
  </externalReferences>
</term>
Figure 2: An example of an OntoTagged output
<kprofile>
 <variables>
<var name="x" type="term" pos="N"/>
  <var name="y" type="term" 
       lemma="produce | generate | release | ! create"/>
  <var name="z" type="term"
       reference="DOLCE-Lite.owl#contamination_pollution"
       reftype="SubClassOf"/>
 </variables>
 <relations>
  <root span="y"/>
  <rel span="x" pivot="y" direction="preceding"/>
  <rel span="z" pivot="y" direction="following"/>
 </relations>
 <events>
  <event target="$y/@tid" lemma="$y/@lemma" pos="$y/@pos"/>
  <role target="$x/@tid" rtype="agent" lemma="$x/@lemma"/>
  <role target="$z/@tid" rtype="patient"lemma="$z/@lemma"/>$
 </events>
</kprofile>
Figure 3: An example of a Kybot profile
<kybotOut>
 <doc name="11767.mw.wsd.ne.onto.kaf">
  <event eid="e1" lemma="generate" pos="V" target="t3504"/>
  <role rid="r1" lemma="industry" rtype="agent" target="t3493" pos="N" event="e1"/>
  <role rid="r2" lemma="pollution" rtype="patient" target="t3495" pos="N" event="e1"/>
 </doc>
 <doc name="16266.mw.wsd.ne.onto.kaf">
  <event eid="e2" lemma="release" pos="V" target="t97"/>
  <role rid="r3" lemma="fuel" rtype="agent" target="t96" pos="N" event="e2"/>
  <role rid="r4" lemma="exhaust_gas" rtype="patient" target="t101" pos="V" event="e2"/>
 </doc>
</kybotOut>
Figure 4: An example of a Kybot output
6
the performance of the mining module over large 
quantities of documents. The advantage for Ky-
bots from ontotagging are many. First of all, they 
are  able  to  run  and  apply  pattern-matching  to 
Base  Concepts  and  ontological  classes  rather 
than just to words or synsets. Moreover, by mak-
ing explicit  the  implicit  ontological  statements, 
Kybots are able to find the same relations hidden 
in  different  expressions  with  different  surface 
realizations:  fish migration,  migratory  fish,  mi-
gration of fish, fishes that migrate, that directly 
or indirectly express the same relations. With on-
totagging,  they  share  the  same ontological  im-
plications which will allow Kybots to apply the 
same patterns and perform the extraction of facts. 
The implications will be represented in the same 
way across different languages, thus facilitating 
cross-lingual extraction of facts. Lastly, ontotag-
ging is a kind of off-line ontological reasoning: 
without  doing reasoning over concepts,  Kybots 
substantially  improve their  performance.  Figure 
2 shows the result of onto-tagging for the term 
pollution.
5 Event and fact extraction
Kybots (Knowledge Yielding Robots) are  com-
puter  programs  that  use  the  mined 
concepts and the generic  concepts  already con-
nected to the language wordnets and the KYOTO 
ontology to extract actual concept instances and 
relations in KAF documents. Kybots incorporate 
technology  for  the  extraction  of  relationships, 
either eventual or not, relative to the general or 
domain concepts already captured by the Tybots. 
That is, the extraction of factual knowledge is be-
ing carried out by the Kybot server by processing 
Kybot profiles on the linguistically enriched doc-
uments.
Kybots  are  defined  following  a  declarative 
format,  the  so  called  Kybot  
profiles, which describe general morpho-syntact-
ic  and  semantic  conditions  on  sequences  of 
terms. Profiles are compiled to generate the Ky-
bots, which scan over KAF documents searching 
for the patterns and extract the relevant informa-
tion from each matching.
Linguistic  patterns  include morphologic  con-
straints and also semantic conditions the matched 
terms must hold.  Kybot are thus able to search 
for term lemmas or part-of-speech tags but also 
for terms linked to ontological process and states 
using  the  mappings  described  in  Section  3.2. 
Thus, it is possible to detect similar eventual in-
formation  across  documents  in  different  lan-
guages, even if expressed differently.
5.1 Example of a Kybot Profile
Kybot Profiles are described using XML syn-
tax.  Figure 3 presents an example of a profile. 
Kybot profiles consist of three main parts: 
?Variable  declaration (<variables> element): 
In this section the search entities are defined. The 
example  defines  three  variables:  x (denoting 
terms  whose  part-of-speech is  noun),  y (which 
are  terms whose lemma is ?release?, ?produce? 
or  ?generate?  but   not  ?create?)  and  z (terms 
linked to  the  ontological  endurant  ?DOLCE-L-
ite.owl#contamination_pollution?, meaning ``be-
ing contaminated with harmful  substances''). 
?Declarations  of  the  relations  among  variables 
(<rel> element): specify the relations among the 
previously  defined variables.  The example pro-
file specifies y  as the main pivot, and states that 
variable  x must  be  preceding  variable  y in  the 
same sentence, and that variable  z must be fol-
lowing variable  y.  Thus,  the Kybot will  search 
for patterns like 'x ? y ? z' in a sentence.
?Output template (<events> element): describes 
the output to be produced on every matching. In 
the example, each match generates a new event 
targeting term  y,  which becomes the main term 
of the event. It also fills two roles of the event, 
the 'agent' role filled by term x and 'patient' role, 
filled by z. 
Figure  4  presents  the  output  of  the  Kybot 
when applied against the benchmark documents.
The Kybot output follows the stand-off architec-
ture when producing new information, and it thus 
forms  a  new KAF layer  on  the  original  docu-
ments.
6 Experimental results
We applied the KYOTO system and resources to 
English documents on estuaries. We collected 50 
URLs for two English estuaries: the Humber Es-
tuary in Hull (UK) and the Chesapeake Bay estu-
ary in the US and for background documents on 
bird  migration,  sedimentation,  habitat  destruc-
tion,  and  climate  change.  In  addition  to  the 
webpages, we extracted 815 PDF files from the 
sites. In total, 4625 files have been extracted. All 
7
the documents have been processed by the lin-
guistic  processor  for  English,  which  generated 
KAF representations for all the documents. From 
this  database,  3  documents  were  selected  for 
benchmarking.
The  documents  were  processed  by  applying 
multiword  tagging,  word-sense-disambiguation, 
named-entity-recognition  and  the  ontological 
tagging to the 3 documents and to the complete 
database; This was done twice: once without the 
domain model and once with the domain model. 
We thus created 4 datasets:  3 benchmark docu-
ments  processed  with  and  without  the  domain 
model; the complete database processed with and 
without the domain model.
Furthermore, we created Kybot profiles based 
on the type of information represented in the do-
main model. We applied the Kybots to all 4 data 
sets. We generate the following data files through 
an WN-LMF export of the domain wordnet:
1. a set of domain multiwords for the multi-
word tagger
2. an extension of the lexicon and the graph 
of  concepts  that  is  used  by  the  WSD 
module
3. an extension of the wordnet-to-ontology 
mappings for the ontotagger
In addition, we constructed mapping lists for all 
WordNet 3.0 synsets to Base Concepts and to ad-
jective and verbs that are matched to the onto-
logy.  These mappings provide the generic  con-
ceptual model based on wordnet and on the onto-
logy. 
Table 1 shows the effects of using the domain 
model for the first 3 modules. We can see that the 
domain  model  has  a  clear  effect  on  the multi-
word  detection  in  the  3  evaluation  documents. 
Using the domain model,  600 multiwords have 
been detected, against 145 with just the generic 
wordnet. This is obvious since the terms are ex-
tracted  from  the  same  documents.  However, 
when applying it  to the complete  database,  we 
see that  still  over 2,300 more multiwords have 
been  detected  using  the domain wordnet.  Note 
that the domain wordnet has only 97 multiwords 
and the generic wordnet has 19,126 multiwords. 
So 0.5% of the multiwords in the domain word-
net add 1.5 times more multiword tokens in the 
database. The third row specifies the number of 
synsets that have been assigned. We can see that 
for the domain model almost 400 more synsets 
have been detected. In the case of the full estuary 
database, we see that relatively few more have 
been detected, almost 1,500 while the database is 
80 times as big. If we look more closely at the 
numbers of actual  domain synsets detected,  we 
see the following results. In the benchmark docu-
ments  637 (or 5%) of  the synsets  is  a  domain 
wordnet  synset,  whereas  5,353 synsets  are  do-
main synsets in the full estuary database, which 
is only 0.52%. Note that in KAF multiwords are 
represented both as a single terms and in terms of 
their elements. The WSD module assigns synsets 
to  both.  The  domain  model  can  thus  only  add 
synsets compared to the processing without the 
domain. 
Finally, if we look at the named-entity-recogni-
tion module, we see a slight negative effect for 
the detection of named-entities due to the domain 
model.  The  named-entity-recognition  module 
does not consider the elements of multiwords but 
just  the multiword terms as a whole. Grouping 
terms  as  multiwords  thus  leads  to  less  named-
entities being detected. This is not necessarily a 
bad things, since the detection heavily over-gen-
erates and could have now more precision.
Table 1: Statistics on processing the estuary documents with and without domain model
bench mark documents (3) estuary documents (4742)
No Domain Domain No Domain Domain
terms 22,204 22,204 2,419,839 2,419,839
multiwords 145 600 4,389 6,671
12,526 12,910 1,021,598 1,023,017
158 126 41,681 40,714
67 66 10,288 10,233
synsets
ne location
ne date
8
Table 2 shows the effect of inserting ontologic-
al  implications  into  the  text  representation.For 
the benchmark documents, we see that more than 
half a million ontological implications have been 
inserted.  Of  these, 82% are implied references, 
that are extracted from the explicit ontology on 
the  basis  of  a  direct  mapping to  the  ontology. 
About  8% of  the  mappings  are  synset-to-onto-
logy mappings (sc) and 9.5% are mappings rep-
resenting the subclass hierarchy. The differences 
between using the domain model and not-using 
the domain model are minimal. For the complete 
database, the implications are 80 times as much 
but the proportions are similar.
Table 3 shows the type of sc-relations that oc-
cur.  Obviously,  sc_subClassOf  and  sc_equival-
entOf  are  the  most  frequent.  Nevertheless,  we 
still  find  about  500  mappings  that  present  the 
participation in a process or state. 
 
     30  reftype="sc_playCoRole"
     32  reftype="sc_hasCoParticipant"
     42  reftype="sc_partOf"
     59  reftype="sc_stateOf"
     92  reftype="sc_playRole"
     94  reftype="sc_hasRole"
     97  reftype="sc_participantOf"
   105  reftype="sc_hasParticipant"
   128  reftype="sc_domainOf"
   169  reftype="sc_hasState"
   312  reftype="sc_hasPart"
 3637  reftype="sc_equivalentOf"
42048  reftype="sc_subClassOf"
Table 3: Type of relations for the wordnet to ontology  
mappings using the domain model
The table clearly shows the impact of role rela-
tions  that  are  encoded  in  the  domain  wordnet. 
When  we  extract  the  mappings  for  the  files 
without the domain model (ony using the map-
pings to the generic wordnet), we get only equi-
valence and subclass mappings.
Finally to complete the knowledge cycle, we cre-
ated a few Kybot profiles for extracting events 
from the  onto-tagged  documents.  As  an  initial 
test, 3 profiles have been created:
1. events of destruction
2. destructions of locations
3. destruction of objects
Using  these  profiles,  we  extracted  211  events 
from the 3 benchmark documents with 396 roles. 
The profiles are created to run over the ontolo-
gical  types  inserted  by  the  ontotagger,  e.g.  re-
stricted to events and change_of_integrity.  Des-
pite the generality of the profiles, we still see a 
clear signature of the domain in the output. This 
is a good indication that we will be able to ex-
tract valuable events from the data, even though 
the  ontotagger  generates  a  massive  amount  of 
implications.  Especially  events  that  combine 
multiple  roles  appear  to  give  rich  information. 
For example, the following sentence:
"One of the greatest challenges to restoration is con-
tinued population growth and development, which 
destroys forests, wetlands and other natural areas"
yielded the following output:
   <event target="t1471" lemma="destroy" pos="V" 
eid="e74"/>
   <role target="t1477" rtype="patient" lemma="area" 
pos="N" event="e74" rid="r138"/>
   <role target="t1472" rtype="patient" 
lemma="forest" pos="N" event="e74" rid="r151"/>
   <role target="t1469" rtype="actor" lemma="devel-
opment" pos="N" event="e74" rid="r180"/>
Running the full set of profiles on the complete data-
base with almost 60 million ontological statements 
took about 2 hours. This shows that our approach is 
scalable and efficient.
Table 2: Ontological implications for the four data sets
bench mark documents (3) estuary documents (4272
No Domain Domain Domain
ontology references 555,677 576,432 48,708,300
implied ontology references 457,332 82.30% 474,916 82.39% 40,523,452 83.20%
direct ontology references 53,178 9.57% 54,769 9.50% 4,377,814 8.99%
45,167 8.13% 46,747 8.11% 3,807,034 7.82%domain synset to ontology mappings
9
7 Conclusions
In this paper, we described an open platform for 
text-mining  using wordnets  and a central  onto-
logy.  The  system  can  be  used  across  different 
languages and can be tailored to mine any type of 
conceptual relations. It can handle semantic im-
plications that are expressed in very different lin-
guistic expressions and yield systematic output. 
As future work, we will carry out benchmarking 
and testing of the mining of events, both for Eng-
lish and for the other languages in the KYOTO 
project.
Acknowledgements
The KYOTO project is co-funded by EU - FP7 
ICT Work Programme 2007 under Challenge 4 - 
Digital  libraries  and  Content,  Objective  ICT-
2007.4.2  (ICT-2007.4.4):  Intelligent  Contsent 
and Semantics  (challenge 4.2).  The Asian part-
ners from Tapei and Kyoto are funded from na-
tional funds. This work has been also supported 
by  Spanish  project  KNOW-2 (TIN2009-14715-
C04-01).
References
Agirre, E., & Soroa, A. (2009) Personalizing PageR-
ank for Word Sense Disambiguation. Proceedings 
of the 12th EACL, 2009. Athens, Greece. 
Agirre, E., Lopez de Lacalle, O., & Soroa, A. (2009) 
Knowledge-based WSD and specific domains: per-
forming over supervised WSD. Proceedings of IJ-
CAI. Pasadena, USA. http://ixa.si.ehu.es/ukb
?lvez J., Atserias J., Carrera J., Climent S., Laparra 
E., Oliver A. and Rigau G. (2008) Complete and 
Consistent  Annotation of  WordNet  using the Top 
Concept Ontology. Proceedings of LREC'08, Mar-
rakesh, Morroco. 2008.
Appelt Douglas E., Jerry R. Hobbs, John Bear, David 
Israel, Megumi Kameyama, Andrew Kehler, David 
Martin,  Karen Myers and Mabry Tyson. Descrip-
tion of the FASTUS System Used for MUC-6. In 
Proceedings  of  MUC-6,  pages  237?248.  San 
Mateo, Morgan Kaufmann, 1995.
Auer A., C. Bizer, G. Kobilarov, J. Lehmann, R. Cy-
ganiak and Z. Ives. DBpedia: A Nucleus for a Web 
of  Open  Data.  In  Proceedings  of  the
International  Semantic  Web  Conference  (ISWC), 
volume 4825 of  Lecture Notes  in Computer Sci-
ence, pages 722-735. 2007.
Bosma, W., Vossen, P., Soroa, A. , Rigau, G., Tesconi, 
M., Marchetti, A., Monachini, M., & Apiprandi, C. 
(2009) KAF: a generic semantic annotation format. 
In Proceedings of the 5th International Conference 
on Generative Approaches to the Lexicon Sept 17-
19, 2009, Pisa, Italy.
Fellbaum,  C.  (Ed.)  (1998)  WordNet:  An  Electronic 
Lexical Database. Cambridge, MA: MIT Press.
Freitag, D. (1998) Information extraction from html: 
Application  of  a  general  machine  learning  ap-
proach.  In  Proceedings  of  the  Fifteenth  National 
Conference on Artificial Intelligence, 1998.
Gangemi  A.,  Guarino  N.,  Masolo  C.,  Oltramari  A., 
Schneider  L.  (2002)  Sweetening  Ontologies  with 
DOLCE. Proceedings of EKAW. 2002
Ide, N. and L. Romary. 2003. Outline of the inter- na-
tional standard Linguistic Annotation Framework. 
In Proceedings of ACL 2003 Workshop on Lin-
guistic Annotation: Getting the Model Right, pages 
1?5.
Izquierdo R., Su?rez A. & Rigau G. Exploring the 
Automatic Selection of Basic Level Concepts. Pro-
ceedings of RANLP'07, Borovetz, Bulgaria. 
September, 2007.
Masolo, C., Borgo, S., Gangemi, A.,  Guarino, N. & 
Oltramari, A. (2003) WonderWeb Deliverable D18: 
Ontology Library, ISTC-CNR, Trento, Italy.
Mizoguchi R., Sunagawa E., Kozaki K. & Kitamura 
Y. (2007 A Model of Roles within an Ontology De-
velopment  Tool:  Hozo.  Journal  of  Applied  Onto-
logy, Vol.2, No.2, 159-179.
Niles, I. & Pease, A. (2001) Formal Ontology in In-
formation Systems. Proceedings of the internation-
al Conference on Formal Ontology in Information 
Systems ? Vol. 2001 Ogunquit, Maine,  USA
Niles, I. and A. Pease. Linking lexicons and ontolo-
gies:  Mapping  WordNet  to  the  Suggested  Upper 
Merged Ontology. In Proc. IEEE IKE, pages 412?
416, 2003.
Vossen, P. (Ed.) (1998) EuroWordNet: a multilingual 
database  with  lexical  semantic  networks  for 
European Languages. Kluwer, Dordrecht.
Vossen P., W. Bosma, E. Agirre, G. Rigau, A. Soroa 
(2010) A full Knowledge Cycle for Semantic Inter-
operability.  Proceedings  of  the  5th  Joint  ISO-
ACL/SIGSEM Workshop on Interoperable Semant-
ic Annotation, (ICGL 2010) Hong Kong, 2010.
10
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 94?100,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Enabling the Discovery of Digital Cultural Heritage Objects through
Wikipedia
Mark M Hall
Paul D Clough
Information School
Sheffield University
Sheffield, UK
m.mhall@shef.ac.uk
p.d.clough@shef.ac.uk
Oier Lopez de Lacalle1,2
1IKERBASQUE
Basque Foundation for Science
Bilbao, Spain
2School of Informatics
University of Edinburgh
Edinburgh, UK
oier.lopezdelacalle@gmail.es
Aitor Soroa
Eneko Agirre
IXA NLP Group
University of the Basque Country
Donostia, Spain
a.soroa@ehu.es
e.agirre@ehu.es
Abstract
Over the past years large digital cultural
heritage collections have become increas-
ingly available. While these provide ad-
equate search functionality for the expert
user, this may not offer the best support for
non-expert or novice users. In this paper
we propose a novel mechanism for intro-
ducing new users to the items in a collection
by allowing them to browse Wikipedia arti-
cles, which are augmented with items from
the cultural heritage collection. Using Eu-
ropeana as a case-study we demonstrate the
effectiveness of our approach for encourag-
ing users to spend longer exploring items
in Europeana compared with the existing
search provision.
1 Introduction
Large amounts of digital cultural heritage (CH)
information have become available over the past
years, especially with the rise of large-scale ag-
gregators such as Europeana1, the European ag-
gregator for museums, archives, libraries, and gal-
leries. These large collections present two chal-
lenges to the new user. The first is discovering
the collection in the first place. The second is
then discovering what items are present in the
collection. In current systems support for item
discovery is mainly through the standard search
paradigm (Sutcliffe and Ennis, 1998), which is
well suited for CH professionals who are highly
familiar with the collections, subject areas, and
have specific search goals. However, for new
users who do not have a good understanding of
what is in the collections, what search keywords
1http://www.europeana.eu
to use, and have vague search goals, this method
of access is unsatisfactory as this quote from
(Borgman, 2009) exemplifies:
?So what use are the digital libraries, if
all they do is put digitally unusable in-
formation on the web??
Alternative item discovery methodolo-
gies are required to introduce new users to
digital CH collections (Geser, 2004; Steem-
son, 2004). Exploratory search models
(Marchionini, 2006; Pirolli, 2009) that en-
able switching between collection overviews
(Hornb[Pleaseinsertintopreamble]k and Hertzum,
2011) and detailed exploration within the
collection are frequently suggested as more
appropriate.
We propose a novel mechanism that enables
users to discover an unknown, aggregated collec-
tion by browsing a second, known collection. Our
method lets the user browse through Wikipedia
and automatically augments the page(s) the user
is viewing with items drawn from the CH collec-
tion, in our case Europeana. The items are chosen
to match the page?s content and enable the user to
acquire an overview of what information is avail-
able for a given topic. The goal is to introduce
new users to the digital collection, so that they can
then successfully use the existing search systems.
2 Background
Controlled vocabularies are often seen as a
promising discovery methodology (Baca, 2003).
However, in the case of aggregated collections
such as Europeana, items from different providers
are frequently aligned to different vocabularies,
requiring an integration of the two vocabularies in
94
order to present a unified structure. (Isaac et al,
2007) describe the use of automated methods for
aligning vocabularies, however this is not always
successfully possible. A proposed alternative is
to synthesise a new vocabulary to cover all aggre-
gated data, however (Chaudhry and Jiun, 2005)
highlight the complexities involved in then link-
ing the individual items to the new vocabulary.
To overcome this automatic clustering and vi-
sualisations based directly on the meta-data have
been proposed, such as 2d semantic maps (An-
drews et al, 2001), automatically generated tree
structures (Chen et al, 2002), multi-dimensional
scaling (Fortuna et al, 2005; Newton et al, 2009),
self-organising maps (Lin, 1992), and dynamic
taxonomies (Papadakos et al, 2009). However
none of these have achieved sufficient success to
find widespread use as exploration interfaces.
Faceted search systems (van Ossenbruggen et
al., 2007; Schmitz and Black, 2008) have arisen
as a flexible alternative for surfacing what meta-
data is available in a collection. Unlike the meth-
ods listed above, faceted search does not require
complex pre-processing and the values to display
for a facet can be calculated on the fly. However,
aggregated collections frequently have large num-
bers of potential facets and values for these facets,
making it hard to surface a sufficiently large frac-
tion to support resource discovery.
Time-lines such as those proposed by (Luo et
al., 2012) do not suffer from these issues, but are
only of limited value if the user?s interest cannot
be focused through time. A user interested in ex-
amples of pottery across the ages or restricted to
a certain geographic area is not supported by a
time-line-based interface.
The alternative we propose is to use a second
collection that the user is familiar with and that
acts as a proxy to the unfamiliar collection. (Villa
et al, 2010) describe a similar approach where
Flickr is used as the proxy collection, enabling
users to search an image collection that has no
textual meta-data.
In our proposed approach items from the unfa-
miliar collection are surfaced via their thumbnail
images and similar approaches for automatically
retrieving images for text have been tried by (Zhu
et al, 2007; Borman et al, 2005). (Zhu et al,
2007) report success rates that approach the qual-
ity of manually selected images, however their
approach requires complex pre-processing, which
Figure 1: Architectural structure of the Wikiana sys-
tem
the dynamic nature of discovery prohibits.
Wikipedia was chosen as the discovery inter-
face as it is known to have good content cover-
age and frequently appears at the top of search
results (Schweitzer, 2008) for many topics, its
use has been studied (Lim, 2009; Lucassen and
Schraagen, 2010), and it is frequently used as
an information source for knowledge modelling
(Suchanek et al, 2008; Milne and Witten, 2008),
information extraction (Weld et al, 2009; Ni et
al., 2009), and similarity calculation (Gabrilovich
and Markovitch, 2007).
3 Discovering Europeana through
Wikipedia
As stated above our method lets users browse
Wikipedia and at the same time exposes them to
items taken from Europeana, enabling them to
discover items that exist in Europeana.
The Wikipedia article is augmented with Euro-
peana items at two levels. The article as a whole
is augmented with up to 20 items that in a pre-
processing step have been linked to the article and
at the same time each paragraph in the article is
augmented with one item relating to that para-
graph.
Our system (Wikiana, figure 1) sits between
the user and the data-providers (Wikipedia, Eu-
ropeana, and the pre-computed article augmenta-
tion links). When the user requests an article from
Wikiana, the system fetches the matching article
from Wikipedia and in a first step strips every-
thing except the article?s main content. It then
queries the augmentation database for Europeana
items that have been linked to the article and se-
lects the top 20 items from the results, as detailed
below. It then processes each paragraph and uses
95
Figure 2: Screenshot of the augmented article
?Mediterranean Sea? with the pre-processed article-
level augmentation at the top and the first two para-
graphs augmented with items as returned by the Euro-
peana API.
keywords drawn from the paragraphs (details be-
low) to query Europeana?s OpenSearch API for
items. A random item is selected from the result-
set and a link to its thumbnail image inserted into
the paragraph. The augmented article is then sent
to the user?s browser, which in turn requests the
thumbnail images from Europeana?s servers (fig.
2).
The system makes heavy use of caching to
speed up the process and also to reduce the
amount of load on the backend systems.
3.1 Article augmentation
To create the article-level augmentations we first
create a Wikipedia ?dictionary?, which maps
strings to Wikipedia articles. The mapping is cre-
ated by extracting all anchor texts from the inter-
article hyperlinks2 and mapping these to the ar-
ticles they link to. For instance, the string ?ro-
man coin? is used as an anchor in a link to the
Wikipedia article Roman currency3. Where
the same string points to multiple articles we se-
lect the most frequent article as the target. In the
case of ties an article is selected arbitrarily.
In a second step, we scan the subset of Eu-
ropeana selected for a European project, which
comprises SCRAN and Culture Grid collections
for English. The items in this sub-set are then
linked to Wikipedia articles. The sub-set of Euro-
2We used the 2008 Wikipedia dump to construct the dic-
tionary.
3http://en.wikipedia.org/wiki/Roman_
currency
<record>
<dc:identifier>http://www.kirkleesimage...</dc:identifier>
<dc:title>Roman Coins found in 1820..., Lindley</dc:title>
<dc:source>Kirklees Image Archive OAI Feed</dc:source>
<dc:language>EN-GB</dc:language>
<dc:subject>Kirklees</dc:subject>
<dc:type>Image</dc:type>
</record>
Figure 3: Example of an ESE record, some fields have
been omitted for clarity.
peana that was processed followed the Europeana
Semantic Elements (ESE) specifications4. Figure
3 shows an example of an ESE record describ-
ing a photograph of a Roman coin belonging to
the Kirklees Image Archive. We scan each ESE
record and try to match the ?dc:title? field with
the dictionary entries. In the example in figure
3, the item will be mapped to the Wikipedia ar-
ticle Roman currency because the string ?ro-
man coins? appears in the title.
As a result, we create a many-to-many mapping
between Wikipedia articles and Europeana items.
The Wikiana application displays at most 20 im-
ages per article, thus the Europeana items need to
be ranked. The goal is to rank interesting items
higher, with ?interestingness? defined as how un-
usual the items are in the collection. This metric
is an adaption of the standard inverse-document-
frequency formula used widely in Information
Retrieval and is adapted to identify items that have
meta-data field-values that are infrequent in the
collection. As in original IDF we diminish the
weight of values that occur very frequently in
the collection, the non-interesting items, and in-
creases the weight of values that occur rarely, the
interesting items. More formally the interesting-
ness ?i of an item i is calculated as follows:
?i =
#{titlei}
?title
log
Ntitle
c(titlei) + 1
+
#{desci}
?desc
log
Ndesc
c(desci) + 1
+
#{subji}
?subj
log
Nsubj
c(subji) + 1
where #{fieldi} is the length in words of the
field of the given item i, ?field is the average length
in words of the field in the collection, Nfield is the
total number of items containing that field in the
4http://version1.europeana.eu/web/
guest/technical-requirements
96
The Roman Empire (Latin: Imperium Romanum) was
the post-Republican period of the ancient Roman civ-
ilization, characterised by an autocratic form of gov-
ernment and large territorial holdings in Europe and
around the Mediterranean.
?Latin language? OR ?Ro-
man Republic? OR ?An-
cient Rome? or ?Autoc-
racy?
Figure 4: Example paragraph with the Wikipedia hy-
perlinks in bold. Below the search keywords extracted
from the hyperlinks and the resulting thumbnail image.
entire collection, and c(fieldi) is the frequency of
the value in that field.
Items are ranked by descending ?i and the for
the top 20 items, the thumbnails for the items are
added to the top of the augmented page.
3.2 Paragraph augmentation
The items found in the article augmentation tend
to be very focused on the article itself, thus to pro-
vide the user with a wider overview of available
items, each paragraph is also augmented. This
augmentation is done dynamically when an arti-
cle is requested. As stated above the augmen-
tation iterates over all paragraphs in the article
and for each article determines its core keywords.
As in the article augmentation the Wikipedia hy-
perlinks are used to define the core keywords, as
the inclusion of the link in the paragraph indi-
cates that this is a concept that the author felt was
relevant enough to link to. For each paragraph
the Wikipedia hyperlinks are extracted, the under-
scores replaced by spaces and these are then used
as the query keywords. The keywords are com-
bined using ?OR? and enclosed in speech-marks
to ensure only exact phrase matches are returned
and then submitted to Europeana?s OpenSearch
API (fig. 4). From the result set an item is ran-
domly selected and the paragraph is augmented
with the link to the item, the item?s thumbnail im-
age and its title. If there are no hyperlinks in a
paragraph or the search returns no results, then no
augmentation is performed for that paragraph.
4 Evaluation
The initial evaluation focuses on the paragraph
augmentation, as the quality of that heavily de-
pends on the results provided by Europeana?s API
and on a log-analysis looking at how users com-
Question Yes No
Familiar 18 18
Appropriate 9 27
Supports 4 32
Visually interesting 13 23
Find out more 3 33
Table 1: Evaluation experiment results reduced from
the 5-point Likert-like scale to a yes/no level.
ing to Europeana from Wikiana behave.
4.1 Paragraph augmentation evaluation
For the paragraph augmentation evaluation 18
wikipedia articles were selected from six topics
(Place, Person, Event, Time period, Concept, and
Work of Art). From each article the first para-
graph and a random paragraph were selected for
augmentation, resulting in a total set of 36 aug-
mented paragraphs. In the experiment interface
the participants were shown the text paragraph,
the augmented thumbnail image, and five ques-
tions (?How familiar are you with the topic??,
?How appropriate is the image??, ?How well does
the image support the core ideas of the para-
graph??, ?How visually interesting is the image??,
and ?How likely are you to click on the image
to find out more??). Each question used a five-
point Likert-like scale for the answers, with 1 as
the lowest score and 5 the highest. Neither the
topic nor the paragraph selection have a statisti-
cally significant influence on the results. To sim-
plify the analysis the results have been reduced
to a yes/no level, where an image is classified as
?yes? for that question if more than half the partic-
ipants rated the image 3 or higher on that question
(table 1).
Considering the simplicity of the augmentation
approach and the fact that the search API is not
under our control, the results are promising. 9
out of 36 (25%) of the items were classified as
appropriate. The non-appropriate images are cur-
rently being analysed to determine whether there
are shared characteristics in the query structure or
item meta-data that could be used to improve the
query or filter out non-appropriate result items.
The difficulty with automatically adding items
taken from Europeana is also highlighted by the
fact that only 13 of the 36 (36%) items were clas-
sified as interesting. While no correlation could
be found between the two interest and appro-
97
Category Sessions 1st q. Med 3rd q.
Wikiana 88 6 11 15.25
All users 577642 3 8 17
Table 2: Summary statistics for the number of items
viewed in per session for users coming from our sys-
tem (Wikiana) and for all Europeana users.
priate results, only one of the 23 uninteresting
items was judged appropriate, while 8 out of 9
of the appropriate items were also judged to be
interesting. We are now looking at whether the
item meta-data might allow filtering uninteresting
items, as they seem unlikely to be appropriate.
Additionally the approach taken by (Zhu et al,
2007), where multiple images are shown per para-
graph, is also being investigated, as this might re-
duce the impact of non-appropriate items.
4.2 Log analysis
Although the paragraph augmentation results are
not as good as we had hoped, a log analysis shows
that the system can achieve its goal of introduc-
ing new users to an unknown CH collection (Eu-
ropeana). The system has been available online
for three months, although not widely advertised,
and we have collected Europeana?s web-logs for
the same period. Using the referer information in
the logs we can distinguish users that came to Eu-
ropeana through our system from all other Euro-
peana users. Based on this classification the num-
ber of items viewed per session were calculated
(table 2). To prevent the evaluation experiment
influencing the log analysis only logs acquired be-
fore the experiment date were used.
Table 2 clearly shows that users coming
through our system exhibit different browsing pat-
terns. The first quartile is higher, indicating that
Wikiana users do not leave Europeana as quickly,
which is further supported by the fact that 30% of
the general users leave Europeana after viewing
three items or less, while for Wikiana users it is
only 19%. At the same time the third quartile is
lower, showing that Wikiana users are less likely
to have long sessions on Europeana. The differ-
ence in the session length distributions has also
been validated using a Kolmogorov-Smirnov test
(p = 0.00287, D = 0.1929).
From this data we draw the hypothesis that
Wikiana is at least in part successfully attracting
users to Europeana that would normally not visit
or not stay and that it successfully helps users
overcome that first hurdle that causes almost one
third of all Europeana users to leave after viewing
three or less items.
5 Conclusion and Future Work
Recent digitisation efforts have led to large dig-
ital cultural heritage (CH) collections and while
search facilities provide access to users familiar
with the collections there is a lack of methods for
introducing new users to these collections. In this
paper we propose a novel method for discover-
ing items in an unfamiliar collection by brows-
ing Wikipedia. As the user browses Wikipedia
articles, these are augmented with a number of
thumbnail images of items taken from the un-
known collection that are appropriate to the ar-
ticle?s content. This enables the new user to be-
come familiar with what is available in the collec-
tion without having to immediately interact with
the collection?s search interface.
An early evaluation of the very straightforward
augmentation process revealed that further work
is required to improve the appropriateness of the
items used to augment the Wikipedia articles. At
the same time a log analysis of Europeana brows-
ing sessions showed that users introduced to Eu-
ropeana through our system were less likely to
leave after viewing less than three items, pro-
viding clear indication that the methodology pro-
posed in this paper is successful in introducing
new users to a large, aggregated CH collection.
Future work will focus on improving the qual-
ity of the augmentation results by including more
collections into the article-level augmentation and
by introducing an ?interestingness? ranking into
the paragraph augmentation. We will also look at
evaluating the system in a task-based setting and
with existing, comparable systems.
Acknowledgements
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013)
under grant agreement n? 270082. We ac-
knowledge the contribution of all project part-
ners involved in PATHS (see: http://www.
paths-project.eu).
98
References
Keith Andrews, Christian Gutl, Josef Moser, Vedran
Sabol, and Wilfried Lackner. 2001. Search result
visualisation with xfind. In uidis, page 0050. Pub-
lished by the IEEE Computer Society.
Murtha Baca. 2003. Practical issues in applying meta-
data schemas and controlled vocabularies to cultural
heritage information. Cataloging & Classification
Quarterly, 36(3?4):47?55.
Christine L. Borgman. 2009. The digital future is
now: A call to action for the humanities. Digital
humanities quarterly, 3(4).
Andy Borman, Rada Mihalcea, and Paul Tarau. 2005.
Picnet: Augmenting semantic resources with pic-
torial representations. In Knowledge Collection
from Volunteer Contributors: Papers from the 2005
Spring Symposium, volume Technical Report SS-
05-03. American Association for Artificial Intelli-
gence.
Abdus Sattar Chaudhry and Tan Pei Jiun. 2005. En-
hancing access to digital information resources on
heritage: A case of development of a taxonomy at
the integrated museum and archives system in sin-
gapore. Journal of Documentation, 61(6):751?776.
Chaomei Chen, Timothy Cribbin, Jasna Kuljis, and
Robert Macredie. 2002. Footprints of information
foragers: behaviour semantics of visual exploration.
International Journal of Human-Computer Studies,
57(2):139 ? 163.
Blaz Fortuna, Marko Grobelnik, and Dunja Mladenic.
2005. Visualization of text document corpus. In-
formatica, 29:497?502.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings
of the 20th international joint conference on Artif-
ical intelligence, IJCAI?07, pages 1606?1611, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Guntram Geser. 2004. Resource discovery - position
paper: Putting the users first. Resource Discovery
Technologies for the Heritage Sector, 6:7?12.
Kasper Hornb?k and Morten Hertzum. 2011. The no-
tion of overview in information visualization. In-
ternational Journal of Human-Computer Studies,
69(7-8):509 ? 525.
Antoine Isaac, Stefan Schlobach, Henk Matthezing,
and Claus Zinn. 2007. Integrated access to cul-
tural heritage resources through representation and
alignment of controlled vocabularies. Library Re-
view, 67(3):187?199.
Sook Lim. 2009. How and why do college students
use wikipedia? Journal of the American Society for
Information Science and Technology, 60(11):2189?
2202.
Xia Lin. 1992. Visualization for the document space.
In Proceedings of the 3rd conference on Visualiza-
tion ?92, VIS ?92, pages 274?281, Los Alamitos,
CA, USA. IEEE Computer Society Press.
Teun Lucassen and Jan Maarten Schraagen. 2010.
Trust in wikipedia: how users trust information
from an unknown source. In Proceedings of the 4th
workshop on Information credibility, WICOW ?10,
pages 19?26, New York, NY, USA. ACM.
Dongning Luo, Jing Yang, Milos Krstajic, William
Ribarsky, and Daniel A. Keim. 2012. Eventriver:
Visually exploring text collections with temporal
references. Visualization and Computer Graphics,
IEEE Transactions on, 18(1):93 ?105, jan.
Gary Marchionini. 2006. Exploratory search: From
finding to understanding. Communications of the
ACM, 49(4):41?46.
David Milne and Ian H. Witten. 2008. Learning
to link with wikipedia. In Proceedings of the
17th ACM conference on Information and knowl-
edge management, CIKM ?08, pages 509?518, New
York, NY, USA. ACM.
Glen Newton, Alison Callahan, and Michel Dumon-
tier. 2009. Semantic journal mappiong for search
visualization in a large scale article digital library.
In Second Workshop on Very Large Digital Li-
braries at ECDL 2009.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng
Chen. 2009. Mining multilingual topics from
wikipedia. In Proceedings of the 18th international
conference on World wide web, WWW ?09, pages
1155?1156, New York, NY, USA. ACM.
Panagiotis Papadakos, Stella Kopidaki, Nikos Arme-
natzoglou, and Yannis Tzitzikas. 2009. Ex-
ploratory web searching with dynamic taxonomies
and results clustering. In Maristella Agosti,
Jose? Borbinha, Sarantos Kapidakis, Christos Pa-
patheodorou, and Giannis Tsakonas, editors, Re-
search and Advanced Technology for Digital Li-
braries, volume 5714 of Lecture Notes in Computer
Science, pages 106?118. Springer Berlin / Heidel-
berg.
Peter Pirolli. 2009. Powers of 10: Modeling
complex information-seeking systems at multiple
scales. Computer, 42(3):33?40.
Patrick L Schmitz and Michael T Black. 2008. The
delphi toolkit: Enabling semantic search for mu-
seum collections. In Museums and the Web 2008:
the international conference for culture and her-
itage on-line.
Nick J. Schweitzer. 2008. Wikipedia and psychology:
Coverage of concepts and its use by undergraduate
students. Teaching of Psychology, 35(2):81?85.
Michael Steemson. 2004. Digicult experts seek out
discovery technologies for cultural heritage. Re-
source Discovery Technologies for the Heritage
Sector, 6:14?20.
99
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203 ? 217. World Wide Web Conference
2007Semantic Web Track.
Alistair Sutcliffe and Mark Ennis. 1998. Towards a
cognitive theory of information retrieval. Interact-
ing with Computers, 10:321?351.
Jacco van Ossenbruggen, Alia Amin, Lynda Hard-
man, Michiel Hildebrand, Mark van Assem, Borys
Omelayenko, Guus Schreiber, Anna Tordai, Vic-
tor de Boer, Bob Wielinga, Jan Wielemaker, Marco
de Niet, Jos Taekema, Marie-France van Orsouw,
and Annemiek Teesing. 2007. Searching and an-
notating virtual heritage collections with semantic-
web technologies. In Museums and the Web 2007.
Robert Villa, Martin Halvey, Hideo Joho, David Han-
nah, and Joemon M. Jose. 2010. Can an interme-
diary collection help users search image databases
without annotations? In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
?10, pages 303?312, New York, NY, USA. ACM.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu.
2009. Using wikipedia to bootstrap open informa-
tion extraction. SIGMOD Rec., 37:62?68, March.
Xiaojin Zhu, Andrew B. Goldberg, Mohamed Eldawy,
Charles A. Dyer, and Bradley Strock. 2007. A text-
to-picture synthesis system for augmenting commu-
nication. In The integrated intelligence track of
the 22nd AAAI Conference on Artificial Intelligence
(AAAI-07).
100
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 31?34,
Dublin, Ireland, August 23, 2014.
Exploring the Use of Word Embeddings and Random Walks on Wikipedia
for the CogAlex Shared Task
Josu Goikoetxea, Eneko Agirre, Aitor Soroa
IXA NLP Group, University of the Basque Country, Basque Country
jgoicoechea009@ikasle.ehu.es, e.agirre@ehu.es, a.soroa@ehu.es
Abstract
In our participation on the task we wanted to test three different kinds of relatedness algorithms:
one based on embeddings induced from corpora, another based on random walks on WordNet
and a last one based on random walks based on Wikipedia. All three of them perform similarly
in noun relatedness datasets like WordSim353, close to the highest reported values. Although
the task definition gave examples of nouns, the train and test data were based on the Edinburgh
Association Thesaurus, and around 50% of the target words were not nouns. The corpus-based
algorithm performed much better than the other methods in the training dataset, and was thus
submitted for the test.
1 Introduction
Measuring semantic similarity and relatedness between terms is an important problem in lexical seman-
tics (Budanitsky and Hirst, 2006). It has applications in many natural language processing tasks, such as
Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like
Information Retrieval. Most of the proposed techniques are evaluated over manually curated word simi-
larity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems
for word pairs are compared with human ratings.
The techniques used to solve this problem can be roughly classified into two main categories: those
relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias)
(Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre
et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman,
2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013).
Our main objective when participating in the CogAlex shared task was to check how a sample of each
kind of technique would cope with the task. We thus selected one of the best corpus-based models to
date and another approach based on random walks over WordNet and Wikipedia.
2 Word Embeddings
Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A
lot of models have been developed, but all of them share two characteristics: they learn meaning from
non-labeled corpora and represent meaning in a distributional way. These models learn the meaning of
words from corpora, and they represent it distributionally by the so-called embeddings. This embeddings
are low-dimensional and dense vectors composed by integers, where the dimensions are latent semantic
features of words.
We have used the Mikolov model (Mikolov et al., 2013) for this task, due to its effectiveness in
similarity experiments (Baroni et al., 2014). This neural network reduces the computational complexity
of previous architectures by deleting the hidden layer, and also, it?s able to train with larger corpora (more
than 10
9
words) and extract embeddings with larger dimensionality.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
31
The Mikolov model has two variants: Continuous Bag of Words (CBOW) and Skip-gram. The first
one is quite similar to the feedforward Neural Net Language Model, but instead of a hidden layer it has
a projection layer; so, all the words are projected in the same position. Word order has thus no influence
in the projection. Training criterion is as follows: knowing past and future words, it will predict the one
in the middle.
The Skip-gram model is related to the previous one. The main difference is that it uses each current
word as an input to a log-linear classifier with a continuous projection layer, and predicts words within a
certain range before and after the current word.
In order to participate in this shared task, we have used the word2vec tool
1
. On the one hand, we
have used embeddings trained with the Skip-gram model on part of Google News corpus (about 100
billion words). The vectors have 300 dimensions and are publicly available
2
. On the other hand, we have
adapted the distance program in word2vec, so that its input is the test-set file of the shared task. The
way distance works is as follows:
? Reads all the vectors from the embeddings file, and stores them in memory.
? Reads the test-set file, and line by line
? Saves the five entry words if they exist in vocabulary.
? Dimension by dimension, sums five entries? embeddings into one vector, and normalizes it.
? Calculates the semantic distance from the normalized vector to all words in vocabulary, and
selects the closest ones.
? Writes in output file the closest words along with their distances, writing the closest word first.
3 Random Walks on Wikipedia
In the last year there have been many attempts to apply graph based techniques to many NLP problems,
including word sense disambiguation (Agirre et al., 2014) or measuring semantic similarity and related-
ness between terms (Agirre et al., 2009). Those techniques consider a given Knowledge Base (KB) as a
graph, where vertices represent KB concepts and relations among concepts are represented by edges.
For this particular task we represented WikiPedia as a graph, where articles are the vertices and
links between articles are the edges. Contrary to other work using Wikipedia links (Gabrilovich and
Markovitch, 2007; Milne and Witten, 2008), the use of the whole graph allows to apply algorithms that
take into account the whole structure of Wikipedia. We applied PageRank and Personalized PageRank
on the Wikipedia graph using freely available software (Agirre and Soroa, 2009; Agirre et al., 2014)
3
.
The PageRank algorithm (Brin and Page, 1998) ranks the vertices in a graph according to their relative
structural importance. The main idea of PageRank is that whenever a link from v
i
to v
j
exists in a graph,
a vote from node i to node j is produced, and hence the rank of node j increases. Besides, the strength of
the vote from i to j also depends on the rank of node i: the more important node i is, the more strength
its votes will have. Alternatively, PageRank can also be viewed as the result of a random walk process,
where the final rank of node i represents the probability of a random walk over the graph ending on node
i, at a sufficiently large time. Personalized PageRank (Haveliwala, 2002) is a variant of the PageRank
algorithm which biases the computation to prefer certain nodes of the graph.
Our method also needs a dictionary, an association between strings and Wikipedia articles. We con-
struct the dictionary using article titles, redirections, disambiguation pages, and anchor text extracted
from a Wikipedia dump
4
. Mentions are lowercased and all text between parenthesis is removed. If the
mention links to a disambiguation page, it is associated with all possible articles the disambiguation page
points to. Each association between a string and article is scored with the prior probability, estimated as
the number of times that the mention occurs in the anchor text of an article divided by the total number
of occurrences of the mention.
1
http://word2vec.googlecode.com/svn/trunk/
2
https:\/\/docs.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\&export=download
3
http://ixa2.si.ehu.es/ukb
4
we used a 2013 Wikipedia dump to build the dictionary
32
The method to compute the answer for a given set of words is very simple. We just compute the
Personalized PageRank algorithm over Wikipedia, initializing the walk using the set of given words,
obtaining a probability distribution over all Wikipedia articles. We then choose the article with maximum
probability, and return the title of the article as the expected answer.
Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calcu-
lation after 30 iterations. Some preliminary experiments on a related Word Sense Disambiguation task
indicated that the algorithm was quite robust to these values, and we did not optimize them.
4 Development results
After running the random walks algorithms on the development data, it was clear that WordNet and
Wikipedia were not sufficient resources for the task, and they were performing poorly. The embeddings,
on the other hand, were doing a good job (accuracy of 14.1%, having returned a word on 1907 of the
2000 train instances). This is in contradiction with the results obtained in word relatedness datasets: for
instance, in the WordSim353 dataset (Gabrilovich and Markovitch, 2007) we obtain Spearman correla-
tions of 68.5 using random walks on WordNet, 72.8 using random walks on Wikipedia, and 71.0 using
the embeddings.
One important difference between datasets like WordSim353 and the CogCalex data, is that in Word-
Sim353, all words are nouns in singular. From a small sample of the CogaLex training data, on the
contrary, we saw that only around 50% of the target words
5
are nouns, with many occurrences of gram-
matical words, and words in plural. Wikipedia only contains nouns, and even if WordNet contains verbs
and adjectives, the semantic relations that we use are not able to check whether a meaning should be
lexicalized as an adjective (absent in the dataset) or noun (absence). Note also that the random walk
algorithm does not use co-occurrence data, and as such it is not able to capture that absent and minded
are closely related as in ?absent minded?.
These differences between the WordSim 353 and the CogaLex data would explain the different be-
haviour of the algorithms. We would also like to mention that the definition of the task mentioned
examples which are closer to the capabilities of WordNet and Wikipedia (e.g. given a set of words like
?gin, drink, scotch, bottle and soda? the expected answer would be whisky). From the definition of the
task, it looked as if the task was about recovering a word given a definition (as in dictionaries), but the
actual data was based on the Edinburgh Association Thesaurus, which is a different kind of resource.
5 Test results
Given the development much better results of the embeddings, we submitted a run based on those. We
obtained 16.35% accuracy, ranking fourth in the evaluation of all twelve submissions.
6 Conclusions
We tested three different kinds of relatedness algorithms: one based on embeddings induced from cor-
pora, another based on random walks on WordNet and a last one based on random walks based on
Wikipedia. All three of them perform similarly in noun relatedness datasets like WordSim353. Although
the task definition gave examples of content nouns alone, the train and test data were based on the Ed-
inburgh Association Thesaurus, and only around 50% of the target words were nouns. The embedding
performed much better than the other methods in this dataset.
Acknowledgements
This material is based in part upon work supported by MINECO, in the scope of the CHIST-ERA READ-
ERS (PCIN-2013-002-C02-01) and SKATER (TIN2012-38584-C06-02) projects.
5
The words that need to be predicted.
33
References
E. Agirre and A. Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of 14th
Conference of the European Chapter of the Association for Computational Linguistics, Athens, Greece.
E. Agirre, A. Soroa, E. Alfonseca, K. Hall, J. Kravalova, and M. Pasca. 2009. A Study on Similarity and Relat-
edness Using Distributional and WordNet-based Approaches. In Proceedings of annual meeting of the North
American Chapter of the Association of Computational Linguistics (NAAC), Boulder, USA, June.
E. Agirre, M. Cuadros, G. Rigau, and A. Soroa. 2010. Exploring Knowledge Bases for Similarity. In Proceedings
of the Seventh conference on International Language Resources and Evaluation (LREC?10), Valletta, Malta,
May. European Language Resources Association (ELRA).
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense
disambiguation. Computational Linguistics, 40(1):57?88.
M.A. Alvarez and S.J. Lim. 2007. A Graph Modeling of Semantic Similarity between Words. Proceedings of the
Conference on Semantic Computing, pages 355?362.
Marco Baroni, Georgiana Dinu, and Germn Kruszewski. 2014. Don?t count, predict! A systematic comparison of
context-counting vs. context-predicting semantic vectors. In Proceedings of ACL.
D. Bollegala, Matsuo Y., and M. Ishizuka. 2007. Measuring Semantic Similarity between Words using Web
Search Engines. In Proceedings of WWW?2007.
S. Brin and L. Page. 1998. The Anatomy of a Large-scale Hypertextual Web Search Engine. In Proceedings of the
seventh international conference on World Wide Web 7, WWW7, pages 107?117, Amsterdam, The Netherlands,
The Netherlands. Elsevier Science Publishers B. V.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Com-
putational Linguistics, 32(1):13?47.
H. Chen, M. Lin, and Y. Wei. 2006. Novel Association Measures using Web Search with Double Checking. In
Proceedings of COCLING/ACL 2006.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing Search
in Context: The Concept Revisited. ACM Transactions on Information Systems, 20(1):116?131.
E Gabrilovich and S Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic
analysis. In Proceedings of IJCAI 2007, pages 1606?1611, Hyderabad, India.
T.H. Haveliwala. 2002. Topic-sensitive PageRank. In Proceedings of the 11th international conference on World
Wide Web (WWW?02), pages 517?526, New York, NY, USA.
T. Hughes and D. Ramage. 2007. Lexical Semantic Relatedness with Random Graph Walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581?589.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word repre-
sentations. In Proceedings of NAACL-HLT, pages 746?751.
D. Milne and I.H. Witten. 2008. An effective, low-cost measure of semantic relatedness obtained from wikipedia
links. In In Proceedings of the first AAAI Workshop on Wikipedia and Artifical Intellegence (WIKIAI?08),
Chicago, I.L.
M. Sahami and T.D. Heilman. 2006. A Web-based Kernel Function for Measuring the Similarity of Short Text
Snippets. Proc. of WWW, pages 377?386.
D. Yang and D.M.W. Powers. 2005. Measuring Semantic Similarity in the Taxonomy of WordNet. Proceedings
of the Australasian conference on Computer Science.
34
