Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 27?35,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Paraphrase Recognition Using Machine Learning to Combine Similarity
Measures
Prodromos Malakasiotis
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
Abstract
This paper presents three methods that can
be used to recognize paraphrases. They
all employ string similarity measures ap-
plied to shallow abstractions of the input
sentences, and a Maximum Entropy clas-
sifier to learn how to combine the result-
ing features. Two of the methods also ex-
ploit WordNet to detect synonyms and one
of them also exploits a dependency parser.
We experiment on two datasets, the MSR
paraphrasing corpus and a dataset that we
automatically created from the MTC cor-
pus. Our system achieves state of the art
or better results.
1 Introduction
Recognizing or generating semantically equiva-
lent phrases is of significant importance in many
natural language applications. In question answer-
ing, for example, a question may be phrased dif-
ferently than in a document collection (e.g., ?Who
is the author of War and Peace?? vs. ?Leo Tol-
stoy is the writer of War and Peace.?), and taking
such variations into account can improve system
performance significantly (Harabagiu et al, 2003;
Harabagiu and Hickl, 2006). A paraphrase gener-
ator, meaning a module that produces new phrases
or patterns that are semantically equivalent (or al-
most equivalant) to a given input phrase or pattern
(e.g., ?X is the writer of Y ? ? ?X wrote Y ? ?
?Y was written by X?? ?X is the author of Y ?,
or ?X produces Y ? ? ?X manufactures Y ? ?
?X is the manufacturer of Y ?) can be used to pro-
duce alternative phrasings of the question, before
matching it against a document collection.
Unlike paraphrase generators, paraphrase rec-
ognizers decide whether or not two given phrases
(or patterns) are paraphrases, possibly by general-
izing over many different training pairs of phrases.
Paraphrase recognizers can be embedded in para-
phrase generators to filter out erroneous generated
paraphrases; but they are also useful on their own.
In question answering, for example, they can be
used to check if a pattern extracted from the ques-
tion (possibly by replacing named entities by their
semantic categories and turning the question into
a statement) matches any patterns extracted from
candidate answers. As a further example, in text
summarization, especially multi-document sum-
marization, a paraphrase recognizer can be used
to check if a sentence is a paraphrase of any other
sentence already present in a partially constructed
summary.
Note that, although ?paraphrasing? and ?textual
entailment? are sometimes used as synonyms, we
use the former to refer to methods that generate
or recognize semantically equivalent (or almost
equivalent) phrases or patterns, whereas in textual
entailment (Dagan et al, 2006; Bar-Haim et al,
2006; Giampiccolo et al, 2007) the expressions or
patterns are not necessarily semantically equiva-
lent; it suffices if one entails the other, even if the
reverse direction does not hold. For example, ?Y
was written by X? textually entails ?Y is the work
of X?, but the reverse direction does not neces-
sarily hold (e.g., if Y is a statue); hence, the two
sentences are not paraphrases.
In this paper, we focus on paraphrase recogni-
tion. We propose three methods that employ string
similarity measures, which are applied to several
abstractions of a pair of input phrases (e.g., the
phrases themselves, their stems, POS tags). The
scores returned by the similarity measures are used
as features in a Maximum Entropy (ME) classifier
(Jaynes, 1957; Good, 1963), which learns to sepa-
rate true paraphrase pairs from false ones. Two of
our methods also exploit WordNet to detect syn-
onyms, and one of them uses additional features
to measure similarities of grammatical relations
27
obtained by a dependency parser.
1
Our experi-
ments were conducted on two datasets: the pub-
licly available Microsoft Research Paraphrasing
corpus (Dolan et al, 2004) and a dataset that we
constructed from the MTC corpus.
2
The experi-
mental results show that our methods perform very
well. Even the simplest one manages to achieve
state of the art results, even though it uses fewer
linguistic resources than other reported systems.
The other two, more elaborate methods perform
even better.
Section 2 presents the three methods, and sec-
tion 3 our experiments. Section 4 covers related
work. Section 5 concludes and proposes further
work.
2 The three methods
The main idea underlying our methods is that by
capturing similarities at various shallow abstrac-
tions of the input (e.g., the original sentences, the
stems of their words, their POS tags), we can rec-
ognize paraphrases and textual entailment reason-
ably well, provided that we learn to assign ap-
propriate weights to the resulting features. Fur-
ther improvements are possible by recognizing
synonyms and by employing similarity measures
that operate on the output of dependency grammar
parsers.
2.1 Method 1 (INIT)
During training, the first method, called INIT, is
given a set {?S
1,1
, S
1,2
, y
1
? , . . . , ?S
n,1
, S
n,2
, y
n
?},
where S
i,1
and S
i,2
are sentences (more gener-
ally, phrases), y
i
= 1 (positive class) if the
two sentences are paraphrases, and y
i
= ?1
(negative class) otherwise. Each pair of sen-
tences ?S
i,1
, S
i,2
? is converted to a feature vec-
tor ~v
i
, whose values are scores returned by sim-
ilarity measures that indicate how similar S
i,1
and S
i,2
are at various levels of abstraction.
The vectors and the corresponding categories
{?~v
1
, y
i
? , . . . , ? ~v
n
, y
n
?} are given as input to the
ME classifier, which learns how to classify new
vectors ~v, corresponding to unseen pairs of sen-
tences ?S
1
, S
2
?.
We use nine string similarity measures: Leven-
shtein distance (edit distance), Jaro-Winkler dis-
tance, Manhattan distance, Euclidean distance, co-
1
We use Stanford University?s ME classifier and parser;
see http://nlp.stanford.edu/.
2
The corpus is available by the LDC, Catalogue Number
LDC2002T01, ISBN 1-58563-217-1.
sine similarity, n-gram distance (with n = 3),
matching coefficient, Dice coefficient, and Jac-
card coefficient. To save space, we do not repeat
the definitions of the similarity measures here,
since they are readily available in the literature
and they are also summarized in our previous work
(Malakasiotis and Androutsopoulos, 2007).
For each pair of input strings ?S
1
, S
2
?, we form
ten new pairs of strings
?
s
1
1
, s
1
2
?
, . . . ,
?
s
10
1
, s
10
2
?
corresponding to ten different levels of abstraction
of S
1
and S
2
, and we apply the nine similarity
measures to the ten new pairs, resulting in a to-
tal of 90 measurements. These measurements are
then included as features in the vector ~v that cor-
responds to ?S
1
, S
2
?. The
?
s
i
1
, s
i
2
?
pairs are:
?
s
1
1
, s
1
2
?
: two strings consisting of the original tokens of S
1
and S
2
, respectively, with the original order of the to-
kens maintained;
3
?
s
2
1
, s
2
2
?
: as in the previous case, but now the tokens are
replaced by their stems;
?
s
3
1
, s
3
2
?
: as in the previous case, but now the tokens are
replaced by their part-of-speech (POS) tags;
?
s
4
1
, s
4
2
?
: as in the previous case, but now the tokens are
replaced by their soundex codes;
4
?
s
5
1
, s
5
2
?
: two strings consisting of only the nouns of S
1
and
S
2
, as identified by a POS-tagger, with the original or-
der of the nouns maintained;
?
s
6
1
, s
6
2
?
: as in the previous case, but now with nouns re-
placed by their stems;
?
s
7
1
, s
7
2
?
: as in the previous case, but now with nouns re-
placed by their soundex codes;
?
s
8
1
, s
8
2
?
: two strings consisting of only the verbs of S
1
and
S
2
, as identified by a POS-tagger, with the original or-
der of the verbs maintained;
?
s
9
1
, s
9
2
?
: as in the previous case, but now with verbs re-
placed by their stems;
?
s
10
1
, s
10
2
?
: as in the previous case, but now with verbs re-
placed by their soundex codes.
Note that the similarities are measured in terms
of tokens, not characters. For instance, the edit
distance of S
1
and S
2
is the minimum number of
operations needed to transform S
1
to S
2
, where an
operation is an insertion, deletion or substitution
of a single token. Moreover, we use high-level
3
We use Stanford University?s tokenizer and POS-tagger,
and Porter?s stemmer.
4
Soundex is an algorithm intended to map English names
to alphanumeric codes, so that names with the same pronun-
ciations receive the same codes, despite spelling differences;
see http://en.wikipedia.org/wiki/Soundex.
28
POS tags only, i.e., we do not consider the num-
ber of nouns, the voice of verbs etc.; this increases
the similarity of positive
?
s
3
1
, s
3
2
?
pairs.
A common problem is that the string similar-
ity measures may be misled by differences in the
lengths of S
1
and S
2
. This is illustrated in the fol-
lowing examples, where the underlined part of S
1
is much more similar to S
2
than the entire S
1
.
S
1
: While Bolton apparently fell and was immobilized,
Selenski used the mattress to scale a 10-foot, razor-wire
fence, Fischi said.
S
2
: After the other inmate fell, Selenski used the mattress
to scale a 10-foot, razor-wire fence, Fischi said.
To address this problem, when we consider a
pair of strings ?s
1
, s
2
?, if s
1
is longer than s
2
, we
obtain all of the substrings s
?
1
of s
1
that have the
same length as s
2
. Then, for each s
?
1
, we compute
the nine values f
j
(s
?
1
, s
2
), where f
j
(1 ? j ? 9)
are the string similarity measures. Finally, we lo-
cate the s
?
1
with the best average similarity (over
all similarity measures) to s
2
, namely s
??
1
:
s
??
1
= argmax
s
?
1
10
?
j=1
f
j
(s
?
1
, s
2
)
and we keep the nine f
j
(s
??
1
, s
2
) values and their
average as ten additional measurements. Simi-
larly, if s
2
is longer than s
1
, we keep the nine
f
j
(s
1
, s
??
2
) values and their average. This process
is applied to pairs
?
s
1
1
, s
1
2
?
, . . . ,
?
s
4
1
, s
4
2
?
, where
large length differences are more likely to appear,
adding 40 more measurements (features) to the
vector ~v of each ?S
1
, S
2
? pair of input strings.
The measurements discussed above provide 130
numeric features.
5
To those, we add two Boolean
features indicating the existence or absence of
negation in S
1
or S
2
, respectively; negation is de-
tected by looking for words like ?not?, ?won?t?
etc. Finally, we add a length ratio feature, de-
fined as
min(L
S
1
,L
S
2
)
max(L
S
1
,L
S
2
)
, where L
S
1
and L
S
2
are the
lengths, in tokens, of S
1
and S
2
. Hence, there is a
total of 133 available features in INIT.
2.2 Method 2 (INIT+WN)
Paraphrasing may involve using synonyms which
cannot be detected by the features we have con-
sidered so far. In the following pair of sentences,
for example, ?dispatched? is used as a synonym
5
All feature values are normalized in [?1, 1]. We use our
own implementation of the string similarity measures.
of ?sent?; treating the two verbs as the same to-
ken during the calculation of the string similarity
measures would yield a higher similarity. The sec-
ond method, called INIT+WN, treats words from
S
1
and S
2
that are synonyms as identical; other-
wise the method is the same as INIT.
S
1
: Fewer than a dozen FBI agents were dispatched to se-
cure and analyze evidence.
S
2
: Fewer than a dozen FBI agents will be sent to Iraq to
secure and analyze evidence of the bombing.
2.3 Method 3 (INIT+WN+DEP)
The features of the previous two methods op-
erate at the lexical level. The third method,
called INIT+WN+DEP, adds features that operate
on the grammatical relations (dependencies) a de-
pendency grammar parser returns for S
1
and S
2
.
We use three measures to calculate similarity at
the level of grammatical relations, namely S
1
de-
pendency recall (R
1
), S
2
dependency recall (R
2
)
and their F -measure (F
R
1
,R
2
), defined below:
R
1
=
|common dependencies|
|S
1
dependencies|
R
2
=
|common dependencies|
|S
2
dependencies|
F
R
1
,R
2
=
2?R
1
?R
2
R
1
+R
2
The following two examples illustrate the use-
fulness of dependency similarity measures in de-
tecting paraphrases. In the first example S
1
and S
2
are not paraphrases and the scores are low, while in
the second example where S
1
and S
2
have almost
identical meanings, the scores are much higher.
Figures 1 and 2 lists the grammatical relations (de-
pendencies) of the two sentences with the common
ones shown in bold.
Example 1:
S
1
: Gyorgy Heizler, head of the local disaster unit, said the
coach was carrying 38 passengers.
S
2
: The head of the local disaster unit, Gyorgy Heizler, said
the coach driver had failed to heed red stop lights.
R
1
= 0.43, R
2
= 0.32, F
R
1
,R
2
= 0.36
Example 2:
S
1
: Amrozi accused his brother, whom he called ?the wit-
ness?, of deliberately distorting his evidence.
S
2
: Referring to him as only ?the witness?, Amrozi accused
his brother of deliberately distorting his evidence.
R
1
= 0.69, R
2
= 0.6, F
R
1
,R
2
= 0.64
29
Grammatical relations of S1 Grammatical relations of S2 
mod(Heizler-2, Gyorgy-1) mod(head-2, The-1) 
arg(said-11, Heizler-2) arg(said-12, head-2) 
mod(head-2, of-3) mod(Heizler-2, head-4) 
mod(head-4, of-5) mod(unit-7, the-4) 
mod(unit-9, the-6) mod(unit-7, local-5) 
mod(unit-9, local-7) mod(unit-7, disaster-6) 
mod(unit-9, disaster-8) arg(of-3, unit-7) 
arg(of-5, unit-9) mod(Heizler-10, Gyorgy-9) 
mod(coach-13, the-12) mod(unit-7, Heizler-10) 
arg(carrying-15, coach-13) mod(driver-15, the-13) 
aux(carrying-15, was-14) mod(driver-15, coach-14) 
arg(said-11, carrying-15) arg(failed-17, driver-15) 
mod(passengers-17, 38-16) aux(failed-17, had-16) 
arg(said-12, failed-17) arg(carrying-15, passengers-17) 
aux(heed-19, to-18) 
arg(failed-17, heed-19) 
mod(lights-22, red-20) 
mod(lights-22, stop-21) 
arg(heed-19, lights-22)
 
Figure 1: Grammatical relations of example 1.
Grammatical relations of S1 Grammatical relations of S2 
arg(accused-2, Amrozi-1) dep(accused-12, Referring-1) 
mod(brother-4, his-3) mod(Referring-1, to-2) 
arg(accused-2, brother-4) arg(to-2, him-3) 
arg(called-8, whom-6) cc(him-3, as-4) 
arg(called-8, he-7) dep(as-4, only-5) 
mod(witness-8, the-7) mod(brother-4, called-8) 
mod(witness-11, the-10) conj(him-3, witness-8) 
arg(accused-12, Amrozi-11) dep(called-8, witness-11) 
mod(brother-4, of-14) mod(brother-14, his-13) 
mod(distorting-16, deliberately-15) arg(accused-12, brother-14) 
arg(of-14, distorting-16) mod(brother-14, of-15) 
mod(evidence-18, his-17) mod(distorting-17, deliberately-16) 
arg(distorting-16, evidence-18) arg(of-15, distorting-17) 
mod(evidence-19, his-18) 
arg(distorting-17, evidence-19) 
 
Figure 2: Grammatical relations of example 2.
30
As with POS-tags, we use only the highest level
of the tags of the grammatical relations, which in-
creases the similarity of positive pairs of S
1
and
S
2
. For the same reason, we ignore the direction-
ality of the dependency arcs which we have found
to improve the results. INIT+WN+DEP employs a
total of 136 features.
2.4 Feature selection
Larger feature sets do not necessarily lead to im-
proved classification performance. Despite seem-
ing useful, some features may in fact be too noisy
or irrelevant, increasing the risk of overfitting the
training data. Some features may also be redun-
dant, given other features; thus, feature selection
methods that consider the value of each feature on
its own (e.g., information gain) may lead to sub-
optimal feature sets.
Finding the best subset of a set of available fea-
tures is a search space problem for which several
methods have been proposed (Guyon et al, 2006).
We have experimented with a wrapper approach,
whereby each feature subset is evaluated accord-
ing to the predictive power of a classifier (treated
as a black box) that uses the subset; in our exper-
iments, the predictive power was measured as F -
measure (defined below, not to be confused with
F
R
1
,R
2
). More precisely, during feature selection,
for each feature subset we performed 10-fold cross
validation on the training data to evaluate its pre-
dictive power. After feature selection, the classi-
fier was trained on all the training data, and it was
evaluated on separate test data.
With large feature sets, an exhaustive search
over all subsets is intractable. Instead, we ex-
perimented with forward hill-climbing and beam
search (Guyon et al, 2006). Forward hill-climbing
starts with an empty feature set, to which it adds
features, one at a time, by preferring to add at each
step the feature that leads to the highest predic-
tive power. Forward beam search is similar, except
that the search frontier contains the k best exam-
ined states (feature subsets) at each time; we used
k = 10. For k = 1, beam search reduces to hill-
climbing.
3 Experiments
We now present our experiments, starting from a
description of the datasets used.
3.1 Datasets
We mainly used the Microsoft Research (MSR)
Paraphrasing Corpus (Dolan et al, 2004), which
consists of 5,801 pairs of sentences. Each pair
is manually annotated by two human judges as a
true or false paraphrase; a third judge resolved dis-
agreements. The data are split into 4,076 training
pairs and 1,725 testing pairs.
We have experimented with a dataset we created
from the MTC corpus. MTC is a corpus containing
news articles in Mandarin Chinese; for each article
11 English translations (by different translators)
are also provided. We considered the translations
of the same Chinese sentence as paraphrases. We
obtained all the possible paraphrase pairs and we
added an equal number of randomly selected non
paraphrase pairs, which contained sentences that
were not translations of the same sentence. In this
way, we constructed a dataset containing 82,260
pairs of sentences. The dataset was then split in
training (70%) and test (30%) parts, with an equal
number of positive and negative pairs in each part.
3.2 Evaluation measures and baseline
We used four evaluation measures, namely accu-
racy (correctly classified pairs over all pairs), pre-
cision (P , pairs correctly classified in the positive
class over all pairs classified in the positive class),
recall (R, pairs correctly classified in the positive
class over all true positive pairs), and F -measure
(with equal weight on precision and recall, defined
as
2?P ?R
P+R
). These measures are not to be confused
with the R
1
, R
2
, and F
R
1
,R
2
of section 2.3 which
are used as features.
A reasonable baseline method (BASE) is to use
just the edit distance similarity measure and a
threshold in order to decide whether two phrases
are paraphrases or not. The threshold is chosen
using a grid search utility and 10-fold cross vali-
dation on the training data. More precisely, in a
first step we search the range [-1, 1] with a step
of 0.1.
6
In each step, we perform 10-fold cross
validation and the value that achieves the best F -
measure is our initial threshold, th, for the second
step. In the second step, we perform the same pro-
cedure in the range [th - 0.1, th + 0.1] and with a
step of 0.001.
6
Recall that we normalize similarity in [-1, 1].
31
3.3 Experimental results
With both datasets, we experimented with a Max-
imum Entropy (ME) classifier. However, prelim-
inary results (see table 1) showed that our MTC
dataset is very easy. BASE achieves approximately
95% in accuracy and F -measure, and an approx-
imate performance of 99.5% in all measures (ac-
curacy, precision, recall, F -measure) is achieved
by using ME and only some of the features of
INIT (we use 36 features corresponding to pairs
?
s
1
1
, s
1
2
?
,
?
s
2
1
, s
2
2
?
,
?
s
3
1
, s
3
2
?
,
?
s
4
1
, s
4
2
?
plus the two
negation features). Therefore, we did not experi-
ment with the MTC dataset any further.
Table 2 (upper part) lists the results of our ex-
periments on the MSR corpus. We optionally per-
formed feature selection with both forward hill-
climbing (FHC) and forward beam search (FBS).
All of our methods clearly perform better than
BASE. As one might expect, there is a lot of re-
dundancy in the complete feature set. Hence, the
two feature selection methods (FHC and FBS) lead
to competitive results with much fewer features (7
and 10, respectively, instead of 136). However,
feature selection deteriorates performance, espe-
cially accuracy, i.e., the full feature set is better,
despite its redundancy. Table 2 also includes all
other reported results for the MSR corpus that we
are aware of; we are not aware of the exact number
of features used by the other researchers.
It is noteworthy that INIT achieves state of the
art performance, even though the other approaches
use many more linguistic resources. For example,
Wan et al?s approach (Wan et al, 2006), which
achieved the best previously reported results, is
similar to ours, in that it also trains a classifier with
similarity measures; but some of Wan et al?s mea-
sures require a dependency grammar parser, unlike
INIT. More precisely, for each pair of sentences,
Wan et al construct a feature vector with values
that measure lexical and dependency similarities.
The measures are: word overlap, length difference
(in words), BLEU (Papineni et al, 2002), depen-
dency relation overlap (i.e., R
1
and R
2
but not
F
R
1
,R
2
), and dependency tree edit distance. The
measures are also applied on sequences containing
the lemmatized words of the original sentences,
similarly to one of our levels of abstraction. Inter-
estingly, INIT achieves the same (and slightly bet-
ter) accuracy as Wan et al?s system, without em-
ploying any parsing. Our more enhanced methods,
INIT+WN and INIT+WN+DEP, achieve even better
results.
Zhang and Patrick (2005) use a dependency
grammar parser to convert passive voice phrases
to active voice ones. They also use a preprocess-
ing stage to generalize the pairs of sentences. The
preprocessing replaces dates, times, percentages,
etc. with generic tags, something that we have also
done in the MSR corpus, but it also replaces words
and phrases indicating future actions (e.g., ?plans
to?, ?be expected to?) with the word ?will?; the
latter is an example of further preprocessing that
could be added to our system. After the prepro-
cessing, Zhang and Patrick create for each sen-
tence pair a feature vector whose values measure
the lexical similarity between the two sentences;
they appear to be using the maximum number of
consecutive common words, the number of com-
mon words, edit distance (in words), and modi-
fied n-gram precision, a measure similar to BLEU.
The produced vectors are then used to train a de-
cision tree classifier. Hence, Zhang and Patrick?s
approach is similar to ours, but we use more and
different similarity measures and several levels of
abstraction of the two sentences. We also use ME,
along with a wrapper approach to feature selec-
tion, rather than decision tree induction and its em-
bedded information gain-based feature selection.
Furthermore, all of our methods, even INIT which
employs no parsing at all, achieve better results
compared to Zhang and Patrick?s.
Qiu et al (2006) first convert the sentences into
tuples using parsing and semantic role labeling.
They then match similar tuples across the two sen-
tences, and use an SVM (Vapnik, 1998) classifier to
decide whether or not the tuples that have not been
matched are important or not. If not, the sentences
are paraphrases. Despite using a parser and a se-
mantic role identifier, Qiu et al?s system performs
worse than our methods.
Finally, Finch et al?s system (2005) achieved
the second best overall results by employing POS
tagging, synonymy resolution, and an SVM. In-
terestingly, the features of the SVM correspond
to machine translation evaluation metrics, rather
than string similarity measures, unlike our system.
We plan to examine further how the features of
Finch et al and other ideas from machine trans-
lation can be embedded in our system, although
INIT+WN+DEP outperforms Finch et al?s system.
Interestingly, even when not using more resources
than Finch et al as in methods INIT and INIT+WN
32
method features accuracy precision recall F -measure
BASE ? 95.30 98.16 92.32 95.15
INIT? 38 99.62 99.50 99.75 99.62
Table 1: Results (%) of our methods on our MTC dataset.
method features accuracy precision recall F -measure
BASE 1 69.04 72.42 86.31 78.76
INIT 133 75.19 78.51 86.31 82.23
INIT+WN 133 75.48 78.91 86.14 82.37
INIT+WN+DEP 136 76.17 79.35 86.75 82.88
INIT+WN+DEP + FHC 7 73.86 75.14 90.67 82.18
INIT+WN+DEP + FBS 10 73.68 73.68 93.98 82.61
Finch et al ? 74.96 76.58 89.80 82.66
Qiu et al ? 72.00 72.50 93.40 81.60
Wan et al ? 75.00 77.00 90.00 83.00
Zhang & Patrick ? 71.90 74.30 88.20 80.70
Table 2: Results (%) of our methods (upper part) and other methods (lower part) on the MSR corpus.
we achieve similar or better accuracy results.
4 Related work
We have already made the distinction between
paraphrase (and textual entailment) generators vs.
recognizers, and we have pointed out that rec-
ognizers can be embedded in generators as fil-
ters. The latter is particularly useful in bootstrap-
ping paraphrase generation approaches (Riloff
and Jones, 1999; Barzilay and McKeown, 2001;
Ravichandran and Hovy, 2001; Ravichandran et
al., 2003; Duclaye et al, 2003; Szpektor et al,
2004), which are typically given seed pairs of
named entities for which a particular relation
holds; the system locates in a document collec-
tion (or the entire Web) contexts were the seeds
cooccur, and uses the contexts as patterns that can
express the relation; the patterns are then used to
locate new named entities that satisfy the relation,
and a new iteration begins. A paraphrase recog-
nizer could be used to filter out erroneous gener-
ated paraphrases between iterations.
Another well known paraphrase generator is Lin
and Pantel?s (2001) DIRT, which produces slotted
semantically equivalent patterns (e.g., ?X is the
writer of Y ? ? ?X wrote Y ? ? ?Y was writ-
ten by X? ? ?X is the author of Y ?), based
on the assumption that different paths of depen-
dency trees (obtained from a corpus) that occur
frequently with the same words (slot fillers) at
their ends are often paraphrases. An extension of
DIRT, named LEDIR, has also been proposed (Bha-
gat et al, 2007) to recognize directional textual
entailment rules (e.g., ?Y was written by X? ?
?Y is the work of X?). Ibrahim et al?s (2003)
method is similar to DIRT, but it uses only de-
pendency grammar paths from aligned sentences
(from a parallel corpus) that share compatible an-
chors (e.g., identical strings, or entity names of the
same semantic category). Shinyama and Sekine
(2003) adopt a very similar approach.
In another generation approach, Barzilay and
Lee (2002; 2003) look for pairs of slotted word
lattices that share many common slot fillers; the
lattices are generated by applying a multiple-
sequence alignment algorithm to a corpus of mul-
tiple news articles about the same events. Finally,
Pang et al (2003) create finite state automata by
merging parse trees of aligned sentences from a
parallel corpus; in each automaton, different paths
represent paraphrases. Again, a paraphrase recog-
nizer could be embedded in all of these methods,
to filter out erroneous generated patterns.
5 Conclusions and further work
We have presented three methods (INIT, INIT+WN,
INIT+WN+DEP) that recognize paraphrases given
pairs of sentences. These methods employ nine
string similarity measures applied to ten shallow
abstractions of the input sentences. Moreover,
INIT+WN and INIT+WN+DEP exploit WordNet for
synonymy resolution, and INIT+WN+DEP uses ad-
ditional features that measure grammatical rela-
tion similarity. Supervised machine learning is
used to learn how to combine the resulting fea-
tures. We experimented with a Maximum Entropy
classifier on two datasets; the publicly available
MSR corpus and one that we constructed from the
33
MTC corpus. However, the latter was found to be
very easy, and consequently we mainly focused on
the MSR corpus.
On the MSR corpus, all of our methods achieved
similar or better performance than the sate of the
art, even INIT, despite the fact that it uses fewer
linguistic resources. Hence, INIT may have prac-
tical advantages in less spoken languages, which
have limited resources. The most elaborate of
our methods, INIT+WN+DEP, achieved the best re-
sults, but it requires WordNet and a reliable depen-
dency grammar parser. Feature selection experi-
ments indicate that there is significant redundancy
in our feature set, though the full feature set leads
to better performance than the subsets produced
by feature selection. Further improvements may
be possible by including in our system additional
features, such as BLEU scores or features for word
alignment.
Our long-term goal is to embed our recognizer
in a bootstrapping paraphrase generator, to filter
out erroneous paraphrases between bootstrapping
iterations. We hope that our recognizer will be ad-
equate for this purpose, possibly in combination
with a human in the loop, who will inspect para-
phrases the recognizer is uncertain of.
Acknowledgements
This work was funded by the Greek PENED 2003
programme, which is co-funded by the European
Union (80%), and the Greek General Secretariat
for Research and Technology (20%).
References
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Gi-
ampiccolo, B. Magnini, and I. Szpektor. 2006. The
2nd PASCAL recognising textual entailment chal-
lenge. In Proceedings of the 2nd PASCAL Chal-
lenges Workshop on Recognising Textual Entail-
ment, Venice, Italy.
R. Barzilay and L. Lee. 2002. Bootstrapping lexi-
cal choice via multiple-sequence alignment. In Pro-
ceedings of EMNLP, pages 164?171, Philadelphia,
PA.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
an unsupervised approach using multiple-sequence
alignment. In Proceedings of HLT-NAACL, pages
16?23, Edmonton, Canada.
R. Barzilay and K. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proceedings of
ACL/EACL, pages 50?57, Toulouse, France.
R. Bhagat, P. Pantel, and E. Hovy. 2007. LEDIR:
An unsupervised algorithm for learning directional-
ity of inference rules. In Proceedings of the EMNLP-
CONLL, pages 161?170.
I. Dagan, O. Glickman, and B. Magnini. 2006. The
PASCAL recognising textual entailment challenge.
In Qui
?
nonero-Candela et al, editor, LNAI, volume
3904, pages 177?190. Springer-Verlag.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of COLING, page 350, Morristown, NJ.
F. Duclaye, F. Yvon, and O. Collin. 2003. Learning
paraphrases to improve a question-answering sys-
tem. In Proceedings of the EACL Workshop on Nat-
ural Language Processing for Question Answering
Systems, pages 35?41, Budapest, Hungary.
A. Finch, Y. S. Hwang, and E. Sumita. 2005. Using
machine translation evaluation techniques to deter-
mine sentence-level semantic equivalence. In Pro-
ceedings of the 3rd International Workshop on Para-
phrasing, Jeju Island, Korea.
D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan.
2007. The third Pascal recognizing textual entail-
ment challenge. In Proceedings of the ACL-Pascal
Workshop on Textual Entailment and Paraphrasing,
pages 1?9, Prague, Czech Republic.
I. J. Good. 1963. Maximum entropy for hypothesis
formulation, especially for multidimentional conti-
gency tables. Annals of Mathematical Statistics,
34:911?934.
I.M. Guyon, S.R. Gunn, M. Nikravesh, and L. Zadeh,
editors. 2006. Feature Extraction, Foundations and
Applications. Springer.
S. Harabagiu and A. Hickl. 2006. Methods for using
textual entailment in open-domain question answer-
ing. In Proceedings of COLING-ACL, pages 905?
912, Sydney, Australia.
S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca.
2003. Open-domain textual question answer-
ing techniques. Natural Language Engineering,
9(3):231?267.
A. Ibrahim, B. Katz, and J. Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Proceedings of the ACL Workshop on
Paraphrasing, pages 57?64, Sapporo, Japan.
E. T. Jaynes. 1957. Information theory and statistical
mechanics. Physical Review, 106:620?630.
D. Lin and P. Pantel. 2001. Discovery of inference
rules for question answering. Natural Language En-
gineering, 7:343?360.
34
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42?47, Prague, June. Association for Compu-
tational Linguistics.
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-
based alignment of multiple translations: extracting
paraphrases and generating new sentences. In Pro-
ceedings of HLT-NAACL, pages 102?109, Edmon-
ton, Canada.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318,
Philadelphia, Pennsylvania.
L. Qiu, M. Y. Kan, and T.S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In Proceedings of EMNLP, pages 18?26, Syd-
ney, Australia.
D. Ravichandran and E. Hovy. 2001. Learning surface
text patterns for a question answering system. In
Proceedings of ACL, pages 41?47, Philadelphia, PA.
D. Ravichandran, A. Ittycheriah, and S. Roukos. 2003.
Automatic derivation of surface text patterns for a
maximum entropy based question answering sys-
tem. In Proceedings of HLT-NAACL, pages 85?87,
Edmonton, Canada.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of AAAI, pages 474?479, Orlando,
FL.
Y. Shinyama and S. Sekine. 2003. Paraphrase ac-
quisition for information extraction. In Proceed-
ings of the ACL Workshop on Paraphrasing, Sap-
poro, Japan.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling Web-based acquisition of entailment rela-
tions. In Proceedings of EMNLP, Barcelona, Spain.
V. Vapnik. 1998. Statistical learning theory. John
Wiley.
S. Wan, M. Dras, R. Dale, and C. Paris. 2006. Us-
ing dependency-based features to take the ?para-
farce? out of paraphrase. In Proceedings of the Aus-
tralasian Language Technology Workshop, pages
131?138, Sydney, Australia.
Y. Zhang and J. Patrick. 2005. Paraphrase identifi-
cation by text canonicalization. In Proceedings of
the Australasian Language Technology Workshop,
pages 160?166, Sydney, Australia.
35
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 42?47,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Textual Entailment using SVMs and String Similarity Measures
Prodromos Malakasiotis and Ion Androutsopoulos
Department of Informatics
Athens University of Economics and Business
Patision 76, GR-104 34 Athens, Greece
Abstract
We present the system that we submitted to
the 3rd Pascal Recognizing Textual Entail-
ment Challenge. It uses four Support Vector
Machines, one for each subtask of the chal-
lenge, with features that correspond to string
similarity measures operating at the lexical
and shallow syntactic level.
1 Introduction
Textual Entailment is desirable in many natural lan-
guage processing areas, such as question answer-
ing, information extraction, information retrieval,
and multi-document summarization. In the Pascal
Recognizing Textual Entailment Challenge (RTE), it
is defined as the task of deciding whether or not the
meaning of a hypothesis text (H) can be inferred
from the meaning of another text (T ).1 For instance:
T : The drugs that slow down or halt Alzheimer?s disease
work best the earlier you administer them.
H: Alzheimer?s disease is treated using drugs.
is a correct entailment pair, but the following is not:
T : Drew Walker, NHS Tayside?s public health director, said:
?It is important to stress that this is not a confirmed case
of rabies.?
H: A case of rabies was confirmed.
In previous RTE challenges (Dagan et al, 2006;
Bar-Haim et al, 2006), several machine-learning ap-
proaches appeared, but their results showed that sig-
nificant improvements were still necessary. In this
paper, we present the system we used in the third
1See http://www.pascal-network.org/.
RTE challenge. The latter had four different devel-
opment and test sets (QA, IR, IE, SUM), intended to
evaluate textual entailment recognition in the four
natural language processing areas mentioned above.
2 System overview
Our system uses SVMs (Vapnik, 1998) to determine
whether each T?H pair constitutes a correct tex-
tual entailment or not. In particular, it employs four
SVMs, each trained on the development dataset of
the corresponding RTE subtask (QA, IR, IE, SUM)
and used on the corresponding test dataset. Pre-
liminary experiments indicated that training a single
SVM on all four subsets leads to worse results, de-
spite the increased size of the training set, presum-
ably because of differences in how the pairs were
constructed in each subtask, which do not allow a
single SVM to generalize well over all four.
The system is based on the assumption that string
similarity at the lexical and shallow syntactic level
can be used to identify textual entailment reason-
ably well, at least in question answering, the main
area we are interested in. We, therefore, try to cap-
ture different kinds of similarity by employing 10
different string similarity measures, to be discussed
below. In each T?H case, every measure is applied
to the following 8 pairs of strings, producing a total
of 80 measurements:
pair 1: two strings with the original words of T and
H , respectively; although we refer to ?words?,
this and the following string pairs also contain
non-word tokens, such as punctuation.2
2We use OPENNLP?s tokenizer, POS-tagger, and chunker (see
http://opennlp.sourceforge.net/), and our own
implementation of Porter?s stemmer.
42
pair 2: two strings containing the corresponding
stems of the words of T and H , respectively;
pair 3: two strings containing the part-of-speech
(POS) tags of the words of T and H;
pair 4: two strings containing the chunk tags (see
below) of the words of T and H;
pair 5: two strings containing only the nouns of T
and H , as identified by a POS-tagger;
pair 6: two strings containing only the stems of the
nouns of T and H;
pair 7: two strings containing only the verbs of T
and H , as identified by a POS-tagger;
pair 8: two strings containing only the stems of the
verbs of T and H .
Chunk tags are of the form B-x, I-x or O, were B and
I indicate the initial and other words of the chunks,
respectively, whereas O indicates words outside all
chunks; x can be NP, VP, or PP, for noun phrase,
verb phrase, and prepositional phrase chunks.
Partial matches: When applying the string simi-
larity measures, one problem is that T may be much
longer than H , or vice versa. Consider, for exam-
ple, the following T?H pair. The difference in the
lengths of T and H may mislead many similarity
measures to indicate that the two texts are very dis-
similar, even though H is included verbatim in T .
T : Charles de Gaulle died in 1970 at the age of eighty. He
was thus fifty years old when, as an unknown officer re-
cently promoted to the (temporary) rank of brigadier gen-
eral, he made his famous broadcast from London reject-
ing the capitulation of France to the Nazis after the deba-
cle of May-June 1940.
H: Charles de Gaulle died in 1970.
To address this problem, when we consider a pair
of strings (s1, s2), if s1 is longer than s2, we also
compute the ten values fi(s?1, s2), where fi (1 ? i ?
10) are the string similarity measures, for every s?1
that is a substring of s1 of the same length as s2. We
then locate the s?1 with the best average similarity to
s2, shown below as s??1 :
s??1 = argmax
s?1
10?
i=1
fi(s
?
1, s2)
and we keep the ten fi(s??1 , s2) values and their aver-
age as 11 additional measurements. Similarly, if s2
is longer than s1, we keep the ten fi(s1, s??2 ) values
and their average. This process could be applied to
all pairs 1?8 above, but the system we submitted ap-
plied it only to pairs 1?4; hence, there is a total of 44
additional measurements in each T?H case.
The 124 measurements discussed above provide
124 candidate numeric features that can be used by
the SVMs.3 To those, we add the following four:
Negation: Two Boolean features, showing if T or
H , respectively, contain negation, identified by
looking for words like ?not?, ?won?t?, etc.
Length ratio: This is min(LT ,LH)max(LT ,LH) , were LT and
LH are the lengths, in words, of T and H .
Text length: Binary feature showing if the markup
of the dataset flags T as ?long? or ?short?.
Hence, there are 128 candidate features in total.
From those, we select a different subset for the
SVM of each subtask, as will be discussed in fol-
lowing sections. Note that similarity measures have
also been used in previous RTE systems as fea-
tures in machine learning algorithms; see, for ex-
ample, Kozareva and Montoyo (2006), Newman et
al. (2006). However, the results of those systems in-
dicate that improvements are still necessary, and we
believe that one possible improvement is the use of
more and different similarity measures.
We did not use similarity measures that operate
on parse trees or semantic representations, as we are
interested in RTE methods that can also be applied to
less spoken languages, where reliable parsers, fact
extractors, etc. are often difficult to obtain.
2.1 String similarity measures
We now describe the ten string similarity measures
that we use.4 The reader is reminded that the mea-
sures are applied to string pairs (s1, s2), where s1
and s2 derive from T and H , respectively.
Levenshtein distance: This is the minimum num-
ber of operations (edit distance) needed to transform
one string (in our case, s1) into the other one (s2),
3All feature values are normalized in [?1, 1].
4We use the SIMMETRICS library; see http://www.
dcs.shef.ac.uk/?sam/simmetrics.html.
43
where an operation is an insertion, deletion, or sub-
stitution of a single character. In pairs of strings that
contain POS or chunk tags, it would be better to con-
sider operations that insert, delete, or substitute en-
tire tags, instead of characters, but the system we
submitted did not do this; we addressed this issue in
subsequent work, as will be discussed below.
Jaro-Winkler distance: The Jaro-Winkler dis-
tance (Winkler, 1999) is a variation of the Jaro dis-
tance (Jaro, 1995), which we describe first. The Jaro
distance dj of s1 and s2 is defined as:
dj(s1, s2) =
m
3 ? l1
+
m
3 ? l2
+
m ? t
3 ? m
,
where l1 and l2 are the lengths (in characters) of s1
and s2, respectively. The value m is the number of
characters of s1 that match characters of s2. Two
characters from s1 and s2, respectively, are taken to
match if they are identical and the difference in their
positions does not exceed max(l1,l2)2 ? 1. Finally, to
compute t (?transpositions?), we remove from s1 and
s2 all characters that do not have matching charac-
ters in the other string, and we count the number of
positions in the resulting two strings that do not con-
tain the same character; t is half that number.
The Jaro-Winkler distance dw emphasizes prefix
similarity between the two strings. It is defined as:
dw(s1, s2) = dj(s1, s2) + l ? p ? [1 ? dj(s1, s2)],
where l is the length of the longest common prefix
of s1 and s2, and p is a constant scaling factor that
also controls the emphasis placed on prefix similar-
ity. The implementation we used considers prefixes
up to 6 characters long, and sets p = 0.1.
Again, in pairs of strings (s1, s2) that contain POS
tags or chunk tags, it would be better to apply this
measure to the corresponding lists of tags in s1 and
s2, instead of treating s1 and s2 as strings of char-
acters, but the system we submitted did not do this;
this issue was also addressed in subsequent work.
Soundex: Soundex is an algorithm intended to
map each English name to an alphanumeric code,
so that names whose pronunciations are the same
are mapped to the same code, despite spelling dif-
ferences.5 Although Soundex is intended to be used
5See http://en.wikipedia.org/wiki/Soundex.
on names, and in effect considers only the first let-
ter and the first few consonants of each name, we
applied it to s1 and s2, in an attempt to capture simi-
larity at the beginnings of the two strings; the strings
were first stripped of all white spaces and non-letter
characters. We then computed similarity between
the two resulting codes using the Jaro-Winkler dis-
tance. A better approach would be to apply Soundex
to all words in T and H , forming a 9th pair (s1, s2),
on which other distance measures would then be ap-
plied; we did this in subsequent work.
Manhattan distance: Also known as City Block
distance or L1, this is defined for any two vectors
~x = ?x1, . . . , xn? and ~y = ?y1, . . . , yn? in an n-
dimensional vector space as:
L1(~x, ~y) =
n?
i=1
|xi ? yi|.
In our case, n is the number of distinct words (or
tags) that occur in s1 and s2 (in any of the two);
and xi, yi show how many times each one of these
distinct words occurs in s1 and s2, respectively.
Euclidean distance: This is defined as follows:
L2(~x, ~y) =
?
?
?
?
n?
i=1
(xi ? yi)2.
In our case, ~x and ~y correspond to s1 and s2, respec-
tively, as in the previous measure.
Cosine similarity: The definition follows:
cos(~x, ~y) =
~x ? ~y
?~x? ? ?~y?
.
In our system ~x and ~y are as above, except that they
are binary, i.e., xi and yi are 1 or 0, depending on
whether or not the corresponding word (or tag) oc-
curs in s1 or s2, respectively.
N-gram distance: This is the same as L1, but in-
stead of words we use all the (distinct) character n-
grams in s1 and s2; we used n = 3.
Matching coefficient: This is |X ? Y |, where X
and Y are the sets of (unique) words (or tags) of s1
and s2, respectively; i.e., it counts how many com-
mon words s1 and s2 have.
44
Dice coefficient: This is the following quantity; in
our case, X and Y are as in the previous measure.
2 ? |X ? Y |
|X| + |Y |
Jaccard coefficient: This is defined as |X?Y ||X?Y | ;
again X and Y are as in the matching coefficient.
2.2 SVM tuning and feature selection
As already noted, we employed four SVMs, one for
each subtask of the challenge (IR, IE, QA, SUM).6
In each subtask, feature selection was performed as
follows. We started with a set of 20 features, which
correspond to the ten similarity measures applied to
both words and stems (string pairs 1 and 2 of section
1); see table 1. We then added the 10 features that
correspond to the ten similarity measures applied to
POS tags (string pair 3). In IE and IR, this addi-
tion led to improved leave-one-out cross-validation
results on the corresponding development sets, and
we kept the additional features (denoted by ?X? in
table 1). In contrast, in QA and SUM the additional
10 features were discarded, because they led to no
improvement in the cross-validation. We then added
the 10 features that corresponded to the ten similar-
ity measures applied to chunk tags (string pair 4),
which were retained only in the IE SVM, and so on.
The order in which we considered the various ex-
tensions of the feature sets is the same as the order of
the rows of table 1, and it reflects the order in which
it occurred to us to consider the corresponding ad-
ditional features while preparing for the challenge.
We hope to investigate additional feature selection
schemes in further work; for instance, start with all
128 features and explore if pruning any groups of
features improves the cross-validation results.
With each feature set that we considered, we
actually performed multiple leave-one-out cross-
validations on the development dataset, for different
values of the parameters of the SVM and kernel, us-
ing a grid-search utility. Each feature set was eval-
uated by considering its best cross-validation result.
The best cross-validation results for the final feature
sets of the four SVMs are shown in table 2.
6We use LIBSVM (Chang and Lin, 2001), with a Radial Basis
Function kernel, including LIBSVM?s grid search tuning utility.
Subtask Accuracy (%)
QA 86.50 (90.00)
IR 80.00 (75.50)
SUM 73.00 (72.50)
IE 62.00 (61.50)
all 75.38 (74.88)
Table 2: Best cross-validation results of our system
on the development datasets. Results with subse-
quent improvements are shown in brackets.
Subtask Accuracy (%) Average Precision (%)
QA 73.50 (76.00) 81.03 (81.08)
IR 64.50 (63.50) 63.61 (67.28)
SUM 57.00 (60.50) 60.88 (61.58)
IE 52.00 (49.50) 58.16 (51.57)
all 61.75 (62.38) 68.08 (68.28)
Table 3: Official results of our system. Results with
subsequent improvements are shown in brackets.
3 Official results and discussion
We submitted only one run to the third RTE chal-
lenge. The official results of our system are shown
in table 3.7 They are worse than the best results we
had obtained in the cross-validations on the devel-
opment datasets (cf. table 2), but this was expected
to a large extent, since the SVMs were tuned on the
development datasets; to some extent, the lower of-
ficial results may also be due to different types of
entailment being present in the test datasets, which
had not been encountered in the training sets.
As in the cross-validation results, our system per-
formed best in the QA subtask; the second and third
best results of our system were obtained in IR and
SUM, while the worst results were obtained in IE.
Although a more thorough investigation is neces-
sary to account fully for these results, it appears that
they support our initial assumption that string simi-
larity at the lexical and shallow syntactic level can be
used to identify textual entailment reasonably well
in question answering systems. Some further reflec-
tions on the results of our system follow.
In the QA subtask of the challenge, it appears that
each T was a snippet returned by a question answer-
ing system for a particular question.8 We are not
aware of exactly how the T s were selected by the
7See the RTE Web site for a definition of ?average precision?.
8Consult http://www.pascal-network.org/Chal
lenges/RTE3/Introduction/.
45
Feature sets features IE IR QA SUM
similarity measures on words 10 X X X X
similarity measures on stems 10 X X X X
+ similarity measures on POS tags +10 X X
+ similarity measures on chunk tags +10 X X
+ average of sim. measures on words of best partial match +1 X
+ average of sim. measures on stems of best partial match +1 X X
+ average of sim. measures on POS tags of best partial match +1 X X
+ average of sim. measures on chunk tags of best partial match +1 X X
+ similarity measures on words of best partial match +10
+ similarity measures on stems of best partial match +10 X
+ similarity measures on POS tags of best partial match +10 X
+ similarity measures on chunk tags of best partial match +10
+ negation +2 X
+ length ratio +1 X
+ similarity measures on nouns +10 X
+ similarity measures on noun stems +10
+ similarity measures on verbs +10 X
+ similarity measures on verb stems +10
+ short/long T +1 X X
Total 128 64 31 23 54
Table 1: Feature sets considered and chosen in each subtask.
systems used, but QA systems typically return T s
that contain the expected answer type of the input
question; for instance, if the question is ?When did
Charles de Gaulle die??, T will typically contain a
temporal expression. Furthermore, QA systems typi-
cally prefer T s that contain many words of the ques-
tion, preferably in the same order, etc. (Radev et
al., 2000; Ng et al, 2001; Harabagiu et al, 2003).
Hence, if the answers are sought in a document col-
lection with high redundancy (e.g., the Web), i.e.,
a collection where each answer can be found with
many different phrasings, the T s (or parts of them)
that most QA systems return are often very similar,
in terms of phrasings, to the questions, provided that
the required answers exist in the collection.
In the QA datasets of the challenge, for each T ,
which was a snippet returned by a QA system for a
question (e.g., ?When did Charle de Gaulle die??),
an H was formed by ?plugging into? the question
an expression of the expected answer type from T .
In effect, this converted all questions to propositions
(e.g., ?Charle de Gaulle died in 1970.?) that require
a ?yes? or ?no? answer. Note that this plugging in
does not always produce a true proposition; T may
contain multiple expressions of the expected answer
type (e.g., ?Charle de Gaulle died in 1970. In 1990,
a monument was erected. . . ?) and the wrong one
may be plugged into the question (H = ?Charle de
Gaulle died in 1990.?).
Let us first consider the case where the proposi-
tion (H) is true. Assuming that the document collec-
tion is redundant and that the answer to the question
exists in the collection, T (or part of it) will often be
very similar to H , since it will be very similar to the
question that H was derived from. In fact, the simi-
larity between T andH may be greater than between
T and the question, since an expression from T has
been plugged into the question to form H . Being
very similar, T will very often entail H , and, hence,
the (affirmative) responses of our system, which are
based on similarity, will be correct.
Let us now consider the case whereH is false. Al-
though the same arguments apply, and, hence, one
might again expect T to be very similar to H , this
is actually less likely now, because H is false and,
hence, it is more difficult to find a very similarly
phrased T in the presumed trustful document collec-
tion. The reduced similarity between T and H will
lead the similarity measures to suggest that the T?H
entailment does not hold; and in most cases, this is a
correct decision, because H is false and, thus, it can-
not be entailed by a (true) T that has been extracted
from a trustful document collection.
Similar arguments apply to the IR subtask, where
our system achieved its second best results. Our re-
sults in this subtask were lower than in the QA sub-
46
task, presumably because the T s were no longer fil-
tered by the additional requirement that they must
contain an expression of the expected answer type.
We attribute the further deterioration of our re-
sults in the SUM subtask to the fact that, accord-
ing to the challenge?s documentation, all the T?H
pairs of that subtask, both true and false entailments,
were chosen to have high lexical similarity, which
does not allow the similarity measures of our system
to distinguish well between the two cases. Finally,
the lower results obtained in the IE subtask may be
due to the fact that the T?H pairs of that subtask
were intended to reflect entailments identified by in-
formation extraction systems, which specialize on
identifying particular semantic relations by employ-
ing more complicated machinery (e.g., named entity
recognizers and matchers, fact extractors, etc.) than
simple string similarity measures; the results may
also be partly due to the four different ways that
were used to construct the T?H pairs of that sub-
task. It is interesting to note (see table 1) that the
feature sets were larger in the subtasks where our
system scored worse, which may be an indication of
the difficulties the corresponding SVMs encountered.
4 Conclusions and further work
We presented a textual entailment recognition sys-
tem that relies on SVMs whose features correspond
to string similarity measures applied to the lexical
and shallow syntactic level. Experimental results in-
dicate that the system performs reasonably well in
question answering (QA), which was our main tar-
get, with results deteriorating as we move to infor-
mation retrieval (IR), multi-document summariza-
tion (SUM), and information extraction (IE).
In work carried out after the official submission
of our system, we incorporated two of the possible
improvements that were mentioned in previous sec-
tions: we treated strings containing POS or chunk
tags as lists of tags; and we applied Soundex to each
word of T and H , forming a 9th pair of strings, on
which all other similarity measures were applied;
feature selection was then repeated anew. The cor-
responding results are shown in brackets in tables
2 and 3. There was an overall improvement in all
tasks (QA, IR, SUM), except for IE, where textual en-
tailment is more difficult to capture via textual simi-
larity, as commented above. We have suggested two
additional possible improvements: applying partial
matching to all of the string pairs that we consider,
and investigating other feature selection schemes. In
future work, we also plan to exploit WordNet to cap-
ture synonyms, hypernyms, etc.
Acknowledgements
This work was funded by the Greek PENED 2003 programme,
which is co-funded by the European Union (75%), and the
Greek General Secretariat for Research and Technology (25%).
References
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo,
B. Magnini, and I. Szpektor. 2006. The 2nd PASCAL recog-
nising textual entailment challenge. In Proceedings of the
2nd PASCAL Challenges Workshop on Recognising Textual
Entailment, Venice, Italy.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
I. Dagan, O. Glickman, and B. Magnini. 2006. The PASCAL
recognising textual entailment challenge. In Quin?onero-
Candela et al, editor, MLCW 2005, LNAI, volume 3904,
pages 177?190. Springer-Verlag.
S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003. Open-
domain textual question answering techniques. Natural Lan-
guage Engineering, 9(3):231?267.
M.A. Jaro. 1995. Probabilistic linkage of large public health
data file. Statistics in Medicine, 14:491?498.
Z. Kozareva and A. Montoyo. 2006. MLENT: The machine
learning entailment system of the University of Alicante. In
Proc. of 2nd PASCAL Challenges Workshop on Recognising
Textual Entailment, Venice, Italy.
E. Newman, J. Dunnion, and J. Carthy. 2006. Constructing a
decision tree classifier using lexical and syntactic features.
In Proc. of 2nd PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
H.T. Ng, J.L.P. Kwan, and Y. Xia. 2001. Question answering
using a large text database: A machine learning approach. In
Proc. of Empirical Methods in Natural Language Process-
ing, Carnegie Mellon Univ., PA.
D.R. Radev, J. Prager, and V. Samn. 2000. Ranking suspected
answers to natural language questions using predictive an-
notation. In Proc. of NAACL-ANLP, pages 150?157, Seattle,
WA.
V. Vapnik. 1998. Statistical learning theory. John Wiley.
W.E. Winkler. 1999. The state of record linkage and current
research problems. Statistical Research Report RR99/04, US
Bureau of the Census, Washington, DC.
47
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 96?106,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Generate and Rank Approach to Sentence Paraphrasing
Prodromos Malakasiotis? and Ion Androutsopoulos?+
?Department of Informatics, Athens University of Economics and Business, Greece
+Digital Curation Unit ? IMIS, Research Centre ?Athena?, Greece
Abstract
We present a method that paraphrases a given
sentence by first generating candidate para-
phrases and then ranking (or classifying)
them. The candidates are generated by ap-
plying existing paraphrasing rules extracted
from parallel corpora. The ranking compo-
nent considers not only the overall quality of
the rules that produced each candidate, but
also the extent to which they preserve gram-
maticality and meaning in the particular con-
text of the input sentence, as well as the de-
gree to which the candidate differs from the
input. We experimented with both a Max-
imum Entropy classifier and an SVR ranker.
Experimental results show that incorporating
features from an existing paraphrase recog-
nizer in the ranking component improves per-
formance, and that our overall method com-
pares well against a state of the art paraphrase
generator, when paraphrasing rules apply to
the input sentences. We also propose a new
methodology to evaluate the ranking compo-
nents of generate-and-rank paraphrase gener-
ators, which evaluates them across different
combinations of weights for grammaticality,
meaning preservation, and diversity. The pa-
per is accompanied by a paraphrasing dataset
we constructed for evaluations of this kind.
1 Introduction
In recent years, significant effort has been devoted
to research on paraphrasing (Androutsopoulos and
Malakasiotis, 2010; Madnani and Dorr, 2010). The
methods that have been proposed can be roughly
classified into three categories: (i) recognition meth-
ods, i.e., methods that detect whether or not two in-
put sentences or other texts are paraphrases; (ii) gen-
eration methods, where the aim is to produce para-
phrases of a given input sentence; and (iii) extraction
methods, which aim to extract paraphrasing rules
(e.g., ?X wrote Y ? ?? Y was authored by X?) or
similar patterns from corpora. Most of the methods
that have been proposed belong in the first category,
possibly because of the thrust provided by related
research on textual entailment recognition (Dagan et
al., 2009), where the goal is to decide whether or not
the information of a given text is entailed by that of
another. Significant progress has also been made in
paraphrase extraction, where most recent methods
produce large numbers of paraphrasing rules from
multilingual parallel corpora (Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Zhao et al,
2008; Zhao et al, 2009a; Zhao et al, 2009b; Kok
and Brockett, 2010). In this paper, we are concerned
with paraphrase generation, which has received less
attention than the other two categories.
There are currently two main approaches to para-
phrase generation. The first one treats paraphrase
generation as a machine translation problem, with
the peculiarity that the target language is the same as
the source one. To bypass the lack of large monolin-
gual parallel corpora, which are needed to train sta-
tistical machine translation (SMT) systems for para-
phrasing, monolingual clusters of news articles re-
ferring to the same event (Quirk et al, 2004) or
other similar monolingual comparable corpora can
be used, though sentence alignment methods for par-
allel corpora may perform poorly on comparable
corpora (Nelken and Shieber, 2006); alternatively,
large collections of paraphrasing rules obtained via
paraphrase extraction from multilingual parallel cor-
pora can be used as monolingual phrase tables in a
96
phrase-based SMT systems (Zhao et al, 2008; Zhao
et al, 2009a); in both cases, paraphrases can then
be generated by invoking an SMT system?s decoder
(Koehn, 2009). A second paraphrase generation ap-
proach is to treat existing machine translation en-
gines as black boxes, and translate each input sen-
tence to a pivot language and then back to the orig-
inal language (Duboue and Chu-Carroll, 2006). An
extension of this approach uses multiple translation
engines and pivot languages (Zhao et al, 2010).
In this paper, we investigate a different paraphrase
generation approach, which does not produce para-
phrases by invoking machine translation system(s).
We use an existing collection of monolingual para-
phrasing rules extracted from multilingual parallel
corpora (Zhao et al, 2009b); each rule is accompa-
nied by one or more scores, intended to indicate the
rule?s overall quality without considering particular
contexts where the rule may be applied. Instead of
using the rules as a monolingual phrase table and in-
voking an SMT system?s decoder, we follow a gen-
erate and rank approach, which is increasingly com-
mon in several language processing tasks.1 Given
an input sentence, we use the paraphrasing rules to
generate a large number of candidate paraphrases.
The candidates are then represented as feature vec-
tors, and a ranker (or classifier) selects the best ones;
we experimented with a Maximum Entropy classi-
fier and a Support Vector Regression (SVR) ranker.
The vector of each candidate paraphrase includes
features indicating the overall quality of the rules
that produced the candidate, the extent to which the
rules preserve grammaticality and meaning in the
particular context of the input sentence, and the de-
gree to which the candidate?s surface form differs
from that of the input; we call the latter factor di-
versity. The intuition is that a good paraphrase is
grammatical, preserves the meaning of the original
sentence, while also being as different as possible.
Experimental results show that including in the
ranking (or classification) component features from
an existing paraphrase recognizer leads to improved
results. We also propose a new methodology to eval-
uate the ranking components of generate-and-rank
paraphrase generators, which evaluates them across
different combinations of weights for grammatical-
1See, for example, Collins and Koo (2005).
ity, meaning preservation, and diversity. The paper
is accompanied by a new publicly available para-
phrasing dataset we constructed for evaluations of
this kind. Further experiments indicate that when
paraphrasing rules apply to the input sentences, our
paraphrasing method is competitive to a state of the
art paraphrase generator that uses multiple transla-
tion engines and pivot languages (Zhao et al, 2010).
We note that paraphrase generation is useful in
several language processing tasks. In question an-
swering, for example, paraphrase generators can be
used to paraphrase the user?s queries (Duboue and
Chu-Carroll, 2006; Riezler and Liu, 2010); and
in machine translation, paraphrase generation can
help improve the translations (Callison-Burch et al,
2006; Marton et al, 2009; Mirkin et al, 2009; Mad-
nani et al, 2007), or it can be used when evaluat-
ing machine translation systems (Lepage and De-
noual, 2005; Zhou et al, 2006; Kauchak and Barzi-
lay, 2006; Pado? et al, 2009).
The remainder of this paper is structured as fol-
lows: Section 2 explains how our method gener-
ates candidate paraphrases; Section 3 introduces the
dataset we constructed, which is also used in sub-
sequent sections; Section 4 discusses how candi-
date paraphrases are ranked; Section 5 compares our
overall method to a state of the art paraphrase gen-
erator; and Section 6 concludes.
2 Generating candidate paraphrases
We use the approximately one million English para-
phrasing rules of Zhao et al (2009b). Roughly
speaking, the rules were extracted from a parallel
English-Chinese corpus, based on the assumption
that two English phrases e1 and e2 that are often
aligned to the same Chinese phrase c are likely to
be paraphrases and, hence, they can be treated as a
paraphrasing rule e1 ? e2.2 Zhao et al?s method ac-
tually operates on slotted English phrases, obtained
from parse trees, where slots correspond to part of
speech (POS) tags. Hence, rules like the following
three may be obtained, where NNi indicates a noun
slot and NNPi a proper name slot.
2This pivot-based paraphrase extraction approach was first
proposed by Bannard and Callison-Burch (2005). It under-
lies several other paraphrase extraction methods (Riezler et al,
2007; Callison-Burch, 2008; Kok and Brockett, 2010).
97
(1) a lot of NN1? plenty of NN1
(2) NNP1 area? NNP1 region
(3) NNP1 wrote NNP2? NNP2 was written by NNP1
In the basic form of their method, called Model
1, Zhao et al (2009b) use a log-linear ranker to as-
sign scores to candidate English paraphrase pairs
?e1, e2?; the ranker uses the alignment probabilities
P (c|e1) and P (e2|c) as features, along with features
that assess the quality of the corresponding align-
ments. In an extension of their method, Model 2,
Zhao et al consider two English phrases e1 and e2 as
paraphrases, if they are often aligned to two Chinese
phrases c1 and c2, which are themselves paraphrases
according to Model 1 (with English used as the pivot
language). Again, a log-linear ranker assigns a score
to each ?e1, e2? pair, now with P (c1|e1), P (c2|c1),
and P (e2|c1) as features, along with similar features
for alignment quality. In a further extension, Model
3, all the candidate phrase pairs ?e1, e2? are collec-
tively treated as a monolingual parallel corpus. The
phrases of the corpus are aligned, as when aligning
a bilingual parallel corpus, and additional features,
based on the alignment, are added to the log-linear
ranker, which again assigns a score to each ?e1, e2?.
The resulting paraphrasing rules e1 ? e2 typi-
cally contain short phrases (up to four or five words
excluding slots) on each side; hence, they can be
used to rewrite only parts of longer sentences. Given
an input (source) sentence S, we generate candidate
paraphrases by applying rules whose left or right
hand side matches any part of S. For example, rule
(1) matches the source sentence (4); hence, (4) can
be rewritten as the candidate paraphrase (5).3
(4) S: He had a lot of [NN 1admiration] for his job.
(5) C: He had plenty of [NN 1admiration] for his job.
Several rules may apply to S; for example, they may
rewrite different parts of S, or they may replace the
same parts of S by different phrases. We allow all
possible combinations of applicable rules to apply to
S, excluding combinations that include rules rewrit-
ing overlapping parts of S.4 To avoid generating too
many candidates (C), we use only the 20 rules (that
3We use Stanford?s POS tagger, MaxEnt classifier, and de-
pendency parser; see http://nlp.stanford.edu/.
4A possible extension, which we have not explored, would
be to recursively apply the same process to the resulting Cs.
apply to S) with the highest scores. Zhao et al actu-
ally associate each rule with three scores. The first
one, hereafter called r1, is the Model 1 score, and the
other two, r2 and r3, are the forward and backward
alignment probabilities of Model 3; see Zhao et al
(2009b) for details. We use the average of the three
scores, hereafter r4, when generating candidates.
Unfortunately, Zhao et al?s scores reflect the over-
all quality of each rule, without considering the con-
text of the particular S where the rule is applied.
Szpektor et al (2008) point out that, for example,
a rule like ?X acquire Y ?? ?X buy Y ? may work
well in many contexts, but not in ?Children acquire
language quickly?. Similarly, ?X charged Y with?
? ?X accused Y of? should not be applied to sen-
tences about charging batteries. Szpektor et al pro-
pose, roughly speaking, to associate each rule with
a model of the contexts where the rule is applicable,
as well as models of the expressions that typically
fill its slots, in order to be able to assess the applica-
bility of each rule in specific contexts. The rules that
we use do not have associated models of this kind,
but we follow Szpektor et al?s idea of assessing the
applicability of each rule in each particular context,
when ranking candidates, as discussed below.
3 A dataset of candidate paraphrases
Our generate and rank method relies on existing
large collections of paraphrasing rules to generate
candidate paraphrases. Our main contribution is in
the ranking of the candidates. To be able to evalu-
ate the performance of different rankers in the task
we are concerned with, we first constructed an eval-
uation dataset that contains pairs ?S,C? of source
(input) sentences and candidate paraphrases, and we
asked human judges to assess the degree to which
the C of each pair was a good paraphrase of S.
We selected randomly 75 source (S) sentences
from the AQUAINT corpus, such that at least one
of the paraphrasing rules applied to each S.5 For
each S, we generated candidate Cs using Zhao et
al.?s rules, as discussed in Section 2. This led to
1,935 ?S,C? pairs, approx. 26 pairs for each S. The
pairs were given to 13 judges other than the authors.6
Each judge evaluated approx. 148 (different) ?S,C?
5The corpus is available from the LDC (LDC2002T31).
6The judges were fluent, but not native English speakers.
98
Figure 1: Distribution of overall quality scores in the
evaluation dataset (1 = totally unacceptable, 4 = perfect).
pairs; each of the 1,935 pairs was evaluated by one
judge. The judges were asked to provide grammati-
cality, meaning preservation, and overall paraphrase
quality scores for each ?S,C? pair, each score on a
1?4 scale (1 for totally unacceptable, 4 for perfect);
guidelines and examples were also provided.
Figure 1 shows the distribution of the overall qual-
ity scores in the 1,935 ?S,C? pairs of the evalua-
tion dataset; the distributions of the grammaticality
and meaning preservation scores are similar. No-
tice that although we used only the 20 applicable
paraphrasing rules with the highest scores to gen-
erate the ?S,C? pairs, less than half of the candidate
paraphrases (C) were considered good, and approx-
imately only 20% perfect. In other words, apply-
ing paraphrasing rules (even only those with the 20
best scores) to each input sentence S and randomly
picking one of the resulting candidate paraphrases
C, without any further filtering (or ranking) of the
candidates, would on average produce unacceptable
paraphrases more frequently than acceptable ones.
Hence, the role of the ranking component is crucial.
We also measured inter-annotator agreement by
constructing, in the same way, 100 additional ?S,C?
pairs (other than the 1,935) and asking 3 of the 13
judges to evaluate all of them. We measured the
mean absolute error, i.e., the mean absolute differ-
ence in the judges? scores (averaged over all pairs
of judges) and the mean (over all pairs of judges)
K statistic (Carletta, 1996). In the overall scores,
K was 0.64, which is in the range often taken to
indicate substantial agreement (0.61?0.80).7 Agree-
ment was higher for grammaticality (K = 0.81),
7It is also close to 0.67, which is sometimes taken to be a
cutoff for substantial agreement in computational linguistics.
mean abs. diff. K-statistic
grammaticality 0.20 0.81
meaning preserv. 0.26 0.59
overall quality 0.22 0.64
Table 1: Inter-annotator agreement when manually eval-
uating candidate paraphrases.
and lower (K = 0.59) for meaning preservation. Ta-
ble 1 shows that the mean absolute difference in the
annotators? scores was 15 to 14 of a point.
Several judges commented that they had trouble
deciding to what extent the overall quality score
should reflect grammaticality or meaning preserva-
tion. They also wondered if it was fair to consider as
perfect candidate paraphrases that differed in only
one or two words from the source sentences, i.e.,
candidates with low diversity. These comments led
us to ignore the judges? overall quality scores in
some experiments, and to use a weighted average
of grammaticality, meaning preservation, and (auto-
matically measured) diversity instead, with different
weight combinations corresponding to different ap-
plication requirements, as discussed further below.
In the same way, 1,500 more ?S,C? pairs (other
than the 1,935 and the 100, not involving previously
seen Ss) were constructed, and they were evaluated
by the first author. The 1,500 pairs were used as
a training dataset in experiments discussed below.
Both the 1,500 training and the 1,935 evaluation
(test) pairs are publicly available.8 We occasionally
refer to the training and evaluation datasets as a sin-
gle dataset, but they are clearly separated.
4 Ranking candidate paraphrases
We now discuss the ranking component of our
method, which assesses the candidate paraphrases.
4.1 Features of the ranking component
Each ?S,C? pair is represented as a feature vector.
To allow the ranking component to assess the degree
to which a candidate C is grammatical, or at least
as grammatical as the source S, we include in the
feature vectors the language model scores of S, C,
and the difference between the two scores. We use
a 3-gram language model trained on approximately
8See the paper?s supplementary material.
99
6.5 million sentences of the AQUAINT corpus.9 To
allow the ranker to consider the (context-insensitive)
quality scores of the rules that generated C from S,
we also include as features the highest, lowest, and
average r1, r2, r3, and r4 scores (Section 2) of these
rules, 12 features in total.
The features discussed so far are similar to those
employed by Zhao et al (2009a) in the only compa-
rable paraphrase generation method we are aware of
that uses paraphrasing rules. That method, hereafter
called ZHAO-RUL, uses the language model score
of C and scores similar to r1, r2, r3 in a log-linear
model.10 The log-linear model of ZHAO-RUL is used
by an SMT-like decoder to identify the transforma-
tions (applications of rules) that produce the (hope-
fully) best paraphrase. By contrast, we first gen-
erate a large number of candidates using the para-
phrasing rules, and we then rank them. Unfortu-
nately, we did not have access to an implementa-
tion of ZHAO-RUL to compare against, but below
we compare against another paraphraser proposed
by Zhao et al (2010), hereafter called ZHAO-ENG,
which uses multiple machine translation engines and
pivot languages, instead of paraphrasing rules, and
which Zhao et al found to outperform ZHAO-RUL.
To further help the ranking component assess the
degree to which C preserves the meaning of S, we
also optionally include in the vectors of the ?S,C?
pairs the features of an existing paraphrase recog-
nizer (Malakasiotis, 2009) that obtained the best
published results (Androutsopoulos and Malakasio-
tis, 2010) on the widely used MSR paraphrasing cor-
pus.11 Most of the recognizer?s features are com-
puted by using nine similarity measures: Leven-
shtein, Jaro-Winkler, Manhattan, Euclidean, and n-
gram (n = 3) distance, cosine similarity, Dice, Jac-
card, and matching coefficients, all computed on to-
kens; consult Malakasiotis (2009) for details. For
each ?S,C? pair, the nine similarity measures are ap-
9We use SRILM; see http://www-speech.sri.com/.
10Application-specific features are also included, which can
be used, for example, to favor paraphrases that are shorter than
the input in sentence compression (Knight and Marcu, 2002;
Clarke and Lapata, 2008). Similar features could also be added
to application-specific versions of our method.
11The MSR corpus contains pairs that are paraphrases or not.
It is a benchmark for paraphrase recognizers, not generators. It
provides only one paraphrase (true or false) of each source, and
few of the true paraphrases can be obtained by the rules we use.
plied to ten different forms ?s1, c1? , . . . , ?s10, c10?
of ?S,C?, described below, leading to 90 features.
?s1, c1? : The original forms of S and C.
?s2, c2? : S and C with tokens replaced by stems.
?s3, c3? : S and C, with tokens replaced by POS tags.
?s4, c4? : S and C, tokens replaced by soundex codes.12
?s5, c5? : S and C, but having removed non-nouns.
?s6, c6? : As previously, but nouns replaced by stems.
?s7, c7? : As previously, nouns replaced by soundex.
?s8, c8? : S and C, but having removed non-verbs.
?s9, c9? : As previously, but verbs replaced by stems.
?s10, c10? : As previously, verbs replaced by soundex.
When constructing all ten forms ?si, ci? of ?S,C?,
synonyms (in any WordNet synset) are treated as
identical words. Additional variants of some of the
90 features compare a sliding window of some of
the si forms to the corresponding ci forms (or vice
versa), adding 40 more features; see Malakasiotis
(2009). Two more Boolean features indicate the ex-
istence or absence of negation in S or C, respec-
tively; and another feature computes the ratio of the
lengths of S and C, measured in tokens. Finally,
three additional features compare the dependency
trees of S and C:
RS =
|common dependencies of S,C|
|dependencies of S|
RC =
|common dependencies of S,C|
|dependencies of C|
F?=1 =
2 ?RS ?RC
RS +RC
The recognizer?s features are 136 in total.13
Hence, the full feature set of our paraphraser?s rank-
ing component comprises 151 features.
12The Soundex algorithm maps English words to alphanu-
meric codes, so that words with the same pronunciations
receive the same codes, despite spelling differences; see
http://en.wikipedia.org/wiki/Soundex.
13Malakasiotis (2009) shows that although there is a lot of re-
dundancy in the recognizer?s feature set, the full feature set still
leads to better paraphrase recognition results, compared to sub-
sets constructed via feature selection with hill-climbing or beam
search. The same paper reports that the recognizer performs al-
most as well without the last three features, which may not be
available in languages with no reliable dependency parsers. No-
tice, also, that the recognizer does not use paraphrasing rules.
100
4.2 Learning rate with a MaxEnt classifier
To obtain a first indication of whether or not a rank-
ing component equipped with the features discussed
above could learn to distinguish good from bad can-
didate paraphrases, and to investigate if our train-
ing dataset is sufficiently large, we initially experi-
mented with a Maximum Entropy classifier (with the
151 features) as the ranking component. This initial
version of the ranking component, called ME-REC,
was trained on increasingly larger parts of the train-
ing dataset of Section 3, and it was always evaluated
on the entire test dataset of that section. For simplic-
ity, we used only the judges? overall quality scores
in these experiments, and we treated the problem as
one of binary classification; overall quality scores of
1 and 2 where conflated to a negative category, and
scores of 3 and 4 to a positive category.
Figure 2 plots the error rate of ME-REC, com-
puted both on the test set and the encountered train-
ing subset. The error rate on the training instances
a learner has encountered is typically lower than the
error rate on the test set (unseen instances); hence,
the former error rate can be seen as a lower bound
of the latter. ME-REC shows signs of having reached
its lower bound when the entire training dataset is
used, suggesting that the training dataset is suffi-
ciently large. The baseline (BASE) of Figure 2 uses
only a threshold on the average r4 (Section 2) of the
rules that turned S into C. If the average r4 is higher
than the threshold, the ?S,C? pair is classified in the
positive class, otherwise in the negative one. The
threshold was tuned by experimenting on a sepa-
rate tuning dataset. Clearly, ME-REC outperforms
the baseline, which uses only the average (context-
insensitive) scores of the applied paraphrasing rules.
4.3 Experiments with an SVR ranker
As already noted, when our dataset were constructed
the judges felt it was not always clear to what ex-
tent the overall quality scores should reflect meaning
preservation or grammaticality; and they also won-
dered if the overall quality scores should have also
taken into consideration diversity. To address these
concerns, in the experiments described in this sec-
tion (and the remainder of the paper) we ignored the
judges? overall scores, and we used a weighted av-
erage of the grammaticality, meaning preservation,
 
15%
20%
25%
30%
35%
40%
45%
50%
75 15
0
22
5
30
0
37
5
45
0
52
5
60
0
67
5
75
0
82
5
90
0
97
5
10
50
11
25
12
00
12
75
13
50
14
25
15
00
E
r
r
o
r
r
a
t
e
Training instances used
ME-REC.TRAIN
ME-REC.TEST
BASE
Figure 2: Learning curves of a Maximum Entropy classi-
fier used as the ranking component of our method.
and diversity scores instead; the grammaticality and
meaning preservation scores were those provided by
the judges, while diversity was automatically com-
puted as the edit distance (Levenshtein, computed
on tokens) between S and C. Stated otherwise, the
correct score y(xi) of each training or test instance
xi (i.e., of each feature vector of an ?S,C? pair) was
taken to be a linear combination of the grammati-
cality score g(xi), the meaning preservation score
m(xi), and the diversity d(xi), as in Equation (6),
where ?3 = 1? ?1 ? ?2.
y(xi) = ?1 ? g(xi) + ?2 ?m(xi) + ?3 ? d(xi) (6)
We believe that the ?i weights should in prac-
tice be application-dependent. For example, when
paraphrasing user queries to a search engine that
turns them into bags of words, diversity and meaning
preservation may be more important than grammati-
cality; by contrast, when paraphrasing the sentences
of a generated text to avoid repeating the same ex-
pressions, grammaticality is very important. Hence,
generic paraphrase generators, like ours, intended to
be useful in many different applications, should be
evaluated for many different combinations of the ?i
weights. Consequently, in the experiments of this
section we trained and evaluated the ranking com-
ponent of our method (on the training and evalua-
tion part, respectively, of the dataset of Section 3)
several times, each time with a different combina-
tion of ?1, ?2, ?3 values, with the values of each ?i
ranging from 0 to 1 with a step of 0.2.
We employed a Support Vector Regression (SVR)
model in the experiments of this section, instead of
101
 0,00%
10,00%
20,00%
30,00%
40,00%
50,00%
60,00%
70,00%
?1=0.0 ?2=0.0
?1=0.0 ?2=0.2
?1=0.0 ?2=0.4
?1=0.0 ?2=0.6
?1=0.0 ?2=0.8
?1=0.0 ?2=1.0
?1=0.2 ?2=0.0
?1=0.2 ?2=0.2
?1=0.2 ?2=0.4
?1=0.2 ?2=0.6
?1=0.2 ?2=0.8?1=0.4 ?2=0.0
?1=0.4 ?2=0.2
?1=0.4 ?2=0.4
?1=0.4 ?2=0.6
?1=0.6 ?2=0.0
?1=0.6 ?2=0.2
?1=0.6 ?2=0.4
?1=0.8 ?2=0.0
?1=0.8 ?2=0.2
?1=1.0 ?2=0.0
SVR-REC
SVR-BASE
?2 
Figure 3: Performance of our method?s SVR ranking com-
ponent with (SVR-REC) and without (SVR-BASE) the ad-
ditional features of the paraphrase recognizer.
a classifier, given that the y(xi) scores that we want
to predict are real values.14 An SVR is very similar
to a Support Vector Machine (Vapnik, 1998; Cris-
tianini and Shawe-Taylor, 2000; Joachims, 2002),
but it is trained on examples of the form ?xi, y(xi)?,
where xi ? Rn and y(xi) ? R, and it learns a rank-
ing function f : Rn ? R that is intended to return
f(xi) values as close as possible to the correct ones
y(xi), given feature vectors xi. In our case, the cor-
rect y(xi) values were those of Equation (6). We call
SVR-REC the SVR ranker with all the 151 features of
Section 4.2, and SVR-BASE the SVR ranker without
the 136 features of the paraphrase recognizer.
We used the squared correlation coefficient ?2 to
evaluate SVR-REC against SVR-BASE.15 The ?2 co-
efficient shows how well the scores returned by the
SVR are correlated with the desired scores y(xi); the
higher the ?2 the higher the agreement. Figure 3
14Additional experiments confirmed that the SVR per-
forms better than ME-REC as the ranking component. We
use the SVR implementation of LIBSVM, available from
http://www.csie.ntu.edu.tw/?cjlin/libsvm/,
with an RBF kernel and default settings. All the features are
normalized in [?1, 1], when using SVR or ME-REC.
15If n is the number of test pairs, f(xi) the score returned by
the SVR for the i-th pair, and y(xi) the correct score, then ?2 is:
(n?ni=1 f(xi)yi ?
?n
i=1 f(xi)
?n
i=1 y(xi))
2
(n
?n
i=1 f(xi)2 ? (
?n
i=1 f(xi))2)(n
?n
i=1 y2i ? (
?n
i=1 y(xi))2)
shows the experimental results. Each line from the
diagram?s center represents a different experimental
setting, i.e., a different combination of ?1 and ?2;
recall that ?3 = 1 ? ?1 ? ?2. The distance of a
method?s curve from the center is the method?s ?2
for that setting. The farther a point is from the center
the higher ?2 is; hence, methods whose curves are
closer to the diagram?s outmost perimeter are better.
Clearly, SVR-REC (which includes the recognizer?s
features) outperforms SVR-BASE (which relies only
on the language model and the scores of the rules).
The two peaks of SVR-REC?s curve are when ?3
is very high (1 or 0.8), i.e., when y(xi) is dominated
by the diversity score; in these cases, SVR-REC is
at a clear advantage, since it includes features for
surface string similarity (e.g., Levenshtein distance
measured on ?s1, c1?), which in effect measure di-
versity, unlike SVR-BASE. Even when ?1 is very
high (1 or 0.8), i.e., when all or most of the weight
is placed on grammaticality, SVR-REC outperforms
SVR-BASE, indicating that the extra features in SVR-
REC also contribute towards assessing grammatical-
ity; by contrast SVR-BASE relies exclusively on the
language model for grammaticality. Unfortunately,
when ?2 is very high (1 or 0.8), i.e., when all or
most of the weight is placed on meaning preserva-
tion, there is no or very small difference between
SVR-REC and SVR-BASE, suggesting that the extra
features of the paraphrase recognizer are not as use-
ful to the SVR, when assessing meaning preserva-
tion, as we would have hoped. Nevertheless, SVR-
REC is overall better than SVR-BASE.
We believe that the dataset of Section 3 and the
evaluation methodology summarized by Figure 3
will prove useful to other researchers, who may wish
to evaluate other ranking components of generate-
and-rank paraphrasing methods against ours, for ex-
ample with different ranking algorithms or features.
Similar datasets of candidate paraphrases can also
be created using different collections of paraphras-
ing rules.16 The same methodology can then be used
to evaluate ranking components on those datasets.
5 Comparison to the state of the art
Having established that SVR-REC is a better config-
uration of our method?s ranker than SVR-BASE, we
16See Androutsopoulos and Malakasiotis (2010) for pointers.
102
proceed to investigate how well our overall generate-
and-rank method (with SVR-REC) compares against
a state of the art paraphrase generator.
As already mentioned, Zhao et al (2010) recently
presented a method (we call it ZHAO-ENG) that out-
performs their previous method (Zhao et al, 2009a),
which used paraphrasing rules and an SMT-like de-
coder (we call that previous method ZHAO-RUL).
Given an input sentence S, ZHAO-ENG produces
candidate paraphrases by translating S to 6 pivot
languages via 3 different commercial machine trans-
lation engines (treated as black boxes) and then back
to the original language, again via 3 machine transla-
tion engines (54 combinations). Roughly speaking,
ZHAO-ENG then ranks the candidate paraphrases by
their average distance from all the other candidates,
selecting the candidate(s) with the smallest distance;
distance is measured as BLEU score (Papineni et
al., 2002).17 Hence, ZHAO-ENG is also, in effect,
a generate-and-rank paraphraser, but the candidates
are generated by invoking multiple machine transla-
tion engines instead of applying paraphrasing rules,
and they are ranked by the average distance measure
rather than using an SVR.
An obvious practical advantage of ZHAO-ENG is
that it exploits the vast resources of existing com-
mercial machine translation engines when generat-
ing candidate paraphrases, which allows it to always
obtain large numbers of candidate paraphrases. By
contrast, the collection of paraphrasing rules that we
currently use does not manage to produce any can-
didate paraphrases in 40% of the sentences of the
New York Times part of AQUAINT, because no rule
applies. Hence, in terms of ability to always para-
phrase the input, ZHAO-ENG is clearly better, though
it should be possible to improve our methods?s per-
formance in that respect by using larger collections
of paraphrasing rules.18 A further interesting ques-
tion, however, is how good the paraphrases of the
two methods are, when both methods manage to
paraphrase the input, i.e., when at least one para-
17We use the version of ZHAO-ENG that Zhao et al (2010)
call ?selection-based?, since they reported it performs overall
better than an alternative decoding-based version.
18Recall that the paraphrasing rules we use were extracted
from an English-Chinese parallel corpus. Additional rules
could be extracted from other parallel corpora, like Europarl
(http://www.statmt.org/europarl/).
phrasing rule applies to S. This scenario can be seen
as an emulation of the case where the collection of
paraphrasing rules is sufficiently large to guarantee
that at least one rule applies to any source sentence.
To answer the latter question, we re-implemented
ZHAO-ENG, with the same machine translation en-
gines and languages used by Zhao et al (2010).
We also trained our paraphraser (with SVR-REC) on
the training part of the dataset of Section 3. We
then selected 300 random source sentences S from
AQUAINT that matched at least one of the paraphras-
ing rules, excluding sentences that had been used be-
fore. Then, for each one of the 300 S sentences, we
kept the single best candidate paraphraseC1 andC2,
respectively, returned by our paraphraser and ZHAO-
ENG. The resulting ?S,C1? and ?S,C2? pairs were
given to 10 human judges. This time the judges
assigned only grammaticality and meaning preser-
vation scores (on a 1?4 scale); diversity was again
computed as edit distance. Each pair was evaluated
by one judge, who was given an equal number of
pairs from the two methods, without knowing which
method each pair came from. The same judge never
rated two pairs with the same S. Since we had no
way to make ZHAO-ENG sensitive to ?1, ?2, ?3, we
trained SVR-REC with ?1 = ?2 = 1/3, as the most
neutral combination of weights.
Table 2 lists the average grammaticality, meaning
preservation, and diversity scores of the two meth-
ods. All scores were normalized in [0, 1], but the
reader should keep in mind that diversity was com-
puted as edit distance, whereas the other two scores
were provided by human judges on a 1?4 scale. The
grammaticality score of our method was better than
ZHAO-ENG?s, and the difference was statistically
significant.19 In meaning preservation, ZHAO-ENG
was slightly better, but the difference was not statis-
tically significant. The difference in diversity was
larger and statistically significant, with the diversity
scores indicating that it takes approximately twice as
many edit operations (insert, delete, replace) to turn
each source sentence to ZHAO-ENG?s paraphrase,
compared to the paraphrase of our method.
We note that our method can be tuned, by ad-
justing the ?i weights, to produce paraphrases with
19We used Analysis of Variance (ANOVA) (Fisher, 1925), fol-
lowed by post-hoc Tukey tests to check whether the scores of
the two methods differ significantly (p < 0.05).
103
score (%) our method ZHAO-ENG
grammaticality 90.89 85.33
meaning preserv. 76.67 78.56
diversity 6.50 14.58
Table 2: Evaluation of our paraphrasing method (with
SVR-REC) against ZHAO-ENG, using human judges. Re-
sults in bold indicate statistically significant differences.
higher grammaticality, meaning preservation, or di-
versity scores; for example, we could increase ?3
and decrease ?1 to obtain higher diversity at the cost
of lower grammaticality in the results of Table 2.20 It
is unclear how ZHAO-ENG could be tuned that way.
Overall, our method seems to perform well
against ZHAO-ENG, despite the vastly larger re-
sources of ZHAO-ENG, provided of course that we
limit ourselves to source sentences to which para-
phrasing rules apply. It would be interesting to in-
vestigate in future work if our method?s coverage
(sentences it can paraphrase) can increase to ZHAO-
ENG?s level by using larger collections of paraphras-
ing rules. It would also be interesting to combine the
two methods, perhaps by using SVR-REC (without
features for the quality scores of the rules) to rank
candidate paraphrases generated by ZHAO-ENG.
6 Conclusions and future work
We presented a generate-and-rank method to para-
phrase sentences. The method first produces can-
didate paraphrases by applying existing paraphras-
ing rules extracted from parallel corpora, and it then
ranks (or classifies) the candidates to keep the best
ones. The ranking component considers not only the
context-insensitive quality scores of the paraphras-
ing rules that produced each candidate, but also fea-
tures intended to measure the extent to which the
rule applications preserve grammaticality and mean-
ing in the particular context of the input sentence, as
well as the degree to which the resulting candidate
differs from the input sentence (diversity).
Initial experiments with a Maximum Entropy
classifier confirmed that the features we use can help
a ranking component select better candidate para-
phrases than a baseline ranker that considers only
20Additional application-specific experiments confirm that
this tuning is possible (Malakasiotis, 2011).
the average context-insensitive quality scores of the
applied rules. Further experiments with an SVR
ranker indicated that our full feature set, which in-
cludes features from an existing paraphrase recog-
nizer, leads to improved performance, compared to
a smaller feature set that includes only the context-
insensitive scores of the rules and language model-
ing scores. We also propose a new methodology to
evaluate the ranking components of generate-and-
rank paraphrase generators, which evaluates them
across different combinations of weights for gram-
maticality, meaning preservation, and diversity. The
paper is accompanied by a paraphrasing dataset we
constructed for evaluations of this kind.
Finally, we evaluated our overall method against
a state of the art sentence paraphraser, which
generates candidates by using several commercial
machine translation systems and pivot languages.
Overall, our method performed well, despite the vast
resources of the machine translation systems em-
ployed by the system we compared against. Our
method performed better in terms of grammaticality,
equally well in meaning preservation, and worse in
diversity, but it could be tuned to obtain higher diver-
sity at the cost of lower grammaticality, whereas it
is unclear how the system we compare against could
be tuned this way. On the other hand, an advantage
of the paraphraser we compared against is that it al-
ways produces paraphrases; by contast, our system
does not produce paraphrases when no paraphrasing
rule applies to the source sentence. Larger collec-
tions of paraphrasing rules would be needed to im-
prove our method in that respect.
Apart from obtaining and experimenting with
larger collections of paraphrasing rules, it would be
interesting to evaluate our method in vivo, for ex-
ample by embedding it in question answering sys-
tems (to paraphrase the questions), in information
extraction systems (to paraphrase extraction tem-
plates), or in natural language generators (to para-
phrase template-like sentence plans). We also plan
to investigate the possibility of embedding our SVR
ranker in the sentence paraphraser we compared
against, i.e., to rank candidates produced by using
several machine translation systems and pivot lan-
guages, as in ZHAO-ENG.
104
Acknowledgments
This work was partly carried out during INDIGO, an
FP6 IST project funded by the European Union, with
additional funding from the Greek General Secre-
tariat of Research and Technology.21
References
I. Androutsopoulos and P. Malakasiotis. 2010. A survey
of paraphrasing and textual entailment methods. Jour-
nal of Artificial Intelligence Research, 38:135?187.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of the 43rd
ACL, pages 597?604, Ann Arbor, MI.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proc. of HLT-NAACL, pages 17?24, New
York, NY.
C. Callison-Burch. 2008. Syntactic constraints on para-
phrases extracted from parallel corpora. In Proc. of
EMNLP, pages 196?205, Honolulu, HI, October.
J. Carletta. 1996. Assessing agreement on classification
tasks: The kappa statistic. Computational Linguistics,
22:249?254.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research,
1(31):399?429.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?69.
N. Cristianini and J. Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009. Rec-
ognizing textual entailment: Rational, evaluation and
approaches. Natural Lang. Engineering, 15(4):i?xvii.
Editorial of the special issue on Textual Entailment.
P. A. Duboue and J. Chu-Carroll. 2006. Answering the
question you wish they had asked: The impact of para-
phrasing for question answering. In Proc. of HLT-
NAACL, pages 33?36, New York, NY.
Ronald A. Fisher. 1925. Statistical Methods for Re-
search Workers. Oliver and Boyd.
T. Joachims. 2002. Learning to Classify Text Using Sup-
port Vector Machines: Methods, Theory, Algorithms.
Kluwer.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL, pages
455?462, New York, NY.
21Consult http://www.ics.forth.gr/indigo/.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probalistic approach to
sentence compression. Artif. Intelligence, 139(1):91?
107.
P. Koehn. 2009. Statistical Machine Translation. Cam-
bridge University Press.
S. Kok and C. Brockett. 2010. Hitting the right para-
phrases in good time. In Proc. of HLT-NAACL, pages
145?153, Los Angeles, CA.
Y. Lepage and E. Denoual. 2005. Automatic genera-
tion of paraphrases to be used as translation references
in objective evaluation measures of machine transla-
tion. In Proc. of the 3rd Int. Workshop on Paraphras-
ing, pages 57?64, Jesu Island, Korea.
N. Madnani and B.J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341?387.
N. Madnani, F. Ayan, P. Resnik, and B. J. Dorr. 2007.
Using paraphrases for parameter tuning in statistical
machine translation. In Proc. of 2nd Workshop on Sta-
tistical Machine Translation, pages 120?127, Prague,
Czech Republic.
P. Malakasiotis. 2009. Paraphrase recognition us-
ing machine learning to combine similarity measures.
In Proc. of the Student Research Workshop of ACL-
AFNLP, Singapore.
P. Malakasiotis. 2011. Paraphrase and Textual Entail-
ment Recognition and Generation. Ph.D. thesis, De-
partment of Informatics, Athens University of Eco-
nomics and Business, Greece.
Y. Marton, C. Callison-Burch, and P. Resnik. 2009.
Improved statistical machine translation using
monolingually-derived paraphrases. In Proc. of
EMNLP, pages 381?390, Singapore.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and I. Szpektor. 2009. Source-language en-
tailment modeling for translating unknown terms. In
Proc. of ACL-AFNLP, pages 791?799, Singapore.
R. Nelken and S. M. Shieber. 2006. Towards robust
context-sensitive sentence alignment for monolingual
corpora. In Proc. of the 11th EACL, pages 161?168,
Trento, Italy.
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Robust machine translation evaluation with en-
tailment features. In Proc. of ACL-AFNLP, pages 297?
305, Singapore.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of the 40th ACL, pages 311?318,
Philadelphia, PA.
C. Quirk, C. Brockett, and W. B. Dolan. 2004. Mono-
lingual machine translation for paraphrase generation.
In Proc. of the Conf. on EMNLP, pages 142?149,
Barcelona, Spain.
105
S. Riezler and Y. Liu. 2010. Query rewriting using
monolingual statistical machine translation. Compu-
tational Linguistics, 36(3):569?582.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proc. of the 45th
ACL, pages 464?471, Prague, Czech Republic.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proc. of ACL-HLT,
pages 683?691, Columbus, OH.
V. Vapnik. 1998. Statistical learning theory. John Wiley.
S. Zhao, H. Wang, T. Liu, and S. Li. 2008. Pivot ap-
proach for extracting paraphrase patterns from bilin-
gual corpora. In Proc. of ACL-HLT, pages 780?788,
Columbus, OH.
S. Zhao, X. Lan, T. Liu, and S. Li. 2009a. Application-
driven statistical paraphrase generation. In Proc. of
ACL-AFNLP, pages 834?842, Singapore.
S. Zhao, H. Wang, T. Liu, and Li. S. 2009b. Extract-
ing paraphrase patterns from bilingual parallel cor-
pora. Natural Language Engineering, 15(4):503?526.
S. Zhao, H. Wang, X. Lan, and T. Liu. 2010. Leverag-
ing multiple MT engines for paraphrase generation. In
Proceedings of the 23rd COLING, pages 1326?1334,
Beijing, China.
L. Zhou, C.-Y. Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proc. of the Conf. on EMNLP, pages 77?84.
106
Proceedings of the EACL 2009 Demonstrations Session, pages 37?40,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
Adaptive Natural Language Interaction
Stasinos Konstantopoulos
Athanasios Tegos
Dimitris Bilidas
NCSR ?Demokritos?, Athens, Greece
Colin Matheson
Human Communication Research Centre
Edinburgh University, U.K.
Ion Androutsopoulos
Gerasimos Lampouras
Prodromos Malakasiotis
Athens Univ. of Economics and Business
Greece
Olivier Deroo
Acapela Group, Belgium
Abstract
The subject of this demonstration is natu-
ral language interaction, focusing on adap-
tivity and profiling of the dialogue man-
agement and the generated output (text
and speech). These are demonstrated in
a museum guide use-case, operating in a
simulated environment. The main techni-
cal innovations presented are the profiling
model, the dialogue and action manage-
ment system, and the text generation and
speech synthesis systems.
1 Introduction
In this demonstration we present a number of
state-of-the art language technology tools, imple-
menting and integrating the latest discourse and
knowledge representation theories into a complete
application suite, including:
? dialogue management, natural language gen-
eration, and speech synthesis, all modulated
by a flexible and highly adaptable profiling
mechanism;
? robust speech recognition and language inter-
pretation; and,
? an authoring environment for developing the
representation of the domain of discourse as
well as the associated linguistic and adaptiv-
ity resources.
The system demonstration is based on a use
case of a virtual-tour guide in a museum domain.
Demonstration visitors interact with the guide us-
ing headsets and are able to experiment with load-
ing different interaction profiles and observing the
differences in the guide?s behaviour. The demon-
stration also includes the screening of videos from
an embodied instantiation of the system as a robot
guiding visitors in a museum.
2 Technical Content
The demonstration integrates a number of state-of-
the-art language components into a highly adap-
tive natural language interaction system. Adap-
tivity here refers to using interaction profiles that
modulate dialogue management as well as text
generation and speech synthesis. Interaction pro-
files are semantic models that extend the objective
ontological model of the domain of discourse with
subjective information, such as how ?interesting?
or ?important? an entity or statement of the objec-
tive domain model is.
Advanced multimodal dialogue management
capabilities involving and combining input and
output from various interaction modalities and
technologies, such as speech recognition and syn-
thesis, natural language interpretation and gener-
ation, and recognition of/response to user actions,
gestures, and facial expressions.
State-of-the art natural language generation
technology, capable of producing multi-sentence,
coherent natural language descriptions of objects
based on their abstract semantic representation.
The resulting descriptions vary dynamically in
terms of content as well as surface language ex-
pressions used to realize each description, depend-
ing on the interaction history (e.g., comparing
to previously given information) and the adaptiv-
ity parameters (exhibiting system personality and
adapting to user background and interests).
3 System Description
The system is capable of interacting in a vari-
ety of modalities, including non-verbal ones such
as gesture and face-expression recognition, but in
this demonstration we focus on the system?s lan-
guage interaction components. In this modality,
abstract, language-independent system actions are
first planned by the dialogue and action manager
(DAM), then realized into language-specific text
37
by the natural language generation engine, and fi-
nally synthesized into speech. All three layers are
parametrized by a profiling and adaptivity module.
3.1 Profiling and Adaptation
Profiling and adaptation modulates the output of
dialogue management, generation, and speech
synthesis so that the system exhibits a synthetic
personality, while at the same time adapting to
user background and interests.
User stereotypes (e.g., ?expert? or ?child?) pro-
vide generation parameters (such as maximum de-
scription length) and also initialize the dynamic
user model with interest rates for all the ontologi-
cal entities (individuals and properties) of the do-
main of discourse. This same information is also
provided in system profiles reflecting the system?s
(as opposed to the users?) preferences; one can,
for example, define a profile that favours using
the architectural attributes to describe a building
where another profile would choose to concentrate
on historical facts regarding the same building.
Stereotypes and profiles are combined into a
single set of parameters by means of personal-
ity models. Personality models are many-valued
Description Logic definitions of the overall pref-
erence, grounded in stereotype and profile data.
These definitions model recognizable personality
traits so that, for example, an open personality will
attend more to the user?s requests than its own
interests in deriving overall preference (Konstan-
topoulos et al, 2008).
Furthermore, the system dynamically adapts
overall preference according to both interaction
history and the current dialogue state. So, for one,
the initial (static model) interest factor of an ontol-
ogy entity is reduced each time this entity is used
in a description in order to avoid repetitions. On
the other hand, preference will increase if, for ex-
ample, in the current state the user has explicitly
asked about an entity.
3.2 Dialogue and Action Management
The DAM is built around the information-state
update dialogue paradigm of the TRINDIKIT
dialogue-engine toolkit (Cooper and Larsson,
1998) and takes into account the combined user-
robot interest factor when determining informa-
tion state updates.
The DAM combines various interaction modal-
ities and technologies in both interpretation/fusion
and generation/fission. In interpreting user ac-
tions the system recognizes spoken utterances,
simple gestures, and touch-screen input, all of
which may be combined into a representation of
a multi-modal user action. Similarly, when plan-
ning robotic actions the DAM coordinates a num-
ber of available output modalities, including spo-
ken language, text (on the touchscreen), the move-
ment and configuration of the robotic platform, fa-
cial expressions, and simple head gestures.1
To handle multimodal input, the DAM uses a fu-
sion module which combines messages from the
language interpretation, gesture, and touchscreen
modules into a single XML structure. Schemati-
cally, this can be represented as:
<userAction>
<userUtterance>hello</userUtterance>
<userButton content="13"/>
</userAction>
This structure represents a user pressing some-
thing on the touchscreen and saying hello at the
same time.2
The representation is passed essentially un-
changed to the DAM, to be processed by its up-
date rules, where the ID of button press is inter-
preted in context and matched with the speech.
In most circumstances, the natural language pro-
cessing component (see 3.3) produces a seman-
tic representation of the input which appears in
the userUtterance element; the use of ?hello?
above is for illustration. An example update rule
which will fire in the context of a greeting from
the user is (in schematic form):
if
in(/latest_utterance/moves, hello)
then
output(start)
Update rules contain a list of conditions and a
list of effects. Here there is one condition (that the
latest moves from the user includes ?hello?), and
one effect (the ?start? procedure). The latter initi-
ates the dialogue by, among other things, having
the system utter a standardised greeting.
As noted above, the DAM is also multimodal
on the output side. An XML representation is
created which can contain robot utterances and
robot movements (both head movements and mo-
bile platform moves). Information can also be pre-
sented on the touchscreen.
1Expressions and gestures will not be demonstrated, as
they can not be materialized in the simulated robot.
2The precise meaning of ?at the same time? is determined
by the fusion module.
38
3.3 Natural Language Processing
The NATURALOWL natural language generation
(NLG) engine (Galanis et al 2009) produces
multi-sentence, coherent natural language descrip-
tions of objects in multiple languages from a sin-
gle semantic representation; the resulting descrip-
tions are annotated with prosodic markup for driv-
ing the speech synthesisers.
The generated descriptions vary dynamically, in
both content and language expressions, depending
on the interaction profile as well as the dynamic
interaction history. The dynamic preference factor
of the item itself is used to decide the level of de-
tail of the description being generated. The prefer-
ence factors of the properties are used to order the
contents of the descriptions to ensure that, in cases
where not all possible facts are to be presented in
a single turn, the most relevant ones are chosen.
The interaction history is used to check previously
given information to avoid repeating the same in-
formation in different contexts and to create com-
parisons with earlier objects.
NaturalOWL demonstrates the benefits of
adopting NLG on the Semantic Web. Organiza-
tions that need to publish information about ob-
jects, such as exhibits or products, can publish
OWL ontologies instead of texts. NLG engines,
embedded in browsers or Web servers, can then
render the ontologies in natural language, whereas
computer programs may access the ontologies, in
effect logical statements, directly. The descrip-
tions can be very simple and brief, relying on
question answering to provide more information
if such is requested. This way, machine-readable
information can be more naturally inspected and
consulted by users.
In order to generate a list of possible follow
up questions that the system can handle, we ini-
tially construct a list of the particular individuals
or classes that are mentioned in the generated de-
scription; the follow up questions will most likely
refer to them. Only individuals and classes for
which there is further information in the ontology
are extracted.
After identifying the referred individuals and
classes, we proceed to predict definition (e.g.,
?Who was Ares??) and property questions (e.g.,
?Where is Mount Penteli??) about them that
could be answered by the information in the on-
tology. We avoid generating questions that cannot
be answered. The expected definition questions
are constructed by inserting the names of the re-
ferred individuals and classes into templates such
as ?who is/was person X?? or ?what do you know
about class or entity Y??.
In the case of referred individuals, we also gen-
erate expected property questions using the pat-
terns NaturalOWL generates the descriptions with.
These patterns, called microplans, show how to
express the properties of the ontology as sentences
of the target languages. For example, if the indi-
vidual templeOfAres has the property excavate-
dIn, and that property has a microplan of the form
?resource was excavated in period?, we anticipate
questions such as ?when was the Temple of Ares
excavated?? and ?which period was the Temple of
Ares excavated in??.
Whenever a description (e.g., of a monument)
is generated, the expected follow up questions for
that description (e.g., about the monument?s ar-
chitect) are dynamically included in the rules of
the speech recognizer?s grammar, to increase word
recognition accuracy. The rules include compo-
nents that extract entities, classes, and properties
from the recognized questions, thus allowing the
dialogue and action manager to figure out what the
user wishes to know.
3.4 Speech Synthesis and Recognition
The natural language interface demonstrates ro-
bust speech recognition technology, capable of
recognizing spoken phrases in noisy environ-
ments, and advanced speech synthesis, capable of
producing spoken output of very high quality. The
main challenge that the automatic speech recogni-
tion (ASR) module needs to address is background
noise, especially in the robot-embodied use case.
A common technique used in order to handle this
is training acoustic models with the anticipated
background noise, but that is not always possi-
ble. The demonstrated ASR module can be trained
on noise-contaminated data where available, but
also incorporates multi-band acoustic modelling
(Dupont, 2003) for robust recognition under noisy
conditions. Speech recognition rates are also sub-
stantially improved by using the predictions made
by NATURALOWL and the DAM to dynamically
restrict the lexical and phrasal expectations at each
dialogue turn.
The speech synthesis module of the demon-
strated system is based on unit selection technol-
ogy, generally recognized as producing more nat-
39
ural output that previous technologies such as di-
phone concatenation or formant synthesis. The
main innovation that is demonstrated is support for
emotion, a key aspect of increasing the naturalness
of synthetic speech. This is achieved by combin-
ing emotional unit recordings with run-time trans-
formations. With respect to the former, a complete
?voice? now comprises three sub-voices (neutral,
happy, and sad), based on recordings of the same
speaker. The recording time needed is substan-
tially decreased by prior linguistic analysis that se-
lects appropriate text covering all phonetic units
needed by the unit selection system. In addition to
the statically defined sub-voices, the speech syn-
thesis module implements dynamic transforma-
tions (e.g., emphasis), pauses, and variable speech
speed. The system combines all these capabilities
in order to dynamically modulate the synthesised
speech to convey the impression of emotionally
modulated speech.
3.5 Authoring
The interaction system is complemented by
ELEON (Bilidas et al, 2007), an authoring tool for
annotating domain ontologies with the generation
and adaptivity resources described above. The do-
main ontology can be authored in ELEON, but any
existing OWL ontology can also annotated.
More specifically, ELEON supports author-
ing linguistic resources, including a domain-
dependent lexicon, which associates classes and
individuals of the ontology with nouns and proper
names of the target natural languages; microplans,
which provide the NLG with patterns for realizing
property instances as sentences; and a partial or-
dering of properties, which allows the system to
order the resulting sentences as a coherent text.
The adaptivity and profiling resources include
interest rates, indicating how interesting the enti-
ties of the ontology are in any given profile; and
stereotype parameters that control generation as-
pects such as the number of facts to include in a
description or the maximum sentence length.
Furthermore, ELEON supports the author with
immediate previews, so that the effect of any
change in either the ontology or the associated re-
sources can be directly reviewed. The actual gen-
eration of the preview is relegated to external gen-
eration engines.
4 Conclusions
The demonstrated system combines semantic rep-
resentation and reasoning technologies with lan-
guage technology into a human-computer interac-
tion system that exhibits a large degree of adapt-
ability to audiences and circumstances and is able
to take advantage of existing domain model cre-
ated independently of the need to build a natural
language interface. Furthermore by clearly sepa-
rating the abstract, semantic layer from that of the
linguistic realization, it allows the re-use of lin-
guistic resources across domains and the domain
model and adaptivity resources across languages.
Acknowledgements
The demonstrated system is being developed by
the European (FP6-IST) project INDIGO.3 IN-
DIGO develops and advances human-robot inter-
action technology, enabling robots to perceive nat-
ural human behaviour, as well as making them
act in ways that are more familiar to humans. To
achieve its goals, INDIGO advances various tech-
nologies, which it integrates in a robotic platform.
References
Dimitris Bilidas, Maria Theologou, and Vangelis
Karkaletsis. 2007. Enriching OWL ontologies
with linguistic and user-related annotations: the
ELEON system. In Proc. 19th Intl. Conf. on
Tools with Artificial Intelligence (ICTAI-2007).
Robin Cooper and Staffan Larsson. 1998. Dia-
logue Moves and Information States. In: Pro-
ceedings of the 3rd Intl. Workshop on Computa-
tional Semantics (IWCS-3).
Ste?phane Dupont. 2003. Robust parameters
for noisy speech recognition. U.S. Patent
2003182114.
Dimitrios Galanis, George Karakatsiotis, Gerasi-
mos Lampouras and Ion Androutsopoulos.
2009. An open-source natural language gener-
ator for OWL ontologies and its use in Prote?ge?
and Second Life. In this volume.
Stasinos Konstantopoulos, Vangelis Karkaletsis,
and Colin Matheson. 2008. Robot personality:
Representation and externalization. In Proc.
Computational Aspects of Affective and Emo-
tional Interaction (CAFFEi 08), Patras, Greece.
3http://www.ics.forth.gr/indigo/
40
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 562?567, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
nlp.cs.aueb.gr: Two Stage Sentiment Analysis
Prodromos Malakasiotis, Rafael Michael Karampatsis
Konstantina Makrynioti and John Pavlopoulos
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
Abstract
This paper describes the systems with which
we participated in the task Sentiment Analysis
in Twitter of SEMEVAL 2013 and specifically
the Message Polarity Classification. We used
a 2-stage pipeline approach employing a lin-
ear SVM classifier at each stage and several
features including BOW features, POS based
features and lexicon based features. We have
also experimented with Naive Bayes classi-
fiers trained with BOW features.
1 Introduction
During the last years, Twitter has become a very
popular microblogging service. Millions of users
publish messages every day, often expressing their
feelings or opinion about a variety of events, top-
ics, products, etc. Analysing this kind of content
has drawn the attention of many companies and re-
searchers, as it can lead to useful information for
fields, such as personalized marketing or social pro-
filing. The informal language, the spelling mis-
takes, the slang and special abbreviations that are
frequently used in tweets differentiate them from
traditional texts, such as articles or reviews, and
present new challenges for the task of sentiment
analysis.
The Message Polarity Classification is defined as
the task of deciding whether a message M conveys a
positive, negative or neutral sentiment. For instance
M1 below expresses a positive sentiment, M2 a neg-
ative one, while M3 has no sentiment at all.
M1: GREAT GAME GIRLS!! On to districts Monday
at Fox!! Thanks to the fans for coming out :)
M2: Firework just came on my tv and I just broke down
and sat and cried, I need help okay
M3: Going to a bulls game with Aaliyah & hope next
Thursday
As sentiment analysis in Twitter is a very recent
subject, it is certain that more research and improve-
ments are needed. This paper presents our approach
for the subtask of Message Polarity Classification
(Wilson et al, 2013) of SEMEVAL 2013. We used a
2-stage pipeline approach employing a linear SVM
classifier at each stage and several features includ-
ing bag of words (BOW) features, part-of-speech
(POS) based features and lexicon based features.
We have also experimented with Naive Bayes clas-
sifiers trained with BOW features.
The rest of the paper is organised as follows. Sec-
tion 2 provides a short analysis of the data used
while section 3 describes our approach. Section 4
describes the experiments we performed and the cor-
responding results and section 5 concludes and gives
hints for future work.
2 Data
Before we proceed with our system description we
briefly describe the data released by the organisers.
The training set consists of a set of IDs correspond-
ing to tweet messages, along with their annotations.
A message can be annotated as positive, negative
or neutral. In order to address privacy concerns,
rather than releasing the original Tweets, the organ-
isers chose to provide a python script for download-
ing the data. This resulted to different training sets
for the participants since tweets may often become
562
SEMEVAL STATS TRAIN (ours) TRAIN (official) Dev DEV (final) TEST (sms)
Positive 3280 37,57% 3640 37,59% 524 34,82% 575 34,76% 492 23,50%
Negative 1289 14,77% 1458 15,06% 308 20,47% 340 20,56% 394 18,82%
Neutral 4161 47,66% 4586 47,36% 673 44,72% 739 44,68% 1208 57,69%
TOTAL 8730 9684 1505 1654 2094
37,57% 
14,77% 
47,66% 
Training data class distribution 
Positive
Negative
Neutral
34,76% 
20,56% 
44,68% 
Development data class distribution 
Positive
Negative
Neutral
23,50% 
18,82% 57,69% 
Test data class distribution (sms) 
Positive
Negative
Neutral
41,23% 
15,76% 
43,01% 
Test data class distribution (twitter) 
Positive
Negative
Neutral
(a)
SEMEVAL STATS TRAIN (ours) TRAIN (official) Dev DEV (final) TEST (sms)
Positive 3280 37,57% 3640 37,59% 524 34,82% 575 34,76% 492 23,50%
Negative 1289 14,7 % 1458 15,06% 308 20,47% 340 20,56% 394 18, 2%
Neutral 4161 47,6 % 4586 47,36% 673 4 ,72% 739 4 ,68% 1208 57,69%
TOTAL 8730 9684 1505 1654 2094
37,57% 
14,7 % 
47,6 % 
Training data clas  distribution 
Positive
Negative
Neutral
34,76% 
20,56% 
4 ,68% 
Development data clas  distribution 
Posit ve
Negative
Neutral
23,50% 
18,82% 57,69% 
Test data class distribution (sms) 
Positive
Negative
Neutral
41,23% 
15,76% 
43,01% 
Test data clas  distribution (twit er) 
Posit ve
Negative
Neutral
(b)
Figure 1: Train and Development data class distribution.
unavailable due to a number of reasons. Concerning
the development and test sets the organisers down-
loaded and provided the tweets. 1 A first analysis
of the data indicates that they suffer from a class im-
balance problem. Specifically the training data we
have downloaded contain 8730 tweets (3280 posi-
tive, 1289 negative, 4161 neutral), while the devel-
opment set contains 1654 tweets (575 positive, 340
negative, 739 neutral). Figure 1 illustrates the prob-
lem on train and development sets.
3 System Overview
The system we propose is a 2?stage pipeline pro-
cedure employing SVM classifiers (Vapnik, 1998)
to detect whether each message M expresses pos-
itive, negative or no sentiment (figure 2). Specifi-
cally, during the first stage we attempt to detect if M
expresses a sentiment (positive or negative) or not.
If so, M is called ?subjective?, otherwise it is called
?objective? or ?neutral?.2 Each subjective message
is then classified in a second stage as ?positive? or
?negative?. Such a 2?stage approach has also been
suggested in (Pang and Lee, 2004) to improve sen-
timent classification of reviews by discarding objec-
tive sentences, in (Wilson et al, 2005a) for phrase-
level sentiment analysis, and in (Barbosa and Feng,
2010) for sentiment analysis on Twitter messages.
1A separate test set with SMS messages was also provided
by the organisers to measure performance of systems over other
types of message data. No training and development data were
provided for this set.
2Hereafter we will use the terms ?objective? and ?neutral?
interchangeably.
3.1 Data Preprocessing
Before we could proceed with feature engineering,
we performed several preprocessing steps. To be
more precise, a twitter specific tokeniser and part-
of-speech (POS) tagger (Ritter et al, 2011) were
used to obtain the tokens and the corresponding
POS tags which are necessary for a particular set
of features to be described later. In addition to these,
six lexicons, originating from Wilson?s (2005b) lexi-
con, were created. This lexicon contains expressions
that given a context (i.e., surrounding words) indi-
cate subjectivity. The expression that in most con-
text expresses sentiment is considered to be ?strong?
subjective, otherwise it is considered weak subjec-
tive (i.e., it has specific subjective usages). So, we
first split the lexicon in two smaller, one contain-
ing strong and one containing weak subjective ex-
pressions. Moreover, Wilson also reports the polar-
ity of each expression out of context (prior polarity)
which can be positive, negative or neutral. As a con-
sequence, we further split each of the two lexicons
into three smaller according to the prior polarity of
the expression, resulting to the following six lexi-
cons:
S+ : Contains strong subjective expressions with
positive prior polarity.
S? : Contains strong subjective expressions with
negative prior polarity.
S0 : Contains strong subjective expressions with
neutral prior polarity.
563
 Subjectivity detection 
SVM 
Polarity detection 
SVM 
Objective 
messages 
Messages 
Subjective 
messages 
Positive 
messages 
Negative 
messages 
Figure 2: Our 2?stage pipeline procedure.
W+ : Contains weak subjective expressions with
positive prior polarity.
W? : Contains weak subjective expressions with
negative prior polarity.
W0 : Contains weak subjective expressions with
neutral prior polarity.
Adding to these, three more lexicons were created,
one for each class (positive, negative, neutral). In
particular, we employed Chi Squared feature selec-
tion (Liu and Setiono, 1995) to obtain the 100 most
important tokens per class from the training set.
Very few tokens were manually erased to result to
the following three lexicons.
T+ : Contains the top-94 tokens appearing in posi-
tive tweets of the training set.
T? : Contains the top-96 tokens appearing in nega-
tive tweets of the training set.
T0 : Contains the top-94 tokens appearing in neutral
tweets of the training set.
The nine lexicons described above are used to cal-
culate precision (P (t, c)), recall (R(t, c)) and F ?
measure (F1(t, c)) of tokens appearing in a mes-
sage with respect to each class. Equations 1, 2 and 3
below provide the definitions of these metrics.
P (t, c) =
#tweets that contain token t and belong to class c
#tweets that contain token t
(1)
R(t, c) =
#tweets that contain token t and belong to class c
#tweets that belong to class c
(2)
F1(t, c) =
2 ? P (t, c) ? R(t, c)
P (t, c) + R(t, c)
(3)
3.2 Feature engineering
We employed three types of features, namely
boolean features, POS based features and lexicon
based features. Our goal is to build a system that is
not explicitly based on the vocabulary of the training
set, having therefore better generalisation capability.
3.2.1 Boolean features
Bag of words (BOW): These features indicate the
existence of specific tokens in a message. We
used feature selection with Info Gain to obtain
the 600 most informative tokens of the training
set and we then manually removed 19 of them
564
to result in 581 tokens. As a consequence we
get 581 features that can take a value of 1 if a
message contains the corresponding token and
0 otherwise.
Time and date: We observed that time and date of-
ten indicated events in the train data and such
messages tend to be objective. Therefore, we
added two more features to indicate if a mes-
sage contains time and/or date expressions.
Character repetition: Repetitive characters are of-
ten added to words by users, in order to give
emphasis or to express themselves more in-
tensely. As a consequence they indicate sub-
jectivity. So we added one more feature having
a value of 1 if a message contains words with
repeating characters and 0 otherwise.
Negation: Negation not only is a good subjectivity
indicator but it also may change the polarity of
a message. We therefore add 5 more features,
one indicating the existence of negation, and
the remaining four indicating the existence of
negation that precedes (in a distance of at most
5 tokens) words from lexicons S+, S?, W+ and
W?.
Hash-tags with sentiment: These features are im-
plemented by getting all the possible sub-
strings of the string after the symbol # and
checking if any of them match with any word
from S+, S?, W+ and W? (4 features). A
value of 1 means that a hash-tag containing a
word from the corresponding lexicon exists in
a message.
3.2.2 POS based features
Specific POS tags might be good indicators of
subjectivity or objectivity. For instance adjectives
often express sentiment (e.g., beautiful, frustrating)
while proper nouns are often reported in objective
messaged. We, therefore, added 10 more features
based on the following POS tags:
1. adjectives,
2. adverbs,
3. verbs,
4. nouns,
5. proper nouns,
6. urls,
7. interjections,
8. hash-tags,
9. happy emoticons, and
10. sad emoticons.
We then constructed our features as follows. For
each message we counted the occurrences of tokens
with these POS tags and we divided this number
with the number of tokens having any of these POS
tags. For instance if a message contains 2 adjectives,
1 adverb and 1 url then the features corresponding to
adjectives, adverbs and urls will have a value of 24 ,
1
4
and 14 respectively while all the remaining features
will be 0. These features can be thought of as a way
to express how much specific POS tags affect the
sentiment of a message.
Going a step further we calculate precision
(P (b, c)), recall (R(b, c)) and F ? measure
(F1(b, c)) of POS tags bigrams with respect to each
class (equations 4, 5 and 6 respectively).
P (b, c) =
#tweets that contain bigram b and belong to class c
#tweets that contain bigram b
(4)
R(b, c) =
#tweets that contain bigram b and belong to class c
#tweets that belong to class c
(5)
F1(b, c) =
2 ? P (b, c) ? R(b, c)
P (b, c) + R(b, c)
(6)
For each bigram (e.g., adjective-noun) in a mes-
sage we calculate F1(b, c) and then we use the aver-
age, the maximum and the minimum of these values
to create 9 additional features. We did not experi-
ment over measures that weight differently Precision
and Recall (e.g., Fb for b = 0.5) or with different
combinations (e.g., F1 and P ).
3.2.3 Lexicon based features
This set of features associates the words of the
lexicons described earlier with the three classes.
Given a message M , similarly to the equations 4 and
565
6 above, we calculate P (t, c) and F1(t, c) for every
token t ? M with respect to a lexicon. We then ob-
tain the maximum, minimum and average values of
P (t, c) and F1(t, c) in M . We note that the combi-
nation of P and F1 appeared to be the best in our
experiments while R(t, c) was not helpful and thus
was not used. Also, similarly to section 3.2.2 we
did not experiment over measures that weight differ-
ently Precision and Recall (e.g., Fb for b = 0.5). The
former metrics are calculated with three variations:
(a) Using words: The values of the metrics con-
sider only the words of the message.
(b) Using words and priors: The same as (a) but
adding to the calculated metrics a prior value.
This value is calculated on the entire lexicon,
and roughly speaking it is an indicator of how
much we can trust L to predict class c. In cases
that a token t of a message M does not appear
in a lexicon L the corresponding scores of the
metrics will be 0.
(c) Using words and their POS tags: The values
of the metrics consider the words of the message
along with their POS tags.
(d) Using words, their POS tags and priors: The
same as (c) but adding to the calculated metrics
an apriori value. The apriori value is calculated
in a similar manner as in (b) with the difference
that we consider the POS tags of the words as
well.
For case (a) we calculated minimum, maximum
and average values of P (t, c) and F1(t, c) with re-
spect to S+, S?, S0, W+, W? and W0 consider-
ing only the words of the message resulting to 108
features. Concerning case (b) we calculated average
P (t, c) and F1(t, c) with respect to S+, S?, S0, W+,
W? and W0, and average P (t, c) with respect to T+,
T? and T0 adding 45 more features. For case (c) we
calculated minimum, maximum and average P (t, c)
with respect to S+, S?, S0, W+, W? and W0 (54
features), and, finally, for case (d) we calculated av-
erage P (t, c) and F1(t, c) with respect to S+, S?,
S0, W+, W? and W0 to add 36 features.
Class F1
Positive 0.6496
Negative 0.4429
Neutral 0.7022
Average 0.5462
Table 1: F1 for development set.
4 Experiments
As stated earlier we use a 2?stage pipeline approach
to identify the sentiment of a message. Preliminary
experiments on the development data showed that
this approach is better than attempting to address the
problem in one stage during which a classifier must
classify a message as positive, negative or neutral.
To be more precise we used a Naive Bayes classifier
and BOW features using both 1?stage and 2?stage
approaches. Although we considered the 2?stage
approach with a Naive Bayes classifier as a baseline
system we used it to submit results for both twitter
and sms test sets.
Having concluded to the 2?stage approach we
employed for each stage an SVM classifier, fed with
the 855 features described in section 3.2.3 Both
SVMs use linear kernel and are tuned in order to
find the optimum C parameter. Observe that we use
the same set of features in both stages and let the
classifier learn the appropriate weights for each fea-
ture. During the first stage, the classifier is trained
on the entire training set after merging positive and
negative classes to one superclass, namely subjec-
tive. In the second stage, the classifier is trained only
on positive and negative tweets of the training and
is asked to determine whether the messages classi-
fied as subjective during the first stage are positive
or negative.
4.1 Results
In order to obtain the best set of features we trained
our system on the downloaded training data and
measured its performance on the provided develop-
ment data. Table 1 illustrates the F1 results on the
development set. A first observation is that there
is a considerable difference between the F1 of the
negative class and the other two, with the former be-
3We used the LIBLINEAR distribution (Fan et al, 2008)
566
Class F1
Positive 0.6854
Negative 0.4929
Neutral 0.7117
Average 0.5891
Table 2: F1 for twitter test set.
Class F1
Positive 0.6349
Negative 0.5131
Neutral 0.7785
Average 0.5740
Table 3: F1 for sms test set.
ing significantly decreased. This might be due to
the quite low number of negative tweets of the ini-
tial training set in comparison with the rest of the
classes. Therefore, the addition of 340 negative ex-
amples from the development set emerged from this
imbalance and proved to be effective as shown in ta-
ble 2 illustrating our results on the test set regarding
tweets. Unfortunately we were not able to submit
results with this system for the sms test set. How-
ever, we performed post-experiments after the gold
sms test set was released. The results shown on table
3 are similar to the ones obtained for the twitter test
set which means that our model has a good general-
isation ability.
5 Conclusion and future work
In this paper we presented our approach for the
Message Polarity Classification task of SEMEVAL
2013. We proposed a pipeline approach to detect
sentiment in two stages; first we discard objective
messages and then we classify subjective (i.e., car-
rying sentiment) ones as positive or negative. We
used SVMs with various extracted features for both
stages and although the system performed reason-
ably well, there is still much room for improvement.
A first problem that should be addressed is the dif-
ficulty in identifying negative messages. This was
mainly due to small number of tweets in the train-
ing data. This was somewhat alleviated by adding
the negative instances of the development data but
still our system reports lower results for this class as
compared to positive and neutral classes. More data
or better features is a possible improvement. An-
other issue that has not an obvious answer is how to
proceed in order to improve the 2?stage pipeline ap-
proach. Should we try and optimise each stage sepa-
rately or should we optimise the second stage taking
into consideration the results of the first stage?
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Beijing, China. Association for Compu-
tational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Huan Liu and Rudy Setiono. 1995. Chi2: Feature se-
lection and discretization of numeric attributes. In
Tools with Artificial Intelligence, 1995. Proceedings.,
Seventh International Conference on, pages 388?391.
IEEE.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?04, Barcelona, Spain. As-
sociation for Computational Linguistics.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In EMNLP, pages 1524?1534.
V. Vapnik. 1998. Statistical learning theory. John Wiley.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005a. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Vancouver, British Columbia, Canada.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Empir-
ical Methods in Natural Language Processing, pages
347?354. Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
567
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 114?118,
Dublin, Ireland, August 23-24, 2014.
AUEB: Two Stage Sentiment Analysis of Social Network Messages
Rafael Michael Karampatsis, John Pavlopoulos and Prodromos Malakasiotis
mpatsis13@gmail.com, annis@aueb.gr, rulller@aueb.gr
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
Abstract
This paper describes the system submit-
ted for the Sentiment Analysis in Twitter
Task of SEMEVAL 2014 and specifically
the Message Polarity Classification sub-
task. We used a 2?stage pipeline approach
employing a linear SVM classifier at each
stage and several features including mor-
phological features, POS tags based fea-
tures and lexicon based features.
1 Introduction
Recently, Twitter has gained significant popularity
among the social network services. Lots of users
often use Twitter to express feelings or opinions
about a variety of subjects. Analysing this kind of
content can lead to useful information for fields,
such as personalized marketing or social profiling.
However such a task is not trivial, because the lan-
guage used in Twitter is often informal presenting
new challenges to text analysis.
In this paper we focus on sentiment analysis,
the field of study that analyzes people?s sentiment
and opinions from written language (Liu, 2012).
Given some text (e.g., tweet), sentiment analysis
systems return a sentiment label, which most often
is positive, negative, or neutral. This classification
can be performed directly or in two stages; in the
first stage the system examines whether the text
carries sentiment and in the second stage, the sys-
tem decides for the sentiment?s polarity (i.e., posi-
tive or negative).
1
This decomposition is based on
the assumption that subjectivity detection and sen-
timent polarity detection are different problems.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
For instance a 2?stage approach is better suited to sys-
tems that focus on subjectivity detection; e.g., aspect based
sentiment analysis systems which extract aspect terms only
from evaluative texts.
We choose to follow the 2?stage approach, be-
cause it allows us to focus on each of the two prob-
lems separately (e.g., features, tuning, etc.). In the
following we will describe the system with which
we participated in the Message Polarity Classi-
fication subtask of Sentiment Analysis in Twit-
ter (Task 9) of SEMEVAL 2014 (Rosenthal et al.,
2014). Specifically Section 2 describes the data
provided by the organizers of the task. Sections 3
and 4 present our system and its performance re-
spectively. Finally, Section 5 concludes and pro-
vides hints for future work.
2 Data
At first, we describe the data used for this year?s
task. For system tuning the organizers released the
training and development data of SEMEVAL 2013
Task 2 (Wilson et al., 2013). Both these sets are
allowed to be used for training. The organizers
also provided the test data of the same Task to be
used for development only. As argued in (Malaka-
siotis et al., 2013) these data suffer from class im-
balance. Concerning the test data, they contained
8987 messages broken down in the following 5
datasets:
? LJ
14
: 2000 sentences from LIVEJOURNAL.
? SMS
13
: SMS test data from last year.
? TW
13
: Twitter test data from last year.
? TW
14
: 2000 new tweets.
? TWSARC
14
: 100 tweets containing sarcasm.
The details of the test data were made available to
the participants only after the end of the Task. Re-
call that SMS
13
and TW
13
were also provided as
development data. In this way the organizers were
able to check, i) the progress of the systems since
last year?s task, and ii) the generalization capabil-
ity of the participating systems.
114
3 System Overview
The main objective of our system is to detect
whether a message M expresses positive, negative
or no sentiment. To achieve that we follow a 2?
stage approach. During the first stage we detect
whether M expresses sentiment (?subjective?) or
not; this process is called subjectivity detection.
In the second stage we classify the ?subjective?
messages of the first stage as ?positive? or ?neg-
ative?. Both stages utilize a Support Vector Ma-
chine (SVM (Vapnik, 1998)) classifier with lin-
ear kernel.
2
Similar approaches have also been
proposed in (Pang and Lee, 2004; Wilson et al.,
2005; Barbosa and Feng, 2010; Malakasiotis et al.,
2013). Finally, we note that the 2?stage approach,
in datasets such the one here (Malakasiotis et al.,
2013), alleviates the class imbalance problem.
3.1 Data preprocessing
A very essential part of our system is data pre-
processing. At first, each message M is passed
through a twitter specific tokenizer and part-of-
speech (POS) tagger (Owoputi et al., 2013) to ob-
tain the tokens and the corresponding POS tags,
which are necessary for some sets of features.
3
We then use a dictionary to replace any slang with
the actual text.
4
We also normalize the text of
each message by combining a trie data structure
(De La Briandais, 1959) with an English dictio-
nary.
5
In more detail, we replace every token of M
not in the dictionary with the most similar word of
the dictionary. Finally, we obtain POS tags of all
the new tokens.
3.2 Sentiment lexicons
Another key attribute of our system is the use of
sentiment lexicons. We have used the following:
? HL (Hu and Liu, 2004).
? SENTIWORDNET (Baccianella et al., 2010).
? SENTIWORDNET lexicon with POS tags
(Baccianella et al., 2010).
? AFINN (Nielsen, 2011).
? MPQA (Wilson et al., 2005).
2
We used the LIBLINEAR distribution (Fan et al., 2008)
3
Tokens could be words, emoticons, hashtags, etc. No
lemmatization or stemming has been applied
4
See http://www.noslang.com/dictionary/.
5
We used the OPENOFFICE dictionary
? NRC Emotion lexicon (Mohammad and Tur-
ney, 2013).
? NRC S140 lexicon (Mohammad et al.,
2013).
? NRC Hashtag lexicon (Mohammad et al.,
2013).
? The three lexicons created from the training
data in (Malakasiotis et al., 2013).
Note that concerning the MPQA Lexicon we
applied preprocessing similar to Malakasiotis et al.
(2013) to obtain the following sub?lexicons:
S
+
: Contains strong subjective expressions with
positive prior polarity.
S
?
: Contains strong subjective expressions with
negative prior polarity.
S
?
: Contains strong subjective expressions with
either positive or negative prior polarity.
S
0
: Contains strong subjective expressions with
neutral prior polarity.
W
+
: Contains weak subjective expressions with
positive prior polarity.
W
?
: Contains weak subjective expressions with
negative prior polarity.
W
?
: Contains weak subjective expressions with
either positive or negative prior polarity.
W
0
: Contains weak subjective expressions with
neutral prior polarity.
3.3 Feature engineering
Our system employs several types of features
based on morphological attributes of the mes-
sages, POS tags, and lexicons of section 3.2.
6
3.3.1 Morphological features
? The existence of elongated tokens (e.g.,
?baaad?).
? The number of elongated tokens.
? The existence of date references.
? The existence of time references.
6
All the features are normalized to [?1, 1]
115
? The number of tokens that contain only upper
case letters.
? The number of tokens that contain both upper
and lower case letters.
? The number of tokens that start with an upper
case letter.
? The number of exclamation marks.
? The number of question marks.
? The sum of exclamation and question marks.
? The number of tokens containing only excla-
mation marks.
? The number of tokens containing only ques-
tion marks.
? The number of tokens containing only excla-
mation or question marks.
? The number of tokens containing only ellip-
sis (...).
? The existence of a subjective (i.e., positive or
negative) emoticon at the message?s end.
? The existence of an ellipsis and a link at the
message?s end.
? The existence of an exclamation mark at the
message?s end.
? The existence of a question mark at the mes-
sage?s end.
? The existence of a question or an exclamation
mark at the message?s end.
? The existence of slang.
3.3.2 POS based features
? The number of adjectives.
? The number of adverbs.
? The number of interjections.
? The number of verbs.
? The number of nouns.
? The number of proper nouns.
? The number of urls.
? The number of subjective emoticons.
7
? The number of positive emoticons.
8
? The number of negative emoticons.
9
? The average, maximum and minimum F
1
scores of the message?s POS bigrams for the
subjective and the neutral classes.
10
? The average, maximum and minimum F
1
scores of the message?s POS bigrams for the
positive and the negative classes.
11
For a bigram b and a class c, F
1
is calculated as:
F
1
(b, c) =
2 ? Pre(b, c) ?Rec(b, c)
Pre(b, c) +Rec(b, c)
(1)
where:
Pre(b, c) =
#messages of c containing b
#messages containing b
(2)
Rec(b, c) =
#messages of c containing b
#messages of c
(3)
3.3.3 Sentiment lexicon based features
For each lexicon we use seven different features
based on the scores provided by the lexicon for
each word present in the message.
12
? Sum of scores.
? Maximum of scores.
? Minimum of scores.
? Average of scores.
? The count of words with scores.
? The score of the last word of the message that
appears in the lexicon.
? The score of the last word of the message.
7
This feature is used only for subjectivity detection.
8
This feature is used only for polarity detection.
9
This feature is used only for polarity detection.
10
This feature is used only for subjectivity detection.
11
This feature is used only for polarity detection.
12
If a word does not appear in the lexicon it is assigned
with a score of 0 and it is not considered in the calculation of
the average, maximum, minimum and count scores. Also, we
have removed from SENTIWORDNET any instances having
positive and negative scores that sum to zero. Moreover, the
MPQA lexicon does not provide scores, so, for each word in
the lexicon we assume a score equal to 1.
116
We also created features based on the Pre and
F
1
scores of MPQA and the train data generated
lexicons in a similar manner to that described in
(Malakasiotis et al., 2013), with the difference that
the features are stage dependent. Thus, for subjec-
tivity detection we use the subjective and neutral
classes and for polarity detection we use the posi-
tive and negative classes to compute the scores.
3.3.4 Miscellaneous features
Negation. Negation not only is a good subjec-
tivity indicator but it also may change the
polarity of a message. We therefore add 7
more features, one indicating the existence
of negation, and the remaining six indicat-
ing the existence of negation that precedes
words from lexicons S
?
, S
+
, S
?
, W
?
, W
+
and W
?
.
13
Each feature is used in the appro-
priate stage.
14
We have not implement this
type of feature for other lexicons but it might
be a good addition to the system.
Carnegie Mellon University?s Twitter clusters.
Owoputi et al. (2013) released a dataset of
938 clusters containing words coming from
tweets. Words of the same clusters share
similar attributes. We try to exploit this
observation by adding 938 features, each of
which indicates if a message?s token appears
or not in the corresponding attributes.
3.4 Feature Selection
To allow our model to better scale on unseen data
we have performed feature selection. More specif-
ically, we first merged training and development
data of SEMEVAL 2013 Task 2. Then, we ranked
the features with respect to their information gain
(Quinlan, 1986) on this dataset. To obtain the best
set of features we started with a set containing the
top 50 features and we kept adding batches of 50
features until we have added all of them. At each
step we evaluated the corresponding feature set on
the TW
13
and SMS
13
datasets and chose the fea-
ture set with the best performance. This resulted in
a system which used the top 900 features for Stage
1 and the top 1150 features for Stage 2.
13
We use a list of words with negation. We assume that a
token precedes a word if it is in a distance of at most 5 tokens.
14
The features concerning S
?
and W
?
are used in subjec-
tivity detection and the remaining four in polarity detection.
Test Set AUEB Median Best
LJ
14
70.75 65.48 74.84
SMS
13
64.32 57.53 70.28
TW
13
63.92 62.88 72.12
TW
14
66.38 63.03 70.96
TWSARC
14
56.16 45.77 58.16
AVG
all
64.31 56.56 68.78
AVG
14
64.43 57.97 67.62
Table 1: F
1
(?) scores per dataset.
Test Set Ranking
LJ
14
9/50
SMS
13
8/50
TW
13
21/50
TW
14
14/50
TWSARC
14
4/50
AVG
all
6/50
AVG
14
5/50
Table 2: Rankings of our system.
4 Experimental Results
The official measure of the Task is the average F
1
score of the positive and negative classes (F
1
(?)).
Table 1 illustrates the F
1
(?) score per evaluation
dataset achieved by our system along with the me-
dian and best F
1
(?). In the same table AVG
all
corresponds to the average F
1
(?) across the five
datasets while AVG
14
corresponds to the average
F
1
(?) across LJ
14
, TW
14
and TWSARC
14
. We
observe that in all cases our results are above the
median. Table 2 illustrates the ranking of our sys-
tem according to F
1
(?). Our system ranked 6th
according to AVG
all
and 5th according to AVG
14
among the 50 participating systems. Note that our
best results were achieved on the new test sets
(LJ
14
, TW
14
, TWSARC
14
) meaning that our sys-
tem has a good generalization ability.
5 Conclusion and future work
In this paper we presented our approach for the
Message Polarity Classification subtask of the
Sentiment Analysis in Twitter Task of SEMEVAL
2014. We proposed a 2?stage pipeline approach,
which first detects sentiment and then decides
about its polarity. The results indicate that our sys-
tem handles well the class imbalance problem and
has a good generalization ability. A possible ex-
planation is that we do not use bag-of-words fea-
117
tures which often suffer from over?fitting. Never-
theless, there is still some room for improvement.
A promising direction would be to improve the
1st stage (subjectivity detection) either by adding
more data or by adding more features, mostly be-
cause the performance of stage 1 greatly affects
that of stage 2. Finally, the addition of more data
for the negative class on stage 2 might be a good
improvement because it would further reduce the
class imbalance of the training data for this stage.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opin-
ion mining. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC?10), Valletta, Malta,
may.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 36?44, Beijing, China.
Rene De La Briandais. 1959. File searching using
variable length keys. In Papers Presented at the the
March 3-5, 1959, Western Joint Computer Confer-
ence, IRE-AIEE-ACM ?59 (Western), pages 295?
298, New York, NY, USA.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Prodromos Malakasiotis, Rafael Michael Karampat-
sis, Konstantina Makrynioti, and John Pavlopoulos.
2013. nlp.cs.aueb.gr: Two stage sentiment analysis.
In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM), Volume 2: Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 562?567,
Atlanta, Georgia, June.
Saif Mohammad and Peter Turney. 2013. Crowdsourc-
ing a word-emotion association lexicon. 29(3):436?
465.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), Atlanta, Georgia, USA,
June.
Finn
?
Arup Nielsen. 2011. A new anew: evaluation of
a word list for sentiment analysis in microblogs. In
Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie,
and Mariann Hardey, editors, Proceedings of the
ESWC2011 Workshop on ?Making Sense of Micro-
posts?: Big things come in small packages, volume
718 of CEUR Workshop Proceedings, pages 93?98,
May.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ?04, Barcelona, Spain.
Ross Quinlan. 1986. Induction of decision trees.
Mach. Learn., 1(1):81?106, March.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Vladimir Vapnik. 1998. Statistical learning theory.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 347?354.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. SemEval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ?13, June.
118
