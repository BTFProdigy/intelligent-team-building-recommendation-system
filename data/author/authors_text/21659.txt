Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 1?9,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Earlier Identification of Epilepsy Surgery Candidates Using Natural
Language Processing
Pawel Matykiewicz1, Kevin Bretonnel Cohen2, Katherine D. Holland1, Tracy A. Glauser1,
Shannon M. Standridge1, Karin M. Verspoor3,4, and John Pestian1?
1 Cincinnati Children?s Hospital Medical Center, Cincinnati OH USA
2 University of Colorado, Denver, CO
3 National ICT Australia and 4The University of Melbourne, Melbourne, Australia
?corresponding author: john.pestian@cchmc.org
Abstract
This research analyzed the clinical notes
of epilepsy patients using techniques from
corpus linguistics and machine learning
and predicted which patients are can-
didates for neurosurgery, i.e. have in-
tractable epilepsy, and which are not.
Information-theoretic and machine learn-
ing techniques are used to determine
whether and how sets of clinic notes
from patients with intractable and non-
intractable epilepsy are different. The re-
sults show that it is possible to predict
from an early stage of treatment which pa-
tients will fall into one of these two cate-
gories based only on text data. These re-
sults have broad implications for develop-
ing clinical decision support systems.
1 Introduction and Significance
Epilepsy is a disease characterized by recurrent
seizures that may cause irreversible brain damage.
While there are no national registries, epidemiolo-
gists have shown that roughly three million Amer-
icans require $17.6 billion USD in care annually
to treat their epilepsy (Epilepsy Foundation, 2012;
Begley et al, 2000). Epilepsy is defined by the
occurrence of two or more unprovoked seizures
in a year. Approximately 30% of those individ-
uals with epilepsy will have seizures that do not
respond to anti-epileptic drugs (Kwan and Brodie,
2000). This population of individuals is said to
have intractable or drug-resistant epilepsy (Kwan
et al, 2010).
Select intractable epilepsy patients are candi-
dates for a variety of neurosurgical procedures that
ablate the portion of the brain known to cause the
seizure. On average, the gap between the ini-
tial clinical visit when the diagnosis of epilepsy
is made and surgery is six years. If it were pos-
sible to predict which patients should be consid-
ered candidates for referral to surgery earlier in the
course of treatment, years of damaging seizures,
under-employment, and psychosocial distress may
be avoided. It is this gap that motivates this re-
search.
In this study, we examine the differences be-
tween the clinical notes of patients early in their
treatment course with the intent of predicting
which patients will eventually be diagnosed as in-
tractable versus which will be amenable to drug-
based treatment. The null hypothesis is that
there will be no detectable differences between
the clinic notes of patients who go on to a di-
agnosis of intractable epilepsy and patients who
do not progress to the diagnosis of intractable
epilepsy (figure 1). To further elucidate the phe-
nomenon, we look at both the patient?s earli-
est clinical notes and notes from a progression
of time points. Here we expect to gain insight
into how the linguistic characteristics (and natu-
ral language processing-based classification per-
formance) evolve over treatment course. We also
study the linguistic features that characterize the
differences between the document sets from the
two groups of patients. We anticipate that this ap-
proach will ultimately be adapted for various clin-
ical decision support systems.
2 Background
2.1 Related work
Although there has been extensive work on build-
ing predictive models of disease progression and
of mortality risk, few models take advantage of
natural language processing in addressing this
task.
(Abhyankar et al, 2012) used univariate anal-
ysis, multivariate logistic regression, sensitivity
analyses, and Cox proportional hazards models to
predict 30-day and 1-year survival of overweight
1
and obese Intensive Care Unit patients. As one of
the features in their system, they used smoking sta-
tus extracted from patient records by natural lan-
guage processing techniques.
(Himes et al, 2009) used a Bayesian network
model to predict which asthma patients would go
on to develop chronic obstructive pulmonary dis-
ease. As one of their features, they also used
smoking status extracted from patient records by
natural language processing techniques.
(Huang et al, under review) is the work most
similar to our own. They evaluated the ability of
a Naive Bayesian classifier to predict future diag-
noses of depression six months prior and twelve
months prior to the actual diagnoses. They used
a number of feature types, including fielded data
such as billing codes, ICD-9 CM diagnoses, and
others, as well as data drawn from natural lan-
guage processing.
In particular, they used an optimized version of
the NCBO Annotator (Jonquet et al, 2009) to rec-
ognize terms from 22 clinically relevant ontolo-
gies and classify them additionally as to whether
they were negated or related to the patient?s fam-
ily history. Their system demonstrated an ability
to predict diagnoses of depression both six months
and one year prior to the actual diagnoses at a rate
that exceeds the success of primary care practi-
tioners in diagnosing active depression.
Considering this body of work overall, natural
language processing techniques have played a mi-
nor role, providing only a fraction of a much larger
set of features?just one feature, in the first two
studies discussed. In contrast, in our work natu-
ral language processing is the central aspect of the
solution.
2.2 Theoretical background to the
approaches used in this work
In comparing the document sets from the two pa-
tient populations, we make use of two lines of in-
quiry. In the first, we use information-theoretic
methods to determine whether or not the contents
of the data sets are different, and if they are dif-
ferent, to characterize the differences. In the sec-
ond, we make use of a practical method from ap-
plied machine learning. In particular, we deter-
mine whether it is possible to train a classifier to
distinguish between documents from the two sets
of patients, given an appropriate classification al-
gorithm and a reasonable set of features.
From information-theoretic methods, we take
Kullback-Leibler divergence as a way to deter-
mine whether the contents of the two sets of docu-
ments are the same or different. Kullback-Leibler
divergence is the relative entropy of two probabil-
ity mass functions??a measure of how different
two probability distributions (over the same event
space) are? (Manning and Schuetze, 1999). This
measure has been previously used to assess the
similarity of corpora (Verspoor et al, 2009). De-
tails of the calculation of Kullback-Leibler diver-
gence are given in the Methods section. Kullback-
Leibler divergence has a lower bound of zero; with
a value of zero, the two document sets would be
identical. A value of 0.005 is assumed to corre-
spond to near-identity.
From practical applications of machine learn-
ing, we test whether or not it is possible to train a
classifier to distinguish between documents from
the two document sets. The line of thought here is
that provided that we have an appropriate classifi-
cation algorithm and a reasonable feature set, then
if clinic notes from the two document sets are in-
deed different, it should be possible to train a clas-
sifier to distinguish between them with reasonable
accuracy.
3 Materials and methods
3.1 Materials
The experimental protocol was approved by our
local Institutional Review Board (#2012-1646).
Neurology clinic notes were extracted from the
electronic medical record system. Records were
sampled from two groups of patients: 1) those
with intractable epilepsy referred for and eventu-
ally undergoing epilepsy surgery and 2) those with
epilepsy who were responsive to medications and
never referred for surgical evaluation. They were
also sampled at three time periods before the ?zero
point?, the date at which patients were either re-
ferred for surgery or the date of last seizure for the
non-intractable group. Table 1 shows the distribu-
tion of patients and clinic notes.
3.2 Methods
As described in the introduction, we applied
information-theoretic and machine learning tech-
niques to determine whether the two document
collections were different (or differentiable).
2
Non-Intractable Intractable
-12 to 0 355 (127) 641 (155)
-6 to +6 453 (128) 898 (155)
0 to +12 months 454 (132) 882 (149)
Table 1: Progress note and patient counts (in
parentheses) for each time period. A minus sign
indicates the period before surgery referral date
for intractable epilepsy patients and before last
seizure for non-intractable patients. A plus sign
indicates the period after surgery referral for in-
tractable epilepsy patients and after last seizure for
non-intractable patients. Zero is the surgery refer-
ral date or date of last seizure for the two popula-
tions, respectively.
3.2.1 Feature extraction
Features for both the calculation of Kullback-
Leibler divergence and the machine learning
experiment were unigrams, bigrams, tri-
grams, and quadrigrams. We applied the
National Library of Medicine stopword list
http://mbr.nlm.nih.gov/Download/
2009/WordCounts/wrd_stop. All words
were lower-cased, all numerals were substituted
with the string NUMB for abstraction, and all
non-ASCII characters were removed.
3.3 Information-theoretic approach
Kullback-Leibler divergence compares probability
distribution of words or n-grams between different
datasets DKL(P ||Q). In particular, it measures
how much information is lost if distribution Q is
used to approximate distribution P . This method,
however, gives an asymmetric dissimilarity mea-
sure. Jensen-Shannon divergence is probably the
most popular symmetrization of DKL and is de-
fined as follows:
DJS =
1
2
DKL(P ||Q) +
1
2
DKL(Q||P ) (1)
where
DKL(P ||Q) =
?
w?P?Q
(
p(w|cP ) log
p(w|cP )
p(w|cQ)
)
(2)
By Zipf?s law any corpus of natural language will
have a very long tail of infrequent words. To ac-
count for this effect we use DJS for the top N
most frequent words/n-grams. We use Laplace
smoothing to account for words or n-grams that
did not appear in one of the corpora.
We also aim to uncover terms that distinguish
one corpus from another. We use a metamor-
phic DJS test, log-likelihood ratios, and weighted
SVM features. Log-likelihood score will help us
understand where precisely the two corpora differ.
nij =
kij
kiP + kiA
(3)
mij =
kPj + kQj
kQP + kPP + kQA + kPA
(4)
LL(w) = 2
?
i,j
kij log
nij
mij
(5)
3.4 Machine learning
For the classification experiment, we used an im-
plementation of the libsvm support vector ma-
chine package that was ported to R (Dimitriadou
et al, 2011). Features were extracted as described
above in Section 3.2.1. We used a cosine kernel.
The optimal C regularization parameter was esti-
mated on a scale from 2?1 to 215.
3.5 Characterizing differences between the
document sets
We used a variety of methods to characterize
differences between the document sets: log-
likelihood ratio, SVM normal vector components,
and a technique adapted from metamorphic test-
ing.
3.5.1 Applying metamorphic testing to
Kullback-Leibler divergence
As one of our methods for characterizing differ-
ences between the two document sets, we used an
adaptation of metamorphic testing, inspired by the
work of (Murphy and Kaiser, 2008) on applying
metamorphic testing to machine learning applica-
tions. The intuition behind metamorphic testing is
that given some output for a given input, it should
be possible to predict in general terms what the
effect of some alternation in the input should be
on the output. For example, given some Kullback-
Leibler divergence for some set of features, it is
possible to predict how Kullback-Leibler diver-
gence will change if a feature is added to or sub-
tracted from the feature vector. We adapted this
observation by iteratively subtracting all features
one by one and ranking them according to how
much of an effect on the Kullback-Leibler diver-
gence its removal had.
3
Figure 1: Two major paths in epilepsy care. At
the begining of epilepsy care two groups of pa-
tients are indistinguishable. Subsequently, the two
groups diverge.
4 Results
4.1 Kullback-Leibler (Jensen-Shannon)
divergence
Table 2 shows the Kullback-Leibler divergence,
calculated as Jensen-Shannon divergence, for
three overlapping time periods?the year preced-
ing surgery referral, the period from 6 months be-
fore surgery referral to six months after surgery re-
ferral, and the year following surgery referral, for
the intractable epilepsy patients; and, for the non-
intractable epilepsy patients, the same time peri-
ods with reference to the last seizure date.
As can be seen in the left-most column (-12 to
0), at one year prior, the clinic notes of patients
who will require surgery and patients who will
not require surgery cannot easily be discriminated
by Kullback-Leibler divergence?the divergence
is only just above the .005 near-identity threshold
even when 8000 unique n-grams are considered. If
the -6 to +6 and 0 to +12 time periods are exam-
ined, we see that the divergence increases as we
reach and then pass the period of surgery (or move
into the year following the last seizure, for the non-
intractable patients), indicating that the difference
between the two collections becomes more pro-
nounced as treatment progresses. The divergence
for these time periods does pass the assumed near-
identity threshold for larger numbers of n-grams,
n-grams -12 to 0
months
-6 to +6
months
0 to +12
months
125 0.00125 0.00193 0.00244
250 0.00167 0.00229 0.00286
500 0.00266 0.00326 0.00389
1000 0.00404 0.00494 0.00585
2000 0.00504 0.00618 0.00718
4000 0.00535 0.00657 0.00770
8000 0.00555 0.00681 0.00796
Table 2: Kullback-Leibler divergence (calculated
as Jensen-Shannon divergence) for difference be-
tween progress notes of the two groups of patients.
Results are shown for the period 1 year before, 6
months before and 6 months after, and one year
after surgery referral for the intractable epilepsy
patients and the last seizure for non-intractable pa-
tients. 0 represents the date of surgery referral for
the intractable epilepsy patients and date of last
seizure for the non-intractable patients.
largely accounted for by terms that are unique to
one notes set or the other.
4.2 Classification with support vector
machines
Table 3 shows the results of building support vec-
tor machines to classify individual notes as be-
longing to the intractable epilepsy or the non-
intractable epilepsy patient population. Three time
periods are evaluated, as described above. The
number of features is varied by row. For each
cell, the average F-measure from 20-fold cross-
validation is shown.
As can be seen in the left-most column (-12 to
0), at one year prior to referral to surgery refer-
ral date or last seizure, the patients who will be-
come intractable epilepsy patients can be distin-
guished from the patients who will become non-
intractable epilepsy patients purely on the basis of
natural language processing-based classification
with an F-measure as high as 0.95. This supports
the conclusion that the two document sets are in-
deed different, and furthermore illustrates that this
difference can be used to predict which patients
will require surgical intervention.
4.3 Characterizing the differences between
clinic notes from the two patient
populations
Tables 4 and 5 show the results of three meth-
ods for differentiating between the document col-
4
n-grams -12 to 0
months
-6 to +6
months
0 to +12
months
125 0.8885 0.9217 0.9476
250 0.8928 0.9297 0.9572
500 0.9107 0.9367 0.9667
1000 0.9245 0.9496 0.9692
2000 0.9417 0.9595 0.9789
4000 0.9469 0.9661 0.9800
8000 0.9510 0.9681 0.9810
Table 3: Average F1 for the three time periods
described above, with increasing numbers of fea-
tures. Values are the average of 20-fold cross-
validation. See Figure 2 for an explanation of the
time periods.
lections representing the two patient populations.
The methodology for each is described above. The
most strongly distinguishing features when just
the 125 most frequent features are used are shown
in Table 4, and the most strongly distinguishing
features when the 8,000 most frequent features are
used are shown in Table 5. Impressionistically,
two trends emerge. One is that more clearly clini-
cally significant features are shown to have strong
discriminatory power when the 8,000 most fre-
quent features are used than when the 125 most
frequent features are used. This result is sup-
ported by the Kullback-Leibler divergence results,
which demonstrated the most divergent vocabular-
ies with larger numbers of n-grams. The other
trend is that the SVM classifier does a better job
of picking out clinically relevant features. This
has implications for the design of clinical decision
support systems that utilize our approach.
5 Discussion
5.1 Behavior of Kullback-Leibler divergence
Kullback-Leibler divergence varies with the num-
ber of words considered. When the vocabularies
of two document sets are merged and the words
are ordered by overall frequency, the further down
the list we go, the higher the Kullback-Leibler
divergence can be expected to be. This is be-
cause the highest-frequency words in the com-
bined set will generally be frequent in both source
corpora, and therefore carry similar probability
mass. As we progress further down the list of
frequency-ranked words, we include progressively
less-common words, with diverse usage patterns,
which are likely to reflect the differences between
the two document sets, if there are any. Thus, the
Kullback-Leibler divergence will rise.
To understand the intuition here, imagine look-
ing at the Kullback-Leibler divergence when just
the 50 most-common words are considered. These
will be primarily function words, and their distri-
butions are unlikely to differ much between the
two document sets unless the syntax of the two
corpora is radically different. Beyond this set of
very frequent common words will be words that
may be relatively frequent in one set as compared
to the other, contributing to divergence between
the sets.
In Table 2, the observed behavior for our two
document collections follows this expected pat-
tern. However, the divergence between the vocab-
ularies remains close to the assumed near-identity
threshold of 0.005, even when larger numbers of
n-grams are considered. The divergence never ex-
ceeds 0.01; this level of divergence for larger num-
bers of n-grams is consistent with prior analyses of
highly similar corpora (Verspoor et al, 2009).
We attribute this similarity to two factors. The
first is that both document sets derive from a single
department within a single hospital; a relatively
small number of doctors are responsible for au-
thoring the notes and there may exist specific hos-
pital protocols related to their content. The second
is that the clinical contexts from which our two
document sets are derived are highly related, in
that all the patients are epilepsy patients. While we
have demonstrated that there are clear differences
between the two sets, it is also to be expected that
they would have many words in common. The
nature of clinical notes combined with the shared
disease context results in generally consistent vo-
cabulary and hence low overall divergence.
5.2 Behavior of classifier
Table 3 demonstrates that classifier performance
increases as the number of features increases. This
indicates that as more terms are considered, the
basis for differentiating between the two different
document collections is stronger.
Examining the SVM normal vector components
(SVMW) in Tables 4 and 5, we find that unigrams,
bigrams and trigrams are useful in differentiation
between the two patient populations. While no
quadrigrams appear in this table, they may in fact
contribute to classifier performance. We will per-
form an ablation study in future work to quantify
5
JS metamorphic test (JSMT) Log-likelihood ratio (LLR) SVM normal vector compo-
nents (SVMW)
family = -0.000114 none = 623.702323 bilaterally = -19.009380
normal = -0.000106 family = -445.117177 age.NUMB = 17.981459
seizure = -0.000053 NUMB.NUMB.NUMB.NUMB
= 422.953816
review = 17.250652
problems = -0.000053 normal = -244.603033 based = -14.846495
none = 0.000043 problems = -207.021130 family.history = -14.659653
detailed = -0.000037 left = 176.434519 NUMB = -14.422525
including = -0.000036 bid = 142.105691 lower = -13.553434
risks = -0.000033 NUMB = 136.255678 mother = -13.436694
NUMB = 0.000032 detailed = -133.012908 first = -13.001744
concerns = -0.000032 right = 120.453596 including = -12.800433
NUMB.NUMB.NUMB.NUMB
= 0.000031
seizure = -120.047686 extremities = 11.709199
additional = -0.000029 including = -119.061518 documented = -11.441394
brain = -0.000026 risks = -116.543250 awake = -11.418535
NUMB.NUMB = 0.000022 concerns = -101.366110 hpi = 11.121019
minutes = -0.000021 additional = -95.880792 follow = -10.550802
NUMB.minutes = -0.000020 clear = 83.848170 neurology = -10.533895
reviewed = -0.000018 brain = -74.267220 call = -10.422606
history = -0.000017 seizures = 71.937757 effects = 10.298221
noted = -0.000017 one = 65.203819 brain = -9.900864
upper = -0.000017 epilepsy = 46.383564 weight = 9.819712
well = -0.000015 hpi = 45.932630 patient.s = -9.603531
side = -0.000015 minutes = -45.278770 discussed = -9.473544
bilaterally = -0.000014 NUMB.NUMB.NUMB =
43.320354
today = 9.390896
motor.normal = -0.000014 negative = 42.914770 allergies = -9.346146
notes = -0.000014 NUMB.minutes = -42.909968 NUMB.NUMB.NUMB.NUMB
= 9.342800
Spearman correlation between
JSMT and LLR = 0.912454
Spearman correlation between
LLR and SVMW = 0.086784
Spearman correlation between
SVMW and JSMT = 0.101965
Table 4: Comparison of three different methods for finding the strongest differentiating features. This
table shows features for the -12 to 0 periods with the 125 most frequent features. The JSMT and LLR
statistics give values greater than zero. We add sign to indicate which corpus has higher relative fre-
quency of the feature: a positive value indicates that the relative frequency of the feature is greater in the
intractable group, while a negative value indicates that the relative frequency of the feature is greater in
the non-intractable group. The last row shows the correlation between two different ranking statistics.
6
JS metamorphic test (JSMT) Log-likelihood ratio (LLR) SVM normal vector compo-
nents (SVMW)
family = -0.000118 family = -830.329965 john = -4.645071
normal = -0.000109 normal = -745.882086 lamotrigine = 4.320412
seizure = -0.000057 problems = -386.238711 surgery = 4.299546
problems = -0.000057 seizure = -369.342334 jane = 4.091609
none = 0.000047 none = 337.461504 epilepsy.surgery = 4.035633
including = -0.000040 detailed = -262.240496 janet = -3.970101
detailed = -0.000040 including = -255.076808 excellent.control = -3.946283
additional.concerns = -0.000038 additional.concerns.noted =
-246.603655
excellent = -3.920620
additional.concerns.noted =
-0.000038
concerns.noted = -246.603655 NUMB.seizure = -3.886997
concerns.noted = -0.000038 additional.concerns = -
243.353912
mother = -3.801364
NUMB = -0.000036 NUMB.NUMB.NUMB.NUMB
= 238.065700
jen = 3.568809
concerns = -0.000036 risks = -232.741511 back = -3.319477
risks = -0.000036 concerns = -228.805299 visit = -3.264600
NUMB.NUMB.NUMB.NUMB
= 0.000035
additional = -204.462411 james = 3.174763
additional = -0.000033 brain = -182.413340 NUMB.NUMB.NUMB.normal
= -3.024471
brain = -0.000030 NUMB = -162.992065 continue = -3.011293
NUMB.NUMB = -0.000026 surgery = 153.646067 idiopathic.localization = -
2.998177
minutes = -0.000025 minutes = -142.761961 idiopathic.localization.related =
-2.998177
surgery = 0.000024 NUMB.minutes = -134.048116 increase = 2.948187
NUMB.minutes = -0.000023 diff = -131.388230 diastat = -2.937431
diff = -0.000023 NUMB.NUMB = -125.067347 taking = -2.902673
history = -0.000021 reviewed = -116.013417 lamictal = 2.898987
reviewed = -0.000021 noted = -114.241532 going = 2.862764
noted = -0.000021 idiopathic = -112.331060 described = 2.844830
upper = -0.000020 shaking = -112.186858 epilepsy = 2.745872
Spearman correlation between
JSMT and LLR = 0.782918
Spearman correlation between
LLR and SVMW = 0.039860
Spearman correlation between
SVMW and JSMT = 0.165159
Table 5: Comparison of three different methods for finding the strongest differentiating features. This
table shows features for the -12 to 0 periods with the 8,000 most frequent features. The JSMT and
LLR statistics give values greater than zero. We add sign to indicate which corpus has higher relative
frequency of the feature: a positive value indicates that the relative frequency of the feature is greater in
the intractable group, while a negative value indicates that the relative frequency of the feature is greater
in the non-intractable group. The last row shows the correlation between two different ranking statistics.
7
the contribution of the different feature sets. In ad-
dition, we find that table 5 shows many clinically
relevant terms, such as seizure frequency (?ex-
cellent [seizure] control?), epilepsy type (?local-
ization related [epilepsy]?), etiology classification
(?idiopathic [epilepsy]?), and drug names (?lamot-
rigine?, ?diastat?, ?lamictal?), giving nearly com-
plete history of the present illness.
6 Conclusion
The classification results from our machine learn-
ing experiments support rejection of the null hy-
pothesis of no detectable differences between the
clinic notes of patients who will progress to the
diagnosis of intractable epilepsy and patients who
do not progress to the diagnosis of intractable
epilepsy. The results show that we can predict
from an early stage of treatment which patients
will fall into these two classes based only on tex-
tual data from the neurology clinic notes. As intu-
ition would suggest, we find that the notes become
more divergent and the ability to predict outcome
improves as time progresses, but the most impor-
tant point is that the outcome can be predicted
from the earliest time period.
SVM classification demonstrates a stronger re-
sult than the information-theoretic measures, uses
less data, and needs just a single run. However, it
is important to note that we cannot entirely rely
on the argument from classification as the sole
methodology in testing whether or not two doc-
ument sets are similar or different. If the find-
ing is positive, i.e., it is possible to train a classi-
fier to distinguish between documents drawn from
the two document sets, then interpreting the re-
sults is straightforward. However, if documents
drawn from the two document sets are not found
to be distinguishable by a classifier, one must
consider the possibility of multiple possible con-
founds, such as selection of an inappropriate clas-
sification algorithm, extraction of the wrong fea-
tures, bugs in the feature extraction software, etc.
Having established that the two sets of clinical
notes differ, we noted some identifying features of
clinic notes from the two populations, particularly
when more terms were considered.
The Institute of Medicine explains that ?. . . to
accommodate the reality that although profes-
sional judgment will always be vital to shaping
care, the amount of information required for any
given decision is moving beyond unassisted hu-
man capacity (Olsen et al, 2007).? This is surely
the case for those who care for the epileptic pa-
tient. Technology like natural language processing
will ultimately serve as a basis for stable clinical
decision support tools. It, however, is not a deci-
sion making tool. Decision making is the respon-
sibility of professional judgement. That judge-
ment will labor over such questions as: what is
the efficacy of neurosurgery, what will be the long
term outcome, will there be any lasting damage,
are we sure that all the medications have been
tested, and how the family will adjust to a poor
outcome. In the end, it is that judgement that will
decide what is best; that decision will be supported
by research like what is presented here.
7 Acknowledgements
This work was supported in part by the National
Institutes of Health, Grants #1R01LM011124-
01,and 1R01NS045911-01; the Cincinnati Chil-
dren?s Hospital Medical Center?s: Research Foun-
dation, Department of Pediatric Surgery and the
Department of Paediatrics?s divisions of Neurol-
ogy and Biomedical Informatics. We also wish
to acknowledge the clinical and surgical wisdom
provided by Drs. John J. Hutton & Hansel M.
Greiner, MD. K. Bretonnel Cohen was supported
by grants XXX YYY ZZZ. Karin Verspoor was
supported by NICTA, which is funded by the Aus-
tralian Government as represented by the Depart-
ment of Broadband, Communications and the Dig-
ital Economy and the Australian Research Coun-
cil.
References
[Abhyankar et al2012] Swapna Abhyankar, Kira Leis-
hear, Fiona M. Callaghan, Dina Demner-Fushman,
and Clement J. McDonald. 2012. Lower short- and
long-term mortality associated with overweight and
obesity in a large cohort study of adult intensive care
unit patients. Critical Care, 16.
[Begley et al2000] Charles E Begley, Melissa Famu-
lari, John F Annegers, David R Lairson, Thomas F
Reynolds, Sharon Coan, Stephanie Dubinsky,
Michael E Newmark, Cynthia Leibson, EL So, et al
2000. The cost of epilepsy in the united states: An
estimate from population-based clinical and survey
data. Epilepsia, 41(3):342?351.
[Dimitriadou et al2011] Evgenia Dimitriadou, Kurt
Hornik, Friedrich Leisch, David Meyer, and An-
dreas Weingessel, 2011. e1071: Misc Func-
tions of the Department of Statistics (e1071), TU
8
Wien. http://CRAN.R-project.org/package=e1071.
R package version 1.5.
[Epilepsy Foundation2012] Epilepsy Foundation,
2012. What is Epilepsy: Incidence and Prevalence.
http://www.epilepsyfoundation.org/ aboutepilepsy
/whatisepilepsy/ statistics.cfm.
[Himes et al2009] Blanca E. Himes, Yi Dai, Isaac S.
Kohane, Scott T. Weiss, and Marco F. Ramoni.
2009. Prediction of chronic obstructive pulmonary
disease (copd) in asthma patients using electronic
medical records. Journal of the American Medical
Informatics Association, 16(3):371?379.
[Huang et alunder review] Sandy H. Huang, Paea LeP-
endu, Srinivasan V Iyer, Anna Bauer-Mehren, Cliff
Olson, and Nigam H. Shah. under review. Develop-
ing computational models for predicting diagnoses
of depression. In American Medical Informatics As-
sociation.
[Jonquet et al2009] Clement Jonquet, Nigam H. Shah,
Cherie H. Youn, Mark A. Musen, Chris Callendar,
and Margaret-Anne Storey. 2009. NCBO Annota-
tor: Semantic annotation of biomedical data. In 8th
International Semantic Web Conference.
[Kwan and Brodie2000] Patrick Kwan and Martin J
Brodie. 2000. Early identification of refrac-
tory epilepsy. New England Journal of Medicine,
342(5):314?319.
[Kwan et al2010] Patrick Kwan, Alexis Arzimanoglou,
Anne T Berg, Martin J Brodie, W Allen Hauser,
Gary Mathern, Solomon L Moshe?, Emilio Perucca,
Samuel Wiebe, and Jacqueline French. 2010. Defi-
nition of drug resistant epilepsy: consensus proposal
by the ad hoc task force of the ilae commission on
therapeutic strategies. Epilepsia, 51(6):1069?1077.
[Manning and Schuetze1999] Christopher Manning
and Hinrich Schuetze. 1999. Foundations of
statistical natural language processing. MIT Press.
[Murphy and Kaiser2008] Christian Murphy and Gail
Kaiser. 2008. Improving the dependability of ma-
chine learning applications.
[Olsen et al2007] LeighAnne Olsen, Dara Aisner, and
J Michael McGinnis. 2007. The learning healthcare
system.
[Verspoor et al2009] K. Verspoor, K.B. Cohen, and
L. Hunter. 2009. The textual characteristics of tradi-
tional and open access scientific journals are similar.
BMC Bioinformatics, 10(1):183.
9
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 72?79,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Recognizing sublanguages in scientific journal articles
through closure properties
Irina P. Temnikova
Linguistic Modelling Laboratory
Bulgarian Academy of Sciences
irina.temnikova@gmail.com
K. Bretonnel Cohen
Computational Bioscience Program
University of Colorado School of Medicine
Department of Linguistics
University of Colorado at Boulder
kevin.cohen@gmail.com
Abstract
It has long been realized that sublanguages
are relevant to natural language process-
ing and text mining. However, practical
methods for recognizing or characterizing
them have been lacking. This paper de-
scribes a publicly available set of tools for
sublanguage recognition. Closure proper-
ties are used to assess the goodness of fit
of two biomedical corpora to the sublan-
guage model. Scientific journal articles
are compared to general English text, and
it is shown that the journal articles fit the
sublanguage model, while the general En-
glish text does not. A number of examples
of implications of the sublanguage char-
acteristics for natural language processing
are pointed out. The software is made pub-
licly available at [edited for anonymiza-
tion].
1 Introduction
1.1 Definitions of ?sublanguage?
The notion of sublanguage has had varied defini-
tions, depending on the aspects of sublanguages
on which the authors focused. (Grishman and Kit-
tredge, 1986) focus on syntactic aspects of sub-
languages: ?. . . the term suggests a subsystem of
language. . . limited in reference to a specific sub-
ject domain. In particular, each sublanguage has
a distinctive grammar, which can profitably be
described and used to solve specific language-
processing problems? (Grishman and Kittredge,
1986).
(Kittredge, 2003) focuses on the spontaneous
appearance of sublanguages in restricted domains,
where the preconditions for a sublanguage to ap-
pear are the sharing of specialized knowledge
about a restricted semantic domain and recurrent
?situations? (e.g. scientific journal articles, or dis-
charge summaries) in which domain experts com-
municate. According to (Kittredge, 2003), charac-
teristics of a sublanguage include a restricted lexi-
con, relatively small number of lexical classes, re-
stricted sentence syntax, deviant sentence syntax,
restricted word co-occurrence patterns, and differ-
ent frequencies of occurrence of words and syn-
tactic patterns from the normal language.
(McDonald, 2000) focuses on the element of re-
striction in sublanguages?the notion that they are
restricted to a specialized semantic domain, a very
?focused? audience, and ?stipulated content,? with
the effect that both word choice and syntactic style
have reduced options as compared to the normal
language.
The notions of restriction that recur in these
definitions of ?sublanguage? lead directly to
(McEnery and Wilson, 2001)?s notion of using
the quantification of closure properties to assess
whether or not a given sample of a genre of lan-
guage use fits the sublanguage model. Closure
refers to the tendency of a genre of language to-
wards finiteness at one or more linguistic levels.
For example, a genre of language might or might
not use a finite set of lexical items, or have a fi-
nite set of sentence structures. Notions of restric-
tion suggest that a sublanguage should tend to-
wards closure on at least some linguistic levels.
To quantify closure, we can examine relationships
between types and tokens in a corpus of the genre.
In particular, we count the number of types that
are observed as an increasing number of tokens
is examined. If a genre does not exhibit closure,
then the number of types will continue to rise con-
tinually as the number of tokens increases. On
the other hand, closure is demonstrated when the
number of types stops growing after some number
of tokens has been examined.
72
1.2 Relevance of sublanguages to natural
language processing
The relevance of sublanguages to natural language
processing has long been recognized in a vari-
ety of fields. (Hirschman and Sager, 1982) and
(Friedman, 1986) show how a sublanguage?based
approach can be used for information extraction
from clinical documents. (Finin, 1986) shows that
sublanguage characterization can be used for the
notoriously difficult problem of interpretation of
nominal compounds. (Sager, 1986) asserts a num-
ber of uses for sublanguage?oriented natural lan-
guage processing, including resolution of syntac-
tic ambiguity, definition of frames for informa-
tion extraction, and discourse analysis. (Sekine,
1994) describes a prototype application of sublan-
guages to speech recognition. (Friedman et al,
1994) uses a sublanguage grammar to extract a va-
riety of types of structured data from clinical re-
ports. (McDonald, 2000) points out that modern
language generation systems are made effective in
large part due to the fact that they are applied to
specific sublanguages. (Somers, 2000) discusses
the relevance of sublanguages to machine trans-
lation, pointing out that many sublanguages can
make machine translation easier and some of them
can make machine translation harder. (Friedman
et al, 2001) uses a sublanguage grammar to ex-
tract structured data from scientific journal arti-
cles.
1.3 Previous work on sublanguage
recognition
Various approaches have been taken to recog-
nizing sublanguages. We posit here two sepa-
rate tasks?recognizing a sublanguage when one
is present, and determining the characteristics of
a sublanguage. Information-theoretic approaches
have a long history. (Sekine, 1994) clustered docu-
ments and then calculated the ratio of the perplex-
ity of the clustered documents to the perplexity
of a random collection of words. (Somers, 1998)
showed that texts drawn from a sublanguage cor-
pus have low weighted cumulative sums. (Stetson
et al, 2002) used relative entropy and squared chi-
square distance to identify a sublanguage of cross-
coverage notes. (Mihaila et al, 2012) looked at
distributions of named entities to identify and dif-
ferentiate between a wide variety of scientific sub-
languages.
Non-information-theoretic, more heuristic
methods have been used to identify sublanguages,
as well. In addition to the information-theoretic
measures described above, (Stetson et al, 2002)
also looked at such measures as length, incidence
of abbreviations, and ambiguity of abbreviations.
(Friedman et al, 2002) use manual analysis to
detect and characterize two biomedical sublan-
guages. (McEnery and Wilson, 2001) examine
closure properties; their approach is so central to
the topic of this paper that we will describe it in
some length separately.
(McEnery and Wilson, 2001) examined the clo-
sure properties of three linguistic aspects of their
material under study. As materials they used two
corpora that were assumed not to meet the sub-
language model?the Canadian Hansard corpus,
containing proceedings from the Canadian Parlia-
ment, and the American Printing House for the
Blind corpus, made up of works of fiction. As
a corpus that was suspected to meet the sublan-
guage model, they used a set of manuals from
IBM. All three corpora differed in size, so they
were sampled to match the size of the smallest
corpus, meaning that all experiments were done
on collections 200,000 words in size. The mate-
rials under study were evaluated for their closure
properties at three linguistic levels. At the most
basic level, they looked at lexical items?simple
word forms. The hypothesis here was that the non-
sublanguage corpora would not tend toward finite-
ness, i.e. would not reach closure. That is, if the
number of word types found was graphed as an
increasing number of tokens was examined, the
resulting line would grow continually and would
show no signs of asymptoting. In contrast, the
sublanguage corpus would eventually reach clo-
sure, i.e. would stop growing appreciably in size
as more tokens were examined.
The next level that they examined was the mor-
phosyntactic level. In particular, they looked at
the number of part-of-speech tags per lexical type.
Here the intuition was that if the lexicon of the
sublanguage is limited, then words might be co-
erced into a greater number of parts of speech.
This would be manifested by a smaller overall
number of unique word/part-of-speech tag combi-
nations. Again, we would expect to see that the
sublanguage corpus would have a smaller number
of word/part-of-speech tag combinations, as com-
pared to the non-sublanguage corpus. Graphing
the count of word type/POS tag sets on the y axis
73
and the cumulative number of tokens examined on
the x axis, we would see slower growth and lower
numbers overall.
The final level that they examined was the syn-
tactic level. In this case, parse tree types were
graphed against the number of sentences exam-
ined. The intuition here is that if the sublanguage
exhibits closure properties on the syntactic level,
then the growth of the line will slow and we will
see lower numbers overall.
(McEnery and Wilson, 2001) found the hy-
potheses regarding closure to be substantiated at
all levels. We will not reproduce their graphs,
but will summarize their findings in terms of ra-
tios. On the lexical level, they found type/token
ratios of 1:140 for the IBM manuals (the assumed
sublanguage), 1:53 for the Hansard corpus (as-
sumed not to represent a sublanguage), and 1:17
for the American Printing House for the Blind cor-
pus (also assumed not to represent a sublanguage).
The IBM manuals consist of a much smaller num-
ber of words which are frequently repeated.
At the morphosyntactic level, they found 7,594
type/POS sets in the IBM manuals, 18,817 in
the Hansard corpus, and 11,638 in the Ameri-
can Printing House for the Blind corpus?a much
smaller number in the apparent sublanguage than
in the non-sublanguage corpora. The word/part-
of-speech tag averages coincided with the ex-
pected findings given these number of types. The
averages were 3.19 for the IBM manuals, 2.45 for
the Hansard corpus, and 2.34 for the American
Printing House for the Blind corpus.
At the syntactic level, they found essentially lin-
ear growth in the number of sentence types as the
number of sentence tokens increased in the two
non-sublanguage corpora?the ratio of sentence
types to sentences in these corpora were 1:1.07 for
the Hansard corpus and 1:1.02 for the American
Printing House for the Blind corpus. In contrast,
the growth of sentence types in the IBM manu-
als was not quite linear. It grew linearly to about
12,000 sentences, asymptoted between 12,000 and
16,000, and then grew essentially linearly but at a
somewhat slower rate from 16,000 to 30,000 sen-
tences. The ratio of sentence types to sentence to-
kens in the IBM manuals was 1:1.66?markedly
higher than in the other two corpora.
1.4 Hypotheses tested in the paper
The null hypothesis is that there will be no differ-
ence in closure properties between the general En-
glish corpus and the two corpora of scientific jour-
nal articles that we examine. If the null hypothesis
is not supported, then it might be deviated from in
three ways. One is that the scientific corpora might
show a greater tendency towards closure than the
general English corpus. A second is that the gen-
eral English corpus might show a greater tendency
towards closure than the scientific corpora. A third
is that there may be no relationship between the
closure properties of the two scientific corpora, re-
gardless of the closure properties of the general
English corpus?one might show a tendency to-
wards closure, and the other not.
2 Materials and Methods
2.1 Materials
The data under examination was drawn from three
sources: the CRAFT corpus (Bada et al, 2012;
Verspoor et al, 2012), the GENIA corpus (Kim
et al, 2003), and a version of the British National
Corpus (Leech et al, 1994) re-tagged with Con-
nexor?s Machinese parser (Ja?rvinen et al, 2004).
The CRAFT and GENIA corpora are composed
of scientific journal articles, while the British Na-
tional Corpus is a representative corpus compris-
ing many different varieties of spoken and written
English.
The CRAFT corpus is a collection of 97 full-
text journal articles from the mouse genomics do-
main. It has been annotated for a variety of lin-
guistic and semantic features; for the purposes of
this study, the relevant ones were sentence bound-
aries, tokenization, and part of speech. We used
the 70-document public release subset of the cor-
pus, which comprises about 453,377 words.
The GENIA corpus is a collection of 1,999 ab-
stracts of journal articles about human blood cell
transcription factors. Like the CRAFT corpus,
it has been annotated for a variety of linguistic
and semantic features, again including sentence
boundaries, tokenization, and part of speech. In
the mid-2000?s, the GENIA corpus was shown to
be the most popular corpus for research in biomed-
ical natural language processing (Cohen et al,
2005). We used version 3.02 of the corpus, con-
taining about 448,843 words.
The experiment requires a corpus of general
English for comparison. For this purpose, we
74
used a subset of the British National Corpus. For
purposes of representativeness, we followed the
Brown corpus strategy of extracting the first 2,000
words from each article until a total of 453,377
words were reached (to match the size of the
CRAFT corpus).
The size of the two data sets is far more than ad-
equate for an experiment of this type?McEnery
and Wilson were able to detect closure properties
using corpora of only 200,000 words in their ex-
periments.
2.2 Methods
2.2.1 Implementation details
To determine the closure properties of arbitrary
corpora, we developed scripts that take a simple
input format into which it should be possible to
convert any annotated corpus. There are two input
file types:
? A file containing one word and its corre-
sponding part-of-speech tag per line. Part of
speech tags can consist of multiple tokens, as
they do in the BNC tag set, or of single to-
kens, as they do in most corpora. This file
format is used as the input for the lexical clo-
sure script and the word type/POS tag script.
? A file containing a sequence of part of speech
tags per line, one line per sentence. This
file format is used as input for the sentence
type closure script. We note that this is an
extremely rough representation of ?syntax,?
and arguably is actually asyntactic in that it
does not represent constituent or dependency
structure at all, but also point out that it has
the advantage of being widely applicable and
agnostic as to any particular theory of syntac-
tic structure. It also increases the sensitivity
of the method to sentence type differences,
providing a stronger test of fit to the sublan-
guage model.
Two separate scripts then process one of these
input files to determine lexical, type/POS, and sen-
tence type closure properties. The output of ev-
ery script is a comma-separated-value file suitable
for importing into Excel or other applications for
producing plots. The two scripts and our scripts
for converting the BNC, CRAFT, and GENIA cor-
pora into the input file formats will be made pub-
licly available at [redacted for anonymization pur-
poses]. To apply the scripts to a new corpus, the
Figure 1: Lexical closure properties. Tick-marks
on x axis indicate increments of 50,000 tokens.
only necessary step is to write a script to convert
from the corpus?s original format to the simple for-
mat of the two input file types described above.
2.2.2 Investigating closure properties
In all three cases, the number of types, whether of
lexical items, lexical type/part-of-speech pair, or
sentence type was counted and graphed on the y
axis, versus the number of tokens that had been
observed up to that point, which was graphed on
the x axis. In the case of the lexical and type/POS
graphs, tokens were words, and in the case of the
sentence graph, ?tokens? were sentences.
We then combined the lines for all three cor-
pora and observed the total size of types, the rate
of growth of the line, and whether or not there was
a tendency towards asymptoting of the growth of
the line, i.e. closure.
Our major deviation from the approach of
(McEnery and Wilson, 2001) was that rather than
parse trees, we used part-of-speech tag sequences
to represent sentence types. This is suboptimal in
that it is essentially asyntactic, and in that it ob-
scures the smoothing factor of abstracting away
from per-token parts of speech to larger syntactic
units. However, as we point out above, it has the
advantages of being widely applicable and agnos-
tic as to any particular theory of syntactic struc-
ture, as well as more sensitive to sentence type dif-
ferences.
3 Results
3.1 Lexical closure properties
Figure 1 shows the growth in number of types of
lexical items as the number of tokens of lexical
items increases. The British National Corpus data
is in blue, the CRAFT data is in red, and the GE-
NIA data is in green.
75
Figure 2: Type-part-of-speech tag closure proper-
ties. Tick-marks on x axis indicate increments of
50,000 tokens.
We note a drastic difference between the curve
for the BNC and the curves for CRAFT and GE-
NIA. The curves for CRAFT and GENIA are quite
similar to each other. Overall, the curve for the
BNC climbs faster and much farther, and is still
climbing at a fast rate after 453,377 tokens have
been examined. In contrast, the curves for CRAFT
and GENIA climb more slowly, climb much less,
and by the time about 50,000 tokens have been ex-
amined the rate of increase is much smaller. The
increase in CRAFT and GENIA does not asymp-
tote, as McEnery and Wilson observed for the IBM
corpus. However, contrasted with the results for
the BNC, there is a clear difference.
The type to token ratios for lexical items for the
corpora as a whole are shown in Table 1. As the
sublanguage model would predict, CRAFT and
GENIA have much higher ratios than BNC.
Corpus name Ratio
BNC 1: 12.650
CRAFT 1: 23.080
GENIA 1: 19.027
Table 1: Lexical type-to-token ratios.
3.2 Type/POS tag closure properties
Figure 2 shows the growth in number of type-
POS tag pairs as the number of tokens of lexical
item/POS tag pairs increases. The data from the
different corpora corresponds to the same colors
as in Figure 1.
Once again, we note a drastic difference be-
tween the curve for the BNC and the curves for
CRAFT and GENIA. If anything, the differences
are more pronounced here than in the case of the
lexical closure graph. Again, we do not see an
asymptote in the increase of the curves for CRAFT
and GENIA, but there is a clear difference when
contrasted with the results for the BNC.
The type-to-token sets ratios for the corpora as a
whole are shown in Table 2. Again, as the sublan-
guage model would predict, we see much higher
ratios in CRAFT and GENIA than in BNC.
Corpus name Ratio
BNC 1: 10.80
CRAFT 1: 19.96
GENIA 1: 18.18
Table 2: Type-to-token ratios for type/POS tags.
Because the Machinese Syntax parser was
used to obtain the part-of-speech tagging for
BNC and the Machinese Syntax parser?s tagset is
much more granular and therefore larger than the
CRAFT and GENIA tag sets, both of which are
adaptations of the Penn treebank tag set, we con-
sidered the hypothesis that the large size differ-
ences of the tag sets were the cause of the differ-
ences observed between BNC and the two corpora
of scientific journal articles. To test this hypothe-
sis, we manually mapped the BNC tag set to the
Penn treebank tag set. The result was a new BNC
list of tags, of the same number and granularity
as the CRAFT/GENIA ones (35-36 tags). Using
this mapping, the BNC part-of-speech tags were
converted to the Penn treebank tag set and the ex-
periment was re-run. The results show that there
is almost no difference between the results from
the first and the second experiments. The resulting
graph is omitted for space, but examining it one
can observe that the differences between the three
corpora in the graph are almost the same in both
graphs. The newly calculated type:tokens ratio for
BNC are also illustrative. They are highly similar
to the type-token ratio for the original tag set?
1:10.82 with the mapped data set vs. 1:10.80 with
the original, much larger tag set. This supports the
original results and demonstrates that differences
in tag set sizes do not interfere with the identifica-
tion of sublanguages.
3.3 Sentence type closure properties
Figure 3 shows the growth in number of sentence
types as the number of sentences increases. The
data from the different corpora corresponds to the
same colors as in Figure 1.
Here we see that all three corpora exhibit sim-
76
Figure 3: Sentence type closure properties. Tick-
marks on x axis indicate increments of 5,000 sen-
tences.
ilar curves?essentially linear, with nearly identi-
cal growth rates. This is a strong contrast with the
results seen in Figures 1 and 2. We suggest some
reasons for this in the Discussion section.
The ratio of sentence types to sentence tokens
for the corpora as a whole are given in Table 3.
As would be expected from the essentially linear
growth observed with token growth for all three
corpora, all three ratios are nearly 1:1.
Corpus name Ratio
BNC 1: 1.03
CRAFT 1: 1.14
GENIA 1: 1.11
Table 3: Sentence type-to-token ratios.
4 Discussion and Conclusions
The most obvious conclusion of this study is that
the null hypothesis can be rejected?the scien-
tific corpora show a greater tendency towards clo-
sure than the general English corpus. Further-
more, we observe that the two scientific corpora
behave quite similarly to each other at all three
levels. This second observation is not necessar-
ily a given. If we can consider for a moment the
notion that there might be degrees of fit to the sub-
language model, it is clear that from a content per-
spective the BNC is unlimited; the CRAFT cor-
pus is limited to mouse genomics, but not to any
particular area of mouse genomics (indeed, it con-
tains articles about development, disease, physiol-
ogy, and other topics); and GENIA is more lim-
ited than CRAFT, being restricted to the topic of
human blood cell transcription factors. If a tech-
nique for sublanguage detection were sufficiently
precise and granular, it might be possible to show a
strict ranking from BNC to CRAFT to GENIA in
terms of fit to the sublanguage model (i.e., BNC
showing no fit, and GENIA showing a greater fit
than CRAFT since its subject matter is even more
restricted). However, this does not occur?in our
data, CRAFT showed a stronger tendency towards
closure at the lexical level, while GENIA shows
a stronger tendency towards closure at the mor-
phosyntactic level. It is possible that the small dif-
ferences at those levels are not significant, and that
the two corpora show the same tendencies towards
closure overall.
One reason that the IBM manuals in the
(McEnery and Wilson, 2001) experiments showed
sentence type closure but the CRAFT and GE-
NIA corpora did not in our experiments is al-
most certainly related to sentence length. The
average length of a sentence in the IBM manu-
als is 11 words, versus 24 in the Hansard corpus
and 21 in the American Printing House for the
Blind corpus. In this respect, the scientific cor-
pora are much more like the Hansard and Ameri-
can Printing House for the Blind corpora than they
are like the IBM manuals?the average length of
a sentence in GENIA is 21.47 words, similar to
the Hansard and American Printing House for the
Blind corpora and about twice the length of sen-
tences in the IBM manuals. Similarly, the aver-
age sentence length of the CRAFT corpus is 22.27
words (twice the average sentence length of the
IBM manuals), and the average sentence length in
the BNC is 20.43 words. Longer sentences imply
greater chances for different sentence types.
Another reason for the tendency towards sen-
tence type closure in the IBM manuals, which was
not observed in CRAFT and GENIA, is the strong
possibility that they were written in a controlled
language that specifies the types of syntactic con-
structions that can be used in writing a manual,
e.g. limiting the use of passives, etc., as well as
lexical choices and limits on other options (Kuhn,
under review). There is no such official controlled
language for writing journal articles.
Finally, one reason that the CRAFT and GENIA
corpora did not show sentence type closure while
the IBM manuals did is that while McEnery and
Wilson represented sentence types as parses, we
represented them as sequences of part-of-speech
tags. Representing sentence types as parse trees
has the effect of smoothing out some variability
at the leaf node level. For this reason, our repre-
77
sentation increases the sensitivity of the method to
sentence type differences, providing a stronger test
of fit to the sublanguage model.
It has been suggested since Harris?s classic
work (Harris et al, 1989) that scientific writing
forms a sublanguage. However, it is also clear
from the work of (Stetson et al, 2002) and (Mi-
haila et al, 2012) that some putative sublanguages
are a better fit to the model than others, and to date
there has been no publicly available, repeatable
method for assessing the fit of a set of documents
to the sublanguage model. This paper presents
the first such package of software and uses it to
evaluate two corpora of scientific journal articles.
Future work will include evaluating the effects of
mapping all numbers to a fixed NUMBER token,
which might affect the tendencies towards lexi-
cal closure; evaluating the effect of the size of
tag sets on type/part-of-speech ratios, which might
affect tendencies towards type/part-of-speech clo-
sure; and seeking a way to introduce more syntac-
tic structure into the sentence type analysis with-
out losing the generality of the current approach.
We will also apply the technique to other biomed-
ical genres, such as clinical documents. There
is also an important next step to take?this work
provides a means for recognizing sublanguages,
but does not tackle the problem of determining
their characteristics. However, despite these limi-
tations, this paper presents a large step towards fa-
cilitating the study of sublanguages by providing
a quantitative means of assessing their presence.
In analyzing the results of the study, some im-
plications for natural language processing are ap-
parent. Some of these are in accord with the is-
sues for sublanguage natural language processing
pointed out in the introduction. Another is that this
work highlights the importance of both classic and
more recent work on concept recognition for sci-
entific journal articles (and other classes of sublan-
guages), such as MetaMap (Aronson, 2001; Aron-
son and Lang, 2010), ConceptMapper (Tanenblatt
et al, 2010), and the many extant gene mention
systems.
Acknowledgments
Irina Temnikova?s work on the research re-
ported in this paper was supported by the project
AComIn ?Advanced Computing for Innovation?,
grant 316087, funded by the FP7 Capacity Pro-
gramme (Research Potential of Convergence Re-
gions). Kevin Bretonnel Cohen?s work was sup-
ported by grants NIH 5R01 LM009254-07 and
NIH 5R01 LM008111-08 to Lawrence E. Hunter,
NIH 1R01MH096906-01A1 to Tal Yarkoni, NIH
R01 LM011124 to John Pestian, and NSF IIS-
1207592 to Lawrence E. Hunter and Barbara
Grimpe. The authors thank Tony McEnery and
Andrew Wilson for advice on dealing with the tag
sets.
References
Alan R. Aronson and Francois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association, 17:229?236.
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: The MetaMap
program. In Proc AMIA 2001, pages 17?21.
Michael Bada, Miriam Eckert, Donald Evans, Kristin
Garcia, Krista Shipley, Dmitry Sitnikov, William
A. Baumgartner Jr., Kevin Bretonnel Cohen, Karin
Verspoor, Judith A. Blake, and Lawrence E. Hunter.
2012. Concept annotation in the craft corpus. BMC
Bioinformatics, 13(161).
K. B. Cohen, Lynne Fox, Philip V. Ogren, and
Lawrence Hunter. 2005. Corpus design for biomed-
ical natural language processing. In Proceedings of
the ACL-ISMB workshop on linking biological liter-
ature, ontologies and databases, pages 38?45. As-
sociation for Computational Linguistics.
Timothy W. Finin. 1986. Constraining the interpre-
tation of nominal compounds in a limited context.
In Ralph Grishman and Richard Kittredge, editors,
Analyzing language in restricted domains: sublan-
guage description and processing, pages 85?102.
Lawrence Erlbaum Associates.
Carol Friedman, Philip O. Anderson, John H.M.
Austin, James J. Cimino, and Stephen B. Johnson.
1994. A general natural-language text processor for
clinical radiology. Journal of the American Medical
Informatics Association, 1:161?174.
Carol Friedman, Pauline Kra, Hong Yu, Michael
Krauthammer, and Andrey Rzhetsky. 2001. GE-
NIES: a natural-language processing system for the
extraction of molecular pathways from journal arti-
cles. Bioinformatics, 17(Suppl. 1):S74?S82.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35:222?235.
Carol Friedman. 1986. Automatic structuring of
sublanguage information. In Ralph Grishman and
Richard Kittredge, editors, Analyzing language in
78
restricted domains: sublanguage description and
processing, pages 85?102. Lawrence Erlbaum As-
sociates.
Ralph Grishman and Richard Kittredge. 1986. Ana-
lyzing language in restricted domains: sublanguage
description and processing. Lawrence Erlbaum As-
sociates.
Zellig Harris, Michael Gottfried, Thomas Ryckman,
Anne Daladier, Paul Mattick, T.N. Harris, and Su-
sanna Harris. 1989. The form of information in
science: analysis of an immunology sublanguage.
Kluwer Academic Publishers.
Lynette Hirschman and Naomi Sager. 1982. Auto-
matic information formatting of a medical sublan-
guage. In Richard Kittredge and John Lehrberger,
editors, Sublanguage: studies of language in re-
stricted semantic domains, pages 27?80. Walter de
Gruyter.
Timo Ja?rvinen, Mikko Laari, Timo Lahtinen, Sirkku
Paajanen, Pirkko Paljakka, Mirkka Soininen, and
Pasi Tapanainen. 2004. Robust language analy-
sis components for practical applications. In Ro-
bust and adaptive information processing for mobile
speech interfaces: DUMAS final workshop, pages
53?56.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. Genia corpus?a semanti-
cally annotated corpus for bio-textmining. Bioinfor-
matics, 19(Suppl. 1):180?182.
Richard I. Kittredge. 2003. Sublanguages and con-
trolled languages. In Ruslan Mitkov, editor, The Ox-
ford Handbook of Computational Linguistics, pages
430?447. Oxford University Press.
Tobias Kuhn. under review. Survey and classification
of controlled natural languages. Computational Lin-
guistics.
G. Leech, R. Garside, and M. Bryant. 1994. The large-
scale grammatical tagging of text: experience with
the British National Corpus. In N. Oostdijk and
P. de Haan, editors, Corpus based research into lan-
guage.
David D. McDonald. 2000. Natural language genera-
tion. In Robert Dale, Hermann Moisl, and Harold
Somers, editors, Handbood of Natural Language
Processing, pages 147?179. Marcel Dekker.
Tony McEnery and Andrew Wilson. 2001. Corpus
Linguistics. Edinburgh University Press, 2nd edi-
tion.
Claudiu Mihaila, Riza Theresa Batista-Navarro, and
Sophia Ananiadou. 2012. Analysing entity type
variation across biomedical subdomains. In Third
workshop on building and evaluating resources for
biomedical text mining, pages 1?7.
Naomi Sager. 1986. Sublanguage: linguistic phe-
nomenon, computational tool. In Ralph Grishman
and Richard Kittredge, editors, Analyzing language
in restricted domains: sublanguage description and
processing, pages 1?17. Lawrence Erlbaum Asso-
ciates.
Satoshi Sekine. 1994. A new direction for sublan-
guage nlp. In Proceedings of the international con-
ference on new methods in natural language pro-
cessing, pages 123?129.
Harold Somers. 1998. An attempt to use weighted
cusums to identify sublanguages. In NeM-
LaP3/CoNLL98: New methods in language process-
ing and computational natural language learning,
pages 131?139.
Harold Somers. 2000. Machine translation. In Robert
Dale, Hermann Moisl, and Harold Somers, editors,
Handbook of Natural Language Processing, pages
329?346. Marcel Dekker.
Peter D. Stetson, Stephen B. Johnson, Matthew Scotch,
and George Hripcsak. 2002. The sublanguage of
cross-coverage. In Proc. AMIA 2002 Annual Sym-
posium, pages 742?746.
Michael Tanenblatt, Anni Coden, and Igor Sominsky.
2010. The ConceptMapper approach to named en-
tity recognition. In Language Resources and Evalu-
ation Conference, pages 546?551.
Karin Verspoor, Kevin Bretonnel Cohen, Arrick Lan-
franchi, Colin Warner, Helen L. Johnson, Christophe
Roeder, Jinho D. Choi, Christopher Funk, Yuriy
Malenkiy, Miriam Eckert, Nianwen Xue, William
A. Baumgartner Jr., Michael Bada, Martha Palmer,
and Lawrence E. Hunter. 2012. A corpus of full-text
journal articles is a robust evaluation tool for reveal-
ing differences in performance of biomedical natu-
ral language processing tools. BMC Bioinformatics,
13(207).
79
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 10?18,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Temporal Expression Recognition for Cell Cycle Phase Concepts in
Biomedical Literature
Negacy D. Hailu, Natalya Panteleyeva and K. Bretonnel Cohen
Computational Bioscience Program, University of Colorado Denver
School of Medicine
negacy.hailu@ucdenver.edu, natalya.panteleyeva@ucdenver.edu,
kevin.cohen@gmail.com
Abstract
In this paper, we present a system for
recognizing temporal expressions related
to cell cycle phase (CCP) concepts in
biomedical literature. We identified 11
classes of cell cycle related temporal ex-
pressions, for which we made extensions
to TIMEX3, arranging them in an on-
tology derived from the Gene Ontology.
We annotated 310 abstracts from PubMed.
Annotation guidelines were developed,
consistent with existing time-related anno-
tation guidelines for TimeML. Two anno-
tators participated in the annotation. We
achieved an inter-annotator agreement of
0.79 for an exact span match and 0.82
for relaxed constraints. Our approach is
a hybrid of machine learning to recognize
temporal expressions and a rule-based ap-
proach to map them to the ontology. We
trained a named entity recognizer using
Conditional Random Fields (CRF) mod-
els. An off-the-shelf implementation of
the linear chain CRF model was used. We
obtained an F-score of 0.77 for temporal
expression recognition. We achieved 0.79
macro-averagee F-score and 0.78 micro-
averaged F-score for mapping to the on-
tology.
1 Introduction
Storing and processing temporal data in biomed-
ical informatics is important, but challeng-
ing (Zhou and Hripcsak, 2007; Augusto, 2005).
Biomedical data is often intrinsically associated
with time. For example, data from electronic med-
ical records are on a clinical timeline (Zhou and
Hripcsak, 2007) which links all information on the
progress of a patient?s status. Temporal reasoning
remains a challenge for medical information sys-
tems (Combi and Shahar, 1997). Conventionally,
dictionaries define time as ?The continuous pas-
sage of existence in which events pass from a state
of potentiality in the future, through the present, to
a state of finality in the past? (Editorial Staff, un-
dated). This traditional linear concept of tempo-
rality does not adequately capture the cyclical na-
ture of some important biological processes, such
as the cell cycle and circadian rhythms. In this pa-
per, we describe a system for the recognition of
temporal expressions related to cell cycle phases
in biomedical literature. The cell cycle is a phe-
nomenon that a cell goes through during its growth
and replication. Its stages are depicted in Figure 1.
We treat each phase as a distinct time component
and we aim at recognizing expressions that de-
scribe them in biomedical literature, then mapping
them to an ontology of cell cycle phases and tran-
sitions. Specifically, we are interested in recog-
nizing expressions that contain one or more of the
concepts shown in Table 1, where the Gene Ontol-
ogy is taken as definitional of concepts related to
phases of the cell cycle.
Recognition of cell cycle phase concepts from
text is a non-trivial problem. Some of the ways
that they can be mentioned in text, such as inter-
phase, anaphase, and prophase are relatively un-
ambiguous and can be recognized and mapped to
an ontology using regular expressions. However,
as is often the case both in general language and in
biomedical language, many of the ways in which
they can be mentioned are highly ambiguous. For
example, M, which stands for mitosis, is often a
unit of measurement, as in . . . removal of histone
HI with 0,6 M NaCl. (PMID: 6183061) M could
also be an abbreviation of an author?s first name,
as in . . . Suzuki S, Nakata M. (PMID: 23844291) S,
which refers to S-phase or synthesis phase, could
also stand for an author?s first name, as well as
a protein name, as in . . . Protein S acts as a co-
factor for tissue factor pathway inhibitor. (PMID:
23841464). In addition, the word synthesis is in it-
10
self ambiguous, even in the context of other men-
tions of cell cycle phases. In the following exam-
ples, it refers to something other than a cell cycle
phase:
? . . . histone synthesis by lymphocytes in G0
and G1. (PMID: 6849885)
? . . . metaphase-anaphase transition, as a re-
sult of fertilization, activation or protein syn-
thesis inhibition. (PMID: 9552372)
We treated recognition of temporal expressions
from literature as a named entity recognition
(NER) problem. Many approaches to named en-
tity recognition are based on machine learning
techniques. Nadeau and Sekine report that al-
though semi-supervised learning algorithms have
been employed in NER challenges, most systems
that perform well are built based on supervised
learning techniques (Nadeau and Sekine, 2007).
Based on this survey report, we used Conditional
Random Fields (CRFs) for the recognition phase
of our approach. The details of our methods are
described in section 4.2.
Figure 1: Schematic of the cell cycle. Outer ring:
I = Interphase, M = Mitosis; inner ring: M = Mi-
tosis, G1 = Gap 1, G2 = Gap 2, S = Synthesis; not
in ring: G0 = Gap 0/Resting [Wikipedia].
2 Motivation
A vast collection of biomedical literature in
PubMed/MEDLINE and other biomedical jour-
nal repositories is estimated to grow exponen-
tially (Hunter and Cohen, 2006), as shown in
Figure 2. Searching for papers specific to
a researcher?s interest in any domain is diffi-
cult. PubMed/MEDLINE allows search using
keywords, but until recently did not rank results by
document relevance. General-purpose search en-
gines such as Google and Bing rank their results,
but are not well-suited for search of specialized in-
formation related to genes and small molecules.
Building a specialized search engine exclusively
to search biomedical literature using genes and
small molecules as keywords could be very use-
ful, for instance, for cancer researchers.
Figure 2: Publication growth rate at Med-
line (Hunter and Cohen, 2006)
Our long term goal is to build a spe-
cialized search engine specific to cancer re-
search. The system will retrieve articles from
PubMed/MEDLINE and rank them according to
their relevance. The system will utilize gene, pro-
tein, and small molecule names as keywords in
document search. We are also interested in identi-
fying the phase(s) of the cell cycle during which
the gene is expressed. After detecting the ac-
tive phase(s) of a gene or gene product, the sys-
tem will link relevant documents to this gene from
PubMed/MEDLINE. In this paper we present our
first step towards that goal, which is extraction of
temporal expressions from biomedical literature.
Temporal expressions will be used to identify ac-
tive phases of genes or gene products.
3 Related Work
Automatic recognition of events and temporal ex-
pressions from text has attracted researchers from
areas such as computer science and linguistics.
11
Concept ID Activities in each phase Synonyms
Interphase GO:0051325 The cell readies itself for meiosis or mitosis and the
replication of its DNA occurs.
karyostasis
G0 phase GO:0044838 Cells enter in response to cues from the cell?s envi-
ronment.
quiescence
G1 phase GO:0051318 Gap phase -
S phase GO:0051320 DNA synthesis takes place. S-phase, synthesis
G2 phase GO:0051319 Gap phase -
Mitosis GO:0007067 The nucleus of a eukaryotic cell divides -
Prophase GO:0051324 Chromosomes condense and the two daughter cen-
trioles and their asters migrate toward the poles of
the cell.
-
Metaphase GO:0051323 Chromosomes become aligned on the equatorial
plate of the cell.
-
Anaphase GO:0051322 The chromosomes separate and migrate towards the
poles of the spindle.
-
Telophase GO:0051326 The chromosomes arrive at the poles of the cell and
the division of the cytoplasm starts.
-
Table 1: Cell cycle phase concepts. Definitions from the Gene Ontology.
The results have contributed to the development
of diverse natural language processing applica-
tions, such as information extraction, information
retrieval, question-answering systems, text sum-
marization, etc. TimeML: Robust Specification of
Event and Temporal Expressions in Text (Puste-
jovsky et al., 2003) is a specification language for
annotation of events and temporal expressions in
human language. TimeML addresses specification
issues like time stamping, order of events, reason-
ing about events, and time expressions.
TempEval is one of the shared challenges in-
cluded in SemEval (Agirre et al., 2009) as of 2007.
It aims at advancing research on processing tem-
poral information. Primarily it focuses on three
tasks: event extraction and classification, temporal
expression extraction and normalization, and tem-
poral relation extraction (UzZaman et al., 2013).
However, this ongoing work on temporal evalu-
ation is based on language data collected from
the news. In the clinical domain, (Styler IV et
al., Undated; Palmer and Pustejovsky, 2012; Al-
bright et al., 2013) describe the THYME annota-
tion project. The scope and language of temporal-
ity related to the cell cycle is different from that of
both TempEval and the clinical domain, and sup-
ports (and demands) different types of reasoning,
specifically related to cyclical time.
Cyclical phenomena are ubiquitous in can-
cer development and progression. The connec-
tion between the cell cycle and cancer is well
known (Vermeulen et al., 2003; Kastan and
Bartek, 2004; Malumbres and Barbacid, 2009),
and the fact that the cell cycle is the main target
for cancer regulation, deregulation, and therapy
is well established (Vermeulen et al., 2003; Kas-
tan and Bartek, 2004; Malumbres and Barbacid,
2009). Circadian rhythms, rounds of chemother-
apy, remissions, and re-occurrences all have a
cyclic nature.Circadian rhythms have been inves-
tigated in the study of cancer treatment (Sahar and
Sassone-Corsi, 2009; Ortiz-Tudela et al., 2013;
Lengyel et al., 2009; Kelleher et al., 2014).
From the perspective of cancer research, identi-
fying cell cycle concepts in the literature is crucial
to being able to retrieve and explore information
related to cyclical biological processes like the cell
life cycle. From the natural language processing
perspective, the novelty of this work consists in
modeling cyclical time. To our knowledge, tempo-
ral event recognition grounded in a cyclical model
of time has not been previously proposed.
4 Methodology
4.1 Materials
We built a corpus of 360 abstracts, consisting of
70,570 words. The concepts are presented in Ta-
ble 1. We balanced our corpus by collecting arti-
cles from the PubMed/MEDLINE database using
the concepts individually as keywords. We used
12
the PubMed/MEDLINE
1
and BioMedLib search
engines
2
, two keyword-based search engines built
on top of MEDLINE, for this purpose. The fol-
lowing keywords were used to collect the abstracts
from PubMed and BioMedLib:
? interphase, G0, G0 phase, G1, G1 phase, syn-
thesis, S phase, G2, G2 phase
? Mitosis, M phase, prophase, metaphase,
anaphase, telophase
? checkpoint
The annotation guidelines addressed the follow-
ing issues:
? The goal of the project: the goal of the anno-
tation project was to develop a highly anno-
tated corpus specific to CCP concepts, which
will be used for automatic recognition and
classification.
? Specification of each tag: this is shown in
Figure 3.
? Tool used to annotate the project: We used
Knowtator (Ogren, 2006), a text annotation
tool built on top of the Prot?eg?e knowledge
representation system.
Modeling the phenomenon was the first step in
understanding the annotation process (Pustejovsky
and Stubbs, 2012). We modeled our corpus as a
triple, Model = <T, R, I>, as shown below:
? Model = <T, R, I> where T = terms, R = re-
lation between the terms, and I = interaction
? T = {Named Entity, time expression, not time
expression}
? R = {Named Entity ::= TIMEXCCP | not
TIMEXCCP}
? I = {TIMEXCCP = list of concepts from Ta-
ble 1 or checkpoints. Examples of check-
points are G1/G2 phase, S/G2 phase, etc.
TimeML is a specification for annotating hu-
man language in text (Pustejovsky et al., 2003).
TIMEX3 is defined in TimeML as a tag for cap-
turing dates, times, durations, and sets of dates and
1
http://www.ncbi.nlm.nih.gov/pubmed/
2
http://bmlsearch.com/
times. In our work we extended TIMEX3. We em-
ploy a single tag set called TIMEXCCP, where the
naming is intended to be consistent with existing
time-related tag sets. Figure 3 shows the attributes
and functions of the tag TIMEXCCP, as well as
examples of usage.
Figure 3: Attributes and functions of the TIMEX-
CCP tag.
Two annotators with training in the domain per-
formed the annotation. Inter-annotator agreement
was calculated as F-measure, following (Hripcsak
and Rothschild, 2005). Inter-annotator agreement
was 0.79 for an exact-span match and 0.82 for re-
laxed matching. The constraints, which are values
of the attributes, were not considered while com-
puting IAA for the latter case.
The annotation effort developed through sev-
eral iterations, applying the annotation devel-
opment cycle introduced by Pustejovsky and
Stubbs (Pustejovsky and Stubbs, 2012). This
methodology is depicted in Figure 4. It is called
the MATTER cycle, which stands for Model, An-
notation, Train, Test, Evaluation, Revise. The ad-
vantage of this methodology is that it allows us
to discover hidden specifications and refine them
during the MATTER cycle.
4.2 Methods
We are particularly interested in recognizing and
classifying temporal expressions in the literature.
For example, in the following sentence, taken from
Wikipedia, the recognition task is to recognize the
blue boxes as shown below and classify them. The
mapping task is to categorize the recognized tem-
poral expressions into the concepts shown in Ta-
ble 1.
"Microhomology-mediated end
joining (MMEJ) uses a Ku
protein and DNA-PK independent
repair mechanism, and
13
Figure 4: The MATTER cycle (Pustejovsky and
Stubbs, 2012)
repair occurs during the
S phase of the cell cycle,
as oposed to the G0/G1 and
early S phases in NHEJ and
late S to G2 phases in HR."
... the S phase of the cell cycle, as opposed
to the G0/G1 and early S phases in NHEJ and
late S to G2 phase in HR.
In this example, there are four temporal expres-
sions: S, G0/G1, early S, and late S to G2. The
expression ?S? is of the type S-phase or synthesis
phase according to the conceptual ontology in
Table 1. The expression ?G0/G1? can be classified
as G0 and G1. Similarly, the expression ?late S to
G2? can be of type S and G2.
Our approach is a hybrid of machine learning
and rule-based techniques. The machine learning
technique, which we refer to as the first layer, is
applied for temporal expression recognition. In
this layer, CRFs are trained to learn to recognize
the expressions from the list of features which is
shown below.
1. Word-level features:
? Is the word in uppercase?
? Is the first character of the word in up-
percase?
? Words themselves are also treated as
features.
? Length of the word.
2. Punctuation-related features:
? Does the word contain at least one of the
most common punctuation marks?
3. Digit-related features:
? Is the word a digit?
? Does the word contain a digit?
4. Does the word contain either of the follow-
ing: phase, arrest, entry? These words typi-
cally come before or after the cell cycle con-
cepts. For example, early mitosis, G0 phase.
5. Part-of-Speech tagging: Window size of 2
before and after the word.
6. Presence of concept modifiers before the
word. Modifiers include: early, mid, late,
early-mid.
Conditional Random Fields (CRFs) are one of
the probabilistic graphical model sequence tag-
ging techniques. They are understood as a sequen-
tial version of Maximum Entropy Models (Klinger
and Tomanek, 2007). One advantage of CRFs over
other probabilistic models like Hidden Markov
Models and Maximum Entropy Models for com-
plex systems is their support for features interact-
ing with one another. The linear chain CRF repre-
sentation is shown in Figure 5.
Figure 5: A linear chain Conditional Ran-
dom Field representation (Klinger and Tomanek,
2007).
In this representation, ~x is a vector of observa-
tions, also known as features in machine learn-
ing, and the y
t
?s are states or labels. In this lin-
ear chain model, a given state is dependent on its
previous, current, and next states. It is also in-
fluenced by the observations for that state. This
argument can be formulated as Equation 1. Ac-
cordingly, state prediction will be an optimization
of Equation 1. ?
c
(~x, ~y) are the factor matrices of
14
the maximal cliques read from the factor graph in
Figure 5 (Klinger and Tomanek, 2007).
P (~y|~x) =
1
Z(~x)
?
cC
?
c
(~x, ~y) (1)
We used the IOB format, which is the most com-
mon method of representation for sequence tag-
ging. In this format, I stands for the inside, O is
the outside, and B is the beginning of a temporal
expression. Table 2 shows an example of IOB la-
beling for the phrase . . . late S to G2 phase in HR.
token tag
. . . . . .
late B TIMEXCCP
S I TIMEXCCP
to I TIMEXCCP
G2 I TIMEXCCP
phase O
in O
HR O
. O
Table 2: IOB format representation of a segment
of a sentence.
The rule-based system is keyword-based. The
rules match simple cell cycle phase concepts. For
example, the phrase early S phase is classified as
synthesis, since there is S in it. The expression
G0/G1 phase is classified as a G0/G1 checkpoint.
5 Experimental setup
We split our dataset of more than 70K tokens into
80% training and 20% test sets. We used 5-fold
cross validation to balance the distribution of the
dataset. The number of positive instances for the
5 runs is shown in Figure 6. The expressions
S and synthesis are displayed separately, despite
their identical meaning, to allow for more granu-
lar evaluation of performance. The same rationale
applies to displaying M and mitosis separately.
The ratio of the individual concepts that we
have in the 5 runs is balanced, as shown in Fig-
ure 6. However, the training dataset is skewed,
since there are almost 98% negative labels, with
the remaining small portion as positive labels.
Among the approximately 10K test tokens, 180
of them are labeled as positive TIMEXCCP, but
the others are negative, i.e. they have the label
O. A positive TIMEXCCP in this case could be
B TIMEXCCP or I TIMEXCCP?beginning or
inside of a temporal expression.,
6 Results
Since the task consisted of two separate steps?
temporal expression recognition, and mapping or
normalization?in this section, we report our find-
ings independently. Our evaluation metrics are in
terms of precision P, recall R, and F-measure. The
system achieved precision P = 0.83, recall R =
0.72 and F = 0.77 for recognizing TIMEXCCP in
biomedical literature.
The temporal expression mapper, which is a
rule-based system, achieved a macro-averaged P=
0.90, R = 0.70, and F = 0.79 and a micro-averaged
P = 0.86, R = 0.71, and F = 0.78. The system per-
formance for the individual concepts is shown in
Figure 7.
7 Discussion
Some of the false positive predictions were due to
human annotation errors.
There were some conditions where the annota-
tors disagreed. For example, . . . early G1 to G2
phase. This examples addresses two questions that
should be explicitly mentioned in the annotation
guidelines:
? Does the modifier ?early? modify only G1, or
both G1 and G2?
? Should there be an attribute for the range of
time from G1 to G2 in the annotation guide-
lines?
Our system achieved good performance on
both time expression recognition and mapping of
highly ambiguous concepts. In spite of the chal-
lenges presented by ambiguity, we obtained 0.85,
0.81, and 0.80 F-measures for recognizing and
mapping the concepts synthesis, M, and S, respec-
tively. The most informative features that con-
tribute to this score are the discriminating words
before and after a target token. These words are:
phase, arrest, and entry. They are often present
before or after CCP concepts. Also, presence of
modifiers is a good indication of CCP concepts.
For example, in the phase early S phase, the mod-
ifier early is one of the most informative features.
However, recognition of complex phrases as in
late S to G2 phase remained a challenge.
The challenges of complex temporal expres-
sions can be seen from a different perspective.
15
Figure 6: Distribution of concepts in 5 runs.
Figure 7: Rule-based classification performance. Average score for 5 runs.
Mostly the system recognizes the individual con-
cepts within a complex phrase, but not the mod-
ifiers nor the words like prepositions within the
complex phrase. In the example given previously,
the system recognizes S and G2 but not the modi-
fier late, nor the preposition to. These challenges
could be tackled by having features that address
the modifiers as well as words within two con-
cepts.
We used a naive tokenizer that splits the text
into words based on white space. In the future,
we would like to test the system with other more
sophisticated tokenizers. We kept punctuation
marks in temporal expressions, for example, the
forward slash in G0/G1 phase. Presence of punc-
tuation marks, such as hyphen (-), forward slash
(/), comma (,) and single quote (?), within a token
is one of our features in training the machine learn-
ing algorithm to recognize temporal expressions.
8 Conclusions & Future work
Cell cycle phase concepts are time expressions,
and can be annotated in a fashion similar to
TimeML. In this work, we annotated a corpus with
cell cycle phase information. This corpus can be
used to train machine learning algorithms to pre-
dict cell cycle phase concepts. The concepts were
annotated using the TIMEXCCP tag, an extension
of TIMEX3, which has the following attributes:
value, modifier, set, and comments. The details
are in Figure 3.
We have developed a temporal expression rec-
ognizer and classifier based on a hybrid of ma-
chine learning and rule-based techniques. We pro-
pose a two-tiered architecture to solve temporal
expression recognition and mapping for CCP con-
cepts. The first tier recognizes temporal expres-
sions using CRFs. In the second tier, a rule-based
system classifies the concepts.
Some of the main future directions for this
works are testing the system with the addition of
more annotated data. We will focus on how we
can capture complex time expressions. This might
take us to redefining the annotation guidelines that
we have right now.
16
Acknowledgments
The authors thank Richard Osborne and Scott
Cramer for helpful discussion of the significance
of this work from a cancer research perspective.
References
Eneko Agirre, Llu??s M`arquez, and Richard Wicen-
towski. 2009. Computational semantic analysis
of language: Semeval-2007 and beyond. Language
Resources and Evaluation, 43(2):97?104.
Daniel Albright, Arrick Lanfranchi, Anwen Fredrik-
sen, William F Styler, Colin Warner, Jena D Hwang,
Jinho D Choi, Dmitriy Dligach, Rodney D Nielsen,
James Martin, et al. 2013. Towards comprehen-
sive syntactic and semantic annotations of the clin-
ical narrative. Journal of the American Medical In-
formatics Association, 20(5):922?930.
Roberta Alfieri, Ivan Merelli, Ettore Mosca, and Lu-
ciano Milanesi. 2007. The Cell Cycle DB: a sys-
tems biology approach to cell cycle analysis. Nu-
cleic Acids Research.
Juan Carlos Augusto. 2005. Temporal reasoning for
decision support in medicine. Artificial Intelligence
in Medicine, 33(1):1?24.
Matteo Brucato, Leon Derczynski, Hector Llorens,
Kalina Bontcheva, and Christian S. Jensen. 2013.
Recognising and interpreting named temporal ex-
pressions. In Galia Angelova, Kalina Bontcheva,
and Ruslan Mitkov, editors, RANLP, pages 113?121.
RANLP 2011 Organising Committee/ACL.
C. Combi and Y. Shahar. 1997. Temporal reasoning
and temporal data maintenance in medicine: Issues
and challenges. Comput Biol Med, 27 (5).
Carlo Combi, Elpida Keravnou-Papailiou, and Yuval
Shahar. 2010. Temporal Information Systems in
Medicine. Springer Publishing Company, Incorpo-
rated, 1st edition.
Collins Editorial Staff. undated. Collins Concise En-
glish Dictionary.
Nicholas Paul Gauthier, Lars Juhl Jensen, Rasmus
Wernersson, S?oren Brunak, and Thomas S. Jensen.
2009. Cyclebase.org: version 2.0, an updated com-
prehensive, multi-species repository of cell cycle
experiments and derived analysis results. Nucleic
Acids Research, 9.
Erik Hatcher, Otis Gospodnetic, and Mike McCand-
less. 2nd revised edition. edition.
George Hripcsak and Adam S Rothschild. 2005.
Agreement, the f-measure, and reliability in infor-
mation retrieval. Journal of the American Medical
Informatics Association, 12(3):296?298.
Lawrence Hunter and K. Bretonnel Cohen. 2006.
Biomedical Language Processing: Perspective
What?s Beyond PubMed? Molecular Cell, 21:589?
594.
Michael Kahn. December 1991. Modeling time in
medical decision-support programs. Med Decision
Making, 11(4):249?264.
Michael B. Kastan and Jiri Bartek. 2004. Cell-cycle
checkpoints and cancer. Nature, 432:316?323.
Fergal C. Kelleher, Aparna Rao, and Anne Maguire.
2014. Circadian molecular clocks and cancer. Can-
cer Letters, 342:9?18.
Roman Klinger and Katrin Tomanek. 2007. Classi-
cal Probabilistic Models and Conditional Random
Fields. Technical Report TR07-2-013, Department
of Computer Science, Dortmund University of Tech-
nology, December.
R. Leaman and Gonzalez G. 2008. BANNER: An exe-
cutable survey of advances in biomedical named en-
tity recognition. Pacific Symposium on Biocomput-
ing, 13:652?663.
Zsuzsanna Lengyel, Zita Batty?ani, Gy?orgy Szekeres,
Val?er Csernus, and Andr?as D. Nagy. 2009. Cir-
cadian clocks and tumor biology: what is to learn
from human skin biopsies? Nature Reviews Cancer,
9:153?166.
Marcos Malumbres and Mariano Barbacid. 2009. Cell
Cycle, CDKs and cancer: a changing paradigm. Na-
ture Reviews Cancer, 9:153?166.
Inderjeet Mani, Ben Wellner, Marc Verhagen, and
James Pustejovsky. 2007. Three approaches to
learning TLINKs in TimeML. Technical report,
Computer Science Department, Brandeis University.
Waltham, USA.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
UK.
Andrew Kachites McCallum. 2002. MALLET: A ma-
chine learning for language toolkit.
David Nadeau and Satoshi Sekine. 2007. A survey
of named entity recognition and classification. Lin-
guisticae Investigationes, 30(1):3?26, January. Pub-
lisher: John Benjamins Publishing Company.
Philip V. Ogren. 2006. Knowtator: a prot?eg?e plug-in
for annotated corpus construction. In Proceedings of
the 2006 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 273?275, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
17
Elisabet Ortiz-Tudela, Ida Iurisci, Jacques Beau, Ab-
doulaye Karaboue, Thierry Moreau, Maria Ange-
les Rol, Juan Antonio Madrid, Francis L?evi, and
Pasquale F. Innominato. 2013. The circadian
rest-activity rhythm, a potential safety pharmacol-
ogy endpoint of cancer chemotherapy. International
Journal of Cancer.
Martha Palmer and James Pustejovsky. 2012. 2012
i2b2 temporal relations challenge annotation guide-
lines.
James Pustejovsky and Amber Stubbs. 2012. Nat-
ural Language Annotation for Machine Learning.
O?REILLY.
James Pustejovsky, Jos`e Castano, Robert Ingria, Roser
Saur, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. TimeML: Robust specification of
event and temporal expressions in text. In Fifth In-
ternational Workshop on Computational Semantics
(IWCS-5).
Saurabh Sahar and Paolo Sassone-Corsi. 2009.
Metabolism and cancer: the circadian clock connec-
tion. Nature, 9:886?896.
Yuval Shahar and Carlo Combi. 1999. Editors? fore-
word: Intelligent temporal information systems in
medicine. J. Intell. Inf. Syst., 13(1-2):5?8.
Paul T. Spellman, Gavin Sherlock, Michael Q. Zhang,
Vishwanath R. Iyer, Kirk Anders, Michael B.
Eisen, Patrick O. Brown, David Botstein, and Bruce
Futcher. 1998. Comprehensive identification of cell
cycle-regulated genes of the yeast Saccharomyces
cerevisiae by microarray hybridization. Molecular
Biology of the Cell, 9.
William F Styler IV, Steven Bethard, Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, and Guer-
gana Savova. Undated. Temporal annotation in the
clinical domain.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Katrien Vermeulen, Dirk R. Van Bockstaele, and
Zwi N. Berneman. 2003. The Cell Cycle: a review
of regulation, deregulation and therapeutic targets in
cancer. Cell Proliferation, 36:131?149.
Michael L. Whitield, Gavin Sherlock, Alok J. Sal-
danha, John I. Murray, Catherine A. Ball, Karen E.
Alexander, John C. Matese, Charles M. Perou,
Myra M. Hurt, Patrick O. Brown, and David Bot-
stein. 2002. Identication of genes periodically ex-
pressed in the human cell cycle and their expression
in tumors. Molecular Biology of the Cell, 13.
Li Zhou and George Hripcsak. 2007. Temporal rea-
soning with medical data - a review with emphasis
on medical natural language processing. Journal of
Biomedical Informatics, 40(2):183?202.
18
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 93?97,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
A repository of semantic types in the MIMIC II database clinical notes
Richard M. Osborne
Computational Bioscience
University of Colorado
School of Medicine
richard.osborne@ucdenver.edu
Alan R. Aronson
National Library of Medicine
Bethesda, MD
alan@nlm.nih.gov
K. Bretonnel Cohen
Computational Bioscience
University of Colorado
School of Medicine
kevin.cohen@gmail.com
Abstract
The MIMIC II database contains
1,237,686 clinical documents of vari-
ous kinds. A common task for researchers
working with this database is to run
MetaMap, which uses the UMLS
Metathesaurus, on those documents
to identify specific semantic types of
entities mentioned in them. However,
this task is computationally expensive
and time-consuming. Research in many
groups could be accelerated if there were
a community-accessible set of outputs
from running MetaMap on this document
collection, cached and available on the
MIMIC-II website. This paper describes a
repository of all MetaMap output from the
MIMIC II database, publicly available,
assuming compliance with usage agree-
ments required by UMLS and MIMIC-II.
Additionally, software for manipulating
MetaMap output, available on Source-
Forge with a liberal Open Source license,
is described.
1 Introduction
1.1 The MIMIC II database and its textual
contents
The Multiparameter Intelligent Monitoring in In-
tensive Care II (MIMIC-II) database is a public-
access intensive care unit database that contains
a broad array of information for over 33,000 pa-
tients. The data were collected over a 7 year pe-
riod, beginning in 2001 from Boston?s Beth Is-
rael Deaconess Medical Center (Saeed et al, 2011;
Goldberger et al., 2000).
Of particular interest are the 1,237,686 clini-
cal documents, which are broadly classified into
the following four groups: MD notes, discharge
summaries, radiology reports and nursing/other.
Each free-text note contains information describ-
ing such things as a given patient?s health, ill-
nesses, treatments and medications, among others.
1.2 Motivation for the resource: MetaMap
runtimes
Part of the motivation for making this resource
publicly available is that considerable resources
must be expended to process it; if multiple groups
can share the output of one processing run, the sav-
ings across the community as a whole could be
quite large. To illustrate why this would be valu-
able from a resources perspective, we provide here
some statistics on the performance of MetaMap.
Random samples of each category (10% each)
were chosen and Monte Carlo simulation was per-
formed (1,000 iterations per note) to obtain the
running times presented below. The clinical notes
ranged from a minimum of 0 words to a maximum
of 6,684 (some of the notes were 0 bytes because
the note for a particular patient and day contained
no text). The mean, median and mode per doc-
ument processed by MetaMap were 17, 5 and 2
seconds, respectively, with a minimum of 1 and a
maximum of 216 seconds.
Figure 1 below plots the number of words
against processing times in seconds for each of the
of notes, sampled as mentioned above.
The majority of the processing was done on a
Sun Fire X4600M2 server with 16 (4 x Quad-
Core AMD Opteron(tm) Processor 8356 cores,
2.3GHz), 128GB memory and 12 TB of disk stor-
age, currently running Fedora Core 17 Linux.
(An Apple MacBook Pro and a Windows desktop
server were also used to speed processing. The
analysis of the random sample of notes was per-
formed in its entirety on the Sun machine, thereby
providing consistent results for the data in Figure
1.)
93
Figure 1: MetaMap Runtimes
1.3 Motivation for the resource:
reproducibility
Any large-scale run of MetaMap over a huge doc-
ument collection will have occasional failures, etc.
The odds of any two runs having the same out-
put are therefore slim. Moreover, there is po-
tential variability in how documents are prepro-
cessed for use with MetaMap. Using this reposi-
tory of MetaMap outputs will ensure reproducibil-
ity of experiments and also preclude the neces-
sity of performing the same preparatory work and
MetaMap processing on the same data.
1.4 Motivation for the resource: semantic
types
The creation of the MIMIC-II repository is an in-
termediate step in our research. We are extracting
the semantic types found in each clinical note in
an attempt to determine if there exists evidence of
subdomains across the categories used by MIMIC-
II to group the notes.
2 Materials and Methods
2.1 Materials
MetaMap is a program developed at the National
Library of Medicine (NLM) that maps biomedical
concepts to the UMLS Metathesaurus and reports
on the corresponding semantic types.
1
The pro-
gram is used extensively by researchers in the field
of biomedical text mining. See Aronson, 2001;
Aronson and Lang, 2010.
1
Users of MetaMap must comply with the UMLS
Metathesaurus license agreement (https://uts.nlm.
nih.gov/license.html).
Although our focus is on the clinical notes con-
tained in a single table, noteevents, MIMIC-II is
both a relational database (PostgreSQL 9.1.9) con-
taining 39 tables of clinical data and bedside mon-
itor waveforms and the associated derived param-
eters and events stored in flat binary files (with
ASCII header descriptors). For each Intensive
Care Unit (ICU) patient, Saeed et al. (2011) col-
lected a wide range of data including inter alia
laboratory data, therapeutic intervention profiles,
MD and nursing progress notes, discharge sum-
maries, radiology reports, International Classifica-
tion of Diseases, 9th Revision codes, and, for a
subset of patients, high-resolution vital sign trends
and waveforms. All data were scrubbed for per-
sonal information to ensure compliance with the
Health Insurance Portability and Accountability
Act (HIPAA). These data were then uploaded to a
relational database thereby allowing for easy ac-
cess to extensive information for each patient?s
stay in the ICU (Saeed et al.). A more detailed
description of the use of the MIMIC-II database
may be found in(Clifford et al. , 2012).
The abbreviated schema in Table 1 below shows
that each ID uniquely identifies a note along with a
SubjectID, a Category and Text.
2
We added the ID
attribute to noteevents as a primary key because
SubjectID, Category and Text are not keys. Thus,
a particular patient might have many notes and
many categories but each note is uniquely iden-
tified.
Attribute Type Cardinality Sample Values
ID integer unique 1, 2, 3, 4...
SubjectID integer many to one ID 95, 100, 100, 99,
Category character varying(26) many to one ID radiology
Text text many to one ID interval placement of ICD
Table 1: Schema MIMIC-II noteevents table.
As mentioned above, the notes in the MIMIC-II
database are categorized as MD reports, radiology
reports, discharge summaries and nursing/other
reports. The contents of these notes varied greatly.
The MD and nursing notes tended to be short and
unstructured with a number of abbreviations and
misspellings, whereas the radiology reports were
longer, more structured and showed fewer errors.
The MIMIC-II version used for this research is
2.6 (April 2011; 32,536 subjects).
The distribution of reports with summary statis-
tics is below in Table 2.
2
The full noteevents table has ten other attributes such as
admission date, various timestamps and patient information
but these were not relevant to our research.
94
MD Discharge Radiology Nursing Totals
min words 0 0 0 0 0
max words 632 6684 2760 632 6684
median words 108 963.5 174 108 135
average words 131.6 1009.2 265.5 131.6 194.7
total notes 23,270 31,877 383,701 798,838 1,237,686
Table 2: MIMIC-II Clinical Note Summary Stats.
2.2 Methods
We used MetaMap to process the clinical notes in
order to find semantic concepts, the latter of which
are being used in our current research. For the
work in this paper, we used MetaMap 2013 with
the 2013AB database.
Before processing the notes with MetaMap,
a number of preparatory steps were taken. As
mentioned above, a primary key was added to
the noteevents table to provide a unique id for
each note. A Python script then queried the
database extracting each note and storing it in a
file named according to the following convention:
uniqueID subjectID category.txt where uniqueID
is the primary key value from the noteevents ta-
ble, subjectID is the unique number assigned to
each patient and category is one of the four cate-
gories mentioned above.
Each of the notes was then processed by a Bash
shell script to remove blank lines and control char-
acters. (This important step was added after a sig-
nificant amount of processing had already taken
place. If this is not done, a number of prob-
lems arise when running MetaMap). Finally, all
files with 0 bytes were removed. These files were
present because many tuples in the noteevents ta-
ble contained clinical note entries with no data.
The number of options available when running
MetaMap is considerable so we chose those that
would provide a full and robust result set which
would be useful to a wide range of researchers. In
our first run, we limited the threshold for the Can-
didate Score to 1,000. However, for the repository,
no threshold was set so that a full range of output
is provided.
3
The output is in XML in order to structure the
data systematically and provide an easier and con-
sistent way to parse the data. Although we chose
XML initially, we intend to provide the same data
in plain text and Prolog formats, again to provide
utility to a broad range of researchers.
In order to process all files, a Bash shell script
3
The exact MetaMap command we used was
metamap13 ?XMLf ?silent ?blanklines 3 filename.txt
was created that called MetaMap on each note and
created a corresponding XML file, named accord-
ing to the same convention as that for notes but
with the txt extension replaced by xml.
3 Results
3.1 The repository of MetaMap output
The repository for the MetaMap output contains
an XML file for each note that originally contained
text in the MIMIC-II database. Each XML file
contains a wealth of information about each note
and a discussion of this is beyond the scope of
this paper (see http://metamap.nlm.nih.
gov/Docs/MM12_XML_Info.shtml).
For our research, we are interested in the se-
mantic types associated with phrases identified by
MetaMap. Below is a section from output file
768591 19458 discharge.xml. This is a discharge
summary for subject 19458 with a unique note id
of 768591. The note contained the phrase ?Admis-
sion Date? which MetaMap matched with a candi-
date score of 1000 and indicated that it is a tempo-
ral concept (tmco).
Ultimately, the MetaMap output files will be up-
loaded to the PhysioNet website and made avail-
able to the public.
4
The files will be organized in
a fashion similar to the original data files on the
site. Namely, data are grouped by subject ids and
compressed in archives with approximately 1000
files each.
<Candidate>
<CandidateScore>-1000</CandidateScore>
<CandidateCUI>C1302393</CandidateCUI>
<CandidateMatched>Admission date
</CandidateMatched>
<CandidatePreferred>Date of admission
</CandidatePreferred>
<MatchedWords Count="2">
<MatchedWord>admission</MatchedWord>
<MatchedWord>date</MatchedWord>
</MatchedWords>
<SemTypes Count="1">
<SemType>tmco</SemType>
</SemTypes>
The original note contained 975 lines, whereas
the MetaMap xml file contained 248,198. Thus
it is obvious that there is a very large amount of
MetaMap output that we don?t consider but which
may be of interest to other researchers.
4
Subject again to the data usage agreement.
95
3.2 A Python module for manipulating
MetaMap output
In order to make information in the XML files ac-
cessible to others, we developed a Python module
(parseMM xml.py) containing a number of meth-
ods or functions that allow one to parse the XML
tree and extract relevant information.
Although we will add more functionality as
needed and requested, at this point the following
methods are implemented:
? parseXMLtree(filename) ? parses the con-
tents of filename and returns a node repre-
senting the top of the document tree.
? getXMLsummary(XMLtree) ? summarizes
the data contained in the parsed XML tree.
The summary contains top-level elements
and their corresponding text. The output is
much like that contained in typical MetaMap
text output.
? getCUIs(XMLtree) ? returns the MetaMap
CUIs found in the XML tree along with the
matching concepts.
? getNegatedConcepts(XMLtree) ? returns
negated concepts and their corresponding
CUIs.
? getSemanticTypes(XMLtree) ? returns
matched concepts, their CUIs, the candidate
scores and the semantic types associated
with the concept.
? findAttribute(attribute) ? searches the docu-
ment tree for an attribute of the user?s choos-
ing. Returns the attributes with their corre-
sponding text values.
We chose Python to create our module be-
cause of its ease of use and its multi-platform
capabilities. Once Python is installed and the
parseMM xml.py is placed in a directory along
with the MetaMap xml file which is to be ana-
lyzed, retrieving relevant information is relatively
straightforward.
5
5
Under most circumstances, Python is already installed
on the Mac OS X and Linux operating systems.
A stylized version of our code is presented be-
low.
# Parse XML tree and return semantic
types.
import parseMM_xml
xml_tree = \
parseXMLtree("noteid_subid_category.xml")
semTypes = getSemanticTypes(xml_tree)
print(semTypes)
A truncated listing of the output:
CandidateCUI ? C0011008
CandidateMatched ? Date
1 ? SemType ? Temporal Concept
CandidateCUI ? C2348077
CandidateMatched ? Date
2 ? SemType ? Food
In order to fully test the robustness of our mod-
ule, we will do further unit and regression testing,
in addition to providing more exception handling.
Ultimately, the code will be available on Source-
Forge, an Open Source web source code repository
available at www.sourceforge.net.
Acknowledgments
We would like to thank George Moody of MIT for
his help with questions concerning the MIMIC-II
database.
References
Alan R. Aronson 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program, Proc AMIA Symp. 2001; 17:21.
Alan R. Aronson, Franois-Michel Lang 2010. An
overview of MetaMap: historical perspective and
recent advances, J Am Med Inform Assoc 2010;
17:3 229-236
Gari D. Clifford, Daniel J. Scott, Mauricio Vil-
larroel 2012. User Guide and Documen-
tation for the MIMIC II Database, http:
//mimic.physionet.org/UserGuide/
UserGuide.pdf
Ary L. Goldberger; Luis A. N. Amaral; Leon Glass;
Jeffrey M. Hausdorff; Plamen Ch. Ivanov; Roger G.
Mark; Joseph E. Mietus; George B. Moody; Chung-
Kang Peng; H. Eugene Stanley, 2000 PhysioBank,
PhysioToolkit, and PhysioNet: Components of a
New Research Resource for Complex Physiologic
Signals, Circulation 101(23): e215-e220.
96
Mohammed Saeed, Mauricio Villarroel, Andrew T.
Reisner, Gari Clifford, Li-Wei Lehman, George
Moody, Thomas Heldt, Tin H. Kyaw, Benjamin
Moody, Roger G. Mark. 2011 The Multiparameter
intelligent monitoring in intensive care II (MIMIC-
II): A public-access ICU database, Critical Care
Medicine; 39(5):952-960
MetaMap Release Notes Website 2013. MetaMap
2013 Release Notes http://metamap.nlm.
nih.gov/Docs/MM12_XML_Info.shtml
MetaMap 2012 Output Website 2014.
MetaMap 2012 XML Output Explained
http://metamap.nlm.nih.gov/Docs/
MM12_XML_Info.shtml
PhysioNet Website 2014. PhysioNet MIMIC-II Web-
site http://physionet.org/mimic2/
97
