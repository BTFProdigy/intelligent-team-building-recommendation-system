Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 37?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Latent Variable Model of Synchronous Syntactic-Semantic Parsing for
Multiple Languages
Andrea Gesmundo
Univ Geneva
Dept Computer Sci
Andrea.Gesmundo@
unige.ch
James Henderson
Univ Geneva
Dept Computer Sci
James.Henderson@
unige.ch
Paola Merlo
Univ Geneva
Dept Linguistics
Paola.Merlo@
unige.ch
Ivan Titov?
Univ Illinois at U-C
Dept Computer Sci
titov@uiuc.edu
Abstract
Motivated by the large number of languages
(seven) and the short development time (two
months) of the 2009 CoNLL shared task, we
exploited latent variables to avoid the costly
process of hand-crafted feature engineering,
allowing the latent variables to induce features
from the data. We took a pre-existing gener-
ative latent variable model of joint syntactic-
semantic dependency parsing, developed for
English, and applied it to six new languages
with minimal adjustments. The parser?s ro-
bustness across languages indicates that this
parser has a very general feature set. The
parser?s high performance indicates that its la-
tent variables succeeded in inducing effective
features. This system was ranked third overall
with a macro averaged F1 score of 82.14%,
only 0.5% worse than the best system.
1 Introduction
Recent research in syntax-based statistical machine
translation and the recent availability of syntac-
tically annotated corpora for multiple languages
(Nivre et al, 2007) has provided a new opportunity
for evaluating the cross-linguistic validity of statis-
tical models of syntactic structure. This opportu-
nity has been significantly expanded with the 2009
CoNLL shared task on syntactic and semantic pars-
ing of seven languages (Hajic? et al, 2009) belonging
to several different language families.
We participate in this task with a generative,
history-based model proposed in the CoNLL 2008
0Authors in alphabetical order.
shared task for English (Henderson et al, 2008) and
further improved to tackle non-planar dependencies
(Titov et al, 2009). This model maximises the joint
probability of the syntactic and semantic dependen-
cies and thereby enforces that the output structure be
globally coherent, but the use of synchronous pars-
ing allows it to maintain separate structures for the
syntax and semantics. The probabilistic model is
based on Incremental Sigmoid Belief Networks (IS-
BNs), a recently proposed latent variable model for
syntactic structure prediction, which has shown very
good performance for both constituency (Titov and
Henderson, 2007a) and dependency parsing (Titov
and Henderson, 2007b). The use of latent variables
enables this architecture to be extended to learning
a synchronous parse of syntax and semantics with-
out overly restrictive assumptions about the linking
between syntactic and semantic structures.
In this work, we evaluate the ability of this
method to generalise across several languages. We
take the model as it was developed for English, and
apply it directly to all seven languages. The only
fine-tuning was to evaluate whether to include one
feature type which we had previously found did not
help for English, but helped overall. No other fea-
ture engineering was done. The use of latent vari-
ables to induce features automatically from the data
gives our method the adaptability necessary to per-
form well across all seven languages, and demon-
strates the lack of language specificity in the models
of Henderson et al (2008) and Titov et al (2009).
The main properties of this model, that differen-
tiate it from other approaches, is the use of syn-
chronous syntactic and semantic derivations and the
37
use of online planarisation of crossing semantic de-
pendencies. This system was ranked third overall
with a macro averaged F1 score of 82.14%, only
0.5% worse than the best system.
2 The Synchronous Model
The use of synchronous parsing allows separate
structures for syntax and semantics, while still mod-
eling their joint probability. We use the approach
to synchronous parsing proposed in Henderson et al
(2008), where we start with two separate derivations
specifying each of the two structures, then synchro-
nise these derivations at each word. The individual
derivations are based on Nivre?s shift-reduce-style
parsing algorithm (Nivre et al, 2006), as discussed
further below. First we illustrate the high-level struc-
ture of the model, discussed in more detail in Hen-
derson et al (2008).
Let Td be a syntactic dependency tree with
derivation D1d, ..., Dmdd , and Ts be a semantic
dependency graph with derivation D1s , ..., Dmss .
To define derivations for the joint structure
Td, Ts, we divide the two derivations into the
chunks between shifting each word onto the
stack, ctd = Db
t
d
d , ..., D
etd
d and cts = Db
t
ss , ..., De
t
ss ,
where Dbtd?1d = Db
t
s?1s = Shiftt?1 and
De
t
d+1
d = De
t
s+1s = Shiftt. Then the actions of
the synchronous derivations consist of quadruples
Ct = (ctd, Switch, cts, Shiftt), where Switch means
switching from syntactic to semantic mode. This
gives us the following joint probability model,
where n is the number of words in the input.
P (Td, Ts) = ?nt=1 P (Ct|C1, . . . , Ct?1) (1)
These synchronous derivations C1, . . . , Cn only re-
quire a single input queue, since the Shift actions are
synchronised, but they require two separate stacks,
one for the syntactic derivation and one for the se-
mantic derivation.
The probability of each synchronous derivation
chunk Ct is the product of four factors, related to
the syntactic level, the semantic level and the two
synchronising steps. The probability of ctd is de-
composed into one probability for each derivation
action Di, conditioned on its history using the chain
rule, and likewise for cts. These probabilities are es-
timated using the method described in section 3.
Syn cross Sem cross Sem tree No parse
Cat 0% 0% 61.4% 0%
Chi 0% 28.0% 28.6% 9.5%
Cze 22.4% 16.3% 6.1% 1.8%
Eng 7.6% 43.9% 21.4% 3.9%
Ger 28.1% 1.3% 97.4% 0.0%
Jap 0.9% 38.3% 11.2% 14.4%
Spa 0% 0% 57.1% 0%
Table 1: For each language, percentage of training sen-
tences with crossing arcs in syntax and semantics, with
semantic arcs forming a tree, and which were not parsable
using the Swap action.
One of the main characteristics of our syn-
chronous representation, unlike other synchronous
representations of syntax and semantics (Nesson et
al., 2008), is that the synchronisation is done on
words, rather than on structural components. We
take advantage of this freedom and adopt different
methods for handling crossing arcs for syntax and
for semantics.
While both syntax and semantics are represented
as dependency graphs, these graphs differ substan-
tially in their properties. Some statistics which in-
dicate these differences are shown in table 1. For
example, English syntactic dependencies form trees,
while semantic dependency structures are only trees
21.4% of the time, since in general each struc-
ture does not form a connected graph and some
nodes may have more than one parent. The syn-
tactic dependency structures for only 7.6% of En-
glish sentences contain crossing arcs, while 43.9%
of the semantic dependency structures contain cross-
ing arcs. Due to variations both in language char-
acteristics and annotation decisions across corpora,
these differences between syntax and semantics vary
across the seven languages, but they are consis-
tent enough to motivate the development of new
techniques specifically for handling semantic depen-
dency structures. In particular, we use a different
method for parsing crossing arcs.
For parsing crossing semantic arcs (i.e. non-
planar graphs), we use the approach proposed in
Titov et al (2009), which introduces an action Swap
that swaps the top two elements on the parser?s
stack. The Swap action allows the parser to reorder
words online during the parse. This allows words
to be processed in different orders during different
38
portions of the parse, so some arcs can be specified
using one ordering, then other arcs can be specified
using another ordering. Titov et al (2009) found that
only using the Swap action as a last resort is the best
strategy for English (compared to using it preemp-
tively to address future crossing arcs) and we use
the same strategy here for all languages.
Syntactic graphs do not use a Swap action.
We adopt the HEAD method of Nivre and Nils-
son (2005) to de-projectivise syntactic dependencies
outside of parsing.1
3 Features and New Developments
The synchronous derivations described above are
modelled with a type of Bayesian Network called an
Incremental Sigmoid Belief Network (ISBN) (Titov
and Henderson, 2007a). As in Henderson et al
(2008), the ISBN model distinguishes two types of
latent states: syntactic states, when syntactic deci-
sions are considered, and semantic states, when se-
mantic decision are considered. Latent states are
vectors of binary latent variables, which are condi-
tioned on variables from previous states via a pattern
of connecting edges determined by the previous de-
cisions. These latent-to-latent connections are used
to engineer soft biases which reflect the relevant do-
mains of locality in the structure being built. For
these we used the set of connections proposed in
Titov et al (2009), which includes latent-to-latent
connections both from syntax states to semantics
states and vice versa. The latent variable vectors are
also conditioned on a set of observable features of
the derivation history. For these features, we start
with the feature set from Titov et al (2009), which
extends the semantic features proposed in Hender-
son et al (2008) to allow better handling of the non-
planar structures in semantics. Most importantly, all
the features previously included for the top of the
stack were also included for the word just under the
top of the stack. To this set we added one more type
of feature, discussed below.
We made some modifications to reflect differ-
ences in the task definition between the 2008 and
2009 shared tasks, and experimented with one
type of features which had been previously imple-
1The statistics in Table 1 suggest that, for some languages,
swapping might be beneficial for syntax as well.
mented. For the former modifications, the system
was adapted to allow the use of the PFEAT and
FILLPRED fields in the data, which both resulted
in improved accuracy for all the languages. The
PFEAT data field (automatically predicted morpho-
logical features) was introduced in the system in
two ways, as an atomic feature bundle that is pre-
dicted when predicting the word, and split into its
elementary components when conditioning on a pre-
vious word, as was done in Titov and Henderson
(2007b). Because the testing data included a spec-
ification of which words were annotated as predi-
cates (the FILLPRED data field), we constrained the
parser?s output so as to be consistent with this speci-
fication. For rare predicates, if the predicate was not
in the parser?s lexicon (extracted from the training
set), then a sense was taken from the list of senses
reported in the Lexicon and Frame Set resources
available for the closed challenge. If this informa-
tion was not available, then a default sense was con-
structed based on the automatically predicted lemma
(PLEMMA) of the predicate.
We also made use of a previously implemented
type of feature that allows the prediction of a seman-
tic link between two words to be conditioned on the
syntactic dependency already predicted between the
same two words. While this feature had previously
not helped for English, it did result in an overall im-
provement across the languages.
Also, in comparison with previous experiments,
the search beam used in the decoding phase was in-
creased from 50 to 80, producing a small improve-
ment in the overall development score.
All development effort took about two person-
months, mostly by someone who had no previous
experience with the system. Most of this time was
spent on the above differences in the task definition
between the 2008 and 2009 shared tasks.
4 Results and Discussion
We participated in the joint task of the closed chal-
lenge, as described in Hajic? et al (2009). The
datasets used in this challenge are described in Taule?
et al (2008) (Catalan and Spanish), Palmer and Xue
(2009) (Chinese), Hajic? et al (2006) (Czech), Sur-
deanu et al (2008) (English), Burchardt et al (2006)
(German), and Kawahara et al (2002) (Japanese).
39
Rank Average Catalan Chinese Czech English German Japanese Spanish
macro F1 3 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
syntactic acc 1 @85.77 @87.86 76.11 @80.38 88.79 87.29 92.34 @87.64
semantic F1 3 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
Table 2: The three main scores for our system. Rank is within task.
Rank Ave Cze-ood Eng-ood Ger-ood
macro F1 3 75.93 @80.70 75.76 71.32
syn Acc 2 78.01 @76.41 80.84 76.77
sem F1 3 73.63 84.99 70.65 65.25
Table 3: Results on out-of-domain for our system. Rank
is within task.
The official results on the testing set are shown in
tables 2, 3, and 4. The symbol ?@? indicates the
best result across systems. In table 5, we show our
rankings across the different datasets, amongst sys-
tems submitted for the same task.
The overall score used to rank systems is the un-
weighted average of the syntactic labeled accuracy
and the semantic labeled F1 measure, across all lan-
guages (?macro F1? in table 2). We were ranked
third, out of 14 systems. There was only a 0.5% dif-
ference between our score and that of the best sys-
tem, while there was a 1.29% difference between our
score and the fourth ranked system. Only consid-
ering syntactic accuracy, we had the highest aver-
age score of all systems, with the highest individual
score for Catalan, Czech, and Spanish. Only con-
sidering semantic F1, we were again ranked third.
Our results for out-of-domain data (table 3) achieved
a similar level of success, although here we were
ranked second for average syntactic accuracy. Our
precision on semantic arcs was generally much bet-
ter than our recall (shown in table 4). However,
other systems had a similar imbalance, resulting in
no change in our third place ranking for semantic
precision and for semantic recall. Only when the se-
mantic precision is averaged with syntactic accuracy
do we squeeze into second place (?macro Prec?).
To get a more detailed picture of the strengths
and weaknesses of our system, we computed its rank
within each dataset, shown in table 5. Overall, our
system is robust across languages, with little fluc-
tuation in ranking for the overall score, including
for out-of-domain data. The one noticeable excep-
tion to this consistency is the syntactic score for En-
data time (min) macro F1
Czech 25% 5007 73.84
50% 3699 77.57
75% 4201 79.10
100% 6870 80.55
English 25% 1300 79.02
50% 1899 81.61
75% 3196 82.41
100% 3191 83.27
Table 6: Training times and development set accuracies
using different percentages of the training data, for Czech
and English.
glish out-of-domain data. The other ranks for En-
glish out-of-domain and English in-domain scores
are also on the poor side. These results support our
claim that our parser has not undergone much hand-
tuning, since it was originally developed for English.
It is not currently clear whether this relative differ-
ence reflects a English-specific weakness in our sys-
tem, or that many of the other systems have been
fine-tuned for English.
On the higher end of our dataset rankings, we
do relatively well on Catalan, Czech, and Span-
ish. Catalan and Spanish are unique amongst these
datasets in that they have no crossing arcs in their
semantic structure. Czech seems to have semantic
structures which are relatively well handled by our
derivations with Swap. As indicated above in ta-
ble 1, only 2% of sentences are unparsable, despite
16% requiring the Swap action. However, this argu-
ment does not explain why our parser did relatively
poorly on German semantic dependencies. Regard-
less, these observations would suggest that our sys-
tem is still having trouble with crossing dependen-
cies, despite the introduction of the Swap operation,
and that our learning method could achieve better
performance with an improved treatment of cross-
ing semantic dependencies.
Table 6 shows how accuracies and training times
vary with the size of the training dataset, for Czech
and English. Training times vary in part because
40
Rank Ave Cat Chi Cze Eng Ger Jap Spa Cze-ood Eng-ood Ger-ood
semantic Prec 3 81.60 79.08 80.93 87.45 84.92 75.60 83.75 79.44 85.90 72.89 75.19
semantic Rec 3 75.56 75.87 71.73 @84.64 81.63 68.33 71.65 75.05 @84.09 68.55 57.63
macro Prec 2 83.68 83.47 78.52 83.91 86.86 81.44 88.05 83.54 81.16 76.86 @75.98
macro Rec 3 80.66 @81.86 73.92 @82.51 85.21 77.81 81.99 81.35 @80.25 74.70 67.20
Table 4: Semantic precision and recall and macro precision and recall for our system. Rank is within task.
Rank by Ave Cat Chi Cze Eng Ger Jap Spa Ave-ood Cze-ood Eng-ood Ger-ood
macro F1 3 2 3 2 4 4 3 2 3 1 4 3
syntactic Acc 1 1 4 1 3 2 2 1 2 1 7 2
semantic F1 3 2 4 2 4 5 4 2 3 2 4 3
Table 5: Our system?s rank within task according to the three main measures, for each dataset.
?1.4
?1.2
?1
?0.8
?0.6
?0.4
?0.2
 0
 0  10  20  30  40  50
M
ac
ro
 F
1 
D
iff
er
en
ce
Words per Second
Jap
Spa
Cat
Ger
Eng
Cze
Chi
Figure 1: Difference in development set macro F1 as the
search beam is decreased from the submitted beam (80)
to 40, 20, 10, and 5, plotted against parser speed.
random variations can result in different numbers of
training cycles before convergence. Accuracies ap-
pear to be roughly log-linear with data size.
Figure 1 shows how the accuracy of the parser de-
grades as we speed it up by decreasing the search
beam used in decoding, for each language. For some
languages, a slightly smaller search beam is actually
more accurate,2 but for smaller beams the trade-off
of accuracy versus words-per-second is roughly lin-
ear. Parsing time per word is also linear in beam
width, with a zero intercept.
5 Conclusion
In the joint task of the closed challenge of the
CoNLL 2009 shared task (Hajic? et al, 2009), we in-
vestigated how well a model of syntactic-semantic
dependency parsing developed for English would
2This fact suggests that we could have gotten improved re-
sults by tailoring the search beam to individual languages.
generalise to the other six languages. This model
provides a single generative probability of the joint
syntactic and semantic dependency structures, but
allows separate representations for these two struc-
tures by parsing the two structures synchronously.
Finding the statistical correlations both between and
within these structures is facilitated through the use
of latent variables, which induce features automat-
ically from the data, thereby greatly reducing the
need for hand-coded feature engineering.
This latent variable model proved very robust
across languages, achieving a ranking of between
second and fourth on each language, including for
out-of-domain data. The extent to which the parser
does not rely on hand-crafting is underlined by the
fact that its worst ranking is for English, the lan-
guage for which it was developed (particularly for
out-of-domain data). The parser was ranked third
overall out of 14 systems, with a macro averaged F1
score of 82.14%, only 0.5% worse than the best sys-
tem.
Both joint learning and conditioning decisions
about semantic dependencies on latent representa-
tions of syntactic parsing states were crucial to the
success of our model, as was previously demon-
strated in Henderson et al (2008). There, remov-
ing this conditioning led to a 3.5% drop in the SRL
score. This result seems to contradict the gen-
eral trend in the CoNLL-2008 shared task, where
joint learning had only limited success. The lat-
ter fact may be explained by recent theoretical re-
sults demonstrating that pipelines can be preferable
to joint learning (Roth et al, 2009) when no shared
hidden representation is learnt. Our system (Hender-
son et al, 2008) was the only one which attempted to
41
learn a common hidden representation for this mul-
titask learning problem and also was the only one
which achieved significant gain from joint parameter
estimation. We believe that learning shared hidden
representations for related NLP problems is a very
promising direction for further research.
Acknowledgements
We thank Gabriele Musillo and Dan Roth for help and
advice. This work was partly funded by Swiss NSF
grants 100015-122643 and PBGE22-119276, European
Community FP7 grant 216594 (CLASSiC, www.classic-
project.org), US NSF grant SoD-HCER-0613885 and
DARPA (Bootstrap Learning Program).
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic dependen-
cies. In Proceedings of CONLL 2008, pages 178?182,
Manchester, UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Rebecca Nesson, Giorgio Satta, and Stuart M. Shieber.
2008. Optimal k-arization of synchronous tree-
adjoining grammar. In Proceedings of ACL-08: HLT,
pages 604?612, Columbus, Ohio, June.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. 43rd Meeting of Asso-
ciation for Computational Linguistics, pages 99?106,
Ann Arbor, MI.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Pseudo-projective de-
pendency parsing with support vector machines. In
Proc. of the Tenth Conference on Computational Nat-
ural Language Learning, pages 221?225, New York,
USA.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Dan Roth, Kevin Small, and Ivan Titov. 2009. Sequential
learning of classifiers for structured prediction prob-
lems. In AISTATS, Clearwater, Florida, USA.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Ivan Titov and James Henderson. 2007a. Constituent
parsing with Incremental Sigmoid Belief Networks. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 632?639,
Prague, Czech Republic.
Ivan Titov and James Henderson. 2007b. Fast and ro-
bust multilingual dependency parsing with a genera-
tive latent variable model. In Proc. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), Prague, Czech Republic. (CoNLL
Shared Task).
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proc. Twenty-First International Joint Confer-
ence on Artificial Intelligence (IJCAI-09), Pasadena,
California.
42
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 899?908,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Heuristic Search for Non-Bottom-Up Tree Structure Prediction
Andrea Gesmundo
Department of Computer Science
University of Geneva
andrea.gesmundo@unige.ch
James Henderson
Department of Computer Science
University of Geneva
james.henderson@unige.ch
Abstract
State of the art Tree Structures Prediction
techniques rely on bottom-up decoding. These
approaches allow the use of context-free fea-
tures and bottom-up features. We discuss
the limitations of mainstream techniques in
solving common Natural Language Process-
ing tasks. Then we devise a new framework
that goes beyond Bottom-up Decoding, and
that allows a better integration of contextual
features. Furthermore we design a system that
addresses these issues and we test it on Hierar-
chical Machine Translation, a well known tree
structure prediction problem. The structure
of the proposed system allows the incorpora-
tion of non-bottom-up features and relies on
a more sophisticated decoding approach. We
show that the proposed approach can find bet-
ter translations using a smaller portion of the
search space.
1 Introduction
Tree Structure Prediction (TSP) techniques have
become relevant in many Natural Language Pro-
cessing (NLP) applications, such as Syntactic Pars-
ing, Semantic Role Labeling and Hierarchical Ma-
chine Translation (HMT) (Chiang, 2007). HMT
approaches have a higher complexity than Phrase-
Based Machine Translation techniques, but exploit
a more sophisticated reordering model, and can
produce translations with higher Syntactic-Semantic
quality.
TSP requires as inputs: a weighted grammar, G,
and a sequence of symbols or a set of sequences en-
coded as a Lattice (Chappelier et al, 1999). The
input sequence is often a sentence for NLP applica-
tions. Tree structures generating the input sequence
can be composed using rules, r, from the weighted
grammar, G. TSP techniques return as output a tree
structure or a set of trees (forest) that generate the
input string or lattice. The output forest can be rep-
resented compactly as a weighted hypergraph (Klein
and Manning, 2001). TSP tasks require finding the
tree, t, with the highest score, or the best-k such
trees. Mainstream TSP relies on Bottom-up Decod-
ing (BD) techniques.
With this paper we propose a new framework
as a generalization of the CKY-like Bottom-up ap-
proach. We also design and test an instantiation of
this framework, empirically showing that wider con-
textual information leads to higher accuracy for TSP
tasks that rely on non-local features, like HMT.
2 Beyond Bottom-up Decoding
TSP decoding requires scoring candidate trees,
cost(t). Some TSP tasks require only local features.
For these cases cost(t) depends only on the local
score of the rules that compose t :
cost(t) =
?
ri?t
cost(ri ) (1)
This is the case for Context Free Grammars. More
complex tasks need non-local features. Those fea-
tures can be represented by a non-local factor,
nonLocal(t), into the overall t score:
cost(t) =
?
ri?t
cost(ri ) + nonLocal(t) (2)
899
For example, in HMT the Language Model (LM) is
a non-local fundamental feature that approximates
the adequacy of the translation with the sum of log-
probabilities of composing n-grams.
CKY-like BD approaches build candidate trees in
a bottom-up fashion, allowing the use of Dynamic
Programming techniques to simplify the search
space by mering sub-trees with the same state, and
also easing application of pruning techniques (such
as Cube Pruning, e.g. Chiang (2007), Gesmundo
(2010)). For clarity of presentation and follow-
ing HMT practice, we will henceforth restrict our
focus to binary grammars. Standard CKY works
by building objects known as items (Hopkins and
Langmead, 2009). Each item, ?, corresponds to a
candidate sub-tree. Items are built linking a rule
instantiation, r, to two sub-items that represents
left context, ?1, and right context, ?2; formally:
? ? ? ?1 ? r ? ?2 ?. An item is a triple that
contains a span, a postcondition and a carry. The
span contains the indexes of the starting and end-
ing input words delimiting the continuous sequence
covered by the sub-tree represented by the item. The
postcondition is a string that represents r?s head non-
terminal label, telling us which rules may be applied.
The carry, ?, stores extra information required to
correctly score the non-local interactions of the item
when it will be linked in a broader context (for HMT
with LM the carry consists of boundary words that
will form new n-grams).
Items, ? ? ??1 ? r ? ?2?, are scored according to
the following formula:
cost(?) = cost(r) + cost(?1) + cost(?2) (3)
+ interaction(r, ?1, ?2)
Where: cost(r) is the cost associated to the weighted
rule r; cost(?1) and cost(?2) are the costs of the two
sub-items computed recursively using formula (3);
interaction(r, ?1, ?2) is the interaction cost between
the rule instantiation and the two sub-items. In HMT
the interaction cost includes the LM score of new n-
grams generated by connecting the childrens? sub-
spans with terminals of r. Notice that formula (3) is
equal to formula (2) for items that cover the whole
input sequence.
In many TSP applications, the search space is
too large to allow an exhaustive search and there-
fore pruning techniques must be used. Pruning deci-
sions are based on the score of partial derivations.
It is not always possible to compute exactly non-
local features while computing the score of partial
derivations, since partial derivations miss part of the
context. Formula (3) accounts for the interaction be-
tween r and sub-items ?1 and ?2, but it does not in-
tegrate the cost relative to the interaction between
the item and the surrounding context. Therefore the
item score computed in a bottom-up fashion is an
approximation of the score the item has in a broader
context. For example, in HMT the LM score for n-
grams that partially overlap the item?s span cannot
be computed exactly since the surrounding words
are not known.
Basing pruning decisions on approximated scores
can introduce search errors. It is possible to reduce
search errors using heuristics based on future cost
estimation. In general the estimation of the interac-
tion between ? and the surrounding context is func-
tion of the carry, ?. In HMT it is possible to estimate
the cost of n-grams that partially overlap ??s span
considering the boundary words. We can obtain the
heuristic cost for an item, ?, adding to formula (3)
the factor, est(?), for the estimation of interaction
with missing context:
heuristicCost(?) = cost(?) + est(?) (4)
And use heuristicCost(?) to guide BD pruning de-
cisions. Anyway, even if a good interaction estima-
tion is available, in practice it is not possible to avoid
search errors while pruning.
More sophisticated parsing models allow the use
of non-bottom-up features within a BD framework.
Caraballo and Charniak (1998) present best-first
parsing with Figures of Merit that allows condition-
ing of the heuristic function on statistics of the input
string. Corazza et al (1994), and Klein and Man-
ning (2003) propose an A* parsing algorithm that
estimates the upper bound of the parse completion
scores using contextual summary features. These
models achieve time efficiency and state-of-the-art
accuracy for PCFG parsing, but still use a BD frame-
work that doesn?t allow the application of a broader
class of non-bottom-up contextual features.
In HMT, knowing the sentence-wide context in
which a sub-phrase is translated is extremely impor-
tant. It is obviously important for word choice: as
900
a simple example consider the translation of the fre-
quent English word ?get? into Chinese. The choice
of the correct set of ideograms to translate ?get? of-
ten requires being aware of the presence of particles
that can be at any distance within the sentence. In a
common English to Chinese dictionary we found 93
different sets of ideograms that could be translations
of ?get?. Sentence-wide context is also important
in the choice of word re-ordering: as an example
consider the following translations from English to
German:
1. EN : I go home.
DE : Ich gehe nach Hause.
2. EN : I say, that I go home.
DE : Ich sage, dass ich nach Hause gehe.
3. EN : On Sunday I go home.
DE : Am Sonntag gehe ich nach Hause.
The English phrase ?I go home? is translated in Ger-
man using the same set of words but with different
orderings. It is not possible to choose the correct
ordering of the phrase without being aware of the
context. Thus a bottom-up decoder without context
needs to build all translations for ?I go home?, intro-
ducing the possibility of pruning errors.
Having shown the importance of contextual fea-
tures, we define a framework that overcomes the
limitations of bottom-up feature approximation.
3 Undirected-CKY Framework
Our aim is to propose a new Framework that over-
comes BD limitations allowing a better integration
of contextual features. The presented framework can
be regarded as a generalization of CKY.
To introduce the new framework let us focus on a
detail of CKY BD. The items are created and scored
in topological order. The ordering constraint can be
formally stated as: an item covering the span [i, j]
must be processed after items covering sub spans
[h, k]|h ? i, k ? j. This ordering constraint im-
plies that full yield information is available when
an item is processed, but information about ances-
tors and siblings is missing. Therefore non-bottom-
up context cannot be used because of the ordering
constraint. Now let us investigate how the decoding
algorithm could change if we remove the ordering
constraint.
Removing the ordering constraint would lead to
the occurrence of cases in which an item is pro-
cessed before all child items have been processed.
For example, we could imagine to create and score
an item, ?, with postcondition X and span [i, j], link-
ing the rule instantiation r : X?AB with only
the left sub-item, ?A, while information for the right
sub-item, ?B is still missing. In this case, we can
rely on local and partial contextual features to score
?. Afterwards, it is possible to process ?B using the
parent item, ?, as a source of additional informa-
tion about the parent context and sibling ?A yield.
This approach can avoid search errors in cases where
pruning at the parent level can be correctly done us-
ing only local and partial yield context, while prun-
ing at the child level needs extra non-bottom-up con-
text to make a better pruning decision. For exam-
ple, consider the translation of the English sentence
?I run? into French using the following synchronous
grammar:
r1 : S ? X 1 X 2 | X 1 X 2
r2 : X ? I | Je
r3 : X ? run | course
r4 : X ? run | courir
r5 : X ? run | cours
r6 : X ? run | courons
.
.
.
Where: r1 is a Glue rule and boxed indexes de-
scribe the alignment; r2 translates ?I? in the cor-
responding French pronoun; r3 translates ?run? as
a noun; remaining rules translate ?run? as one of
the possible conjugations of the verb ?courir?. Us-
ing only bottom-up features it is not possible to re-
solve the ambiguity of the word ?run?. If the beam
is not big enough the correct translation could be
pruned. Anyway a CKY decoder would give the
highest score to the most frequent translation. In-
stead, if we follow a non bottom-up approach, as
described in Figure 1, we can: 1) first translate ?I?;
2) Then create an item using r1 with missing right
child; 3) finally choose the correct translation for
?run? using r1 to access a wider context. Notice
that with this undirected approach it is possible to
reach the correct translation using only beam size of
901
Figure 1: Example of undirected decoding for HMT. The arrows point to the direction in which information is propa-
gated. Notice that the parent link at step 3 is fundamental to correctly disambiguate the translation for ?run?.
1 and the LM feature.
To formalize Undirected-CKY, we define a gen-
eralized item called undirected-item. Undirected-
items, ??, are built linking rule instantiations with
elements in L ? {left child, right child, parent};
for example: ?? ? ???1 ? r ?? ??p?, is built linking
r with left child, ??1, and parent, ??p. We denote
with L+?? the set of links for which the undirected-
item, ??, has a connection, and with L??? the set
of missing links. An undirected-item is a triple
that contains a span, a carry and an undirected-
postcondition. The undirected-postcondition is a
set of strings, one string for each of ???s missing links,
l ? L??? . Each string represents the non-terminal re-
lated to one of the missing links available for expan-
sion. Bottom-up items can be considered specific
cases of undirected-items having L+ = { left child,
right child} and L? = {parent}. We can formally
describe the steps of the example depicted in Figure
1 with:
1) r2 : X ? I|Je , terminal : [0, 1]??1 : [0, 1, {X p }, ?1]
2)
r1 : S ? X 1X 2 |X 1X 2 , ??1 : [0, 1, {X p }, ?1]
??2 : [0, 1, {X 2 }, ?2)]
3)r5 :X ? run|cours,? ?2 : [? ? ? ] , terminal : [1, 2]??3 : [0, 2, {}, ?3 ]
The scoring function for undirected-items can be ob-
tained generalizing formula (3):
cost(??) = cost(r)
+
?
l?L+
cost(??l ) (5)
+ interaction(r ,L+)
In CKY, each span is processed separately in
topological order, and the best-k items for each span
are selected in sequence according to scoring func-
tion (4). In the proposed framework, the selec-
tion of undirected-items can be done in any order,
for example: in a first step selecting an undirected-
item for span s1, then selecting an undirected-item
for span s2, and in a third step selecting a second
undirected-item for s1, and so on. As in agenda
based parsing (Klein and Manning, 2001), all candi-
date undirected-items can be handled with an unique
queue. Allowing the system to decide decoding or-
der based on the candidates? scores, so that candi-
dates with higher confidence can be selected earlier
and used as context for candidates with lower confi-
dence.
Having all candidates in the same queue intro-
duces comparability issues. In CKY, candidates are
comparable since each span is processed separately
and each candidate is scored with the estimation of
the yield score. Instead, in the proposed framework,
902
the unique queue contains candidates relative to dif-
ferent nodes and with different context scope. To
ensure comparability, we can associate to candidate
undirected-items a heuristic score of the full deriva-
tion:
heuristicCost(??) = cost(??) + est(??) (6)
Where est(??) estimates the cost of the missing
branches of the derivation as a function of ???s par-
tial structure and carry.
In this framework, the queue can be initialized
with a candidate for each rule instantiation. These
initializing candidates have no context information
and can be scored using only local features. A
generic decoding algorithm can loop selecting the
candidate undirected-item with the highest score, ??,
and then propagating its information to neighboring
candidates, which can update using ?? as context. In
this general framework the link to the parent node is
not treated differently from links to children. While
in CKY the information is always passed from chil-
dren to parent, in Undirected-CKY the information
can be propagated in any direction, and any decod-
ing order is allowed.
We can summarize the steps done to generalize
CKY into the proposed framework: 1) remove the
node ordering constraint; 2) define the scoring of
candidates with missing children or parent; 3) use
a single candidate queue; 4) handle comparability of
candidates from different nodes and/or with differ-
ent context scope; 5) allow information propagation
in any direction.
4 Undirected Decoding
In this section we propose Undirected Decod-
ing (UD), an instantiation of the Undirected-CKY
framework presented above. The generic framework
introduces many new degrees of freedom that could
lead to a higher complexity of the decoder. In our
actual instantiation we apply constraints on the ini-
tialization step, on the propagation policy, and fix a
search beam of k. These constraints allow the sys-
tem to converge to a solution in practical time, al-
low the use of dynamic programming techniques to
merge items with equivalent states, and gives us the
possibility of using non-bottom-up features and test-
ing their relevance.
Algorithm 1 Undirected Decoding
1: function decoder (k) : out-forest
2: Q? LeafRules();
3: while |Q| > 0 do
4: ??? PopBest (Q);
5: if CanPop(??) then
6: out-forest.Add(??);
7: if ??.HasChildrenLinks() then
8: for all r ? HeadRules(??) do
9: C?? NewUndirectedItems(r ,? ?);
10: for all c? ? C? do
11: if CanPop(c?) then
12: Q.Insert(c?);
13: end if
14: end for
15: end for
16: end if
17: end if
18: end while
Algorithm 1 summarizes the UD approach. The
beam size, k, is given as input. At line 2 the queue
of undirected-item candidates, Q, is initialized with
only leafs rules. At line 3 the loop starts, it will
terminate when Q is empty. At line 4 the candi-
date with highest score, ??, is popped from Q. line 5
checks if ?? is within the beam width: if ?? has a span
for which k candidates were already popped, then ??
is dropped and a new iteration is begun. Otherwise
?? is added to the out-forest at line 6. From line 7
to line 10 the algorithm deals with the generation of
new candidate undirected-items. line 7 checks if ??
has both children links, if not a new decoding iter-
ation is begun. line 8 loops over the rule instantia-
tions, r, that can use ?? as child. At line 9, the set of
new candidates, C?, is built linking r with ?? and any
context already available in the out-forest. Finally,
between line 10 and line 12, each element c? in C?
is inserted in Q after checking that c? is within the
beam width: if c? has a span for which k candidates
were already popped it doesn?t make sense to insert
it in Q since it will be surely discarded at line 5.
In more detail, the function
NewUndirectedItems(r,? ?) at line 9 creates new
undirected-items linking r using: 1) ?? as child; 2)
(optionally) as other child any other undirected-item
that has already been inserted in the out-forest and
903
doesn?t have a missing child and matches missing
span coverage; 3) and using as parent context the
best undirected-item with missing child link that
has been incorporated in the out-forest and can
expand the missing child link using r. In our current
method, only the best possible parent context is
used because it only provides context for ranking
candidates, as discussed at the end of this section.
In contrast, a different candidate is generated for
each possible other child in 2), as well as for
the case where no other child is included in the
undirected-item.
We can make some general observations on the
Undirected Decoding Algorithm. Notice that, the
if statement at line 7 and the way new undirected-
items are created at line 9, enforce that each
undirected-item covers a contiguous span. An
undirected-item that is missing a child link cannot
be used as child context but can be used as parent
context since it is added to the out-forest at line 6
before the if statement at line 7. Furthermore, the
if statements at line 5 and line 11 check that no
more than k candidates are selected for each span,
but the algorithm does not require the the selection
of exactly k candidates per span as in CKY.
The queue of candidates, Q, is ordered according
to the heuristic cost of formula (6). The score of the
candidate partial structure is accounted for with fac-
tor cost(??) computed according to formula (5). The
factor est(??) accounts for the estimation of the miss-
ing part of the derivation. We compute this factor
with the following formula:
est(??) =
?
l?L???
(
localCost(??, l) + contextEst(??, l)
)
(7)
For each missing link, l ? L??? , we estimate the cost
of the corresponding derivation branch with two fac-
tors: localCost(??, l) that computes the context-free
score of the branch with highest score that could
be attached to l; and contextEst(??, l) that estimates
the contextual score of the branch and its interac-
tion with ??. Because our model is implemented in
the Forest Rescoring framework (e.g. Huang and
Chiang (2007), Dyer et al (2010), Li et al (2009)),
localCost(??, l) can be efficiently computed exactly.
In HMT it is possible to exhaustively represent and
search the context-free-forest (ignoring the LM),
which is done in the Forest Rescoring framework be-
fore our task of decoding with the LM. We exploit
this context-free-forest to compute localCost(??, l):
for missing child links the localCost(?) is the In-
side score computed using the (max, +) semiring
(also known as the Viterbi score), and for missing
parent links the localCost(?) is the corresponding
Outside score. The factor contextEst(?) estimates
the LM score of the words generated by the missing
branch and their interaction with the span covered
by ??. To compute the expected interaction cost we
use the boundary words information contained in ???s
carry as done in BD. To estimate the LM cost of the
missing branch we use an estimation function, con-
ditioned on the missing span length, whose parame-
ters are tuned on held-out data with gradient descent,
using the search score as objective function.
To show that UD leads to better results than BD,
the two algorithms are compared in the same search
space. Therefore we ensure that candidates em-
bedded in the UD out-forest would have the same
score if they were scored from BD. We don?t need
to worry about differences derived from the missing
context estimation factor, est(?), since this factor is
only considered while sorting the queue, Q, accord-
ing to the heuristicCost(?). Also, we don?t have to
worry about candidates that are scored with no miss-
ing child and no parent link, because in that case
scoring function (3) for BD is equivalent to scoring
function (5) for UD. Instead, for candidates that are
scored with parent link, we remove the parent link
factor from the cost(?) function when inserting the
candidate into the out forest. And for the candi-
dates that are scored with a missing child, we ad-
just the score once the link to the missing child is
created in the out-forest. In this way UD and BD
score the same derivation with the same score and
can be regarded as two ways to explore the same
search space.
5 Experiments
In this section we test the algorithm presented, and
empirically show that it produces better translations
searching a smaller portion of the search space.
We implemented UD on top of a widely-used
HMT open-source system, cdec (Dyer et al, 2010).
We compare with cdec Cube Pruning BD. The ex-
904
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 2  4  6  8  10  12  14  16
Te
st
 S
et
Beam Size
UD best score
BD best score
Figure 2: Comparison of the quality of the translations.
periments are executed on the NIST MT03 Chinese-
English parallel corpus. The training corpus con-
tains 239k sentence pairs with 6.9M Chinese words
and 8.9M English words. We use a hierarchical
phrase-based translation grammar extracted using a
suffix array rule extractor (Lopez, 2007). The NIST-
03 test set is used for decoding, it has 919 sentence
pairs. The experiments can be reproduced on an
average desktop computer. Since we compare two
different decoding strategies that rely on the same
training technique, the evaluation is primarily based
on search errors rather than on BLEU. We compare
the two systems on a variety of beam sizes between
1 and 16.
Figure 2 reports a comparison of the translation
quality for the two systems in relation to the beam
size. The blue area represents the portion of sen-
tences for which UD found a better translation. The
white area represents the portion of sentences for
which the two systems found a translation with the
same search score. With beam 1 the two systems ob-
viously have a similar behavior, since both the sys-
tems stop investigating the candidates for a node af-
ter having selected the best candidate immediately
available. For beams 2-4, UD has a clear advan-
tage. In this range UD finds a better translation for
two thirds of the sentences. With beam 4, we ob-
serve that UD is able to find a better translation for
63.76% of the sentences, instead BD is able to find a
better translation for only 21.54% of the sentences.
For searches that employ a beam bigger than 8, we
notice that the UD advantage slightly decreases, and
-126.5
-126
-125.5
-125
-124.5
-124
-123.5
-123
 2  4  6  8  10  12  14  16
Se
ar
ch
 S
co
re
Beam Size
Bottom-up Decoding
Guided Decoding
Figure 3: Search score evolution for BD and UD.
the number of sentences with equivalent translation
slowly increases. We can understand this behavior
considering that as the beam increases the two sys-
tems get closer to exhaustive search. Anyway with
this experiment UD shows a consistent accuracy ad-
vantage over BD.
Figure 3 plots the search score variation for dif-
ferent beam sizes. We can see that UD search leads
to an average search score that is consistently bet-
ter than the one computed for BD. Undirected De-
coding improves the average search score by 0.411
for beam 16. The search score is the logarithm of
a probability. This variation corresponds to a rel-
ative gain of 50.83% in terms of probability. For
beams greater than 8 we see that the two curves keep
a monotonic ascendant behavior while converging to
exhaustive search.
Figure 4 shows the BLEU score variation. Again
we can see the consistent improvement of UD over
BD. In the graph we report also the performance ob-
tained using BD with beam 32. BD reaches BLEU
score of 32.07 with beam 32 while UD reaches
32.38 with beam 16: UD reaches a clearly higher
BLEU score using half the beam size. The differ-
ence is even more impressive if we consider that UD
reaches a BLEU of 32.19 with beam 4.
In Figure 5 we plot the percentage reduction of the
size of the hypergraphs generated by UD compared
to those generated by BD. The size reduction grows
quickly for both nodes and edges. This is due to the
fact that BD, using Cube Pruning, must select k can-
didates for each node. Instead, UD is not obliged to
905
 31.2
 31.4
 31.6
 31.8
 32
 32.2
 32.4
 2  4  6  8  10  12  14  16
BL
EU
 S
co
re
Beam Size
Bottom-up Decoding
Undirected Decoding
BD beam 30
Figure 4: BLEU score evolution for BD and UD.
 5
 10
 15
 20
 25
 30
 35
 40
 2  4  6  8  10  12  14  16
R
ed
uc
tio
n 
(%
)
Beam Size
Nodes Reduction
Edges Reduction
Figure 5: Percentage of reduction of the size of the hy-
pergraph produced by UD.
select k candidates per f -node. As we can see from
Algorithm 1, the decoding loop terminates when the
queue of candidates is empty, and the statements at
line 5 and line 11 ensure that no more than k can-
didates are selected per f -node, but nothing requires
the selection of k elements, and some bad candidates
may not be generated due to the sophisticated prop-
agation strategy. The number of derivations that a
hypergraph represents is exponential in the number
of nodes and edges composing the structure. With
beam 16, the hypergraphs produced by UD contain
on average 4.6k fewer translations. Therefore UD
is able to find better translations even if exploring a
smaller portion of the search space.
Figure 6 reports the time comparison between
BD and UD with respect to sentence length. The
 0
 200
 400
 600
 800
 1000
 1200
 10  15  20  25  30  35  40  45  50
Ti
m
e 
(m
s)
Input Sentence Size
Bottom-up Decoding, beam = 16
Undirected Decoding, beam = 8
Figure 6: Time comparison between BD and UD.
sentence length is measured with the number of
ideogram groups appearing in the source Chinese
sentences. We compare BD with beam of 16 and
UD with beam of 8, so that we compare two sys-
tems with comparable search score. We can notice
that for short sentences UD is faster, while for longer
sentences UD becomes slower. To understand this
result consider that for simple sentences UD can
rely on the advantage of exploring a smaller search
space. While, for longer sentences, the amount of
candidates considered during decoding grows ex-
ponentially with the size of the sentence, and UD
needs to maintain an unique queue whose size is not
bounded by the beam size k, as for the queues used
in BD?s Cube Pruning. It may be possible to address
this issue with more efficient handling of the queue.
In conclusion we can assert that, even if explor-
ing a smaller portion of the search space, UD finds
often a translation that is better than the one found
with standard BD. UD?s higher accuracy is due to
its sophisticated search strategy that allows a more
efficient integration of contextual features. This set
of experiments show the validity of the UD approach
and empirically confirm our intuition about the BD?s
inadequacy in solving tasks that rely on fundamental
contextual features.
6 Future Work
In the proposed framework the link to the parent
node is not treated differently from links to child
nodes, the information in the hypergraph can be
propagated in any direction. Then the Derivation
906
Hypergraph can be regarded as a non-directed graph.
In this setting we could imagine applying mes-
sage passing algorithms from graphical model the-
ory (Koller and Friedman, 2010).
Furthermore, considering that the proposed
framework lets the system decide the decoding or-
der, we could design a system that explicitly learns
to infer the decoding order at training time. Sim-
ilar ideas have been successfully tried: Shen et al
(2010) and Gesmundo (2011) investigate the Guided
Learning framework, that dynamically incorporates
the tasks of learning the order of inference and train-
ing the local classifier.
7 Conclusion
With this paper we investigate the limitations of
Bottom-up parsing techniques, widely used in Tree
Structures Prediction, focusing on Hierarchical Ma-
chine Translation. We devise a framework that al-
lows a better integration of non-bottom-up features.
Compared to a state of the art HMT decoder the pre-
sented system produces higher quality translations
searching a smaller portion of the search space, em-
pirically showing that the bottom-up approximation
of contextual features is a limitation for NLP tasks
like HMT.
Acknowledgments
This work was partly funded by Swiss NSF grant
CRSI22 127510 and European Community FP7
grant 216594 (CLASSiC, www.classic-project.org).
References
Sharon A. Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing, Computational Linguistics, 24:275-298.
J. C. Chappelier and M. Rajman and R. Arages and A.
Rozenknop. 1999. Lattice Parsing for Speech Recog-
nition. In Proceedings of TALN 1999, Cargse, France.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201-228,
2007.
Anna Corazza, Renato De Mori, Roberto Gretter and
Giorgio Satta. 1994. Optimal Probabilistic Evalu-
ation Functions for Search Controlled by Stochastic
Context-Free Grammars. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 16(10):1018-
1027.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec:
A Decoder, Alignment, and Learning framework for
finite-state and context-free translation models. In
Proceedings of the Conference of the Association of
Computational Linguistics 2010, Uppsala, Sweden.
Andrea Gesmundo and James Henderson 2010. Faster
Cube Pruning. Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), Paris, France.
Andrea Gesmundo 2011. Bidirectional Sequence Classi-
fication for Tagging Tasks with Guided Learning. Pro-
ceedings of TALN 2011, Montpellier, France.
Mark Hopkins and Greg Langmead 2009. Cube prun-
ing as heuristic search. Proceedings of the Conference
on Empirical Methods in Natural Language Processing
2009, Singapore.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the Conference of the Association of
Computational Linguistics 2007, Prague, Czech Re-
public.
Dan Klein and Christopher D. Manning. 2001 Pars-
ing and Hypergraphs, In Proceedings of the Interna-
tional Workshop on Parsing Technologies 2001, Bei-
jing, China.
Dan Klein and Christopher D. Manning. 2003 A* Pars-
ing: Fast Exact Viterbi Parse Selection, In Proceed-
ings of the Conference of the North American Associ-
ation for Computational Linguistics 2003, Edmonton,
Canada.
Daphne Koller and Nir Friedman. 2010. Probabilistic
Graphical Models: Principles and Techniques. The
MIT Press, Cambridge, Massachusetts.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient Minimum Error Rate
Training and Minimum Bayes-Risk decoding for
translation hypergraphs and lattices, In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, Suntec,
Singapore.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N. G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An Open Source Toolkit for
Parsing-based Machine Translation. In Proceedings of
the Workshop on Statistical Machine Translation 2009,
Athens, Greece.
Adam Lopez. 2007. Hierarchical Phrase-Based Transla-
tion with Suffix Arrays. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing 2007, Prague, Czech Republic.
907
Haitao Mi, Liang Huang and Qun Liu. 2008. Forest-
Based Translation. In Proceedings of the Conference
of the Association of Computational Linguistics 2008,
Columbus, OH.
Libin Shen, Giorgio Satta and Aravind Joshi. 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of the Conference of the As-
sociation of Computational Linguistics 2007, Prague,
Czech Republic.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing
2002, Denver, CO.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing,
Proceedings of the Workshop on Statistical Machine
Translation, New York City, New York.
908
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 97?101,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
HadoopPerceptron: a Toolkit for Distributed Perceptron Training and
Prediction with MapReduce
Andrea Gesmundo
Computer Science Department
University of Geneva
Geneva, Switzerland
andrea.gesmundo@unige.ch
Nadi Tomeh
LIMSI-CNRS and
Universite? Paris-Sud
Orsay, France
nadi.tomeh@limsi.fr
Abstract
We propose a set of open-source software
modules to perform structured Perceptron
Training, Prediction and Evaluation within
the Hadoop framework. Apache Hadoop
is a freely available environment for run-
ning distributed applications on a com-
puter cluster. The software is designed
within the Map-Reduce paradigm. Thanks
to distributed computing, the proposed soft-
ware reduces substantially execution times
while handling huge data-sets. The dis-
tributed Perceptron training algorithm pre-
serves convergence properties, thus guar-
anties same accuracy performances as the
serial Perceptron. The presented modules
can be executed as stand-alone software or
easily extended or integrated in complex
systems. The execution of the modules ap-
plied to specific NLP tasks can be demon-
strated and tested via an interactive web in-
terface that allows the user to inspect the
status and structure of the cluster and inter-
act with the MapReduce jobs.
1 Introduction
The Perceptron training algorithm (Rosenblatt,
1958; Freund and Schapire, 1999; Collins, 2002)
is widely applied in the Natural Language Pro-
cessing community for learning complex struc-
tured models. The non-probabilistic nature of the
perceptron parameters makes it possible to incor-
porate arbitrary features without the need to cal-
culate a partition function, which is required for
its discriminative probabilistic counterparts such
as CRFs (Lafferty et al 2001). Additionally, the
Perceptron is robust to approximate inference in
large search spaces.
Nevertheless, Perceptron training is propor-
tional to inference which is frequently non-linear
in the input sequence size. Therefore, training can
be time-consuming for complex model structures.
Furthermore, for an increasing number of tasks is
fundamental to leverage on huge sources of data
as the World Wide Web. Such difficulties render
the scalability of the Perceptron a challenge.
In order to improve scalability, Mcdonald et
al. (2010) propose a distributed training strat-
egy called iterative parameter mixing, and show
that it has similar convergence properties to the
standard perceptron algorithm; it finds a separat-
ing hyperplane if the training set is separable; it
produces models with comparable accuracies to
those trained serially on all the data; and reduces
training times significantly by exploiting comput-
ing clusters.
With this paper we present the HadoopPer-
ceptron package. It provides a freely available
open-source implementation of the iterative pa-
rameter mixing algorithm for training the struc-
tured perceptron on a generic sequence labeling
tasks. Furthermore, the package provides two ad-
ditional modules for prediction and evaluation.
The three software modules are designed within
the MapReduce programming model (Dean and
Ghemawat, 2004) and implemented using the
Apache Hadoop distributed programming Frame-
work (White, 2009; Lin and Dyer, 2010). The
presented HadoopPerceptron package reduces ex-
ecution time significantly compared to its serial
counterpart while maintaining comparable perfor-
mance.
97
PerceptronIterParamMix(T = {(xt,yt)}|T |t=1)
1. Split T into S pieces T = {T1, . . . ,TS}
2. w = 0
3. for n : 1..N
4. w(i,n) = OneEpochPerceptron(Ti ,w)
5. w =
?
i ?i,nw(i,n)
6. return w
OneEpochPerceptron(Ti ,w?)
1. w(0) = w?; k = 0
2. for n : 1..T
3. Let y? = argmaxy? w(k).f(xt,y?t)
4. if y? 6= yt
5. x(k+1) = x(k) + f(xt,yt)? f(xt,y?t)
6. k = k + 1
7. return w(k)
Figure 1: Distributed perceptron with iterative param-
eter mixing strategy. Each w(i,n) is computed in par-
allel. ?n = {?1,n, . . . , ?S,n}, ??i,n ? ?n : ?i,n ?
0 and ?n : ?i ?i,n = 1.
2 Distributed Structured Perceptron
The structured perceptron (Collins, 2002) is an
online learning algorithm that processes train-
ing instances one at a time during each training
epoch. In sequence labeling tasks, the algorithm
predicts a sequence of labels (an element from
the structured output space) for each input se-
quence. Prediction is determined by linear opera-
tions on high-dimensional feature representations
of candidate input-output pairs and an associated
weight vector. During training, the parameters are
updated whenever the prediction that employed
them is incorrect.
Unlike many batch learning algorithms that can
easily be distributed through the gradient calcula-
tion, the perceptron online training is more subtle
to parallelize. However, Mcdonald et al(2010)
present a simple distributed training through a pa-
rameter mixing scheme.
The Iterative Parameter Mixing is given in Fig-
ure 2 (Mcdonald et al 2010). First the training
data is divided into disjoint splits of example pairs
(xt,yt) where xt is the observation sequence and
yt is the associated labels. The algorithm pro-
ceeds to train a single epoch of the perceptron
algorithm for each split in parallel, and mix the
local models weights w(i,n) to produce the global
weight vector w. The mixed model is then passed
to each split to reset the perceptron local weights,
and a new iteration is started. Mcdonald et al
(2010) provide bound analysis for the algorithm
and show that it is guaranteed to converge and find
a seperation hyperplane if one exists.
3 MapReduce and Hadoop
Many algorithms need to iterate over number
of records and 1) perform some calculation on
each of them and then 2) aggregate the results.
The MapReduce programming model implements
a functional abstraction of these two operations
called respectively Map and Reduce. The Map
function takes a value-key pairs and produces a
list of key-value pairs: map(k, v) ? (k?, v?)?;
while the input the Reduce function is a key with
all the associated values produced by all the map-
pers: reduce(k?, (v?)?) ? (k??, v??)?. The model
requires that all values with the same key are re-
duced together.
Apache Hadoop is an open-source implementa-
tion of the MapReduce model on cluster of com-
puters. A cluster is composed by a set of comput-
ers (nodes) connected into a network. One node
is designated as the Master while other nodes
are referred to as Worker Nodes. Hadoop is de-
signed to scale out to large clusters built from
commodity hardware and achieves seamless scal-
ability. To allow rapid development, Hadoop
hides system-level details from the application
developer.The MapReduce runtime automatically
schedule worker assignment to mappers and re-
ducers;handles synchronization required by the
programming model including gathering, sort-
ing and shuffling of intermediate data across the
network; and provides robustness by detecting
worker failures and managing restarts. The frame-
work is built on top of he Hadoop Distributed
File System (HDFS), which allows to distribute
the data across the cluster nodes. Network traffic
is minimized by moving the process to the node
storing the data. In Hadoop terminology an entire
MapReduce program is called a job while individ-
ual mappers and reducers are called tasks.
4 HadoopPerceptron Implementation
In this section we give details on how the train-
ing, prediction and evaluation modules are im-
plemented for the Hadoop framework using the
98
Figure 2: HadoopPerceptron in MapReduce.
MapReduce programming model1.
Our implementation of the iterative parame-
ter mixing algorithm is sketched in Figure 2.
At the beginning of each iteration, the train-
ing data is split and distributed to the worker
nodes. The set of training examples in a
data split is streamed to map workers as pairs
(sentence-id, (xt,yt)). Each map worker per-
forms a standard perceptron training epoch and
outputs a pair (feature-id, wi,f ) for each feature.
The set of such pairs emitted by a map worker rep-
resents its local weight vector. After map workers
have finished, the MapReduce framework guaran-
tees that all local weights associated with a given
feature are aggregated together as input to a dis-
tinct reduce worker. Each reduce worker produces
as output the average of the associated feature
weight. At the end of each iteration, the reduce
workers outputs are aggregated into the global av-
eraged weight vector. The algorithm iterates N
times or until convergence is achieved. At the
beginning of each iteration the weight vector of
each distinct model is initialized with the global
averaged weight vector resultant from the previ-
ous iteration. Thus, for all the iterations except
for the first, the global averaged weight vector re-
sultant from the previous iteration needs to be pro-
vided the map workers. In Hadoop it is possible
to pass this information via the Distributed Cache
System.
In addition to the training module, the Hadoop-
Perceptron package provides separate modules
for prediction and evaluation both of them are
designed as MapReduce programs. The evalu-
1The Hadoop Perceptron toolkit is available from
https://github.com/agesmundo/HadoopPerceptron .
ation module output the accuracy measure com-
puted against provided gold standards. Prediction
and evaluation modules are independent from the
training modules, the weight vector given as input
could have been computed with any other system
using any other training algorithm as long as they
employ the same features.
The implementation is in Java, and we inter-
face with the Hadoop cluster via the native Java
API. It can be easily adapted to a wide range of
NLP tasks. Incorporating new features by mod-
ifying the extensible feature extractor is straight-
forward. The package includes the implementa-
tion of the basic feature set described in (Suzuki
and Isozaki, 2008).
5 The Web User Interface
Hadoop is bundled with several web interfaces
that provide concise tracking information for jobs,
tasks, data nodes, etc. as shown in Figure 3. These
web interfaces can be used to demonstrate the
HadoopPerceptron running phases and monitor
the distributed execution of the training, predic-
tion and evaluation modules for several sequence
labeling tasks including part-of-speech tagging
and named entity recognition.
6 Experiments
We investigate HadoopPerceptron training time
and prediction accuracy on a part-of-speech
(POS) task using the PennTreeBank corpus (Mar-
cus et al 1994). We use sections 0-18 of the Wall
Street Journal for training, and sections 22-24 for
testing.
We compare the regular percepton trained se-
rially on all the training data with the distributed
perceptron trained with iterative parameter mix-
ing with variable number of splits S ? {10, 20}.
For each system, we report the prediction accu-
racy measure on the final test set to determine
if any loss is observed as a consequence of dis-
tributed training.
For each system, Figure 4 plots accuracy re-
sults computed at the end of every training epoch
against consumed wall-clock time. We observe
that iterative mixing parameter achieves compa-
rable performance to its serial counterpart while
converging orders of magnitude faster.
Furthermore, we note that the distributed al-
gorithm achieves a slightly higher final accuracy
99
Figure 3: Hadoop interfaces for HadoopPerceptron.
Figure 4: Accuracy vs. training time. Each point cor-
responds to a training epoch.
than serial training. Mcdonald et al(2010) sug-
gest that this is due to the bagging effect that
the distributed training has, and due to parameter
mixing that is similar to the averaged perceptron.
We note also that increasing the number of
splits increases the number of epoch required to
attain convergence, while reducing the time re-
quired per epoch. This implies a trade-off be-
tween slower convergence and quicker epochs
when selecting a larger number of splits.
7 Conclusion
The HadoopPerceptron package provides the first
freely-available open-source implementation of
iterative parameter mixing Perceptron Training,
Prediction and Evaluation for a distributed Map-
Reduce framework. It is a versatile stand alone
software or building block, that can be easily
extended, modified, adapted, and integrated in
broader systems.
HadoopPerceptron is a useful tool for the in-
creasing number of applications that need to per-
form large-scale structured learning. This is the
first freely available implementation of an ap-
proach that has already been applied with success
in private sectors (e.g. Google Inc.). Making it
possible for everybody to fully leverage on huge
data sources as the World Wide Web, and develop
structured learning solutions that can scale keep-
ing feasible execution times and cluster-network
usage to a minimum.
Acknowledgments
This work was funded by Google and The Scot-
tish Informatics and Computer Science Alliance
(SICSA). We thank Keith Hall, Chris Dyer and
Miles Osborne for help and advice.
100
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP ?02:
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, PA, USA.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapre-
duce: simplified data processing on large clusters.
In Proceedings of the 6th Symposium on Opeart-
ing Systems Design and Implementation, San Fran-
cisco, CA, USA.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algo-
rithm. Machine Learning, 37(3):277?296.
John Lafferty, Andrew Mccallum, and Fernando
Pereira. 2001. John lafferty and andrew mc-
callum and fernando pereira. In Proceedings of
the International Conference on Machine Learning,
Williamstown, MA, USA.
Jimmy Lin and Chris Dyer. 2010. Data-Intensive Text
Processing with MapReduce. Morgan & Claypool
Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Ryan Mcdonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In NAACL ?10: Proceedings of the 11th
Conference of the North American Chapter of the
Association for Computational Linguistics, Los An-
geles, CA, USA.
Frank Rosenblatt. 1958. The Perceptron: A proba-
bilistic model for information storage and organiza-
tion in the brain. Psychological Review, 65(6):386?
408.
Jun Suzuki and Hideki Isozaki. 2008. Semi-
supervised sequential labeling and segmentation us-
ing giga-word scale unlabeled data. In ACL ?08:
Proceedings of the 46th Conference of the Associa-
tion for Computational Linguistics, Columbus, OH,
USA.
Tom White. 2009. Hadoop: The Definitive Guide.
O?Reilly Media Inc.
101
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 10?19,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Undirected Machine Translation with
Discriminative Reinforcement Learning
Andrea Gesmundo
Google Inc.
andrea.gesmundo@gmail.com
James Henderson
Xerox Research Centre Europe
james.henderson@xrce.xerox.com
Abstract
We present a novel Undirected Machine
Translation model of Hierarchical MT that
is not constrained to the standard bottom-
up inference order. Removing the order-
ing constraint makes it possible to condi-
tion on top-down structure and surround-
ing context. This allows the introduc-
tion of a new class of contextual features
that are not constrained to condition only
on the bottom-up context. The model
builds translation-derivations efficiently in
a greedy fashion. It is trained to learn
to choose jointly the best action and the
best inference order. Experiments show
that the decoding time is halved and forest-
rescoring is 6 times faster, while reaching
accuracy not significantly different from
state of the art.
1 Introduction
Machine Translation (MT) can be addressed as a
structured prediction task (Brown et al., 1993; Ya-
mada and Knight, 2001; Koehn et al., 2003). MT?s
goal is to learn a mapping function, f , from an in-
put sentence, x, into y = (t, h), where t is the
sentence translated into the target language, and
h is the hidden correspondence structure (Liang
et al., 2006). In Hierarchical MT (HMT) (Chi-
ang, 2005) the hidden correspondence structure is
the synchronous-tree composed by instantiations
of synchronous rules from the input grammar, G.
Statistical models usually define f as: f(x) =
argmax
y?Y
Score(x, y), where Score(x, y) is a
function whose parameters can be learned with a
specialized learning algorithm. In MT applica-
tions, it is not possible to enumerate all y ? Y .
HMT decoding applies pruning (e.g. Cube Prun-
ing (Huang and Chiang, 2005)), but even then
HMT has higher complexity than Phrase Based
MT (PbMT) (Koehn et al., 2003). On the other
hand, HMT improves over PbMT by introducing
the possibility of exploiting a more sophisticated
reordering model not bounded by a window size,
and producing translations with higher syntactic-
semantic quality. In this paper, we present the
Undirected Machine Translation (UMT) frame-
work, which retains the advantages of HMT and
allows the use of a greedy decoder whose com-
plexity is lower than standard quadratic beam-
search PbMT.
UMT?s fast decoding is made possible through
even stronger pruning: the decoder chooses a sin-
gle action at each step, never retracts that action,
and prunes all incompatible alternatives to that ac-
tion. If this extreme level of pruning was ap-
plied to the CKY-like beam-decoding used in stan-
dard HMT, translation quality would be severely
degraded. This is because the bottom-up infer-
ence order imposed by CKY-like beam-decoding
means that all pruning decisions must be based on
a bottom-up approximation of contextual features,
which leads to search errors that affect the qual-
ity of reordering and lexical-choice (Gesmundo
and Henderson, 2011). UMT solves this problem
by removing the bottom-up inference order con-
straint, allowing many different inference orders
for the same tree structure, and learning the in-
ference order where the decoder can be the most
confident in its pruning decisions.
Removing the bottom-up inference order con-
straint makes it possible to condition on top-down
structure and surrounding context. This undirected
approach allows us to integrate contextual features
such as the Language Model (LM) in a more flex-
10
ible way. It also allows us to introduce a new class
of undirected features. In particular, we introduce
the Context-Free Factor (CFF) features. CFF fea-
tures compute exactly and efficiently a bound on
the context-free cost of a partial derivation?s miss-
ing branches, thereby estimating the future cost of
partial derivations. The new class of undirected
features is fundamental for the success of a greedy
approach to HMT, because the additional non-
bottom-up context is sometimes crucial to have the
necessary information to make greedy decisions.
Because UMT prunes all but the single cho-
sen action at each step, both choosing a good in-
ference order and choosing a correct action re-
duce to a single choice of what action to take
next. To learn this decoding policy, we propose
a novel Discriminative Reinforcement Learning
(DRL) framework. DRL is used to train mod-
els that construct incrementally structured out-
put using a local discriminative function, with
the goal of optimizing a global loss function.
We apply DRL to learn the UMT scoring func-
tion?s parameters, using the BLEU score as the
global loss function. DRL learns a weight vector
for a linear classifier that discriminates between
decisions based on which one leads to a com-
plete translation-derivation with a better BLEU
score. Promotions/demotions of translations are
performed by applying a Perceptron-style update
on the sequence of decisions that produced the
translation, thereby training local decisions to op-
timize the global BLEU score of the final trans-
lation, while keeping the efficiency and simplic-
ity of the Perceptron Algorithm (Rosenblatt, 1958;
Collins, 2002).
Our experiments show that UMT with DRL re-
duces decoding time by over half, and the time to
rescore translations with the Language Model by
6 times, while reaching accuracy non-significantly
different from the state of the art.
2 Undirected Machine Translation
In this section, we present the UMT frame-
work. For ease of presentation, and following
synchronous-grammar based MT practice, we will
henceforth restrict our focus to binary grammars
(Zhang et al., 2006; Wang et al., 2007).
A UMT decoder can be formulated as a func-
tion, f , that maps a source sentence, x ? X , into
a structure defined by y = (t, h) ? Y , where t
is the translation in the target language, and h
is the synchronous tree structure generating the
input sentence on the source side and its trans-
lation on the target side. Synchronous-trees are
composed of instantiations of synchronous-rules,
r, from a grammar, G. A UMT decoder builds
synchronous-trees, h, by recursively expanding
partial synchronous-trees, ? . ? includes a partial
translation. Each ? is required to be a connected
sub-graph of some synchronous-tree h. Thus, ?
is composed of a subset of the rules from any h
that generates x on the source side, such that there
is a connected path between any two rules in ? .
Differently from the partial structures built by a
bottom-up decoder, ? does not have to cover a
contiguous span on x. Formally, ? is defined by:
1) The set of synchronous-rule instantiations in ? :
I ? {r
1
, r
2
, ? ? ? , r
k
|r
i
? G, 1 ? i ? k};
2) The set of connections among the synchronous-
rule instantiations, C .
Let c
i
= (r
i
, r
j
i
) be the notation to represent the
connection between the i-th rule and the rule r
j
i
.
The set of connections can be expressed as:
C ? {(r
1
, r
j
1
), (r
2
, r
j
2
), ? ? ? , (r
k?1
, r
j
k?1
)}
3) The postcondition set, P , which specifies
the non-terminals in ? that are available for
creating new connections. Each postcondition,
p
i
= (r
x
,X y )i, indicates that the rule rx has the
non-terminal X y available for connections. The
index y identifies the non-terminal in the rule. In
a binary grammar y can take only 3 values: 1 for
the first non-terminal (the left child of the source
side), 2 for the second non-terminal, and h for the
head. The postcondition set can be expressed as:
P?{(r
x
1
,X
y
1
)
1
, ? ? ? , (r
x
m
,X
y
m
)
m
}
4) The set of carries, K . We define a different
carry, ?
i
, for each non-terminal available for
connections. Each carry stores the extra infor-
mation required to correctly score the non-local
interactions between ? and the rule that will be
connected at that non-terminal. Thus |K| = |P |.
Let ?
i
be the carry associated with the postcon-
dition p
i
. The set of carries can be expressed as:
K ? {?
1
, ?
2
, ? ? ? , ?
m
}
Partial synchronous-trees, ? , are expanded by
performing connection-actions. Given a ? we can
connect to it a new rule, r?, using one available non-
terminal represented by postcondition, p
i
? P ,
and obtain a new partial synchronous-tree ?? . For-
mally: ?? ? ? ? ? a? ?, where, a? = [r?, p
i
],
represents the connection-action.
11
Algorithm 1 UMT Decoding
1: function Decoder (x; w, G) : (t,h)
2: ?.{I, C, P,K} ? {?, ?, ?, ?} ;
3: Q? LeafRules(G);
4: while |Q| > 0 do
5: [r?, p
i
]? PopBestAction (Q,w);
6: ? ? CreateConnection(?, r? , p
i
);
7: UpdateQueue(Q, r?, p
i
);
8: end while
9: Return(?);
10: procedure CreateConnection(? , r?, p
i
) : ??
11: ?? .I ? ?.I + r?;
12: ?? .C ? ?.C + (r?, r
p
i
);
13: ?? .P ? ?.P ? p
i
;
14: ?? .K ? ?.K ? ?
i
;
15: ?? .K .UpdateCarries(r?, p
i
);
16: ?? .P .AddAvailableConnectionsFrom(r? , p
i
);
17: ?? .K .AddCarriesForNewConnections(r? , p
i
);
18: Return(?? );
19: procedure UpdateQueue( Q, r?, p
i
) :
20: Q.RemoveActionsWith(p
i
);
21: Q.AddNewActions(r?, p
i
);
2.1 Decoding Algorithm
Algorithm 1 gives details of the UMT decoding
algorithm. The decoder takes as input the source
sentence, x, the parameters of the scoring func-
tion, w, and the synchronous-grammar, G. At
line 2 the partial synchronous-tree ? is initialized
by setting I , C , P and K to empty sets ?. At
line 3 the queue of candidate connection-actions
is initialized as Q ? { [r
leaf
, null] | r
leaf
is a
leaf rule}, where null means that there is no post-
condition specified, since the first rule does not
need to connect to anything. A leaf rule r
leaf
is
any synchronous rule with only terminals on the
right-hand sides. At line 4 the main loop starts.
Each iteration of the main loop will expand ? us-
ing one connection-action. The loop ends when
Q is empty, implying that ? covers the full sen-
tence and has no more missing branches or par-
ents. The best scoring action according to the
parameter vector w is popped from the queue at
line 5. The scoring of connection-actions is dis-
cussed in details in Section 3.2. At line 6 the se-
lected connection-action is used to expand ? . At
line 7 the queue of candidates is updated accord-
ingly (see lines 19-21). At line 8 the decoder it-
erates the main loop, until ? is complete and is
returned at line 9.
Lines 10-18 describe the CreateConnection(?)
procedure, that connects the partial synchronous-
tree ? to the selected rule r? via the postcondi-
tion p
i
specified by the candidate-action selected
in line 5. This procedure returns the resulting par-
tial synchronous-tree: ?? ? ? ? ? [r?, p
i
] ?. At
line 11, r? is added to the rule set I . At line 12 the
connection between r? and r
p
i
(the rule specified
in the postcondition) is added to the set of connec-
tions C . At line 13, p
i
is removed from P . At
line 14 the carry k
i
matching with p
i
is removed
from K . At line 15 the set of carries K is updated,
in order to update those carries that need to pro-
vide information about the new action. At line 16
new postconditions representing the non-terminals
in r? that are available for subsequent connections
are added in P . At line 17 the carries associated
with these new postconditions are computed and
added to K . Finally at line 18 the updated partial
synchronous-tree is returned.
In the very first iteration, the
CreateConnection(?) procedure has nothing
to compute for some lines. Line 11 is not exe-
cuted since the first leaf rule needs no connection
and has nothing to connect to. lines 12-13 are
not executed since P and K are ? and p
i
is not
specified for the first action. Line 15 is not
executed since there are no carries to be updated.
Lines 16-17 only add the postcondition and carry
relative to the leaf rule head link.
The procedure used to update Q is reported in
lines 19-21. At line 20 all the connection-actions
involving the expansion of p
i
are removed from
Q. These actions are the incompatible alternatives
to the selected action. In the very first iteration,
all actions in Q are removed because they are all
incompatible with the connected-graph constraint.
At line 21 new connection-actions are added to
Q. These are the candidate actions proposing a
connection to the available non-terminals of the
selected action?s new rule r?. The rules used for
these new candidate-actions must not be in con-
flict with the current structure of ? (e.g. the rule
cannot generate a source side terminal that is al-
ready covered by ? ).
12
3 Discriminative Reinforcement
Learning
Training a UMT model simply means training the
parameter vector w that is used to choose the best
scoring action during decoding. We propose a
novel method to apply a kind of minimum error
rate training (MERT) to w. Because each ac-
tion choice must be evaluated in the context of
the complete translation-derivation, we formalize
this method in terms of Reinforcement Learning.
We propose Discriminative Reinforcement Learn-
ing as an appropriate way to train a UMT model to
maximize the BLEU score of the complete deriva-
tion. First we define DRL as a novel generic train-
ing framework.
3.1 Generic Framework of DRL
RL can be applied to any task, T , that can be for-
malized in terms of:
1) The set of states S1;
2) A set of actions A
s
for each state s ? S;
3) The transition function T : S ? A
s
? S, that
specifies the next state given a source state and
performed action2;
4) The reward function, R : S ?A
s
? R;
5) The discount factor, ? ? [0, 1].
A policy is defined as any map ? : S ? A. Its
value function is given by:
V
pi
(s
0
) =
?
?
i=0
?
i
R(s
i
, ?(s
i
)) (1)
where path(s
0
|?)? ?s
0
, s
1
, ? ? ? , s
?
|?? is the se-
quence of states determined by following policy ?
starting at state s
0
. The Q-function is the total fu-
ture reward of performing action a
0
in state s
0
and
then following policy ?:
Q
pi
(s
0
, a
0
) = R(s
0
, a
0
) + ?V
pi
(s
1
) (2)
Standard RL algorithms search for a policy that
maximizes the given reward.
Because we are taking a discriminative ap-
proach to learn w, we formalize our optimization
task similarly to an inverse reinforcement learning
problem (Ng and Russell, 2000): we are given in-
formation about the optimal action sequence and
we want to learn a discriminative reward func-
tion. As in other discriminative approaches, this
1
S can be either finite or infinite.
2For simplicity we describe a deterministic process. To
generalize to the stochastic process, replace the transition
function with the transition probability: P
sa
(s
?
), s
?
? S.
Algorithm 2 Discriminative RL
1: function Trainer (?,T ,D ) : w
2: repeat
3: s?SampleState(S);
4: a?? ?
w
(s);
5: a? ?SampleAction(A
s
);
6: if Qpiw(s, a?) < Qpiw(s, a?) in D then
7: w? w + ?w(s, a?)? ?w(s, a?);
8: end if
9: until convergence
10: Return(w);
approach simplifies the task of learning the re-
ward function in two respects: the learned reward
function only needs to be monotonically related
to the true reward function, and this property only
needs to hold for the best competing alternatives.
This is all we need in order to use the discrimina-
tive reward function in an optimal classifier, and
this simplification makes learning easier in cases
where the true reward function is too complicated
to model directly.
In RL, an optimal policy ?? is one which, at
each state s, chooses the action which maximizes
the future reward Qpi?(s, a). We assume that the
future discriminative reward can be approximated
with a linear function ?Qpi(s, a) in some feature-
vector representation ? : S ?A
s
? R
d that maps
a state-action pair to a d-dimensional features vec-
tor:
?
Q
pi
(s, a) = w ?(s, a) (3)
where w ? Rd. This gives us the following policy:
?
w
(s) = argmax
a?A
s
w ?(s, a) (4)
The set of parameters of this policy is the vec-
tor w. With this formalization, all we need to
learn is a vector w such that the resulting deci-
sions are compatible with the given information
about the optimal action sequence. We propose a
Perceptron-like algorithm to learn these parame-
ters.
Algorithm 2 describes the DRL meta-algorithm.
The Trainer takes as input ?, the task T , and a
generic set of data D describing the behaviors we
want to learn. The output is the weight vector w
of the learned policy that fits the data D. The al-
gorithm consists in a single training loop that is
repeated until convergence (lines 2-9). At line 3
a state, s, is sampled from S. At line 4, a? is set to
13
be the action that would be preferred by the cur-
rent w-policy. At line 5 an action, a?, is sampled
from A
s
such that a? 6= a?. At line 6 the algo-
rithm checks if preferring path(T (s, a?), ?
w
) over
path(T (s, a
?
), ?
w
) is a correct choice according
to the behaviors data D that the algorithm aims to
learn. If the current w-policy contradicts D, line 7
is executed to update the weight vector to promote
?
w
(s, a
?
) and penalize ?w(s, a?), where ?w(s, a)
is the summation of the features vectors of the en-
tire derivation path starting at (s, a) and following
policy ?
w
. This way of updating w has the ef-
fect of increasing the ?Q(?) value associated with
all the actions in the sequence that generated the
promoted structure, and reducing the ?Q(?) value
of the actions in the sequence that generated the
penalized structure3 .
We have described the DRL meta-algorithm to
be as general as possible. When applied to a spe-
cific problem, more details can be specified: 1) it
is possible to choose specific sampling techniques
to implement lines 3 and 5; 2) the test at line 6
needs to be detailed according to the nature of T
and D; 3) the update statement at line 7 can be re-
placed with a more sophisticated update approach.
We address these issues and describe a range of
alternatives as we apply DRL to UMT in Section
3.2.
3.2 Application of DRL to UMT
To apply DRL we formalize the task of translating
x with UMT as T ? {S, {A
s
}, T,R, ?}:
1) The set of states S is the space of all possible
UMT partial synchronous-trees, ? ;
2) The set A
?,x
is the set of connection-actions
that can expand ? connecting new synchronous-
rule instantiations matching the input sentence x
on the source side;
3) The transition function T is the connection
function ?? ? ? ? ? a ? formalized in Section 2
and detailed by the procedure CreateConnection(?)
in Algorithm 1;
4) The true reward function R is the BLEU score.
BLEU is a loss function that quantifies the differ-
ence between the reference translation and the out-
put translation t. The BLEU score can be com-
puted only when a terminal state is reached and a
full translation is available. Thus, the rewards are
all zero except at terminal states, called a Pure De-
3Preliminary experiments with updating only the features
for a? and a? produced substantially worse results.
layed Reward function;
5) Considering the nature of the problem and re-
ward function, we choose an undiscounted setting:
? = 1.
Next we specify the details of the DRL algo-
rithm. The data D consists of a set of pairs of
sentences, D ? {(x, t?)}, where x is the source
sentence and t? is the reference translation. The
feature-vector representation function ? maps a
pair (?, a) to a real valued vector having any num-
ber of dimensions. Each dimension corresponds
to a distinct feature function that maps: {?} ?
A
?,x
? R. Details of the features functions im-
plemented for our model are given in Section 4.
Each loop of the DRL algorithm analyzes a single
sample (x, t?) ? D. The state s is sampled from a
uniform distribution over ?s
0
, s
1
, ? ? ? , s
?
|??. The
action a? is sampled from a Zipfian distribution
over {A
?,x
? a?} sorted with the ?Qpiw(s, a) func-
tion. In this way actions with higher score have
higher probability to be drawn, while actions at the
bottom of the rank still have a small probability to
be selected. The if at line 6 tests if the translation
produced by path(T (s, a?), ?
w
) has higher BLEU
score than the one produced by path(T (s, a?), ?
w
).
For the update statement at line 7 we use
the Averaged Perceptron technique (Freund and
Schapire, 1999). Algorithm 2 can be eas-
ily adapted to implement the efficient Averaged
Perceptron updates (e.g. see Section 2.1.1 of
(Daume? III, 2006)). In preliminary experiments,
we found that other more aggressive update tech-
nique, such as Passive-Aggressive (Crammer et
al., 2006), Aggressive (Shen et al., 2007), or
MIRA (Crammer and Singer, 2003), lead to worst
accuracy. To see why this might be, consider that
a MT decoder needs to learn to construct struc-
tures (t, h), while the training data specifies the
gold translation t? but gives no information on the
hidden-correspondence structure h. As discussed
in (Liang et al., 2006), there are output structures
that match the reference translation using a wrong
internal structure (e.g. assuming wrong internal
alignment). While in other cases the output trans-
lation can be a valid alternative translation but gets
a low BLEU score because it differs from t?. Ag-
gressively promoting/penalizing structures whose
correctness can be only partially verified can be
expected to harm generalization ability.
14
4 Undirected Features
In this section we show how the features designed
for bottom-up HMT can be adapted to the undi-
rected approach, and we introduce a new feature
from the class of undirected features that are made
possible by the undirected approach.
Local features depend only on the action rule r.
These features can be used in the undirected ap-
proach without adaptation, since they are indepen-
dent of the surrounding structure. For our experi-
ments we use a standard set of local features: the
probability of the source phrase given the target
phrase; the lexical translation probabilities of the
source words given the target words; the lexical
translation probabilities of the target words given
the source words; and the Word Penalty feature.
Contextual features are dependent on the inter-
action between the action rule r and the avail-
able context. In UMT all the needed information
about the available context is stored in the carry
?
i
. Therefore, the computation of contextual fea-
tures whose carry?s size is bounded (like the LM)
requires constant time.
The undirected adaptation of the LM feature
computes the scores of the new n-grams formed
by adding the terminals of the action rule r to the
current partial translation ? . In the case that the
action rule r is connected to ? via a child non-
terminal, the carry is expressed as ?
i
? ([W
L
?
W
R
]). Where W
L
and W
R
are respectively the left
and right boundary target words of the span cov-
ered by ? . This notation is analogous to the stan-
dard star notation used for the bottom-up decoder
(e.g. (Chiang, 2007) Section 5.3.2). In the case
that r is connected to ? via the head non-terminal,
the carry is expressed as ?
i
? (W
R
]-[W
L
). Where
W
L
and W
R
are respectively the left and right
boundary target words of the surrounding context
provided by ? . The boundary words stored in the
carry and the terminals of the action rule are all the
information needed to compute and score the new
n-grams generated by the connection-action.
In addition, we introduce the Context-Free Fac-
tor (CFF) features. An action rule r is connected
to ? via one of r?s non-terminals, X
r,?
. Thus, the
score of the interaction between r and the context
structure attached to X
r,?
can be computed ex-
actly, while the score of the structures attached to
other r nonterminals (i.e. those in postconditions)
cannot be computed since these branches are miss-
ing. Each of these postcondition nonterminals
has an associated CFF feature, which is an upper
bound on the score of its missing branch. More
precisely, it is an upper bound on the context-free
component of this score. This upper bound can be
exactly and efficiently computed using the Forest
Rescoring Framework (Huang and Chiang, 2007;
Huang, 2008). This framework separates the MT
decoding in two steps. In the first step only the
context-free factors are considered. The output of
the first step is a hypergraph called the context-
free-forest, which compactly represents an expo-
nential number of synchronous-trees. The second
step introduces contextual features by applying a
process of state-splitting to the context-free-forest,
rescoring with non-context-free factors, and effi-
ciently pruning the search space.
To efficiently compute CFF features we run
the Inside-Outside algorithm with the (max,+)
semiring (Goodman, 1999) over the context-free-
forest. The result is a map that gives the maxi-
mum Inside and Outside scores for each node in
the context-free forest. This map is used to get the
value of the CFF features in constant time while
running the forest rescoring step.
5 Experiments
We implement our model on top of Cdec (Dyer et
al., 2010). Cdec provides a standard implemen-
tation of the HMT decoder (Chiang, 2007) and
MERT training (Och, 2003) that we use as base-
line.
We experiment on the NIST Chinese-English
parallel corpus. The training corpus contains
239k sentence pairs with 6.9M Chinese words and
8.9M English words. The test set contains 919
sentence pairs. The hierarchical translation gram-
mar was extracted using the Joshua toolkit (Li et
al., 2009) implementation of the suffix array rule
extractor algorithm (Callison-Burch et al., 2005;
Lopez, 2007).
Table 1 reports the decoding time measures.
HMT with beam1 is the fastest possible configu-
ration for HMT, but it is 71.59% slower than UMT.
This is because HMT b1 constructs O(n2) sub-
trees, many of which end up not being used in
the final result, whereas UMT only constructs the
rule instantiations that are required. HMT with
beam30 is the fastest configuration that reaches
state of the art accuracy, but increases the aver-
age time per sentence by an additional 131.36%
when compared with UMT. The rescoring time is
15
Model sent. t. sent. t. var. resc. t. resc. t. var.
UMT 135.2ms - 38.9 ms -
HMT b1 232.0ms +71.59% 141.3 ms +263.23%
HMT b30 312.8ms +131.36% 226.9 ms +483.29%
Table 1: Decoding speed comparison.
Model sent. t. sent. t. var.
UMT with DRL 267.4 ms -
HMT b1 765.2 ms +186.16%
HMT b30 1153.5 ms +331.37%
Table 2: Training speed comparison.
Model BLEU relative loss p-value
UMT with DRL 30.14 6.33% 0.18
HMT b1 30.87 4.07% 0.21
HMT b30 32.18 - -
Table 3: Accuracy comparison.
the average time spent on the forest rescoring step,
which is the only step where the decoders actu-
ally differ. This is the step that involves the inte-
gration of the Language Model and other contex-
tual features. For HMT b30, rescoring takes two
thirds of the total decoding time. Thus rescoring
is the most time consuming step in the pipeline.
The rescoring time comparison shows even bigger
gains for UMT. HMT b30 is almost 6 times slower
than UMT.
Table 2 reports the training time measures.
These results show HMT b30 training is more
than 4 times slower than UMT training with DRL.
Comparing with Table 1, we notice that the rela-
tive gain on average training time is higher than
the gain measured at decoding time. This is be-
cause MERT has an higher complexity than DRL.
Both of the training algorithms requires 10 train-
ing epochs to reach convergence.
Table 3 reports the accuracy measures. As ex-
pected, accuracy degrades the more aggressively
the search space is pruned. UMT trained with
DRL loses 2.0 BLEU points compared to HMT
b30. This corresponds to a relative-loss of 6.33%.
Although not inconsequential, this variation is
not considered big (e.g. at the WMT-11 Ma-
chine Translation shared task (Callison-Burch et
al., 2011)). To measure the significance of the
variation, we compute the sign test and measure
the one-tail p-value for the presented models in
comparison to HMT b30. From the values re-
ported in the fourth column, we can observe that
the BLEU score variations would not normally be
considered significant. For example, at WMT-11
two systems were considered equivalent if p >
0.1, as in these cases. The accuracy cannot be
compared in terms of search score since the mod-
els we are comparing are trained with distinct al-
gorithms and thus the search scores are not com-
parable.
To test the impact of the CFF features, we
trained and tested UMT with DRL with and with-
out these features. This resulted in an accuracy de-
crease of 2.3 BLEU points. Thus these features are
important for the success of the greedy approach.
They provide an estimate of the score of the miss-
ing branches, thus helping to avoid some actions
that have a good local score but lead to final trans-
lations with low global score.
To validate the results, additional experiments
were executed on the French to Italian portion
of the Europarl corpus v6. This portion contains
190k pairs of sentences. The first 186k sentences
were used to extract the grammar and train the two
models. The final tests were performed on the re-
maining 4k sentence pairs. With this corpus we
measured a similar speed gain. HMT b30 is 2.3
times slower at decoding compared to UMT, and
6.1 times slower at rescoring, while UMT loses
1.1 BLEU points in accuracy. But again the ac-
curacy differences are not considered significant.
We measured a p-value of 0.25, which is not sig-
nificant at the 0.1 level.
6 Related Work
Models sharing similar intuitions have been pre-
viously applied to other structure prediction tasks.
For example, Nivre et al. (2006) presents a linear
time syntactic dependency parser, which is con-
strained in a left-to-right decoding order. This
model offers a different accuracy/complexity bal-
ance than the quadratic time graph-based parser of
Mcdonald et al. (2005).
Other approaches learning a model specifically
for greedy decoding have been applied with suc-
16
cess to other less complex tasks. Shen et al. (2007)
present the Guided Learning (GL) framework for
bidirectional sequence classification. GL success-
fully combines the tasks of learning the order of
inference and training the local classifier in a sin-
gle Perceptron-like algorithm, reaching state of the
art accuracy with complexity lower than the ex-
haustive counterpart (Collins, 2002).
Goldberg and Elhadad (2010) present a simi-
lar training approach for a Dependency Parser that
builds the tree-structure by recursively creating
the easiest arc in a non-directional manner. This
model also integrates the tasks of learning the or-
der of inference and training the parser in a single
Perceptron. By ?non-directional? they mean the
removal of the constraint of scanning the sentence
from left to right, which is typical of shift-reduce
models. However this algorithm still builds the
tree structures in a bottom-up fashion. This model
has a O(n log n) decoding complexity and accu-
racy performance close to the O(n2) graph-based
parsers (Mcdonald et al., 2005).
Similarities can be found between DRL and pre-
vious work that applies discriminative training to
structured prediction: Collins and Roark (2004)
present an Incremental Parser trained with the Per-
ceptron algorithm. Their approach is specific to
dependency parsing and requires a function to test
exact match of tree structures to trigger parameter
updates. On the other hand, DRL can be applied to
any structured prediction task and can handle any
kind of reward function. LASO (Daume? III and
Marcu, 2005; Daume? III et al., 2005) and SEARN
(Daume? III et al., 2009; Daume? III et al., 2006)
are generic frameworks for discriminative training
for structured prediction: LASO requires a func-
tion that tests correctness of partial structures to
trigger early updates, while SEARN requires an
optimal policy to initialize the learning algorithm.
Such a test function or optimal policy cannot be
computed for tasks such as MT where the hidden
correspondence structure h is not provided in the
training data.
7 Discussion and Future Work
In general, we believe that greedy-discriminative
solutions are promising for tasks like MT, where
there is not a single correct solution: normally
there are many correct ways to translate the same
sentence, and for each correct translation there
are many different derivation-trees generating that
translation, and each correct derivation tree can be
built greedily following different inference orders.
Therefore, the set of correct decoding paths is a
reasonable portion of UMT?s search space, giving
a well-designed greedy algorithm a chance to find
a good translation even without beam search.
In order to directly evaluate the impact of our
proposed decoding strategy, in this paper the only
novel features that we consider are the CFF fea-
tures. But to take full advantage of the power
of discriminative training and the lower decoding
complexity, it would be possible to vastly increase
the number of features. The UMT?s undirected na-
ture allows the integration of non-bottom-up con-
textual features, which cannot be used by stan-
dard HMT and PbMT. And the use of a history-
based model allows features from an arbitrarily
wide context, since the model does not need to be
factorized. Exploring the impact of this advantage
is left for future work.
8 Conclusion
The main contribution of this work is the pro-
posal of a new MT model that offers an accu-
racy/complexity balance that was previously un-
available among the choices of hierarchical mod-
els.
We have presented the first Undirected frame-
work for MT. This model combines advantages
given by the use of hierarchical synchronous-
grammars with a more efficient decoding algo-
rithm. UMT?s nature allows us to design novel
undirected features that better approximate con-
textual features (such as the LM), and to introduce
a new class of undirected features that cannot be
used by standard bottom-up decoders. Further-
more, we generalize the training algorithm into
a generic Discriminative Reinforcement Learning
meta-algorithm that can be applied to any struc-
tured prediction task.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
17
phrases. In ACL ?05: Proceedings of the 43rd Con-
ference of the Association for Computational Lin-
guistics, Ann Arbor, MI, USA.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In WMT ?11:
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, Edinburgh, Scotland.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
?05: Proceedings of the 43rd Conference of the As-
sociation for Computational Linguistics, Ann Arbor,
MI, USA.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ?04:
Proceedings of the 42rd Conference of the Associa-
tion for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP ?02:
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, Philadel-
phia, PA, USA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
Hal Daume? III and Daniel Marcu. 2005. Learning
as search optimization: approximate large margin
methods for structured prediction. In ICML ?05:
Proceedings of the 22nd International Conference
on Machine Learning, Bonn, Germany.
Hal Daume? III, John Langford, and Daniel Marcu.
2005. Search-based structured prediction as clas-
sification. In ASLTSP ?05: Proceedings of the
NIPS Workshop on Advances in Structured Learn-
ing for Text and Speech Processing, Whistler, British
Columbia, Canada.
Hal Daume? III, John Langford, and Daniel Marcu.
2006. Searn in practice. Technical report.
Hal Daume? III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Submit-
ted to Machine Learning Journal.
Hal Daume? III. 2006. Practical structured learning
techniques for natural language processing. Ph.D.
thesis, University of Southern California.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Hendra Setiawan, Ferhan Ture, Vladimir Ei-
delman, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL ?10: Proceedings of the ACL 2010 System
Demonstrations, Uppsala, Sweden.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296.
Andrea Gesmundo and James Henderson. 2011.
Heuristic Search for Non-Bottom-Up Tree Structure
Prediction. In EMNLP ?11: Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, Edinburgh, Scotland, UK.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In NAACL ?10: Proceedings of the
11th Conference of the North American Chapter of
the Association for Computational Linguistics, Los
Angeles, CA, USA.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25:573?605.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT ?05: Proceedings of the 9th Inter-
national Workshop on Parsing Technology, Vancou-
ver, British Columbia, Canada.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL ?07: Proceedings of the 45th Confer-
ence of the Association for Computational Linguis-
tics, Prague, Czech Republic.
Liang Huang. 2008. Forest-based algorithms in natu-
ral language processing. Ph.D. thesis, University of
Pennsylvania.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 4th Conference of
the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In WMT ?09: Proceedings of the
4th Workshop on Statistical Machine Translation,
Athens, Greece.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In COLING-
ACL ?06: Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
Conference of the Association for Computational
Linguistics, Sydney, Australia.
18
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL ?07:
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, Prague,
Czech Republic.
Ryan Mcdonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL ?05: Proceedings of the
43rd Conference of the Association for Computa-
tional Linguistics, Ann Arbor, MI, USA.
Andrew Y. Ng and Stuart Russell. 2000. Algorithms
for inverse reinforcement learning. In ICML ?00:
Proceedings of the 17th International Conference on
Machine Learning, Stanford University, CA, USA.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In LREC ?06: Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, Genoa, Italy.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Conference of the Association
for Computational Linguistics, Sapporo, Japan.
Frank Rosenblatt. 1958. The Perceptron: A proba-
bilistic model for information storage and organiza-
tion in the brain. Psychological Review, 65(6):386?
408.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifi-
cation. In ACL ?07: Proceedings of the 45th Confer-
ence of the Association for Computational Linguis-
tics, Prague, Czech Republic.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based ma-
chine translation accuracy. In EMNLP-CoNLL ?07:
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, Prague,
Czech Republic.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In ACL ?01: Pro-
ceedings of the 39th Conference of the Association
for Computational Linguistics, Toulouse, France.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In NAACL ?06: Proceedings of the
7th Conference of the North American Chapter of
the Association for Computational Linguistics, New
York, New York.
19
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 28?32,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Projecting the Knowledge Graph to Syntactic Parsing
Andrea Gesmundo and Keith B. Hall
Google, Inc.
{agesmundo,kbhall}@google.com
Abstract
We present a syntactic parser training
paradigm that learns from large scale
Knowledge Bases. By utilizing the
Knowledge Base context only during
training, the resulting parser has no
inference-time dependency on the Knowl-
edge Base, thus not decreasing the speed
during prediction. Knowledge Base infor-
mation is injected into the model using an
extension to the Augmented-loss training
framework. We present empirical results
that show this approach achieves a signif-
icant gain in accuracy for syntactic cat-
egories such as coordination and apposi-
tion.
1 Introduction
Natural Language Processing systems require
large amounts of world knowledge to achieve
state-of-the-art performance. Leveraging Knowl-
edge Bases (KB) provides allows us to inject hu-
man curated world-knowledge into our systems.
As these KBs have increased in size, we are now
able to leverage this information to improve upon
the state-of-the-art. Large scale KB have been de-
veloped rapidly in recent years, adding large num-
bers of entities and relations between the entities.
Such entities can be of any kind: an object, a per-
son, a place, a company, a book, etc. Entities
and relations are stored in association with rele-
vant data that describes the particular entity or re-
lation; for example, the name of a book, it?s author,
other books by the same author, etc.. Large scale
KB annotation efforts have focused on the collec-
tion of both current and historical entities, but are
biased towards the contemporary entities.
Of the many publicly available KBs, we focus
this study on the use of Freebase
1
: a large collab-
orative Knowledge Base composed and updated
by a member community. Currently it contains
roughly 40 million entities and 1.1 billion rela-
tions.
The aim of the presented work is to use the in-
formation provided by the KB to improve the ac-
curacy of the statistical dependency parsing task
(Kubler et al., 2009). In particular we focus on the
recognition of relations such as coordination and
apposition. This choice is motivated by the fact
that the KB stores information about real-world
entities while many of the errors associated with
coordination and apposition is the lack of knowl-
edge of these real-world entities.
We begin by defining the task (section 2). Fol-
lowing, we present the modified augmented-loss
training framework (section 3). In section 4, we
define how the Knowledge Base data is integrated
into the training process. Finally, we discuss the
empirical results (section 5).
2 Task
Apposition is a relation between two adjacent
noun-phrases, where one noun-phrase specifies or
modifying the other. For example, in the sentence
?My friend Anna?, the nouns ?friend? and ?Anna?
are in apposition. Coordination between nouns
relates two or more elements of the same kind.
The coordination is often signaled by the appear-
ance of a coordinating conjunction. For example,
in the sentence ?My friend and Anna?, the nouns
?friend? and ?Anna? are in coordination. The se-
mantic difference between the two relations is that
the nouns in apposition refer to the same entity,
1
www.freebase.com
28
while the nouns in coordination refer to distinct
entities of the same kind or sharing some proper-
ties.
Statistical parsers are inaccurate in classifying
relations involving proper nouns that appear rarely
in the training set. In the sentence:
?They invested in three companies, Google,
Microsoft, and Yahoo.?
?companies? is in apposition with the coordina-
tion ?Google, Microsoft, and Yahoo?. By integrat-
ing the information provided by a large scale KB
into the syntactic parser, we attempt to increase
the ability to disambiguate the relations involving
these proper nouns, even if the parser has been
trained on a different domain.
3 Model
We present a Syntactic Parsing model that learns
from the KB. An important constraint that we im-
pose, is that the speed of the Syntactic Parser must
not decrease when this information is integrated.
As the queries to the KB would significantly slow
down the parser, we limit querying the KB to train-
ing. This constraint reduces the impact that the KB
can have on the accuracy, but allows us to design a
parser that can be substituted in any setting, even
in the absence of the KB.
We propose a solution based on the Augmented-
loss framework (Hall et al., 2011a). Augmented-
loss is a training framework for structured predic-
tion tasks such as parsing. It can be used to ex-
tend a standard objective function with additional
loss-functions and be integrated with the struc-
tured perceptron training algorithm. The input
is enriched with multiple datasets each associated
with a loss function. The algorithm iterates over
the datasets triggering parameter updates when-
ever the loss function is positive.
Loss functions return a positive value if the pre-
dicted output is ?worse? than the gold standard.
Augmented-loss allows for the inclusion of mul-
tiple objective functions, either based on intrinsic
parsing quality or task-specific extrinsic measures
of quality. In the original formalization, both the
intrinsic and extrinsic losses require gold standard
information. Thus, each dataset must specify a
gold standard output for each input.
We extend the Augmented-loss framework to
apply it when the additional dataset gold-standard
is unknown. Without the gold standard, it is not
possible to trigger updates using a loss function.
Instead, we use a sampling function, S(?), that is
defined such that: if y? is a candidate parse tree,
then S(y?) returns a parse tree that is guaranteed to
be ?not worse? than y?. In other words:
L
S
(y?, S(y?)) ? 0 (1)
Where the L
S
(?) is the implicit loss function. This
formalization will allow us to avoid stating explic-
itly the loss function. Notice that S(y?) is not guar-
anteed to be the ?best? parse tree. It can be any
parse tree in the search space that is ?not worse?
than y?. S(y?) can represent an incremental im-
provement over y?.
Algorithm 1 Augmented-loss extension
1: {Input loss function: L(?)}
2: {Input sample function: S(?)}
3: {Input data sets}:
4: D
L
= {d
L
i
= (x
L
i
, y
L
i
) | 1 ? i ? N
L
}
5: D
S
= {d
S
i
= (x
S
i
) | 1 ? i ? N
S
}
6: ? =
~
0
7: repeat
8: for i = 1 . . . N
L
do
9: y? = F
?
(x
L
i
)
10: if L(y?, y
L
i
) > 0 then
11: ? = ? + ?(y
L
i
)? ?(y?)
12: end if
13: end for
14: for i = 1 . . . N
S
do
15: y? = F
?
(x
S
i
)
16: y
?
= S(y?)
17: ? = ? + ?(y
?
)? ?(y?)
18: end for
19: until converged
20: {Return model ?}
Algorithm 1 summarizes the extension to the
Augmented-loss algorithm.
The algorithm takes as input: the loss func-
tion L(?); the sample function S(?); the loss func-
tion data samples D
L
; and the sample function
data samples D
S
. Notice that D
L
specifies the
gold standard parse y
L
i
for each input sentence x
L
i
.
While, D
S
specifies only the input sentence x
S
i
.
The model parameter are initialized to the zero
vector (line 6). The main loop iterates until the
model reaches convergence (lines 7-19). After
which the model parameters are returned.
The first inner loop iterates over D
L
(lines 8-
13) executing the standard on-line training. The
candidate parse, y?, for the current input sentence,
29
xL
i
, is predicted given the current model parame-
ters, ? (line 9). In the structured perceptron setting
(Collins and Roark, 2004; Daum?e III et al., 2009),
we have that:
F
?
(x) = argmax
y?Y
? ? ?(y) (2)
Where ?(?) is the mapping from a parse tree y to
a high dimensional feature space. Then, the algo-
rithm tests if the current prediction is wrong (line
10). In which case the model is updated promot-
ing features that fire in the gold-standard ?(y
L
i
),
and penalizing features that fire in the predicted
output, ?(y?) (line 11).
The second inner loop iterates over D
S
(lines
14-18). First, the candidate parse, y?, is predicted
(line 15). Then the sample parse, y
?
, is pro-
duced by the sample function (line 16). Finally,
the parameters are updated promoting the features
of y
?
. The updates are triggered without test-
ing if the loss is positive, since it is guaranteed
that L
S
(y?, y
?
) ? 0. Updating in cases where
L
S
(y?, y
?
) = 0 does not harm the model. To opti-
mize the algorithm, updates can be avoided when
y? = y
?
.
In order to simplify the algorithmic descrip-
tion, we define the algorithm with only one loss
function and one sample function, and we formal-
ized it for the specific task we are considering.
This definitions can be trivially generalized to in-
tegrate multiple loss/sample functions and to be
formalized for a generic structured prediction task.
This generalization can be achieved following the
guidelines of (Hall et al., 2011a). Furthermore, we
defined the algorithm such that it first iterates over
D
L
and then over D
S
. In practice, the algorithm
can switch between the data sets with a desired fre-
quency by using a scheduling policy as described
in (Hall et al., 2011a). For the experiments, we
trained on 8 samples ofD
L
followed by 1 samples
of D
S
, looping over the training sets.
4 Sample Function
We integrate the Knowledge Base data into the
training algorithm using a sampling function. The
idea is to correct errors in the candidate parse
by using the KB. The sample function corrects
only relations among entities described in the KB.
Thus, it returns a better or equal parse tree that
may still contain errors. This is sufficient to guar-
antee the constraint on the implicit loss function
(equation 1).
The sample function receives as input the can-
didate dependency parse and the input sentence
enriched with KB annotation. Then, it corrects
the labels of each arc in the dependency tree con-
necting two entities. The labels are corrected ac-
cording to the predictions produced by a classifier.
As classifier we use a standard multi-class percep-
tron (Crammer and Singer, 2003). The classifier is
trained in a preprocessing step on a parsed corpus
enriched with KB data. The features used by the
classifier are:
? Lexical features of the head and modifier.
? Sentence level features: words distance be-
tween head and modifier; arc direction (L/R);
neighboring words.
? Syntactic features: POS and syntactic label of
head and modifier and modifier?s left sibling.
? Knowledge Base features: types defined for
entities and for their direct relations.
5 Experiments
The primary training corpus is composed of manu-
ally annotated sentences with syntactic tress which
are converted to dependency format using the
Stanford converter v1.6 (de Marneffe et al., 2006).
We run experiments using 10k sentences or 70k
sentences from this corpus. The test set contains
16k manually syntactically annotated sentences
crawled from the web. The test and train sets are
from different domains. This setting may degrade
the parser accuracy in labelling out-of-domain en-
tities, as we discussed in section 2. Thus, we use
web text as secondary training set to be used for
the Augmented-loss loss sample training. Web
text is available in any quantity, and we do not
need to provide gold-standard parses in order to
integrate it in the Augmented-loss sample train-
ing. The classifier is trained on 10k sentences ex-
tracted from news text which has been automati-
cally parsed. We chose to train the classifier on
news data as the quality of the automatic parses is
much higher than on general web text. We do this
despite the fact that we will apply the classifier to
a different domain (the web text).
As dependency parser, we use an implemen-
tation of the transition-based dependency parsing
framework (Nivre, 2008) with the arc-eager tran-
sition strategy. The part of Augmented-loss train-
ing based on the standard loss function, applies
30
Training set size Model appos F1 conj F1 LAS UAS
70k sentences Baseline 54.36 83.72 79.55 83.50
Augmented-loss 55.64 84.47 79.71 83.71
10k sentences Baseline 45.13 80.36 75.99 86.02
Augmented-loss 48.06 81.63 76.16 86.18
Table 1: Accuracy Comparison.
the perceptron algorithm as in (Zhang and Clark,
2008) with a beam size of 16. The baseline is the
same model but trained only the primary training
corpus without Augmented-loss.
Table 1 reports the results of the accuracy com-
parison. It reports the metrics for Labeled At-
tachment Score (LAS) and Unlabeled Attachment
Score (UAS) to measure the overall accuracy. The
syntactic classes that are affected the most are ap-
position (appos) and conjunction (conj). On the
development set we measured that the percentage
of arcs connecting 2 entities that are labeled as
conjunction is 36.11%. While those that are la-
belled as apposition is 25.06%. Each of the other
40 labels cover a small portion of the remaining
38.83%.
Training the models with the full primary train-
ing corpus (70k sentences), shows a significant
gain for the Augmented-loss model. Apposition
F1 gains 1.28, while conjunction gains 0.75. The
LAS gain is mainly due to the gain of the two men-
tioned classes. It is surprising to measure a simi-
lar gain also for the unlabeled accuracy. Since the
classifier can correct the label of an arc but never
change the structure of the parse. This implies
that just by penalizing a labeling action, the model
learns to construct better parse structures.
Training the model with 10k sentences shows a
significantly bigger gain on all the measures. This
results shows that, in cases where the set of la-
beled data is small, this approach can be applied
to integrate in unlimited amount of unlabeled data
to boost the learning.
6 Related Work
As we mentioned, Augmented-loss (Hall et al.,
2011a; Hall et al., 2011b) is perhaps the closest to
our framework. Another difference with its origi-
nal formalization is that it was primarily aimed to
cases where the additional weak signal is precisely
what we wish to optimize. Such as cases where
we wish to optimize parsing to be used as an input
to a downstream natural language processing tasks
and the accuracies to be optimized are those of the
downstream task and not directly the parsing ac-
curacy. While our work is focused on integrating
additional data in a semi-supervised fashion with
the aim of improving the primary task?s accuracy
and/or adapt it to a different domain.
Another similar idea is (Chang et al., 2007)
which presents a constraint driven learning. In this
study, they integrate a weak signal into the training
framework with the aim to improve the structured
prediction models on the intrinsic evaluation met-
rics.
7 Conclusion
We extended the Augmented-loss framework
defining a method for integrating new types of sig-
nals that require neither gold standard data nor an
explicit loss function. At the same time, they al-
low the integration of additional information that
can inform training to learn for specific types of
phenomena.
This framework allows us to effectively inte-
grate large scale KB in the training of structured
prediction tasks. This approach integrates the data
at training time without affecting the prediction
time.
Experiments on syntactic parsing show that a
significant gain for categories that model relation
between entities defined in the KB.
References
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL ?07: Proceedings of the 45th Con-
ference of the Association for Computational Lin-
guistics.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ?04:
Proceedings of the 42rd Conference of the Associa-
tion for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
31
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Submit-
ted to Machine Learning Journal.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
LREC.
Keith Hall, Ryan McDonald, Jason Katz-brown, and
Michael Ringgaard. 2011a. Training dependency
parsers by jointly optimizing multiple objectives. In
EMNLP ?11: Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing.
Keith Hall, Ryan McDonald, and Slav Petrov. 2011b.
Training structured prediction models with extrinsic
loss functions. In Domain Adaptation Workshop at
NIPS, October.
Sandra Kubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. In Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool Publishers.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. volume 34, pages
513?553.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
EMNLP ?08: Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562?571.
32
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 296?300,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Heuristic Cube Pruning in Linear Time
Andrea Gesmundo
Department of
Computer Science
University of Geneva
andrea.gesmundo@unige.ch
Giorgio Satta
Department of
Information Engineering
University of Padua
satta@dei.unipd.it
James Henderson
Department of
Computer Science
University of Geneva
james.henderson@unige.ch
Abstract
We propose a novel heuristic algorithm for
Cube Pruning running in linear time in the
beam size. Empirically, we show a gain in
running time of a standard machine translation
system, at a small loss in accuracy.
1 Introduction
Since its first appearance in (Huang and Chiang,
2005), the Cube Pruning (CP) algorithm has quickly
gained popularity in statistical natural language pro-
cessing. Informally, this algorithm applies to sce-
narios in which we have the k-best solutions for two
input sub-problems, and we need to compute the k-
best solutions for the new problem representing the
combination of the two sub-problems.
CP has applications in tree and phrase based ma-
chine translation (Chiang, 2007; Huang and Chi-
ang, 2007; Pust and Knight, 2009), parsing (Huang
and Chiang, 2005), sentence alignment (Riesa and
Marcu, 2010), and in general in all systems combin-
ing inexact beam decoding with dynamic program-
ming under certain monotonic conditions on the def-
inition of the scores in the search space.
Standard implementations of CP run in time
O(k log(k)), with k being the size of the in-
put/output beams (Huang and Chiang, 2005). Ges-
mundo and Henderson (2010) propose Faster CP
(FCP) which optimizes the algorithm but keeps the
O(k log(k)) time complexity. Here, we propose a
novel heuristic algorithm for CP running in time
O(k) and evaluate its impact on the efficiency and
performance of a real-world machine translation
system.
2 Preliminaries
Let L = ?x0, . . . , xk?1? be a list over R, that is,
an ordered sequence of real numbers, possibly with
repetitions. We write |L| = k to denote the length of
L. We say that L is descending if xi ? xj for every
i, j with 0 ? i < j < k. Let L1 = ?x0, . . . , xk?1?
and L2 = ?y0, . . . , yk??1? be two descending lists
over R. We write L1 ? L2 to denote the descending
list with elements xi+yj for every i, j with 0 ? i <
k and 0 ? j < k?.
In cube pruning (CP) we are given as input two
descending lists L1, L2 over R with |L1| = |L2| =
k, and we are asked to compute the descending list
consisting of the first k elements of L1 ?L2.
A problem related to CP is the k-way merge
problem (Horowitz and Sahni, 1983). Given de-
scending lists Li for every i with 0 ? i < k, we
write mergek?1i=0 Li to denote the ?merge? of all the
lists Li, that is, the descending list with all elements
from the lists Li, including repetitions.
For ? ? R we define shift(L,?) = L ? ???. In
words, shift(L,?) is the descending list whose ele-
ments are obtained by ?shifting? the elements of L
by ?, preserving the order. Let L1,L2 be descend-
ing lists of length k, with L2 = ?y0, . . . , yk?1?.
Then we can express the output of CP on L1,L2 as
the list
mergek?1i=0 shift(L1, yi) (1)
truncated after the first k elements. This shows that
the CP problem is a particular instance of the k-way
merge problem, in which all input lists are related by
k independent shifts.
296
Computation of the solution of the k-way merge
problem takes time O(q log(k)), where q is the
size of the output list. In case each input list has
length k this becomes O(k2 log(k)), and by restrict-
ing the computation to the first k elements, as re-
quired by the CP problem, we can further reduce to
O(k log(k)). This is the already known upper bound
on the CP problem (Huang and Chiang, 2005; Ges-
mundo and Henderson, 2010). Unfortunately, there
seems to be no way to achieve an asymptotically
faster algorithm by exploiting the restriction that the
input lists are all related by some shifts. Nonethe-
less, in the next sections we use the above ideas to
develop a heuristic algorithm running in time linear
in k.
3 Cube Pruning With Constant Slope
Consider lists L1,L2 defined as in section 2. We say
that L2 has constant slope if yi?1? yi = ? > 0 for
every i with 0 < i < k. Throughout this section we
assume that L2 has constant slope, and we develop
an (exact) linear time algorithm for solving the CP
problem under this assumption.
For each i ? 0, let Ii be the left-open interval
(x0 ? (i + 1) ? ?, x0 ? i ? ?] of R. Let alo s =
?(x0 ? xk?1)/?? + 1. We split L1 into (possibly
empty) sublists ?i, 0 ? i < s, called segments, such
that each ?i is the descending sublist consisting of
all elements fromL1 that belong to Ii. Thus, moving
down one segment in L1 is the closest equivalent to
moving down one element in L2.
Let t = min{k, s}; we define descending lists
Mi, 0 ? i < t, as follows. We set M0 =
shift(?0, y0), and for 1 ? i < t we let
Mi = merge{shift(?i, y0), shift(Mi?1,??)} (2)
We claim that the ordered concatenation of M0,
M1, . . . , Mt?1 truncated after the first k elements
is exactly the output of CP on input L1,L2.
To prove our claim, it helps to visualize the de-
scending list L1 ? L2 (of size k2) as a k ? k matrix
L whose j-th column is shift(L1, yj), 0 ? j < k.
For an interval I = (x, x?], we define shift(I, y) =
(x+ y, x?+ y]. Similarly to what we have done with
L1, we can split each column of L into s segments.
For each i, j with 0 ? i < s and 0 ? j < k, we de-
fine the i-th segment of the j-th column, written ?i,j ,
as the descending sublist consisting of all elements
of that column that belong to shift(Ii, yj). Then we
have ?i,j = shift(?i, yj).
For any d with 0 ? d < t, consider now all
segments ?i,j with i + j = d, forming a sub-
antidiagonal in L. We observe that these segments
contain all and only those elements of L that belong
to the interval Id. It is not difficult to show by in-
duction that these elements are exactly the elements
that appear in descending order in the list Mi defined
in (2).
We can then directly use relation (2) to iteratively
compute CP on two lists of length k, under our as-
sumption that one of the two lists has constant slope.
Using the fact that the merge of two lists as in (2) can
be computed in time linear in the size of the output
list, it is not difficult to implement the above algo-
rithm to run in time O(k).
4 Linear Time Heuristic Solution
In this section we further elaborate on the exact al-
gorithm of section 3 for the constant slope case, and
develop a heuristic solution for the general CP prob-
lem. Let L1,L2, L and k be defined as in sections 2
and 3. Despite the fact that L2 does not have a con-
stant slope, we can still split each column of L into
segments, as follows.
Let I?i, 0 ? i < k ? 1, be the left-open interval
(x0 + yi+1, x0+ yi] of R. Note that, unlike the case
of section 3, intervals I?i?s are not all of the same size
now. Let alo I?k?1 = [xk?1 + yk?1, x0 + yk?1].
For each i, j with 0 ? j < k and 0 ? i < k ?
j, we define segment ??i,j as the descending sublist
consisting of all elements of the j-th column of L
that belong to I?i+j . In this way, the j-th column
of L is split into segments I?j , I?j+1, . . . , I?k?1, and
we have a variable number of segments per column.
Note that segments ??i,j with a constant value of i+j
contain all and only those elements of L that belong
to the left-open interval I?i+j .
Similarly to section 3, we define descending lists
M?i, 0 ? i < k, by setting M?0 = ??0,0 and, for
1 ? i < k, by letting
M?i = merge{??i,0 , path(M?i?1, L)} (3)
Note that the function path(M?i?1, L) should not re-
turn shift(M?i?1,??), for some value ?, as in the
297
1: Algorithm 1 (L1, L2) : L??
2: L??.insert(L[0, 0]);
3: referColumn? 0;
4: xfollow ? L[0, 1];
5: xdeviate ? L[1, 0];
6: C ? CircularList([0, 1]);
7: C-iterator? C.begin();
8: while |L??| < k do
9: if xfollow > xdeviate then
10: L??.insert(xfollow );
11: if C-iterator.current()=[0, 1] then
12: referColumn++;
13: [i, j]? C-iterator.next();
14: xfollow ? L[i,referColumn+j];
15: else
16: L??.insert(xdeviate );
17: i? xdeviate .row();
18: C-iterator.insert([i,?referColumn]);
19: xdeviate ? L[i + 1, 0];
case of (2). This is because input list L2 does not
have constant slope in general. In an exact algo-
rithm, path(M?i?1, L) should return the descending
list L?i?1 = mergeij=1 ??i?j,j: Unfortunately, we do
not know how to compute such a i-way merge with-
out introducing a logarithmic factor.
Our solution is to define path(M?i?1, L) in such a
way that it computes a list L?i?1 which is a permu-
tation of the correct solution L?i?1. To do this, we
consider the ?relative? path starting at x0+yi?1 that
we need to follow in L in order to collect all the el-
ements of M?i?1 in the given order. We then apply
such a path starting at x0 + yi and return the list of
collected elements. Finally, we compute the output
list L?? as the concatenation of all lists M?i up to the
first k elements.
It is not difficult to see that when L2 has constant
slope we have M?i = Mi for all i with 0 ? i < k,
and list L?? is the exact solution to the CP prob-
lem. When L2 does not have a constant slope, list
L?? might depart from the exact solution in two re-
spects: it might not be a descending list, because
of local variations in the ordering of the elements;
and it might not be a permutation of the exact so-
lution, because of local variations at the end of the
list. In the next section we evaluate the impact that
 
 
	

 


 


 
 


 

 


 

  

  


  
 

  
 

Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 368?372,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Lemmatisation as a Tagging Task
Andrea Gesmundo
Department of Computer Science
University of Geneva
andrea.gesmundo@unige.ch
Tanja Samardz?ic?
Department of Linguistics
University of Geneva
tanja.samardzic@unige.ch
Abstract
We present a novel approach to the task of
word lemmatisation. We formalise lemmati-
sation as a category tagging task, by describ-
ing how a word-to-lemma transformation rule
can be encoded in a single label and how a
set of such labels can be inferred for a specific
language. In this way, a lemmatisation sys-
tem can be trained and tested using any super-
vised tagging model. In contrast to previous
approaches, the proposed technique allows us
to easily integrate relevant contextual informa-
tion. We test our approach on eight languages
reaching a new state-of-the-art level for the
lemmatisation task.
1 Introduction
Lemmatisation and part-of-speech (POS) tagging
are necessary steps in automatic processing of lan-
guage corpora. This annotation is a prerequisite
for developing systems for more sophisticated au-
tomatic processing such as information retrieval, as
well as for using language corpora in linguistic re-
search and in the humanities. Lemmatisation is es-
pecially important for processing morphologically
rich languages, where the number of different word
forms is too large to be included in the part-of-
speech tag set. The work on morphologically rich
languages suggests that using comprehensive mor-
phological dictionaries is necessary for achieving
good results (Hajic?, 2000; Erjavec and Dz?eroski,
2004). However, such dictionaries are constructed
manually and they cannot be expected to be devel-
oped quickly for many languages.
In this paper, we present a new general approach
to the task of lemmatisation which can be used to
overcome the shortage of comprehensive dictionar-
ies for languages for which they have not been devel-
oped. Our approach is based on redefining the task
of lemmatisation as a category tagging task. Formu-
lating lemmatisation as a tagging task allows the use
of advanced tagging techniques, and the efficient in-
tegration of contextual information. We show that
this approach gives the highest accuracy known on
eight European languages having different morpho-
logical complexity, including agglutinative (Hungar-
ian, Estonian) and fusional (Slavic) languages.
2 Lemmatisation as a Tagging Task
Lemmatisation is the task of grouping together word
forms that belong to the same inflectional morpho-
logical paradigm and assigning to each paradigm its
corresponding canonical form called lemma. For ex-
ample, English word forms go, goes, going, went,
gone constitute a single morphological paradigm
which is assigned the lemma go. Automatic lemma-
tisation requires defining a model that can determine
the lemma for a given word form. Approaching it
directly as a tagging task by considering the lemma
itself as the tag to be assigned is clearly unfeasible:
1) the size of the tag set would be proportional to the
vocabulary size, and 2) such a model would overfit
the training corpus missing important morphologi-
cal generalisations required to predict the lemma of
unseen words (e.g. the fact that the transformation
from going to go is governed by a general rule that
applies to most English verbs).
Our method assigns to each word a label encod-
368
ing the transformation required to obtain the lemma
string from the given word string. The generic trans-
formation from a word to a lemma is done in four
steps: 1) remove a suffix of length Ns; 2) add a
new lemma suffix, Ls; 3) remove a prefix of length
Np; 4) add a new lemma prefix, Lp. The tuple
? ? ?Ns, Ls, Np, Lp? defines the word-to-lemma
transformation. Each tuple is represented with a
label that lists the 4 parameters. For example, the
transformation of the word going into its lemma is
encoded by the label ?3, ?, 0, ??. This label can be
observed on a specific lemma-word pair in the train-
ing set but it generalizes well to the unseen words
that are formed regularly by adding the suffix -ing.
The same label applies to any other transformation
which requires only removing the last 3 characters
of the word string.
Suffix transformations are more frequent than pre-
fix transformations (Jongejan and Dalianis, 2009).
In some languages, such as English, it is sufficient
to define only suffix transformations. In this case, all
the labels will have Np set to 0 and Lp set to ?. How-
ever, languages richer in morphology often require
encoding prefix transformations too. For example,
in assigning the lemma to the negated verb forms in
Czech the negation prefix needs to be removed. In
this case, the label ?1, t, 2, ?? maps the word neve?de?l
to the lemma ve?de?t. The same label generalises to
other (word, lemma) pairs: (nedoka?zal, doka?zat),
(neexistoval, existovat), (nepamatoval, pamatovat).1
The set of labels for a specific language is induced
from a training set of pairs (word, lemma). For each
pair, we first find the Longest Common Substring
(LCS) (Gusfield, 1997). Then we set the value of
Np to the number of characters in the word that pre-
cede the start of LCS and Ns to the number of char-
acters in the word that follow the end of LCS. The
value of Lp is the substring preceding LCS in the
lemma and the value of Ls is the substring follow-
ing LCS in the lemma. In the case of the example
pair (neve?de?l, ve?de?t), the LCS is ve?de?, 2 characters
precede the LCS in the word and 1 follows it. There
are no characters preceding the start of the LCS in
1The transformation rules described in this section are well
adapted for a wide range of languages which encode morpho-
logical information by means of affixes. Other encodings can be
designed to handle other morphological types (such as Semitic
languages).
 0
 50
 100
 150
 200
 250
 300
 350
 0  10000  20000  30000  40000  50000  60000  70000  80000  90000
la
be
l s
et
 s
ize
word-lemma samples
English
Slovene
Serbian
Figure 1: Growth of the label set with the number of train-
ing instances.
the lemma and ?t? follows it. The generated label is
added to the set of labels.
3 Label set induction
We apply the presented technique to induce the la-
bel set from annotated running text. This approach
results in a set of labels whose size convergences
quickly with the increase of training pairs.
Figure 1 shows the growth of the label set size
with the number of tokens seen in the training set for
three representative languages. This behavior is ex-
pected on the basis of the known interaction between
the frequency and the regularity of word forms that
is shared by all languages: infrequent words tend to
be formed according to a regular pattern, while ir-
regular word forms tend to occur in frequent words.
The described procedure leverages this fact to in-
duce a label set that covers most of the word occur-
rences in a text: a specialized label is learnt for fre-
quent irregular words, while a generic label is learnt
to handle words that follow a regular pattern.
We observe that the non-complete convergence of
the label set size is, to a large extent, due to the pres-
ence of noise in the corpus (annotation errors, ty-
pos or inconsistency). We test the robustness of our
method by deciding not to filter out the noise gener-
ated labels in the experimental evaluation. We also
observe that encoding the prefix transformation in
the label is fundamental for handling the size of the
label sets in the languages that frequently use lemma
prefixes. For example, the label set generated for
369
Czech doubles in size if only the suffix transforma-
tion is encoded in the label. Finally, we observe that
the size of the set of induced labels depends on the
morphological complexity of languages, as shown in
Figure 1. The English set is smaller than the Slovene
and Serbian sets.
4 Experimental Evaluation
The advantage of structuring the lemmatisation task
as a tagging task is that it allows us to apply success-
ful tagging techniques and use the context informa-
tion in assigning transformation labels to the words
in a text. For the experimental evaluations we use
the Bidirectional Tagger with Guided Learning pre-
sented in Shen et al (2007). We chose this model
since it has been shown to be easily adaptable for
solving a wide set of tagging and chunking tasks ob-
taining state-of-the-art performances with short ex-
ecution time (Gesmundo, 2011). Furthermore, this
model has consistently shown good generalisation
behaviour reaching significantly higher accuracy in
tagging unknown words than other systems.
We train and test the tagger on manually anno-
tated G. Orwell?s ?1984? and its translations to seven
European languages (see Table 2, column 1), in-
cluded in the Multext-East corpora (Erjavec, 2010).
The words in the corpus are annotated with both
lemmas and detailed morphosyntactic descriptions
including the POS labels. The corpus contains 6737
sentences (approximatively 110k tokens) for each
language. We use 90% of the sentences for training
and 10% for testing.
We compare lemmatisation performance in differ-
ent settings. Each setting is defined by the set of fea-
tures that are used for training and prediction. Table
1 reports the four feature sets used. Table 2 reports
the accuracy scores achieved in each setting. We es-
tablish the Base Line (BL) setting and performance
in the first experiment. This setting involves only
features of the current word, [w0], such as the word
form, suffixes and prefixes and features that flag the
presence of special characters (digits, hyphen, caps).
The BL accuracy is reported in the second column of
Table 2).
In the second experiment, the BL feature set is
expanded with features of the surrounding words
([w?1], [w1]) and surrounding predicted lemmas
([lem?1], [lem1]). The accuracy scores obtained in
Base Line [w0], flagChars(w0),
(BL) prefixes(w0), suffixes(w0)
+ context BL + [w1], [w?1], [lem1], [lem?1]
+ POS BL + [pos0]
+cont.&POS BL + [w1], [w?1], [lem1], [lem?1],
[pos0], [pos?1], [pos1]
Table 1: Feature sets.
Base + + +cont.&POS
Language Line cont. POS Acc. UWA
Czech 96.6 96.8 96.8 97.7 86.3
English 98.8 99.1 99.2 99.6 94.7
Estonian 95.8 96.2 96.5 97.4 78.5
Hungarian 96.5 96.9 97.0 97.5 85.8
Polish 95.3 95.6 96.0 96.8 85.8
Romanian 96.2 97.4 97.5 98.3 86.9
Serbian 95.0 95.3 96.2 97.2 84.9
Slovene 96.1 96.6 97.0 98.1 87.7
Table 2: Accuracy of the lemmatizer in the four settings.
the second experiment are reported in the third col-
umn of Table 2. The consistent improvements over
the BL scores for all the languages, varying from
the lowest relative error reduction (RER) for Czech
(5.8%) to the highest for Romanian (31.6%), con-
firm the significance of the context information. In
the third experiment, we use a feature set in which
the BL set is expanded with the predicted POS tag of
the current word, [pos0].2 The accuracy measured
in the third experiment (Table 2, column 4) shows
consistent improvement over the BL (the best RER
is 34.2% for Romanian). Furthermore, we observe
that the accuracy scores in the third experiment are
close to those in the second experiment. This allows
us to state that it is possible to design high quality
lemmatisation systems which are independent of the
POS tagging. Instead of using the POS information,
which is currently standard practice for lemmatisa-
tion, the task can be performed in a context-wise set-
ting using only the information about surrounding
words and lemmas.
In the fourth experiment we use a feature set con-
sisting of contextual features of words, predicted
lemmas and predicted POS tags. This setting com-
2The POS tags that we use are extracted from the mor-
phosyntactic descriptions provided in the corpus and learned
using the same system that we use for lemmatisation.
370
bines the use of the context with the use of the pre-
dicted POS tags. The scores obtained in the fourth
experiment are considerably higher than those in the
previous experiments (Table 2, column 5). The RER
computed against the BL varies between 28.1% for
Hungarian and 66.7% for English. For this set-
ting, we also report accuracies on unseen words only
(UWA, column 6 in Table 2) to show the generalisa-
tion capacities of the lemmatizer. The UWA scores
85% or higher for all the languages except Estonian
(78.5%).
The results of the fourth experiment show that in-
teresting improvements in the performance are ob-
tained by combining the POS and context informa-
tion. This option has not been explored before.
Current systems typically use only the information
on the POS of the target word together with lem-
matisation rules acquired separately from a dictio-
nary, which roughly corresponds to the setting of
our third experiment. The improvement in the fourth
experiment compared to the third experiment (RER
varying between 12.5% for Czech and 50% for En-
glish) shows the advantage of our context-sensitive
approach over the currently used techniques.
All the scores reported in Table 2 represent per-
formance with raw text as input. It is important to
stress that the results are achieved using a general
tagging system trained only a small manually an-
notated corpus, with no language specific external
sources of data such as independent morphological
dictionaries, which have been considered necessary
for efficient processing of morphologically rich lan-
guages.
5 Related Work
Jurs?ic? et al (2010) propose a general multilingual
lemmatisation tool, LemGen, which is tested on
the same corpora that we used in our evaluation.
LemGen learns word transformations in the form of
ripple-down rules. Disambiguition between multi-
ple possible lemmas for a word form is based on the
gold-standard morphosyntactic label of the word.
Our system outperforms LemGen on all the lan-
guages. We measure a Relative Error Reduction
varying between 81% for Serbian and 86% for En-
glish. It is worth noting that we do not use manually
constructed dictionaries for training, while Jurs?ic? et
al. (2010) use additional dictionaries for languages
for which they are available.
Chrupa?a (2006) proposes a system which, like
our system, learns the lemmatisation rules from a
corpus, without external dictionaries. The mappings
between word forms and lemmas are encoded by
means of the shortest edit script. The sets of edit
instructions are considered as class labels. They are
learnt using a SVM classifier and the word context
features. The most important limitation of this ap-
proach is that it cannot deal with both suffixes and
prefixes at the same time, which is crucial for effi-
cient processing of morphologically rich languages.
Our approach enables encoding transformations on
both sides of words. Furthermore, we propose a
more straightforward and a more compact way of
encoding the lemmatisation rules.
The majority of other methods are concentrated
on lemmatising out-of-lexicon words. Toutanova
and Cherry (2009) propose a joint model for as-
signing the set of possible lemmas and POS tags
to out-of-lexicon words which is language indepen-
dent. The lemmatizer component is a discrimina-
tive character transducer that uses a set of within-
word features to learn the transformations from in-
put data consisting of a lexicon with full morpho-
logical paradigms and unlabelled texts. They show
that the joint model outperforms the pipeline model
where the POS tag is used as input to the lemmati-
sation component.
6 Conclusion
We have shown that redefining the task of lemma-
tisation as a category tagging task and using an ef-
ficient tagger to perform it results in a performance
that is at the state-of-the-art level. The adaptive gen-
eral classification model used in our approach makes
use of different sources of information that can be
found in a small annotated corpus, with no need for
comprehensive, manually constructed morphologi-
cal dictionaries. For this reason, it can be expected
to be easily portable across languages enabling good
quality processing of languages with complex mor-
phology and scarce resources.
7 Acknowledgements
The work described in this paper was partially
funded by the Swiss National Science Foundation
grants CRSI22 127510 (COMTIS) and 122643.
371
References
Grzegorz Chrupa?a. 2006. Simple data-driven context-
sensitive lemmatization. In Proceedings of the So-
ciedad Espan?ola para el Procesamiento del Lenguaje
Natural, volume 37, page 121131, Zaragoza, Spain.
Tomaz? Erjavec and Sas?o Dz?eroski. 2004. Machine learn-
ing of morphosyntactic structure: lemmatizing un-
known Slovene words. Applied Artificial Intelligence,
18:17?41.
Tomaz? Erjavec. 2010. Multext-east version 4: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), pages 2544?2547, Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Andrea Gesmundo. 2011. Bidirectional sequence clas-
sification for tagging tasks with guided learning. In
Proceedings of TALN 2011, Montpellier, France.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences - Computer Science and Computational Bi-
ology. Cambridge University Press.
Jan Hajic?. 2000. Morphological tagging: data vs. dic-
tionaries. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 94?101, Seattle, Washington.
Association for Computational Linguistics.
Bart Jongejan and Hercules Dalianis. 2009. Automatic
training of lemmatization rules that handle morpholog-
ical changes in pre-, in- and suffixes alike. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP,
pages 145?153, Suntec, Singapore, August. Associa-
tion for Computational Linguistics.
Matjaz? Jurs?ic?, Igor Mozetic?, Tomaz? Erjavec, and Nada
Lavrac?. 2010. LemmaGen: Multilingual lemmatisa-
tion with induced ripple-down rules. Journal of Uni-
versal Computer Science, 16(9):1190?1214.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 760?
767, Prague, Czech Republic. Association for Compu-
tational Linguistics.
Kristina Toutanova and Colin Cherry. 2009. A global
model for joint lemmatization and part-of-speech pre-
diction. In Proceedings of the 47th Annual Meeting
of the ACL and the 4th IJCNLP of the AFNLP, page
486494, Suntec, Singapore. Association for Computa-
tional Linguistics.
372
