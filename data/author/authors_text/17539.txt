Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 477?481,
Dublin, Ireland, August 23-24, 2014.
RelAgent: Entity Detection and Normalization for Diseases in
Clinical Records: a Linguistically Driven Approach
Sv Ramanan
RelAgent Tech Pvt Ltd
Adyar, Chennai
India
ramanan@relagent.com
Senthil Nathan
RelAgent Tech Pvt Ltd
Adyar, Chennai
India
senthil@relagent.com
Abstract
We refined the performance of Co-
coa/Peaberry, a linguistically moti-
vated system, on extracting disease en-
tities from clinical notes in the train-
ing and development sets for Task 7.
Entities were identified in noun chunks
by use of dictionaries, and events (?The
left atrium is dilated?) through our own
parser and predicate-argument struc-
tures. We also developed a mod-
ule to map the extracted entities to
the SNOMED subset of UMLS. The
module is based on direct matching
against UMLS entries through regu-
lar expressions derived from a small
set of morphological transformations,
along with priority rules when multi-
ple UMLS entries were matched. The
performance on training and develop-
ment sets was 81.0% and 83.3% respec-
tively (Task A), and the UMLS match-
ing scores were respectively 75.3% and
78.2% (Task B). However, the perfor-
mance against the test set was low
by comparison, 72.0% for Task A and
63.9% for Task B, even while the pure
UMLS mapping score was reasonably
high (relaxed score in Task B = 91.2%).
We speculate that our moderate perfor-
mance on the test set derives primarily
from chunking/parsing errors.
1 Introduction
The increasing use of electronic health records,
both for satisfying mandatory requirements as
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and
proceedings footer are added by the organisers. Licence
details: http://creativecommons.org/licenses/by/
4.0/
well as for administrative reasons, has cre-
ated a need for systems to automatically tag
and normalize disease/sign/symptom men-
tions. Statistically significant correlations
extracted from automated analysis of large
databases of clinical records are felt to be use-
ful in detecting phenotype-genotype correla-
tions (reviewed in Kohane (2011)), phenotype-
phenotype correlations (Roque et al., 2011) as
well as in continuous monitoring of events such
as adverse reactions and even early detection
of outbreaks of epidemics/infectious diseases
(Botsis et al., 2013; Collier, 2012). In this
context, Task 7 of SemEval 2014, which is
a continuation of the ShARe/CLEF eHealth
2013 task (Pradhan et al., 2013), provides a
testbed to evaluate systems that automatically
tag and normalize mentions of diseases, signs
and symptoms in clinical records, which in-
clude discharge summaries and echo, radiology
and ECG reports.
Our system consists of (i) Cocoa, a chunk-
based entity tagger and (ii) Peaberry, a parser,
followed by a module for predicate-argument
structure. We have tested the system in a va-
riety of tasks, such as detecting and normal-
izing mentions of chemicals, proteins/genes,
diseases and action terms in the BioCreative
13 Chemdner and CTD tasks (Ramanan and
Senthil Nathan, 2013a; Ramanan and Senthil
Nathan, 2013b), as well as in detecting cel-
lular and pathological events in the BioNLP
cancer genetics task (Ramanan and Senthil
Nathan, 2013c); we also participated in the
eHealth 2013 task (Ramanan et al., 2013d).
Throughout, we have retained a common core
platform for simultaneous detection of a mul-
tiplicity of entity types as well as for chunk-
ing and parsing; we restrict task-specific op-
timization primarily to post-processing mod-
ules. While this strategy may not be optimal
477
for any individual task, we feel that it is neces-
sary for multi-document spanning tasks such
as literature-based discovery (Swanson, 1988),
where connections are established across a va-
riety of scales, e.g. from molecular events
to patho-physiological phenotypes. Moreover,
these linkages need to be made across a mul-
tiplicity of documents from various sources,
which encompass a linguistic range from com-
plex syntactical utterances in biomedical pub-
lications to free-form phrase-centered clinical
notes.
We refined performance against the pro-
vided training and development sets, with
reasonable performance in Task A (relaxed
f = 0.94, strict f = 0.81 ? 0.83, strict recall
0.80 ? 0.82). A module to match text from
gold-annotated exact spans to UMLS codes
also achieved reasonable performance for Task
B (relaxed accuracy = 0.94?96). However, the
results against from the test set were quite low
for Task A, (relaxed f = 0.87, strict f = 0.72,
strict recall = 0.70) as well as for Task B
(strict f = 0.64). Comparatively, the module
for UMLS normalization fared better (relaxed
f = 0.91 in Task B). We speculate that the
test set contains entities that are rare in the
training/development sets which were chun-
ked incorrectly, and also that the parse errors
in the test set arose from syntactic structures
missing in the training sets. It is possible that
a post-processing statistical module trained on
a combination of gold annotations as well as
linguistic output may be needed for improv-
ing the performance of our system on clinical
notes.
2 System description
The basic structure of the entity-tagging
system is unchanged from that used in
Share/CLEF eHealth 13 (Pradhan et al.,
2013) and BioNLP-ST 13. In summary, the
system comprises of a sentence splitter, fol-
lowed by a TBL-based POS tagger and chun-
ker, entity tagging at the single-token level,
a module to handle multi-word entities, a
noun phrase coordination module, a depen-
dency parser (Ramanan and Senthil Nathan,
2013c), and finally a semantic module to tag
disease-related events.
The generic system has dictionaries and
morphological rules for detecting diseases and
body parts. However, there are many exten-
sions needed for clinical notes, which (i) make
extensive use of common words and phrases
for describing symptoms, which requires word
sense disambiguation, (ii) use unusual phrases
for signs and symptoms and (iii) are full of
undefined acronyms. We isolated such special-
ization to disease-related entities within noun
phrases in clinical documents inside a subrou-
tine in the multi-word tagger module. These
were identified by a frequency-based analysis
of words and phrases in the training and de-
velopment corpora. Thus, a few ambiguous
words and phrases such as ?crackles?, ?com-
plaints?, ?mass effect? and ?focal consolidation?
were tagged as disease markers regardless of
context. Generally, however, even common
clinical words such as ?redness? and ?swelling?
were tagged only in the presence of neighbor-
ing context words. The appearance of major
body parts such as ?Abdomen?, ?Neck, ?Ex-
tremities? at the beginning of a line followed
by a colon or a hyphen was taken as a dis-
course reference marker for the rest of the line
to tag acronyms such as ?NT/ND? and dan-
gling adjectives such as ?soft? and ?warm?. Very
common acronyms (? 100) both for anatomi-
cal parts (?LUQ?) and diseases (?DMII?) were
also tagged inside the specialized subroutine,
as were common abbreviations (?regurg? for re-
gurgitation) and words with common spelling
errors. Finally, some event/process words
which we found to almost always represent
clinical conditions in the training text were
tagged as disease markers. Examples are ?as-
piration?, agitation? and ?confusion?.
We also extended our generic event pro-
cessing module with a task-specific routine
to take into account descriptions of (mostly)
signs/symptoms specific to clinical documents.
These fall into several categories: (i) abnor-
mal changes in body parts or organ systems,
such as ?The left atrium was moderately en-
larged?, ?Nose is bloody? and ?redistribution of
pulmonary blood flow? (ii) symptoms such as
?The patient was unable to walk?, ?His speech
was slurred?, ?He had difficulty breathing? and
?alteration of consciousness? (iii) changes in pa-
rameters marked by phrases/clauses such as
?elevation of troponin?, ?QR interval was pro-
478
longed? and ?decreased blood sugar?. Certain
environmental conditions such as ?exposure to
asbestos? were also handled. Finally, events
with a default animate theme were tagged
regardless of their actual arguments to han-
dle sentences/phrases where our syntax mod-
ule failed to extract the correct theme or the
theme is to be inferred from the discourse; the
? 40 words in this set included verbs such
as ?vomit?, ?shivering?, ?lethargic?, ?violent? and
?somnolent?.
The above treatment served to demar-
cate spans for diseases that overlap with
the gold annotations. The system merges
words/phrases denoting a body part with ad-
joining words that denote diseases, and also
merges words denoting severity into the dis-
ease span, since our system design strategy
was to generate the longest contiguous span
that can refer to a disease. However, the pri-
mary score in the shared task are with re-
spect to exact matches with the gold anno-
tations. We therefore wrote a small post-
processing module to omit words in an approx-
imate match that refer to severity (?acute?) as
well as to excise phrases dealing with intra-
organ parts or their location (such as ?lobes?
or ?left/right?) - such words/phrases are usu-
ally omitted from the UMLS descriptions of
diseases to which the gold annotations hew
closely. Also, we noticed that certain words
such as ?wounds? and ?lesions? do not embed
an anatomical entity within their description
in the gold annotations. Yet another point is
that, while parameters are marked up as in-
dicative of a symptom only when they take on
abnormal values (?elevated LDL?), the direc-
tion of change is almost always omitted from
the gold annotations. Descriptors of the pa-
tient (?He?) are also excised. Altogether, we
constructed about 40 rules to trim the approx-
imate span into one more conformant to the
exact form in the gold annotations.
Task B requires mapping diseases phrases
into the SNOMED subset of UMLS as spec-
ified in the task description. We proceeded
on the assumption that the exact (gold)
entity spans were constructed by annota-
tors to closely map into the UMLS descrip-
tions. Accordingly, we used the text as
defined by the gold spans and attempted
to map them directly into the UMLS def-
initions after some preprocessing steps that
constructed a regular expression: (a) com-
mon spelling errors were corrected (b) body
part and disease acronyms were expanded
(c) common variants were added as alter-
nates i.e. ?tumou?rs?? were expanded into
?(tumou?r|neoplasm|carcinoma)s?? (d) adjec-
tival and nominal variants were added e.g.
both ?atrium? and ?atrial? were converted into
?(atri)(al?|um)?, and more generally, adjec-
tival endings were generalized, for example,
the ending ?ic? was converted into ?(i[ac]|ism)?.
(e) singular and plural forms were converted
into choices e.g. ?artery? was rendered as
?arter(y|ies)?.
Altogether, we have ? 120 rules for vari-
ant morphological forms, covering adjectives,
nouns and number. The resulting regular ex-
pression was directly matched (using ?grep?)
against UMLS text entries. Generally, sev-
eral matches were found. Matches against the
defining entry (the first one) were prioritized,
otherwise the entry with the largest CUID was
taken. Finally, we noted that some UMLS
CUID?s were preferred to others; for exam-
ple, ?C0007115 - Malignant neoplasm of thy-
roid? is preferred to ?C0549473 - Thyroid car-
cinoma?. The preferred choices were inferred
from gold annotation frequencies, and corre-
spond to ? 100 remapping rules.
3 Results and Discussion
With a few minor changes to the system used
in the Share/CLEF 2013, we obtained a re-
laxed f-measure in Task A of 0.88 in the
training and development sets. Thereafter
we alternately refined performance in Task A
against the provided training set using the
development set as a testbed, or vice versa.
As described in the last section, these re-
finements took the form of adding context-
sensitive rules for disease-related words and
phrases in order of their frequencies in the
training/development sets. While we could
thereby improve performance against both
training and development sets (relaxed f =
0.94), we noticed that improvements in the
performance against the training set did not
correlate with better performance against the
development set and vice versa, probably im-
479
plying that 6% or more of the entities are
unique to each set, or that we were unable to
catch similarities. A similar orthogonal situa-
tion resulted in our attempt to improve perfor-
mance against exact matches on the training
and development sets, strict f = 0.81 ? 0.83,
strict recall 0.80 ? 0.82. The observation of
orthogonal entity sets in different datasets for
about 6% of entities is seemingly validated in
the test set, where the results showed a re-
laxed f = 0.87, which is quite close to the
baseline performance (0.88 in the Share/CLEF
2013 task); the highest scoring system had re-
laxed f = 0.91 by comparison. We speculate
that our insistence on contextual clues for en-
tity tagging is another cause for low relaxed
performance on the test set.
Performance of the system for exact
matches on the test set (strict f = 0.72)
suffered greatly in comparison to the train-
ing/development sets. This could be partly
ascribed to the 7% lower performance on the
relaxed f-score (i.e. we missed many entities
altogether) from 0.94 in training/development
sets to 0.87 in the test set. Even account-
ing for this, there is an additional perfor-
mance drop of about 3?4% in exact match on
the test set compared to training/development
sets. One implication is that that our rule-
base method for pruning approximate matches
to exact spans is probably sub-optimal, and
should be supplemented or replaced by a sta-
tistical algorithm. As noted earlier, gold anno-
tations are probably made by annotators with
respect to UMLS definitions, and have some
degree of arbitrariness associated with them
depending on the granularity of the UMLS def-
inition e.g. in the choice of whether to remove
or retain a body location in the gold span.
Given the size of the UMLS definition set, a
statistical approach is probably likely to do
better than a rule-based system in the task of
reducing approximate matches to exact spans.
The poor performance in Task A (strict
recall = 0.70) directly impinges on our low
?strict? score in Task B (= 0.64); this score is
simply a product of the strict recall in Task A
and the accuracy of mapping to UMLS, where
the latter score is given by the Task B ?relaxed?
score (= 0.91). An interesting feature is the
mapping accuracy for our system on the test
set suffered a relatively small drop when com-
pared to the mapping accuracies on the train-
ing and development sets, which were 0.94 and
0.96 respectively. We interpret this reasonably
high figure for the mapping score (the best
among the top 10+ teams in Task B) as vali-
dation of our hypothesis that gold annotations
are made with respect to UMLS definitions,
which also strengthens the case (made above)
for the need to incorporate a (semi-)statistical
approach for pruning overlap matches to exact
matches in our system.
Clinical documents are terse and full of
phrasal observations and incomplete sen-
tences, often with missing punctuation. We
have adapted a linguistically based system
to detect disease-related entities and events
with moderate performance; our observation
on the training/development sets is that most
errors arise from parsing/ chunking errors
on grammatically incomplete phrases. The
second task, namely mapping disease-related
entities/events to SNOMED/UMLS, requires
tagged entity spans to correspond closely to
UMLS definitions; system performance in this
regard can probably be usefully supplemented
by statistical approaches. Given proper entity
spans, a small set of morphological transfor-
mations gives high performance in mapping
to UMLS ID?s. We speculate that a chunk-
annotated corpus of clinical records may help
in improving performance for linguistically de-
rived systems.
References
Isaac S. Kohane. 2011. Using electronic health
records to drive discovery in disease genomics.
Nat Rev Genet. 2011 Jun;12(6):417-28.
Francisco S. Roque, Peter B. Jensen, Henri-
ette Schmock, Marlene Dalgaard, Massimo An-
dreatta, Thomas Hansen, Karen Soeby, Soren
Bredkjor, Anders Juul, Thomas Werge, Lars J.
Jensen and Soren Brunak. 2011. Using elec-
tronic patient records to discover disease corre-
lations and stratify patient cohorts. PLoS Comp.
Bio. 7(8):e1002141.
Sv Ramanan and Senthil Nathan. 2013. Perfor-
mance of a multi-class biomedical tagger on the
BioCreative IV CTD task. Proceedings of the
Fourth BioCreative Challenge Evaluation Work-
shop vol. 1. Bethesda, MD.
480
Sv Ramanan and Senthil Nathan. 2013. Adapt-
ing Cocoa a multi-class entity detector for the
CHEMDNER task of BioCreative IV. Proceed-
ings of the Fourth BioCreative Challenge Eval-
uation Workshop vol. 2. Bethesda, MD.
Sv Ramanan and Senthil Nathan. 2013. Perfor-
mance and limitations of the linguistically moti-
vated Cocoa/Peaberry system in a broad biomed-
ical domain. Proceedings of Workshop. BioNLP
Shared Task 2013. ACL. Sofia.
Sv Ramanan, Shereen Broido and Senthil Nathan.
2013. Performance of a multi-class biomedi-
cal tagger on clinical records. Proceedings of
ShARe/CLEF eHealth Evaluation Labs.
Don R. Swanson. 1988. Migraine and Magnesium:
Eleven Neglected Connections. Persp. Bio. Med.
31(4), 526-557.
Sameer Pradhan, Noemie Elhadad, Brett R.
South, David Martinez, Lee Christensen, Amy
Vogel, Hanna Suominen, Wendy W. Chapman
and Guergana Savova. 2013. Online Work-
ing Notes of the CLEF 2013 Evaluation Labs
and Workshop. Proceedings of ShARe/CLEF
eHealth Evaluation Labs, 23-26 September, Va-
lencia, Spain
Taxiarchis Botsis , Michael D. Nguyen , Emily
J. Woo, Marianthi Markatou and Robert Ball.
2011. Text mining for the Vaccine Adverse
Event Reporting System: medical text classifica-
tion using informative feature selection. J Am
Med Inform Assoc. 2011 Sep-Oct;18(5):631-8
Nigel Collier. 2012. Uncovering text mining: A
survey of current work on web-based epidemic
intelligence. Glob Public Health. Aug 2012;
7(7): 731-749.
481
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 86?93,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Performance and limitations of the linguistically motivated Cocoa/Pea-
berry system in a broad biomedical domain.
S. V. Ramanan
RelAgent Private Ltd.
56, Venkatratnam Nagar
Adyar, Chennai 600020
ramanan@npjoint.com
P. Senthil Nathan
RelAgent Private Ltd.
56, Venkatratnam Nagar
Adyar, Chennai 600020
senthil@npjoint.com
Abstract
We tested a linguistically motivated rule-
based system in the Cancer Genetics task 
of the BioNLP13 shared task challenge. 
The performance of the system was very 
moderate, ranging from 52% against the 
development set to 45% against the test 
set. Interestingly, the performance of the 
system did not change appreciably when 
using only  entities tagged by the inbuilt 
tagger as compared to performance using 
the gold-tagged entities. The lack of an 
event anaphoric module, as well as prob-
lems in reducing events generated by a 
large trigger class to the task-specific 
event subset, were likely major contribut-
ory factors to the rather moderate per-
formance.
1 Introduction
The Cancer Genetics (CG) task of the BioN-
LP-13 shared task (Pyysalo et al, 2013) has 
event types defined from a strict subset of GO 
biological processes. However, the events in the 
CG task have arguments that span a range of en-
tities from molecules to system-wide processes, 
the latter focused primarily on cancer. Thus the 
CG task is an interesting case-study for text min-
ing from a biological point of view, in that the 
task spans the literature from molecular events to 
behaviors linked to phenotypes, and thus con-
siders a broader context than earlier BioNLP 
shared tasks (Kim et al, 2009, 2011). 
An early article by Swanson (1988) explored 
the value of literature-based discovery (LBD) in 
discovering relations that span scientific sub-spe-
cializations. The LBD program of Swanson in-
volves 3 nominally independent subtasks: (i) ac-
curate representations of events within a docu-
ment (b) normalization of entities to a standard 
representation to facilitate inter-document span-
ning and (c) a strategy to span event graphs 
across multiple documents. We explored the CG 
task primarily in the context of subtask (a) of this 
LBD program.
2 Methods
Our system currently consists of the following 
major components (a) Cocoa, a NER module that 
detects over 20 biomedical entity classes,  in-
cluding macromolecules, chemicals, 
protein/DNA parts, complexes, organisms, pro-
cesses, anatomical parts, locations, physiological 
terms, parameters, values, experimental tech-
niques, surgical procedures, and foods and (b) 
Peaberry, a 'stitcher' that combines local predic-
ate-argument  structures  to produce a  depend-
ency-parse like output. The system also resolves 
sortal/pronominal anaphora and coreferences.
2.1 Entity detection
As entity detection is not part of the CG task, 
we provide only a brief overview of this module. 
However, as we did not use the entities provided 
by the event organizers on the test set, this de-
scription may be of interest given that our results 
with and without gold entities on the develop-
ment and test sets are comparable (please see the 
Results section below).
The Cocoa entity detection system consists of 
the following modules run as a pipeline: (a) sen-
tence boundary detection (b) acronym detection 
(c) a POS tagger based on Brill's tagger, post-
modified for the biological domain (d) a fnTBL-
based chunker, also heavily postmodified for the 
biomedical domain (e) an entity tagging module, 
86
driven by dictionaries based both on words as 
well as morphological features, primarily pre-
fixes and suffixes for biomedical entities, but 
also using infixes for chemical entities (f) entity 
tag based correction of chunks, primarily mis-
tagged VP chunks (g) a narrow context/trigger 
based tagging of entities that are orthographically 
defined (presence of caps or numbers) such as 
assigning a protein tag for Cx43 from the phrase 
'phosphorylation of Cx43' (h) a multi-word entity 
aggregator (i) a shallow coordination module for 
NPs (j) a limited set of hypernymic and apposi-
tional relations, followed by reuse of tags for or-
thographically defined unlabeled entities (k) a 
chemical formula detector. The entity tagger per-
forms reasonably against proteins, anatomical 
parts and diseases as evaluated against existing 
tagged datasets (RelAgent, 2012).
2.2 Event Detection
The main steps here are: (a) detecting voice/fi-
niteness of verbs (b) predicate-argument  struc-
ture  extraction  for trigger words (c) argument 
merging and discourse connective parser (d) ana-
phora detection (e) discourse-connective based 
filling of empty themes and (f) sense disambigu-
ation (WSD) of trigger words based on argument 
structure.  A block-level pipeline of the system is 
given in Figure 1.
Figure  1.  Block  level  pipeline  of  the  system. 
Blocks with a light gray background are part of 
the  event  detection  system (Peaberry),  and  are 
discussed here. The other blocks are part of the 
Cocoa entity tagger. WSD = Word sense disam-
biguation.
 
We will use a single sentence throughout to il-
lustrate processing by the various modules:
 "Concomitantly, immunostaining for apoptosis 
inducing factor (AIF) showed a time-dependent 
translocation from the mitochondria to the nucle-
us."
2.2.1 Voice detection
The voice detection module uses about 150 
rules to detect the voice of a verb. It also classi-
fies the verb as finite/nonfinite while marking its 
presence in a reduced or finite relative clause. 
The module determines these various aspects of 
a verb primarily with the local context, but uses 
the aspects of a previous verb in cases of co-
ordinated verbs. Voice detection is facilitated by 
specific handling of (a)  middle verbs, which ap-
pear to be in the active voice, but whose theme is 
the subject ('The protein translocated to the nuc-
leus')  (b) ergative verbs,  which act  like middle 
verbs when they do not have a direct object, but 
behave regularly when they are used transitively 
('Protein levels increased' vs 'Application of the 
chemical increased protein levels') (c) intransit-
ives, which are verbs that do not take a direct ob-
ject, but whose subject is the agent ('The patient 
fell'), (d)  verbs in the active voice, but with an 
object separated from the verb  by a  preposition 
('leads to', 'resulted in', 'binds to'). Voice markup 
is therefore determined primarily by the roles of 
the  subject/object,  and  is  thus  a  little  different 
from  the  voice  markings  as  conventionally 
defined.
In the sample sentence, there is only one verb, 
and the output reads:
"[ Concomitantly AV] , [ immunostaining NP] 
for [ apoptosis inducing factor (AIF) NP] [ 
showed VP_Af] [ a time -dependent transloca-
tion NP] from [ the mitochondria NP] to [ the 
nucleus NP] ."
where the verb phrase 'showed' is in the active 
voice ('A') and is finite ('f'). Another sentences 
better illustrates a wider range in voice markings:
[ Adult naive T cells NP] , which [ are 
VP_Pfcr] at [ rest NP] in [ normal conditions 
NP] , [ proliferate strongly VP_Pf] when [ trans-
ferred VP_Pnd] to [ lymphopenic hosts NP] .
Here 'VP_Pfcr' stands for passive voice ('P'), fi-
nite ('f'), copula ('c'), and relative clause ('r'), 
while 'VP_Pnd' stands for Passive ('P'), non-fin-
ite ('n') and reduced ('d').
87
2.2.2 Argument extraction
Local arguments are extracted for all verbs in 
a sentence, as well as all nominals marked as po-
tential  triggers  by  the  entity  tagger.  Currently, 
there  are  approximately  60  classes  of  predic-
ate-argument structures based both on the partic-
ular prepositions heading noun phrases as well as 
entity tags; these classes cover about 500 specific 
trigger words. Additionally, there are generic ar-
gument  structures  for  verbs  and  nominals  not 
covered in the specific classes above. We accom-
modate  3  additional  arguments  apart  from  the 
agent/theme, such as FromLoc, ToLoc and AtLoc 
for movement-type trigger words. In addition, we 
also mark the subject/object nature of the argu-
ments. 
The argument  structures  for  the  sample sen-
tence are shown in a pipe separated format (verb|
cause|theme):
immunostaining  |  -  |  apoptosis  inducing  factor 
(AIF)
showed  |  immunostaining  |  a  time  -dependent 
translocation
-dependent | time | translocation
translocation | - | - | FromLoc:the nucleus| ToLoc: 
the mitochondria
2.2.3 Argument stitching and connectives
We link argument  structures  for  individual 
triggers  by looking for missing syntactical con-
stituents for verbs (subject/object) or semantic 
constituents for nominals (agent/theme). For 
verbs, we use the voice/finite aspects of the cur-
rent verb to locate previous verbs with which the 
current verb is associated with, either through 
embedding or by coordination. For example, in 
the sentence fragment: '...  had no effect on the 
ability  of  beta-adrenergic  agonists  to  stimulate 
internalization  of  beta2ARs  ,  but  blocked  the 
ability of ...?,  'blocked' coordinates with the fi-
nite  verb  'had'  but  not  with  the  non-finite  'to 
stimulate'. An example of an embedding is: 'With 
major  interfering  currents  inhibited,  NaCaEC 
was measured as the current that is sensitive to 
the  nickel  (Ni)  during  a  descending  voltage 
ramp.'. Here the VP  'was measured' is finite, and 
this allows its object 'the current' to be identified 
as the subject of 'is sensitive'. 
Other examples of rules for resolving the argu-
ments  of  relative  clauses  (RCs)  are:  (a)  Dis-
course  connectives  ('whereas,  'whereby, 
'because')  form  clausal  boundaries  and  should 
not be crossed (b) Certain coordination markers 
('besides',  'via')  also  should  not  be  crossed  for 
RC's (c) If an RC is recognized as coordinated 
with a prior RC, the arguments are transferred.
A general point in inferring missing arguments 
is that the nature of the current trigger word can 
also determine the nature  of  the  induced argu-
ment. Certain trigger words ('induce', 'cause', en-
hance',  'prevent')  can take an event as an argu-
ment ,  although most  trigger  words  do not 
(theme argument for 'methylation'). Triggers in 
the former class are primarily regulatory actions 
and/or belief statements, which can take a clause 
or  a  nominal as an  argument.  The  distinction 
between these two types of trigger words is re-
lated  to  that  between  'embedding  propositions' 
and 'atomic propositions' noted in Kilicoglu and 
Bergler  (2012).  An  example  is:  'Promoter 
methylation may interfere with AP1 binding to 
the  promoter  to  cause  aberrant  Cx43 gene  ex-
pression.', where it is the interference that causes 
aberrant expression.
The stitcher/parser  does not examine  the in-
ternal structure of chunks to locate missing argu-
ments  for  predicates.  This  rule  is  violated  for 
trigger  words  that  can  accept  events  as  argu-
ments, where the presence of an event trigger (as 
marked  by  the  NER  tagger)  inside  a  NP  is 
checked  for.  While  this  makes  the  process  in 
some sense 'domain-neutral',  it  may also  intro-
duce errors unless the  predicate-argument rules 
are  complete  and comprehensive for individual 
triggers.
The parser also locates discourse connectives 
('whereas', 'because' , 'via', 'when') and assembles 
argument frames for these connectives, based on 
finiteness of verbs when possible. Connectives 
('by') that can take nominals as arguments ('Loc-
alization ... by fusing') are also handled by the 
parser. Hypernymic and appositional relations 
are also detected at this stage. A final check loc-
ates all unattached prepositional phrases in the 
sentence and attaches them as verbal phrases to 
the nearest verb in a greedy step. At any point, 
the parser looks back no more than 2 verbs back 
for resolution, with parse time thus ~ O(2x), 
where 'x' is the number of trigger words in a sen-
tence.
We  recognize  that  the  description  of  the 
'stitching' process above is somewhat brief, but 
feel that a full description may not be appropriate 
here due to the large number of rules and interde-
pendencies in the system. We note that:  (a) the 
final output of the process is similar to a depend-
ency parse, except that semantic roles are identi-
fied (b) the stitching is done in a shallow manner, 
88
with two verbs look-back at most, and is hence 
reasonably fast and (c) the implementation is our 
own, and does not borrow from existing parsers. 
We plan to describe this system in greater detail 
in a separate publication elsewhere.
As an example, in the paraphrase 'X activates 
Y to increase Z', the arguments are:
activates | X | Y
increase  | - | Z
and the stitcher recognizes the infinitival 'to' con-
struct, and transfers the previous event as the 
agent for 'increase':
activates | X         | Y
increase  | activates | Z
2.3 Anaphora
We implemented the algorithm of Lappin and 
Leass (1994) for pronominal anaphora, as imple-
mented by Kennedy and Boguraev (1996), with 
additional weights for matching entity tags for 
headwords. The weights were refined against 
handpicked abstracts, but are  yet to be com-
pletely validated. In addition, we also resolved 
sortal anaphora  ('this protein', 'these genes') and 
prenominal anaphora ('its binding partner', 'their 
properties')  by the same rules  as  used for  pro-
nominal anaphors ('it', 'they'), but with different 
weights. We also implemented event anaphora, 
i.e. reference of one trigger word to another trig-
ger word with the same root (lemma) or another 
event in the same class (for regulation triggers). 
Due to lack of time, we could not completely test 
the performance of event anaphora, and they 
were dropped in the test set. Coreference resolu-
tion with the determiner 'the' ('the gene') was not 
implemented.
2.4 Transferring arguments across events
Certain arguments can be resolved by compar-
ing argument structures for events linked by dis-
course connectives (DCs), such as :
'found to overexpress eph mRNAs without 
gene amplification' (DC: 'without')
'Upon retroviral transduction of the mouse c-
myc gene, Rat 6 cells showed mildly altered 
morphology' (DC: 'Upon')
'SCAI acts on the RhoA-Dia1 signal transduc-
tion pathway and localizes in the nucleus, where 
it binds and inhibits the myocardin-related tran-
scription factor MAL by forming a ternary com-
plex with serum response factor (SRF).' (ana-
phoric resolution for 'it' followed by a discourse 
connective:'by')
When events are linked by a discourse con-
nective, arguments can be transferred if the 
events are in the same event class. Even if the 
events are of different classes, the theme can be 
transferred if it satisfies the entity type con-
straints of the recipient event. Further, certain be-
lief/demonstration trigger words ('display', 
'show', 'exhibit', 'demonstrate') that take an event 
as the theme have a similar structure: 'Cloning of 
a  human  phosphoinositide  3-kinase  with  a  C2 
domain  that  displays  reduced sensitivity  to  the 
inhibitor wortmannin.' or 'X exhibits cytotoxicity 
against  cell  lines'.  Agent arguments for such 
verbs are transferred to the appropriate argument 
slot of the theme event. In certain contexts, verbs 
such as  'act'   which can take  an infinitival  'to' 
complement behave similarly:   'p15 may act as 
an effector of TGF-beta-mediated cell cycle ar-
rest.'.
For the sample sentence, the trigger/belief 
word 'showed' causes a transfer of the theme slot 
of its cause process ('immunostaining', a 
Planned_process in the CG task) to the same slot 
in the theme event ('translocation'):
immunostaining | - | apoptosis inducing factor 
(AIF)
showed | immunostaining | a time -dependent 
translocation
-dependent | time | translocation
translocation | - | apoptosis inducing factor (AIF) 
| FromLoc:the nucleus| ToLoc: the mitochondria
2.5 Runtimes
The run-time of the system is about 100 
ms/sentence on a 2007 vintage dual-core system. 
This time was estimated by processing whole ab-
stracts varying from 10-15 sentences. This figure 
includes the time for all components, including 
entity recognition, parsing, intra-document ana-
phora resolution (both sortal/pronominal and 
event), event extraction and final A1/A2 output. 
The extrapolated time of processing for the entire 
Medline corpus (1.2 x 10^8 sentences in 2013) is 
about 180 CPU-days.
3 Results
We first tested the system against the develop-
ment  set  by  using  the  internal  entity  detector 
(Cocoa) to tag entities, and using these tags alone 
till  the  end of  the  event  extraction  phase,  and 
only then remapping the Cocoa-tagged entities to 
89
entities in the gold annotations ('a1' entities) giv-
en by the task organizers. This gave a score (f-
measure) of 52.2% with the evaluation options '-
s  -p'  which  stand  respectively  for  soft  span 
matching  and  partial  recursive  matching.  We 
then reran the event extraction module after re-
moving  all  internally  generated  entity  tags  for 
chemicals, proteins and anatomical parts and tag-
ging only such entities as were specified in the 
gold 'a1' files. To our surprise, the f-measure was 
2% lower on the development set when using the 
gold  entities.  This  probably  indicates  an  un-
wanted dependence of the event extraction mod-
ule on some peculiarities in the way the internal 
Cocoa tagger tags entities. We are currently ana-
lyzing the results for such dependencies (see Dis-
cussion  for  some  examples).  Nevertheless,  the 
results  are  encouraging in  that  the  system per-
formance is similar with or without reference en-
tities and thus may be indicative of performance 
on a new document collection where entities are 
not specified manually beforehand.
As the task allows only one submission,  we 
submitted the results of the system with entities 
tagged by the internal tagger and mapped only at 
the  end  to  the  gold  tagged  entities.  This  was 
based on the better performance of this approach 
against the development set. However, the results 
of the system were considerably lower on the test 
set (f = 45.3%; best score by TEES 2.1 system = 
55.4%; Pyysalo et  al.,  2013).  Using  the evalu-
ation portal for the test dataset, the results with 
gold-tagged  entities  improved  the  performance 
only by 0.3%, confirming that, at least at the per-
formance levels of this system, the inbuilt Cocoa 
entity tags can substitute for pre-annotated entit-
ies.
The  performance  on  the  test  set  was  low 
primarily  against  the  events  in  the  regulation 
class (f=35.6%),  which form about 40% of the 
events in both the test and development sets. This 
is  similar  to  the  result  in  the  development  set, 
where  the  performance  in  the  regulation  class 
was also quite low at f=37%. Part of the reason 
for this is that the system's rules for regulatory 
triggers generally give preference to other events 
over entities as causes/agents. Thus for example 
in  the  sentence  fragment  (PMID:21963494) 
'AglRhz  induced  activation  of  caspase-3  and 
poly(ADP-ribose) polymerase (PARP), and DNA 
fragmentation in HT-29 cells, leads to induction 
of apoptosis as well as suppression of tumorigen-
icity of HT-29 cells.', the gold annotations state 
that  'AglRhz'  is  the  cause for  the  trigger  word 
'leads',  while  the  Peaberry  system  prefers  the 
trigger  word  'induced'  for  the  causative  agent. 
However, we have not done a detailed study to 
examine  if  such  differences  account  for  more 
than a small minority of the errors that contribute 
to low performance in the regulatory class. Over-
all, and surprisingly for a rule-based system, the 
precision  was  quite  low  on  both  the  test  set 
(49%) and the development set (54%). The low 
overall  precision was dominated by the corres-
ponding number for regulatory events (37% and 
44% on test and development sets respectively), 
but  the  precision of  non-regulatory events  was 
quite dismal as well (please see Discussion sec-
tion below). 
The  low recall  for  regulatory  events  can  be 
caused by low recall for those primary (i.e. non-
regulatory) events that are regulated. In the de-
velopment set, the recall for these non-regulatory 
classes varied between 55% and 75%, but in the 
test set the recall for some primary event classes 
(Pathology and General event classes) dropped to 
~30-40% (see Table 1 below). Another reason for 
low recall is the absence of themes for primary 
events  when these  themes  are  lifted/transferred 
from mentions of the same trigger word in previ-
ous  sentences.  Our  lack  of  a  event  anaphora 
module would thus certainly have contributed to 
the low recall  for such primary events.  We are 
analyzing the gold annotations to determine other 
causes for the low precision and recall in the de-
velopment dataset.
Event Class Recall Precision Fscore
Anatomy 63.34 80.29 70.82
Pathology 43.30 54.20 48.14
Molecule 57.46 64.38 60.72
General 34.67 49.82 40.89
Regulation 34.22 37.05 35.58
Modifier 26.24 37.50 30.88
Total 41.73 49.58 45.32
Table 1. Summary of results for the Test set. Re-
call,  precision and F-score are shown for event 
classes  for anatomical  changes,  pathology,  mo-
lecular processing events,  general events (bind-
ing and movement), regulatory events, modifiers 
(negation and speculation) and the total score.
4 Discussion
We have developed a rule-based linguistically 
motivated system for tagging entities and extract-
ing events from biomedical documents.  A major 
90
problem with our linguistically-based system is 
the large open-ended number of trigger words 
that generate events. This explosive event gener-
ation occurs as the system generates predicate ar-
gument structures for all verbs in a document as 
well as for generically defined nominal processes 
(which are marked as event triggers by morpho-
logical considerations, such as words  ending in 
"ation"). Moreover, the entity tagger also marks a 
variety of other words as event triggers when 
they are known to stand for biological or disease 
processes, in the Gene Ontology for example. 
Projecting the system output into a limited sets of 
trigger words for a particular task was somewhat 
problematic for us, although a good training ex-
ercise on transferring arguments (e.g. the theme) 
from 'other' trigger words into the subset of trig-
ger words sufficient for the task. It is possible 
that defects  in  this argument transfer process 
could account for some of the low performance 
in the test set.
Developing a rule-based system involves a 
large amount of manual work in tuning the vari-
ous aspects of the system to the task at hand. 
This is true even if the framework for the system 
is already in place. For example, with the CG 
task, the predicate-argument structures  for each 
individual trigger have to be exhaustively 
worked out to handle all possible locations of ar-
gument structures. For certain triggers, the theme 
in the CG task is somewhat indirect, as in the 
sentence: 'Almost all patients respond to G-CSF 
with increased neutrophils, reduced infections, 
and improved survival.', where the theme of 're-
sponse' are not the patients but the 'increased 
neutrophils'. This is perhaps clearer in the para-
phrase: "Organism responded to Drug with 
Symptoms", and cellular symptoms are the ap-
propriate theme for the trigger 'responded' in the 
CG task. Distinguishing such a sentence from a 
syntactically similar but semantically distinct 
sentence ' Organism responded well to Drug' is a 
challenging, and perhaps arduous, task for a rule-
based linguistically motivated system. We note 
that the CG task annotations are quite consistent 
in this aspect, as the theme is again Symptoms in 
the paraphrase 'Drug protects Organism from 
Symptoms'.
Further, in certain sentences, it is somewhat 
hard to express the meaning in the A2 notation. 
This is particularly true for adjectives which 
refer to the state of an entity rather than an event. 
Consider (PMID 17367752): "These results sug-
gest that SWAP-70 may be required for oncogen-
ic transformation and contributes to cell growth 
in MEFs transformed by v-Src." where one of the 
gold annotations transcribes functionally as 
'contributes ( Agent: SWAP-70, Theme: trans-
formed ( Theme: MEFs ) )'
which suggests that SWAP-70 contributes to the 
transformation of MEF's, whereas 'transformed' 
is only an attribute of the MEF's for this annota-
tion. These aspects of the CG task annotations 
are particularly hard to capture in a rule-based 
system. A similar problematic sentence is 'recom-
binant  EBVs  that  lack  the  BHRF1  miRNA 
cluster display a reduced ability to transform B 
lymphocytes in vitro' where the gold annotations 
read:
reduced  (Agent:  recombinant_EBVs  Theme: 
transform (B_lymphocytes))
The sentence however suggests that it is the 'lack' 
of a 'miRNA cluster'  in the EBV's that reduces 
the transformation. Again, this reading is some-
what hard to express in A2 notation.
As an additional example of the task com-
plexity, we noted that distinguishing between the 
role of the trigger word 'transform' as 'Cell_trans-
formation' and its role as a 'Planned_process' 
seems to require some level of discourse analysis 
at least in the CG training data.
Some defects in the system output arise from 
differences  in  interpretation.  In  the  sentence 
'Merlin protein might contribute to the initiation 
of metastasis of NSCLC.', (PMID:2174350), the 
gold annotations read: 
'contribute(Agent:Merlin,  Theme:  initiation 
(Theme: NSCLC))' 
'contribute  (Agent:  Merlin  Theme:  metastasis 
(Theme: NSCLC))
where NSCLC is a cancer. Peaberry gives instead 
'contribute  (Agent:  Merlin,  Theme: 
initiation(Theme: metastasis(Theme: NSCLC)))
As  'initiation'  generally  requires  an  event/pro-
cess/disease as a theme, its theme could be either 
'metastasis' or 'NSCLC', and the system makes a 
greedy choice in this case. As changes in this lo-
gic would have a system-wide impact,  this ex-
ample perhaps shows the inflexibility of the sys-
tem.
A straightforward example shows the costs of 
missed  anaphora:  'Gene  silencing  and over-ex-
91
pression  techniques  were  used  to  modulate 
RASSF1C  expression  in  human  breast  cancer 
cells.' The system misses both events 'expression 
(Theme:RASSF1C)'  and  'over-expression 
(Theme:  RASSF1C)',  both  themes  resolving  to 
the anaphoric entity 'Gene', which needs resolu-
tion.  Similar  considerations  apply  for  the  sen-
tence:  'knockdown  of  HDGF,  an  up-regulated 
protein and a target of NF-kappaB, induced cell 
apoptosis',  where  'protein'  and  not  'HDGF'  is 
seen as the theme of the trigger 'up-regulated'.
Rule based systems have been used in previ-
ous  BioNLP shared  tasks.  Such  a  system,  de-
scribed  by  Kilicoglu  and  Bergler  (2012),  was 
employed for the BioNLP shared task 2011. This 
system used  output  from the  Stanford  depend-
ency parser together with the notion of embed-
ding to construct a semantic graph, from which 
propositions were extracted.  These propositions 
were converted into events,  and semantic roles 
were derived depending on the nature of the pre-
dicate trigger word.  In comparing the perform-
ance  of  this  system  on  the  2011  GENIA task 
against our system on the CG task in common 
categories, the striking difference is that our pre-
cision is far lower in most categories (see Table 
2), even while recall is comparable. In particular, 
the difference in precision in non-regulation cat-
egories is quite noticeable. We are yet to under-
stand the reasons for these low precision scores 
in the Peaberry system.
Event Class GENIA CG
Localization 90.36 59.43
Binding 49.66 34.69
Gene expression 86.84 71.46
Transcription 58.95 100.00
Phosphorylation 94.56 70.83
Regulation 45.85 37.05
Modifier 40.89 37.50
Total 59.58 49.58
Table 2.  Comparison of  precision  between two 
rule-based systems for similar event classes: (a) 
system of  Kilicoglu  and Bergler  (2012)  in  the 
GENIA task of BioNLP 11 (b) current system in 
the CG task of BioNLP 13.
 
We noted in the Results section that perform-
ance of the system with and without gold-tagged 
entities (tagged in the latter case by the internal 
Cocoa tagger) was similar, 0.7% better with the 
gold entities in the test run, and 2% better with 
internal entities on the development set.  A pre-
liminary  analysis  shows  that  the  reduction  in 
some cases with gold entities was due to peculi-
arities in the way the system handles acronyms. 
The internal  tagger lumps together an acronym 
with its  expansion as a  single token, while the 
gold annotations tokenizes the acronym and the 
definition  separately.  This  affects  downstream 
processing,  especially  in  the  stitching  module. 
The gold annotations also do not markup sortal 
anaphors  ('gene'  in  'this  gene'),  and the system 
depends on entities being marked up in such ana-
phors to find a referent. Altogether, while the res-
ults  may initially  seem surprising,  they  do  not 
support  any notion that  automatically predicted 
entities are somehow better than gold annotated 
entities for event extraction systems. At most, the 
similarities in results with and without gold an-
notated  entities  are  indicative  of  a  comparable 
performance, a very moderate f =~ 0.45, of the 
complete system on a new document collection 
without gold annotations.
We note that it seems possible that the rules 
developed for the CG task can be extended 
without major modifications to the PC and the 
GE tasks, whose set of event triggers are a subset 
of the CG task, without degrading the perform-
ance of the CG task. This may be one of the few 
advantages of a labor-intensive rule-based sys-
tem; however, we are yet to validate such a sup-
position.  
Cancer is founded at the 
molecular/genetic/cellular level and is localized 
to an individual organ/tissue before metastasis. It 
would thus seem that the text processing logic 
used for the CG task should be generalizable (at 
least) to diseases of individual organs. However, 
cancer is not a true multi-organ systemic prob-
lem of the type that characterizes life-style dis-
eases such as diabetes and cardiovascular dis-
ease, which are both linked to multiple genomic 
loci as well as to multiple organs, and it would be 
interesting to explore coverage of event extrac-
tion schemes for these diseases with the text min-
ing techniques developed in the CG task. In this 
context, we note that automatic annotation of all 
events in a document needs to be followed by 
highlighting of the novel events/properties in the 
document, which may require some discourse 
analysis.
Reference
C. Kennedy and B. Boguraev (1996) Anaphora for 
Everyone: Pronominal Anaphora Resolution 
without a Parser. COLING '96 Proceedings of the 
92
16th conference on Computational linguistics. 
1:113-118
H. Kilicoglu and S. Bergler  2012. Biological  Event 
Composition. BMC Bioinformatics 13:Supplement 
11. Edited by J-D. Kim, S. Pyysalo, C. Nedellec, S. 
Ananiadou and J. Tsujii. 
J. D. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii. 
2009. Overview of BioNLP'09 shared task on event 
extraction. In Proceedings of the Workshop on 
BioNLP: Shared Task. 2009:1-9.
J. D. Kim , S. Pyysalo, T. Ohta, R. Bossy, and J. 
Tsujii. 2011. Overview of BioNLP Shared Task 
2011. In Proceedings of the BioNLP 2011 Work-
shop Companion Volume for Shared Task. 2011:1-
6.
S. Lappin and H. J. Leass. 1994. An Algorithm for 
Pronominal Anaphora  Resolution. J. Comp. Ling. 
20:(4):535-561
S. Pyysalo, T. Ohta, M. Miwa, H.C. Cho, J. Tsujii J, 
and S. Ananiadou. 2012. Event extraction across 
multiple levels of biological organization. Bioin-
formatics. 28(18):i575-i581
S. Pyysalo,  T. Ohta and S. Ananiadou. Overview of 
the Cancer Genetics (CG) task of BioNLP Shared 
Task 2013. In Proceedings of BioNLP Shared Task 
2013 Workshop. To appear.
S. Pyysalo, T. Ohta and S. Ananiadou. 2013. Cancer 
Genetics task. Final evaluation results - RelAgent. 
http://weaver.nlplab.org/~bionlp-st/BioNLP-ST-
2013/CG/final-results/RelAgent.html
RelAgent. 2012. Evaluation of Cocoa against some 
corpora. http://npjoint.com/CocoaEval.html
D. Swanson. 1988. Migraine and Magnesium: Eleven 
Neglected Connections. Persp. Bio. Med. 
31(4):526?557.
93
