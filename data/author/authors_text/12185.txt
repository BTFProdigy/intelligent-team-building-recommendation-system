Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 272?280,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Predicting Risk from Financial Reports with Regression
Shimon Kogan
McCombs School of Business
University of Texas at Austin
Austin, TX 78712, USA
shimon.kogan@mccombs.utexas.edu
Dimitry Levin
Mellon College of Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
dimitrylevin@gmail.com
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Jacob S. Sagi
Owen Graduate School of Management
Vanderbilt University
Nashville, TN 37203, USA
Jacob.Sagi@Owen.Vanderbilt.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We address a text regression problem: given a
piece of text, predict a real-world continuous
quantity associated with the text?s meaning. In
this work, the text is an SEC-mandated finan-
cial report published annually by a publicly-
traded company, and the quantity to be pre-
dicted is volatility of stock returns, an empiri-
cal measure of financial risk. We apply well-
known regression techniques to a large cor-
pus of freely available financial reports, con-
structing regression models of volatility for
the period following a report. Our models ri-
val past volatility (a strong baseline) in pre-
dicting the target variable, and a single model
that uses both can significantly outperform
past volatility. Interestingly, our approach is
more accurate for reports after the passage of
the Sarbanes-Oxley Act of 2002, giving some
evidence for the success of that legislation in
making financial reports more informative.
1 Introduction
We consider a text regression problem: given a piece
of text, predict a R-valued quantity associated with
that text. Specifically, we use a company?s annual
financial report to predict the financial risk of invest-
ment in that company, as measured empirically by a
quantity known as stock return volatility.
Predicting financial risk is of clear interest to
anyone who invests money in stocks and central
to modern portfolio choice. Financial reports are
a government-mandated artifact of the financial
world that?one might hypothesize?contain a large
amount of information about companies and their
value. Indeed, it is an important question whether
mandated disclosures are informative, since they are
meant to protect investors but are costly to produce.
The intrinsic properties of the problem are attrac-
tive as a test-bed for NLP research. First, there is
no controversy about the usefulness or existential
reality of the output variable (volatility). Statisti-
cal NLP often deals in the prediction of variables
ranging from text categories to linguistic structures
to novel utterances. While many of these targets are
uncontroversially useful, they often suffer from eval-
uation difficulties and disagreement among annota-
tors. The output variable in this work is a statistic
summarizing facts about the real world; it is not sub-
ject to any kind of human expertise, knowledge, or
intuition. Hence this prediction task provides a new,
objective test-bed for any kind of linguistic analysis.
Second, many NLP problems rely on costly anno-
tated resources (e.g., treebanks or aligned bilingual
corpora). Because the text and historical financial
data used in this work are freely available (by law)
and are generated as a by-product of the American
272
economy, old and new data can be obtained by any-
one with relatively little effort.
In this paper, we demonstrate that predicting fi-
nancial volatility automatically from a financial re-
port is a novel, challenging, and easily evaluated nat-
ural language understanding task. We show that a
very simple representation of the text (essentially,
bags of unigrams and bigrams) can rival and, in
combination, improve over a strong baseline that
does not use the text. Analysis of the learned models
provides insights about what can make this problem
more or less difficult, and suggests that disclosure-
related legislation led to more transparent reporting.
2 Stock Return Volatility
Volatility is often used in finance as a measure of
risk. It is measured as the standard deviation of
a stock?s returns over a finite period of time. A
stock will have high volatility when its price fluctu-
ates widely and low volatility when its price remains
more or less constant.
Let rt = PtPt?1 ? 1 be the return on a given stockbetween the close of trading day t ? 1 and day t,
where Pt is the (dividend-adjusted) closing stock
price at date t. The measured volatility over the time
period from day t? ? to day t is equal to the sample
s.d.:
v[t??,t] =
????
??
i=0
(rt?i ? r?)2
/
? (1)
where r? is the sample mean of rt over the period. In
this work, the above estimate will be treated as the
true output variable on training and testing data.
It is important to note that predicting volatility is
not the same as predicting returns or value. Rather
than trying to predict how well a stock will perform,
we are trying to predict how stable its price will be
over a future time period. It is, by now, received
wisdom in the field of economics that predicting a
stock?s performance, based on easily accessible pub-
lic information, is difficult. This is an attribute of
well-functioning (or ?efficient?) markets and a cor-
nerstone of the so-called ?efficient market hypoth-
esis? (Fama, 1970). By contrast, the idea that one
can predict a stock?s level of risk using public in-
formation is uncontroversial and a basic assumption
made by many economically sound pricing mod-
els. A large body of research in finance suggests
that the two types of quantities are very different:
while predictability of returns could be easily traded
away by the virtue of buying/selling stocks that are
under- or over-valued (Fama, 1970), similar trades
are much more costly to implement with respect to
predictability of volatility (Dumas et al, 2007). By
focusing on volatility prediction, we avoid taking
a stance on whether or not the United States stock
market is informationally efficient.
3 Problem Formulation
Given a text document d, we seek to predict the
value of a continuous variable v. We do this via a
parameterized function f :
v? = f(d;w) (2)
where w ? Rd are the parameters or weights. Our
approach is to learn a human-interpretable w from
a collection of N training examples {?di, vi?}Ni=1,
where each di is a document and each vi ? R.
Support vector regression (Drucker et al, 1997)
is a well-known method for training a regression
model. SVR is trained by solving the following op-
timization problem:
min
w?Rd
1
2?w?
2+CN
N?
i=1
max
(
0,
???vi ? f(di;w)
???? 
)
? ?? ?
-insensitive loss function(3)
where C is a regularization constant and  controls
the training error.1 The training algorithm finds
weights w that define a function f minimizing the
(regularized) empirical risk.
Let h be a function from documents into some
vector-space representation? Rd. In SVR, the func-
tion f takes the form:
f(d;w) = h(d)>w =
N?
i=1
?iK(d,di) (4)
where Equation 4 re-parameterizes f in terms of a
kernel function K with ?dual? weights ?i. K can
1Given the embedding h of documents in Rd,  defines
a ?slab? (region between two parallel hyperplanes, some-
times called the ?-tube?) in Rd+1 through which each
?h(di), f(di;w)? must pass in order to have zero loss.
273
year words documents words/doc.
1996 5.5M 1,408 3,893
1997 9.3M 2,260 4,132
1998 11.8M 2,462 4,808
1999 14.5M 2,524 5,743
2000 13.4M 2,425 5,541
2001 15.4M 2,596 5,928
2002 22.7M 2,846 7,983
2003 35.3M 3,612 9,780
2004 38.9M 3,559 10,936
2005 41.9M 3,474 12,065
2006 38.8M 3,308 11,736
total 247.7M 26,806 9,240
Table 1: Dimensions of the dataset used in this paper,
after filtering and tokenization. The near doubling in av-
erage document size during 2002?3 is possibly due to the
passage of the Sarbanes-Oxley Act of 2002 in the wake
of Enron?s accounting scandal (and numerous others).
be seen as a similarity function between two docu-
ments. At test time, a new example is compared to a
subset of the training examples (those with ?i 6= 0);
typically with SVR this set is sparse. With the linear
kernel, the primal and dual weights relate linearly:
w =
N?
i=1
?ih(di) (5)
The full details of SVR and its implementation are
beyond the scope of this paper; interested readers are
referred to Scho?lkopf and Smola (2002). SVMlight
(Joachims, 1999) is a freely available implementa-
tion of SVR training that we used in our experi-
ments.2
4 Dataset
In the United States, the Securities Exchange Com-
mission mandates that all publicly-traded corpora-
tions produce annual reports known as ?Form 10-
K.? The report typically includes information about
the history and organization of the company, equity
and subsidiaries, as well as financial information.
These reports are available to the public and pub-
lished on the SEC?s web site.3 The structure of the
10-K is specified in detail in the legislation. We have
collected 54,379 reports published over the period
2Available at http://svmlight.joachims.org.
3http://www.sec.gov/edgar.shtml
1996?2006 from 10,492 different companies. Each
report comes with a date of publication, which is im-
portant for tying the text back to the financial vari-
ables we seek to predict.
From the perspective of predicting future events,
one section of the 10-K is of special interest: Section
7, known as ?management?s discussion and anal-
ysis of financial conditions and results of opera-
tions? (MD&A), and in particular Subsection 7A,
?quantitative and qualitative disclosures about mar-
ket risk.? Because Section 7 is where the most im-
portant forward-looking content is most likely to
be found, we filter other sections from the reports.
The filtering is done automatically using a short,
hand-written Perl script that seeks strings loosely
matching the Section 7, 7A, and 8 headers, finds the
longest reasonable ?Section 7? match (in words) of
more than 1,000 whitespace-delineated tokens.
Section 7 typically begins with an introduction
like this (from ABC?s 1998 Form 10-K, before to-
kenization for readability; boldface added):
The following discussion and analysis of
ABC?s consolidated financial condition and
consolidated results of operation should be
read in conjunction with ABC?s Consoli-
dated Financial Statements and Notes thereto
included elsewhere herein. This discus-
sion contains certain forward-looking state-
ments which involve risks and uncertain-
ties. ABC?s actual results could differ mate-
rially from the results expressed in, or implied
by, such statements. See ?Regarding Forward-
Looking Statements.?
Not all of the documents downloaded pass the fil-
ter at all, and for the present work we have only used
documents that do pass the filter. (One reason for the
failure of the filter is that many 10-K reports include
Section 7 ?by reference,? so the text is not directly
included in the document.)
In addition to the reports, we used the Center
for Research in Security Prices (CRSP) US Stocks
Database to obtain the price return series along with
other firm characteristics.4 We proceeded to calcu-
late two volatilities for each firm/report observation:
the twelve months prior to the report (v(?12)) and
the twelve months after the report (v(+12)).
4The text and volatility data are publicly available at http:
//www.ark.cs.cmu.edu/10K.
274
Tokenization was applied to the text, including
punctuation removal, downcasing, collapsing all
digit sequences,5 and heuristic removal of remnant
markup. Table 1 gives statistics on the corpora
used in this research; this is a subset of the cor-
pus for which there is no missing volatility informa-
tion. The drastic increase in length during the 2002?
2003 period might be explained by the passage by
the US Congress of the Sarbanes-Oxley Act of 2002
(and related SEC and exchange rules), which im-
posed revised standards on reporting practices of
publicly-traded companies in the US.
5 Baselines and Evaluation Method
Volatility displays an effect known as autoregressive
conditional heteroscedasticity (Engle, 1982). This
means that the variance in a stock?s return tends to
change gradually. Large changes in price are pre-
saged by other changes, and periods of stability tend
to continue. Volatility is, generally speaking, not
constant, yet prior volatility (e.g., v(?12)) is a very
good predictor of future volatility (e.g., v(+12)). At
the granularity of a year, which we consider here
because the 10-K reports are annual, there are no
existing models of volatility that are widely agreed
to be significantly more accurate than our histor-
ical volatility baseline. We tested a state-of-the-
art model known as GARCH(1, 1) (Engle, 1982;
Bollerslev, 1986) and found that it was no stronger
than our historical volatility baseline on this sample.
Throughout this paper, we will report perfor-
mance using the mean squared error between the
predicted and true log-volatilities:6
MSE = 1N ?
N ??
i=1
(log(vi)? log(v?i))2 (6)
where N ? is the size of the test set, given in Table 1.
6 Experiments
In our experiments, we vary h (the function that
maps inputs to a vector space) and the subset of the
5While numerical information is surely informative about
risk, recall that our goal is to find indicators of risk expressed in
the text; automatic predictors of risk from numerical data would
use financial data streams directly, not text reports.
6We work in the log domain because it is standard in finance,
due to the dynamic range of actual volatilities; the distribution
over log v across companies tends to have a bell shape.
data used for training. We will always report perfor-
mance over test sets consisting of one year?s worth
of data (the subcorpora described in Table 1). In
this work, we focus on predicting the volatility over
the year following the report (v(+12)). In all experi-
ments,  = 0.1 and C is set using the default choice
of SVMlight , which is the inverse of the average of
h(d)>h(d) over the training data.7
6.1 Feature Representation
We first consider how to represent the 10-K reports.
We adopt various document representations, all us-
ing word features. Let M be the vocabulary size
derived from the training data.8 Let freq(xj ;d) de-
note the number of occurrences of the jth word in
the vocabulary in document d.
? TF: hj(d) = 1|d| freq(xj ;d), ?j ? {1, ...,M}.
? TFIDF: hj(d) = 1|d| freq(xj ;d)? log(N/|{d :
freq(xj ;d) > 0}|), where N is the number of
documents in the training set. This is the classic
?TFIDF? score.
? LOG1P: hj(d) = log(1 + freq(xj ;d)). Rather
than normalizing word frequencies as for TF,
this score dampens them with a logarithm. We
also include a variant of LOG1P where terms
are the union of unigrams and bigrams.
Note that each of these preserves sparsity; when
freq(xj ;d) = 0, hj(d) = 0 in all cases.
For interpretability of results, we use a linear ker-
nel. The usual bias weight b is included. We found
it convenient to work in the logarithmic domain for
the predicted variable, predicting log v instead of v,
since volatility is always nonnegative. In this setting,
the predicted volatility takes the form:
log v? = b+
M?
j=1
wjhj(d) (7)
Because the goal of this work is to explore how text
might be used to predict volatility, we also wish
7These values were selected after preliminary and cursory
exploration with 1996?2000 as training data and 2001 as the
test set. While the effects of  and C were not large, further
improvements may be possible with more careful tuning.
8Preliminary experiments that filtered common or rare
words showed a negligible or deleterious effect on performance.
275
features 2001 2002 2003 2004 2005 2006 micro-ave.
his
tor
y v(?12) (baseline) 0.1747 0.1600 0.1873 0.1442 0.1365 0.1463 0.1576
v(?12) (SVR with bias) 0.2433 0.4323 0.1869 0.2717 0.3184 5.6778 1.2061
v(?12) (SVR without bias) 0.2053 0.1653 0.2051 0.1337 0.1405 0.1517 0.1655
wo
rds
TF 0.2219 0.2571 0.2588 0.2134 0.1850 0.1862 0.2197
TFIDF 0.2033 0.2118 0.2178 0.1660 0.1544 0.1599 0.1842
LOG1P 0.2107 0.2214 0.2040 0.1693 0.1581 0.1715 0.1873
LOG1P, bigrams 0.1968 0.2015 ?0.1729 0.1500 0.1394 0.1532 0.1667
bo
th
TF+ 0.1885 0.1616 0.1925 ?0.1230 ?0.1272 ?0.1402 ?0.1541
TFIDF+ 0.1919 0.1618 0.1965 ?0.1246 ?0.1276 ?0.1403 ?0.1557
LOG1P+ 0.1846 0.1764 ?0.1671 ?0.1309 ?0.1319 0.1458 ?0.1542
LOG1P+, bigrams 0.1852 0.1792 ?0.1599 ?0.1352 ?0.1307 0.1448 ?0.1538
Table 2: MSE (Eq. 6) of different models on test data predictions. Lower values are better. Boldface denotes
improvements over the baseline, and ? denotes significance compared to the baseline under a permutation test (p <
0.05).
to see whether text adds information beyond what
can be predicted using historical volatility alone (the
baseline, v(?12)). We therefore consider models
augmented with an additional feature, defined as
hM+1 = log v(?12). Since this is historical informa-
tion, it is always available when the 10-K report is
published. These models are denoted TF+, TFIDF+,
and LOG1P+.
The performance of these models, compared to
the baseline from Section 5, is shown in Table 2.
We used as training examples all reports from the
five-year period preceding the test year (so six ex-
periments on six different training and test sets are
shown in the figure). We also trained SVR models
on the single feature v(?12), with and without bias
weights (b in Eq. 7); these are usually worse and
never signficantly better than the baseline.
Strikingly, the models that use only the text to
predict volatility come very close to the historical
baseline in some years. That a text-only method
(LOG1P with bigrams) for predicting future risk
comes within 5% of the error of a strong baseline
(2003?6) shows promise for the overall approach.
A combined model improves substantially over the
baseline in four out of six years (2003?6), and this
difference is usually robust to the representation
used. Table 3 shows the most strongly weighted
terms in each of the text-only LOG1P models (in-
cluding bigrams). These weights are recovered us-
ing the relationship expressed in Eq. 5.
6.2 Training Data Effects
It is well known that more training data tend to im-
prove the performance of a statistical method; how-
ever, the standard assumption is that the training
data are drawn from the same distribution as the test
data. In this work, where we seek to predict the
future based on data from past, that assumption is
obviously violated. It is therefore an open question
whether more data (i.e., looking farther into the past)
is helpful for predicting volatility, or whether it is
better to use only the most recent data.
Table 4 shows how performance varies when one,
two, or five years of historical training data are used,
averaged across test years. In most cases, using
more training data (from a longer historical period)
is helpful, but not always. One interesting trend,
not shown in the aggregate statistics of Table 4,
is that recency of the training set affected perfor-
mance much more strongly in earlier train/test splits
(2001?3) than later ones (2004?6). This experiment
leads us to conclude that temporal changes in fi-
nancial reporting make training data selection non-
trivial. Changes in the macro economy and spe-
cific businesses make older reports less relevant for
prediction. For example, regulatory changes like
Sarbanes-Oxley, variations in the business cycle,
and technological innovation like the Internet influ-
ence both the volatility and the 10-K text.
6.3 Effects of Sarbanes-Oxley
We noted earlier that the passage of the Sarbanes-
Oxley Act of 2002, which sought to reform financial
reporting, had a clear effect on the lengths of the
10-K reports in our collection. But are the reports
more informative? This question is important, be-
cause producing reports is costly; we present an em-
pirical argument based on our models that the legis-
276
19
96
?2
00
0
19
97
?2
00
1
19
98
?2
00
2
19
99
?2
00
3
20
00
?2
00
4
20
01
?2
00
5
ne
tlo
ss
0.0
26
ye
ar
#
0.0
28
los
s
0.0
23
los
s
0.0
26
los
s
0.0
25
los
s
0.0
26
hig
hv?
ye
ar
#
0.0
24
ne
tlo
ss
0.0
23
ne
tlo
ss
0.0
20
ne
tlo
ss
0.0
20
ne
tlo
ss
0.0
17
ne
tlo
ss
0.0
18
?
los
s
0.0
20
ex
pe
nse
s
0.0
20
ex
pe
nse
s
0.0
17
ex
pe
nse
s
0.0
17
ye
ar
#
0.0
16
go
ing
co
nc
ern
0.0
14
ex
pe
nse
s
0.0
19
los
s
0.0
20
ye
ar
#
0.0
15
go
ing
co
nc
ern
0.0
15
ex
pe
nse
s
0.0
15
ex
pe
nse
s
0.0
14
co
ve
na
nts
0.0
17
ex
pe
rie
nc
ed
0.0
15
ob
lig
ati
on
s
0.0
15
ye
ar
#
0.0
15
go
ing
co
nc
ern
0.0
14
ag
oin
g
0.0
14
dil
ute
d
0.0
14
of
$#
0.0
15
fin
an
cin
g
0.0
14
fin
an
cin
g
0.0
14
ag
oin
g
0.0
13
pe
rso
nn
el
0.0
13
co
nv
ert
ibl
e
0.0
14
co
ve
na
nts
0.0
15
co
nv
ert
ibl
e
0.0
14
ag
oin
g
0.0
14
ad
mi
nis
tra
tiv
e
0.0
13
fin
an
cin
g
0.0
13
da
te
0.0
14
ad
dit
ion
al
0.0
14
ad
dit
ion
al
0.0
14
ad
dit
ion
al
0.0
13
pe
rso
nn
el
0.0
13
ad
mi
nis
tra
tiv
e
0.0
12
lon
gte
rm
-0.
01
4
me
rge
ra
gre
em
en
t
-0.
01
5
un
sec
ure
d
-0.
01
2
dis
trib
uti
on
s
-0.
01
2
dis
trib
uti
on
s
-0.
01
1
po
lic
ies
-0.
01
1
rat
es
-0.
01
5
div
ide
nd
s
-0.
01
5
ear
nin
gs
-0.
01
2
an
nu
al
-0.
01
2
ins
ura
nc
e
-0.
01
1
by
the
-0.
01
1
div
ide
nd
-0.
01
5
un
sec
ure
d
-0.
01
7
dis
trib
uti
on
s
-0.
01
2
div
ide
nd
-0.
01
2
cri
tic
al
acc
ou
nti
ng
-0.
01
2
ear
nin
gs
-0.
01
1
un
sec
ure
d
-0.
01
5
div
ide
nd
-0.
01
7
div
ide
nd
s
-0.
01
5
div
ide
nd
s
-0.
01
2
low
er
int
ere
st
-0.
01
2
div
ide
nd
s
-0.
01
2
me
rge
ra
gre
em
en
t
-0.
01
7
pro
pe
rtie
s
-0.
01
8
inc
om
e
-0.
01
6
rat
es
-0.
01
3
div
ide
nd
s
-0.
01
3
un
sec
ure
d
-0.
01
2
pro
pe
rtie
s
-0.
01
8
ne
tin
co
me
-0.
01
9
pro
pe
rtie
s
-0.
01
6
pro
pe
rtie
s
-0.
01
5
pro
pe
rtie
s
-0.
01
4
pro
pe
rtie
s
-0.
01
3
inc
om
e
-0.
02
1
inc
om
e
-0.
02
1
ne
tin
co
me
-0.
01
9
rat
e
-0.
01
9
rat
e
-0.
01
7
rat
e
-0.
01
4
?
rat
e
-0.
02
2
rat
e
-0.
02
5
rat
e
-0.
02
2
ne
tin
co
me
-0.
02
3
ne
tin
co
me
-0.
02
1
ne
tin
co
me
-0.
01
8
low
v?
Ta
ble
3:
Mo
sts
tro
ng
ly-
we
igh
ted
ter
ms
in
mo
de
lsl
ear
ne
df
rom
var
iou
sti
me
pe
rio
ds
(LO
G1
Pm
od
el
wi
th
un
igr
am
sa
nd
big
ram
s).
?#
?d
en
ote
sa
ny
dig
its
eq
ue
nc
e. features 1 2 5
TF+ 0.1509 0.1450 0.1541
TFIDF+ 0.1512 0.1455 0.1557
LOG1P+ 0.1621 0.1611 0.1542
LOG1P+, bigrams 0.1617 0.1588 0.1538
Table 4: MSE of volatility predictions using reports from
varying historical windows (1, 2, and 5 years), micro-
averaged across six train/test scenarios. Boldface marks
best in a row. The historical baseline achieves 0.1576
MSE (see Table 2).
lation has actually been beneficial.
Our experimental results in Section 6.1, in which
volatility in the years 2004?2006 was more accu-
rately predicted from the text than in 2001?2002,
suggest that the Sarbanes-Oxley Act led to more in-
formative reports. We compared the learned weights
(LOG1P+, unigrams) between the six overlapping
five-year windows ending in 2000?2005; measured
in L1 distance, these were, in consecutive order,
?52.2, 59.9, 60.7, 55.3, 52.3?; the biggest differ-
ences came between 2001 and 2002 and between
2002 and 2003. (Firms are most likely to have be-
gun compliance with the new law in 2003 or 2004.)
The same pattern held when only words appearing
in all five models were considered. Variation in the
recency/training set size tradeoff (?6.2), particularly
during 2002?3, also suggests that there were sub-
stantial changes in the reports during that time.
6.4 Qualitative Evaluation
One of the advantages of a linear model is that we
can explore what each model discovers about dif-
ferent unigram and bigram terms. Some manually
selected examples of terms whose learned weights
(w) show interesting variation patterns over time are
shown in Figure 1, alongside term frequency pat-
terns, for the text-only LOG1P model (with bigrams).
These examples were suggested by experts in fi-
nance from terms with weights that were both large
and variable (across training sets).
A particularly interesting case, in light of
Sarbanes-Oxley, is the term accounting policies.
Sarbanes-Oxley mandated greater discussion of ac-
counting policy in the 10-K MD&A section. Be-
fore 2002 this term indicates high volatility, per-
haps due to complicated off-balance sheet transac-
tions or unusual accounting policies. Starting in
2002, explicit mention of accounting policies indi-
277
00.2
0.4
0.6
0.8
a
v
e
.
 
t
e
r
m
 
f
r
e
q
u
e
n
c
y
-0.015
-0.010
-0.005
0
0.005
w
accounting policies
estimates
-0.010
-0.005
0
0.005
w
reit
mortgages
-0.010
-0.005
0
0.005
0.010
96-00 97-01 98-02 99-03 00-04 01-05
w
higher margin
lower margin
0
0.05
0.10
0.15
0.20
a
v
e
.
 
t
e
r
m
 
f
r
e
q
u
e
n
c
y
0
2
4
6
8
a
v
e
.
 
t
e
r
m
 
f
r
e
q
u
e
n
c
y
Figure 1: Left:
learned weights for
selected terms across
models trained on
data from different
time periods (x-axis).
These weights are
from the LOG1P
(unigrams and
bigrams) models
trained on five-year
periods, the same
models whose
extreme weights are
summarized in
Tab. 3. Note that all
weights are within
0? 0.026. Right: the
terms? average
frequencies (by
document) over the
same periods.
cates lower volatility. The frequency of the term
also increases drastically over the same period, sug-
gesting that the earlier weights may have been in-
flated. A more striking example is estimates, which
averages one occurrence per document even in the
1996?2000 period, experiences the same term fre-
quency explosion, and goes through a similar weight
change, from strongly indicating high volatility to
strongly indicating low volatility.
As a second example, consider the terms mort-
gages and reit (Real Estate Investment Trust, a tax
designation for businesses that invest in real estate).
Given the importance of the housing and mortgage
market over the past few years, it is interesting to
note that the weight on both of these terms increases
over the period from a strong low volatility term to a
weak indicator of high volatility. It will be interest-
ing to see how the dramatic decline in housing prices
in late 2007, and the fallout created in credit markets
in 2008, is reflected in future models.
Finally, notice that high margin and low mar-
gin, whose frequency patterns are fairly flat ?switch
places,? over the sample: first indicating high and
low volatility, respectively, then low and high. There
is no a priori reason to expect high or low margins
would be associated with high or low stock volatil-
ity. However, this is an interesting example where
bigrams are helpful (the word margin by itself is
uninformative) and indicates that predicting risk is
highly time-dependent.
6.5 Delisting
An interesting but relatively infrequent phenomenon
is the delisting of a company, i.e., when it ceases to
be traded on a particular exchange due to dissolution
after bankruptcy, a merger, or violation of exchange
rules. The relationship between volatility and delist-
ing has been studied by Merton (1974), among oth-
ers. Our dataset includes a small number of cases
where the volatility figures for the period following
the publication of a 10-K report are unavailable be-
cause the company was delisted. Learning to predict
delisting is extremely difficult because fewer than
4% of the 2001?6 10-K reports precede delisting.
Using the LOG1P representation, we built a lin-
ear SVM classifier for each year in 2001?6 (trained
on the five preceding years? data) to predict whether
a company will be delisted following its 10-K re-
port. Performance for various precision measures is
shown in Table 5. Notably, for 2001?4 we achieve
278
precision (%) at ... ?01 ?02 ?03 ?04 ?05 ?06
recall = 10% 80 93 79 100 47 21
n = 5 100 100 40 100 60 80
n = 10 80 90 70 90 60 70
n = 100 38 48 53 29 24 20
oracle F1 (%) 35 42 44 36 31 16
6 bulletin, creditors, dip, otc
5 court
4 chapter, debtors, filing, prepetition
3 bankruptcy
2 concern, confirmation, going, liquidation
1 debtorinpossession, delisted, nasdaq, petition
Table 5: Left: precision of delisting predictions. The ?oracle F1? row shows the maximal F1 score obtained for any
n. Right: Words most strongly predicting delisting of a company. The number is how many of the six years (2001?6)
the word is among the ten most strongly weighted. There were no clear patterns across years for words predicting that
a company would not be delisted. The word otc refers to ?over-the-counter? trading, a high-risk market.
above 75% precision at 10% recall. Our best (or-
acle) F1 scores occur in 2002 and 2003, suggesting
again a difference in reports around Sarbanes-Oxley.
Table 5 shows words associated with delisting.
7 Related Work
In NLP, regression is not widely used, since most
natural language-related data are discrete. Regres-
sion methods were pioneered by Yang and Chute
(1992) and Yang and Chute (1993) for information
retrieval purposes, but the predicted continuous vari-
able was not an end in itself in that work. Blei
and McAuliffe (2007) used latent ?topic? variables
to predict movie reviews and popularity from text.
Lavrenko et al (2000b) and Lavrenko et al (2000a)
modeled influences between text and time series fi-
nancial data (stock prices) using language models.
Farther afield, Albrecht and Hwa (2007) used SVR
to train machine translation evaluation metrics to
match human evaluation scores and compared tech-
niques using correlation. Regression has also been
used to order sentences in extractive summarization
(Biadsy et al, 2008).
While much of the information relevant for in-
vestors is communicated through text (rather than
numbers), only recently is this link explored. Some
papers relate news articles to earning forecasts, stock
returns, volatility, and volume (Koppel and Shtrim-
berg, 2004; Tetlock, 2007; Tetlock et al, 2008; Gaa,
2007; Engelberg, 2007). Das and Chen (2001) and
Antweiler and Frank (2004) ask whether messages
posted on message boards can help explain stock
performance, while Li (2005) measures the associ-
ation between frequency of words associated with
risk and subsequent stock returns. Weiss-Hanley and
Hoberg (2008) study initial public offering disclo-
sures using word statistics. Many researchers have
focused the related problem of predicting sentiment
and opinion in text (Pang et al, 2002; Wiebe and
Riloff, 2005), sometimes connected to extrinsic val-
ues like prediction markets (Lerman et al, 2008).
In contrast to text regression, text classification
comprises a widely studied set of problems involv-
ing the prediction of categorial variables related to
text. Applications have included the categorization
of documents by topic (Joachims, 1998), language
(Cavnar and Trenkle, 1994), genre (Karlgren and
Cutting, 1994), author (Bosch and Smith, 1998),
sentiment (Pang et al, 2002), and desirability (Sa-
hami et al, 1998). Text categorization has served as
a test application for nearly every machine learning
technique for discrete classification.
8 Conclusion
We have introduced and motivated a new kind of
task for NLP: text regression, in which text is used
to make predictions about measurable phenomena
in the real world. We applied the technique to pre-
dicting financial volatility from companies? 10-K re-
ports, and found text regression model predictions
to correlate with true volatility nearly as well as his-
torical volatility, and a combined model to perform
even better. Further, improvements in accuracy and
changes in models after the passage of the Sarbanes-
Oxley Act suggest that financial reporting reform
has had interesting and measurable effects.
Acknowledgments
The authors are grateful to Jamie Callan, Chester Spatt,
Anthony Tomasic, Yiming Yang, and Stanley Zin for
helpful discussions, and to the anonymous reviewers for
useful feedback. This research was supported by grants
from the Institute for Quantitative Research in Finanace
and from the Center for Analytical Research in Technol-
ogy at the Tepper School of Business, Carnegie Mellon
University.
279
References
J. S. Albrecht and R. Hwa. 2007. Regression for
sentence-level MT evaluation with pseudo references.
In Proc. of ACL.
W. Antweiler and M. Z. Frank. 2004. Is all that talk
just noise? the information content of internet stock
message boards. Journal of Finance, 59:1259?1294.
F. Biadsy, J. Hirschberg, and E. Filatova. 2008. An
unsupervised approach to biography production using
Wikipedia. In Proc. of ACL.
D. M. Blei and J. D. McAuliffe. 2007. Supervised topic
models. In Advances in NIPS 21.
T. Bollerslev. 1986. Generalized autoregressive con-
ditional heteroskedasticity. Journal of Econometrics,
31:307?327.
R. Bosch and J. Smith. 1998. Separating hyperplanes
and the authorship of the disputed Federalist papers.
American Mathematical Monthly, 105(7):601?608.
W. B. Cavnar and J. M. Trenkle. 1994. n-gram-based
text categorization. In Proc. of SDAIR.
S. Das and M. Chen. 2001. Yahoo for Amazon: Ex-
tracting market sentiment from stock mesage boards.
In Proc. of Asia Pacific Finance Association Annual
Conference.
H. Drucker, C. J. C. Burges, L. Kaufman, A. Smola, and
V. Vapnik. 1997. Support vector regression machines.
In Advances in NIPS 9.
B. Dumas, A. Kurshev, and R. Uppal. 2007. Equilibrium
portfolio strategies in the presence of sentiment risk
and excess volatility. Swiss Finance Institute Research
Paper No. 07-37.
J. Engelberg. 2007. Costly information processing: Ev-
idence from earnings announcements. Working paper,
Northwestern University.
R. F. Engle. 1982. Autoregressive conditional het-
eroscedasticity with estimates of variance of united
kingdom inflation. Econometrica, 50:987?1008.
E. F. Fama. 1970. Efficient capital markets: A review
of theory and empirical work. Journal of Finance,
25(2):383?417.
C. Gaa. 2007. Media coverage, investor inattention, and
the market?s reaction to news. Working paper, Univer-
sity of British Columbia.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proc. of ECML.
T. Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support
Vector Learning. MIT Press.
J. Karlgren and D. Cutting. 1994. Recognizing text gen-
res with simple metrics using discriminant analysis. In
Proc. of COLING.
M. Koppel and I. Shtrimberg. 2004. Good news or bad
news? let the market decide. In AAAI Spring Sympo-
sium on Exploring Attitude and Affect in Text: Theo-
ries and Applications.
V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie,
D. Jensen, and J. Allan. 2000a. Language models for
financial news recommendation. In Proc. of CIKM.
V. Lavrenko, M. Schmill, D. Lawrie, P. Ogilvie,
D. Jensen, and J. Allan. 2000b. Mining of concurrent
text and time series. In Proc. of KDD.
K. Lerman, A. Gilder, M. Dredze, and F. Pereira. 2008.
Reading the markets: Forecasting public opinion of
political candidates by news analysis. In COLING.
F. Li. 2005. Do stock market investors understand the
risk sentiment of corporate annual reports? Working
Paper, University of Michigan.
R. Merton. 1974. On the pricing of corporate debt: The
risk structure of interest rates. Journal of Finance,
29:449?470.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP.
M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz.
1998. A Bayesian approach to filtering junk email. In
Proc. of AAAI Workshop on Learning for Text Catego-
rization.
B. Scho?lkopf and A. J. Smola. 2002. Learning with Ker-
nels: Support Vector Machines, Regularization, Opti-
mization, and Beyond. MIT Press.
P. C. Tetlock, M. Saar-Tsechansky, and S. Macskassy.
2008. More than words: Quantifying language to
measure firms? fundamentals. Journal of Finance,
63(3):1437?1467.
P. C. Tetlock. 2007. Giving content to investor senti-
ment: The role of media in the stock market. Journal
of Finance, 62(3):1139?1168.
K. Weiss-Hanley and G. Hoberg. 2008. Strategic disclo-
sure and the pricing of initial public offerings. Work-
ing paper.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In CICLing.
Y. Yang and C. G. Chute. 1992. A linear least squares fit
mapping method for information retrieval from natural
language texts. In Proc. of COLING.
Y. Yang and C. G. Chute. 1993. An application of least
squares fit mapping to text information retrieval. In
Proc. of SIGIR.
280
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594?604,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting a Scientific Community?s Response to an Article
Dani Yogatama Michael Heilman Brendan O?Connor Chris Dyer
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,mheilman,brenocon,cdyer}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We consider the problem of predicting mea-
surable responses to scientific articles based
primarily on their text content. Specif-
ically, we consider papers in two fields
(economics and computational linguistics)
and make predictions about downloads and
within-community citations. Our approach is
based on generalized linear models, allowing
interpretability; a novel extension that cap-
tures first-order temporal effects is also pre-
sented. We demonstrate that text features
significantly improve accuracy of predictions
over metadata features like authors, topical
categories, and publication venues.
1 Introduction
Written communication is an essential component
of the complex social phenomenon of science. As
such, natural language processing is well-positioned
to provide tools for understanding the scientific pro-
cess, by analyzing the textual artifacts (papers, pro-
ceedings, etc.) that it produces. This paper is about
modeling collections of scientific documents to un-
derstand how their textual content relates to how a
scientific community responds to them. While past
work has often focused on citation structure (Borner
et al, 2003; Qazvinian and Radev, 2008), our em-
phasis is on the text content, following Ramage et
al. (2010) and Gerrish and Blei (2010).
Instead of task-independent exploratory data anal-
ysis (e.g., topic modeling) or multi-document sum-
marization, we consider supervised models of the
collective response of a scientific community to a
published article. There are many measures of im-
pact of a scientific paper; ours come from direct
measurements of the number of downloads (from
an established website where prominent economists
post papers before formal publication) and citations
(within a fixed scientific community). We adopt a
discriminative approach based on generalized lin-
ear models that can make use of any text or meta-
data features, and show that simple lexical fea-
tures offer substantial power in modeling out-of-
sample response and in forecasting response for fu-
ture articles. Realistic forecasting evaluations re-
quire methodological care beyond the usual best
practices of train/test separation, and we elucidate
these issues.
In addition, we introduce a new regularization
technique that leverages the intuition that the rela-
tionship between observable features and response
should evolve smoothly over time. This regularizer
allows the learner to rely more strongly on more re-
cent evidence, while taking into account a long his-
tory of training data. Our time series-inspired regu-
larizer is computationally efficient in learning and is
a significant advance over earlier text-driven fore-
casting models that ignore the time variable alto-
gether (Kogan et al, 2009; Joshi et al, 2010).
We evaluate our approaches in two novel experi-
mental settings: predicting downloads of economics
articles and predicting citation of papers at ACL
conferences. Our approaches substantially outper-
594
0
15
00
4 9
log(# downloads)
# d
oc
s.
0
25
00
0 18
# citations
# d
oc
s.
Figure 1: Left: the distribution of log download counts
for papers in the NBER dataset one year after post-
ing. Right: the distribution of within-dataset citations of
ACL papers within three years of publication (outliers ex-
cluded for readability).
form text-ignorant baselines on ground-truth predic-
tions. Our time series models permit flexibility in
features and offer a novel and perhaps more inter-
pretable view of the data than summary statistics.
2 Data
We make use of two collections of scientific litera-
ture, one from the economics domain, and the other
from computational linguistics and natural language
processing. Statistics are summarized in Table 1.
2.1 NBER
Our first dataset consists of research papers in eco-
nomics from the National Bureau of Economic
Research (NBER) from 1999 to 2009 (http://
www.nber.org). Approximately 1,000 research
economists are affiliated with the NBER. New
NBER working papers are posted to the website
weekly. The papers are not yet peer-reviewed, but
given the prominence of many economists affiliated
with the NBER, many of the papers are widely read.
Text from the abstracts of the papers and related
metadata are publicly available. Full text is available
to subscribers (universities typically have access).
The NBER provided us with download statistics
for these papers. For each paper, we computed
the total number of downloads in the first year af-
ter each paper?s posting.1 The download counts are
log-normally distributed, as shown in Figure 1, and
so our regression models (?3) minimize squared er-
rors in the log space. Our download logs begin in
1For the vast majority of papers, most of the downloads oc-
cur soon after the paper?s posting. We explored different mea-
sures with different download windows (two years, for exam-
ple) with broadly similar results. We leave a more detailed anal-
ysis of the time series patterns of downloads to future work.
Dataset # Docs. Avg. #
Words
Response
NBER 8,814 155 # downloads in first
year (mean 761)
ACL 4,026 3,966 at least 1 citation in
first 3 years? (54% no)
Table 1: Descriptive statistics about the datasets.
1999. We use the 8,814 papers from 1999?2009 pe-
riod (there are 16,334 papers in the full dataset dat-
ing back to 1985). We only use text from the ab-
stracts, since we were able to obtain full texts for
just a portion of the papers, and since the OCR of
the full texts we do have is very noisy.
2.2 ACL
Our second dataset consists of research papers
from the Association for Computational Linguis-
tics (ACL) from 1980 to 2006 (Radev et al, 2009a;
Radev et al, 2009b). We have the full texts for pa-
pers (OCR output) as well as structured citation data.
There are 15,689 papers in the whole dataset. For
the citation prediction task, we include conference
papers from ACL, EACL, HLT, and NAACL.2 We
remove journal papers, since they are characteristi-
cally different from conference papers, as well as
workshop papers. We do include short papers, in-
teractive demo session papers, and student research
papers that are included in the companion volumes
for these conferences (such papers are cited less than
full papers, but many are still cited). The resulting
dataset contains 4,026 papers. The number of pa-
pers in each year varies because not all conferences
are annual.
We look at citations in the three-year window fol-
lowing publication, excluding self-citations and only
considering citations from papers within these con-
ferences. Figure 1 shows a histogram; note that
many papers (54%) are not cited at all, and the dis-
tribution of citations per paper is neither normal nor
log-normal. We organize the papers into two classes:
those with zero citations and those with non-zero ci-
tations in the three-year window.
2EMNLP is a relatively recent conference, and, in this col-
lection, complete data for its papers postdate the end of the last
training period, so we chose to exclude it from our dataset.
595
3 Model
Our forecasting approach is based on generalized
linear models for regression and classification. The
models are trained with an `2-penalty, often called
a ?ridge? model (Hoerl and Kennard, 1970).3 For
the NBER data, where (log) number of downloads is
nearly a continuous measure, we use linear regres-
sion. For the ACL data, where response is the bi-
nary cited-or-not variable we use logistic regression,
often referred to as a ?maximum entropy? model
(Berger et al, 1996) or a log-linear model. We
briefly review the class of models. Then, we de-
scribe a time series model appropriate for time series
data.
3.1 Generalized Linear Models
Consider a model that predicts a response y given a
vector input x = ?x1, . . . , xd? ? Rd. Our models
are linear functions of x and parameterized by the
vector ?. Given a corpus of M document features,
X , and responses Y , we estimate:
?? = argmin? R(?) + L(?,X, Y ) (1)
where L is a model-dependent loss function and R
is a regularization penalty to encourage models with
small weight vectors. We describe models and loss
functions first and then turn to regularization.
For the NBER data, the (log) number of down-
loads is continuous, and so we use least-squares
linear regression model. The loss function is the
sum of the squared errors for the M documents in
our training data: L(?,X, Y ) = ?Mi=1(yi ? y?i)2,
where the prediction rule for new documents is:
y? =
?d
j=0 ?jxj . Probabilistically, this equates to an
assumption that ?>x is the mean of a normal (i.e.,
Gaussian) distribution from which random variable
y is drawn.
For the ACL data, we predict y from a discrete
set C (specifically, the binary set of zero citations or
more than zero citations), and we use logistic regres-
sion. This model assumes that for the ith training
input xi, the output yi is drawn according to:
p(yi | xi) =
(
exp?>c xi
) /(?
c??C exp?>c?xi
)
3Preliminary experiments found no consistent benefit from
`1 (?lasso?) models, though we note that `1-regularization leads
to sparse, compact models that may be more interpretable.
where there is a feature vector ?c for each class
c ? C. Under this interpretation, parameter esti-
mation is maximum a posteriori inference for ?,
and R(?) is a log-prior for the weights. The loss
function is the negative log likelihood for the M
documents: L(?,X, Y ) = ??Mi=1 log p(yi | xi).
The prediction rule for a new document is: y? =
argmaxc?C
?d
j=0 ?c,jxj . Generalized linear mod-
els and penalized regression are well-studied with
an extensive literature (Mccullagh and Nelder, 1989;
Hastie et al, 2009). We leave other types of mod-
els, such as Poisson (Cameron and Trivedi, 1998)
or ordinal (McCullagh, 1980) regression models, to
future work.
3.2 Ridge Regression
With large numbers of features, regularization is
crucial to avoid overfitting. In ridge regression (Ho-
erl and Kennard, 1970), a standard method to which
we compare the time series regularization discussed
in ?3.3, the penalty R(?) is proportional to the `2-
norm of ?:
R(?) = ????2 = ?
?
j ?2j
where ? is a regularization hyperparameter that is
tuned on development data or by cross-validation.4
This penalty pushes many ?j close (but not com-
pletely) to zero. In practice, we multiply the penalty
by the number of examples M to facilitate tuning of
?.
The ridge linear regression model can be inter-
preted probabilistically as each coefficient ?j is
drawn i.i.d. from a normal distribution with mean
0 and variance 2??1.
3.3 Time Series Regularization
A simple way to capture temporal variation is to con-
join traditional features with a time variable. Here,
we divide the dataset into T time steps (years). In the
new representation, the feature space expands from
Rd to RT?d. For a document published at year t, the
elements of x are non-zero only for those features
that correspond to year-t; that is xt?,j = 0 for all
t? 6= t.
4The linear regression has a bias ?0 that is always active.
The logistic regression also has an unpenalized bias ?c,0 for
each class c. This weight is not regularized.
596
Estimating this model with the new features using
the `2-penalty would be effectively estimating sepa-
rate models for each year under the assumption that
each ?t,j is independent; even for features that dif-
fered only temporally (e.g., ?t,j and ?t+1,j).
In this work, we apply time series regularization
to GLMs, enabling models that have coefficients that
change over time but prefer gradual changes across
time steps. Boyd and Vandenberghe (2004, ?6.3) de-
scribe a general version of this sort of regularizer.
To our knowledge, such regularizers have not previ-
ously been applied to temporal modeling of text.
The time series regularization penalty becomes:
R(?) = ?
T?
t=1
d?
j=1
?2t,j+??
T?
t=2
d?
j=0
(?t,j ? ?t?1,j)2
It includes a standard `2-penalty on the coefficients,
and a penalty for differences between coefficients
for adjacent time steps to induce smooth changes.5
Similar to the previous model, in practice, we mul-
tiply the regularization constant ? by MT to facili-tate tuning of ? for datasets with different numbers
of examples M and numbers of time steps T . The
new parameter, ?, controls the smoothness of the es-
timated coefficients. Setting ? to zero imposes no
penalty for time-variation in the coefficients and re-
sults in independent ridge regressions at each time
step. Also, when the number of examples is con-
stant across time steps, setting a large ? parameter
(? ? ?) results in a single ridge regression over all
years since it imposes ?t,j = ?t+1,j for all t ? T .
The partial derivative is:
?R/??t,j = 2??t,j
+ 1{t > 1}2??(?t,j ? ?t?1,j)
+ 1{t < T}2??(?t,j ? ?t+1,j)
This time series regularization can be applied more
generally, not just to linear and logistic regression.
With either ridge regularization or this time se-
ries regularization scheme, Eq. 1 is an unconstrained
convex optimization problem for the linear models
5Our implementation of the time series regularizer does not
penalize the magnitude of the weight for the bias feature (as in
ridge regression). It does, however, penalize the difference in
the bias weight between time steps (as with other features).
?
1
?
2
?
3
?
T
Y
1
Y
2
Y
3
Y
T
...
X
1
X
2
X
3
X
T
?,?
Figure 2: Time series regression as a graphical model;
the variables Xt and Yt are the sets of feature vectors
and response variables from documents dated t.
we describe here. There exist a number of optimiza-
tion procedures for it; we use the L-BFGS quasi-
Newton algorithm (Liu and Nocedal, 1989).
Probabilistic Interpretation
We can interpret the time series regularization prob-
abilistically as follows. Let the coefficient for the
jth feature over time be ?j = ??1,j , ?2,j , ..., ?T,j?.
?j are draws from a multivariate normal distribu-
tion with a tridiagonal precision matrix ??1 = ? ?
RT?T :
? = ?
?
??????
1 + ? ?? 0 0 . . .
?? 1 + 2? ?? 0 . . .
0 ?? 1 + 2? ?? . . .
0 0 ?? 1 + 2? . . .
... ... ... ... . . .
?
??????
The form of R(?) follows from noting:
?2 log p(?j ;?, ?) = ?>j ??j + constant
The squared difference between adjacent time steps
comes from the off-diagonal entries in the preci-
sion matrix.6 Figure 2 shows a graphical represen-
tation of the time series regularization in our model.
Its Markov chain structure corresponds to the off-
diagonals.
There is a rich literature on time series analysis
(Box et al, 2008; Hamilton, 1994). The prior dis-
tribution over the sequence ??1,j , . . . , ?T,j? that our
regularizer posits is closely linked to a first-order au-
toregressive process, AR(1).
6Consistent with the previous section, we assume that pa-
rameters for different features, ?j and ?k, are independent.
597
NBER ACL
Response log(#downloads+1) 1{#citations > 0}
GLM type normal / squared-loss logistic / log-loss
Metric 1 mean absolute error accuracy
Metric 2 Kendall?s ? Kendall?s ?
Table 2: Summary of the setup for the NBER download
and ACL citation prediction experiments.
4 Features
NBER metadata features
? Authors? last names. We treat each name as a bi-
nary feature. If a paper has multiple authors, all
authors are used and they have equal weights re-
gardless of their ordering.
? NBER program(s).7 There are 19 major re-
search programs at the NBER (e.g., Monetary
Economics, Health Economics, etc.).
ACL metadata features
? Authors? last names as binary features.
? Conference venues. We use first letter of the ACL
anthology paper ID, which correlates with its con-
ference venue (e.g., P for the ACL main confer-
ence, H for the HLT conference, etc.).8
Text features
? Binary indicator features for the presence of each
unigram, bigram, and trigram. For the NBER
data, we have separate features for titles and ab-
stracts. For the ACL data, we have separate fea-
tures for titles and full texts. We pruned text fea-
tures by document frequency (details in ?5).
? Log transformed word counts. We include fea-
tures for the numbers of words in the title and the
abstract (NBER) or the full text (ACL).
7Almost all NBER papers are tagged with one or more pro-
grams (we assign untagged papers a ?null? tag). The complete
list of NBER programs can be found at http://www.nber.
org/programs
8Papers in the ACL dataset have a tag which shows which
workshop, conference, or journal they appeared in. However,
sometimes a conference is jointly held with another confer-
ence, such that meta information in the dataset is different even
though the conference is the same. For this reason, we simply
use the first letter of the paper ID.
5 Experiments
For each of the datasets in ?2, we test our models
for two tasks: forecasting about future papers (i.e.,
making predictions about papers that appeared af-
ter a training dataset) and modeling held-out papers
from the past (i.e., making predictions within the
same time period as the training dataset, on held-out
examples).
For the NBER dataset, the task is to predict the
number of downloads a paper will receive in its first
year after publication. For the ACL dataset, the task
is to predict whether a paper will be cited at all (by
another ACL paper in our dataset) within the first
three years after its publication. To our knowledge,
clean, reliable citation counts are not available for
the NBER dataset; nor are download statistics avail-
able for the ACL dataset. Table 2 summarizes the
variables of interest, model types, and evaluation
metrics for the tasks.
5.1 Extrapolation
The lag between a paper?s publication and when its
outcome (download or citation count) can be ob-
served poses a unique methodological challenge.
Consider predicting the number of downloads over
g future time steps. If t is the time of forecasting,
we can observe the texts of all articles published be-
fore t. However, any article published in the interval
[t ? g, t] is too recent for the outcome measurement
of y to be taken. We refer to the interval [t? g, t] as
the ?forecast gap?. Since recent articles are some-
times the most relevant predictions at t, we do not
want to ignore them. Consider a paper at time step
t?, t?g < t? < t. To extrapolate its number of down-
loads, we consider the observed number in [t?, t], and
then estimate the ratio r of downloads that occur in
the first t?t? time steps, against the first g time steps,
using the fully observed portion of the training data.
We then scale the observed downloads during [t?, t]
by r?1 to extrapolate. The same method is used to
extrapolate citation counts.
In preliminary experiments, we observed that ex-
trapolating responses for papers in the forecast gap
led to better performance in general. For example,
for the ridge regressions trained on all past years
with the full feature set, the error dropped from 262
to 259 when using extrapolation compared to with-
598
out extrapolation. Also, the extrapolated download
counts were quite close to the true values (which we
have but do not use because of the forecast gap): for
example, the mean absolute error of the extrapolated
responses was 99 when extrapolated based on the
median of the fully observed portion of the training
data (measured monthly).
5.2 Forecasting NBER Downloads
In our first set of experiments, we predict the number
of downloads of an NBER paper within one year of
its publication.
We compare four approaches for predicting
downloads. The first is a baseline that simply uses
the median of the log of the training and develop-
ment data as the prediction. The second and third
use GLMs with ridge regression-style regularization
(?3.2), trained on all past years (?all years?) and on
the single most recent past year (?one year?), respec-
tively. The last model (?time series?) is a GLM with
time series regularization (?3.3).
We divided papers by year. Figure 3 illustrates the
experimental setup. We held out a random 20% of
papers for each year from 1999?2007 as a test set for
the task of modeling the past. To define the feature
set and tune hyperparameters, we used the remain-
ing 80% of papers from 1999?2005 as our training
data and the remaining papers in 2006 as our devel-
opment data. After pruning,9 we have 37,251 to-
tal features, of which 2,549 are metadata features.
When tuning hyperparameters, we simulated the ex-
istence of a forecast gap by using extrapolated re-
sponses for papers in the last year of the training
data instead of their true responses. We considered
? ? 5{2,1,...,?5,?6}, and ? ? 5{3,2,...,?1,?2} and se-
lected those that led to the best performance on the
development set.
We then used the selected feature set and hyperpa-
rameters to test the forecasting and modeling capa-
bilities of each model. For forecasting, we predicted
numbers of downloads of papers in 2008 and 2009.
We used the baseline median, ridge regression, and
time series regularization models trained on papers
in 1999?2007 and 1999?2008, respectively. We
treated the last year of the training data (2007 and
9For NBER, text features appearing in less than 0.1% or
more than 99.9% of the training documents were removed. For
ACL, the thresholds were 2% and 98%.
training
modeling test (unused)
gap dev.
trr taitan tag
training gap test
trr tai tam tao
ddd
ddd
modeling test (unused)
oae
lae
oae
lae
training gap test
trr tamddd
modeling test
oae
lae
tao tar
NBER Experiments
ACL Experiments
training
modeling test (unused)
dev.
toa tro ta ddd
oae
lae
s(u)p)v.(uu)p)vProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1357?1367, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Word Salad: Relating Food Prices and Descriptions
Victor Chahuneau Kevin Gimpel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,kgimpel}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Lily Scherlis
Phillips Academy
Andover, MA 01810, USA
lily.scherlis@gmail.com
Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We investigate the use of language in food
writing, specifically on restaurant menus and
in customer reviews. Our approach is to build
predictive models of concrete external vari-
ables, such as restaurant menu prices. We
make use of a dataset of menus and customer
reviews for thousands of restaurants in several
U.S. cities. By focusing on prediction tasks
and doing our analysis at scale, our method-
ology allows quantitative, objective measure-
ments of the words and phrases used to de-
scribe food in restaurants. We also explore
interactions in language use between menu
prices and sentiment as expressed in user re-
views.
1 Introduction
What words might a menu writer use to justify the
high price of a steak? How does describing an item
as chargrilled vs. charbroiled affect its price? When
a customer writes an unfavorable review of a restau-
rant, how is her word choice affected by the restau-
rant?s prices? In this paper, we explore questions
like these that relate restaurant menus, prices, and
customer sentiment. Our goal is to understand how
language is used in the food domain, and we di-
rect our investigation using external variables such
as restaurant menu prices.
We build on a thread of NLP research that seeks
linguistic understanding by predicting real-world
quantities from text data. Recent examples include
prediction of stock volatility (Kogan et al 2009)
and movie revenues (Joshi et al 2010). There, pre-
diction tasks were used for quantitative evaluation
and objective model comparison, while analysis of
learned models gave insight about the social process
behind the data.
We echo this pattern here as we turn our atten-
tion to language use on restaurant menus and in user
restaurant reviews. We use data from a large cor-
pus of restaurant menus and reviews crawled from
the web and formulate several prediction tasks. In
addition to predicting menu prices, we also consider
predicting sentiment along with price.
The relationship between language and senti-
ment is an active area of investigation (Pang and
Lee, 2008). Much of this research has focused on
customer-written reviews of goods and services, and
perspectives have been gained on how sentiment is
expressed in this type of informal text. In addition
to sentiment, however, other variables are reflected
in a reviewer?s choice of words, such as the price of
the item under consideration. In this paper, we take
a step toward joint modeling of multiple variables
in review text, exploring connections between price
and sentiment in restaurant reviews.
Hence this paper contributes an exploratory data
1357
analysis of language used to describe food (by its
purveyors and by its consumers). While our primary
goal is to understand the language used in our cor-
pus, our findings bear relevance to economics and
hospitality research as well. This paper is a step on
the way to the eventual goal of using linguistic anal-
ysis to understand social phenomena like sales and
consumption.
2 Related Work
There are several areas of related work scattered
throughout linguistics, NLP, hospitality research,
and economics.
Freedman and Jurafsky (2011) studied the use of
language in food advertising, specifically the words
on potato chip bags. They argued that, due to
the ubiquity of food writing across cultures, eth-
nic groups, and social classes, studying the use of
language for describing food can provide perspec-
tive on how different socioeconomic groups self-
identify using language and how they are linguisti-
cally targeted. In particular, they showed that price
affects how ?authenticity? is realized in marketing
language, a point we return to in ?5. This is an ex-
ample of how price can affect how an underlying
variable is expressed in language. Among other ex-
plorations in this paper, we consider how price inter-
acts with expression of sentiment in user reviews of
restaurants.
As mentioned above, our work is related to re-
search in predicting real-world quantities using text
data (Koppel and Shtrimberg, 2006; Ghose et al
2007; Lerman et al 2008; Kogan et al 2009; Joshi
et al 2010; Eisenstein et al 2010; Eisenstein et
al., 2011; Yogatama et al 2011). Like much of
this prior work, we aim to learn how language is
used in a specific context while building models that
achieve competitive performance on a quantitative
prediction task.
Along these lines, there is recent interest in ex-
ploring the relationship between product sales and
user-generated text, particularly online product re-
views. For example, Ghose and Ipeirotis (2011)
studied the sales impact of particular properties of
review text, such as readability, the presence of
spelling errors, and the balance between subjective
and objective statements. Archak et al(2011) had a
similar goal but decomposed user reviews into parts
describing particular aspects of the product being
reviewed (Hu and Liu, 2004). Our paper differs
from price modeling based on product reviews in
several ways. We consider a large set of weakly-
related products instead of a homogeneous selection
of a few products, and the reviews in our dataset are
not product-centered but rather describe the overall
experience of visiting a restaurant. Consequently,
menu items are not always mentioned in reviews and
rarely appear with their exact names. This makes it
difficult to directly use review features in a pricing
model for individual menu items.
Menu planning and pricing has been studied for
many years by the culinary and hospitality research
community (Kasavana and Smith, 1982; Kelly et al
1994), often including recommendations for writing
menu item descriptions (Miller and Pavesic, 1996;
McVety et al 2008). Their guidelines frequently
include example menus from successful restaurants,
but typically do not use large corpora of menus or
automated analysis, as we do here. Other work
focused more specifically on particular aspects of
the language used on menus, such as the study by
Zwicky and Zwicky (1980), who made linguistic ob-
servations through manual analysis of a corpus of
200 menus.
Relatedly, Wansink et al(2001; 2005) showed
that the way that menu items are described af-
fects customers? perceptions and purchasing behav-
ior. When menu items are described evocatively,
customers choose them more often and report higher
satisfaction with quality and value, as compared to
when they are given the same items described with
conventional names. Wansink et aldid not use a
corpus, but rather conducted a small-scale experi-
ment in a working cafeteria with customers and col-
lected surveys to analyze consumer reaction. While
our goals are related, our experimental approach is
different, as we use automated analysis of thousands
of restaurant menus and rely on a set of one mil-
lion reviews as a surrogate for observing customer
behavior.
Finally, the connection between products and
prices is also a central issue in economics. How-
ever, the stunning heterogeneity in products makes
empirical work challenging. For example, there are
over 50,000 menu items in New York that include
1358
City # Restaurants # Menu Items # Reviews
train dev. test train dev. test train dev. test
Boston 930 107 113 63,422 8,426 8,409 80,309 10,976 11,511
Chicago 804 98 100 51,480 6,633 6,939 73,251 9,582 10,965
Los Angeles 624 80 68 17,980 2,938 1,592 75,455 13,227 5,716
New York 3,965 473 499 365,518 42,315 45,728 326,801 35,529 37,795
Philadelphia 1,015 129 117 83,818 11,777 9,295 52,275 7,347 5,790
San Francisco 1,908 255 234 103,954 12,871 12,510 499,984 59,378 67,010
Washington, D.C. 773 110 121 47,188 5,957 7,224 71,179 11,852 14,129
Total 10,019 1,252 1,252 733,360 90,917 91,697 1,179,254 147,891 152,916
Table 1: Dataset statistics.
the word chicken. What is the price of chicken? This
is an important practical and daunting matter when
measuring inflation (e.g., Consumer Price Index is
measured with a precisely-defined basket of goods).
Price dispersion across goods and the variation of
the goods is an important area of industrial organi-
zation economic theory. For example, economists
are interested in models of search, add-on pricing,
and obfuscation (Baye et al 2006; Ellison, 2005).
3 Data
We crawled Allmenus.com (www.allmenus.
com) to gather menus for restaurants in seven
U.S. cities: Boston, Chicago, Los Angeles, New
York, Philadelphia, San Francisco, and Washing-
ton, D.C. Each menu includes a list of item names
with optional text descriptions and prices. Most All-
menus restaurant pages contain a link to the cor-
responding page on Yelp (www.yelp.com) with
metadata and user reviews for the restaurant, which
we also collected.
The metadata consist of many fields for each
restaurant, which can be divided into three cate-
gories: location (city, neighborhood, transit stop),
services available (take-out, delivery, wifi, parking,
etc.), and ambience (good for groups, noise level,
attire, etc.). Also, the category of food and a price
range ($ to $$$$, indicating the price of a typical
meal at the restaurant) are indicated. The user re-
views include a star rating on a scale of 1 to 5.
The distribution of prices of individual menu
items is highly skewed, with a mean of $9.22 but
a median of $6.95. On average, a restaurant has
73 items on its menu with a median price of $8.69
and 119 Yelp reviews with a median rating of 3.55
????????
0
100k
200k
300k
400k
500k
  
 
 
 
 
 
 
 
 
star rating
??????? ????????
?????????????
????????????????????????????
???Dynamic Language Models for Streaming Text
Dani Yogatama? Chong Wang? Bryan R. Routledge? Noah A. Smith? Eric P. Xing?
?School of Computer Science
?Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
?{dyogatama,chongw,nasmith,epxing}@cs.cmu.edu, ?routledge@cmu.edu
Abstract
We present a probabilistic language model that
captures temporal dynamics and conditions on
arbitrary non-linguistic context features. These
context features serve as important indicators
of language changes that are otherwise difficult
to capture using text data by itself. We learn
our model in an efficient online fashion that is
scalable for large, streaming data. With five
streaming datasets from two different genres?
economics news articles and social media?we
evaluate our model on the task of sequential
language modeling. Our model consistently
outperforms competing models.
1 Introduction
Language models are a key component in many NLP
applications, such as machine translation and ex-
ploratory corpus analysis. Language models are typi-
cally assumed to be static?the word-given-context
distributions do not change over time. Examples
include n-gram models (Jelinek, 1997) and proba-
bilistic topic models like latent Dirichlet allocation
(Blei et al., 2003); we use the term ?language model?
to refer broadly to probabilistic models of text.
Recently, streaming datasets (e.g., social media)
have attracted much interest in NLP. Since such data
evolve rapidly based on events in the real world, as-
suming a static language model becomes unrealistic.
In general, more data is seen as better, but treating all
past data equally runs the risk of distracting a model
with irrelevant evidence. On the other hand, cau-
tiously using only the most recent data risks overfit-
ting to short-term trends and missing important time-
insensitive effects (Blei and Lafferty, 2006; Wang
et al., 2008). Therefore, in this paper, we take steps
toward methods for capturing long-range temporal
dynamics in language use.
Our model also exploits observable context vari-
ables to capture temporal variation that is otherwise
difficult to capture using only text. Specifically for
the applications we consider, we use stock market
data as exogenous evidence on which the language
model depends. For example, when an important
company?s price moves suddenly, the language model
should be based not on the very recent history, but
should be similar to the language model for a day
when a similar change happened, since people are
likely to say similar things (either about that com-
pany, or about conditions relevant to the change).
Non-linguistic contexts such as stock price changes
provide useful auxiliary information that might indi-
cate the similarity of language models across differ-
ent timesteps.
We also turn to a fully online learning framework
(Cesa-Bianchi and Lugosi, 2006) to deal with non-
stationarity and dynamics in the data that necessitate
adaptation of the model to data in real time. In on-
line learning, streaming examples are processed only
when they arrive. Online learning also eliminates
the need to store large amounts of data in memory.
Strictly speaking, online learning is distinct from
stochastic learning, which for language models built
on massive datasets has been explored by Hoffman
et al. (2013) and Wang et al. (2011). Those tech-
niques are still for static modeling. Language model-
ing for streaming datasets in the context of machine
translation was considered by Levenberg and Os-
borne (2009) and Levenberg et al. (2010). Goyal
et al. (2009) introduced a streaming algorithm for
large scale language modeling by approximating n-
gram frequency counts. We propose a general online
learning algorithm for language modeling that draws
inspiration from regret minimization in sequential
predictions (Cesa-Bianchi and Lugosi, 2006) and on-
181
Transactions of the Association for Computational Linguistics, 2 (2014) 181?192. Action Editor: Eric Fosler-Lussier.
Submitted 10/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
line variational algorithms (Sato, 2001; Honkela and
Valpola, 2003).
To our knowledge, our model is the first to bring
together temporal dynamics, conditioning on non-
linguistic context, and scalable online learning suit-
able for streaming data and extensible to include
topics and n-gram histories. The main idea of our
model is independent of the choice of the base lan-
guage model (e.g., unigrams, bigrams, topic models,
etc.). In this paper, we focus on unigram and bi-
gram language models in order to evaluate the basic
idea on well understood models, and to show how it
can be extended to higher-order n-grams. We leave
extensions to topic models for future work.
We propose a novel task to evaluate our proposed
language model. The task is to predict economics-
related text at a given time, taking into account the
changes in stock prices up to the corresponding day.
This can be seen an inverse of the setup considered
by Lavrenko et al. (2000), where news is assumed
to influence stock prices. We evaluate our model
on economics news in various languages (English,
German, and French), as well as Twitter data.
2 Background
In this section, we first discuss the background for
sequential predictions then describe how to formulate
online language modeling as sequential predictions.
2.1 Sequential Predictions
Let w1, w2, . . . , wT be a sequence of response vari-
ables, revealed one at a time. The goal is to design
a good learner to predict the next response, given
previous responses and additional evidence which
we denote by xt ? RM (at time t). Throughout this
paper, we use the term features for x. Specifically, at
each round t, the learner receives xt and makes a pre-
diction w?t, by choosing a parameter vector?t ? RM .
In this paper, we refer to ? as feature coefficients.
There has been an enormous amount of work on
online learning for sequential predictions, much of it
building on convex optimization. For a sequence of
loss functions `1, `2, . . . , `T (parameterized by ?),
an online learning algorithm is a strategy to minimize
the regret, with respect to the best fixed ?? in hind-
sight.1 Regret guarantees assume a Lipschitz con-
1Formally, the regret is defined as RegretT (??) =
dition on the loss function ` that can be prohibitive
for complex models. See Cesa-Bianchi and Lugosi
(2006), Rakhlin (2009), Bubeck (2011), and Shalev-
Shwartz (2012) for in-depth discussion and review.
There has also been work on online and stochastic
learning for Bayesian models (Sato, 2001; Honkela
and Valpola, 2003; Hoffman et al., 2013), based on
variational inference. The goal is to approximate pos-
terior distributions of latent variables when examples
arrive one at a time.
In this paper, we will use both kinds of techniques
to learn language models for streaming datasets.
2.2 Problem Formulation
Consider an online language modeling problem, in
the spirit of sequential predictions. The task is to
build a language model that accurately predicts the
texts generated on day t, conditioned on observ-
able features up to day t, x1:t. Every day, after
the model makes a prediction, the actual texts wt
are revealed and we suffer a loss. The loss is de-
fined as the negative log likelihood of the model
`t = ? log p(wt | ?,?1:t?1,x1:t?1,n1:t?1), where
? and ?1:T are the model parameters andn is a back-
ground distribution (details are given in ?3.2). We
can then update the model and proceed to day t+ 1.
Notice the similarity to the sequential prediction de-
scribed above. Importantly, this is a realistic setup for
building evolving language models from large-scale
streaming datasets.
3 Model
3.1 Notation
We index timesteps by t ? {1, . . . , T} and word
types by v ? {1, . . . , V }, both are always given as
subscripts. We denote vectors in boldface and use
1 : T as a shorthand for {1, 2, . . . , T}. We assume
words of the form {wt}Tt=1 for wt ? RV , which is
the vector of word frequences at timetstep t. Non-
linguistic context features are {xt}Tt=1 for xt ? RM .
The goal is to learn parameters ? and ?1:T , which
will be described in detail next.
3.2 Generative Story
The main idea of our model is illustrated by the fol-
lowing generative story for the unigram language
PT
t=1 `t(xt,?t, wt)? inf??
PT
t=1 `t(xt,??, wt).
182
model. (We will discuss the extension to higher-order
language models later.) A graphical representation
of our proposed model is given in Figure 1.
1. Draw feature coefficients ? ? N(0, ?I).2 Here
? is a vector in RM , where M is the dimension-
ality of the feature vector.
2. For each timestep t:
(a) Observe non-linguistic context features xt.
(b) Draw ?t ?
N
(?t?1
k=1 ?k
exp(?>f(xt,xk))Pt?1
j=1 ?j exp(?>f(xt,xj))
?k, ?I
)
.
Here, ?t is a vector in RV , where V is
the size of the word vocabulary, ? is
the variance parameter and ?k is a fixed
hyperparameter; we discuss them below.
(c) For each word wt,v, draw wt,v ?
Categorical
(
exp(n1:t?1,v+?t,v)P
j?V exp(n1:t?1,j+?t,j)
)
.
In the last step, ?t and n are mapped to the V -
dimensional simplex, forming a distribution over
words. n1:t?1 ? RV is a background (log) distri-
bution, inspired by a similar idea in Eisenstein et al.
(2011). In this paper, we set n1:t?1,v to be the log-
frequency of v up to time t? 1. We can interpret ?
as a time-dependent deviation from the background
log-frequencies that incorporates world-context. This
deviation comes in the form of a weighted average of
earlier deviation vectors.
The intuition behind the model is that the probabil-
ity of a word appearing at day t depends on the back-
ground log-frequencies, the deviation coefficients of
the word at previous timesteps ?1:t?1, and the sim-
ilarity of current conditions of the world (based on
observable features x) to previous timesteps through
f(xt,xk). That is, f is a function that takes d-
dimensional feature vectors at two timesteps xt and
xk and returns a similarity vector f(xt,xk) ? RM
(see ?6.1.1 for an example of f that we use in our
experiments). The similarity is parameterized by ?,
and decays over time with rate ?k. In this work, we
assume a fixed window size c (i.e., we consider c
most recent timesteps), so that ?1:t?c?1 = 0 and
?t?c:t?1 = 1. This allows up to cth order depen-
dencies.3 Setting ? this way allows us to bound the
2Feature coefficients ? can be also drawn from other distri-
butions such as ? ? Laplace(0, ?).
3In online Bayesian learning, it is known that forgetting
inaccurate estimates from earlier timesteps is important (Sato,
 
xtxsxrxq
wq wr ws wt
 t s r q
?
NrNq Ns Nt
T
Figure 1: Graphical representation of the model. The
subscript indices q, r, s are shorthands for the previ-
ous timesteps t ? 3, t ? 2, t ? 1. Only four timesteps
are shown here. There are arrows from previous
?t?4,?t?5, . . . ,?t?c to ?t, where c is the window size
as described in ?3.2. They are not shown here, for read-
ability.
number of past vectors ? that need to be kept in
memory. We set ?0 to 0.
Although the generative story described above
is for unigram language models, extensions can be
made to more complex models (e.g., mixture of un-
igrams, topic models, etc.) and to longer n-gram
contexts. In the case of topic models, the model
will be related to dynamic topic models (Blei and
Lafferty, 2006) augmented by context features, and
the learning procedure in ?4 can be used to perform
online learning of dynamic topic models. However,
our model captures longer-range dependencies than
dynamic topic models, and can condition on non-
linguistic features or metadata. In the case of higher-
order n-grams, one simple way is to draw more ?,
one for each history. For example, for a bigram
model, ? is in RV 2 , rather than RV in the unigram
model. We consider both unigram and bigram lan-
guage models in our experiments in ?6. However, the
main idea presented in this paper is largely indepen-
dent of the base model.
Related work. Mimno and McCallum (2008) and
Eisenstein et al. (2010) similarly conditioned text on
2001; Honkela and Valpola, 2003). Since we set ?1:t?c?1 = 0,
at every timestep t, ?k leads to forgetting older examples.
183
observable features (e.g., author, publication venue,
geography, and other document-level metadata), but
conducted inference in a batch setting, thus their ap-
proaches are not suitable for streaming data. It is not
immediately clear how to generalize their approach to
dynamic settings. Algorithmically, our work comes
closest to the online dynamic topic model of Iwata
et al. (2010), except that we also incorporate context
features.
4 Learning and Inference
The goal of the learning procedure is to minimize the
overall negative log likelihood,
? logL(D) =
? log
?
d?1:T p(?1:T | ?,x1:T )p(w1:T | ?1:T ,n).
However, this quantity is intractable. Instead, we
derive an upper bound for this quantity and minimize
that upper bound. Using Jensen?s inequality, the vari-
ational upper bound on the negative log likelihood
is:
? logL(D) ? ?
?
d?1:T q(?1:T | ?1:T ) (4)
log p(?1:T | ?,x1:T )p(w1:T | ?1:T ,n)q(?1:T | ?1:T )
.
Specifically, we use mean-field variational inference
where the variables in the variational distribution q
are completely independent. We use Gaussian distri-
butions as our variational distributions for ?, denoted
by ? in the bound in Eq. 4. We denote the parameters
of the Gaussian variational distribution for ?t,v (word
v at timestep t) by ?t,v (mean) and ?t,v (variance).
Figure 2 shows the functional form of the varia-
tional bound that we seek to minimize, denoted by B?.
The two main steps in the optimization of the bound
are inferring ?t and updating feature coefficients ?.
We next describe each step in detail.
4.1 Learning
The goal of the learning procedure is to minimize the
upper bound in Figure 2 with respect to ?. However,
since the data arrives in an online fashion, and speed
is very important for processing streaming datasets,
the model needs to be updated at every timestep t (in
our experiments, daily).
Notice that at timestep t, we only have access
to x1:t and w1:t, and we perform learning at every
timestep after the text for the current timestep wt
is revealed. We do not know xt+1:T and wt+1:T .
Nonetheless, we want to update our model so that
it can make a better prediction at t + 1. Therefore,
we can only minimize the bound until timestep t.
Let Ck , exp(?>f(xt,xk))Pt?1
j=t?c exp(?>f(xt,xj))
. Our learning al-
gorithm is a variational Expectation-Maximization
algorithm (Wainwright and Jordan, 2008).
E-step Recall that we use variational inference and
the variational parameters for ? are ? and ?. As
shown in Figure 2, since the log-sum-exp in the last
term of B is problematic, we introduce additional
variational parameters ? to simplify B and obtain
B? (Eqs. 2?3). The E-step deals with all the local
variables ?, ?, and ?.
Fixing other variables and taking the derivative
of the bound B? w.r.t. ?t and setting it to zero,
we obtain the closed-form update for ?t: ?t =?
v?V exp (n1:t?1,v) exp
(
?t,v + ?t,v2
).
To minimize with respect to ?t and ?t, we apply
gradient-based methods since there are no closed-
form solutions. The derivative w.r.t. ?t,v is:
?B?
??t,v
=?t,v ? Ck?k,v?
? nt,v +
nt
?t
exp (n1:t?1,v) exp
(
?t,v +
?t,v
2
)
,
where nt =?v?V nt,v.
The derivative w.r.t. ?t,v is:
?B?
??t,v
= 12?t,v
+ 12? +
nt
2?t
exp (n1:t?1,v) exp
(
?t,v +
?t,v
2
)
.
Although we require iterative methods in the E-step,
we find it to be reasonably fast in practice.4 Specifi-
cally, we use the L-BFGS quasi-Newton algorithm
(Liu and Nocedal, 1989).
We can further improve the bound by updating
the variational parameters for timestep 1 : t? 1, i.e.,
?1:t?1 and?1:t?1, as well. However, this will require
storing the texts from previous timesteps. Addition-
ally, this will complicate the M-step update described
4Approximately 16.5 seconds/day (walltime) to learn the
model on the EN:NA dataset on a 2.40GHz CPU with 24GB
memory.
184
B =?
T?
t=1
Eq[log p(?t | ?k,?,xt)]?
T?
t=1
Eq[log p(wt | ?t,nt)]?H(q) (1)
=
T?
t=1
?
??
??
1
2
?
j?V
log ?t,j? ? Eq
?
???
(
?t ?
?t?1
k=t?c Ck?k
)2
2?
?
??? Eq
?
??
v?wt
n1:t?1,v + ?t,v ? log
?
j?V
exp(n1:t?1,j + ?t,j)
?
?
?
??
??
(2)
?
T?
t=1
?
??
??
1
2
?
j?V
log ?t,v? +
(
?t ?
?t?1
k=t?c Ck?k
)2
2? +
?t +
?t?1
k=t?c C2k?k
2?
?
?
v?wt
?
??t,v ? log ?t ?
1
?t
?
j?V
exp (n1:t?1,j) exp
(
?t,j +
?t,j
2
)
?
?
?
??
??
+ const (3)
Figure 2: The variational bound that we seek to minimize, B. H(q) is the entropy of the variational distribution q. The
derivation from line 1 to line 2 is done by replacing the probability distributions p(?t | ?k,?,xt) and p(wt | ?t,nt)
by their respective functional forms. Notice that in line 3 we compute the expectations under the variational distributions
and further bound B by introducing additional variational parameters ? using Jensen?s inequality on the log-sum-exp in
the last term. We denote the new bound B?.
below. Therefore, for each s < t, we choose to fix
?s and ?s once they are learned at timestep s.
M-step In the M-step, we update the global pa-
rameter ?, fixing ?1:t. Fixing other parameters and
taking the derivative of B? w.r.t. ?, we obtain:5
?B?
?? =
(?t ?
?t?1
k=t?cCk?k)(?
?t?1
k=t?c
?Ck
?? )
?
+
?t?1
k=t?cCk?k ?Ck??
? ,
where:
?Ck
?? =Ckf(xt,xk)
?Ck
?t?1
s=t?c f(xt,xs) exp(?>f(xt,xs))?t?1
s=t?c exp(?>f(xt,xs))
.
We follow the convex optimization strategy and sim-
ply perform a stochastic gradient update: ?t+1 =
?t + ?t ?B???t (Zinkevich, 2003). While the variational
bound B? is not convex, given the local variables ?1:t
5In our implementation, we augment ? with a squared L2
regularization term (i.e., we assume that ? is drawn from a
normal distribution with mean zero and variance ?) and use the
FOBOS algorithm (Duchi and Singer, 2009). The derivative
of the regularization term is simple and is not shown here. Of
course, other regularizers (e.g., the L1-norm, which we use for
other parameters, or the L1/?-norm) can also be explored.
and ?1:t, optimizing ? at timestep t without know-
ing the future becomes a convex problem.6 Since
we do not reestimate ?1:t?1 and ?1:t?1 in the E-step,
the choice to perform online gradient descent instead
of iteratively performing batch optimization at every
timestep is theoretically justified.
Notice that our overall learning procedure is still
to minimize the variational upper bound B?. All these
choices are made to make the model suitable for
learning in real time from large streaming datasets.
Preliminary experiments showed that performing
more than one EM iteration per day does not consid-
erably improve performance, so in our experiments
we perform one EM iteration per day.
To learn the parameters of the model, we rely on
approximations and optimize an upper bound B?. We
have opted for this approach over alternatives (such
as MCMC methods) because of our interest in the
online, large-data setting. Our experiments show that
we are still able to learn reasonable parameter esti-
mates by optimizing B?. Like online variational meth-
ods for other latent-variable models such as LDA
(Sato, 2001; Hoffman et al., 2013), open questions re-
main about the tightness of such approximations and
the identifiability of model parameters. We note, how-
6As a result, our algorithm is Hannan consistent w.r.t. the
best fixed ? (for B?) in hindsight; i.e., the average regret goes to
zero as T goes to?.
185
ever, that our model does not include latent mixtures
of topics and may be generally easier to estimate.
5 Prediction
As described in ?2.2, our model is evaluated by the
loss suffered at every timestep, where the loss is
defined as the negative log likelihood of the model
on text at timestep wt. Therefore, at each timestep t,
we need to predict (the distribution of)wt. In order
to do this, for each word v ? V , we simply compute
the deviation means ?t,v as weighted combinations
of previous means, where the weights are determined
by the world-context similarity encoded in x:
Eq[?t,v | ?t,v] =
t?1?
k=t?c
exp(?>f(xt,xk))?t?1
j=t?c exp(?>f(xt,xj))
?k,v.
Recall that the word distribution that we use for
prediction is obtained by applying the operator pi
that maps ?t and n to the V -dimensional simplex,
forming a distribution over words: pi(?t,n1:t?1)v =
exp(n1:t?1,v+?t,v)P
j?V exp(n1:t?1,j+?t,j)
, where n1:t?1,v ? RV is a
background distribution (the log-frequency of word
v observed up to time t? 1).
6 Experiments
In our experiments, we consider the problem of pre-
dicting economy-related text appearing in news and
microblogs, based on observable features that reflect
current economic conditions in the world at a given
time. In the following, we describe our dataset in de-
tail, then show experimental results on text prediction.
In all experiments, we set the window size c = 7 (one
week) or c = 14 (two weeks), ? = 12|V | (V is thesize of vocabulary of the dataset under consideration),
and ? = 1.
6.1 Dataset
Our data contains metadata and text corpora. The
metadata is used as our features, whereas the text
corpora are used for learning language models and
predictions. The dataset (excluding Twitter) can
be downloaded at http://www.ark.cs.cmu.
edu/DynamicLM.
6.1.1 Metadata
We use end-of-day stock prices gathered from
finance.yahoo.com for each stock included in
the Standard & Poor?s 500 index (S&P 500). The
index includes large (by market value) companies
listed on US stock exchanges.7 We calculate daily
(continuously compounded) returns for each stock, o:
ro,t = logPo,t? logPo,t?1, where Po,t is the closing
stock price.8 We make a simplifying assumption that
text for day t is generated after Po,t is observed.9
In general, stocks trade Monday to Friday (except
for federal holidays and natural disasters). For days
when stocks do not trade, we set ro,t = 0 for all
stocks since any price change is not observed.
We transform returns into similarity values as fol-
lows: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k)
and 0 otherwise. While this limits the model by ig-
noring the magnitude of price changes, it is still rea-
sonable to capture the similarity between two days.10
There are 500 stocks in the S&P 500, so xt ? R500
and f(xt,xk) ? R500.
6.1.2 Text data
We have five streams of text data. The first four
corpora are news streams tracked through Reuters.11
Two of them are written in English, North American
Business Report (EN:NA) and Japanese Investment
News (EN:JP). The remaining two are German Eco-
nomic News Service (DE, in German) and French
Economic News Service (FR, in French). For all four
of the Reuters streams, we collected news data over
a period of thirteen months (392 days), 2012-05-26
to 2013-06-21. See Table 1 for descriptive statistics
of these datasets. Numerical terms are mapped to a
single word, and all letters are downcased.
The last text stream comes from the Deca-
hose/Gardenhose stream from Twitter. We collected
public tweets that contain ticker symbols (i.e., sym-
bols that are used to denote stocks of a particular
company in a stock market), preceded by the dollar
7For a list of companies listed in the S&P 500 as of
2012, see http://en.wikipedia.org/wiki/List_
of_S\%26P_500_companies. This set was fixed during
the time periods of all our experiments.
8We use the ?adjusted close? on Yahoo that includes interim
dividend cash flows and also adjusts for ?splits? (changes in the
number of outstanding shares).
9This is done in order to avoid having to deal with hourly
timesteps. In addition, intraday price data is only available
through commercial data provided.
10Note that daily stock returns are equally likely to be positive
or negative and display little serial correlation.
11http://www.reuters.com
186
Dataset Total # Doc. Avg. # Doc. #Days Unigrams BigramsTotal # Tokens Size Vocab. Total # Tokens Size Vocab.
EN:NA 86,683 223 392 28,265,550 10,000 11,804,201 5,000
EN:JP 70.807 182 392 16,026,380 10,000 7,047,095 5,000
FR 62,355 160 392 11,942,271 10,000 3,773,517 5,000
DE 51,515 132 392 9,027,823 10,000 3,499,965 5,000
Twitter 214,794 336 639 1,660,874 10,000 551,768 5,000
Table 1: Statistics about the datasets. Average number of documents (third column) is per day.
sign $ (e.g., $GOOG, $MSFT, $AAPL, etc.). These
tags are generally used to indicate tweets about the
stock market. We look at tweets from the period
2011-01-01 to 2012-09-30 (639 days). As a result,
we have approximately 100?800 tweets per day. We
tokenized the tweets using the CMU ARK TweetNLP
tools,12 numerical terms are mapped to a single word,
and all letters are downcased.
We perform two experiments using unigram and
bigram language models as the base models. For
each dataset, we consider the top 10,000 unigrams
after removing corpus-specific stopwords (the top
100 words with highest frequencies). For the bigram
experiments, we only use 5,000 words to limit the
number of unique bigrams so that we can simulate
experiments for the entire time horizon in a reason-
able amount of time. In standard open-vocabulary
language modeling experiments, the treatment of un-
known words deserves care. We have opted for a
controlled, closed-vocabulary experiment, since stan-
dard smoothing techniques will almost surely interact
with temporal dynamics and context in interesting
ways that are out of scope in the present work.
6.2 Baselines
Since this is a forecasting task, at each timestep, we
only have access to data from previous timesteps.
Our model assumes that all words in all documents
in a corpus come from a single multinomial distri-
bution. Therefore, we compare our approach to the
corresponding base models (standard unigram and bi-
gram language models) over the same vocabulary (for
each stream). The first one maintains counts of every
word and updates the counts at each timestep. This
corresponds to a base model that uses all of the avail-
able data up to the current timestep (?base all?). The
second one replaces counts of every word with the
12https://www.ark.cs.cmu.edu/TweetNLP
counts from the previous timestep (?base one?). Ad-
ditionally, we also compare with a base model whose
counts decay exponentially (?base exp?). That is, the
counts from previous timesteps decay by exp(??s),
where s is the distance between previous timesteps
and the current timestep and ? is the decay constant.
We set the decay constant ? = 1. We put a symmetric
Dirichlet prior on the counts (?add-one? smoothing);
this is analogous to our treatment of the background
frequencies n in our model. Note that our model,
similar to ?base all,? uses all available data up to
timestep t? 1 when making predictions for timestep
t. The window size c only determines which previ-
ous timesteps? models can be chosen for making a
prediction today. The past models themselves are es-
timated from all available data up to their respective
timesteps.
We also compare with two strong baselines: a lin-
ear interpolation of ?base one? models for the past
week (?int. week?) and a linear interpolation of ?base
all? and ?base one? (?int one all?). The interpolation
weights are learned online using the normalized expo-
nentiated gradient algorithm (Kivinen and Warmuth,
1997), which has been shown to enjoy a stronger
regret guarantee compared to standard online gra-
dient descent for learning a convex combination of
weights.
6.3 Results
We evaluate the perplexity on unseen dataset to eval-
uate the performance of our model. Specifically, we
use per-word predictive perplexity:
perplexity = exp
(
?
?T
t=1 log p(wt | ?,x1:t,n1:t?1)?T
t=1
?
j?V wt,j
)
.
Note that the denominator is the number of tokens
up to timestep T . Lower perplexity is better.
Table 2 and Table 3 show the perplexity results for
187
Dataset base all base one base exp int. week int. one all c = 7 c = 14
EN:NA 3,341 3,677 3,486 3,403 3,271 3,262 3,285
EN:JP 2,802 3,212 2,750 2,949 2,708 2,656 2,689
FR 3,603 3,910 3,678 3,625 3,416 3,404 3,438
DE 3,789 4,199 3,979 3,926 3,634 3,649 3,687
Twitter 3,880 6,168 5,133 5,859 4,047 3,801 3,819
Table 2: Perplexity results for our five data streams in the unigram experiments. The base models in ?base all,? ?base
one,? and ?base exp? are unigram language models. ?int. week? is a linear interpolation of ?base one? from the past
week. ?int. one all? is a linear interpolation of ?base one? and ?base all?. The rightmost two columns are versions of
our model. Best results are highlighted in bold.
Dataset base all base one base exp int. week int. one all c = 7
EN:NA 242 2,229 1,880 2,200 244 223
EN:JP 185 2,101 1,726 2,050 189 167
FR 159 2,084 1,707 2,068 166 139
DE 268 2,634 2,267 2,644 282 243
Twitter 756 4,245 4,253 5,859 4,046 739
Table 3: Perplexity results for our five data streams in the bigram experiments. The base models in ?base all,? ?base
one,? and ?base exp? are bigram language models. ?int. week? is a linear interpolation of ?base one? from the past
week. ?int. one all? is a linear interpolation of ?base one? and ?base all?. The rightmost column is a version of our
model with c = 7. Best results are highlighted in bold.
each of the datasets for unigram and bigram experi-
ments respectively. Our model outperformed other
competing models in all cases but one. Recall that we
only define the similarity function of world context
as: f(xo,t, xo,k) = 1 iff sign(ro,t) = sign(ro,k) and
0 otherwise. A better similarity function (e.g., one
that takes into account market size of the company
and the magnitude of increase or decrease in the stock
price) might be able to improve the performance fur-
ther. We leave this for future work. Furthermore,
the variations can be captured using models from the
past week. We discuss why increasing c from 7 to 14
did not improve performance of the model in more
detail in ?6.4.
We can also see how the models performed over
time. Figure 4 traces perplexity for four Reuters news
stream datasets.13 We can see that in some cases the
performance of the ?base all? model degraded over
time, whereas our model is more robust to temporal
13In both experiments, in order to manage the time and space
complexities of updating ?, we apply a sparsity shrinkage tech-
nique by using OWL-QN (Andrew and Gao, 2007) when maxi-
mizing it, with regularization constant set to 1. Intuitively, this
is equivalent to encouraging the deviation vector to be sparse
(Eisenstein et al., 2011).
shifts.
In the bigram experiments, we only ran our model
with c = 7, since we need to maintain ? in RV 2 ,
instead of RV in the unigram model. The goal of
this experiment is to determine whether our method
still adds benefit to more expressive language mod-
els. Note that the weights of the linear interpolation
models are also learned in an online fashion since
there are no classical training, development, and test
sets in our setting. Since the ?base one? model per-
formed poorly in this experiment, the performance of
the interpolated models also suffered. For example,
the ?int. one all? model needed time to learn that the
?base one? model has to be downweighted (we started
with all interpolated models having uniform weights),
so it was not able to outperform even the ?base all?
model.
6.4 Analysis and Discussion
It should not be surprising that conditioning on
world-context reduces perplexity (Cover and Thomas,
1991). A key attraction of our model, we believe, lies
in the ability to inspect its parameters.
Deviation coefficients. Inspecting the model al-
lows us to gain insight into temporal trends. We
188
Twitter:Google
timestep
?
0 100 200 300 400 500 600
0.0
0.5
1.0
1.5
2.0
goog
@google
google+
#goog
r
GOOG
Twitter:Microsoft
timestep
?
0 100 200 300 400 500 600
0.0
0.5
1.0
1.5
microsoft
msft
#microsoft
r
MSFT
Figure 3: Deviation coefficients ? over time for Google- and Microsoft-related words on Twitter with unigram base
model (c = 7). Significant changes (increases or decreases) in the returns of Google and Microsoft stocks are usually
followed by increases in ? of related words.
investigate the deviations learned by our model on the
Twitter dataset. Examples are shown in Figure 3. The
left plot shows ? for four words related to Google:
goog, #goog, @google, google+. For compari-
son, we also show the return of Google stock for the
corresponding timestep (scaled by 50 and centered at
0.5 for readability, smoothed using loess (Cleveland,
1979), denoted by rGOOG in the plot). We can see
that significant changes of return of Google stocks
(e.g., the rGOOG spikes between timesteps 50?100,
150?200, 490?550 in the plot) occurred alongside
an increase in ? of Google-related words. Similar
trends can also be observed for Microsoft-related
words in the right plot. The most significant loss of
return of Microsoft stocks (the downward spike near
timestep 500 in the plot) is followed by a sudden
sharp increase in ? of the words #microsoft and
microsoft.
Feature coefficients. We can also inspect the
learned feature coefficients ? to investigate which
stocks have higher associations with the text that
is generated. Our feature coefficients are designed
to reflect which changes (or lack of changes) in
stock prices influence the word distribution more,
not which stocks are talked about more often. We
find that the feature coefficients do not correlate with
obvious company characteristics like market capi-
talization (firm size). For example, on the Twitter
dataset with bigram base models, the five stocks with
the highest weights are: ConAgra Foods Inc., Intel
Corp., Bristol-Myers Squibb, Frontier Communica-
tions Corp., and Amazon.com Inc. Strongly negative
weights tended to align with streams with less activ-
time lags
frequ
ency
0
20
40
60
80
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Figure 5: Distributions of the selection probabilities of
models from the previous c = 14 timesteps, on the EN:NA
dataset with unigram base model. For simplicity, we show
E-step modes. The histogram shows that the model tends
to favor models from days closer to the current date.
ity, suggesting that these were being used to smooth
across all c days of history. A higher weight for stock
o implies an increase in probability of choosing mod-
els from previous timesteps s, when the state of the
world for the current timestep t and timestep s is the
same (as represented by our similarity function) with
respect to stock o (all other things being equal), and
a decrease in probability for a lower weight.
Selected models. Besides feature coefficients, our
model captures temporal shift by modeling similar-
ity across the most recent c days. During inference,
our model weights different word distributions from
the past. The similarity is encoded in the pairwise
features f(xt,xk) and the parameters ?. Figure 5
shows the distributions of the strongest-posterior
models from previous timesteps, based on how far
189
EN:NA
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
EN:JP
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
FR
timestep
perpl
exity
0 50 100 150 200 250 300 350
200
400
600
base all
complete
int. one all
DE
timestep
perpl
exity
0 50 100 150 200 250 300 350
300
500
700
base all
complete
int. one all
Figure 4: Perplexity over time for four Reuters news streams (c = 7) with bigram base models.
190
in the past they are at the time of use, aggregated
across rounds on the EN:NA dataset, for window size
c = 14. It shows that the model tends to favor models
from days closer to the current date, with the t ? 1
models selected the most, perhaps because the state
of the world today is more similar to dates closer to
today compare to more distant dates. The plot also
explains why increasing c from 7 to 14 did not im-
prove performance of the model, since most of the
variation in our datasets can be captured with models
from the past week.
Topics. Latent topic variables have often figured
heavily in approaches to dynamic language model-
ing. In preliminary experiments incorporating single-
membership topic variables (i.e., each document be-
longs to a single topic, as in a mixture of unigrams),
we saw no benefit to perplexity. Incorporating top-
ics also increases computational cost, since we must
maintain and estimate one language model per topic,
per timestep. It is straightforward to design mod-
els that incorporate topics with single- or mixed-
membership as in LDA (Blei et al., 2003), an in-
teresting future direction.
Potential applications. Dynamic language models
like ours can be potentially useful in many applica-
tions, either as a standalone language model, e.g.,
predictive text input, whose performance may de-
pend on the temporal dimension; or as a component
in applications like machine translation or speech
recognition. Additionally, the model can be seen as
a step towards enhancing text understanding with
numerical, contextual data.
7 Conclusion
We presented a dynamic language model for stream-
ing datasets that allows conditioning on observable
real-world context variables, exemplified in our ex-
periments by stock market data. We showed how to
perform learning and inference in an online fashion
for this model. Our experiments showed the predic-
tive benefit of such conditioning and online learning
by comparing to similar models that ignore temporal
dimensions and observable variables that influence
the text.
Acknowledgements
The authors thank several anonymous reviewers for help-
ful feedback on earlier drafts of this paper and Brendan
O?Connor for help with collecting Twitter data. This re-
search was supported in part by Google, by computing
resources at the Pittsburgh Supercomputing Center, by
National Science Foundation grant IIS-1111142, AFOSR
grant FA95501010247, ONR grant N000140910758, and
by the Intelligence Advanced Research Projects Activ-
ity via Department of Interior National Business Center
contract number D12PC00347. The U.S. Government is
authorized to reproduce and distribute reprints for Govern-
mental purposes notwithstanding any copyright annotation
thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as nec-
essarily representing the official policies or endorsements,
either expressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable training
of l1-regularized log-linear models. In Proc. of ICML.
David M. Blei and John D. Lafferty. 2006. Dynamic topic
models. In Proc. of ICML.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Se?bastien Bubeck. 2011. Introduction to online opti-
mization. Technical report, Department of Operations
Research and Financial Engineering, Princeton Univer-
sity.
Nicolo` Cesa-Bianchi and Ga?bor Lugosi. 2006. Prediction,
Learning, and Games. Cambridge University Press.
William S. Cleveland. 1979. Robust locally weighted
regression and smoothing scatterplots. Journal of the
American Statistical Association, 74(368):829?836.
Thomas M. Cover and Joy A. Thomas. 1991. Elements of
Information Theory. John Wiley & Sons.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10(7):2899?
2934.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Proc. of
ICML.
Amit Goyal, Hal Daume III, and Suresh Venkatasubrama-
nian. 2009. Streaming for large scale NLP: Language
modeling. In Proc. of HLT-NAACL.
191
Matt Hoffman, David M. Blei, Chong Wang, and John
Paisley. 2013. Stochastic variational inference. Jour-
nal of Machine Learning Research, 14:1303?1347.
Antti Honkela and Harri Valpola. 2003. On-line varia-
tional Bayesian learning. In Proc. of ICA.
Tomoharu Iwata, Takeshi Yamada, Yasushi Sakurai, and
Naonori Ueda. 2010. Online multiscale dynamic topic
models. In Proc. of KDD.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
Jyrki Kivinen and Manfred K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132:1?63.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000. Mining
of concurrent text and time series. In Proc. of KDD
Workshop on Text Mining.
Abby Levenberg and Miles Osborne. 2009. Stream-based
randomised language models for SMT. In Proc. of
EMNLP.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for sta-
tistical machine translation. In Proc. of HLT-NAACL.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503?528.
David Mimno and Andrew McCallum. 2008. Topic mod-
els conditioned on arbitrary features with Dirichlet-
multinomial regression. In Proc. of UAI.
Alexander Rakhlin. 2009. Lecture notes on online learn-
ing. Technical report, Department of Statistics, The
Wharton School, University of Pennsylvania.
Masaaki Sato. 2001. Online model selection based on the
variational bayes. Neural Computation, 13(7):1649?
1681.
Shai Shalev-Shwartz. 2012. Online learning and online
convex optimization. Foundations and Trends in Ma-
chine Learning, 4(2):107?194.
Martin J. Wainwright and Michael I. Jordan. 2008. Graph-
ical models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1(1?2):1?305.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time dynamic topic models. In Proc.
of UAI.
Chong Wang, John Paisley, and David M. Blei. 2011. On-
line variational inference for the hierarchical Dirichlet
process. In Proc. of AISTATS.
Martin Zinkevich. 2003. Online convex programming
and generalized infinitesimal gradient ascent. In Proc.
of ICML.
192
