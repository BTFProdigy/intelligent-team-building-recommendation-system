A word-grammar based morl)hoh)gieal nalyzer 
for agglutinative languages 
Aduriz 1.+, Agirre E., Aldezabal I., Alegria I., Arregi X., Arriohl J. M., Artola X., Gojenola K., 
Marilxalar A., Sarasola K., Urkia M.+ 
l)ept, of Colllptiier 1Aulgtlages and Systems, University of lhe Basqtlo Cotlnlry, 64.9 P. K., 
E-20080 1)onostia, Basque Counh'y 
tUZEI, Aldapeta 20, E-20009 1)onostia, Basque Country 
+Universidad de Barcelona, Grin Vfii de Isis Cortes CalaiallaS, 585, E-08007 Flarcelona 
j ipgogak @ si.elm, es. 
Abst rac l  
Agglutinative languages presenl rich 
morphology and for sonic applications 
they lleed deep analysis at word level. 
Tile work here presenled proposes a 
model for designing a full nlorpho- 
logical analyzer. 
The model integrates lhe two-level 
fornlalisnl alld a ullificalion-I)asod 
fornialisni. In contrast to other works, 
we propose to separate the treatment of 
sequential and non-sequetTtial mou)ho- 
lactic constraints. Sequential constraints 
are applied in lhe seglllenlalion phase, 
and non-seqtlontial OlleS ill the filial 
feature-combination phase. Early appli- 
cation of sequential nlorpholactic 
coilsli'aiills during tile segnloillaiioi/ 
process nlakes feasible :,ill officienl 
iinplenleilialion of tile full morpho- 
logical analyzer. 
The result of lhis research has been tile 
design and imi)len~entation of a full 
nlorphosynlactic analysis procedure for 
each word in unrestricted Basque texts. 
I n t roduct ion  
Morphological analysis of woMs is a basic 
tool for automatic language processing, and 
indispensable when dealing willl highly 
agglutinative languages like Basque (Aduriz el 
al., 98b). In lhis conlext, some applications, 
like spelling corfeclion, do ilOI need illOl'e lhan 
the seglllOlltation of each word inlo its 
different COlllponenl nlorphellles alollg with 
their morphological information, ltowever, 
there are oiher applications such as lemnializa- 
tion, lagging, phrase recognition, and 
delernlinaiion of clause boundaries (Aduriz el 
al., 95), which need an additional global 
morphological i)arsing j of the whole word. 
Such a complete nlorphological analyzer has 
lo consider three main aspects (l~,ilchie et al, 
92; Sproal, 92): 
1 Morl)hographenfics (also called morpho- 
phonology). This ternl covers orthographic 
variations that occur when linking 
I l lOfphellleS. 
2) morpholactics. Specil'ication of which 
nlorphenles can or cannot combine with 
each other lo form wflid words. 
3) Feature-combination. Specification of how 
these lnorphemes can be grouped and how 
their nlorphosyntactic features can be 
comlfined. 
The system here presented adopts, oil the one 
hand, tile lwo-level fornlalisnl to deal with 
morphogralfilemics and sequential morl)ho- 
lactics (Alegria el al., 96) and, on the other 
hand, a unification-based woM-grammar 2 to 
combine the grammatical information defined 
in nlorphemes and to  tackle complex 
nlorphotactics. This design allowed us to 
develop a full coverage analyzer that processes 
efl'iciently unrestricted texts in Basque. 
The remainder of tills paper is organized sis 
follows. After a brief' description of Basque 
nlorphology, section 2 describes tile 
architecture for morphological processing, 
where the morphosynlactic omponent is 
included. Section 3 specifies tile plaenomena 
covered by the analyzer, explains its desigi~ 
criteria, alld presents implementation and 
ewthialion details. Section d compares file 
I This has also been called mo*7)hOSh,ntactic 
parsitlg. When we use lhc \[(fill #11017~\]lOSyltl~/X WC 
will always refer to il~c lficrarchical structure at 
woM level, conlbining morphology and synlax. 
2 '\]'\]lt3 \[IDl'll\] WOl'd-gF(lllllllUl" should not be confused 
with the synlaclic lilcory presented in (Hudson, 84). 
system with previous works. Finally, the paper 
ends with some concluding renmrks. 
1 Brief description of Basque 
morphology 
These are the most important features of 
Basque morphology (Alegria et al, 96): 
? As prepositional functions are realized by 
case suffixes inside word-fornls, Basque 
presents a relatively high power to generate 
inflected word-forms. For instance, froth a 
single noun a minimum of 135 inflected 
forms can be generated. Therefore, the 
number of simple word-forms covered by 
the current 70,000 dictionary entries woukl 
not be less than 10 million. 
? 77 of the inflected forms are simple 
combinations of number, determination, 
and case marks, not capable of further 
inflection, but the other 58 word-forms 
ending in one of the two possible genitives 
(possessive and locative) can be further 
inflected with the 135 morphemes. This 
kind of recursive construction reveals a 
noun ellipsis inside a noun phrase and 
could be theoretically exteuded ad 
infinitum; however, in practice it is not 
usual to fiud more than two levels of this 
kind of recursion in a word-form. Taking 
into account a single level of noun ellipsis, 
the number of word-forum coukl be 
estimated over half a billion. 
? Verbs offer a lot of grammatical 
information. A verb tbrln conveys informa- 
tion about the subject, the two objects, as 
well as the tense and aspect. For example: 
diotsut (Eng.: 1 am telling you something). 
o Word-formation is very productive in 
Basque. It is very usual to create new 
compounds as well as derivatives. 
As a result of this wealth of infornmtion 
contained within word-forms, complex struc- 
tures have to be built to represent complete 
morphological information at word level. 
2 An architecture for the full 
morphological ana lyzer  
The framework we propose for the 
morphological treatment is shown in Figure 1. 
The morphological nalyzer is the fiont-end to 
all present applications for the processing of 
Basque texts. It is composed of two modules: 
the segmentation module and the 
morphosyntactic analyzer. 
conformant .................. ~ U~atabas N TEZ-conf~ 
\[Segmentation module 
____~| HorphograDhemics 
Morphotactics I 
TEI-FS .............. ~ ~ ~ ~  ~ - p ~  
conformant Cegmented TexN 
Morphosyntactic 
analyzer 
Feature- combination 
Morphotactics II 
TEI-FS \] .............. ~ actically 
Lermnatization, linguistic Analysis tagging tools 
Figure 1. Architecture 1"o1" morphological processing. 
The segmentation ,nodule was previously 
implemented in (Alegria et al, 96). This 
system applies two-level morphology 
(Koskenniemi, 83) for the morphological 
description and obtains, for each word, its 
possible segmentations (one or many) into 
component morphemes. The two-level system 
has the following components: 
? A set of 24 morphograf~hemic rules, 
compiled into transducers (Karttunen, 94). 
? A lexicon made up of around 70,000 items, 
grouped into 120 sublexicons and stored in 
a general lexical database (Aduriz et al, 
98a). 
This module has full coverage of free-running 
texts in Basque, giving an average number of 
2.63 different analyses per word. The result is 
the set of possible morphological segmenta- 
tions of a word, where each morpheme is 
associated with its corresponding features in 
the lexicon: part of speech (POS), 
subcategory, declension case, number, 
definiteness, as well as syntactic function and 
some semantic features. Therefore, the output 
of the segmeutation phase is very rich, as 
shown in Figure 2 with the word amarengan 
(Eng.: on the mother). 
grammar 
mother) 
POS noun) 
subc~t common 
:count: +) 
(an imate  +) 
(nleasurable "-) 
aren 
(of life) 
(POS decl-suffix) 
(definite +) 
(number sing) 
(case genitive) 
(synt-f @nouncomp) 
J gan \] 
(o.1 / 
(POS decl-suf fix) I 
(case inossivo) \] 
(synt-f @adverbial)I 
=> 
amarengan 
(o. the mother) 
POS noun) 
subcat common) 
number sing) 
definite +) 
case inessive) 
count +) 
animate +) 
measurable -) 
synt-f @adverbial) 
iq:e, ure 2. Morphosynlactic analysis eof (unureugun (l{ng.: (m 
The architecture is a modular envhoument that 
allows different ypes of output depending on 
the desired level of analysis. The foundation of 
the architecture lies in the fact lhat TEI- 
confommnt SGML has been adopted for the 
comnmnication allloIlg modules (Ide and 
VCFOIIiS, 95). l~'eature shucluleS coded 
accoMing TIU are used to represent linguistic 
information, illcluding tile input mM outl)ut of 
the morplaological analyzer. This reprcscnta- 
tion rambles the use of SGML-aware parsers 
and tools, and Call he easily filtered into 
different formats (Artola et ill., 00). 
3 Word level morl)hosyntactic analysis 
This section Hrst presents the l~henomena lhat 
must be covered by the morphosyntactic 
analyzer, then explains ils design criteria, and 
finally shows implementation and ewfluation 
details. 
3.1 Phenomena covered by the analyzer 
There are several features that emphasized the 
need of morphosyntactic almlysis in order to 
build up word level information: 
I) Multiplicity of values for the same feature 
in successive morphemes. In the analysis 
of Figure 2 there are two different values 
for the POS (noun and declension suffix), 
two for the case (genitive and inessive), 
and two for the syntactic function 
(@nouncomp and @adverbial). Multiple 
values at moq~hemc-level will have to be 
merged to obtain the word level infer 
mation. 
2) Words with phrase structure. Although the 
segmentation is done for isolated words, 
independently of context, in several cases 
3 l?calurc wtlues starling with the "@" character 
correspond to syntactic functions, like @noullcomp 
(norm complement) or @adverbial. 
the mother) 
tile resulting structure is oquiwflent o the 
aualysis of a phrase, as can be seen i, 
Figure 2. 111 this case, although there are 
two different cases (genitive and inessive), 
lhe case of the full word-form is simply 
inessive. 
3) Noun ellipsis inside word-lbrms. A noun 
ellipsis can occur withi, the word 
(oceasi(mally more than once). This 
information must be made explicit in the 
resulting analysis. For example, Figure 3 
shows the analysis of a single word-forln 
like diotsudumtrel&z (Eng.: with what I am 
lelling you). The first line shows its 
segmentation into four morphemes 
(die tsut+en+ 0 +arekin). The feature 
compl ill tile final analysis conveys the 
information for the verb (l um lelliHg you), 
that carries information about pc'rson, 
number and case o1' subject, object and 
indirect object. The feature comp2 
represents an elided noun and its 
declension stfffix (with). 
4) l)erivation and composition are productive 
in Basque. There arc more than 80 deri- 
w/tion morphemes (especially suffixes) 
intensively used in word-fornlatioll. 
3.2 Design of the word-grammar 
The need to impose hierarchical structure upon 
sequences of morphemes and to build complex 
constructions from them forced us to choose a 
unil'ication mechanism. This task is currently 
unsolwlble using finite-state techniques, clue to 
the growth in size of the resulting network 
(Beesley, 98). We have developed a unifica- 
tion based word-grammar, where each rule 
combines information flom different 
mot+lJlemes giving as a result a feature 
structure for each interpretation of a word- 
fol'nl, treating the previously mentioned cases. 
3 
diotsut 
I am tellh,g you) 
POS verb) 
(tense present) 
(pers-ergative is)\[ 
(pets-dative 2s) 
(pers-absol 3s) 
en 
(what) 
(POS relation) 
(subcat subord) 
(relator relative 
(synt-f @rel-clause 
0 
() 
(POS ellipsis) 
arekin 
(wire) 
(POS declension-suffix)) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
=> diotsudanarekin (wi~ what lamtel l ingyou) 
(POS verb-noun_ellipsis) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
(compl (POS verb) 
(subcat subord) 
(relator relative) 
(synt-f @tel-clause) 
(tense present) 
(pers-ergative is) 
(pets-dative 2s) 
(pers-absol 3s)) 
(comp2 (POS noun) 
(subcat common) 
(number sing) 
(definite+) 
(synt-f @adverbial)) 
Figure 3. Morphosyntactic analysis of diotxudanarekin (Eng.: with what I am tellittg you) 
As a consequence of the rich naorphology of 
Basque we decided to control morphotactic 
phenomena, as much as possible, in the 
morphological segmentation phase. Alterna- 
tively, a model with minimal morphotactic 
treatment (Ritchie et al, 92) would produce 
too many possible analyses after segmentation, 
which should be reiected in a second phase. 
Therefore, we propose to separate sequential 
morphotactics (i.e., which sequences of 
morphemes can or cannot combine with each 
other to form valid words), which will be 
recognized by the two-level system by means 
of continuation classes, and non-sequential 
morphotactics like long-distance dependencies 
that will be controlled by the word-gmnunar. 
The general linguistic principles used to define 
unification equations in the word-grannnar 
rules are the following: 
1) Information risen from the lemma. The 
POS and semantic features are risen flom 
the lemnm. This principle is applied to 
common nouns, adjectives and adverbs. 
The lemma also gives the mnnber in 
proper nouns, pronouns and determiners 
(see Figure 2). 
2) lnfornmtion risen from case suffixes. 
Simple case suffixes provide information 
on declension case, number and syntactic 
function. For example, tile singular 
genitive case is given by the suffix -tell in 
ama+ren (Eng.: of the mother). For 
compound case suffixes the number and 
determination are taken from the first 
suffix and the case from the second one. 
First, both suffixes are joined and after 
that they are attached to the lemma. 
3) Noun ellipsis. When an ellipsis occurs, the 
POS of the whole word-form is expressed 
by a compound, which indicates both the 
presence of the ellipsis (always a noun) 
and the main POS of the word. 
For instance, the resulting POS is 
verb-noun_e l l ips is  when a noun- 
ellipsis occurs after a verb. All the 
information corresponding to both units, 
the explicit lemma and the elided one, is 
stored (see Figure 3). 
4) Subordination morl~hemes. When a 
subordination morpheme is attached to a 
verb, the verb POS and its featm'es are 
risen as well as the subordhmte relation 
and the syntactic fnnction conveyed by the 
naorpheme. 
5) Degree morphemes attached to adjectives, 
past participles and adverbs. The POS and 
diotsudan 
(diotsut + en) 
(POS verb) 
(tense present) 
(relator relative) 
/ \ / 
diotsut 
(POS verb) 
(tense present 
diotsudanarekin 
(diotsut + en -I 0 + arekin) 
(POS verb-noun_ell ipsis) 
(case sociative) 
arekin 
(0 + arekin) 
(POS noun ellipsis) 
(case sociative) 
en 
(pos 
? . . 
o 
(POS e l l ips i s  re la t ion)  
arekin 
(case sociative) 
Figure 4. Parse tree for diotmuhmarekitl (Eng.: with what I am lellittg yott) 
main features arc taken from the lemma 
and the features corresponding to the 
degrees of comparison (comparative, 
supcrhttive) aft taken from the degree 
morphemes. 
6) l)efiwttion. 1)miwttion suffixes select tile 
POS of the base-form to create the deriw> 
tive anti in most cases to change its POS. 
For instance, the suffix -garri (Eng.: -able) 
is applied to verbs and the derived word is 
an adjective. When the derived form is 
obtained by means o1' a prefix, it does not 
change the POS of the base-form. In both 
cases the morphosyntactic rules add a new 
feature representing the structure of tile 
word as a derivative (root and affixes). 
7) Composition. At the moment, we only 
treat the most freqttent kind of 
composition (noun-noun). Since Basque is 
syntactically characterized as a right-head 
hmguage, the main information of the 
compound is taken from the second 
element. 
8) Order of application of the mofphosyn- 
tactic phenomena. When several morpho- 
syntactic phenomena are applied to the 
same leml l la ,  so as to eliminate 
nonsensical readings, the natural order to 
consider them in Basque is the following: 
lemmas, derbation prefixes, deriwltion 
suffixes, composition and inflection (see 
Figure 4). 
9) Morl)hotactic constraints. Elimination of 
illegal sequences of morphemes, such as 
those due to long-distance dependencies, 
which are difficult to restrict by means of 
conti.uation classes. 
The first and second principles are defined lo 
combine information of previously recognized 
mOrl~hemcs, but all the other principles arc 
related to both feature-combination a d non- 
sequential moq~hotactics. 
3.3 Implementation 
We have chosen the PATR formalism 
(Shiebcr, 86) for the definition of the moqflm- 
syntactic rules. There were two main reasons 
for this choice: 
? The formalism is based o.  unification. 
Unification is adequate for the treatment of 
complex phenomena (e.g., agreement of 
conslituents in case, tmmber and definite- 
hess) and complex linguistic structures. 
? Simplicity. The grammar is not linked to a 
linguistic theory, e.g. GPSG in (Ritchie et 
al., 92)? The fact that PATR is simpler than 
more sophisticated formalisms will allow 
that in @e future the grammar could be 
adapted to any of them. 
25 rules have been defined, distributed in the 
following way: 
? 11 rules for the merging of declension 
morphemes and their combination with the 
main categories, 
? 9 rules for the description of verbal 
subordination morphenles, 
? 2 general fulcs for derivation, 
? 1 rule for each of the following 
phenomeml: ellipsis, degree of COlnpavison 
of adjectives (comparative and SUl)erlative) 
and noun composition. 
3.4 Evaluat ion 
As a cousequence of the size of the lexical 
database and tile extensive treatment of 
nlorphosyntax, the resulting analyzer offers 
full coverage when applied to real texts, 
capable of treating unknown words and non- 
standard forms (dialectal wtriants and typical 
errors). 
We performed four experilnents to ewtluate 
tile efficiency of the implemented analyzer 
(see Table 1). A 10,832-word text was 
randomly selected from newspapers. We 
measured tile number of words per second 
analyzed by the morphosyntactic analyzer and 
also by the whole morphological analyzer 
(results taken on a Sun Ultra 10). Ill the first 
experiment all tile word-t'ornls were analyzed 
one-by-one; while ill tile other three experi- 
ments words with more than one occurrence 
were analyzed only once. Ill the last two 
experimeuts a memory with the analysis of tile 
most frequent word-forms (MFW) in Basque 
was used, so that only word-forms not found 
in the MFW were analyzed. 
Test 
description 
All 
word forms 
Diffcrent 
word forms 
MFW 
10,000 words 
(I 5 Mb) 
MFW 
50,000 words 
(75 mb) 
# words/scc 
analyzed Morphosynt. 
words analyzer 
10,832 
3,692 
1,483 
533 
15,13 
44 40 
111 95 
308 270 
words/see 
Full 
morphological 
analyzer 
13,5 
Table 1. Evaluation results. 
Even when our language is agglutinative, and 
its morphological phenomena need more 
computational resources to build complex and 
deep structures, the results prove tile feasibility 
of implementiug efficiently a fifll 
morphological analyzer, although efficiency 
was not the main concern of our 
implementation. The system is currently being 
applied to unrestricted texts in real-time 
applications. 
4 Related work 
(Koskeniemmi, 83) defined the formalism 
named two-level morphology. Its main 
contributiou was the treatment of 
morl)hographemics and morphotactics. The 
formalisnl has been stmcessfully applied to a 
wide wlriety ot' languages. 
(Karttunen, 94) speeds the two-level model 
compiling two-level rules into lexical 
transducers, also increasing the expressiveness 
of the model 
The morphological analyzer created by 
(Ritchie et al, 92) does not adopt finite state 
mechanisms to control morphotactic 
phenomena. Their two-level implementation 
incorporates a straightforward morphotactics, 
reducing tile number of sublexicons to the 
indispensable (prefixes, lemmas and suffixes). 
This approximation would be highly 
inefficient for agglutinative languages, as it 
would create lnany nonsensical interpretatiolas 
that should be rejected by tile unification 
phase. They use the word-grammar for both 
morphotactics and feature-conlbination. 
ill a similar way, (Trost, 90) make a proposal 
to combine two-level morphology and non- 
sequential morphotactics. 
The PC-Kimmo-V2 system (Antworth, 94) 
presents an architecture similar to ours applied 
to English, using a finite-state segmentation 
phase before applying a unification-based 
grammar. 
(Pr6szdky and Kis, 99) describe a morpho- 
syntactic analyzer for Hungarian, an agglu- 
tinative language. The system clots not use the 
two-level model for segmentation, precom- 
piling suffix-sequences to improve efficiency. 
They claim the need of a word-grammar, 
giving a first outline of its design, although 
they do not describe it in detail. 
(Oflazer, 99) presents a different approach for 
the treatment of Turkish, an agglutinative 
language, applying directly a dependency 
parsing scheme to morpheme groups, that is, 
merging morphosyntax and syntax. Although 
we are currently using a similar model to 
Basque, there are several applications that are 
word-based and need full morphological 
parsing of each word-t'orm, like the word- 
oriented Constraint Graminar formalism for 
disambiguation (Karlsson et aI., 95). 
Conc lus ion  
We propose a model for fllll morphological 
analysis iutegrating two different components. 
On tile one hand, the two-level formalism 
deals with morphographenfics and sequential 
morphotactics and, on the other hand, a 
unil\]cation-based word-grammar combines lhe 
granlll-iatical in\['ornlatioli defined in illoi'- 
phelllOS alld also handles COlllplcx illori)ho- 
tactics. 
Early application of sCqtloniial I/lOrl)hotactic 
conslraints dtu-ing the segmentation process 
avoids all excessive laUlllber of nleaningless 
segmentation possibilities before the 
coulputationally lllOlO expensive unification 
process. Unification permits lhe resohition of a 
wide variety of morl)hological phenonlena, 
like ellipsis, thal force the definition of: 
complex and deep structures Io roprosenl the 
output of the analyzer. 
This design allowed us io develop a full 
coverage allalyzor that processes efficiently 
unrestricted loxis in Basque, a strongly 
agglulinafive langttage. 
The anaiyzcl" has bccll integrated ill a gCllOl'al 
franlework for the l)lOCessing of l~asquc, with 
all the linguistic inodulos communicating by 
l l leallS O\[: foattll'C stltlClll l 'eS ill accord  {o the 
principles of ihe Text Encoding Initiative. 
Acknowledgements  
This research was partially supported by the 
Basque Government, the University of the 
\]71aS(lUe Cotlntry {/lid the CICYq' (Cotllisidn 
lntcrministorial de Ciencia y Tecnologfil). 
References 
Aduriz 1., Aldczabal I., Ansa ()., Arlola X., I)faz de 
Ilarraza A., Insau.~li .I.M. (1998a) EI)BL: a 
Mttlli-l~ttrposed Lexica/ Sttl)l)c;rl .lot the 
Treatment of Ba,s'que. Proceedings of the l;irst 
Inlernational Confcncncc on l Auiguagc Resources 
and Ewduation, Granada. 
Aduriz I., Agirre E., Aldczabal 1., Alegria 1., Ansa 
O., Arrcgi X., Arriola J.M., ArtolaX., I)faz de 
lhu'raza A., Ezciza N., Gqicnola K., Maritxahu" 
A., Maritxalar M., Oronoz M., Sarasola K., 
Soroa A., Urizar R., Urkia M. (1998b) A 
Framework .for the Automatic Pmce.vsi#~g (if" 
Basqtte. Proceedings o1 the First Ii~ternational 
Con \[elel i te on Lall.gtlagc Resources turf 
Evaluation, Granada. 
Aduriz I., Alcgria I., Arriohl J.M., Artola X., l)faz 
do Ilarraza A., Ecciza N., Gojcnola K., 
Maritxalar M. (1995) Di\[.ferelt! Issues in the 
Design qf a lemmatizer/Tagger fo Ba,s'qtte. From 
Tcxls to Tags: Issues in Mullilingual Language 
Analysis. ACL SIGI)AT Workshop, l)ublin. 
Alcgria 1., Art(Ha X., Sarasoht K., Urkia M. (1996) 
Automatic moqdzological analysis of Basque. 
IAtcrary and IAnguistic Computing, 11 (4): 193- 
203. Oxford University. 
Aniworlh E. I.. (1994) Morphological Par, ffng with 
a lhl(fication-ba,s'ed Word Grcmmutr. Norlh 
Te, xas Natural l~anguage Processing Workshop, 
Texas. 
Arlola X., Dfaz de \]larraza A., Ezciza N., Oo.icnohi 
K., Marilxahu' A., Soma A. (2000) A proposal 
for the integration of NLP tools using SGML- 
lagged documeHls. Proceedings of ll~e Second 
Cotfforence or1 Language Resources and 
Evaltmfion (IA~,EC 2000). Athens, Greece 2000. 
Bcesl%, K. (1998)AraDic Morphological Analysis 
(m the lnlernet, l'rocccdings of the International 
Conference on Mulii-IAngual Computing (Arabic 
& lhlglish), Cambridge. 
Hudson R. (1990) English Word Grammmar. 
Oxford: Basil Blackwcll. 
ldc N., Vcronis J. K. (1995) Text-Ettcoding hHtia- 
tire, Bac:kgmtmd and Context. Kluwcr Academic 
Publishers. 
Karlsson F., Voulilaincn A., Heikkiht J., Anltila A. 
(1995) Constrai, t Gnmmmr: A lxm,?tmge- 
i#ldcpcndent System Jor Pm:ffng Um'estricled 
Text, Mouton do Gruyicr ed.. 
Kartmnen 1,. (1994) Con,s'tructin~ l,e.vical 
7)'ansdttcers. Proc. of CO13NG'94, 406-411. 
Koskcnniemi, K, (1983) Two-level Mc;qdlo\[ogy: A 
ge,eral Comptttational Model ./br Word-Form 
Recognition and Pmduclioth University of 
Ilclsinki, l)clmrtmcnt of General IAnguisiics. 
l~ublications " 11. 
()flazcr K (1999) l)epetMe/t O' Parsing, with a, 
E.rtended I:inite State Approac\]t. ACL'99, 
Maryland. 
Pr6sz6ky G., Kis B (1999)A Unificati(m-hascd 
Apl~roach to Moqdto-syntactic I'arsitl<~ of 
Agghttinative and Other (Highly) lnjlectional 
Languages. ACtd99, Ma,yhmd. 
Ritchie G., Pulhnan S. G., FJlack A. W., Russcl G. 
J. (1992) Comlmtational Moudu)logy: Practical 
Mechanism,s'.fi)r the l#lglish l,exico,. ACL-MIT 
Series on Natural Language Processing, MIT 
Press. 
Shicbcr S. M. (1986) At/ lntroductiotz to 
Unification-Based Approaches to Grammar. 
CSLI, Slanford. 
Sproat R. (1992) Morphology anU Computcaion. 
ACL-MIT Press series in Natural Language 
Processing. 
Trost It. (1990) The application of two-level 
morldzo/ogy to rzon-concatenative German 
moqgtology. COIANG'90, Hclsinki. 
7 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Machine Learning Techniques to Build a Comma Checker for 
Basque
I?aki Alegria Bertol Arrieta Arantza Diaz de Ilarraza Eli Izagirre Montse Maritxalar
Computer Engineering Faculty. University of the Basque Country.
Manuel de Lardizabal Pasealekua, 1
20018 Donostia, Basque Country, Spain.
{acpalloi,bertol,jipdisaa,jibizole,jipmaanm}@ehu.es
Abstract
In this paper, we describe the research 
using  machine  learning  techniques  to 
build a comma checker to be integrated 
in a grammar checker for Basque. After 
several experiments, and trained with a 
little corpus of 100,000 words, the sys?
tem guesses correctly not placing com?
mas with a precision of 96% and a re?
call of 98%. It also gets a precision of 
70% and a recall of 49% in the task of 
placing  commas.  Finally,  we  have 
shown  that  these  results  can  be  im?
proved using a bigger and a more ho?
mogeneous  corpus  to  train,  that  is,  a 
bigger corpus written by one unique au?
thor. 
1 Introduction
In the last years, there have been many studies 
aimed  at  building  a  grammar  checker  for  the 
Basque language (Ansa et al, 2004; Diaz De Il?
arraza et al, 2005). These works have been fo?
cused, mainly, on building rule sets ??taking into 
account syntactic information extracted from the 
corpus  automatically??  that  detect  some  erro?
neous grammar forms. The research here presen?
ted wants to complement the earlier work by fo?
cusing on  the  style  and the  punctuation of  the 
texts. To be precise, we have experimented using 
machine learning techniques for the special case 
of the comma, to evaluate their performance and 
to analyse the possibility of applying it in other 
tasks of the grammar checker.  
However,  developing  a  punctuation  checker 
encounters  one  problem  in  particular:  the  fact 
that the punctuation rules are not totally estab?
lished. In general, there is no problem when us?
ing the  full  stop,  the  question mark or  the ex?
clamation mark.  Santos (1998) highlights these 
marks are reliable punctuation marks, while all 
the rest are unreliable. Errors related to the reli?
able ones (putting or not the initial  question or 
exclamation mark depending on the language, for 
instance) are not so hard to treat. A rule set to 
correct some of these has already been defined 
for the Basque language (Ansa et al, 2004). In 
contrast, the comma is the most polyvalent and, 
thus, the least defined punctuation mark (Bayrak?
tar et al, 1998; Hill and Murray, 1998). The am?
biguity of the comma, in fact,  has been shown 
often (Bayraktar et  al.,  1998; Beeferman et al, 
1998;  Van  Delden  S.  and  Gomez  F.,  2002). 
These works have shown the lack of fixed rules 
about the comma. There are only some intuitive 
and  generally  accepted  rules,  but  they  are  not 
used in a standard way. In Basque, this problem 
gets even more evident, since the standardisation 
and  normalisation  of  the  language  began  only 
about twenty?five years ago and it  has not fin?
ished yet. Morphology is mostly defined, but, on 
the contrary, as far as syntax is concerned, there 
is  quite  work  to  do.  In  punctuation  and  style, 
some basic rules have been defined and accepted 
by the Basque Language Academy (Zubimendi, 
2004).  However,  there  are  not  final  decisions 
about the case of the comma. 
Nevertheless,  since  Nunberg?s  monograph 
(Nunberg, 1990), the importance of the comma 
has  been  undeniable,  mainly  in  these  two  as?
pects: i) as a due to the syntax of the sentence 
(Nunberg, 1990; Bayraktar et al, 1998; Garzia, 
1997), and ii) as a basis to improve some natural 
language  processing  tools  (syntactic  analysers, 
error  detection  tools?),  as  well  as  to  develop 
some  new  ones  (Briscoe  and  Carroll,  1995; 
Jones, 1996). The relevance of the comma for the 
syntax of the sentence may be easily proved with 
some clarifying examples where the sentence is 
understood in  one or  other  way,  depending on 
whether  a  comma  is  placed  or  not  (Nunberg, 
1990): 
a. Those students who can, contribute to the 
United Fund. 
b. Those students who can contribute to the 
United Fund. 
1
In the same sense,  it  is  obvious  that  a  well 
punctuated  text,  or  more  concretely,  a  correct 
placement of the commas, would help consider?
ably  in  the  automatic  syntactic  analysis  of  the 
sentence,  and, therefore,  in the development of 
more and better tools in the NLP field. Say and 
Akman (1997) summarise the research efforts in 
this direction.
As an important background for our work, we 
note  where  the  linguistic  information  on  the 
comma for the Basque language was formalised. 
This  information  was  extracted  after  analysing 
the  theories  of  some experts  in  Basque  syntax 
and punctuation (Aldezabal et al, 2003). In fact, 
although no final decisions have been taken by 
the Basque Language Academy yet,  the theory 
formalised in the above mentioned work has suc?
ceeded in unifying the main points of view about 
the  punctuation in  Basque.  Obviously,  this  has 
been the basis for our work. 
2 Learning commas
We have designed two different but combinable 
ways to get the comma checker:
? based on clause boundaries
? based directly on corpus
Bearing  in  mind  the formalised  theory  of 
Aldezabal et  al.  (2003)1,  we realised that if  we 
got to split the sentence into clauses, it would be 
quite easy to develop rules for detecting the exact 
places where commas would have to go. Thus, 
the best way to build a comma checker would be 
to get, first, a clause identification tool. 
Recent papers in this area report quite good 
results using machine learning techniques. Car?
reras and M?rquez (2003) get one of the best per?
formances in this  task (84.36% in test).  There?
fore, we decided to adopt this as a basis in order 
to  get  an  automatic  clause  splitting  tool  for 
Basque.  But  as  it  is  known,  machine  learning 
techniques cannot be applied if no training cor?
pus is available, and one year ago, when we star?
ted this  process,  Basque texts  with this  tagged 
clause splits were not available.
Therefore, we decided to use the second al?
ternative.  We  had  available  some  corpora  of 
Basque, and we decided to try learning commas 
from raw text, since a previous tagging was not 
needed. The problem with the raw text is that its 
commas are not the result of applying consistent 
rules.
1 From now on, we will speak about this as ?the accepted theory of Basque 
punctuation?. 
Related work
Machine learning techniques have been applied 
in many fields and for  many purposes,  but  we 
have found only one reference in the literature 
related to the use of machine learning techniques 
to assign commas automatically. 
Hardt (2001) describes research in using the 
Brill tagger (Brill 1994; Brill, 1995) to learn to 
identify incorrect commas in Danish. The system 
was developed by randomly inserting commas in 
a text, which were tagged as incorrect, while the 
original  commas  were  tagged  as  correct.  This 
system identifies incorrect commas with a preci?
sion  of  91%  and  a  recall  of  77%,  but  Hardt 
(2001) does not mention anything about identify?
ing correct commas. 
In  our  proposal,  we have tried  to  carry out 
both aspects, taking as a basis other works that 
also use machine learning techniques in similar 
problems  such  as  clause  splitting  (Tjong  Kim 
Sang E.F. and D?jean H., 2001) or detection of 
chunks (Tjong Kim Sang E.F. and Buchholz S., 
2000).
3 Experimental setup
Corpora
As we have mentioned before, some corpora 
in Basque are available. Therefore, our first task 
was to select the training corpora, taking into ac?
count that well punctuated corpora were needed 
to train the machine correctly. For that purpose, 
we looked for corpora that satisfied as much as 
possible our ?accepted theory of Basque punctu?
ation?.  The  corpora  of  the  unique  newspaper 
written in Basque, called  Egunkaria (nowadays 
Berria), were chosen, since they are supposed to 
use the ?accepted theory of Basque punctuation?. 
Nevertheless,  after  some brief  verifications, we 
realised that the texts of the corpora do not fully 
match with our theory. This can be understood 
considering that a lot of people work in a news?
paper. That is, every journalist can use his own 
interpretation of  the  ?accepted theory?,  even if 
all of them were instructed to use it in the same 
way. Therefore, doing this  research, we had in 
mind that the results we would get were not go?
ing to be perfect.
To counteract this problem, we also collected 
more  homogeneous  corpora  from  prestigious 
writers: a translation of a book of philosophy and 
a novel. Details about these corpora are shown in 
Table 1.
2
Size of the corpora
Corpora from the newspaper Egunkaria 420,000 words
Philosophy texts written by one unique author 25,000 words
Literature texts written by one unique author 25,000 words
Table 1. Dimensions of the used corpora
A short version of the first corpus was used in 
different experiments in order to tune the system 
(see section 4). The differences between the re?
sults  depending on the type of  the corpora are 
shown in section 5.
Evaluation
Results are shown using the standard measures in 
this area: precision, recall and f?measure2, which 
are calculated based on the test corpus. The res?
ults are shown in two colums ("0" and "1") that 
correspond to the result categories used. The res?
ults for the column ?0? are the ones for the in?
stances that are not followed by a comma. On the 
contrary, the results for the column ?1? are the 
results for the instances that should be followed 
by a comma. 
Since  our  final  goal  is  to  build  a  comma 
checker,  the precision in the column ?1? is the 
most  important  data  for  us,  although the recall 
for the same column is also relevant. In this kind 
of tools, the most important thing is to first ob?
tain all the comma proposals right (precision in 
columns ?1?), and then to obtain all the possible 
commas (recall in columns ?1?).
Baselines
In  the  beginning,  we  calculated  two  possible 
baselines based on a big part of the newspaper 
corpora in order to choose the best one. 
The  first  one  was  based  on  the  number  of 
commas  that  appeared  in  these  texts.  In  other 
words,  we  calculated  how  many  commas  ap?
peared in the corpora (8% out of all words), and 
then we put commas randomly in this proportion 
in the test corpus. The results obtained were not 
very good (see Table 2, baseline1), especially for 
the  instances  ?followed by  a  comma? (column 
?1?).
The second baseline was developed using the 
list  of  words appearing before a comma in the 
training corpora. In the test corpus, a word was 
tagged as ?followed by a comma? if it was one of 
the words of the mentioned list. The results (see 
baseline 2, in Table 2) were better, in this case, 
for the instances followed by a comma (column 
named  ?1?).  But,  on  the  contrary,  baseline  1 
provided us with better results for the instances 
not followed by a comma (column named ?0?). 
That is why we decided to take, as our baseline, 
2 f?measure = 2*precision*recall / (precision+recall)
the best data offered by each baseline (the ones 
in bold in table 2). 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
baseline 1 0.927 0.924 0.926 0.076 0.079 0.078
baseline 2 0.946 0.556 0.700 0,096 0.596 0.165
Table 2: The baselines
Methods and attributes
We  use  the  WEKA3 implementation  of  these 
classifiers: the Naive Bayes based classifier (Na?
iveBayes),  the  support  vector  machine  based 
classifier  (SMO)  and  the  decision?tree  (C4.5) 
based one (j48).
It  has  to  be  pointed  out  that  commas  were 
taken  away  from  the  original  corpora.  At  the 
same time, for each token, we stored whether it 
was followed by a  comma or not.  That  is,  for 
each  word  (token),  it  was  stored  whether  a 
comma was placed next to it or not. Therefore, 
each token in the corpus is equivalent to an ex?
ample (an instance). The attributes of each token 
are based on the token itself and some surround?
ing ones. The application window describes the 
number of tokens considered as information for 
each token.
Our initial application window was [?5, +5]; 
that means we took into account the previous and 
following 5 words (with their corresponding at?
tributes)  as  valid  information  for  each  word. 
However, we tuned the system with different ap?
plication windows (see section 4). 
Nevertheless, the attributes managed for each 
word can be as complex as we want. We could 
only use words, but we thought some morpho?
syntactic information would be beneficial for the 
machine to learn. Hence, we decided to include 
as much information as we could extract using 
the shallow syntactic parser of Basque (Aduriz et 
al.,  2004).  This  parser  uses  the  tokeniser,  the 
lemmatiser, the chunker and the morphosyntactic 
disambiguator  developed by  the  IXA4 research 
group. 
The attributes we chose to use for each token 
were the following:
? word?form
? lemma
? category 
? subcategory
? declension case
? subordinate?clause type
3 WEKA is a collection of machine learning algorithms for data mining tasks 
(http://www.cs.waikato.ac.nz/ml/weka/).
4 http://ixa.si.ehu.es
3
? beginning of chunk (verb, nominal, enti?
ty, postposition)
? end of chunk (verb, nominal, entity, post?
position)
? part of an apposition
? other  binary  features:  multiple  word  to?
ken,  full  stop,  suspension  points,  colon, 
semicolon,  exclamation  mark  and  ques?
tion mark 
We also included some additional  attributes 
which were automatically calculated: 
? number of verb chunks to the beginning 
and to the end of the sentence 
? number of nominal chunks to the begin?
ning and to the end of the sentence
? number  of  subordinate?clause  marks  to 
the beginning and to the end of the sen?
tence
? distance (in tokens) to the beginning and 
to the end of the sentence 
We also did other experiments using binary 
attributes that correspond to most used colloca?
tions (see section 4).
Besides, we used the result attribute ?comma? 
to store whether a comma was placed after each 
token. 
4 Experiments
Dimension of the corpus
In  this  test,  we  employed the  attributes  de?
scribed in section 3 and an initial window of [?5, 
+5], which means we took into account the pre?
vious 5 tokens and the following 5. We also used 
the C4.5 algorithm initially, since this algorithm 
gets very good results in other similar machine 
learning  tasks  related  to  the  surface  syntax 
(Alegria et al, 2004).
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
100,000 train / 30,000 test 0,955 0,981 0,968 0,635 0,417 0,503
160,000 train / 45,000 test 0,947 0,981 0,964 0,687 0,43 0,529
330,000 train / 90,000 test 0,96 0,982 0,971 0,701 0,504 0,587
Table 3. Results depending on the size of corpora 
(C4.5 algorithm; [?5,+5] window).
As it  can be seen in table 3, the bigger the 
corpus,  the  better  the results,  but  logically,  the 
time expended to obtain the results also increases 
considerably. That is why we chose the smallest 
corpus  for  doing  the  remaining  tests  (100,000 
words  to  train  and  30,000  words  to  test).  We 
thought that the size of this corpus was enough to 
get good comparative results. This test, anyway, 
suggested that the best  results  we could obtain 
would  be  always  improvable  using  more  and 
more corpora. 
Selecting the window
Using the corpus and the attributes described be?
fore, we did some tests to decide the best applic?
ation window. As we have already mentioned, in 
some problems of this type, the information of 
the  surrounding  words  may  contain  important 
data to decide the result of the current word. 
In this test, we wanted to decide the best ap?
plication window for our problem. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
-5+5 0,955 0,981 0,968 0,635 0,417 0,503
-2+5 0,956 0,982 0,969 0,648 0,431 0,518
-3+5 0,957 0,979 0,968 0,627 0,441 0,518
-4+5 0,957 0,98 0,968 0,634 0,446 0,52
-5+2 0,956 0,982 0,969 0,65 0,424 0,514
-5+3 0,956 0,981 0,969 0,643 0,432 0,517
-5+4 0,955 0,982 0,968 0,64 0,417 0,505
-6+2 0,956 0,982 0,969 0,645 0,421 0,509
-6+3 0,956 0,982 0,969 0,646 0,426 0,514
-8+2 0,956 0,982 0,969 0,645 0,425 0,513
-8+3 0,956 0,979 0,967 0,615 0,431 0,507
-8+8 0,956 0,978 0,967 0,604 0,422 0,497
Table  4.  Results  depending  on  the  application 
window (C4.5 algorithm; 100,000 train / 30,000 
test)
As it can be seen, the best f?measure for the 
instances followed by a comma was obtained us?
ing the application window [?4,+5]. However, as 
we have said before, we are more interested in 
the precision. Thus, the application window [?5
,+2] gets the best precision, and, besides, its f?
measure is almost the same as the best one. This 
is the reason why we decided to choose the [?5
,+2] application window. 
Selecting the classifier
With  the  selected  attributes,  the  corpus  of 
130,000 words and the application window of [?5
, +2], the next step was to select the best classifi?
er for our problem. We tried the WEKA imple?
mentation of these classifiers:  the Naive Bayes 
based classifier (NaiveBayes), the support vector 
machine based classifier (SMO) and the decision 
tree based one (j48).  Table 5 shows the results 
obtained:
4
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
NB 0,948 0,956 0,952 0,376 0,335 0,355
SMO 0,936 0,994 0,965 0,672 0,143 0,236
J48 0,956 0,982 0,969 0,652 0,424 0,514
Table 5. Results depending on the classifier 
(100,000 train / 30,000 test; [?5, +2] window).
As we can see, the f?measure for the instances 
not followed by a comma (column ?0?) is almost 
the same for the three classifiers, but, on the con?
trary, there is a considerable difference when we 
refer  to  the  instances  followed  by  a  comma 
(column ?1?). The best f?measure gives the C4.5 
based classifier (J48) due to the better recall, al?
though the best precision is for the support vector 
machine  based  classifier  (SMO).  Definitively, 
the Na?ve Bayes (NB) based classifier was dis?
carded, but we had to think about the final goal 
of our research to choose between the other two 
classifiers.  Since our  final  goal  was to  build  a 
comma checker, we would have to have chosen 
the classifier that gave us the best precision, that 
is, the support vector machine based one. But the 
recall of the support vector machine based classi?
fier was not as good as expected to be selected. 
Consequently,  we  decided  to  choose  the  C4.5 
based classifier. 
Selecting examples
At this  moment,  the results  we get  seem to be 
quite good for the instances not  followed by a 
comma, but  not  so good for  the  instances  that 
should follow a comma. This could be explained 
by the fact that we have no balanced training cor?
pus. In other words, in a normal text, there are a 
lot  of  instances not  followed by a  comma, but 
there are not so many followed by it. Thus, our 
training  corpus,  logically,  has  very  different 
amounts of instances followed by a comma and 
not followed by a comma. That is the reason why 
the system will learn more easily to avoid the un?
necessary  commas  than  placing  the  necessary 
ones. 
Therefore,  we  resolved  to  train  the  system 
with a corpus where the number of instances fol?
lowed by a comma and not followed by a comma 
was the same. For that purpose, we prepared a 
perl program that changed the initial corpus, and 
saved only x words for each word followed by a 
comma. 
In  table  6,  we can see  the  obtained results. 
One to one means that in that case, the training 
corpus  had  one  instance  not  followed  by  a 
comma, for each instance followed by a comma. 
On the  other  hand,  one to  two means that  the 
training corpus had two instances not  followed 
by  a  comma  for  each  word  followed  by  a 
comma, and so on. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
normal 0,955 0,981 0,968 0,635 0,417 0,503
one to one 0,989 0,633 0,772 0,164 0,912 0,277
one to two 0,977 0,902 0,938 0,367 0,725 0,487
one to three 0,969 0,934 0,951 0,427 0,621 0,506
one to four 0,966 0,952 0,959 0,484 0,575 0,526
one to five 0,966 0,961 0,963 0,534 0,568 0,55
one to six 0,963 0,966 0,964 0,55 0,524 0,537
Table  6.  Results  depending  on  the  number  of 
words  kept  for  each  comma  (C4.5  algorithm; 
100,000 train / 30,000 test; [?5, +2] window). 
As  observed  in  the  previous  table,  the  best 
precision in the case of the instances followed by 
a comma is the original one: the training corpus 
where  no  instances  were  removed.  Note  that 
these results are referred as normal in table 6.
The corpus where a unique instance not fol?
lowed by a comma is kept for each instance fol?
lowed by a comma gets the best  recall  results, 
but the precision decreases notably. 
The  best  f?measure  for  the  instances  that 
should be followed by a comma is obtained by 
the one to five scheme, but as mentioned before, 
a comma checker must take care of offering cor?
rect comma proposals. In other words, as the pre?
cision of the original corpus is quite better (ten 
points better), we decided to continue our work 
with  the  first  choice:  the  corpus  where  no  in?
stances were removed. 
Adding new attributes
Keeping the best results obtained in the tests de?
scribed above (C4.5 with the [?5,  +2] window, 
and not removing any ?not comma? instances), 
we thought that giving importance to the words 
that appear normally before the comma would in?
crease our results. Therefore, we did the follow?
ing tests: 
1) To search a big corpus in order to extract 
the most  frequent  one hundred words  that  pre?
cede a  comma,  the  most  frequent  one hundred 
pairs of words (bigrams) that precede a comma, 
and the most frequent one hundred sets of three 
words (trigrams) that precede a comma, and use 
them as attributes in the learning process. 
2) To use only three attributes instead of the 
mentioned three hundred to encode the informa?
tion  about  preceding  words.  The  first  attribute 
would indicate whether a word is or not one of 
5
the  most  frequent  one  hundred  words.  The 
second attribute would mean whether a word is 
or not the last part of one of the most frequent 
one hundred pairs of words. And the third attrib?
ute would mean whether a word is or not the last 
part of one of the most frequent one hundred sets 
of three words. 
3) The case (1), but with a little difference: 
removing the attributes ?word? and ?lemma? of 
each instance. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
(0): normal 0,956 0,982 0,969 0,652 0,424 0,514
(1): 300 attributes + 0,96 0,983 0,972 0,696 0,486 0,572
(2): 3 attributes + 0,96 0,981 0,97 0,665 0,481 0,558
(3): 300 attributes +,  
no lemma, no word 0,955 0,987 0,971 0,71 0,406 0,517
Table 7. Results depending on the new attributes 
used (C4.5 algorithm; 100,000 train / 30,000 test; 
[?5, +2] window; not removed instances).
Table 7 shows that case number 1 (putting the 
300 data as attributes) improves the precision of 
putting  commas  (column  ?1?)  in  more  than  4 
points. Besides, it also improves the recall, and, 
thus, we improve almost 6 points its f?measure. 
The third test gives the best precision, but the 
recall decreases considerably. Hence, we decided 
to choose the case number 1, in table 7.
5 Effect of the corpus type
As we have said before (see section 3), depend?
ing on the quality of the texts, the results could 
be different.
In table 8, we can see the results using the dif?
ferent types of corpus described in table 1. Obvi?
ously,  to  give  a  correct  comparison,  we  have 
used the same size for all the corpora (20,000 in?
stances to train and 5,000 instances to test, which 
is the maximum size we have been able to ac?
quire for the three mentioned corpora).
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
Newspaper 0.923 0.977 0.949 0.445 0.188 0.264
Philosophy 0.932 0.961 0.946 0.583 0.44 0.501
Literature 0.925 0.976 0.95 0.53 0.259 0.348
Table 8. Results depending on the type of corpo?
ra (20,000 train / 5,000 test).
The first line shows the results obtained using 
the short version of the newspaper. The second 
line  describes  the  results  obtained  using  the 
translation of a book of philosophy, written com?
pletely by one author. And the third one presents 
the  results  obtained  using  a  novel  written  in 
Basque. 
In any case, the results prove that our hypo?
thesis  was  correct.  Using  texts  written  by  a 
unique author improves the results. The book of 
philosophy has the best precision and the best re?
call.  It  could be  because it  has  very long sen?
tences  and  because  philosophical  texts  use  a 
stricter syntax comparing with the free style of a 
literature writer.  
As it was impossible for us to collect the ne?
cessary  amount  of  unique  author  corpora,  we 
could not go further in our tests.
6 Conclusions and future work
We have used machine learning techniques for 
the  task  of  placing  commas  automatically  in 
texts. As far as we know, it is quite a novel ap?
plication field. Hardt (2001) described a system 
which identified incorrect commas with a preci?
sion of 91% and a recall of 77% (using 600,000 
words  to  train).  These  results  are  comparable 
with the ones we obtain for the task of guessing 
correctly when not to place commas (see column 
?0? in the tables). Using 100,000 words to train, 
we obtain 96% of precision and 98.3% of recall. 
The main reason could be that we use more in?
formation to learn.
However, we have not obtained as good res?
ults as we hoped in the task of placing commas 
(we  get  a  precision  of  69.6%  and  a  recall  of 
48.6%). Nevertheless, in this particular task, we 
have  improved  considerably  with  the  designed 
tests, and more improvements could be obtained 
using more corpora and more specific corpora as 
texts written by a unique author or by using sci?
entific texts. 
Moreover,  we have detected some possible 
problems that could have brought these regular 
results in the mentioned task:
? No fixed rules for commas in the Basque 
language
? Negative influence when training using 
corpora from different writers
In this sense, we have carried out a little ex?
periment with some English corpora. Our hypo?
thesis was that a completely settled language like 
English,  where  comma  rules  are  more  or  less 
fixed, would obtain better results. Taking a com?
parative English corpus5 and similar learning at?
tributes6 to  Basque?s  one,  we  got,  for  the  in?
stances  followed  by  a  comma  (column  ?1?  in 
tables), a better precision (%83.3) than the best 
5 A newspaper corpus, from Reuters
6 Linguistic information obtained using Freeling (http://garraf.ep?
sevg.upc.es/freeling/)
6
one obtained for the Basque language. However, 
the recall was worse than ours: %38.7. We have 
to take into account that we used less learning at?
tributes with the English corpus and that we did 
not  change  the  application  window chosen  for 
the Basque experiment. Another application win?
dow would have been probably more suitable for 
English.  Therefore, we believe that with a few 
tests  we  easily  would  achieve  a  better  recall. 
These  results,  anyway,  confirm our  hypothesis 
and our diagnosis of the detected problems. 
Nevertheless,  we think the presented results 
for the Basque language could be improved. One 
way would  be  to  use  ?information  gain? tech?
niques in order to carry out the feature selection. 
On the other hand, we think that more syntactic 
information, concretely clause splits tags, would 
be especially beneficial to detect those commas 
named delimiters by Nunberg (1990).
In fact, our main future research will consist 
on clause identification. Based on the ?accepted 
theory of the comma?, we can assure that a good 
identification of clauses (together with some sig?
nificant linguistic information we already have) 
would enable us to put commas correctly in any 
text,  just  implementing some simple rules.  Be?
sides, a combination of both methods ??learning 
commas  and  putting  commas  after  identifying 
clauses??  would  probably  improve  the  results 
even more. 
Finally,  we contemplate building an ICALL 
(Intelligent Computer Assisted Language Learn?
ing) system to help learners to put commas cor?
rectly.
Acknowledgements
We would like to thank all the people who have 
collaborated in this research: Juan Garzia,  Joxe 
Ramon  Etxeberria,  Igone  Zabala,  Juan  Carlos 
Odriozola, Agurtzane Elorduy, Ainara Ondarra, 
Larraitz Uria and Elisabete Pociello. 
This research is supported by the University 
of  the  Basque  Country  (9/UPV00141.226?
14601/2002) and the Ministry of Industry of the 
Basque  Government  (XUXENG  project, 
OD02UN52).
References
Aduriz  I., Aranzabe  M., Arriola  J., D?az  de  Ilarraza 
A., Gojenola  K., Oronoz  M., Uria  L.   2004.
A  Cascaded  Syntactic  Analyser  for  Basque  
Computational  Linguistics  and  Intelligent  Text  
Processing. 2945  LNCS  Series.pg.  124?135. 
Springer Verlag. Berlin (Germany).
Aldezabal I., Aranzabe M., Arrieta B., Maritxalar M., 
Oronoz M. 2003.  Toward a punctuation checker 
for Basque. Atala Workshop on Punctuation. Paris 
(France).
Alegria I., Arregi  O., Ezeiza N., Fernandez I., Urizar 
R. 2004. Design and Development of a Named En?
tity  Recognizer  for  an  Agglutinative  Language. 
First International Joint Conference on NLP (IJC?
NLP?04). Workshop on Named Entity Recognition. 
Ansa O., Arregi X., Arrieta B., Ezeiza N., Fernandez 
I.,  Garmendia  A.,  Gojenola  K.,  Laskurain  B., 
Mart?nez  E.,  Oronoz  M.,  Otegi  A.,  Sarasola  K., 
Uria L. 2004. Integrating NLP Tools for Basque in  
Text Editors. Workshop on International Proofing 
Tools  and Language Technologies.  University  of 
Patras (Greece).
Aranzabe M., Arriola J.M., D?az de Ilarraza A.  2004.
Towards  a  Dependency  Parser  of  Basque.
Proceedings of the Coling 2004 Workshop on Re?
cent Advances in Dependency Grammar. Geneva 
(Switzerland).
Bayraktar M., Say B., Akman V. 1998. An Analysis of  
English Punctuation:  the special  case of  comma. 
International  Journal  of  Corpus  Linguistics 
3(1):pp. 33?57.  John  Benjamins  Publishing  Com?
pany. Amsterdam (The Netherlands).
Beeferman D.,  Berger  A.,  Lafferty  J.  1998.  Cyber?
punc: a lightweight punctuation annotation system 
for speech. Proceedings of the IEEE International 
Conference on Acoustics, Speech and Signal Pro?
cessing, pages 689?692, Seattle (WA).
Brill, E. 1994.  Some Advances in rule?based part of  
speech tagging. In Proceedings of the Twelfth Na?
tional Conference on Artificial Intelligence. Seattle 
(WA). 
Brill,  E.  1995.  Transformation?based  error?driven 
learning and natural language processing: a case 
study  in  part  of  speech  tagging. Computational 
Linguistics 21(4). MIT Press. Cambridge (MA).
Briscoe T., Carroll J. 1995.  Developing and evaluat?
ing a probabilistic lr parser of part?of?speech and  
punctuation  labels.  ACL/SIGPARSE 4th  interna?
tional Workshop on Parsing Technologies, Prague / 
Karlovy Vary (Czech Republic). 
Carreras X., M?rquez L. 2003. Phrase Recognition by 
Filtering and Ranking with Perceptrons. Proceed?
ings of the 4th RANLP Conference. Borovets (Bul?
garia).
D?az de  Ilarraza A., Gojenola K., Oronoz M.   2005.
Design and Development of a System for the De?
tection of Agreement Errors in Basque. CICLing?
2005, Sixth International Conference on Intelligent 
Text  Processing  and  Computational  Linguistics. 
Mexico City (Mexico).
Garzia  J.  1997.  Joskera  Lantegi. Herri  Arduralar?
itzaren Euskal Erakundea. Gasteiz, Basque Country 
(Spain).
7
Hardt D. 2001.  Comma checking in Danish.  Corpus 
linguistics. Lancaster (England). 
Hill R.L., Murray W.S. 1998.  Commas and Spaces: 
the Point of Punctuation. 11th Annual CUNY Con?
ference  on  Human  Sentence  Processing.  New 
Brunswick, New Jersey (USA). 
Jones B. 1996. Towards a Syntactic Account of Punc?
tuation. Proceedings of the 16th International Con?
ference on Computational Linguistics. Copenhagen 
(Denmark). 
Nunberg,  G.  1990.  The  linguistics  of  punctuation. 
Center for the Study of Language and Information. 
Leland Stanford Junior University (USA).
Say B., Akman V. 1996.  Information?Based Aspects 
of  Punctuation.  Proceedings  ACL/SIGPARSE In?
ternational  Meeting  on  Punctuation  in  Computa?
tional  Linguistics,  pages  pp. 49?56,  Santa  Cruz, 
California (USA). 
Tjong Kim Sang E.F. and Buchholz S. 2000.  Intro?
duction to the CoNLL?2000 shared task: chunking. 
In  proceedings  of  CoNLL?2000  and  LLL?2000. 
Lisbon (Portugal).
Tjong Kim Sang E.F. and D?jean H. 2001. Introduc?
tion to the CoNLL?2001 shared task: clause identi?
fication. In proceedings of CoNLL?2001. Tolouse 
(France).
Van Delden  S.,  Gomez  F.  2002.  Combining  Finite 
State Automata and a Greedy Learning Algorithm 
to Determine the Syntactic Roles of Commas. 14th 
IEEE International Conference on Tools with Arti?
ficial Intelligence. Washington, D.C. (USA)
Zubimendi,  J.R. 2004.  Ortotipografia.  Estilo liburu?
aren lehen atala. Eusko Jaurlaritzaren Argitalpen 
Zerbitzu  Nagusia.  Gasteiz,  Basque  Country 
(Spain).
8
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 580?584, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
EHU-ALM: Similarity-Feature Based Approach for Student Response
Analysis
Itziar Aldabe, Montse Maritxalar
IXA NLP Group
University of Basque Country (UPV-EHU)
itziar.aldabe@ehu.es
montse.maritxalar@ehu.es
Oier Lopez de Lacalle
University of Edinburgh
IKERBASQUE,
Basque Foundation for Science
oier.lopezdelacalle@gmail.com
Abstract
We present a 5-way supervised system based
on syntactic-semantic similarity features. The
model deploys: Text overlap measures,
WordNet-based lexical similarities, graph-
based similarities, corpus-based similarities,
syntactic structure overlap and predicate-
argument overlap measures. These measures
are applied to question, reference answer and
student answer triplets. We take into account
the negation in the syntactic and predicate-
argument overlap measures. Our system uses
the domain-specific data as one dataset to
build a robust system. The results show that
our system is above the median and mean on
all the evaluation scenarios of the SemEval-
2013 task #7.
1 Introduction
In this paper we describe our participation with a
feature-based supervised system to the SemEval-
2013 task #7: The Joint Student Response Analy-
sis and 8th Recognizing Textual Entailment Chal-
lenge (Dzikovska et al, 2013). The goal of our
participation is to build a generic system that is
robust enough across domains and scenarios. A
domain-specific system requires new training ex-
amples when shifting to a new domain. However,
domain-specific data is difficult to obtain and creat-
ing new resources is expensive.
We seek robustness by mixing the instances from
BEETLE and SCIENTSBANK. We show our strategy
is suitable to build a generic system that performs
competitively on any domain in the 5-way task.
The paper proceeds as follows. Section 2 de-
scribes the system presenting the learning features
and the runs. In Section 3 we show the optimiza-
tion details, followed by the results (Section 4) and
a preliminary error analysis (Section 5).
2 System description
Our system aims for robustness using the domain-
specific training data as one dataset. Therefore,
we do not differentiate between examples from the
given domains (BEETLE and SCIENTSBANK) when
training the system. In contrast, our approach dintin-
guishes between new questions (unseen answer vs.
unseen question) as well as question types (how,
what and why) by means of simple heuristics.
The runs are organized according to different sys-
tem designs. Although all the runs use the same fea-
ture set, we split the training set to build more spe-
cialized classifiers. Training examples are grouped
depending on: i) the answer is unseen; ii) the ques-
tion is unseen; and iii) the question type (i.e. what,
how, why). Each run defines a framework to explore
the different ways to approach the problem. While
the first run is the simplest and is the most generic
in nature, the third tries to split the task into simpler
problems and creates more specialized classifiers.
2.1 Similarity learning features
Our model is based on various text similarity fea-
tures. Almost all of the measures are computed be-
tween question, reference answer and student an-
swer triplets. The measures based on syntactic struc-
ture and predicate-argument overlaps are only ap-
plied to the student and reference answer pairs. In
580
total, we defined 30 features which can be grouped
as follows:
Text overlapmeasures The similarity of two texts
is computed based on the number of overlapping
words. We obtain the similarity of two texts based
on the F-Measure, the Dice Coefficient, The Cosine,
and the Lesk measures. For that, we use the imple-
mentation available in the Text::Similarity package1.
WordNet-based lexical similarities All the simi-
larity metrics based on WordNet (Miller, 1995) fol-
low the methodology proposed in (Mihalcea et al,
2006). For each open-class word in one of the in-
put texts, we obtain the maximun semantic similar-
ity or relatedness value matching the same open-
class words in the other input text. The values of
each matching are summed up and normalized by
the length of the two input texts as explained in
(Mihalcea et al, 2006). We compute the measures
of Resnik, Lin, Jiang-Conrath, Leacock-Chodorow,
Wu-Palmer, Banerjee-Pedersen, and Patwardhan-
Pedersen provided in the WordNet::Similarity pack-
age (Patwardhan et al, 2003).
Graph-based similarities The similarity of two
texts is based on a graph-based representation
(Agirre and Soroa, 2009) of WordNet. The method
is a two-step process: first the personalized PageR-
ank over WordNet is computed for each text. This
produces a probability distribution over WordNet.
Then, the probability distributions are encoded as
vectors and the cosine similarity between those vec-
tors is calculated.
Corpus-based similarities We compute two
corpus-based similarity measures: Latent Semantic
Analysis (Deerwester et al, 1990) and Latent
Dirichlet Allocation (Blei et al, 2003). We estimate
100 dimensions for LSA and 50 topics for LDA.
Both models are obtained from a subset of the En-
glish Wikipedia following the hierarchy of science
categories. We started with a small set of categories
and recovered the articles below the sub-hierarchy.
We only went 3 levels down to avoid noisy articles
as the category system is rather flat. The similarity
of two texts is the cosine similarity between the
1http://www.d.umn.edu/ tpederse/text-similarity.html
resulting vectors associated with each text in the
latent space.
Syntactic structure overlap The role of syntax is
studied by the use of graph subsumption based on
the approach proposed in (McCarthy et al, 2008).
The text is mapped into a graph with nodes rep-
resenting words and links indicating syntactic de-
pendencies between them. The similarity of two
texts is computed based on the overlap of the syn-
tactic structures. Negation is handled explicitly in
the graph.
Predicate-argument overlap The similarity of
two texts is computed by analyzing the overlap of
the predicates and their associated semantic argu-
ments. The system looks for verbal and nominal
predicates. The similarity is also based on the ap-
proach proposed in (McCarthy et al, 2008). The
graph is represented with words as nodes and the
semantic role of arguments as links. First, the ver-
bal propositions and their arguments are automat-
ically obtained (Bjo?rkelund et al, 2009) as repre-
sented in PropBank (Palmer et al, 2005). Second,
a generalization of the predicates is obtained based
on VerbNet (Kipper, 2005) and NomBank (Meyers
et al, 2004). Finally, the similarity of two texts
is computed based on the overlap of the predicate-
argument relations.
2.2 Architecture of the runs
Generic Framework RUN1 This is the simplest
framework for the assessment of student answers.
The system relies on a single classifier, which has
been optimized on the unseen question scenario.
The scenario is simulated by splitting the training
set so that each question and its answers are in the
same fold.
Unseen Framework RUN2 This framework relies
on two classifiers. The first is tuned on an unseen
answer scenario and the second is prepared for the
question scenario (cf. RUN1). In order to build the
unseen answer classifier, we split the training set so
that answers to the same question can occur in dif-
ferent folders. In test time, the instance is classified
depending on whether it is an unseen answer or an
581
BEETLE SCIENTSBANK OVERALL
Uns-answ Uns-qst All Uns-answ Uns-qst Uns-dom All All
RUN1 0.499 (6) 0.352 (7) 0.404 0.396 (7) 0.283 (4) 0.345 (3) 0.348 0.406
RUN2 0.526 (4) 0.352 (7) 0.413 0.418 (6) 0.283 (4) 0.345 (3) 0.350 0.414
RUN3 0.502 (5) 0.370 (6) 0.415 0.424 (5) 0.260 (8) 0.337 (5) 0.340 0.403
LOWEST 0.170 0.173 - 0.089 0.095 0.121 - -
BEST 0.619 0.552 - 0.478 0.307 0.380 - -
MEAN 0.435 0.343 - 0.341 0.240 0.267 - -
MEDIAN 0.437 0.326 - 0.376 0.259 0.268 - -
Table 1: 5-way results of the runs in F1 macro-average on BEETLE and SCIENTSBANK domains across different
scenarios. Along with the runs, the LOWEST and the BEST system in each scenario are shown. The MEAN and
MEDIAN of the dataset are also presented. Finally, the OVERALL results are showed summing up both domains. Uns-
answ refers to unseen answers scenario, Uns-qst stands for unseen question, Uns-dom unseen domain and All refers
to the sum of all scenarios. The run results are presented together with the ranked position in the task.
unseen question2.
Question-type Framework RUN3 The run con-
sists of a set of question-type expert classifiers. We
divided the training set based on whether an instance
reflected a what, how or why question. We then par-
titioned each question type into unseen answer and
unseen question scenarios. In total, the framework
deploys 6 classifiers, i.e. a test instance is classified
according to the question type and scenario. We set
heuristics to automatically distinguish the instance
type.
3 Optimization on training set
We set a heuristic to create the training instances.
For each student answer, if the matching reference
answer is indicated in it, we create a triplet with the
question, the student answer, and the matching ref-
erence answer. If there is no matching answer, the
reference answer is randomly selected giving pref-
erence to the best reference answers.
Once we have a training set, we split it into dif-
ferent ways to simulate the scenarios described in
Section 2.2. All the models are optimized using 10-
fold cross-validation of the pertaining training set.
For the classifiers in RUN1 and RUN2 we used 8910
training instances. For RUN3 the instances were di-
vided as follows: 1235 instances for how questions,
3089 for what questions and 4589 for why ques-
tions. In total, we obtained 8 models which were
distributed through the runs.
2We treat unseen-domain instances as unseen-question in-
stances.
Our approach uses Support Vector Ma-
chine (Chang and Lin, 2011) to build the classifiers.
As the number of features is not high, we used the
gaussian kernel in order to solve the non-linear
problem. The main parameters of the kernel (? and
C) were tuned using grid search over the parameter
in the cross-validation setting. We focused on
optimizing the F1 macro average of the classifier
in order to avoid a bias towards the major classes.
Each of the 8 classifiers were tuned independently.
The triplets of question, student answer and ref-
erence answer of the test instances were always cre-
ated selecting the first reference answer of the given
set of answers.
4 Results
A total of 8 teams participated in the 5-way task,
submitting a total of 16 system runs (Dzikovska et
al., 2013). Table 1 shows the performance obtained
by our systems across domains and different scenar-
ios. Our three runs ranked differently based on the
evaluation scenario: beetle-uns-answ (6,4,5 rank for
RUN1, RUN2, RUN3, respectively); beetle-uns-qst
(7,7,6); scientsbank-uns-answ (7,6,5); scientsbank-
uns-qst (4,4,8) and scientsbank-uns-dom (3,3,5). We
also evaluated our runs on the entire domain (All
columns) and on the whole test set (OVERALL).
The results show we built robust systems. Despite
being below the best system of each evaluation sce-
nario, the results show that the runs are competitive.
All our runs are above the median and outperform
the average results on each evaluation. Overall, the
results attained in SCIENTSBANK are lower than in
582
BEETLE. This might be due to the questions and
answers being longer in SCIENTSBANK, making it
difficult to obtain good patterns.
As regards our runs, there is no significant overall
difference. While RUN3 performs better in BEETLE
unseen question and SCIENTSBANK unseen answer,
in the rest of scenarios RUN2 outperforms the rest
of the runs. As expected, RUN2 outperforms RUN1
in the unseen answer scenario since the former has
a module specializing in unseen answers. However,
although RUN3 is an ensemble of six classifiers, it is
not the best run. This is probably because the train-
ing sets are not big enough.
Unseen framework (RUN2)
Prec Rec F1
correct 0.552 0.677 0.608
partially correct 0.324 0.323 0.323
contradictory 0.239 0.121 0.160
irrelevant 0.472 0.377 0.419
non domain 0.415 0.849 0.557
Macro average 0.400 0.469 0.414
Micro average 0.443 0.464 0.446
Table 2: results of the RUN2 system on a entire test set.
Table 2 shows the detailed results of the RUN2
system on the entire test set. It is noticeable the
low results obtained on the contradictory class. This
might be because the defined features are not able
to model negation properly and do not deal with
antonymy. Surprisingly, the non domain class is not
the most problematic, even if the system was trained
on a low number of instances.
5 Preliminary Error Analysis
We conducted a preliminary error analysis and stud-
ied some of the misclassified test instances to detect
some problematic issues and to define improvements
to our approach.
Example 5.1 Sam and Jasmine were sitting on a
park bench eating their lunches. A mosquito landed
on Sam?s arm and Sam began slapping at it. When
he did that, he knocked Jasmine?s soda into her lap,
causing her to jump up. What was Sam?s response?
R: Sam?s response was to slap the mosquito.
S1: Sam?s response was to say sorry
S2: To smack the bee.
Some of the detected errors suggest that our use
of syntax and lexical overlap is not sufficient to iden-
tify the correct class. Our system marks the student
answer S1 from Example 5.13 as correct. The ref-
erence answer and the student answer share a great
number of words and the dependency trees are al-
most identical, but not the meanings. In addition, the
question contains additional information that may
require other types of features to correctly classify
the instance.
The predicate-argument overlap feature tries to
generalize the predicate information to find similar-
ities between verbs with the same meaning. How-
ever, our system does not always work in a correct
way. The verb smack in the student answer S2 and
the verb slap in the reference answer mean the same.
Our system classifies the answer incorrectly. If we
look at PropBank and VerbNet, we find that there
is not mapping between PropBank and VerbNet for
these particular verbs.
Example 5.2 Why do you think the other terminals
are being held in a different electrical state than that
of the negative terminal?
R: Terminals 4, 5 and 6 are not connected to the
negative battery terminal
S1: They are connected to the positive battery ter-
minal
We consider the negation as part of the syntac-
tic and predicate-argument overlap measures. How-
ever, our system does not characterize the similar-
ity between not connected to the negative and con-
nected to the positive (Example 5.2). This type of
examples suggest that the system needs to model the
negation and antonyms with additional features.
In the future, further error analysis will be car-
ried out to design features to better model the prob-
lem. We also anticipate creating a specialized fea-
ture space for each question type.
Acknowledgments
This research was partially funded by the Ber2Tek
project (IE12-333), the SKaTeR project (TIN2012-
38584-C06-02) and the NewsReader project (FP7-
ICT-2011-8-316404).
3R refers to the reference answer and S1 and S2 to student
answers.
583
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th conference of the European chapter of
the Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Anders Bjo?rkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of The Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL-2009),
pages 43?48.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Scott Deerwester, Susan Dumais, Goerge Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In D. Wilson and G. Sut-
cliffe, editors, Proceedings of the 21st International
Florida Artificial Intelligence Research Society Con-
ference, pages 201?206, Menlo Park, CA: The AAAI
Press.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The nombank project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May 2 - May 7. Associ-
ation for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings the
American Association for Artificial Intelligence (AAAI
2006), Boston.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38(11):39?41.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic role. Computational Linguistics, 31(1):71?
106.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using measures of semantic related-
ness for word sense disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics.
584
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 39?48,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning word-level dialectal variation as phonological replacement rules
using a limited parallel corpus
Mans Hulden
University of Helsinki
Language Technology
mans.hulden@helsinki.fi
In?aki Alegria
IXA taldea
UPV-EHU
i.alegria@ehu.es
Izaskun Etxeberria
IXA taldea
UPV-EHU
izaskun.etxeberria@ehu.es
Montse Maritxalar
IXA taldea
UPV-EHU
montse.maritxalar@ehu.es
Abstract
This paper explores two different methods of
learning dialectal morphology from a small
parallel corpus of standard and dialect-form
text, given that a computational description
of the standard morphology is available. The
goal is to produce a model that translates in-
dividual lexical dialectal items to their stan-
dard dialect counterparts in order to facili-
tate dialectal use of available NLP tools that
only assume standard-form input. The results
show that a learning method based on induc-
tive logic programming quickly converges to
the correct model with respect to many phono-
logical and morphological differences that are
regular in nature.
1 Introduction
In our work with the Basque language, a morpho-
logical description and analyzer is available for the
standard language, along with other tools for pro-
cessing the language (Alegria et al, 2002). How-
ever, it would be convenient to be able to analyze
variants and dialectal forms as well. As the dialectal
differences within the Basque language are largely
lexical and morphophonological, analyzing the di-
alectal forms would in effect require a separate mor-
phological analyzer that is able to handle the unique
lexical items in the dialect together with the differ-
ing affixes and phonological changes.
Morphological analyzers are traditionally hand-
written by linguists, most commonly using some
variant of the popular finite-state morphology ap-
proach (Beesley and Karttunen, 2002). This entails
having an expert model a lexicon, inflectional and
derivational paradigms as well as phonological al-
ternations, and then producing a morphological an-
alyzer/generator in the form of a finite-state trans-
ducer.
As the development of such wide-coverage mor-
phological analyzers is labor-intesive, the hope is
that an analyzer for a variant could be automatically
learned from a limited parallel standard/dialect cor-
pus, given that an analyzer already exists for the
standard language. This is an interesting problem
because a good solution to it could be applied to
many other tasks as well: to enhancing access to
digital libraries (containing diachronic and dialectal
variants), for example, or to improving treatment of
informal registers such as SMS messages and blogs,
etc.
In this paper we evaluate two methods of learning
a model from a standard/variant parallel corpus that
translates a given word of the dialect to its standard-
form equivalent. Both methods are based on finite-
state phonology. The variant we use for experiments
is Lapurdian,1 a dialect of Basque spoken in the La-
purdi (fr. Labourd) region in the Basque Country.
Because Basque is an agglutinative, highly in-
flected language, we believe some of the results can
be extrapolated to many other languages facing sim-
ilar challenges.
One of the motivations for the current work is
that there are a large number of NLP tools avail-
able and in development for standard Basque (also
called Batua): a morphological analyzer, a POS tag-
ger, a dependency analyzer, an MT engine, among
1Sometimes also called Navarro-Labourdin or Labourdin.
39
others (Alegria et al, 2011). However, these tools
do not work well in processing the different dialects
of Basque where lexical items have a different ortho-
graphic representation owing to slight differences in
phonology and morphology.
Here is a brief contrastive example of the kinds
of differences found in the (a) Lapurdian dialect and
standard Basque (b) parallel corpus:2
(a) Ez gero uste izan nexkatxa guziek tu egiten dautatela
(b) Ez gero uste izan neskatxa guztiek tu egiten didatela
As the example illustrates, the differences are mi-
nor overall?the word order and syntax are unaf-
fected, and only a few lexical items differ. This re-
flects the makeup of our parallel corpus quite well?
in it, slightly less than 20% of the word tokens
are distinct. However, even such relatively small
discrepancies cause great problems in the poten-
tial reuse of current tools designed for the standard
forms only.
We have experimented with two approaches that
attempt to improve on a simple baseline of mem-
orizing word-pairs in the dialect and the standard.
The first approach is based on work by Almeida
et al (2010) on contrasting orthography in Brazil-
ian Portuguese and European Portuguese. In this
approach differences between substrings in distinct
word-pairs are memorized and these transformation
patterns are then applied whenever novel words are
encountered in the evaluation. To prevent over-
generation, the output of this learning process is
later subject to a morphological filter where only ac-
tual standard-form outputs are retained. The second
approach is an Inductive Logic Programming-style
(ILP) (Muggleton and De Raedt, 1994) learning
algorithm where phonological transformation rules
are learned from word-pairs. The goal is to find a
minimal set of transformation rules that is both nec-
essary and sufficient to be compatible with the learn-
ing data, i.e. the word pairs seen in the training data.
The remainder of the paper is organized as fol-
lows. The characteristics of the corpus available to
us are described in section 2. In sections 3, 4, and 5,
we describe the steps and variations of the methods
we have applied and how they are evaluated. Sec-
tion 6 presents the experimental results, and finally,
2English translation of the example: Don?t think all girls spit
on me
we discuss the results and present possibilities for
potential future work in section 7.
1.1 Related work
The general problem of supervised learning of di-
alectal variants or morphological paradigms has
been discussed in the literature with various connec-
tion to computational phonology, morphology, ma-
chine learning, and corpus-based work. For exam-
ple, Kestemont et al (2010) presents a language-
independent system that can ?learn? intra-lemma
spelling variation. The system is used to produce
a consistent lemmatization of texts in Middle Dutch
literature in a medieval corpus, Corpus-Gysseling,
which contains manuscripts dated before 1300 AD.
These texts have enormous spelling variation which
makes a computational analysis difficult.
Koskenniemi (1991) provides a sketch of a dis-
covery procedure for phonological two-level rules.
The idea is to start from a limited number of
paradigms (essentially pairs of input-output forms
where the input is the surface form of a word and the
output a lemmatization plus analysis). The problem
of finding phonological rules to model morpholog-
ical paradigms is essentially similar to the problem
presented in this paper. An earlier paper, Johnson
(1984), presents a ?discovery procedure? for learning
phonological rules from data, something that can be
seen as a precursor to the problem dealt with by our
ILP algorithm.
Mann and Yarowsky (2001) present a method
for inducing translation lexicons based on transduc-
tion models of cognate pairs via bridge languages.
Bilingual lexicons within languages families are in-
duced using probabilistic string edit distance mod-
els. Inspired by that paper, Scherrer (2007) uses
a generate-and-filter approach quite similar to our
first method. He compares different measures of
graphemic similarity applied to the task of bilin-
gual lexicon induction between Swiss German and
Standard German. Stochastic transducers are trained
with the EM algorithm and using handmade trans-
duction rules. An improvement of 11% in F-score is
reported over a baseline method using Levenshtein
Distance.
40
Full corpus 80% part. 20% part.
Sentences 2,117 1,694 423
Words 12,150 9,734 2,417
Unique words
Standard Basque 3,553 3,080 1,192
Lapurdian 3,830 3,292 1,239
Filtered pairs 3,610 3,108 1,172
Identical pairs 2,532 2,200 871
Distinct pairs 1,078 908 301
Table 1: Characteristics of the parallel corpus used for
experiments.
2 The corpus
The parallel corpus used in this research is part of
?TSABL? project developed by the IKER group in
Baiona (fr. Bayonne).3 The researchers of the IKER
project have provided us with examples of the La-
purdian dialect and their corresponding forms in
standard Basque. Our parallel corpus then contains
running text in two variants: complete sentences of
the Lapurdian dialect and equivalent sentences in
standard Basque.
The details of the corpus are presented in table 1.
The corpus consists of 2,117 parallel sentences, to-
taling 12,150 words (roughly 3,600 types). In order
to provide data for our learning algorithms and also
to test their performance, we have divided the cor-
pus into two parts: 80% of the corpus is used for the
learning task (1,694 sentences) and the remaining
20% (423 sentences) for evaluation of the learning
process. As is seen, roughly 23% of the word-pairs
are distinct. Another measure of the average devi-
ation between the word pairs in the corpus is given
by aligning all word-pairs by minimum edit distance
(MED): aligning the 3,108 word-pairs in the learn-
ing corpus can be done at a total MED cost of 1,571.
That is, roughly every 14th character in the dialect
data is different from the standard form.
3 The baseline
The baseline of our experiments is a simple method,
based on a dictionary of equivalent words with the
list of correspondences between words extracted
3Towards a Syntactic Atlas of the Basque Language, web
site: http://www.iker.cnrs.fr/-tsabl-towards-a-syntactic-atlas-
of-.html
from the learning portion (80%) of the corpus. This
list of correspondences contains all different word
pairs in the variant vs. standard corpus. The baseline
approach consists simply of memorizing all the dis-
tinct word pairs seen between the dialectal and stan-
dard forms, and subsequently applying this knowl-
edge during the evaluation task. That is, if an in-
put word during the evaluation has been seen in the
training data, we provide the corresponding previ-
ously known output word as the answer. Otherwise,
we assume that the output word is identical to the
input word.
4 Overview of methods
We have employed two different methods to produce
an application that attempts to extract generaliza-
tions from the training corpus to ultimately be able
to produce the equivalent standard word correspond-
ing to a given dialectal input word. The first method
is based on already existing work by Almeida et al
(2010) that extracts all substrings from lexical pairs
that are different. From this knowledge we then pro-
duce a number of phonological replacement rules
that model the differences between the input and
output words. In the second method, we likewise
produce a set of phonological replacement rules, us-
ing an ILP approach that directly induces the rules
from the pairs of words in the training corpus.
The core difference between the two methods is
that while both extract replacement patterns from
the word-pairs, the first method does not consider
negative evidence in formulating the replacement
rules. Instead, the existing morphological analyzer
is used as a filter after applying the rules to unknown
text. The second method, however, uses negative
evidence from the word-pairs in delineating the re-
placement rules as is standard in ILP-approaches,
and the subsequent morphological filter for the out-
put plays much less of a role. Evaluating and com-
paring both approaches is motivated because the first
method may produce much higher recall by virtue
of generating a large number of input-output candi-
dates during application, and the question is whether
the corresponding loss in precision may be mitigated
by judicious application of post-processing filters.
41
4.1 Format of rules
Both of the methods we have evaluated involve
learning a set of string-transformation rules to
convert words, morphemes, or individual letters
(graphemes) in the dialectal forms to the stan-
dard variant. The rules that are learned are in
the format of so-called phonological replacement
rules (Beesley and Karttunen, 2002) which we have
later converted into equivalent finite-state transduc-
ers using the freely available foma toolkit (Hulden,
2009a). The reason for the ultimate conversion of
the rule set to finite-state transducers is twofold:
first, the transducers are easy to apply rapidly to
input data using available tools, and secondly, the
transducers can further be modified and combined
with the standard morphology already available to
us as a finite transducer.
In its simplest form, a replacement rule is of the
format
A? B || C D (1)
where the arguments A,B,C,D are all single sym-
bols or strings. Such a rule dictates the transfor-
mation of a string A to B, whenever the A occurs
between the strings C and D. Both C and D are
optional arguments in such a rule, and there may
be multiple conditioning environments for the same
rule.
For example, the rule:
h -> 0 || p , t , l , a s o
(2)
would dictate a deletion of h in a number of con-
texts; when the h is preceded by a p, t, or l, or suc-
ceeded by the sequence aso, for instance transform-
ing ongiethorri (Lapurdian) to ongietorri (Batua).
As we will be learning several rules that each tar-
get different input strings, we have a choice as to the
mode of application of the rules in the evaluation
phase. The learned rules could either be applied in
some specific order (sequentially), or applied simul-
taneously without regard to order (in parallel).
For example, the rules:
u -> i || z a (3)
k -> g || z a u (4)
would together (in parallel) change zaukun into zai-
gun. Note that if we imposed some sort of ordering
on the rules and the u ? i rule in the set would
apply first, for example, the conditioning environ-
ment for the second rule would no longer be met
after transforming the word into zaikun. We have
experimented with sequential as well as parallel pro-
cessing, and the results are discussed below.
4.2 Method 1 (lexdiff) details
The first method is based on the idea of identi-
fying sequences inside word pairs where the out-
put differs from the input. This was done through
the already available tool lexdiff which has been
used in automatic migration of texts between differ-
ent Portuguese orthographies (Almeida et al, 2010).
The lexdiff program tries to identify sequences of
changes from seen word pairs and outputs string cor-
respondences such as, for example: 76 ait ->
at ; 39 dautz -> diz (stemming from pairs
such as (joaiten/joaten and dautzut/dizut), indicating
that ait has changed into at 76 times in the cor-
pus, etc., thus directly providing suggestions as to
phonologically regular changes between two texts,
with frequency information included.
With such information about word pairs we gen-
erate a variety of replacement rules which are then
compiled into finite transducers with the foma ap-
plication. Even though the lexdiff program provides
a direct string-to-string change in a format that is
directly compilable into a phonological rule trans-
ducer, we have experimented with some possible
variations of the specific type of phonological rule
we want to output:
? We can restrict the rules by frequency and re-
quire that a certain type of change be seen at
least n times in order to apply that rule. For
example, if we set this threshold to 3, we will
only apply a string-to-string changing rule that
has been seen three or more times.
? We limit the number of rules that can be
applied to the same word. Sometimes the
lexdiff application divides the change be-
tween a pair of words into two separate rules.
For example the word-word correspondence
agerkuntza/agerpena is expressed by two rules:
rkun -> rpen and ntza -> na. Now,
given these two rules, we have to be able to
apply both to produce the correct total change
42
Figure 1: The role of the standard Basque (Batua) ana-
lyzer in filtering out unwanted output candidates created
by the induced rule set produced by method 1.
agerkuntza/agerpena. By limiting the number
of rules that can apply to a single input word we
can avoid creating many spurious outputs, but
also at the same time we may sacrifice some
ability to produce the desired output forms.
? We can also control the application mode of the
rules: sequential or parallel. If the previous
two rules are applied in parallel, the form ob-
tained from agerkuntza will not be correct
since the n overlaps with the two rules. That
is, when applying rules simultaneously in par-
allel, the input characters for two rules may not
overlap. However, if these two rules applied
in sequence (the order in this example is irrel-
evant), the output will be the correct: we first
change rkun -> rpen and later ntza ->
na. We have not a priori chosen to use parallel
or sequential rules and have decided to evaluate
both approaches.
? We can also compact the rules output by lex-
diff by eliminating redundancies and construct-
ing context-sensitive rules. For example: given
a rule such as rkun -> rpen, we can con-
vert this into a context-sensitive rule that only
changes ku into pe when flanked by r and n
to the left and right, respectively, i.e. producing
a rule:
k u -> p e || r n (5)
This has a bearing on the previous point and
will allow more rewritings within a single word
in parallel replacement mode since there are
fewer characters overlapping.
Once a set of rules is compiled with some instanti-
ation of the various parameters discussed above and
converted to a transducer, we modify the transducer
in various ways to improve on the output.
First, since we already have access to a large-scale
morphological transducer that models the standard
Basque (Batua), we restrict the output from the con-
version transducer to only allow those words as out-
put that are legitimate words in standard Basque.
Figure 1 illustrates this idea. In that figure, we see an
input word in the dialect (emaiten) produce a num-
ber of candidates using the rules induced. However,
after adding a morphological filter that models the
Batua, we retain only one output.
Secondly, in the case that even after applying
the Batua filter we retain multiple outputs, we sim-
ply choose the most frequent word (these unigram
counts are gathered from a separate newspaper cor-
pus of standard Basque).
4.3 Method 2 (ILP) details
The second method we have employed works
directly from a collection of word-pairs (di-
alect/standard in this case). We have developed an
algorithm that from a collection of such pairs seeks
a minimal hypothesis in the form of a set of replace-
ment rules that is consistent with all the changes
found in the training data. This approach is gener-
ally in line with ILP-based machine learning meth-
ods (Muggleton and De Raedt, 1994). However, in
contrast to the standard ILP, we do not learn state-
ments of first-order logic that fit a collection of data,
but rather, string-to-string replacement rules.4
4Phonological string-to-string replacement rules can be de-
fined as collections of statements in first-order logic and com-
piled into transducers through such logical statements as well;
43
The two parameters to be induced are (1) the col-
lection of string replacements X ? Y needed to
characterize the training data, and (2) the minimal
conditioning environments for each rule, such that
the collection of rules model the string transforma-
tions found in the training data.
The procedure employed for the learning task is
as follows:
(1) Align all word pairs (using minimum edit dis-
tance by default).
(2) Extract a collection of phonological rewrite
rules.
(3) For each rule, find counterexamples.
(4) For each rule, find the shortest conditioning en-
vironment such that the rule applies to all pos-
itive examples, and none of the negative exam-
ples. Restrict rule to be triggered only in this
environment.
The following simple example should illustrate
the method. Assuming we have a corpus of only
two word pairs:
emaiten ematen
igorri igorri
in step (1) we would perform the alignment and pro-
duce the output
e m a i t e n i g o r r i
e m a ? t e n i g o r r i
From this data we would in step (2) gather that
the only active phonological rule is i ? ?, since
all other symbols are unchanged in the data. How-
ever, we find two counterexamples to this rule (step
3), namely two i-symbols in igorri which do not al-
ternate with ?. The shortest conditioning environ-
ment that accurately models the data and produces
no overgeneration (does not apply to any of the is in
igorri) is therefore:
i -> ? || a (6)
see e.g. Hulden (2009b) for details. In other words, in this
work, we skip the intermediate step of defining our observa-
tions as logical statements and directly convert our observations
into phonological replacement rules.
the length of the conditioning environment being 1
(1 symbol needs to be seen to the left plus zero sym-
bols to the right). Naturally, in this example we have
two competing alternatives to the shortest general-
ization: we could also have chosen to condition the
i-deletion rule by the t that follows the i. Both con-
ditioning environments are exactly one symbol long.
To resolve such cases, we a priori choose to favor
conditioning environments that extend farther to the
left. This is an arbitrary decision?albeit one that
does have some support from phonology as most
phonological assimilation rules are conditioned by
previously heard segments?and very similar results
are obtained regardless of left/right bias in the learn-
ing. Also, all the rules learned with this method are
applied simultaneously (in parallel) in the evaluation
phase.
4.3.1 String-to-string vs. single-symbol rules
In some cases several consecutive input symbols
fail to correspond to the output in the learning data,
as in for example the pairing
d a u t
d i ? t
corresponding to the dialect-standard pair daut/dit.
Since there is no requirement in our formalism of
rewrite rules that they be restricted to single-symbol
rewrites only, there are two ways to handle this: ei-
ther one can create a string-to-string rewriting rule:
au? i / CONTEXT
or create two separate rules
a? i / CONTEXT , u? ? / CONTEXT
where CONTEXT refers to the minimal condition-
ing environment determined by the rest of the data.
We have evaluated both choices, and there is no no-
table difference between them in the final results.
5 Evaluation
We have measured the quality of different ap-
proaches by the usual parameters of precision, re-
call and the harmonic combination of them, the F1-
score, and analyzed how the different options in the
two approaches affect the results of these three pa-
rameters. Given that we, especially in method 1,
extract quite a large number of rules and that each
44
input word generates a very large number of candi-
dates if we use all the rules extracted, it is possible to
produce a high recall on the conversion of unknown
dialect words to the standard form. However, the
downside is that this naturally leads to low precision
as well, which we try to control by introducing a
number of filters to remove some of the candidates
output by the rules. As mentioned above, we use
two filters: (1) an obligatory filter which removes
all candidate words that are not found in the stan-
dard Basque (by using an existing standard Basque
morphological analyzer), and (2) using an optional
filter which, given several candidates in the standard
Basque, picks the most frequently occurring one by
a unigram count from the separate newspaper cor-
pus. This latter filter turns out to serve a much more
prominent role in improving the results of method 1,
while it is almost completely negligible for method
2.
6 Results
As mentioned above, the learning process has made
use of 80% of the corpus, leaving 20% of the corpus
for evaluation of the above-mentioned approaches.
In the evaluation, we have only tested those words
in the dialect that differ from words in the standard
(which are in the minority). In total, in the evalu-
ation part, we have tested the 301 words that differ
between the dialect and the standard in the evalua-
tion part of the corpus.
The results for the baseline?i.e. simple memo-
rization of word-word correspondences?are (in %):
P = 95.62, R = 43.52 and F1 = 59.82. As ex-
pected, the precision of the baseline is high: when
the method gives an answer it is usually the correct
one. But the recall of the baseline is low, as is ex-
pected: slightly less than half the words in the eval-
uation corpus have been encountered before.5
6.1 Results with the lexdiff method
Table 2 shows the initial experiment of method
1 with different variations on the frequency
5The reason the baseline does not show 100% precision is
that the corpus contains minor inconsistencies or accepted al-
ternative spellings, and our method of measuring the precision
suffers from such examples by providing both learned alterna-
tives to a dialectal word, while only one is counted as being
correct.
P R F1
f ? 1 38.95 66.78 49.20
f ? 2 46.99 57.14 51.57
f ? 3 49.39 53.82 51.51
Table 2: Values obtained for Precision, Recall and F-
scores with method 1 by changing the minimum fre-
quency of the correspondences to construct rules for
foma. The rest of the options are the same in all three
experiments: only one rule is applied within a word.
P R F1
f ? 1 70.28 58.13 63.64
f ? 2 70.18 53.16 60.49
f ? 3 71.76 51.50 59.96
Table 3: Values obtained for Precision, Recall and F-
score with method 1 by changing the threshold frequency
of the correspondences and applying a post-filter.
threshold?this is the limit on the number of times
we must see a string-change to learn it. The re-
sults clearly show that the more examples we extract
(frequency 1), the better results we obtain for recall
while at the same time the precision suffers since
many spurious outputs are given?even many differ-
ent ones that each legitimately correspond to a word
in the standard dialect. The F1-score doesn?t vary
very much and it maintains similar values through-
out. The problem with this approach is one which
we have noted before: the rules produce a large
number of outputs for any given input word and
the consequence is that the precision suffers, even
though only those output words are retained that cor-
respond to actual standard Basque.
With the additional unigram filter in place, the
results improve markedly. The unigram-filtered re-
sults are given in table 3.
We have also varied the maximum number of
possible rule applications within a single word as
well as applying the rules in parallel or sequentially,
and compacting the rules to provide more context-
sensitivity. We shall here limit ourselves to present-
ing the best results of all these options in terms of
the F1-score in table 4.
In general, we may note that applying more than
45
P R F1
Exp1 72.20 57.81 64.21
Exp2 72.13 58.47 64.59
Exp3 75.10 60.13 66.79
Table 4: Method 1. Exp1: frequency 2; 2 rules applied;
in parallel; without contextual conditioning. Exp2: fre-
quency 1; 1 rule applied; with contextual conditioning.
Exp3: frequency 2; 2 rules applied; in parallel; with con-
textual conditioning.
one rule within a word has a negative effect on
the precision while not substantially improving the
recall. Applying the unigram filter?choosing the
most frequent candidate?yields a significant im-
provement: much better precision but also slightly
worse recall. Choosing either parallel or sequential
application of rules (when more than one rule is ap-
plied to a word) does not change the results signifi-
cantly. Finally, compacting the rules and producing
context-sensitive ones is clearly the best option.
In all cases the F1-score improves if the unigram
filter is applied; sometimes significantly and some-
times only slightly. All the results of the table 4
which lists the best performing ones come from ex-
periments where the unigram filter was applied.
Figure 2 shows how precision and recall val-
ues change in some of the experiments done with
method 1. There are two different groups of points
depending on if the unigram filter is applied, illus-
trating the tradeoff in precision and recall.
6.2 Results with the ILP method
The ILP-based results are clearly better overall, and
it appears that the gain in recall by using method
1 does not produce F1-scores above those produced
with the ILP-method, irrespective of the frequency
filters applied. Crucially, the negative evidence
and subsequent narrowness of the replacement rules
learned with the ILP method is responsible for the
higher accuracy. Also, the results from the ILP-
based method rely very little on the post-processing
filters, as will be seen.
The only variable parameter with the ILP method
concerns how many times a word-pair must be seen
to be used as learning evidence for creating a re-
placement rule. As expected, the strongest result
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
Re
ca
ll
Precision
P vs R
without filter
with filter
Figure 2: Tradeoffs of precision and recall values in the
experiments with method 1 using various different pa-
rameters. When the unigram filter is applied the precision
is much better, but the recall drops.
P R F1
n = 1 85.02 (86.13) 58.47 (57.80) 69.29 (69.18)
n = 2 82.33 (83.42) 54.15 (53.49) 65.33 (65.18)
n = 3 80.53 (82.07) 50.83 (50.17) 62.32 (62.26)
n = 4 81.19 (82.32) 50.17 (49.50) 62.01 (61.83)
Table 5: Experiments with the ILP method using a thresh-
old of 1?4 (times a word-pair is seen) to trigger rule learn-
ing. The figures in parentheses are the same results with
the added postprocessing unigram filter that, given sev-
eral output candidates of the standard dialect, chooses the
most frequent one.
is obtained by using all word-pairs, i.e. setting the
threshold to 1. Table 5 shows the degradation of per-
formance resulting from using higher thresholds.
Interestingly, adding the unigram filter that im-
proved results markedly in method 1 to the output of
the ILP method slightly worsens the results in most
cases, and gives no discernible advantage in others.
In other words, in those cases where the method pro-
vides multiple outputs, choosing the most frequent
one on a unigram frequency basis gives no improve-
ment over not doing so.
Additionally, there is comparatively little advan-
tage with this method in adding the morphological
filter to the output of the words in method 2 (this
is the filter that rules out non-standard words). The
results in table 5 include the morphological filter,
but omitting it altogether brings down the best F1
46
P R F1
Baseline 95.62 43.52 59.82
Method 1 (lexdiff) 75.10 60.13 66.79
Method 2 (ILP) 85.02 58.47 69.29
Table 6: The best results (per F1-score of the two meth-
ods). The parameters of method 1 included using only
those string transformations that occur at least 2 times in
the training data, and limiting rule application to a maxi-
mum of 2 times within a word, and including a unigram
post-filter. Rules were contextually conditioned. For
method 2, all the examples (threshold 1) in the training
data were used as positive and negative evidence, with-
out a unigram filter.
to 56.14 from 69.29. By contrast, method 1 de-
pends heavily on it and omitting the filter brings
down the F1-score from 66.79 to 11.53 with the
otherwise strongest result of method 1 seen in ta-
ble 6. The most prominent difference between the
two approaches is that while method 1 can be fine-
tuned using frequency information and various fil-
ters to yield results close to method 2, the ILP ap-
proach provides equally robust results without any
additional information?in particular, frequency in-
formation of the target language. We also find a
much lower rate of errors of commission with the
ILP method; this is somewhat obvious as it takes ad-
vantage of negative evidence directly while the first
method only does so indirectly through filters added
later.
7 Conclusions and future work
We have presented a number of experiments to solve
a very concrete task: given a word in the Lapurdian
dialect of Basque, produce the equivalent standard
Basque word. As background knowledge, we have
a complete standard Basque morphological analyzer
and a small parallel corpus of dialect and standard
text. The approach has been based on the idea of
extracting string-to-string transformation rules from
the parallel corpus, and applying these rules to un-
seen words. We have been able to improve on the
results of a naive baseline using two methods to in-
fer phonological rules of the information extracted
from the corpus and applying them with finite state
transducers. In particular, the second method, in-
ferring minimal phonological rewrite rules using
an Inductive Logic Programming-style approach,
seems promising as regards inferring phonological
and morphological differences that are quite regu-
lar in nature between the two language variants. We
expect that a larger parallel corpus in conjunction
with this method could potentially improve the re-
sults substantially?with a larger set of data, thresh-
olds could be set so that morphophonological gener-
alizations are triggered only after a sufficient num-
ber of training examples (avoiding overgeneration),
and, naturally, many more unique, non-regular, lexi-
cal correspondences could be learned.
During the current work, we have also accumu-
lated a small but valuable training and test corpus
which may serve as a future resource for evaluation
of phonological and morphological rule induction
algorithms.
In order to improve the results, we plan to re-
search the combination of the previous methods with
other ones which infer dialectal paradigms and rela-
tions between lemmas and morphemes for the di-
alect and the standard. These inferred relations
could be contrasted with the information of a larger
corpus of the dialect without using an additional par-
allel corpus.
Acknowledgments
We are grateful for the insightful comments
provided by the anonymous reviewers. This re-
search has been partially funded by the Spanish
Science and Innovation Ministry via the OpenMT2
project (TIN2009-14675-C03-01) and the European
Commission?s 7th Framework Program under grant
agreement no. 238405 (CLARA).
References
Alegria, I., Aranzabe, M., Arregi, X., Artola, X.,
D??az de Ilarraza, A., Mayor, A., and Sarasola, K.
(2011). Valuable language resources and applica-
tions supporting the use of Basque. In Vetulani,
Z., editor, Lecture Notes in Artifitial Intelligence,
volume 6562, pages 327?338. Springer.
Alegria, I., Aranzabe, M., Ezeiza, N., Ezeiza, A.,
and Urizar, R. (2002). Using finite state tech-
nology in natural language processing of basque.
47
In LNCS: Implementation and Application of Au-
tomata, volume 2494, pages 1?12. Springer.
Almeida, J. J., Santos, A., and Simoes, A.
(2010). Bigorna?a toolkit for orthography migra-
tion challenges. In Seventh International Con-
ference on Language Resources and Evaluation
(LREC2010), Valletta, Malta.
Beesley, K. R. and Karttunen, L. (2002). Finite-state
morphology: Xerox tools and techniques. Stud-
ies in Natural Language Processing. Cambridge
University Press.
Hulden, M. (2009a). Foma: a finite-state compiler
and library. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association
for Computational Linguistics: Demonstrations
Session, pages 29?32, Athens, Greece. Associa-
tion for Computational Linguistics.
Hulden, M. (2009b). Regular expressions and pred-
icate logic in finite-state language processing. In
Piskorski, J., Watson, B., and Yli-Jyra?, A., edi-
tors, Finite-State Methods and Natural Language
Processing?Post-proceedings of the 7th Interna-
tional Workshop FSMNLP 2008, volume 191 of
Frontiers in Artificial Intelligence and Applica-
tions, pages 82?97. IOS Press.
Johnson, M. (1984). A discovery procedure for cer-
tain phonological rules. In Proceedings of the
10th international conference on Computational
linguistics, COLING ?84, pages 344?347. Asso-
ciation for Computational Linguistics.
Kestemont, M., Daelemans, W., and Pauw, G. D.
(2010). Weigh your words?memory-based
lemmatization for Middle Dutch. Literary and
Linguistic Computing, 25(3):287?301.
Koskenniemi, K. (1991). A discovery procedure for
two-level phonology. Computational Lexicology
and Lexicography: A Special Issue Dedicated to
Bernard Quemada, pages 451?446.
Mann, G. S. and Yarowsky, D. (2001). Multi-
path translation lexicon induction via bridge lan-
guages. In Proceedings of the second meeting of
the North American Chapter of the Association
for Computational Linguistics on Language tech-
nologies, NAACL ?01, pages 1?8.
Muggleton, S. and De Raedt, L. (1994). Inductive
Logic Programming: theory and methods. The
Journal of Logic Programming, 19:629?679.
Scherrer, Y. (2007). Adaptive string distance mea-
sures for bilingual dialect lexicon induction. In
Proceedings of the 45th Annual Meeting of the
ACL: Student Research Workshop, ACL ?07,
pages 55?60. Association for Computational Lin-
guistics.
48
