Proceedings of the Second Workshop on Statistical Machine Translation, pages 193?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multi-Engine Machine Translation
with an Open-Source Decoder for Statistical Machine Translation
Yu Chen1, Andreas Eisele1,2, Christian Federmann2,
Eva Hasler3, Michael Jellinghaus1, Silke Theison1
(authors listed in alphabetical order)
1: Saarland University, Saarbru?cken, Germany
2: DFKI GmbH, Saarbru?cken, Germany
3: University of Cologne, Germany
Abstract
We describe an architecture that allows
to combine statistical machine translation
(SMT) with rule-based machine translation
(RBMT) in a multi-engine setup. We use a
variant of standard SMT technology to align
translations from one or more RBMT sys-
tems with the source text. We incorporate
phrases extracted from these alignments into
the phrase table of the SMT system and use
the open-source decoder Moses to find good
combinations of phrases from SMT training
data with the phrases derived from RBMT.
First experiments based on this hybrid archi-
tecture achieve promising results.
1 Introduction
Recent work on statistical machine translation has
led to significant progress in coverage and quality of
translation technology, but so far, most of this work
focuses on translation into English, where relatively
simple morphological structure and abundance of
monolingual training data helped to compensate for
the relative lack of linguistic sophistication of the
underlying models. As SMT systems are trained on
massive amounts of data, they are typically quite
good at capturing implicit knowledge contained in
co-occurrence statistics, which can serve as a shal-
low replacement for the world knowledge that would
be required for the resolution of ambiguities and the
insertion of information that happens to be missing
in the source text but is required to generate well-
formed text in the target language.
Already before, decades of work went into the im-
plementation of MT systems (typically rule-based)
for frequently used language pairs1, and these sys-
tems quite often contain a wealth of linguistic
knowledge about the languages involved, such as
fairly complete mechanisms for morphological and
syntactic analysis and generation, as well as a large
number of bilingual lexical entries spanning many
application domains.
It is an interesting challenge to combine the differ-
ent types of knowledge into integrated systems that
could then exploit both explicit linguistic knowledge
contained in the rules of one or several conventional
MT system(s) and implicit knowledge that can be
extracted from large amounts of text.
The recently started EuroMatrix2 project will ex-
plore this integration of rule-based and statistical
knowledge sources, and one of the approaches to
be investigated is the combination of existing rule-
based MT systems into a multi-engine architecture.
The work described in this paper is one of the
first incarnations of such a multi-engine architec-
ture within the project, and a careful analysis of the
results will guide us in the choice of further steps
within the project.
2 Architectures for multi-engine MT
Combinations of MT systems into multi-engine ar-
chitectures have a long tradition, starting perhaps
with (Frederking and Nirenburg, 1994). Multi-
engine systems can be roughly divided into simple
1See (Hutchins et al, 2006) for a list of commercial MT
systems
2See http://www.euromatrix.net
193
Figure 1: Architecture for multi-engine MT driven
by a SMT decoder
architectures that try to select the best output from a
number of systems, but leave the individual hypothe-
ses as is (Tidhar and Ku?ssner, 2000; Akiba et al,
2001; Callison-Burch and Flournoy, 2001; Akiba et
al., 2002; Nomoto, 2004; Eisele, 2005) and more so-
phisticated setups that try to recombine the best parts
from multiple hypotheses into a new utterance that
can be better than the best of the given candidates,
as described in (Rayner and Carter, 1997; Hogan and
Frederking, 1998; Bangalore et al, 2001; Jayaraman
and Lavie, 2005; Matusov et al, 2006; Rosti et al,
2007).
Recombining multiple MT results requires find-
ing the correspondences between alternative render-
ings of a source-language expression proposed by
different MT systems. This is generally not straight-
forward, as different word order and errors in the
output can make it hard to identify the alignment.
Still, we assume that a good way to combine the var-
ious MT outcomes will need to involve word align-
ment between the MT output and the given source
text, and hence a specialized module for word align-
ment is a central component of our setup.
Additionally, a recombination system needs a way
to pick the best combination of alternative building
blocks; and when judging the quality of a particu-
lar configuration, both the plausibility of the build-
ing blocks as such and their relation to the context
need to be taken into account. The required opti-
mization process is very similar to the search in a
SMT decoder that looks for naturally sounding com-
binations of highly probable partial translations. In-
stead of implementing a special-purpose search pro-
cedure from scratch, we transform the information
contained in the MT output into a form that is suit-
able as input for an existing SMT decoder. This has
the additional advantage that resources used in stan-
dard phrase-based SMT can be flexibly combined
with the material extracted from the rule-based MT
results; the optimal combination can essentially be
reduced to the task of finding good relative weights
for the various phrase table entries.
A sketch of the overall architecture is given in
Fig. 1, where the blue (light) parts represent the
modules and data sets used in purely statistical MT,
and the red (dark) parts are the additional modules
and data sets derived from the rule-based engines. It
should be noted that this is by far not the only way
to combine systems. In particular, as this proposed
setup gives the last word to the SMT decoder, we
risk that linguistically well-formed constructs from
one of the rule-based engines will be deteriorated in
the final decoding step. Alternative architectures are
under exploration and will be described elsewhere.
3 MT systems and other knowledge
sources
For the experiments, we used a set of six rule-based
MT engines that are partly available via web inter-
faces and partly installed locally. The web based
systems are provided by Google (based on Systran
for the relevant language pairs), SDL, and ProMT
which all deliver significantly different output. Lo-
cally installed systems are OpenLogos, Lucy (a re-
cent offspring of METAL), and translate pro by lin-
genio (only for German? English). In addition to
these engines, we also used the scripts included in
the Moses toolkit (Koehn et al, 2006)3 to generate
phrase tables from the training data. We enhanced
the phrase tables with information on whether a
given pair of phrases can also be derived via a third,
intermediate language. We assume that this can be
useful to distinguish different degrees of reliability,
but due to lack of time for fine-tuning we could not
yet show that it indeed helps in increasing the overall
quality of the output.
3see http://www.statmt.org/moses/
194
4 Implementation Details
4.1 Alignment of MT output
The input text and the output text of the MT systems
was aligned by means of GIZA++ (Och and Ney,
2003), a tool with which statistical models for align-
ment of parallel texts can be trained. Since training
new models on merely short texts does not yield very
accurate results, we applied a method where text can
be aligned based on existing models that have been
trained on the Europarl Corpus (Koehn, 2005) be-
forehand. This was achieved by using a modified
version of GIZA++ that is able to load given mod-
els.
The modified version of GIZA++ is embedded
into a client-server setup. The user can send two
corresponding files to the server, and specify two
models for both translation directions from which
alignments should be generated. After generating
alignments in both directions (by running GIZA++
twice), the system also delivers a combination of
these alignments which then serves as input to the
following steps described below.
4.2 Phrase tables from MT output
We then concatenated the phrase tables from the
SMT baseline system and the phrase tables obtained
from the rule-based MT systems and augmented
them by additional columns, one for each system
used. With this additional information it is clear
which of the MT systems a phrase pair stems from,
enabling us to assign relative weights to the con-
tributions of the different systems. The optimal
weights for the different columns can then be as-
signed with the help of minimum error rate training
(Och, 2003).
5 Results
We compared the hybrid system to a purely statis-
tical baseline system as well as two rule-based sys-
tems. The only differences between the baseline sys-
tem and our hybrid system are the phrase table ? the
hybrid system includes more lexical entries than the
baseline ? and the weights obtained from minimum
error rate training.
For a statistical system, lexical coverage becomes
an obstacle ? especially when the bilingual lexical
entries are trained on documents from different do-
mains. However, due to the distinct mechanisms
used to generate these entries, rule-based systems
and statistical systems usually differ in coverage.
Our system managed to utilize lexical entries from
various sources by integrating the phrase tables de-
rived from rule-based systems into the phrase table
trained on a large parallel corpus. Table 1 shows
Systems Token #
Ref. 2091 (4.21%)
R-I 3886 (7.02%)
R-II 3508 (6.30%)
SMT 3976 (7.91%)
Hybrid 2425 (5.59%)
Table 1: Untranslated tokens (excl. numbers and
punctuations) in output for news commentary task
(de-en) from different systems
a rough estimation of the number of untranslated
words in the respective output of different systems.
The estimation was done by counting ?words? (i.e.
tokens excluding numbers and punctuations) that ap-
pear in both the source document and the outputs.
Note that, as we are investigating translations from
German to English, where the languages share a lot
of vocabulary, e.g. named entities such as ?USA?,
there are around 4.21% of words that should stay the
same throughout the translation process. In the hy-
brid system, 5.59% of the words remain unchanged,
which is is the lowest percentage among all systems.
Our baseline system (SMT in Table 1), not compris-
ing additional phrase tables, was the one to produce
the highest number of such untranslated words.
Baseline Hybrid
test 18.07 21.39
nc-test 21.17 22.86
Table 2: Performance comparison (BLEU scores)
between baseline and hybrid systems, on in-domain
(test) and out-of-domain (nc-test) test data
Higher lexical coverage leads to better perfor-
mance as can be seen in Table 2, which compares
BLEU scores of the baseline and hybrid systems,
both measured on in-domain and out-of-domain test
data. Due to time constraints these numbers reflect
195
results from using a single RBMT system (Lucy);
using more systems would potentially further im-
prove results.
6 Outlook
Due to lack of time for fine-tuning the parameters
and technical difficulties in the last days before de-
livery, the results submitted for the shared task do
not yet show the full potential of our architecture.
The architecture described here places a strong
emphasis on the statistical models and can be seen
as a variant of SMT where lexical information from
rule-based engines is used to increase lexical cover-
age. We are currently also exploring setups where
statistical alignments are fed into a rule-based sys-
tem, which has the advantage that well-formed syn-
tactic structures generated via linguistic rules can-
not be broken apart by the SMT components. But
as rule-based systems typically lack mechanisms for
ruling out implausible results, they cannot easily
cope with errors that creep into the lexicon due to
misalignments and similar problems.
7 Acknowledgements
This research has been supported by the European
Commission in the FP6-IST project EuroMatrix. We
also want to thank Teresa Herrmann for helping us
with the Lucy system.
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using multiple edit distances to automatically
rank machine translation output. In Proceedings of MT
Summit VIII, Santiago de Compostela, Spain.
Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita.
2002. Using language and translation models to select
the best among outputs from multiple mt systems. In
COLING.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In ASRU, Italy.
Chris Callison-Burch and Raymond S. Flournoy. 2001.
A program for automatically selecting the best output
from multiple machine translation engines. In Proc. of
MT Summit VIII, Santiago de Compostela, Spain.
Andreas Eisele. 2005. First steps towards multi-engine
machine translation. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts, June.
Robert E. Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In ANLP, pages 95?100.
Christopher Hogan and Robert E. Frederking. 1998. An
evaluation of the multi-engine MT architecture. In
Proceedings of AMTA, pages 113?123.
John Hutchins, Walter Hartmann, and Etsuo Ito. 2006.
IAMT compendium of translation software. Twelfth
Edition, January.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. of EAMT, Budapest, Hungary.
P. Koehn, M. Federico, W. Shen, N. Bertoldi, O. Bo-
jar, C. Callison-Burch, B. Cowan, C. Dyer, H. Hoang,
R. Zens, A. Constantin, C. C. Moran, and E. Herbst.
2006. Open source toolkit for statistical machine trans-
lation: Factored translation models and confusion net-
work decoding. Final Report of the 2006 JHU Summer
Workshop.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the MT
Summit.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In In Proc. EACL, pages 33?40.
Tadashi Nomoto. 2004. Multi-engine machine translation
with voted language model. In Proc. of ACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
Sapporo, Japan, July.
Manny Rayner and David M. Carter. 1997. Hybrid lan-
guage processing in the spoken language translator. In
Proc. ICASSP ?97, pages 107?110, Munich, Germany.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining translations from multiple machine
translation systems. In Proceedings of the Conference
on Human Language Technology and North American
chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL?2007), pages 228?
235, Rochester, NY, April 22-27.
Dan Tidhar and Uwe Ku?ssner. 2000. Learning to select a
good translation. In COLING, pages 843?849.
196
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 328?337,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Dynamic Topic Adaptation for Phrase-based MT
Eva Hasler
1
, Phil Blunsom
2
, Philipp Koehn
1
, Barry Haddow
1
1
School of Informatics, University of Edinburgh
2
Dept. of Computer Science, University of Oxford
Abstract
Translating text from diverse sources
poses a challenge to current machine
translation systems which are rarely
adapted to structure beyond corpus level.
We explore topic adaptation on a diverse
data set and present a new bilingual vari-
ant of Latent Dirichlet Allocation to com-
pute topic-adapted, probabilistic phrase
translation features. We dynamically in-
fer document-specific translation proba-
bilities for test sets of unknown origin,
thereby capturing the effects of document
context on phrase translations. We show
gains of up to 1.26 BLEU over the base-
line and 1.04 over a domain adaptation
benchmark. We further provide an anal-
ysis of the domain-specific data and show
additive gains of our model in combination
with other types of topic-adapted features.
1 Introduction
In statistical machine translation (SMT), there has
been a lot of interest in trying to incorporate in-
formation about the provenance of training exam-
ples in order to improve translations for specific
target domains. A popular approach are mixture
models (Foster and Kuhn, 2007) where each com-
ponent contains data from a specific genre or do-
main. Mixture models can be trained for cross-
domain adaption when the target domain is known
or for dynamic adaptation when the target domain
is inferred from the source text under translation.
More recent domain adaptation methods employ
corpus or instance weights to promote relevant
training examples (Matsoukas et al., 2009; Fos-
ter et al., 2010) or do more radical data selection
based on language model perplexity (Axelrod et
al., 2011). In this work, we are interested in the
dynamic adaptation case, which is challenging be-
cause we cannot tune our model towards any spe-
cific domain.
In previous literature, domains have often been
loosely defined in terms of corpora, for exam-
ple, news texts would be defined as belonging to
the news domain, ignoring the specific content of
news documents. It is often assumed that the data
within a domain is homogeneous in terms of style
and vocabulary, though that is not always true in
practice. The term topic on the other hand can
describe the thematic content of a document (e.g.
politics, economy, medicine) or a latent cluster in a
topic model. Topic modelling for machine transla-
tion aims to find a match between thematic context
and topic clusters. We view topic adaptation as
fine-grained domain adaptation with the implicit
assumption that there can be multiple distributions
over translations within the same data set. If these
distributions overlap, then we expect topic adapta-
tion to help separate them and yield better trans-
lations than an unadapted system. Topics can be
of varying granularity and are therefore a flexi-
ble means to structure data that is not uniform
enough to be modelled in its entirety. In recent
years there have been several attempts to integrat-
ing topical information into SMT either by learn-
ing better word alignments (Zhao and Xing, 2006),
by adapting translation features cross-domain (Su
et al., 2012), or by dynamically adapting lexical
weights (Eidelman et al., 2012) or adding sparse
topic features (Hasler et al., 2012).
We take a new approach to topic adaptation by
estimating probabilistic phrase translation features
in a completely Bayesian fashion. The motivation
is that automatically identifying topics in the train-
ing data can help to select the appropriate transla-
tion of a source phrase in the context of a docu-
ment. By adapting a system to automatically in-
duced topics we do not have to trust data from a
given domain to be uniform. We also overcome
the problem of defining the level of granularity for
domain adaptation. With more and more training
data automatically extracted from the web and lit-
tle knowledge about its content, we believe this
is an important area to focus on. Translation of
web sites is already a popular application for MT
systems and could be helped by dynamic model
adaptation. We present results on a mixed data
set of the TED corpus, parts of the Commoncrawl
corpus which contains crawled web data and parts
of the News Commentary corpus which contains
328
Figure 1: Phrasal LDA model for inference on
training data.
documents about politics and economics. We be-
lieve that the broad range of this data set makes it a
suitable testbed for topic adaptation. We focus on
translation model adaptation to learn how words
and phrases translate in a given document-context
without knowing the origin of the document. By
learning translations over latent topics and com-
bining several topic-adapted features we achieve
improvements of more than 1 BLEU point.
2 Bilingual topic models over phrase
pairs
Our model is based on LDA and infers topics
as distributions over phrase pairs instead of over
words. It is specific to machine translation in that
the conditional dependencies between source and
target phrases are modelled explicitly, and there-
fore we refer to it as phrasal LDA. Topic distribu-
tions learned on a training corpus are carried over
to tuning and test sets by running a modified in-
ference algorithm on the source side text of those
sets. Translation probabilities are adapted sepa-
rately to each source text under translation which
makes this a dynamic topic adaptation approach.
In the following we explain our approach to topic
modelling with the objective of estimating better
phrase translation probabilities for data sets that
exhibit a heterogeneous structure in terms of vo-
cabulary and style. The advantage from a mod-
elling point of view is that unlike with mixture
models, we avoid sparsity problems that would
arise if we treated documents or sets of documents
as domains and learned separate models for them.
2.1 Latent Dirichlet Allocation (LDA)
LDA is a generative model that learns latent top-
ics in a document collection. In the original
formulation, topics are multinomial distributions
over words of the vocabulary and each docu-
ment is assigned a multinomial distribution over
topics (Blei et al., 2003). Our goal is to learn
topic-dependent phrase translation probabilities
and hence we modify this formulation by replac-
ing words with phrase pairs. This is straightfor-
ward when both source and target phrases are ob-
served but requires a modified inference approach
when only source phrases are observed in an un-
known test set. Different from standard LDA and
previous uses of LDA for MT, we define a bilin-
gual topic model that learns topic distributions
over phrase pairs. This allows us to model the
units of interest in a more principled way, without
the need to map per-word or per-sentence topics to
phrase pairs. Figure 1 shows a graphical represen-
tation of the following generative process.
For each of N documents in the collection
1. Choose topic distribution ?
d
? Dirichlet(?).
2. Choose the number of phrases pairs P
d
in the
document, P
d
? Poisson(?).
3. For every position d
i
in the document corre-
sponding to a phrase pair p
d,i
of source and
target phrase s
i
and t
i
1
:
(a) Choose a topic z
d,i
?Multinomial(?
d
).
(b) Conditioned on topic z
d,i
, choose a
source phrase s
d,i
?Multinomial(?
z
d,i
).
(c) Conditioned on z
d,i
and s
d,i
, choose tar-
get phrase t
d,i
?Multinomial(?
s
d,i
,z
d,i
).
?, ? and ? are parameters of the Dirichlet dis-
tributions, which are asymmetric for k = 0. Our
inference algorithm is an implementation of col-
lapsed variational Bayes (CVB), with a first-order
Gaussian approximation (Teh et al., 2006). It has
been shown to be more accurate than standard VB
and to converge faster than collapsed Gibbs sam-
pling (Teh et al., 2006; Wang and Blunsom, 2013),
with little loss in accuracy. Because we have to
do inference over a large number of phrase pairs,
CVB is more practical than Gibbs sampling.
2.2 Overview of training strategy
Ultimately, we want to learn translation probabil-
ities for all possible phrase pairs that apply to a
given test document during decoding. Therefore,
topic modelling operates on phrase pairs as they
will be seen during decoding. Given word-aligned
parallel corpora from several domains, we extract
lists of per-document phrase pairs produced by the
extraction algorithm in the Moses toolkit (Koehn
et al., 2007) which contain all phrase pairs consis-
tent with the word alignment. We run CVB on the
set of all training documents to learn latent topics
without providing information about the domains.
1
Parallel documents are modelled as bags of phrase pairs.
329
Using the trained model, CVB with modified in-
ference is run on all test documents with the set of
possible phrase translations that a decoder would
load from a phrase table before decoding. When
test inference has finished, we compute adapted
translation probabilities at the document-level by
marginalising over topics for each phrase pair.
3 Bilingual topic inference
3.1 Inference on training documents
The aim of inference on the training data is to
find latent topics in the distributions over phrase
pairs in each document.This is done by repeatedly
visiting all phrase pair positions in all documents,
computing conditional topic probabilities and up-
dating counts. To bias the model to cluster stop
word phrases in one topic, we place an asymmet-
ric prior over the hyperparameters
2
as described in
(Wallach et al., 2009) to make one of the topics a
priori more probable in every document. We use
a fixed-point update (Minka, 2012) to update the
hyperparameters after every iteration. For CVB
the conditional probability of topic z
d,i
given the
current state of all variables except z
d,i
is
P(z
d,i
= k|z
?(d,i)
,s, t,d,?,?,?) ?
(E
q?
[n
?(d,i)
.,k,s,t
]+?)
(E
q?
[n
?(d,i)
.,k,s,.
]+T
s
??)
(E
q?
[n
?(d,i)
.,k,s,.
]+ ?)
(E
q?
[n
?(d,i)
.,k,.
]+S ? ?)
?(E
q?
[n
?(d,i)
d,k,.
]+?) (1)
where s and t are all source and target phrases in
the collection. n
?(d,i)
.,k,s,t
, n
?(d,i)
.,k,s,.
and n
?(d,i)
d,k,.
are cooc-
currence counts of topics with phrase pairs, source
phrases and documents respectively. E
q?
is the
expectation under the variational posterior and in
comparison to Gibbs sampling where the posterior
would otherwise look very similar, counts are re-
placed by their means. n
?(d,i)
.,k,.
is a topic occurrence
count, T
s
is the number of possible target phrases
for a given source phrase and S is the total num-
ber of source phrases. By modelling phrase trans-
lation probabilities separately as P(t
i
|s
i
,z
i
= k, ..)
and P(s
i
|z
i
= k, ..), we can put different priors on
these distributions. For example, we want a sparse
distribution over target phrases for a given source
phrase and topic to express our translation prefer-
ence under each topic. The algorithm stops when
the variational posterior has converged for all doc-
uments or after a maximum of 100 iterations.
3.2 Inference on tuning and test documents
To compute translation probabilities for tuning
and test documents where target phrases are not
2
Omitted from the following equations for simplicity.
observed, the variational posterior is adapted as
shown in Equation 2
P(z
d,i
= k, t
i, j
|z
?(d,i)
,s, t
?(d,i)
,d,?,?,?) ?
(E
q?
[n
?(d,i)
.,k,s,t
j
]+?)
(E
q?
[n
?(d,i)
.,k,s,.
]+T
s
??)
(E
q?
[n
?(d,i)
.,k,s,.
]+ ?)
(E
q?
[n
?(d,i)
.,k,.
]+S ? ?)
?(E
q?
[n
?(d,i)
d,k,.
]+?) (2)
which now computes the joint conditional prob-
ability of a topic k and a target phrase t
i, j
, given the
source phrase s
i
and the test document d. There-
fore, the size of the support changes from K to
K ?T
s
. While during training inference we compute
a distribution over topics for each source-target
pair, in test inference we can use the posterior to
marginalise out the topics and get a distribution
over target phrases for each source phrase.
We use the Moses decoder to produce lists of
translation options for each document in the tun-
ing and test sets. These lists comprise all phrase
pairs that will enter the search space at decod-
ing time. By default, only 20 target phrases per
source phrase are loaded from the phrase table,
so in order to allow for new phrase pairs to en-
ter the search space and for translation probabil-
ities to be computed more accurately, we allow
for up to 200 target phrases per source. For each
source sentence, we consider all possible phrase
segmentations and applicable target phrases. Un-
like in training, we do not iterate over all phrase
pairs in the list but over blocks of up to 200 target
phrases for a given source phrase. The algorithm
stops when all marginal translation probabilities
have converged though in practice we stopped ear-
lier to avoid overfitting.
3.3 Phrase translation probabilities
After topic inference on the tuning and test data,
the forward translation probabilities P(t|s,d) are
computed. This is done separately for every doc-
ument d because we are interested in the trans-
lation probabilities that depend on the inferred
topic proportions for a given document. For ev-
ery document, we iterate over source positions p
d,i
and use the current variational posterior to com-
pute P(t
i, j
|s
i
,d) for all possible target phrases by
marginalizing over topics:
P(t
i, j
|s
i
,d) =
?
k
P(z
i
= k, t
i, j
|z
?(d,i)
,s, t
?(d,i)
,d)
This is straightforward because during test in-
ference the variational posterior is normalised to
a distribution over topics and target phrases for
a given source phrase. If a source phrase oc-
curs multiple times in the same document, the
probabilities are averaged over all occurrences.
The inverse translation probabilities can be com-
puted analogously except that in cases where we
330
do not have variational posteriors for a given pair
of source and target phrases, an approximation is
needed. We omit the results here since our exper-
iments so far did not indicate improvements with
the inverse features included.
4 More topic-adapted features
Inspired by previous work on topic adaptation for
SMT, we add three additional topic-adapted fea-
tures to our model. All of these features make
use of the topic mixtures learned by our bilingual
topic model. The first feature is an adapted lexi-
cal weight, similar to the features in the work of
Eidelman et al. (2012). Our feature is different in
that we marginalise over topics to produce a single
adapted feature where v[k] is the k
th
element of a
document topic vector for document d and w(t|s,k)
is a topic-dependent word translation probability:
lex(
?
t|s?,d) =
|t|
?
i
1
{ j|(i, j) ? a}
?
?(i, j)?a
?
k
w(t|s,k) ? v[k]
? ?? ?
w(t|s)
(3)
The second feature is a target unigram feature
similar to the lazy MDI adaptation of Ruiz and
Federico (2012). It includes an additional term
that measures the relevance of a target word w
i
by
comparing its document-specific probability P
doc
to its probability under the asymmetric topic 0:
trgUnigrams
t
=
|t|
?
i=1
f (
P
doc
(w
i
)
P
baseline
(w
i
)
)
? ?? ?
lazy MDI
? f (
P
doc
(w
i
)
P
topic0
(w
i
)
)
? ?? ?
relevance
(4)
f (x) =
2
1+
1
x
, x > 0 (5)
The third feature is a document similarity fea-
ture, similar to the semantic feature described by
Banchs and Costa-juss? (2011):
docSim
t
= max
i
(1? JSD(v
train doc
i
,v
test doc
)) (6)
where v
train_doc
i
and v
test_doc
are document topic
vector of training and test documents. Because
topic 0 captures phrase pairs that are common to
many documents, we exclude it from the topic
vectors before computing similarities.
4.1 Feature combination
We tried integrating the four topic-adapted fea-
tures separately and in all possible combinations.
As we will see in the results section, while all fea-
tures improve over the baseline in isolation, the
adapted translation feature P(t|s,d) is the strongest
feature. For the features that have a counterpart in
the baseline model (p(t|s,d) and lex(t|s,d)), we ex-
perimented with either adding or replacing them in
Data Mixed CC NC TED
Train 354K (6450) 110K 103K 140K
Dev 2453 (39) 818 817 818
Test 5664 (112) 1892 1878 1894
Table 1: Number of sentence pairs and documents
(in brackets) in the French-English data sets. The
training data has 2.7M English words per domain.
the log-linear model. We found that while adding
the features worked well and yielded close to zero
weights for their baseline counterparts after tun-
ing, replacing them yielded better results in com-
bination with the other adapted features. We be-
lieve the reason could be that fewer phrase table
features in total are easier to optimise.
5 Experimental setup
5.1 Data and baselines
Our experiments were carried out on a mixed
data set, containing the TED corpus (Cettolo et
al., 2012), parts of the News Commentary cor-
pus (NC) and parts of the Commoncrawl corpus
(CC) from the WMT13 shared task (Bojar et al.,
2013) as described in Table 1. We were guided
by two constraints in chosing our data set. 1) the
data has document boundaries and the content of
each document is assumed to be topically related,
2) there is some degree of topical variation within
each data set. In order to compare to domain adap-
tation approaches, we chose a setup with data from
different corpora. We want to abstract away from
adaptation effects that concern tuning of length
penalties and language models, so we use a mixed
tuning set containing data from all three domains
and train one language model on the concatenation
of (equally sized) target sides of the training data.
Word alignments are trained on the concatenation
of all training data and fixed for all models.
Our baseline (ALL) is a phrase-based French-
English system trained on the concatenation of
all parallel data. It was built with the Moses
toolkit (Koehn et al., 2007) using the 14 standard
core features including a 5gram language model.
Translation quality is evaluated on a large test set,
using the average feature weights of three optimi-
sation runs with PRO (Hopkins and May, 2011).
We use the mteval-v13a.pl script to compute case-
insensitive BLEU. As domain-aware benchmark
systems, we use the phrase table fill-up method
(FILLUP) of Bisazza et al. (2011) which pre-
serves the translation scores of phrases from the
IN model and the linear mixture models (LIN-
TM) of Sennrich (2012b) (both available in the
Moses toolkit). For both systems, we build sepa-
rate phrase tables for each domain and use a wrap-
per to decode tuning and test sets with domain-
specific tables. Both benchmarks have an advan-
331
Model Mixed CC NC TED
IN 26.77 18.76 29.56 32.47
ALL 26.86 19.61 29.42 31.88
Table 2: BLEU of in-domain and baseline models.
Model Avg JSD Rank1-diff
Ted-IN vs ALL 0.15 10.8%
CC-IN vs ALL 0.17 18.4%
NC-IN vs ALL 0.13 13.3%
Table 3: Average JSD of IN vs. ALL models.
Rank1-diff: % PT entries where preferred transla-
tion changes.
tage over our model because they are aware of do-
main boundaries in the test set. Further, LIN-TM
adapts phrase table features in both translation di-
rections while we only adapt the forward features.
Table 2 shows BLEU scores of the baseline sys-
tem as well as the performance of three in-domain
models (IN) tuned under the same conditions. For
the IN models, every portion of the test set is de-
coded with a domain-specific model. Results on
the test set are broken down by domain but also
reported for the entire test set (mixed). For Ted
and NC, the in-domain models perform better than
ALL, while for CC the all-domain model improves
quite significantly over IN.
5.2 General properties of the data sets
In this section we analyse some internal properties
of our three data sets that are relevant for adapta-
tion. All of the scores were computed on the sets
of source side tokens of the test set which were
limited to contain content words (nouns, verbs, ad-
jectives and adverbs). The test set was tagged with
the French TreeTagger (Schmid, 1994). The top of
Table 3 shows the average Jensen-Shannon diver-
gence (using log
2
, JSD ? [0,1]) of each in-domain
model in comparison to the all-domain model,
which is an indicator of how much the distribu-
tions in the IN model change when adding out-of-
domain data. Likewise, Rank1-diff gives the per-
centage of word tokens in the test set where the
preferred translation according to p(e| f ) changes
between IN and ALL. These are the words that
are most affected by adding data to the IN model.
Both numbers show that for Commoncrawl the IN
and ALL models differ more than in the other two
data sets. According to the JS divergence between
NC-IN and ALL, translation distibutions in the NC
phrase table are most similar to the ALL phrase
table. Table 4 shows the average JSD for each IN
model compared to a model trained on half of its
in-domain data. This score gives an idea of how
diverse a data set is, measured by comparing dis-
tributions over translations for source words in the
test set. According to this score, Commoncrawl
is the most diverse data set and Ted the most uni-
Model Avg JSD
Ted-half vs Ted-full 0.07
CC-half vs CC-full 0.17
NC-half vs NC-full 0.09
Table 4: Average JSD of in-domain models
trained on half vs. all of the data.
form. Note however, that these divergence scores
do not provide information about the relative qual-
ity of the systems under comparison. For CC,
the ALL model yields a much higher BLEU score
than the IN model and it is likely that this is due to
noisy data in the CC corpus. In this case, the high
divergence is likely to mean that distributions are
corrected by out-of-domain data rather than being
shifted away from in-domain distributions.
5.3 Topic-dependent decoding
The phrase translation probabilities and additional
features described in the last two sections are used
as features in the log-linear translation model in
addition to the baseline translation features. When
combining all four adapted features, we replace
P(t|s) and lex(t|s) by their adapted counterparts.
We construct separate phrase tables for each doc-
ument in the development and test sets and use a
wrapper around the decoder to ensure that each in-
put document is paired with a configuration file
pointing to its document-specific translation table.
Documents are decoded in sequence so that only
one phrase table needs to be loaded at a time. Us-
ing the wrapped decoder we can run parameter op-
timisation (PRO) in the usual way to get one set of
tuned weights for all test documents.
6 Results
In this section we present experimental results
with phrasal LDA. We show BLEU scores in com-
parison to a baseline system and two domain-
aware benchmark systems. We also evaluate
the adapted translation distributions by looking at
translation probabilities under specific topics and
inspect translations of ambiguous source words.
6.1 Analyis of bilingual topic models
We experimented with different numbers of top-
ics for phrasal LDA. The diagrams in Figure 2
shows blocks of training and test documents in
each of the three domains for a model with 20 top-
ics. Darker shading means that documents have
a higher proportion of a particular topic in their
document-topic distribution. The first topic is the
one that was affected by the asymmetric prior and
inspecting its most probable phrase pairs showed
that it had ?collected? a large number of stop word
phrases. This explains why it is the topic that
is most shared across documents and domains.
332
Figure 2: Document-topic distributions for train-
ing (top) and test (bottom) documents, grouped by
domain and averaged into blocks for visualisation.
Topic 8 Topic 11
europ?enne? european crise? crisis
politiques? political taux? rate
politique? policy financi?re?financial
int?r?ts? interests mon?taire? monetary
Topic 14 Topic 19
h?tel? hotel web? web
plage? beach utiliser? use
situ?? located logiciel? software
chambres? bedrooms donn?es? data
Figure 3: Frequent phrase pairs in learned topics.
There is quite a clear horizontal separation be-
tween documents of different domains, for exam-
ple, topics 6, 8, 19 occur mostly in Ted, NC and
CC documents respectively. The overall structure
is very similar between training (top) and test (bot-
tom) documents, which shows that test inference
was successful in carrying over the information
learned on training documents. There is also some
degree of topic sharing across domains, for exam-
ple topics 4 and 15 occur in documents of all three
domains. Figure 3 shows examples of latent topics
found during inference on the training data. Topic
8 and 11 seem to be about politics and economy
and occur frequently in documents from the NC
corpus. Topic 14 contains phrases related to ho-
tels and topic 19 is about web and software, both
frequent themes in the CC corpus.
6.2 Comparison according to BLEU
In Table 5 we compare our topic-adapted features
when added separately to the baseline phrase ta-
ble. The inclusion of each feature improves over
the concatenation baseline but the combination
of all four features gives the best overall results.
Though the relative performance differs slightly
for each domain portion in the test set, overall the
adapted lexical weight is the weakest feature and
the adapted translation probability is the strongest
feature. We also performed feature ablation tests
and found that no combination of features was su-
perior to combining all four features. This con-
firms that the gains of each feature lead to additive
improvements in the combined model.
In Table 6 we compare topic-adapted models
Model Mixed CC NC TED
lex(e|f,d) 26.99 19.93 29.34 32.19
trgUnigrams 27.15 19.90 29.54 32.50
docSim 27.22 20.11 29.63 32.40
p(e|f,d) 27.31 20.23 29.52 32.58
All features 27.67 20.40 30.04 33.08
Table 5: BLEU scores of pLDA features (50 top-
ics), separately and combined.
Model Mixed CC NC TED
ALL -26.86 19.61 29.42 31.88
3 topics -26.95 19.83 29.46 32.02
5 topics *27.48 19.98 29.94 33.04
10 topics *27.65 20.34 29.99 33.14
20 topics *27.63 20.39 29.93 33.09
50 topics *27.67 20.40 30.04 33.08
100 topics *27.65 20.54 30.00 32.90
>ALL +0.81 +0.93 +0.62 +1.26
Table 6: BLEU scores of baseline and topic-
adapted systems (pLDA) with all 4 features and
largest improvements over baseline.
with varying numbers of topics to the concatena-
tion baseline. We see a consistent gain on all do-
mains when increasing the number of topics from
three to five and ten topics. This is evidence that
the number of domain labels is in fact smaller
than the number of underlying topics. The opti-
mal number of latent topics varies for each domain
and reflects our insights from section 5.2. The CC
domain was shown to be the most diverse and the
best performance on the CC portion of the test set
is achieved with 100 topics. Likewise, the TED
domain was shown to be least diverse and here
the best performance is achieved with only 10 top-
ics. The best performance on the entire test set is
achieved with 50 topics, which is also the optimal
number of topics for the NC domain. The bot-
ton row of the table indicates the relative improve-
ment of the best topic-adapted model per domain
over the ALL model. Using all four topic-adapted
features yields an improvement of 0.81 BLEU on
the mixed test set. The highest improvement on a
given domain is achieved for TED with an increase
of 1.26 BLEU. The smallest improvement is mea-
sured on the NC domain. This is in line with the
observation that distributions in the NC in-domain
table are most similar to the ALL table, therefore
we would expect the smallest improvement for do-
main or topic adaptation. We used bootstrap re-
sampling (Koehn, 2004) to measure significance
on the mixed test set and marked all statistically
significant results compared to the respective base-
lines with asterisk (*: p? 0.01).
To demonstrate the benefit of topic adaptation
over more standard domain adaptation approaches
for a diverse data set, we show the performance
333
Model Mixed CC NC TED
FILLUP -27.12 19.36 29.78 32.71
LIN-TM -27.24 19.61 29.87 32.73
pLDA *27.67 20.40 30.04 33.08
>FILLUP +0.55 +1.04 +0.26 +0.37
>LIN-TM +0.43 +0.79 +0.17 +0.35
Table 7: Comparison of best pLDA system with
two domain-aware benchmark systems.
Model Mixed CC NC TED
LIN-LM
+ ALL -27.16 19.71 29.77 32.46
+ FILLUP -27.20 19.37 29.84 32.90
+ LIN-TM -27.34 19.59 29.92 33.02
+ pLDA *27.84 20.48 30.03 33.57
>ALL +0.68 +0.77 +0.26 +1.11
Table 8: Combination of all models with addi-
tional LM adaptation (pLDA: 50 topics).
of two state-of-the-art domain-adapted systems in
Table 7. Both FILLUP and LIN-TM improve over
the ALL model on the mixed test set, by 0.26 and
0.38 BLEU respectively. The largest improvement
is on TED while on the CC domain, FILLUP de-
creases in performance and LIN-TM yields no im-
provement either. This shows that relying on in-
domain distributions for adaptation to a noisy and
diverse domain like CC is problematic. The pLDA
model yields the largest improvement over the
domain-adapted systems on the CC test set, with
in increase of 1.04 BLEU over FILLUP and 0.79
over LIN-TM. The improvements on the other two
domains are smaller but consistent.
We also compare the best model from Table 6
to all other models in combination with linearly
interpolated language models (LIN-LM), interpo-
lated separately for each domain. Though the
improvements are slightly smaller than without
adapted language models, there is still a gain over
the concatenation baseline of 0.68 BLEU on the
mixed test set and similar improvements to before
over the benchmarks (on TED the improvements
are actually even larger). Thus, we have shown
that topic-adaptation is effective for test sets of
diverse documents and that we can achieve sub-
stantial improvements even in comparison with
domain-adapted translation and language models.
6.3 Properties of adapted distributions and
topic-specific translations
The first column of Table 9 shows the average en-
tropy of phrase table entries in the adapted models
according to p(t|s,d) versus the all-domain model,
computed over source tokens in the test set that
are content words. The entropy decreases in the
adapted tables in all cases which is an indicator
that the distributions over translations of content
Set Model Avg entropy Avg perplexity
CC
pLDA 3.74 9.21
ALL 3.99 10.13
NC
pLDA 3.42 6.96
ALL 3.82 7.51
TED
pLDA 3.33 9.17
ALL 4.00 9.71
Table 9: Average entropy of translation distribu-
tions and test set perplexity of the adapted model.
r?gime
topic 6 diet = 0.79 diet aids = 0.04
topic 8 regime* = 0.82 rule = 0.05
topic 19 restrictions = 0.53 diplomats = 0.10
noyau
topic 9 nucleus* = 0.89 core = 0.01
topic 11 core* = 0.93 inner = 0.03
topic 19 kernel = 0.58 core = 0.11
d?mon
topic 6 devil = 0.89 demon = 0.07
topic 8 demon* = 0.98 devil = 0.01
topic 19 daemon = 0.95 demon = 0.04
Table 10: The two most probable translations of
r?gime, noyau and d?mon and probabilities under
different latent topics (*: preferred by ALL).
words have become more peaked. The second col-
umn shows the average perplexity of target tokens
in the test set which is a measure of how likely a
model is to produce words in the reference trans-
lation. We use the alignment information between
source and reference and therefore limit our anal-
ysis to pairs of aligned words, but nevertheless
this shows that the adapted translation distribu-
tions model the test set distributions better than the
baseline model. Therefore, the adapted distribu-
tions are not just more peaked but also more often
peaked towards the correct translation.
Table 10 shows examples of ambiguous French
words that have different preferred translations de-
pending on the latent topic. The word r?gime can
be translated as diet, regime and restrictions and
the model has learned that the probability over
translations changes when moving from one topic
to another (preferred translations under the ALL
model are marked with *). For example, the trans-
lation to diet is most probable under topic 6 and
the translation to regime which would occur in
a political context is most probable under topic
8. Topic 6 is most prominent among Ted docu-
ments while topic 8 is found most frequently in
News Commentary documents which have a high
percentage of politically related text. The French
word noyau can be translated to nucleus (physics),
core (generic) and kernel (IT) among other trans-
lations and the topics that exhibit these preferred
translations can be attributed to Ted (which con-
tains many talks about physics), NC and CC (with
334
Src: ?il suffit d??jecter le noyau et d?en ins?rer un autre, comme ce qu?on fait pour le cl?nage.?
BL: ?it is the nucleus eject and insert another, like what we do to the cl?nage.?
pLDA: ?he just eject the nucleus and insert another, like what we do to the cl?nage.? (nucleus = 0.77)
Ref: ?you can just pop out the nucleus and pop in another one, and that?s what you?ve all heard about with cloning.?
Src: ?pourtant ceci obligerait les contribuables des pays de ce noyau ? fournir du capital au sud?
BL: ?but this would force western taxpayers to provide the nucleus of capital in the south?
pLDA: ?but this would force western taxpayers to provide the core of capital in the south? (core = 0.78)
Ref: ?but this would unfairly force taxpayers in the core countries to provide capital to the south?
Src: ?le noyau contient de nombreux pilotes, afin de fonctionner chez la plupart des utilisateurs.?
BL: ?the nucleus contains many drivers, in order to work for most users.?
pLDA: ?the kernel contains many drivers, to work for most users.? (kernel = 0.53)
Ref: ?the precompiled kernel includes a lot of drivers, in order to work for most users.?
Figure 4: pLDA correctly translates noyau in test docs from Ted, NC and CC (adapted probabilities in
brackets). The baseline (nucleus = 0.27, core = 0.27, kernel = 0.23) translates all instances to nucleus.
many IT-related documents). The last example,
d?mon, has three frequent translations in English:
devil, demon and daemon. The last translation
refers to a computer process and would occur in an
IT context. The topic-phrase probabilities reveal
that its mostly likely translation as daemon occurs
under topic 19 which clusters IT-related phrase
pairs and is frequent in the CC corpus. These
examples show that our model can disambiguate
phrase translations using latent topics.
As another motivating example, in Figure 4 we
compare the output of our adapted models to the
output produced by the all-domain baseline for the
word noyau from Table 10. While the ALL base-
line translates each instance of noyau to nucleus,
the adapted model translates each instance differ-
ently depending on the inferred topic mixtures for
each document and always matches the reference
translation. The probabilities in brackets show
that the chosen translations were indeed the most
likely under the respective adapted model. While
the ALL model has a flat distribution over pos-
sible translations, the adapted models are peaked
towards the correct translation. This shows that
topic-specific translation probabilities are neces-
sary when the translation of a word shifts between
topics or domains and that peaked, adapted distri-
butions can lead to more correct translations.
7 Related work
There has been a lot of previous work using topic
information for SMT, most of it using monolin-
gual topic models. For example, Gong and Zhou
(2011) use the topical relevance of a target phrase,
computed using a mapping between source and
target side topics, as an additional feature in de-
coding. Axelrod et al. (2012) build topic-specific
translation models from the TED corpus and se-
lect topic-relevant data from the UN corpus to im-
prove coverage. Su et al. (2012) perform phrase
table adaptation in a setting where only monolin-
gual in-domain data and parallel out-of-domain
data are available. Eidelman et al. (2012) use
topic-dependent lexical weights as features in the
translation model, which is similar to our work
in that topic features are tuned towards useful-
ness of topic information and not towards a tar-
get domain. Hewavitharana et al. (2013) per-
form dynamic adaptation with monolingual top-
ics, encoding topic similarity between a conversa-
tion and training documents in an additional fea-
ture. This is similar to the work of Banchs and
Costa-juss? (2011), both of which inspired our
document similarity feature. Also related is the
work of Sennrich (2012a) who explore mixture-
modelling on unsupervised clusters for domain
adaptation and Chen et al. (2013) who compute
phrase pair features from vector space representa-
tions that capture domain similarity to a develop-
ment set. Both are cross-domain adaptation ap-
proaches, though. Instances of multilingual topic
models outside the field of MT include Boyd-
Graber and Blei (2009; Boyd-Graber and Resnik
(2010) who learn cross-lingual topic correspon-
dences (but do not learn conditional distributions
like our model does). In terms of model structure,
our model is similar to BiTAM (Zhao and Xing,
2006) which is an LDA-style model to learn topic-
based word alignments. The work of Carpuat and
Wu (2007) is similar to ours in spirit, but they pre-
dict the most probable translation in a context at
the token level while our adaptation operates at the
type level of a document.
8 Conclusion
We have presented a novel bilingual topic model
based on LDA and applied it to the task of transla-
tion model adaptation on a diverse French-English
data set. Our model infers topic distributions over
phrase pairs to compute document-specific trans-
lation probabilities and performs dynamic adap-
tation on test documents of unknown origin. We
have shown that our model outperforms a concate-
nation baseline and two domain-adapted bench-
mark systems with BLEU gains of up to 1.26 on
domain-specific test set portions and 0.81 overall.
We have also shown that a combination of topic-
adapted features performs better than each feature
in isolation and that these gains are additive. An
analysis of the data revealed that topic adaptation
compares most favourably to domain adaptation
when the domain in question is rather diverse.
335
Acknowledgements
This work was supported by funding from the
Scottish Informatics and Computer Science Al-
liance (Eva Hasler) and funding from the Eu-
ropean Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 287658
(EU BRIDGE) and grant agreement 288769 (AC-
CEPT). Thanks to Chris Dyer for an initial discus-
sion about the phrasal LDA model.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP. Association
for Computational Linguistics.
Amittai Axelrod, Xiaodong He, Li Deng, Alex Acero,
and Mei-Yuh Hwang. 2012. New methods and
evaluation experiments on translating TED talks in
the IWSLT benchmark. In Proceedings of ICASSP.
IEEE.
Rafael E. Banchs and Marta R. Costa-juss?. 2011. A
semantic feature for statistical machine translation.
In Proceedings of the Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
SSST-5. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus Interpolation Methods for
Phrase-based SMT Adaptation. In Proceedings of
IWSLT.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
JMLR.
Ond
?
rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of WMT 2013. Asso-
ciation for Computational Linguistics.
Jordan Boyd-Graber and David Blei. 2009. Multilin-
gual Topic Models for Unaligned Text. In Proceed-
ings of the Twenty-Fifth Conference on Uncertainty
in Artificial Intelligence. AUAI Press.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
Sentiment Analysis Across Languages: Multilingual
Supervised Latent Dirichlet Allocation. In Proceed-
ings of EMNLP. Association for Computational Lin-
guistics.
Marine Carpuat and Dekai Wu. 2007. How phrase
sense disambiguation outperforms word sense dis-
ambiguation for SMT. In International Conference
on Theoretical and Methodological Issues in MT.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of EAMT.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in SMT. In
Proceedings of ACL. Association for Computational
Linguistics.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of ACL. Associa-
tion for Computational Linguistics.
G. Foster and R. Kuhn. 2007. Mixture-model adapta-
tion for SMT. In Proceedings of WMT. Association
for Computational Linguistics.
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimi-
native instance weighting for domain adaptation in
SMT. In Proceedings of EMNLP. Association for
Computational Linguistics.
Zhengxian Gong and Guodong Zhou. 2011. Employ-
ing topic modeling for SMT. In Proceedings of
IEEE (CSAE), volume 4.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse lexicalised features and topic adaptation for
SMT. In Proceedings of IWSLT.
S. Hewavitharana, D. Mehay, S. Ananthakrishnan, and
P. Natarajan. 2013. Incremental topic-based TM
adaptation for conversational SLT. In Proceedings
of ACL. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for SMT. In ACL 2007: Demo and
poster sessions. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of EMNLP. Association for Computational Linguis-
tics.
S. Matsoukas, A. Rosti, and B. Zhang. 2009. Discrim-
inative corpus weight estimation for MT. In Pro-
ceedings of EMNLP. Association for Computational
Linguistics.
Thomas P Minka. 2012. Estimating a Dirichlet distri-
bution. Technical report.
Nick Ruiz and Marcello Federico. 2012. MDI Adap-
tation for the Lazy: Avoiding Normalization in LM
Adaptation for Lecture Translation. In Proceedings
of IWSLT.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
336
Rico Sennrich. 2012a. Mixture-modeling with unsu-
pervised clusters for domain adaptation in SMT. In
Proceedings of EAMT.
Rico Sennrich. 2012b. Perplexity Minimization for
Translation Model Domain Adaptation in SMT. In
Proceedings of EACL. Association for Computa-
tional Linguistics.
J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong,
and Q. Liu. 2012. Translation model adaptation
for SMT with monolingual topic information. In
Proceedings of ACL. Association for Computational
Linguistics.
Yee Whye Teh, David Newman, and Max Welling.
2006. A collapsed variational Bayesian inference
algorithm for LDA. In Proceedings of NIPS.
Hanna M. Wallach, David M. Mimno, and Andrew Mc-
Callum. 2009. Rethinking LDA: Why priors matter.
In Proceedings of NIPS.
Pengyu Wang and Phil Blunsom. 2013. Collapsed
variational Bayesian inference for Hidden Markov
Models. In AISTATS, volume 31 of JMLR Proceed-
ings, pages 599?607.
Bing Zhao and Eric P. Xing. 2006. Bilingual topic ad-
mixture models for word alignment. In Proceedings
of ACL. Association for Computational Linguistics.
337
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 688?693,
Dublin, Ireland, August 23-24, 2014.
UEdin: Translating L1 Phrases in L2 Context
using Context-Sensitive SMT
Eva Hasler
ILCC, School of Informatics
University of Edinburgh
e.hasler@ed.ac.uk
Abstract
We describe our systems for the SemEval
2014 Task 5: L2 writing assistant where a
system has to find appropriate translations
of L1 segments in a given L2 context. We
participated in three out of four possible
language pairs (English-Spanish, French-
English and Dutch-English) and achieved
the best performance for all our submit-
ted systems according to word-based ac-
curacy. Our models are based on phrase-
based machine translation systems and
combine topical context information and
language model scoring.
1 Introduction
In the past years, the fields of statistical machine
translation (SMT) and word sense disambigua-
tion (WSD) have developed largely in parallel,
with each field organising their own shared tasks
aimed at improving translation quality (Bojar et
al., 2013) and predicting word senses, e.g. Agirre
et al. (2010). Because sense disambiguation is
a central problem in machine translation, there
has been work on integrating WSD classifiers into
MT systems (Carpuat and Wu, 2007a; Carpuat
and Wu, 2007b; Chan et al., 2007). However,
one problem with the direct integration of WSD
techniques into MT has been the mismatch be-
tween word predictions of WSD systems and the
phrase segmentations of MT system. This prob-
lem was adressed in Carpuat and Wu (2007b) by
extending word sense disambiguation to phrase
sense disambiguation. The relation between word
sense distinctions and translation has also been
explored in past SemEval tasks on cross-lingual
word sense disambiguation, where senses are not
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
defined in terms of WordNet senses as in previ-
ous tasks, but instead as translations to another
language (Lefever and Hoste, 2010; Lefever and
Hoste, 2013).
This year?s L2 writing assistant task is simi-
lar to the cross-lingual word sense disambigua-
tion task but differs in the context provided for
disambiguation and the length of the fragments
(source phrases instead of words). While in other
translation and disambiguation tasks the source
language context is given, the L2 writing assis-
tant task assumes a given target language con-
text that constrains the possible translations of L1
fragments. This is interesting from a machine
translation point-of-view because it allows for a
direct comparison with systems that exploit the
target context using a language model. As lan-
guage models have become more and more power-
ful over the years, mostly thanks to increased com-
puting power, new machine translation techniques
are also judged by their ability to improve perfor-
mance over a baseline system with a strong lan-
guage model. Another difference to previous Se-
mEval tasks is the focus on both lexical and gram-
matical forms, while previous tasks have mostly
focused on lexical selection.
2 Translation Model for L1 Fragments in
L2 Context
Our model for translating L1 fragments in L2 con-
text is a phrase-based machine translation system
with an additional context similarity feature. We
aim to resolve lexical ambiguities by taking the
entire topical L2 context of an L1 fragment into
account rather than only relying on the phrasal L1
context. We do not explicitly model the grammat-
icality of target word forms but rather use a stan-
dard 5-gram language model to score target word
sequences. We describe the context similarity fea-
ture in the following section.
688
2.1 Context Similarity Feature
The context similarity feature is derived from the
phrase pair topic model (PPT) described in Hasler
et al. (2014). At training time, this model learns
topic distributions for all phrase pairs in the phrase
table in an unsupervised fashion, using a variant of
Latent Dirichlet Allocation (LDA). The underly-
ing assumption is that all phrase pairs share a set of
global topics of predefined size, thus each phrase
pair is assigned a distribution over the same set of
global topics. This is in contrast to Word Sense In-
duction (WSI) methods which typically learn a set
of topics or senses for each word type, for example
in Lau et al. (2010).
The input to the model are distributional profiles
of words occurring in the context of each phrase
pair, thus, the model learns lower-dimensional
representations of the likely context words of a
phrase pair. While in a normal machine transla-
tion setup the source sentence context is given, it is
straightforward to replace source language words
with target language words as given in the L2 con-
text for each test example. At test time, the topic
model is applied to the given L2 context to infer a
topic distribution of the test context. The topic dis-
tribution of an applicable phrase pair is compared
to the topic distribution of a given test context (de-
fined as all L2 words in the same sentence as the
L1 fragment, excluding stop words) using cosine
similarity.
To adapt the translation system to the context
of each test sentence, the phrase table is filtered
per test sentence and each applicable phrase pair
receives one additional feature that expresses its
topical similarity with the test context. While
the baseline system (the system without similar-
ity feature) translates the entire test set with the
same translation model, the context-sensitive sys-
tem loads an adapted phrase table for each test sen-
tence. While the phrase pair topic model can also
deal with document-level context, here we con-
sider only sentence-level context as no wider con-
text was available. We evaluate three variations of
the context similarity feature on top of a standard
phrase-based MT system:
? 50-topics The cosine similarity according to
the PPT model trained with 50 topics (sub-
mitted as run1)
? mixture:geoAvg The geometric average of
the cosine similarities according to PPT mod-
els trained with 20, 50 and 100 topics (sub-
mitted as run2)
? mixture:max For each source phrase, the co-
sine similarity according to the PPT model
that yields the lowest entropy (out of the
models with 20, 50 and 100 topics) when
converting the similarities into probabilities
(submitted as run3)
2.2 Language Model Scoring of L2 Context
On top of using the words in the L2 context for
computing the similarity feature described above,
we introduce a simple method for using a language
model to score the target sequence that includes
the translated L1 segments and the words to the
left and right of the translated segments. In order
to use the language model scoring implemented in
the Moses decoder, we present the decoder with
an input sentence that contains the L1 fragment as
well as the L2 context with XML markup. While
the L1 fragments are translated without special
treatment, the L2 tokens are passed through un-
translated by specifying the identity translation as
markup. The XML markup also includes reorder-
ing walls to prevent the decoder from reordering
the L2 context. An example input sentence with
markup (French-English) is shown below:
<wall/>les manifesteurs<wall/>
<np translation=?want?>want</np><wall/>
<np translation=?to?>to</np><wall/>
<np translation=?replace?>replace</np><wall/>
<np translation=?the?>the</np><wall/>
<np translation=?government?>government</np><wall/>
<np translation=?.?>.</np><wall/>
3 Experimental Setup
Although the task is defined as building a transla-
tion assistance system rather than a full machine
translation system, we use a standard machine
translation setup to translate L1 phrases. We used
the Moses toolkit (Koehn et al., 2007) to build
phrase-based translation systems for the language
pairs English-Spanish, French-English and Dutch-
English
1
. For preprocessing, we applied punctua-
tion normalisation, truecasing and tokenisation us-
ing the scripts provided with the Moses toolkit.
The model contains the following standard fea-
tures: direct and inverse phrase translation prob-
abilities, lexical weights, word and phrase penalty,
lexicalised reordering and distortion features and
a 5-gram language model with modified Kneser-
Ney smoothing. In addition, we add the context
similarity feature described in Section 2.1.
1
We left out the English-German language pair for time
reasons.
689
Training data English-Spanish French-English Dutch-English
Europarl 1.92M 1.96M 1.95M
News Commentary 192K 181K n/a
TED 157K 159K 145K
News 2.1G 2.1G 2.1G
Commoncrawl 50M 82M -
Table 1: Overview of parallel and monolingual training data (top/bottom, in number of sentences/words).
3.1 Training Data
Most of the training data was taken from the
WMT13 shared task (Bojar et al., 2013), ex-
cept where specified otherwise. For the English-
Spanish and French-English systems, we used par-
allel training data from the Europarl and News
Commentary corpora, as well as the TED corpus
(Cettolo et al., 2012). For Dutch-English, we used
parallel data from the Europarl and TED corpus.
The language models were trained on the target
sides of the parallel data and additional news data
from the years 2007-2012. For English-Spanish
and French-English, we used additional language
model data from the Commoncrawl corpus
2
. Sep-
arate language models were trained for each cor-
pus and interpolated on a development set. An
overview of the training data is shown in Table 1.
3.2 Tuning Model Parameters
The parameters of the baseline MT excluding
the similarity feature were tuned with kbest-mira
(Cherry and Foster, 2012) on mixed development
sets containing the trial data (500 sentence pairs
with XML markup) distributed for the task as well
as development data from the news and TED cor-
pora for the English-Spanish and French-English
systems and development data from the TED cor-
pus for the Dutch-English system. Because the do-
main(s) of the test examples was not known be-
forehand, we aimed for learning model weights
that would generalise across domains by using
rather diverse tuning sets. In total, the develop-
ment sets consisted of 3435, 3705 and 3516 sen-
tence pairs, respectively. We did not tune the
weight of the similarity feature automatically, but
set it to an empirically determined value instead.
3.3 Simulating Ambiguous Development
Data
When developing our systems using the trial data
supplied by the task organisers, we noticed that
2
For the Dutch-English system, the Commoncrawl data
did not seem to improve performance.
Source words Translations
cha??ne chain, string, channel, station
mati`ere matter, material, subject
flux stream, flow, feed
d?emon demon, daemon, devil
r?egime regime, diet, rule
Table 2: Examples of ambiguous source words
and their different translations in the simulated de-
velopment set.
System French-English
Baseline 0.314
+ LM context 0.726
20-topics 0.603
+ LM context 0.845
50-topics 0.674
+ LM context 0.886
100-topics 0.628
+ LM context 0.872
mixture:arithmAvg 0.650
+ LM context 0.869
mixture:geoAvg 0.670
+ LM context 0.883
mixture:max 0.690
+ LM context 0.889
Table 3: Word accuracy (best) on the simulated de-
velopment set for the smaller baseline system and
the systems with added context similarity feature,
with and without language model context.
the context similarity feature did not add much to
the overall performance, which we attributed to
the small number of ambiguous examples in the
trial data. Therefore, we extracted a set of 1076
development instances containing 14 ambiguous
French words and their English translations from
a mixed corpus containing data from the News
Commentary, TED and Commoncrawl corpora as
used in Hasler et al. (2014). Examples of ambigu-
ous source words and their translations in that de-
690
velopment set are shown in Table 2.
Translating the L1 fragments in the simulated
development set using a smaller baseline system
trained on this mixed data set yields the results at
the top of Table 3. Note that even though the in-
stances were extracted from the training set, this
does not affect the translation model since the
L1 fragments contain only the ambiguous source
words and no further source context that could be
memorised.
The bottom part of Table 3 shows the perfor-
mance of the three context similarity features de-
scribed in Section 2.1 plus some further variants
(the models with 20 and 100 topics as well as
the arithmetic average of the cosine similarities
of models trained with 20, 50 and 100 topics).
First, we observe that each of the features clearly
outperforms the baseline system without language
model context. Second, each context similarity
feature together with the language model context
still outperforms the Baseline + LM context. Even
though the gain of the context similarity features
is smaller when the target context is scored with
a language model, the topical context still pro-
vides additional information that improves lexical
choice. We trained versions of the three best mod-
els from Table 3 (in bold) for our submissions on
the official test sets.
4 Results and Discussion
In this section we report the experimental results
of our systems on the official test sets. The re-
sults without scoring the L2 context with a lan-
guage model are shown in Table 4 and including
language model scoring of L2 context in Table 5.
We limit the reported scores to word accuracy and
do not report recall because our systems produce
output for every L1 phrase.
In Table 4, we compare the performance of the
baseline MT system to systems including one of
three variants of the similarity feature as described
in Section 2.1, according to the 1-best transla-
tion (best) as well as the 5-best translations (out-
of-five) in a distinct n-best list. For five out of
the six tasks, at least one of the systems includ-
ing the similiary feature yields better performance
than the baseline system. Only for French-English
best, the baseline system yields the best word ac-
curacy. Among the three variants, 50-topics and
mixture:geoAvg perform slightly better than mix-
ture:max in most cases.
Table 5 shows the results of our submitted runs
Input: There are many ways of cooking
<f>des ?ufs</f> for breakfast.
Reference: There are many ways of cooking
<f>eggs</f> for breakfast.
Input: I loved animals when I was <f>un
enfant</f>.
Reference: I loved animals when I was <f>a
kid<alt>a child</alt></f>.
Figure 1: Examples of official test instances.
(run1-run3) as well as the baseline system, all
with language model scoring of L2 context via
XML markup. The first thing to note in com-
parison to Table 4 is that providing the L2 con-
text for language model scoring yields quite sub-
stantial improvements (0.165, 0.101 and 0.073, re-
spectively). Again, in five out of six cases at least
one of the systems with context similarity feature
performs better than the baseline system. Only for
Spanish-English best, the baseline system yields
higher word accuracy than the three submitted
runs. As before, 50-topics and mixture:geoAvg
perform slightly better than mixture:max, with a
preference for 50-topics. For comparison, we also
show the word accuracies of the 2nd-ranked sys-
tem for both tasks and each language pair. We
note that the distance to the respective runner-up
system is largest for French-English and on aver-
age larger for the out-of-five task than for the best
task.
As a general observation, we can state that
although the similarity feature improves perfor-
mance in most cases, the improvements are small
compared to the improvements achieved by scor-
ing the L2 language model contexts. We suspect
two reasons for this effect: first, we do not explic-
itly model grammaticality of word forms. There-
fore, our system relies on the language model to
choose the best word form for those test examples
that do not contain any lexical ambiguity. Second,
we have noticed that for some of the test exam-
ples, the correct translations do not depend partic-
ularly on words in the L2 context, as shown in Fig-
ure 1 where the most common translations of the
source phrases without context would match the
reference translations. These are cases where we
do not expect much of an improvement in transla-
tion by taking the L2 context into account.
Since in Section 3.3 we have provided evidence
that topical similarity features can improve lexical
choice over simply using a target language model,
we believe that the lower performance of the sim-
ilarity features on the official test set is caused by
691
System English-Spanish French-English Dutch-English
best oof best oof best oof
Baseline 0.674 0.854 0.722 0.884 0.613 0.750
50-topics 0.682 0.860 0.719 0.896 0.616 0.759
mixture:geoAvg 0.677 0.863 0.715 0.896 0.619 0.756
mixture:max 0.679 0.860 0.712 0.887 0.618 0.753
Table 4: Word accuracy (best and out-of-five) of the baseline system and the systems with added context
similarity feature. All systems were run without scoring the language model context.
System English-Spanish French-English Dutch-English
best oof best oof best oof
Baseline + LM context 0.839 0.944 0.823 0.934 0.686 0.809
run1:
50-topics + LM context 0.827 0.946 0.824 0.938 0.692 0.811
run2:
mixture:geoAvg + LM context 0.827 0.944 0.821 0.939 0.688 0.808
run3:
mixture:max + LM context 0.820 0.949 0.816 0.937 0.688 0.808
2nd-ranked systems 0.809
1
0.887
2
0.694
2
0.839
2
0.679
3
0.753
3
Table 5: Word accuracy (best and out-of-five) of all submitted systems (runs 1-3) as well as the baseline
system without the context similarity feature. All systems were run with the language model context
provided via XML input. Systems on 2nd rank:
1
UNAL-run2,
2
CNRC-run1,
3
IUCL-run1
different levels of ambiguity in the simulated de-
velopment set and the official test set. For the
simulated development set, we explicitly selected
ambiguous source words in contexts which trig-
ger multiple different translations, while the offi-
cial test set also contains examples where the fo-
cus is on correct verb forms. It further contains ex-
amples where the baseline system without context
information could easily provide the correct trans-
lation, as shown above. Thus, the performance of
our topical context models should ideally be eval-
uated on test sets that contain a sufficient number
of ambiguous source phrases in order to measure
its ability to improve lexical selection.
Finally, in Figure 2 we show some examples
where the 50-topics system (with LM context)
produced semantically better translations than the
baseline system and where words in the L2 con-
text would have helped in promoting them over the
choice of the baseline system.
5 Conclusion
We have described our systems for the SemEval
2014 Task 5: L2 writing assistant which achieved
the best performance for all submitted language
pairs and both the best and out-of-five tasks. All
Input: Why has Air France authorised <f>les ap-
pareils ?electroniques</f> at take-off?
Baseline: .. <f>the electronics</f> ..
50-topics: .. <f>electronic devices</f> ..
Reference: .. <f>electronic devices</f> ..
Input: This project represents one of the rare ad-
vances in strenghtening <f>les liens</f>
between Brazil and the European Union.
Baseline: .. <f>the links</f> ..
50-topics: .. <f>the ties</f> ..
Reference: .. <f>the ties<alt>relations</alt><alt>
the bonds</alt></f> ..
Figure 2: Examples of improved translation output
with the context similarity feature.
systems are based on phrase-based machine trans-
lation systems with an added context similarity
feature derived from a topic model that learns
topic distributions for phrase pairs. We show that
the additional similarity feature improves perfor-
mance over our baseline models and that further
gains can be achieved by passing the L2 context
through the decoder via XML markup, thereby
producing language model scores of the sequences
of L2 context words and translated L1 fragments.
We also provide evidence that the relative perfor-
mance of the context similarity features depends
on the level of ambiguity in the L1 fragments.
692
Acknowledgements
This work was supported by funding from the
Scottish Informatics and Computer Science Al-
liance and funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement 287658 (EU BRIDGE).
References
Eneko Agirre, Oier Lopez De Lacalle, Christiane Fell-
baum, Maurizio Tesconi, Monica Monachini, Piek
Vossen, and Roxanne Segers. 2010. SemEval-2010
Task 17: All-words Word Sense Disambiguation on
a Specific Domain. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
WMT 2013.
Marine Carpuat and Dekai Wu. 2007a. How Phrase
Sense Disambiguation outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation.
In International Conference on Theoretical and
Methodological Issues in MT.
Marine Carpuat and Dekai Wu. 2007b. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. In Proceedings of EMNLP, pages 61?
72.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web Inventory of Transcribed
and Translated Talks. In Proceedings of EAMT.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Statis-
tical Machine Translation. In Proceedings of ACL.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of NAACL.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2014.
Dynamic Topic Adaptation for SMT using Distribu-
tional Profiles. In Proceedings of the 9th Workshop
on Statistical Machine Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for SMT. In Proceedings of ACL:
Demo and poster sessions.
Jey Han Lau, Paul Cook, Diana Mccarthy, David New-
man, and Timothy Baldwin. 2010. Word Sense In-
duction for Novel Sense Detection. In Proceedings
of EACL.
Els Lefever and Veronique Hoste. 2010. SemEval-
2010 Task 3: Cross-lingual Word Sense Disam-
biguation. In Proceedings of the 5th International
Workshop on Semantic Evaluation.
Els Lefever and Veronique Hoste. 2013. SemEval-
2013 Task 10: Cross-lingual Word Sense Disam-
biguation. In Proceedings of the 7th International
Workshop on Semantic Evaluation, in Conjunction
with the Second Joint Conference on Lexical and
Computational Semantics.
693
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207?214,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Edinburgh?s Syntax-Based Systems at WMT 2014
Philip Williams
1
, Rico Sennrich
1
, Maria Nadejde
1
,
Matthias Huck
1
, Eva Hasler
1
, Philipp Koehn
1,2
1
School of Informatics, University of Edinburgh
2
Center for Speech and Language Processing, The Johns Hopkins University
Abstract
This paper describes the string-to-tree sys-
tems built at the University of Edin-
burgh for the WMT 2014 shared trans-
lation task. We developed systems for
English-German, Czech-English, French-
English, German-English, Hindi-English,
and Russian-English. This year we
improved our English-German system
through target-side compound splitting,
morphosyntactic constraints, and refine-
ments to parse tree annotation; we ad-
dressed the out-of-vocabulary problem us-
ing transliteration for Hindi and Rus-
sian and using morphological reduction
for Russian; we improved our German-
English system through tree binarization;
and we reduced system development time
by filtering the tuning sets.
1 Introduction
For this year?s WMT shared translation task we
built syntax-based systems for six language pairs:
? English-German ? German-English
? Czech-English ? Hindi-English
? French-English ? Russian-English
As last year (Nadejde et al., 2013), our systems are
based on the string-to-tree pipeline implemented
in the Moses toolkit (Koehn et al., 2007).
We paid particular attention to the production of
grammatical German, trying various parsers and
incorporating target-side compound splitting and
morphosyntactic constraints; for Hindi and Rus-
sian, we employed the new Moses transliteration
model to handle out-of-vocabulary words; and for
German to English, we experimented with tree bi-
narization, obtaining good results from right bina-
rization.
We also present our first syntax-based results
for French-English, the scale of which defeated us
last year. This year we were able to train a sys-
tem using all available training data, a task that
was made considerably easier through principled
filtering of the tuning set. Although our system
was not ready in time for human evaluation, we
present BLEU scores in this paper.
In addition to the five single-system submis-
sions described here, we also contributed our
English-German and German-English systems for
use in the collaborative EU-BRIDGE system com-
bination effort (Freitag et al., 2014).
This paper is organised as follows. In Sec-
tion 2 we describe the core setup that is com-
mon to all systems. In subsequent sections we de-
scribe language-pair specific variations and exten-
sions. For each language pair, we present results
for both the development test set (newstest2013
in most cases) and for the filtered test set (new-
stest2014) that was provided after the system sub-
mission deadline. We refer to these as ?devtest?
and ?test?, respectively.
2 System Overview
2.1 Pre-processing
The training data was normalized using the WMT
normalize-punctuation.perl script then
tokenized and truecased. Where the target lan-
guage was English, we used the Moses tokenizer?s
-penn option, which uses a tokenization scheme
that more closely matches that of the parser. For
the English-German system we used the default
Moses tokenization scheme, which is similar to
that of the German parsers.
For the systems that translate into English, we
used the Berkeley parser (Petrov et al., 2006;
Petrov and Klein, 2007) to parse the target-side of
the training corpus. As we will describe in Sec-
tion 3, we tried a variety of parsers for German.
We did not perform any corpus filtering other
than the standard Moses method, which removes
207
sentence pairs with dubious length ratios and sen-
tence pairs where parsing fails for the target-side
sentence.
2.2 Translation Model
Our translation grammar is a synchronous context-
free grammar (SCFG) with phrase-structure labels
on the target side and the generic non-terminal la-
bel X on the source side.
The grammar was extracted from the word-
aligned parallel data using the Moses implemen-
tation (Williams and Koehn, 2012) of the GHKM
algorithm (Galley et al., 2004; Galley et al., 2006).
For word alignment we used MGIZA++ (Gao and
Vogel, 2008), a multi-threaded implementation of
GIZA++ (Och and Ney, 2003).
Minimal GHKM rules were composed into
larger rules subject to parameterized restrictions
on size defined in terms of the resulting target tree
fragment. A good choice of parameter settings
depends on the annotation style of the target-side
parse trees. We used the settings shown in Table 1,
which were chosen empirically during the devel-
opment of last years? systems:
Parameter Value
Rule depth 5
Node count 20
Rule size 5
Table 1: Parameter settings for rule composition.
Further to the restrictions on rule composition,
fully non-lexical unary rules were eliminated us-
ing the method described in Chung et al. (2011)
and rules with scope greater than 3 (Hopkins and
Langmead, 2010) were pruned from the trans-
lation grammar. Scope pruning makes parsing
tractable without the need for grammar binariza-
tion.
2.3 Language Model
We used all available monolingual data to train
5-gram language models. Language models
for each monolingual corpus were trained using
the SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) and then interpolated using weights tuned to
minimize perplexity on the development set.
2.4 Feature Functions
Our feature functions are unchanged from the pre-
vious two years. They include the n-gram lan-
guage model probability of the derivation?s target
yield, its word count, and various scores for the
synchronous derivation.
Each grammar rule has a number of pre-
computed scores. For a grammar rule r of the form
C ? ??, ?,??
where C is a target-side non-terminal label, ? is a
string of source terminals and non-terminals, ? is
a string of target terminals and non-terminals, and
? is a one-to-one correspondence between source
and target non-terminals, we score the rule accord-
ing to the following functions:
? p (C, ? | ?,?) and p (? | C, ?,?), the direct
and indirect translation probabilities.
? p
lex
(? | ?) and p
lex
(? | ?), the direct and
indirect lexical weights (Koehn et al., 2003).
? p
pcfg
(pi), the monolingual PCFG probability
of the tree fragment pi from which the rule
was extracted.
? exp(?1/count(r)), a rule rareness penalty.
? exp(1), a rule penalty. The main grammar
and glue grammars have distinct penalty fea-
tures.
2.5 Tuning
The feature weights were tuned using the Moses
implementation of MERT (Och, 2003) for all sys-
tems except English-to-German, for which we
used k-best MIRA (Cherry and Foster, 2012) due
to the larger number of features.
We used tuning sentences drawn from all of
the previous years? test sets (except newstest2013,
which was used as the development test set). In
order to speed up the tuning process, we used sub-
sets of the full tuning sets with sentence pairs up
to length 30 (Max-30) and further applied a fil-
tering technique to reduce the tuning set size to
2,000 sentence pairs for the language pairs involv-
ing German, French and Czech
1
. We also experi-
mented with random subsets of size 2,000.
For the filtering technique, we make the as-
sumption that finding suitable weights for all the
feature functions requires the optimizer to see a
range of feature values and to see hypotheses that
can partially match the reference translations in
order to rank the hypotheses. For example, if a
1
For Russian and Hindi, the development sets are smaller
and no filtering was applied.
208
tuning example contains many out-of-vocabulary
words or is difficult to translate for other reasons,
this will result in low quality translation hypothe-
ses and provide the system with little evidence for
which features are useful to produce good transla-
tions. Therefore, we select high quality examples
using a smooth version of sentence-BLEU com-
puted on the 1-best output of a single decoder run
on the development set. Standard sentence-BLEU
tends to select short examples because they are
more likely to have perfect n-gram matches with
the reference translation. Very short sentence pairs
are less informative for tuning but also tend to have
more extreme source-target length ratios which
can affect the weight of the word penalty. Thus,
we penalize short examples by padding the de-
coder output with a fixed number of non-matching
tokens
2
to the left and right before computing
sentence-BLEU. This has the effect of reducing
the precision of short sentences against the refer-
ence translation while affecting longer sentences
proportionally less. Experiments on phrase-based
systems have shown that the resulting tuning sets
are of comparable diversity as randomly selected
sets in terms of their feature vectors and maintain
BLEU scores in comparison with tuning on the en-
tire development set.
Table 2 shows the size of the full tuning sets
and the size of the subsets with up to length 30,
Table 3 shows the results of tuning with different
sets. Reducing the tuning sets to Max-30 results
in a speed-up in tuning time but affects the per-
formance on some of the devtest/test sets (mostly
for Czech-English). However, tuning on the full
set took more than 18 days using 12 cores for
German-English which is not feasible when try-
ing out several model variations. Further filter-
ing these subsets to a size of 2,000 sentence pairs
as described above maintains the BLEU scores in
most cases and even improves the scores in some
cases. This indicates that the quality of the se-
lected examples is more important than the total
number of tuning examples. However, the exper-
iments with random subsets from Max-30 show
that random selection also yields results which im-
prove over the results with Max-30 in most cases,
though are not always as good as with the filtered
sets.
3
The filtered tuning sets yield reasonable per-
2
These can be arbitrary tokens that do not match any ref-
erence token.
3
For random subsets from the full tuning set the perfor-
mance was similar but resulted in standard deviations of up
formance compared to the full tuning sets except
for the German-English devtest set where perfor-
mance drops by 0.5 BLEU
4
.
Tuning set Cs-En En-De De-En
Full 13,055 13,071 13,071
Max-30 10,392 9,151 10,610
Table 2: Size of full tuning sets and with sentence
length up to 30.
devtest
Tuning set Cs-En En-De De-En
Full 25.1 19.9 26.7
Max-30 24.7 19.8 26.2
Filtered 24.9 19.8 26.2
Random 24.8 19.7 26.4
test
Tuning set Cs-En En-De De-En
Full 27.5 19.2 26.9
Max-30 27.2 19.2 27.0
Filtered 27.5 19.1 27.2
Random 27.3 19.4 27.0
Table 3: BLEU results on devtest and test sets with
different tuning sets: Full, Max-30, filtered subsets
of Max-30 and average of three random subsets of
Max-30 (size of filtered/random subsets: 2,000).
3 English to German
We use the projective output of the dependency
parser ParZu (Sennrich et al., 2013) for the syn-
tactic annotation of our primary submission. Con-
trastive systems were built with other parsers: Bit-
Par (Schmid, 2004), the German Stanford Parser
(Rafferty and Manning, 2008), and the German
Berkeley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
The set of syntactic labels provided by ParZu
has been refined to reduce overgeneralization phe-
nomena. Specifically, we disambiguate the labels
ROOT (used for the root of a sentence, but also
commas, punctuation marks, and sentence frag-
ments), KON and CJ (coordinations of different
constituents), and GMOD (pre- or postmodifying
genitive modifier).
to 0.36 across three random sets.
4
Note however that due to the long tuning times, we are
reporting single tuning runs.
209
NN
SEGMENT
gericht
COMP
JUNC
@s@
SEGMENT
berufung
COMP
JUNC
@es@
SEGMENT
Bund
Figure 1: Syntactic representation of split com-
pound Bundesberufungsgericht (Engl: federal ap-
peals court).
We discriminatively learn non-terminal labels
for unknown words using sparse features, rather
than estimating a probability distribution of non-
terminal labels from singleton statistics in the
training corpus.
We perform target-side compound splitting, us-
ing a hybrid method described by Fritzinger and
Fraser (2010) that combines a finite-state mor-
phology and corpus statistics. As finite-state mor-
phology analyzer, we use Zmorge (Sennrich and
Kunz, 2014). An original contribution of our
experiments is a syntactic representation of split
compounds which eliminates typical problems
with target-side compound splitting, namely er-
roneous reorderings and compound merging. We
represent split compounds as a syntactic tree with
the last segment as head, preceded by a modifier.
A modifier consists of an optional modifier, a seg-
ment and a (possibly empty) joining element. An
example is shown in Figure 1. This hierarchical
representation ensures that compounds can be eas-
ily merged in post-processing (by removing the
spaces and special characters around joining ele-
ments), and that no segments are placed outside of
a compound in the translation.
We use unification-based constraints to model
morphological agreement within German noun
phrases, and between subjects and verbs (Williams
and Koehn, 2011). Additionally, we add con-
straints that operate on the internal tree structure of
the translation hypotheses, to enforce several syn-
tactic constraints that were frequently violated in
the baseline system:
? correct subcategorization of auxiliary/modal
verbs in regards to the inflection of the full
verb.
? passive clauses are not allowed to have ac-
cusative objects.
system
BLEU
devtest test
Stanford Parser 19.0 18.3
Berkeley Parser 19.3 18.6
BitPar 19.5 18.6
ParZu 19.6 19.1
+ modified label set 19.8 19.1
+ discriminative UNK weights 19.9 19.2
+ German compound splitting 20.0 19.8
+ grammatical constraints 20.2 20.1
Table 4: English to German translation results
on devtest (newstest2013) and test (newstest2014)
sets.
? relative clauses must contain a relative (or in-
terrogative) pronoun in their first constituent.
Table 4 shows BLEU scores with systems
trained with different parsers, and for our exten-
sions of the baseline system.
4 Czech to English
For Czech to English we used the core setup de-
scribed in Section 2 without modification. Table 5
shows the BLEU scores.
BLEU
system devtest test
baseline 24.8 27.0
Table 5: Czech to English results on the devtest
(newstest2013) and test (newstest2014) sets.
5 French to English
For French to English, alignment of the parallel
corpus was performed using fast_align (Dyer et
al., 2013) instead of MGIZA++ due to the large
volume of parallel data.
Table 6 shows BLEU scores for the system and
Table 7 shows the resulting grammar sizes after
filtering for the evaluation sets.
BLEU
system devtest test
baseline 29.4 32.3
Table 6: French to English results on the devtest
(newsdev2013) and test (newstest2014) sets.
210
system devtest test
baseline 86,341,766 88,657,327
Table 7: Grammar sizes of the French to En-
glish system after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
6 German to English
German compounds were split using the script
provided with Moses.
For training the primary system, the target parse
trees were restructured before rule extraction by
right binarization. Since binarization strategies
increase the tree depth and number of nodes by
adding virtual non-terminals, we increased the ex-
traction parameters to: Rule Depth = 7, Node
Count = 100, Rule Size = 7. A thorough in-
vestigation of binarization methods for restructur-
ing Penn Treebank style trees was carried out by
Wang et al. (2007).
Table 8 shows BLEU scores for the baseline
system and two systems employing different bi-
narization strategies. Table 9 shows the result-
ing grammar sizes after filtering for the evaluation
sets. Results on the development set showed no
improvement when left binarization was used for
restructuring the trees, although the grammar size
increased significantly.
BLEU
system devtest test
baseline 26.2 27.2
+ right binarization (primary) 26.8 28.2
+ left binarization 26.3 -
Table 8: German to English results on the devtest
(newsdev2013) and test (newstest2014) sets.
system devtest test
baseline 11,462,976 13,811,304
+ right binarization 24,851,982 29,133,910
+ left binarization 21,387,976 -
Table 9: Grammar sizes of the German to En-
glish systems after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
7 Hindi to English
English-Hindi has the least parallel training data
of this year?s language pairs. Out-of-vocabulary
(OOV) input words are therefore a comparatively
large source of translation error: in the devtest set
(newsdev2014) and filtered test set (newstest2014)
the average OOV rates are 1.08 and 1.16 unknown
words per sentence, respectively.
Assuming a significant fraction of OOV words
to be named entities and thus amenable to translit-
eration, we applied the post-processing translitera-
tion method described in Durrani et al. (2014) and
implemented in Moses. In brief, this is an unsuper-
vised method that i) uses EM to induce a corpus of
transliteration examples from the parallel training
data; ii) learns a monotone character-level phrase-
based SMT model from the transliteration corpus;
and iii) substitutes transliterations for OOVs in the
system output by using the monolingual language
model and other features to select between translit-
eration candidates.
5
Table 10 shows BLEU scores with and without
transliteration on the devtest and filtered test sets.
Due to a bug in the submitted system, the language
model trained on the HindEnCorp corpus was used
for transliteration candidate selection rather than
the full interpolated language model. This was
fixed subsequent to submission.
BLEU
system devtest test
baseline 12.9 14.7
+ transliteration (submission) 13.3 15.1
+ transliteration (fixed) 13.6 15.5
Table 10: Hindi to English results with and with-
out transliteration on the devtest (newsdev2014)
and test (newstest2014) sets.
Transliteration increased 1-gram precision from
48.1% to 49.4% for devtest and from 49.1% to
50.6% for test. Of the 2,913 OOV words in test,
938 (32.2%) of transliterations exactly match the
reference. Manual inspection reveals that there are
also many near matches. For instance, translitera-
tion produces Bernat Jackie where the reference is
Jacqui Barnat.
8 Russian to English
Compared to Hindi-English, the Russian-English
language pair has over six times as much parallel
data. Nonetheless, OOVs remain a problem: the
average OOV rates are approximately half those
5
This is the variant referred to as Method 2 in Dur-
rani et al. (2014).
211
of Hindi-English, at 0.47 and 0.51 unknown words
per sentence for the devtest (newstest2013) and fil-
tered test (newstest2014) sets, respectively. We
address this in part using the same transliteration
method as for Hindi-English.
Data sparsity issues for this language pair are
exacerbated by the rich inflectional morphology of
Russian. Many Russian word forms express gram-
matical distinctions that are either absent from En-
glish translations (like grammatical gender) or are
expressed by different means (like grammatical
function being expressed through syntactic config-
uration rather than case). We adopt the widely-
used approach of simplifying morphologically-
complex source forms to remove distinctions that
we believe to be redundant. Our method is simi-
lar to that of Weller et al. (2013) except that ours
is much more conservative (in their experiments,
Weller et al. (2013) found morphological reduc-
tion to harm translation indicating that useful in-
formation was likely to have been discarded).
We used TreeTagger (Schmid, 1994) to obtain
a lemma-tag pair for each Russian word. The tag
specifies the word class and various morphosyn-
tactic feature values. For example, the adjective
??????????????? (?republican?) gets the lemma-
tag pair ??????????????? + Afpfsnf, where
the code A indicates the word class and the re-
maining codes indicate values for the type, degree,
gender, number, case, and definiteness features.
Like Weller et al. (2013), we selectively re-
placed surface forms with their lemmas and re-
duced tags, reducing tags through feature dele-
tion. We restricted morphological reduction to ad-
jectives and verbs, leaving all other word forms
unchanged. Table 11 shows the features that
were deleted. We focused on contextual inflec-
tion, making the assumption that inflectional dis-
tinctions required by agreement alone were the
least likely to be useful for translation (since the
same information was marked elsewhere in the
sentence) and also the most likely to be the source
of ?spurious? variation.
Table 12 shows the BLEU scores for Russian-
English with transliteration and morphological re-
duction. The effect of transliteration was smaller
than for Hindi-English, as might be expected from
the lower baseline OOV rate. 1-gram precision in-
creased from 57.1% to 57.6% for devtest and from
62.9% to 63.6% for test. Morphological reduction
decreased the initial OOV rates by 3.5% and 4.1%
Adjective Verb
Type 7 Type 7
Degree 3 VForm 3
Gender 7 Tense 3
Number 7 Person 3
Case 7 Number 3
Definiteness 7 Gender 7
Voice 3
Definiteness 7
Aspect 3
Case 3
Table 11: Feature values that are retained (3)
or deleted (7) during morphological reduction of
Russian.
BLEU
system devtest test
baseline 23.3 29.7
+ transliteration 23.7 30.3
+ morphological reduction 23.8 30.3
Table 12: Russian to English results on the devtest
(newstest2013) and test (newstest2014) sets.
on the devtest and filtered test sets. After both
morphological and transliteration the 1-gram pre-
cisions for devtest and test were 57.7% and 63.8%.
9 Conclusion
We have described Edinburgh?s syntax-based sys-
tems in the WMT 2014 shared translation task.
Building upon the already-strong string-to-tree
systems developed for previous years? shared
translation tasks, we have achieved substantial im-
provements over our baseline setup: we improved
translation into German through target-side com-
pound splitting, morphosyntactic constraints, and
refinements to parse tree annotation; we have ad-
dressed unknown words using transliteration (for
Hindi and Russian) and morphological reduction
(for Russian); and we have improved our German-
English system through tree binarization.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658 (EU-BRIDGE).
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1_148717.
212
References
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montr?al, Canada, June. Association for
Computational Linguistics.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 413?417, Portland, Oregon, USA, June.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp
Koehn. 2014. Integrating an Unsupervised Translit-
eration Model into Statistical Machine Translation.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the ACL (EACL 2014), Gothenburg,
Sweden, April. To appear.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In In Proc. NAACL/HLT 2013,
pages 644?648.
Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, Teresa Herrmann, Eunah Cho, and Alex
Waibel. 2014. EU-BRIDGE MT: Combined Ma-
chine Translation. In Proceedings of the ACL 2014
Ninth Workshop on Statistical Machine Translation,
Baltimore, MD, USA, June.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 224?234, Uppsala,
Sweden.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In HLT-NAACL ?04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL-
44: Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th an-
nual meeting of the Association for Computational
Linguistics, pages 961?968, Morristown, NJ, USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646?655, Cambridge,
MA, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Morristown, NJ, USA.
Association for Computational Linguistics.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh?s Syntax-Based Machine Transla-
tion Systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 170?
176, Sofia, Bulgaria, August.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, ACL ?03, pages
160?167, Morristown, NJ, USA.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433?440.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
213
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich and Beat Kunz. 2014. Zmorge: A Ger-
man Morphological Lexicon Extracted from Wik-
tionary. In Proceedings of the 9th International
Conference on Language Resources and Evaluation
(LREC 2014), Reykjavik, Iceland, May.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In Intl. Conf. Spoken
Language Processing, Denver, Colorado, September
2002.
Wei Wang, Kevin Knight, Daniel Marcu, and Marina
Rey. 2007. Binarizing Syntax Trees to Improve
Syntax-Based Machine Translation Accuracy. In
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 746?754.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Rich?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions at WMT13: Mor-
phological and syntactic processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 232?239, Sofia, Bul-
garia, August.
Philip Williams and Philipp Koehn. 2011. Agreement
Constraints for Statistical Machine Translation into
German. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 217?226, Ed-
inburgh, Scotland, July.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 388?394, Montr?al,
Canada, June.
214
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 445?456,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Dynamic Topic Adaptation for SMT using Distributional Profiles
Eva Hasler
1
Barry Haddow
1
Philipp Koehn
1,2
1
School of Informatics, University of Edinburgh
2
Center for Language and Speech Processing, Johns Hopkins University
e.hasler@ed.ac.uk, {bhaddow,pkoehn}@inf.ed.ac.uk
Abstract
Despite its potential to improve lexical
selection, most state-of-the-art machine
translation systems take only minimal con-
textual information into account. We cap-
ture context with a topic model over dis-
tributional profiles built from the context
words of each translation unit. Topic dis-
tributions are inferred for each transla-
tion unit and used to adapt the translation
model dynamically to a given test context
by measuring their similarity. We show
that combining information from both lo-
cal and global test contexts helps to im-
prove lexical selection and outperforms a
baseline system by up to 1.15 BLEU. We
test our topic-adapted model on a diverse
data set containing documents from three
different domains and achieve competitive
performance in comparison with two su-
pervised domain-adapted systems.
1 Introduction
The task of lexical selection plays an important
role in statistical machine translation (SMT). It
strongly depends on context and is particularly dif-
ficult when the domain of a test document is un-
known, for example when translating web doc-
uments from diverse sources. Selecting transla-
tions of words or phrases that preserve the sense
of the source words is closely related to the field
of word sense disambiguation (WSD), which has
been studied extensively in the past.
Most approaches to WSD model context at the
sentence level and do not take the wider context
of a word into account. Some of the ideas from
the field of WSD have been adapted for machine
translation (Carpuat and Wu, 2007b; Carpuat and
Wu, 2007a; Chan et al., 2007). For example,
Carpuat and Wu (2007a) extend word sense dis-
ambiguation to phrase sense disambiguation and
show improved performance due to the better fit
with multiple possible segmentations in a phrase-
based system. Carpuat (2009) test the ?one sense
per discourse? hypothesis (Gale et al., 1992) for
MT and find that enforcing it as a constraint at the
document level could potentially improve transla-
tion quality. Our goal is to make correct lexical
choices in a given context without explicitly en-
forcing translation consistency.
More recent work in SMT uses latent repre-
sentations of the document context to dynam-
ically adapt the translation model with either
monolingual topic models (Eidelman et al., 2012;
Hewavitharana et al., 2013) or bilingual topic
models (Hasler et al., 2014), thereby allowing the
translation system to disambiguate source phrases
using document context. Eidelman et al. (2012)
also apply a topic model to each test sentence and
find that sentence context is sufficient for pick-
ing good translations, but they do not attempt to
combine sentence and document level informa-
tion. Sentence-level topic adaptation for SMT has
also been employed by Hasler et al. (2012). Other
approaches to topic adaptation for SMT include
Zhao and Xing (2007) and Tam et al. (2008), both
of which use adapted lexical weights.
In this paper, we present a topic model that
learns latent distributional representations of the
context of a phrase pair which can be applied to
both local and global contexts at test time. We
introduce similarity features that compare latent
representations of phrase pair types to test con-
texts to disambiguate senses for improved lexi-
cal selection. We also propose different strate-
gies for combining local and global topical context
and show that using clues from both levels of con-
texts is beneficial for translation model adaptation.
We evaluate our model on a dynamic adaptation
task where the domain of a test document is un-
known and hence the problem of lexical selection
is harder.
445
2 Related work
Most work in the WSD literature has modelled
disambiguation using a limited window of con-
text around the word to disambiguate. Cai et al.
(2007), Boyd-graber and Blei (2007) and Li et al.
(2010) further tried to integrate the notion of la-
tent topics to address the sparsity problem of the
lexicalised features typically used in WSD classi-
fiers. The most closely related work in the area
of sense disambiguation is by Dinu and Lapata
(2010) who propose a disambiguation method for
solving lexical similarity and substitution tasks.
They measure word similarity in context by learn-
ing distributions over senses for each target word
in the form of lower-dimensional distributional
representations. Before computing word similar-
ities, they contextualise the global sense distribu-
tion of a word using the sense distribution of words
in the test context, thereby shifting the sense distri-
bution towards the test context. We adopt a simi-
lar distributional representation, but argue that our
representation does not need this disambiguation
step because at the level of phrase pairs the ambi-
guity is already much reduced.
Our model performs adaptation using similar-
ity features which is similar to the approach of
Costa-juss`a and Banchs (2010) who learn a vec-
tor space model that captures the source context
of every training sentence. In Banchs and Costa-
juss`a (2011), the vector space model is replaced
with representations inferred by Latent Seman-
tic Indexing. However, because their latent rep-
resentations are learned over training sentences,
they have to compare the current test sentence to
the latent vector of every training instance associ-
ated with a translation unit. The highest similar-
ity value is then used as a feature value. Instead,
our model learns latent distributional representa-
tions of phrase pairs that can be directly compared
to test contexts and are likely to be more robust.
Because context words of a phrase pair are tied to-
gether in the distributional representations, we can
use sparse priors to cluster context words associ-
ated with the same phrase pair into few topics.
Recently, Chen et al. (2013) have proposed a
vector space model for domain adaptation where
phrase pairs are assigned vectors that are defined
in terms of the training corpora. A similar vector
is built for an in-domain development set and the
similarity to the development set is used as a fea-
ture during translation. While their vector repre-
sentations are similar to our latent topic represen-
tations, their model has no notion of structure be-
yond corpus boundaries and is adapted towards a
single target domain (cross-domain). Instead, our
model learns the latent topical structure automati-
cally and the translation model is adapted dynam-
ically to each test instance.
We are not aware of prior work in the field of
MT that investigates combinations of local and
global context. In their recent work on neural lan-
guage models, Huang et al. (2012) combine the
scores of two neural networks modelling the word
embeddings of previous words in a sequence as
well as those of words from the surrounding doc-
ument by averaging over all word embeddings oc-
curring in the same document. The score of the
next word in a sequence is computed as the sum of
the scores of both networks, but they do not con-
sider alternative ways of combining contextual in-
formation.
3 Phrase pair topic model (PPT)
Our proposed model aims to capture the relation-
ship between phrase pairs and source words that
frequently occur in the local context of a phrase
pair, that is, context words occurring in the same
sentence. It therefore follows the distributional
hypothesis (Harris, 1954) which states that words
that occur in the same contexts tend to have sim-
ilar meanings. For a phrase pair, the idea is that
words that occur frequently in its context are in-
dicative of the sense that is captured by the target
phrase translating the source phrase.
We assume that all phrase pairs share a global
set of topics and during topic inference the distri-
bution over topics for each phrase pair is induced
from the latent topic of its context words in the
training data. In order to learn topic distributions
for each phrase pair, we represent phrase pairs as
documents containing all context words from the
source sentence context in the training data. These
distributional profiles of phrase pairs are the in-
put to the topic modelling algorithm which learns
topic clusters over context words.
Figure 1a shows a graphical representation of
the following generative process for training. For
each of P phrase pairs pp
i
in the collection
1. Draw a topic distribution from an asymmetric
Dirichlet prior, ?
p
? Dirichlet(?
0
, ? . . . ?).
2. For each position c in the distributional pro-
file of pp
i
, draw a topic from that distribution,
z
p,c
?Multinomial(?
p
).
446
(a) Inference on phrase pair documents (training).
(b) Inference on local test contexts (test).
Figure 1: Graphical representation of the phrase
pair topic (PPT) model.
3. Conditioned on topic z
p,c
, choose a context
word w
p,c
?Multinomial(?
z
p,c
).
? and ? are parameters of the Dirichlet distribu-
tions and ?
k
denotes topic-dependent vocabularies
over context words. Test contexts are generated
similarly by drawing topic mixtures ?
l
for each test
context
1
as shown in Figure 1b, drawing topics z
for each context position and then drawing context
wordsw for each z. The asymmetric prior on topic
distributions (?
0
for topic 0 and ? for all other top-
ics) encodes the intuition that there are words oc-
curring in the context of many phrase pairs which
1
A local test context is defined as all words in the test
sentence excluding stop words, while contexts of phrase pairs
in training do not include the words belonging to the source
phrase. The naming in the figure refers to local test contexts
L, but global test contexts will be defined similarly.
can be grouped under a topic with higher a priori
probability than the other topics. Figure 1a shows
the model for training inference on the distribu-
tional representations for each phrase pair, where
C
l?all
denotes the number of context words in all
sentence contexts that the phrase pair was seen in
the training data, P denotes the number of phrase
pairs and K denotes the number of latent topics.
The model in Figure 1b has the same structure
but shows inference on test contexts, where C
l
de-
notes the number of context words in the test sen-
tence context and L denotes the number of test in-
stances. ?
p
and ?
l
denote the topic distribution for
a phrase pair and a test context, respectively.
3.1 Inference for PPT model
We use collapsed variational Bayes (Teh et al.,
2006) to infer the parameters of the PPT model.
The posterior distribution over topics is computed
as shown below
P (z
p,c
= k|z
?(p,c)
,w
c
, p, ?, ?) ?
(E
q?
[n
?(p,c)
.,k,w
c
] + ?)
(E
q?
[n
?(p,c)
.,k,.
] +W
c
? ?)
? (E
q?
[n
?(p,c)
d,k,.
] + ?)
(1)
where z
p,c
denotes the topic at position c in
the distributional profile p, w
c
denotes all con-
text word tokens in the collection, W
c
is the total
number of context words and E
q?
is the expecta-
tion under the variational posterior. n
?(p,c)
.,k,w
c
and
n
?(p,c)
p,k,.
are counts of topics occurring with context
words and distributional profiles, respectively, and
n
?(p,c)
.,k,.
is a topic occurrence count.
Before training the topic model, we remove stop
words from all documents. When inferring top-
ics for test contexts, we ignore unseen words be-
cause they do not contribute information for topic
inference. In order to speed up training inference,
we limit the documents in the collection to those
corresponding to phrase pairs that are needed to
translate the test set
2
. Inference was run for 50 it-
erations on the distributional profiles for training
and for 10 iterations on the test contexts. The out-
put of the training inference step is a model file
with all the necessary statistics to compute pos-
terior topic distributions (which are loaded before
running test inference), and the set of topic vectors
for all phrase pairs. The output of test inference is
2
Reducing the training contexts by scaling or sampling
would be expected to speed up inference considerably.
447
the set of induced topic vectors for all test con-
texts.
3.2 Modelling local and global context
At training time, our model has access to context
words only from the local contexts of each
phrase pair in their distributional profiles, that is,
other words in the same source sentence as the
phrase pair. This is useful for reducing noise and
constraining the semantic space that the model
considers for each phrase pair during training. At
test time, however, we are not limited to applying
the model only to the immediate surroundings of
a source phrase to disambiguate its meaning. We
can potentially take any size of test context into
account to disambiguate the possible senses of a
source phrase, but for simplicity we consider two
sizes of context here which we refer to as local
and global context.
Local context Words appearing in the sentence
around a test source phrase, excluding stop words.
Global context Words appearing in the document
around a test source phrase, excluding stop words.
4 Similarity features
We define similarity features that compare the
topic vector ?
p
assigned to a phrase pair
3
to the
topic vector assigned to a test context, The fea-
ture is defined for each source phrase and all its
possible translations in the phrase table, as shown
below
sim(pp
i
, test context) = cosine(?
p
i
, ?
c
),
?pp
i
? {pp
i
|s??
?
t
i
} (2)
Unlike Banchs and Costa-juss`a (2011), we do
not learn topic vectors for every training sentence
which results in a topic vector per phrase pair to-
ken, but instead we learn topic vectors for each
phrase pair type. This is more efficient but also
more appealing from a modelling point of view, as
the topic distributions associated with phrase pairs
can be thought of as expected latent contexts. The
application of the similarity feature is visualised
in Figure 2. On the left, there are two applicable
phrase pairs for the source phrase noyau, noyau
? kernel and noyau? nucleus, with their distri-
butional representations (words belonging to the
3
The mass of topic 0 is removed from the vectors and
the vectors are renormalised before computing similarity fea-
tures.
IT topic versus the scientific topic) and assigned
topic vectors ?
p
. The local and global test contexts
are similarly represented by a document contain-
ing the context words and a resulting topic vector
?
l
or ?
g
. The test context vector ?
c
can be one of
?
l
and ?
g
or a combination of both. In this ex-
ample, the distributional representation of noyau
? kernel has a larger topical overlap with the test
context and will more likely be selected during de-
coding.
Figure 2: Similarity between topic vectors of two
applicable phrase pairs ?
p
and the topic vectors ?
l
and ?
g
from the local and global test context dur-
ing test time.
While this work focuses on exploring vec-
tor space similarity for adaptation, mostly for
computational ease, it may be possible to derive
probabilistic translation features from the PPT
model. This could be a useful addition to the
model and we leave this as an avenue for future
work.
Types of similarity features
We experiment with local and global phrase simi-
larity features, phrSim-local and phrSim-global, to
perform dynamic topic adaptation. These two sim-
ilarity features can be combined by adding them
both to the log-linear SMT model, in which case
each receive separate feature weights. Whenever
we use the + symbol in our results tables, the
additional features were combined with existing
features log-linearly. However, we also experi-
mented with an alternative combination of local
and global information where we combine the lo-
cal and global topic vectors for each test context
before computing similarity features.
4
We were
4
The combined topic vectors were renormalised before
computing their similarities with each candidate phrase pair.
448
motivated by the observation that there are cases
where the local and global features have an op-
posite preference for one translation over another,
but the log-linear combination can only learn a
global preference for one of the features. Com-
bining the topic vectors allows us to potentially
encode a preference for one of the contexts that
depends on each test instance.
For similarity features derived from combined
topic vectors, ? denotes the additive combination
of topic vectors,? denotes the multiplicative com-
bination of topic vectors and~ denotes a combina-
tion that favours the local context for longer sen-
tences and backs off incrementally to the global
context for shorter sentences.
5
The intuition be-
hind this combination is that if there is already suf-
ficient evidence in the local context, the local topic
mixture may be more reliable than the global mix-
ture.
We also experiment with a combination of the
phrase pair similarity features derived from the
PPT model with a document similarity feature
from the pLDA model described in Hasler et al.
(2014). The motivation is that their model learns
topic mixtures for documents and uses phrases in-
stead of words to infer the topical context. There-
fore, it might provide additional information to our
similarity features.
5 Data and experimental setup
Our experiments were carried out on a mixed
French-English data set containing the TED cor-
pus (Cettolo et al., 2012), parts of the News Com-
mentary corpus (NC) and parts of the Common-
crawl corpus (CC) from the WMT13 shared task
(Bojar et al., 2013) as described in Table 1. To
ensure that the baseline model does not have an
implicit preference for any particular domain, we
selected subsets of the NC and CC corpora such
that the training data contains 2.7M English words
per domain. We were guided by two constraints
in chosing our data set in order to simulate an
environment where very diverse documents have
to be translated, which is a typical scenario for
web translation engines: 1) the data has docu-
ment boundaries and the content of each docu-
ment is assumed to be topically related, 2) there is
some degree of topical variation within each data
set. This setup allows us to evaluate our dynamic
5
The interpolation weights between local and global topic
vectors were set proportional to sentence lengths between 1
and 30. The length of longer sentences was clipped to 30.
topic adaptation approach because the test docu-
ments are from different domains and also differ
within each domain, which makes lexical selec-
tion a much harder problem. The topic adaptation
approach does not make use of the domain labels
in training or test, because it infers topic mixtures
in an unsupervised way. However, we compare the
performance of our dynamic approach to domain
adaptation methods by providing them the domain
labels for each document in training and test.
In order to abstract away from adaptation ef-
fects that concern tuning of length penalties and
language models, we use a mixed tuning set con-
taining data from all three domains and train one
language model on the concatenation of the tar-
get sides of the training data. Word alignments
are trained on the concatenation of all training data
and fixed for all models. Table 2 shows the aver-
age length of a document for each domain. While
a CC document contains 29.1 sentences on aver-
age, documents from NC and TED are on average
more than twice as long. The length of a document
could have an influence on how reliable global
topic information is but also on how important it
is to have information from both local and global
test contexts.
Data Mixed CC NC TED
Train 354K (6450) 110K 103K 140K
Dev 2453 (39) 818 817 818
Test 5664 (112) 1892 1878 1894
Table 1: Number of sentence pairs and documents
(in brackets) in the data sets.
Data CC NC TED
Test documents 65 31 24
Avg sentences/doc 29.1 60.6 78.9
Table 2: Average number of sentences per docu-
ment in the test set (per domain).
5.1 Unadapted baseline system
Our baseline is a phrase-based French-English
system trained on the concatenation of all parallel
data. It was built with the Moses toolkit (Koehn
et al., 2007) using the 14 standard core features
including a 5-gram language model. Translation
quality is evaluated on a large test set, using the
average feature weights of three optimisation runs
with PRO (Hopkins and May, 2011). We use the
449
noyau? kernel noyau? nucleus noyau? core
Figure 3: Topic distributions for source phrase noyau and three of its translations (20 topics without topic
0). Colored bars correspond to topics IT, politics, science, economy with topic proportions ?10%.
mteval-v13a.pl script to compute case-insensitive
BLEU scores.
5.2 Domain-adapted benchmark systems
As domain-aware benchmark systems, we use
the linear mixture model (DOMAIN1) of Sen-
nrich (2012) and the phrase table fill-up method
(DOMAIN2) of Bisazza et al. (2011) (both avail-
able in the Moses toolkit). For both systems,
the domain labels of the documents are used to
group documents of the same domain together. We
build adapted tables for each domain by treating
the remaining documents as out-of-domain data
and combining in-domain with out-of-domain ta-
bles. For development and test, the domain labels
are used to select the respective domain-adapted
model for decoding. Both systems have an advan-
tage over our model because of their knowledge
of domain boundaries in the data. This allows for
much more confident lexical choices than using an
unadapted system but is not possible without prior
knowledge about each document.
5.3 Implementation of similarity features
After all topic vectors have been computed, a fea-
ture generation step precomputes the similarity
features for all pairs of test contexts and applica-
ble phrase pairs for translating source phrases in
a test instance. The phrase table of the baseline
model is filtered for every test instance (a sentence
or document, depending on the context setting)
and each entry is augmented with features that ex-
press its semantic similarity to the test context. We
use a wrapper around the Moses decoder to reload
the phrase table for each test instance, which en-
ables us to run parameter optimisation (PRO) in
the usual way to get one set of tuned weights for
all test sentences. It would be conceivable to use
topic-specific weights instead of one set of global
weights, but this is not the focus of this work.
6 Qualitative evaluation of phrase pair
topic distributions
In order to verify that the topic model is learning
useful topic representations for phrase pairs, we
inspect the inferred topic distributions for three
phrase pairs where the translation of the same
source word differs depending on the topical
context: noyau ? kernel, noyau ? nucleus
and noyau ? core. Figure 3 shows the topic
distributions for a PPT model with 20 topics
(with topic 0 removed) and highlights the most
prominent topics with labels describing their
content (politics, IT, science, economy)
6
. The
most peaked topic distribution was learned for
the phrase pair noyau ? kernel which would be
expected to occur mostly in an IT context and
the topic with the largest probability mass is in
fact related to IT. The most prominent topic for
the phrase pair noyau ? nucleus is the science
topic, though it seems to be occurring in with the
political topic as well. The phrase pair noyau
? core was assigned the most ambiguous topic
distribution with peaks at the politics, economy
and IT topics. Note also that its topic distribution
overlaps with those of the other translations, for
example, like the phrase pair noyau ? kernel,
it can occur in IT contexts. This shows that the
model captures the fact that even within a given
topic there can still be ambiguity about the correct
translation (both target phrases kernel and core
are plausible translations in an IT context).
6
Topic labels were assigned by inspecting the most prob-
able context words for each topic according to the model.
450
Ambiguity of phrase pair topic vectors
The examples in the previous section show that
the level of ambiguity differs between phrase pairs
that constitute translations of the same source
phrase. It is worth noting that introducing bilin-
gual information into topic modelling reduces the
sense ambiguity present in monolingual text by
preserving only the intersection of the senses of
source and target phrases. For example, the distri-
butional profiles of the source phrase noyau would
contain words that belong to the senses IT, poli-
tics, science and economy, while the words in the
context of the target phrase kernel can belong to
the senses IT and food (with source context words
such as grain, prot?eines, produire). Thus, the
monolingual representations would still contain a
relatively high level of ambiguity while the distri-
butional profile of the phrase pair noyau? kernel
preserves only the IT sense.
7 Results and discussion
In this section we present experimental results
of our model with different context settings and
against different baselines. We used bootstrap re-
sampling (Koehn, 2004) to measure significance
on the mixed test set and marked all statistically
significant results compared to the respective base-
lines with asterisk (*: p ? 0.01).
7.1 Local context
In Table 3 we compare the results of the con-
catenation baseline and a model containing the
phrSim-local feature in addition to the baseline
features, for different numbers of latent topics. We
show results for the mixed test set containing doc-
uments from all three domains as well as the in-
dividual results on the documents from each do-
main. While all topic settings yield improvements
over the baseline, the largest improvement on the
mixed test set (+0.48 BLEU) is achieved with 50
topics. Topic adaptation is most effective on the
TED portion of the test set where the increase in
BLEU is 0.59.
7.2 Global context
Table 4 shows the results of the baseline plus the
phrSim-global feature that takes into account the
whole document context of a test sentence. While
the largest overall improvement on the mixed test
set is equal to the improvement of the local feature,
there are differences in performance for the indi-
vidual domains. For Commoncrawl documents,
Model Mixed CC NC TED
Baseline -26.86 19.61 29.42 31.88
10 topics *27.15 19.87 29.63 32.36
20 topics *27.19 19.92 29.76 32.31
50 topics *27.34 20.13 29.70 32.47
100 topics *27.26 20.02 29.75 32.40
>Baseline +0.48 +0.52 +0.34 +0.59
Table 3: BLEU scores of baseline system +
phrSim-local feature for different numbers of top-
ics.
the results vary slightly but the largest improve-
ment is still achieved with 50 topics and is al-
most the same for both. For News Commentary,
the scores with the local feature are consistently
higher than the scores with the global feature (0.20
and 0.22 BLEU higher for 20 and 50 topics). For
TED, the trend is opposite with the global feature
performing better than the local feature for all top-
ics (0.28 and 0.40 BLEU higher for 10 and 20 top-
ics). The best improvement over the baseline for
TED is 0.83 BLEU, which is higher than the im-
provement with the local feature.
Model Mixed CC NC TED
Baseline -26.86 19.61 29.42 31.88
10 topics *27.30 20.01 29.61 32.64
20 topics *27.34 20.07 29.56 32.71
50 topics *27.27 20.12 29.48 32.55
100 topics *27.24 19.95 29.66 32.52
>Baseline +0.48 +0.51 +0.24 +0.83
Table 4: BLEU scores of baseline system +
phrSim-global feature for different numbers of
topics.
7.3 Relation to properties of test documents
To make these results more interpretable, Ta-
ble 5 lists some of the properties of the test doc-
uments per domain. Of the three domains, CC
has the shortest documents on average and TED
the longest. To understand how this affects topic
inference, we measure topical drift as the aver-
age divergence (cosine distance) of the local topic
distributions for each test sentence to the global
topic distribution of their surrounding document.
There seems to be a correlation between docu-
ment length and topical drift, with CC documents
showing the least topical drift and TED documents
showing the most. This makes sense intuitively
451
because the longer a document is, the more likely
it is that the content of a given sentence diverges
from the overall topical structure of the document.
While this can explain why for CC documents us-
ing local or global context results in similar perfor-
mance, it does not explain the better performance
of the local feature for NC documents. The last
row of Table 5 shows that sentences in the NC
documents are on average the longest and longer
sentences would be expected to yield more reli-
able topic estimates than shorter sentences. Thus,
we assume that local context yields better perfor-
mance for NC because on average the sentences
are long enough to yield reliable topic estimates.
When local context provides reliable information,
it may be more informative than global context be-
cause it can be more specific.
For TED, we see the largest topical drift per
document, which could lead us to believe that the
document topic mixtures do not reflect the topical
content of the sentences too well. But considering
that the sentences are on average shorter than for
the other two domains, it is more likely that the
local context in TED documents can be unreliable
when the sentences are too short. TED documents
contain transcribed speech and are probably less
dense in terms of information content than News
commentary documents. Therefore, the global
context may be more informative for TED which
could explain why relying on the global topic
mixtures yields better results.
Property CC NC TED
Per document
Avg number of sentences 29.1 60.6 78.9
Avg topical divergence 0.35 0.43 0.49
Avg sentence length 26.2 31.5 21.7
Table 5: Properties of test documents per domain.
Average topical divergence is defined as the aver-
age cosine distance of local to global topic distri-
butions in a document.
7.4 Combinations of local and global context
In Table 6 we compare a system that already con-
tains the global feature from a model with 50 top-
ics to the combinations of local and global simi-
larity features described in Section 4.
Of the four combinations, the additive combi-
nation of topic vectors (?) yields the largest im-
provement over the baseline with +0.63 BLEU on
Model Mixed CC NC TED
Baseline -26.86 19.61 29.42 31.88
+ global -27.27 20.12 29.48 32.55
+ local *27.43 20.18 29.65 32.79
? local *27.49 20.30 29.66 32.76
? local -27.34 20.24 29.61 32.50
~ local *27.45 20.22 29.51 32.79
? >BL +0.63 +0.69 +0.24 +0.88
Table 6: BLEU scores of baseline and combina-
tions of phrase pair similarity features with local
and global context (significance compared to base-
line+global). All models were trained with 50 top-
ics.
the mixed test set and +0.88 BLEU on TED. The
improvements of the combined model are larger
than the improvements for each context on its own,
with the only exception being the NC portion of
the test set where the improvement is not larger
than using just the local context. A possible reason
is that when one feature is consistently better for
one of the domains (local context for NC), the log-
linear combination of both features (tuned on data
from all domains) would result in a weaker overall
model for that domain. However, if both features
encode similar information, as we assume to be the
case for CC documents, the presence of both fea-
tures would reinforce the preference of each and
result in equal or better performance. For the ad-
ditive combination, we expect a similar effect be-
cause adding together two topics vectors that have
peaks at different topics would make the resulting
topic vector less peaked than either of the original
vectors.
The additive topic vector combination is
slightly better than the log-linear feature combina-
tion, though the difference is small. Nevertheless,
it shows that combining topic vectors before com-
puting similarity features is a viable alternative
to log-linear combination, with the potential to
design more expressive combination functions.
The multiplicative combination performs slightly
worse than the additive combination, which
suggests that the information provided by the two
contexts is not always in agreement. In some
cases, the global context may be more reliable
while in other cases the local context may have
more accurate topic estimates and a voting ap-
proach does not take advantage of complementary
information. The combination of topic vectors
452
Source: Le noyau contient de nombreux pilotes, afin de fonctionner chez la plupart des utilisateurs.
Reference: The precompiled kernel includes a lot of drivers, in order to work for most users.
Source: Il est prudent de consulter les pages de manuel ou les faq sp?ecifiques `a votre os.
Reference: It?s best to consult the man pages or faqs for your os.
Source: Nous fournissons nano (un petit ?editeur), vim (vi am?elior?e), qemacs (clone de emacs), elvis, joe .
Reference: Nano (a lightweight editor), vim (vi improved), qemacs (emacs clone), elvis and joe.
Source: Elle a introduit des politiques [..] `a cot?e des relations de gouvernement `a gouvernement traditionnelles.
Reference: She has introduced policies [..] alongside traditional government-to-government relations.
Figure 4: Examples of test sentences and reference translations with the ambiguous source words and
their translations in bold.
depending on sentence length (~) performs well
for CC and TED but less well for NC where we
would expect that it helps to prefer the local
information. This indicates that the rather ad-
hoc way in which we encoded dependency on
the sentence length may need further refinement
to make better use of the local context information.
Model noyau? os?
Baseline nucleus bones
global kernel* os*
local nucleus bones
global?local kernel* os*
Table 7: Translations of ambiguous source words
where global context yields the correct translation
(* denotes the correct translation).
Model elvis? relations?
Baseline elvis* relations*
global the king relationship
local elvis* relations*
global?local the king relations*
Table 8: Translations of ambiguous source words
where local context yields the correct translation
(* denotes the correct translation).
7.5 Effect of contexts on translation
To give an intuition of how lexical selection is af-
fected by contextual information, Figure 4 shows
four test sentences with an ambiguous source word
and its translation in bold. The corresponding
translations with the baseline, the global and lo-
cal similarity features and the additive combina-
tion are shown in Table 7 for the first two examples
where the global context yields the correct transla-
tion (as indicated by *) and in Table 8 for the last
two examples where the local context yields the
correct translation.
7
In Table 7, the additive com-
bination preserves the choice of the global model
and yields the correct translations, while in Table 8
only the second example is translated correctly by
the combined model. A possible explanation is
that the topical signal from the global context is
stronger and results in more discriminative simi-
larity values. In that case, the preference of the
global context would be likely to have a larger in-
fluence on the similarity values in the combined
model. A useful extension could be to try to de-
tect for a given test instance which context pro-
vides more reliable information (beyond encoding
sentence length) and boost the topic distribution
from that context in the combination.
7.6 Comparison with domain adaptation
Table 9 compares the additive model (?) to the
two domain-adapted systems that know the do-
main label of each document during training and
test. Our topic-adapted model yields overall com-
petitive performance with improvements of +0.37
and +0.25 BLEU on the mixed test set, respec-
tively. While it yields slightly lower performance
on the NC documents, it achieves equal perfor-
mance on TED documents and improves by up to
+0.94 BLEU on Commoncrawl documents. This
can be explained by the fact that Commoncrawl is
the most diverse of the three domains with docu-
ments crawled from all over web, thus we expect
topic adaptation to be most effective in compari-
son to domain adaptation in this scenario. Our dy-
namic approach allows us to adapt the similarity
features to each test sentence and test document
individually and is therefore more flexible than
7
For these examples, the local model happens to yield the
same translations as the baseline model.
453
Type of adaptation Model Mixed CC NC TED
Domain-adapted
DOMAIN1 -27.24 19.61 29.87 32.73
DOMAIN2 -27.12 19.36 29.78 32.71
Topic-adapted global ? local *27.49 20.30 29.66 32.76
>DOMAIN1 +0.25 +0.69 -0.21 +0.03
>DOMAIN2 +0.37 +0.94 -0.12 +0.05
Table 9: BLEU scores of translation model using similarity features derived from PPT model (50 topics)
in comparison with two (supervised) domain-adapted systems.
Model Mixed CC NC TED
Baseline -26.86 19.61 29.42 31.88
+ docSim -27.22 20.11 29.63 32.40
+ phrSim-global ? phrSim-local *27.58 20.34 29.71 32.96
+ phrSim-global ~ phrSim-local *27.60 20.35 29.70 33.03
global~local>BL +0.74 +0.74 +0.38 +1.15
Table 10: BLEU scores of baseline, baseline + document similarity feature and additional phrase pair
similarity features (significance compared to baseline+docSim). All models were trained with 50 topics.
cross-domain adaptation approaches while requir-
ing no information about the domain of a test in-
stance.
7.7 Combination with an additional
document similarity feature
To find out whether similarity features derived
from different types of topic models can provide
complementary information, we add the phrSim
features to a system that already includes a docu-
ment similarity feature (docSim) derived from the
pLDA model (Hasler et al., 2014) which learns
topic distributions at the document level and uses
phrases instead of words as the minimal units. The
results are shown in Table 10. Adding the two
best combinations of local and global context from
Table 6 yields the best results on TED documents
with an increase of 0.63 BLEU over the baseline +
docSim model and 1.15 BLEU over the baseline.
On the mixed test set, the improvement is 0.38
BLEU over the baseline + docSim model and 0.74
BLEU over the baseline. Thus, we show that com-
bining different scopes and granularities of sim-
ilarity features consistently improves translation
results and yields larger gains than using each of
the similarity features alone.
8 Conclusion
We have presented a new topic model for dynamic
adaptation of machine translation systems that
learns topic distributions for phrase pairs. These
latent topic representations can be compared to la-
tent representations of local or global test contexts
and integrated into the translation model via simi-
larity features.
Our experimental results show that it is ben-
eficial for adaptation to use contextual informa-
tion from both local and global contexts, with
BLEU improvements of up to 1.15 over the base-
line system on TED documents and 0.74 on a
large mixed test set with documents from three do-
mains. Among four different combinations of lo-
cal and global information, we found that the ad-
ditive combination of topic vectors performs best.
We conclude that information from both contexts
should be combined to correct potential topic de-
tection errors in either of the two contexts. We
also show that our dynamic adaptation approach
performs competitively in comparison with two
supervised domain-adapted systems and that the
largest improvement is achieved for the most di-
verse portion of the test set.
In future work, we would like to experiment
with more compact distributional profiles to speed
up inference and explore the possibilities of de-
riving probabilistic translation features from the
PPT model as an extension to the current model.
Another avenue for future work could be to com-
bine contextual information that captures different
types of information, for example, to distinguish
between semantic and syntactic aspects in the lo-
cal context.
454
Acknowledgements
This work was supported by funding from the
Scottish Informatics and Computer Science Al-
liance (Eva Hasler) and funding from the Eu-
ropean Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 287658
(EU BRIDGE) and grant agreement 288769 (AC-
CEPT). Thanks to Annie Louis for helpful com-
ments on a draft of this paper and thanks to the
anonymous reviewers for their useful feedback.
References
Rafael E Banchs and Marta R Costa-juss`a. 2011. A Se-
mantic Feature for Statistical Machine Translation.
In SSST-5 Proceedings of the Fifth Workshop on Syn-
tax, Semantics and Structure in Statistical Transla-
tion, pages 126?134.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus Interpolation Methods for
Phrase-based SMT Adaptation. In Proceedings of
IWSLT.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
WMT 2013.
Jordan Boyd-graber and David Blei. 2007. A Topic
Model for Word Sense Disambiguation. In Proceed-
ings of EMNLP-CoNLL, pages 1024?1033.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Improving Word Sense Disambiguation Using Topic
Features. In Proceedings of EMNLP, pages 1015?
1023.
Marine Carpuat and Dekai Wu. 2007a. How Phrase
Sense Disambiguation outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation.
In International Conference on Theoretical and
Methodological Issues in MT.
Marine Carpuat and Dekai Wu. 2007b. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. In Proceedings of EMNLP, pages 61?
72.
Marine Carpuat. 2009. One Translation per Discourse.
In Proceedings of the NAACL HLT Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions, pages 19?27.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web Inventory of Transcribed
and Translated Talks. In Proceedings of EAMT.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Statis-
tical Machine Translation. In Proceedings of ACL.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector Space Model for Adaptation in Statistical
Machine Translation. In Proceedings of ACL, pages
1285?1293.
Marta R. Costa-juss`a and Rafael E. Banchs. 2010. A
vector-space dynamic feature for phrase-based sta-
tistical machine translation. Journal of Intelligent
Information Systems, 37(2):139?154, August.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
Distributional Similarity in Context. In Proceedings
of EMNLP, pages 1162?1172.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic Models for Dynamic Trans-
lation Model Adaptation. In Proceedings of ACL.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One Sense Per Discourse. In
Proceedings of the workshop on Speech and Natu-
ral Language.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised Features and Topic Adaptation
for SMT. In Proceedings of IWSLT.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic Topic Adaptation for
Phrase-based MT. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics, Gothenburg, Swe-
den.
Sanjika Hewavitharana, Dennis N Mehay, and
Sankaranarayanan Ananthakrishnan. 2013. Incre-
mental Topic-Based Translation Model Adaptation
for Conversational Spoken Language Translation.
In Proceedings of ACL, pages 697?701.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Edinburgh, United Kingdom.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL, pages
873?882.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for SMT. In Proceedings of ACL:
Demo and poster sessions.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of
EMNLP.
455
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic Models for Word Sense Disambigua-
tion and Token-based Idiom Detection. In Proceed-
ings of ACL, pages 1138?1147.
Rico Sennrich. 2012. Perplexity Minimization for
Translation Model Domain Adaptation in Statistical
Machine Translation. In Proceedings of EACL.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2008.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187?
207, November.
Yee Whye Teh, David Newman, and Max Welling.
2006. A collapsed variational Bayesian inference
algorithm for LDA. In Proceedings of NIPS.
B Zhao and E P Xing. 2007. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation.
Neural Information Processing.
456
