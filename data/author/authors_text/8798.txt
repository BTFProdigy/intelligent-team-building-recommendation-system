Maytag: A multi-staged approach to identifying
complex events in textual data
Conrad Chang, Lisa Ferro, John Gibson, Janet Hitzeman, Suzi Lubar, Justin Palmer,
Sean Munson, Marc Vilain, and Benjamin Wellner
The MITRE Corporation
202 Burlington Rd.
Bedford, MA 01730 USA
contact: mbv@mitre.org (Vilain)
Abstract
We present a novel application of NLP
and text mining to the analysis of finan-
cial documents. In particular, we de-
scribe an implemented prototype, May-
tag, which combines information extrac-
tion and subject classification tools in an
interactive exploratory framework. We
present experimental results on their per-
formance, as tailored to the financial do-
main, and some forward-looking exten-
sions to the approach that enables users
to specify classifications on the fly.
1 Introduction
Our goal is to support the discovery of complex
events in text. By complex events, we mean
events that might be structured out of multiple
occurrences of other events, or that might occur
over a span of time. In financial analysis, the
domain that concerns us here, an example of
what we mean is the problem of understanding
corporate acquisition practices. To gauge a
company?s modus operandi in acquiring other
companies, it isn?t enough to know just that an
acquisition occurred, but it may also be impor-
tant to understand the degree to which it was
debt-leveraged, or whether it was performed
through reciprocal stock exchanges.
In other words, complex events are often
composed of multiple facets beyond the basic
event itself. One of our concerns is therefore to
enable end users to access complex events
through a combination of their possible facets.
Another key characteristic of rich domains
like financial analysis, is that facts and events are
subject to interpretation in context. To a finan-
cial analyst, it makes a difference whether a
multi-million-dollar loss occurs in the context of
recurring operations (a potentially chronic prob-
lem), or in the context of a one-time event, such
as a merger or layoff. A second concern is thus
to enable end users to interpret facts and events
through automated context assessment.
The route we have taken towards this end is to
model the domain of corporate finance through
an interactive suite of language processing tools.
Maytag, our prototype, makes the following
novel contribution. Rather than trying to model
complex events monolithically, we provide a
range of multi-purpose information extraction
and text classification methods, and allow the
end user to combine these interactively. Think
of it as Boolean queries where the query terms
are not keywords but extracted facts, events, en-
tities, and contextual text classifications.
2 The Maytag prototype
Figure 1, below, shows the Maytag prototype
in action. In this instance, the user is browsing a
particular document in the collection, the 2003
securities filings for 3M Corporation. The user
has imposed a context of interpretation by select-
ing the ?Legal matters? subject code, which
causes the browser to only retrieve those portions
of the document that were statistically identified
as pertaining to law suits. The user has also se-
lected retrieval based on extracted facts, in this
case monetary expenses greater than $10 million.
This in turn causes the browser to further restrict
retrieval to those portions of the document that
contain the appropriate linguistic expressions,
e.g., ?$73 million pre-tax charge.?
As the figure shows, the granularity of these
operations in our browser is that of the para-
graph, which strikes a reasonable compromise
between providing enough context to interpret
retrieval results, but not too much. It is also ef-
131
fective at enabling combination of query terms.
Whereas the original document contains 5161
paragraphs, the number of these that were tagged
with the ?Legal matters? code is 27, or .5 percent
of the overall document. Likewise, the query for
expenses greater than $10 million restricts the
return set to 26 paragraphs (.5 percent). The
conjunction of both queries yields a common
intersection of only 4 paragraphs, thus precisely
targeting .07 percent of the overall document.
Under the hood, Maytag consists of both an
on-line component and an off-line one. The on-
line part is a web-based GUI that is connected to
a relational database via CGI scripts (html,
JavaScript, and Python). The off-line part of the
system hosts the bulk of the linguistic and statis-
tical processing that creates document meta-data:
name tagging, relationship extraction, subject
identification, and the like. These processes are
applied to documents entering the text collection,
and the results are stored as meta-data tables.
The tables link the results of the off-line process-
ing to the paragraphs in which they were found,
thereby supporting the kind of extraction- and
classification-based retrieval shown in Figure 1.
3 Extraction in Maytag
As is common practice, Maytag approaches
extraction in stages. We begin with atomic
named entities, and then detect structured
entities, relationships, and events. To do so, we
rely on both rule-based and statistical means.
3.1 Named entities
In Maytag, we currently extract named entities
with a tried-but-true rule-based tagger based on
the legacy Alembic system (Vilain, 1999). Al-
though we?ve also developed more modern sta-
tistical methods (Burger et al 1999, Wellner &
Vilain, 2006), we do not currently have adequate
amounts of hand-marked financial data to train
these systems. We therefore found it more con-
venient to adapt the Alembic name tagger by
manual hill climbing. Because this tagger was
originally designed for a similar newswire task,
we were able to make the port using relatively
small amounts of training data. We relied on two
100+ page-long Securities filings (singly anno-
tated), one for training, and the other for test, on
which we achieve an accuracy of F=94.
We found several characteristics of our finan-
cial data to be especially challenging. The first is
the widespread presence of company name look-
alikes, by which we mean phrases like ?Health
Care Markets? or ?Business Services? that may
look like company names, but in fact denote
business segments or the like. To circumvent
this, we had to explicitly model non-names, in
effect creating a business segment tagger that
captures company name look-alikes and prevents
them from being tagged as companies.
Another challenging characteristic of these fi-
nancial reports is their length, commonly reach-
ing hundreds of pages. This poses a quandary
Figure 1: The Maytag interface
132
for the way we handle discourse effects. As with
most name taggers, we keep a ?found names? list
to compensate for the fact that a name may not
be clearly identified throughout the entire span of
the input text. This list allows the tagger to
propagate a name from clear identifying contexts
to non-identified occurrences elsewhere in the
discourse. In newswire, this strategy boosts re-
call at very little cost to precision, but the sheer
length of financial reports creates a dispropor-
tionate opportunity for found name lists to intro-
duce precision errors, and then propagate them.
3.2 Structured entities, relations, and events
Another way in which financial writing differs
from general news stories is the prevalence of
what we?ve called structured entities, i.e., name-
like entities that have key structural attributes.
The most common of these relate to money. In
financial writing, one doesn?t simply talk of
money: one talks of a loss, gain or expense, of
the business purpose associated therewith, and of
the time period in which it is incurred. Consider:
Worldwide expenses for environmental
compliance [were] $163 million in 2003.
To capture such cases as this, we?ve defined a
repertoire of structured entities. Fine-grained
distinctions about money are encoded as color of
money entities, with such attributes as their color
(in this case, an operating expense), time stamp,
and so forth. We also have structured entities for
expressions of stock shares, assets, and debt.
Finally, we?ve included a number of constructs
that are more properly understood as relations
(job title) or events (acquisitions).
3.3 Statistical training
Because we had no existing methods to address
financial events or relations, we took this oppor-
tunity to develop a trainable approach. Recent
work has begun to address relation and event
extraction through trainable means, chiefly SVM
classification (Zelenko et al 2003, Zhou et al
2005). The approach we?ve used here is classi-
fier-based as well, but relies on maximum en-
tropy modeling instead.
Most trainable approaches to event extraction
are entity-anchored: given a pair of relevant enti-
ties (e.g., a pair of companies), the object of the
endeavor is to identify the relation that holds be-
tween them (e.g., acquisition or subsidiary). We
turn this around: starting with the head of the
relation, we try to find the entities that fill its
constituent roles. This is, unavoidably, a
strongly lexicalized approach. To detect an
event such as a merger or acquisition, we start
from indicative head words, e.g., ?acquire,?
?purchases,? ?acquisition,? and the like.
The process proceeds in two stages. Once
we?ve scanned a text to find instances of our in-
dicator heads, we classify the heads to determine
whether their embedding sentence represents a
valid instance of the target concept. In the case
of acquisitions, this filtering stage eliminates
such non-acquisitions as the use of the word
?purchases? in ?the company purchases raw ma-
terials.? If a head passes this filter, we find the
fillers of its constituent roles through a second
classification stage
The role stage uses a shallow parser to chunk
the sentence, and considers the nominal chunks
and named entities as candidate role fillers. For
acquisition events, for example, these roles in-
clude the object of the acquisition, the buying
agent, the bought assets, the date of acquisition,
and so forth (a total of six roles). E.g.
In the fourth quarter of 2000 (WHEN), 3M
[AGENT] also acquired the multi-layer inte-
grated circuit packaging line [ASSETS] of
W.L. Gore and Associates [OBJECT].
The maximum entropy role classifier relies on
a range of feature types: the semantic type of the
phrase (for named entities), the phrase vocabu-
lary, the distance to the target head, and local
context (words and phrases).
Our initial evaluation of this approach has
given us encouraging first results. Based on a
hand-annotated corpus of acquisition events,
we?ve measured filtering performance at F=79,
and role assignment at F=84 for the critical case
of the object role. A more recent round of ex-
periments has produced considerably higher per-
formance, which we will report on later this year.
4 Subject Classification
Financial events with similar descriptions can
mean different things depending on where these
events appear in a document or in what context
they appear. We attempt to extract this important
contextual information using text classification
methods. We also use text classification methods
to help users to more quickly focus on an area
where interesting transactions exist in an interac-
tive environment. Specifically, we classify each
paragraph in our document collection into one of
several interested financial areas. Examples in-
clude: Accounting Rule Change, Acquisitions
and Mergers, Debt, Derivatives, Legal, etc.
133
4.1 Experiments
In our experiments, we picked 3 corporate an-
nual reports as the training and test document set.
Paragraphs from these 3 documents, which are
from 50 to 150 pages long, were annotated with
the types of financial transactions they are most
related to. Paragraphs that did not fall into a
category of interest were classified as ?other?.
The annotated paragraphs were divided into ran-
dom 4x4 test/training splits for this test. The
?other? category, due to its size, was sub-
sampled to the size of the next-largest category.
As in the work of Nigam et al(2002) or Lodhi
et al(2002), we performed a series of experi-
ments using maximum entropy and support vec-
tor machines. Besides including the words that
appeared in the paragraphs as features, we also
experimented with adding named entity expres-
sions (money, date, location, and organization),
removal of stop words, and stemming. In gen-
eral, each of these variations resulted in little dif-
ference compared with the baseline features con-
sisting of only the words in the paragraphs.
Overall results ranged from F-measures of 70-75
for more frequent categories down to above 30-
40 for categories appearing less frequently.
4.2 Online Learning
We have embedded our text classification
method into an online learning framework that
allows users to select text segments, specify
categories for those segments and subsequently
receive automatically classified paragraphs simi-
lar to those already identified. The highest con-
fidence paragraphs, as determined by the classi-
fier, are presented to the user for verification and
possible re-classification.
Figure 1, at the start of this paper, shows the
way this is implemented in the Maytag interface.
Checkboxes labeled pos and neg are provided
next to each displayed paragraph: by selecting
one or the other of these checkboxes, users indi-
cate whether the paragraph is to be treated as a
positive or a negative example of the category
they are elaborating. In our preliminary studies,
we were able to achieve the peak performance
(the highest F1 score) within the first 20 training
examples using 4 different categories.
5 Discussion and future work
The ability to combine a range of analytic
processing tools, and the ability to explore their
results interactively are the backbone of our ap-
proach. In this paper, we?ve covered the frame-
work of our Maytag prototype, and have looked
under its hood at our extraction and classification
methods, especially as they apply to financial
texts. Much new work is in the offing.
Many experiments are in progress now to as-
sess performance on other text types (financial
news), and to pin down performance on a wider
range of events, relations, and structured entities.
Another question we would like to address is
how best to manage the interaction between clas-
sification and extraction: a mutual feedback
process may well exist here.
We are also concerned with supporting finan-
cial analysis across multiple documents. This
has implications in the area of cross-document
coreference, and is also leading us to investigate
visual ways to define queries that go beyond the
paragraph and span many texts over many years.
Finally, we are hoping to conduct user studies
to validate our fundamental assumption. Indeed,
this work presupposes that interactive application
of multi-purpose classification and extraction
techniques can model complex events as well as
monolithic extraction tools ? laMUC.
Acknowledgements
This research was performed under a MITRE
Corporation sponsored research project.
References
Zhou, G., Su J., Zhang, J., and Zhang, M. 2005. Ex-
ploring various knowledge in relation extraction.
Proc. of the 43rd ACL Conf, Ann Arbor, MI.
Nigam, K., Lafferty, J., and McCallum, A. 1999. Us-
ing maximum entropy for text classification. Proc.
of IJCAI ?99 Workshop on Information Filtering.
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini,
and N., Watkins, C. 2002. Text classification using
string kernels. Journal of Machine Learning Re-
search, Vol. 2, pp. 419-444.
Vilain, M. and Day, D. 1996. Finite-state Phrase Pars-
ing by Rule Sequences, Proc. of COLING-96.
Vilain, M. 1999. Inferential information extraction.
In Pazienza, M.T. & Basili, R., Information Ex-
traction. Springer Verlag.
Wellner, B., and Vilain, M. (2006) Leveraging ma-
chine readable dictionaries in discriminative se-
quence models. Proc. of LREC 2006 (to appear).
Zelenko D., Aone C. and Richardella. 2003. Kernel
methods for relation extraction. Journal of Ma-
chine Learning Research. pp1083-1106.
134
Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
The MITRE Logical Form Generation System
Samuel Bayer and John Burger and Warren Greiff and Ben Wellner
The MITRE Corporation
Bedford, MA 01730
 
sam,john,greiff,wellner  @mitre.org
Abstract
In this paper, we describe MITRE?s contribution to
the logical form generation track of Senseval-3. We
begin with a description of the context of MITRE?s
work, followed by a description of the MITRE sys-
tem and its results. We conclude with a commentary
on the form and structure of this evaluation track.
1 Introduction
The logic form identification track of the 2004 Sen-
seval evaluation requires its participants to produce
a version of each input sentence with each input
word in citation form, annotated with both a scope-
free Davidsonian logic and lexical category infor-
mation for major categories. The output ignores el-
ements like determiners and negation, and features
such as plurals and verb tenses.
This evaluation is of interest to the MITRE Cor-
poration because it has a long-standing interest in
text processing and understanding, in all its various
dimensions. In our current internally funded Read-
ing Comprehension (RC) project, we focus on the
detailed understanding of individual stories, using
the ability to answer comprehension questions as-
sociated with these stories as our evaluation metric.
At the moment, we are interested in getting a sense
of how much inference is routinely needed in order
to answer RC questions; so generation of sentence
meanings is not currently our research focus. How-
ever, in the context of our exploration, we continue
to maintain an automated system for producing sen-
tence meanings from text.
2 The MITRE logic generation system
The system which MITRE employed for the
Senseval-3 logical form evaluation consists of the
following components:
 the Humphreys/Carroll/Minnen morphological
analyzer (Minnen et al, 2001)
 the CMU Link Grammar parser (Sleator and
Temperley, 1991)
 a link interpretation language which is used to
produce a dependency graph
 additional lexical knowledge sources
 an argument canonicalizer based partially on
the principles of Relational Grammar (Perl-
mutter, 1983)
 a task-specific logical form generator
The morphological analyzer is straightforward,
and we will not say more about it. We discuss the
remaining components below.
2.1 The CMU Link Grammar parser
The Link Grammar formalism consists of labeled,
undirected links among pairs of words. Each word
in the Link Grammar dictionary is mapped to a com-
plex logical expression of the link ends the word can
participate in. These link ends have a major compo-
nent (indicated by uppercase letters), a minor com-
ponent (indicated by lowercase letters), and a re-
quired direction (looking leftward (-) or rightward
(+)). Two words can be joined by a link if their link
ends are compatible. The Link Parser provides rea-
sonable performance achieving 75% labeled con-
stituent accuracy on the TreeBank data. There are
a large number of link types some of which pro-
vide very detailed distinctions beyond those found
in phrase structure grammars. For further details,
see (Sleator and Temperley, 1991).
Figure 1 shows the processing of the simple sen-
tence Chris loves Sam. We describe link parser
output as a set of 6-tuples, consisting of the index,
word, and link end for each end of the link; we omit
the direction information from the link, since it can
be inferred from the tuple. For instance, loves at in-
dex 2 is joined to Sam at index 3 via an O link; loves
bears O looking rightward in the lexicon, and Sam
bears O looking leftward, and these link ends are
compatible. As mentioned, ndividual lexical items
may (and often do) have multiple link types associ-
ated with them (e.g. Sam also bears S looking right-
ward for the case when Sam is a subject.)
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
input sentence Chris loves Sam
link parser 1 Chris Ss 2 loves Ss
output 2 loves O 3 Sam Os
rules (1) LINK ( S SF SX ) role[left]: arg:S role[right]: head
(2) LINK O role[left]: head role[right]: arg:O
(3) FEAT ( S- SX- ) category: v
dependency [v [ Chris 1:H]:S loves 2:H,3singular,present [Sam 3:H]:O]
object
logic form Chris (x1) loves:v (e1, x1, x2) Sam (x2)
Figure 1: Processing ?Chris loves Sam?
Link parses contain a great deal of detail, but be-
cause the link parser is a general-purpose tool, ex-
tracting this detail for a particular task may require
further processing. In particular, the category and
head/dependent information that is needed for logi-
cal form generation can be computed to a large de-
gree, but is not explicitly present. Our link interpre-
tation language addresses this issue.
2.2 The link interpretation language
Our link interpretation language operates on the out-
put of the link parser, and assembles a dependency
graph. The link interpretation language can assign
properties and categories to individual link ends via
FEAT rules, and assign head/dependency relations
to links via LINK rules.
Look again at Figure 1. Rule (1) applies to any
link whose ends are compatible with the link ends
S, SF or SX1 . This rule assigns the arg:S role
(i.e., subject argument) to the left end of the link,
and the head role to the right end. In other words,
if two words are linked by an S link, the left element
is the subject of the right element. Rule (2) creates
an analogous dependency for the O link, making the
right element the object of the left element. Rule (3)
says that anything on the leftward-looking end of an
S or SX link) should be assigned the category v; i.e.,
it?s a verb.
The LINK rules can assign a range of roles, in-
cluding:
 head
 argument of a particular type (e.g., S or O)
 modifier of a particular type (e.g., DET)
 merge, which promotes all dependents of the
merged element and constructs a complex lex-
ical head (e.g., for idioms or multi-word proper
names)
1S links are simple subject-verb relations, SF is used for the
special case where the subject is it or there (e.g. It was raining.),
and SX is used whent he subject is the first person pronoun I.
 filler and hole, which establish relationships re-
lated to unbounded dependencies
In addition, LINK and FEAT rules can assign
roles, properties and categories to the parents of the
left and right elements when necessary, and the pro-
cessor postpones these assignments until the appro-
priate parent relationships are established.
The processor which interprets this language be-
gins by assigning a dependency object to each word
in the sentence; the word is the head of the depen-
dency object, and the object has no dependents. The
processor then looks at each of the links, in any or-
der. It applies all relevant FEAT operators to each
link end, and finds the first LINK rule which ap-
plies. If any LINK rules which must be postponed
are found, the processor collects all candidate rules,
and chooses among them after the parent relation-
ships are established.
The output of this procedure as shown in the
fourth row of Figure 1 is a set of interconnected de-
pendency objects. Every dependency object which
has been identified as a non-head link end will have
the object it depends on as its parent. In the ideal
case, this set will have only one parentless object,
which will be the dependency object associated with
the matrix verb. Figure 1 also shows the topmost
dependency object for our example sentence; in this
representation, each word or constituent bears a suf-
fix indicating that it is the head (:H) or the relation-
ship it bears to the head (e.g., :O).
In general the process of adding LINK and FEAT
rules was carried out in a data-driven manner. Cur-
rently, there are 88 LINK rules and 63 FEAT rules.
While the number of potential rules is quite large
due to a large number of link types, catagories, and
properties, we have found that these rules general-
ize reasonably well and expect that the remaining
rules that would be required to represent very spe-
cific cases.
2.3 Additional lexical knowledge sources
For the purposes of deriving logical forms, the link
parser output doesn?t contain quite enough informa-
tion. We rely on two additional sources of lexical
knowledge: a small dictionary, developed in concert
with the link interpretation language, which identi-
fies features such as auxiliary for verbs, and a body
of lexical control information, derived from sub-
categorization classes in Comlex (Macleod et al,
1998). The first source informs the link interpre-
tation process, by identifying which verbs are de-
pendents of other verbs. The second source informs
our next step, the argument canonicalizer.
2.4 The argument canonicalizer
In this step, we construct an argument network for
each dependency object, in the spirit of Relational
Grammar (Perlmutter, 1983). For those predicative
phrases in argument positions which lack a subject,
we determine and assign a subject to control the
phrase. We use the lowest available grammatical
relation (first object, then subject) as the controller,
unless the information we?ve collected from Com-
lex indicates otherwise (e.g., in the case of promise).
We then identify those argument networks to which
Passive has applied, and undo it, and do the same for
Dative Movement, in order to derive the canonical
predicate argument order.
2.5 Deriving the logical forms
At this point, we have all the information we need to
derive the logical forms required for this evaluation
track. We generate logical forms via the following
steps:
1. We eliminate those words for which no output
is required (e.g., determiners).
2. We identify the remaining words which require
a part of speech suffix (e.g., nouns but not
proper nouns).
3. We identify the remaining words which take
arguments (e.g., verbs but not nouns) and those
which add their own instance variable (e.g.,
verbs but not prepositions).
4. We add the appropriate argument structures for
noun-noun compounds, and make other task-
specific adjustments.
5. We collect and format the appropriate predi-
cates and argument lists.
In some cases, a subject argument was required, but
we could not infer the appropriate filler; in these
cases, we insert the string ?MISSING? as the log-
ical subject in the logical form.
3 Results
Table 1 shows the precision and recall over both ar-
guments and predicates. Table 2 includes the pre-
centage of sentences of which all arguments were
identified (SentArg) and all predicates were identi-
fied (SentPred). SentArgPred indicates the percent-
age of sentences for which all arguments were iden-
tified correctly out of sentences that had all pred-
icates identified correctly. SentArgPredSent is the
percentage of sentences for which all arguments and
all predicates were identified correctly (SentArg-
PredSent).
Precision Recall
Arguments 0.74 0.66
Predicates 0.84 0.78
Table 1: Argument and predicate precision and re-
call.
Accuracy
SentArg 0.27
SentPred 0.21
SentArgPred 0.40
SentArgPredSent 0.087
Table 2: Sentence-based accuracy of extracted logic
forms.
Clearly, these results indicate room for improve-
ment in this task.
4 Comments on the evaluation
We found some problems in this evaluation.
4.1 Resolving vagueness in the task
In some cases, the details of the task are vague.
One example is collocations. The task description
clearly allows for collocations (e.g. proud of, at a
loss), but there is little guidance about how to decide
whether some word sequence should be a colloca-
tion. These decisions affect the system scores, and
the absence of clear guidance on this issue clearly
suggests uncertainty about what the scores mean.
Having an official list of collocations is only one
part of the solution, however. Since collocations
obscure internal structure, creating a collocation po-
tentially loses information; so the issue isn?t simply
to know what?s on the list, but to have some guide-
line for deciding what should be on the list.
One way in which to motivate guidelines, define
scoring metrics, etc. is to include a more goal-
directed task description. The last two decades of
research in computational linguistics have cemented
the crucial role of system evaluation, but the sum-
mary in (Hirschman and Thompson, 1996) makes
it clear that the best evaluations are defined with a
specific task in mind. In a previous attempt to define
predicate-argument structure, Semeval, the effort
was abandoned because so many constructs would
require detailed attention and resolution, and be-
cause most information-extraction systems did not
generate full predicate-argument structures (most
likely because the task did not require it) (Grishman
and Sundheim, 1996). While introducing a task cre-
ates its own problems by removing domain indepen-
dence, the constraints it provides are worth consid-
eration. For example, in a task such as Question An-
swering, certain distinctions in the logic-form pre-
sented here may serve no purpose or perhaps finer
grained distinctions are required.
As another example of this issue, the scorer pro-
vided for this task computes the precision and recall
for both predicates and predicate arguments in the
logic forms. In some circumstances, the scorer as-
signs the same score for predication of an incorrect,
independently specified variable (e.g., x2 instead of
x1 as the first argument of loves in Figure 1) as
for predication of an otherwise unspecified variable
(e.g., x3 instead of x1). This may be an informa-
tive scoring strategy, but having a more specific task
would help make this decision.
4.2 Suggested improvements in the logic
In many ways, it?s also impossible to make judg-
ments about the syntax and implied model for the
logic without a more specific task, but it?s still worth
pointing out some inconsistencies.
First, the implied account of noun-noun com-
pounds introduces an nn predicate, but assigns to
the resulting phrase a different variable than either
of the nominal constituents. Adjectival modifica-
tion, on the other hand, is represented by sharing
of variables. (Rus, 2002) argues for this account
of noun-noun compounds (p. 111), but provides
no motivation for treating the noun-noun compound
goat hair as having a separate variable from its
head but not doing the same for the adjective-noun
sequence curly hair.
Second, the account of pronominal possessives
(our, my) would lead to a poor account of full pos-
sessives. The possessive pronoun shares a variable
with its possesseed, which does not allow a paral-
lel or adequate account at all of the full possessives
(e.g., the poor boy?s father could only have boy,
poor, and father assigned to the same index). The
possessive should be treated like noun-noun com-
pounds, with a poss operator.
Finally, adverbs which modify adjectives have
nothing to attach to. In the single example of this
construction in the sample data (Sunshine makes me
very happy) the modifier very is predicated of me,
because happy is predicated of me. This account
leads immediately to problems with examples like
John is very tall but hardly imposing, where all four
modifying elements would end up being predicated
of John, introducing unnecessary ambiguity. In-
troducing properties in the logic as individuals (cf.
(Chierchia and Turner, 1988)) would almost cer-
tainly be an improvement.
References
G. Chierchia and R. Turner. 1988. Semantics
and property theory. Linguistics and Philosophy,
11:261?302.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference - 6: A brief his-
tory. In Papers presented to the Sixteenth Inter-
national Conference On Computational Linguis-
tics (COLING -96), University of Copenhagen.
L. Hirschman and H. Thompson. 1996. Overview
of evaluation in speech and natural language pro-
cessing. In R. Cole, editor, Survey of the State
of the Art in Human Language Technology, chap-
ter 13. Cambridge University Press, Cambridge.
Catherine Macleod, Ralph Grishman, and Adam
Meyers, 1998. COMLEX Syntax Reference Man-
ual. Proteus Project, NYU.
G. Minnen, J. Carroll, and D. Pearce. 2001. Ap-
plied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
David M. Perlmutter, editor. 1983. Studies in Rela-
tional Grammar 1. University of Chicago Press.
Vasile Rus. 2002. Logic Forms for Wordnet
Glosses. Ph.D. thesis, Southern Methodist Uni-
versity.
Daniel Sleator and Davy Temperley. 1991. Pars-
ing English with a Link Grammar. Technical Re-
port CMU-CS-91-196, Carnegie Mellon Univer-
sity Dept. of Computer Science, October.
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 192?200,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A simple feature-copying approach for long-distance dependencies 
Marc Vilain, Jonathan Huggins, and Ben Wellner The MITRE Corporation 202 Burlington Rd Bedford, MA 01730 (USA) {mbv,jhuggins,wellner}@mitre.org 
 
 
Abstract 
This paper is concerned with statistical meth-ods for treating long-distance dependencies.  We focus in particular on a case of substantial recent interest: that of long-distance depend-ency effects in entity extraction.  We intro-duce a new approach to capturing these effects through a simple feature copying preprocess, and demonstrate substantial performance gains on several entity extraction tasks. 1 Long-distance dependencies The linguistic phenomena known as long-distance dependencies have a long history in computational linguistics.  Originally arising in phrase-structure grammar, the term aptly describes phenomena that are not strictly grammatical, and has thus gained currency in other endeavors, including that of con-cern to us here: entity extraction.  The common thread, however, is simply that the treatment of a linguistic constituent ? might be influenced by the treatment of a non-local constituent ?. In phrase-structure grammar, dependencies arise between matrix phrases and the gapped phrases that they dominate, as in ?the cake that I hope you?ll serve ??.  The idea that these are long-distance dependencies arises from the fact that the separation between linked constituents can be arbi-trarily increased while their dependency continues to hold (as in ?the cake that I hope you?ll ask Fred to tell Joan to beg Maryanne to serve ??). With entity extraction, long-distance dependen-cies typically occur between mentions of the same entity.  Consider, for example, the italicized refer-ences to Thomas White in this newswire excerpt:  
Bank of America on Friday named Thomas White head of global markets.  White has been global head of credit products. The fact that the first of these mentions is easily understood as person-denoting has substantial bearing on interpreting the second mention as per-son-denoting as well.  But while local evidence for personhood is abundant for the first instance (e.g., the given name ?Thomas? or the verb ?named?), the evidence local to the second instance is weak, and it is highly unlikely that a learning procedure would on its own acquire the relevant 5-gram con-text (? has been ?JJ ?title).  The dependency between these instances of White is thus a significant factor in interpreting both as names. It is well known that capturing this kind of de-pendency can dramatically improve the perform-ance of entity extraction systems.  In this paper, we pursue a very simple method that enables statistical models to exploit these long-distance dependencies for entity extraction.  The method obtains compa-rable or better results than those achieved by more elaborate techniques, and while we focus here on the specific case of entity extraction, we believe that the method is simple and reliable enough to apply generally to other long-distance phenomena.  2 Approaches to name dependencies The problem of capturing long-distance dependen-cies between names has a traditional heuristic solu-tion.  This method, which goes back to systems participating in the original MUC-6 evaluation (Sundheim, 1995), is based on a found names list.  The method requires two passes through the input.  A first pass captures named entities based on local 
192
evidence, and enters these names into a found names registry.  A second pass identifies candidate entities that were missed by the first pass, and compares them to entries in the registry.  Where there is string overlap between the candidate and a previously found name, the entity type assigned to the existing entry is copied to the candidate. Overall, this is an effective strategy, and we used it ourselves in a rule-based name tagger from the MUC-6 era (Vilain and Day, 1996).  The strat-egy?s Achilles heel, however, is what happens when erroneous entries are added to the found names list.  These can get copied willy-nilly, thereby drastically increasing the scope of what may originally have started as a single local error.  Clearly, the approach is begging to be given a firmer evidence-weighing foundation. 2.1 A statistical hybrid An early such attempt at reformulating the ap-proach is due to Minkheev et al(1999).   As with previous approaches, Mikheev and his colleagues use a rule-based first pass to populate a found-names list.  The second pass, however, is based on a maximum entropy classifier that labels non-first-passed candidates based on evidence accrued from matching entries on the found-names list.  The sta-tistical nature of the decision eliminates some of the failure modes of the heuristic found-names strategy, and in particular, prevents the copying of single errors committed in the first pass.  The ma-jor weakness of the approach, however, is the heu-ristic first pass.  Minkheev et alnote that their method is most effective with a high-precision found-names list, implemented as a tightly con-trolled (but incomplete) rule-based first pass. 2.2 Fully-statistical models Several more recent efforts have attempted to re-move the need for a heuristic first-pass tagger, and have thus cast the problem as one-pass statistical models (Bunescu and Mooney, 2004; Sutton and McCallum, 2004; Finkel et al 2005).  While the technical details differ, all three methods approach the problem through conditional random fields (CRFs).  In order to capture the long-distance de-pendencies between name instances, these ap-proaches extend the linear-chain sequence models that are typically used for extracting entities with a CRF (Sha and Pereira, 2003).  The resulting models 
consist of sentence-length sequences interlinked on those words that might potentially have long-distance interactions.  Because of the graph-like nature of these models, the simplifying assump-tions of linear-chain CRFs no longer hold.  Since complete parameter estimation is intractable under these conditions, these three approaches introduce approximate methods for parameter estimation or decoding (Perceptron training for the first, loopy belief propagation for the first two, Gibbs sampling and simulated annealing for the third). Krishnan and Manning (2006) provide a lucid critique of these extended models and of their computational ramifications.  In a nutshell, their critique centers on the complexity of constructing the linked graphs (which they deemed high), the stability of Perceptron training (potentially unsta-ble), and the run-time cost of simulated annealing (undesirably high).  Since these undesirable prop-erties are directly due to the treatment of long-distance dependencies through graphical models, it is natural to ask whether graphical models are ac-tually required to capture these dependencies. 2.3 Avoiding non-sequential dependencies In point of fact, Krishnan and Manning (2006) pre-sent an alternative to these graph-based methods.  In particular, they break the explicit links that mu-tually condition non-adjacent lexemes, and instead rely on separate passes in a way that is reminiscent of earlier methods.  A first-pass CRF is used to identify entities based solely on local information.  The entity labels assigned by this first CRF are summarized in terms of lexeme-by-lexeme major-ity counts; these counts are then passed to a second CRF in the form of lexical features. Consider, for example, a financial news source, where we would expect that a term like ?Bank? might be assigned a preponderance of ORG labels by the first-pass CRF.  This would be signaled to the second-pass CRF through a token majority fea-ture that would take on the value ORG for all in-stances of the lexeme ?Bank?. This effectively aggregates local first-pass labeling decisions that apply to this lexeme, and makes the second-pass CRF sensitive to these first-pass decisions.  Further refinements capture cases where a lexeme?s label diverges from the token majority, for example: ?Left Bank,? where ?Bank? will be assigned a LOC-valued entity majority feature whenever it ap-
193
pears in that particular word sequence. By captur-ing long-distance dependencies through lexical features, Krishnan and Manning avoid the need for graphical models, thus regaining tractability. How well does this work?  Returning to our earlier example, the idea behind these majority count features is that a term like ?White? might be assigned the PER label by the first CRF when it ap-pears in the context ?Thomas White.?  Say, for the sake of argument, that sufficiently many instances of ?White? are labeled PER by the first pass to sum to a majority.  The second-stage CRF might then be expected to exploit the majority count features for ?White? to PER-label any instances of White that were left unlabeled in the first pass (or that were given erroneous first-pass labels). The method would be expected to fail, how-ever, in cases where the first pass yields a majority of erroneous labels.  Krishnan and Manning sug-gest that this is a fairly unlikely scenario, and dem-onstrate that their approach effectively captures long-distance name dependencies for the CoNLL English name-tagging task.  They measured a best-in-class error reduction of 13.3% between their two-pass method and a single-stage CRF equipped with comparable features.  3 A contradictory data set Just how unlikely, however, is the majority-error scenario that Krishnan and Manning discount?  As it turns out, we encountered precisely this scenario while working with a corpus that is closely related to the CoNLL data used by Krishnan and Manning. The corpus in question was drawn from the on-line edition of Reuters business news.  The articles cover a range of business topics: mergers and ac-quisitions (M+A), stock valuations, management change, and so forth.  This corpus is highly perti-nent to this discussion, as the CoNLL English data are also Reuters news stories, drawn from the gen-eral news distribution.  Our business data thus rep-resent a natural branch of the overall CoNLL data. A characteristic of these Reuters business sto-ries that distinguishes them from general news is the prevalence of organization names, in particular company names.  In these data, instances of com-pany names significantly outnumber the next-most-common entities (money, dates, and the like).  Even state-of-the-art CRFs trained on these data therefore err on the side of generating companies, 
meaning that in the absence of countermanding evidence (such as the presence of a person?s given name), an entity will tend to be labeled ORG by default.  Our earlier ?Thomas White? example is a case in point: where the full name would typically be labeled PER, last-name-only instances (?White?) might go unlabeled or be marked ORGs. Table 1, above, shows a qualitative analysis of this phenomenon for PER entities in our M+A test set.  The table considers person-denoting entities with three or more instances in the test set (n=35), and summarizes the majority accuracy of the labels assigned to them by a feature-rich 1-pass CRF.  Of these thirty-five cases, we eliminate from consid-eration six trivial test cases that are present unam-biguously in the training data (e.g., ?Carl Icahn?), since the CRF will effectively memorizes these cases during training.  Of the remaining twenty-nine non-trivial cases, not quite half of them (45%) were accurately labeled by the CRF for the majority of their instances.  A larger number of entities ei-ther received an incorrect majority label (38%) or were equivocally labeled, receiving an equal num-ber of correct and incorrect tags (17%). For this data set then, majority count features are poor models of the long-distance dependencies between person names, as they are just about as likely to predict the wrong label as the correct one. 4 A feature-copying alternative A further analysis of our business news test sample revealed an intriguing fact.  While in the absence of compelling evidence, the CRF might label a mention of a person entity as an org (or leave it unlabeled), for those mentions where compelling evidence existed, the CRF generally got it right.  By compelling evidence, we mean such linguistic cues as the presence of a given name, contextual prox-imity to agentive verbs (e.g. ?said?), and so forth. This suggests an alternative approach to captur-ing these kinds of long-distance dependencies be-
Label accuracy count % test cases Trivially correct (present in both test and training) 6 ? Majority correct, test only 13 45% Majority incorrect, test only 11 38% Equivocal, test only 5 17% Table 1: effectiveness of majority counts as predictors of entity type, Reuters business news sample 
194
tween names.  In contrast to previous approaches, what is needed is not so much a way of coordinat-ing non-local decisions about an entity?s label, as a way of coordinating non-local evidence pertinent to the labeling decision.  That is, instead of condi-tioning the labeling decision of a lexeme on the labeling decisions for that lexeme elsewhere in the corpus, we ought to condition the decision on the key evidence supporting those decisions. 4.1 Displaced features Our approach operates by identifying those fea-tures of a CRF that are most predictive over a cor-pus.  Each of those features is then duplicated: for a given token ?, one version of the feature applies directly to ?, while the other version applies to all other instances where ??s word form appears in the current document.  In particular, what we duplicate is the indicator function for a feature.  The local version of an indicator ? signals true if it applies locally to ?, while the displaced version ?d signals true if it applies to any token ?? that is an instance of the same word from as ?. To make this concrete, consider our opening example, now indexed with word positions: Thomas7 White8 ? White13 has14 been15 ? Say that ? is a feature indicator that is true of a token ?i just in case the token to its left, ?i-1, is a given name.  In this instance, ?(White8) is true and ?(White13) is false.  Then ?d, the displaced version of ?, will be true of ?i just in case there is some token ?j with the same word form such that ?(?j) is true.  In this instance ?d(White8) and ?d(White13) are both true by virtue of ? being true of White8. This feature displacement scheme introduces non-local evidence into labeling decisions, effec-tively capturing the long-distance dependencies exhibited by name-tagging tasks.  The method dif-fers from previous approaches in that the models are not made conditional on non-local decisions (as in the case of graphical models), nor are they made conditional on aggregated first-pass decisions (as in Krishnan & Manning), but rather are made con-ditional on non-local evidence (displaced features). 4.2 Identifying features to displace Because a typical entity extraction model can use tens or hundreds of thousands of features, it is not practical to displace every one of them.  Though 
technically this only doubles the number of fea-tures under consideration, the lexical indexing rap-idly gets out of hand.  In addition, training and run times increase and, in our experience, a risk of over-fitting emerges.  In point of fact, however, capturing long-distance name dependencies does not require us to replicate every last bit of feature-borne evidence.  Instead, we only need to displace the evidence that is most reliably predictive. To select predictive features to displace, we?ve had most success with a method based on informa-tion gain.  Specifically, we use a one-time pre-process that measures feature gain relative to a corpus.  The pre-process considers the same com-plement of feature schemas as are used by the ac-tual CRF, and grounds the schemas on a training corpus to instantiate free lexical and P-O-S parame-ters.  Gain for the instantiated features is measured through K-L divergence, and the n features with highest gain are then selected for displacement (with n typically ranging from 1,000 to 10,000). As in (Schneider, 2004), gain for a given fea-ture ?, is found through a variant of the familiar Kullback-Leibler divergence formula, 
? 
D
KL
(P ||Q) = p(x
i
)log
2
p(x
i
)
q(x
i
)
i
?
 For our purposes, the xi are the non-null entity labels defined for the training set (PER, ORG, etc.), P is the probability distribution of the labels over the training set, Q is the distribution of the labels over tokens for which ? applies, and p and q are their respective smoothed probability estimates (Laplace smoothing).  Note in particular that this formulation excludes the null label (?not an en-tity?).  This effectively means that K-L divergence is giving us a measure of the degree to which a feature predicts one or more non-null entity labels.  Because the null label is generally the dominant label in named-entity tasks, including the null label in the calculation of K-L divergence tends to overwhelm the statistics, and leads to the selection of uninformative features that predict non-entities. Figure 1 demonstrates the effectiveness of this feature selection method, along with sensitivity to the threshold parameter.  The figure charts F-score on a Reuters business news task (M+A) as a func-tion of the number of displaced features.  From a baseline of F=89.3, performance improves rapidly with the addition of displaced features to the CRF model, reaching a maximum of F=91.4 with the 
195
addition of 1,000 displaced features.  Performance then fluctuates asymptotically around this level. The chart also shows comparable growth curves for two alternative feature selection methods.  The feature count method is similar to feature gain, but instead of ranking features with K-L divergence, it ranks them according to the number of times they match against the corpus.  Feature weight does not use a schema-grounding first pass to generate can-didate features, but trains a CRF model on the cor-pus, and then ranks features according to the weight assigned to them in the model.  In prelimi-nary experiments, neither of these methods yielded as high-performing a set of displaced features as feature gain.  Additionally their growth curves ex-hibit sensitivity to parameter setting, which sug-gests a risk of over-fitting.  For these reasons, we did not pursue these approaches further. Note finally that the feature schemas we con-sider for displacement only encode local evidence (see Table 2 below).  In particular, they do not en-code the assigned label of a word form, as this would effectively introduce the kind of graphical conditional dependencies that lie outside the scope of linear-chain CRF methods. 4.3 Training and decoding Aside from two pre-processing steps, training or decoding a CRF with displaced features is no dif-ferent from training or decoding one with only conventional features.  As to the pre-processing steps, the first applies to the corpus overall, as we must initially select a collection of locally predic-tive features to displace.  The second step applies on a per-document basis and consists of the crea-tion of the inverted lexical indices that are used to trigger indicator functions for displaced features. 
While these additional steps complicate training and decoding somewhat, they have little effect on actual decoding run times.  Most importantly, they retain the linear-chain properties of the CRF, and therefore do not require the graphical modeling and involved parameter estimation called for by most previous approaches.  In addition, the training logistics are of a lesser magnitude than those re-quired by Krishnan and Manning?s approach, since training their second-stage model first requires round-robin training of one-fold-left-out classifiers that estimate first-stage majority counts. 5 Experimental design To evaluate the effectiveness of feature copying with long-distance dependencies, we undertook a number of information extraction experiments.  We focused on the traditional name-tagging task, relying on both current and archival data sets.  For each data set, we trained entity-extraction models that corresponded to three different strategies for capturing long-distance dependencies. ? Baseline model: a feature-rich CRF trained with only local features and no long-distance dependency features; ? Feature-copying model: a CRF trained with the same local features, along with displaced versions of high-gain features; ? Majority model: a re-implementation of the Krishnan and Manning strategy, using the same feature set as the baseline CRF as well as their majority count features. We used held-out development test sets to tune the selection of displaced features, in particular, the number of features to displace. 5.1 CRF configurations We used the Carafe open-source implementation of sequence-based conditional random fields.1  Carafe has achieved competitive results for standard se-quence modeling tasks (Wellner & Vilain, 2006, Wellner et al 2007), and allows for flexible feature design.  Carafe provides several learning methods, including a fast gradient descent method using pe-riodic step-size adjustment (Huang et al 2007).  Preliminary trials, however, produced better results                                                        1 http://sourceforge.net/projects/carafe 
Figure 1: F score on the Reuters M+A task, as a  function of number of displaced features 
196
with conditional log-likelihood learning (L-BFGS optimization).  We used this latter method here, L2-regularized by a spherical Gaussian prior with variance set to 10.0 (based on preliminary trials). Our baseline CRF was given a feature set that has proven its mettle in the literature (see Table 2).  Along with contextual n-grams and the like, these features capture linguistic regularities through membership in vocabulary lists, e.g., first names, major geographical names, honorifics, etc.  They also include hand-engineered lists from our legacy rule-based tagger, e.g., head word lists for organi-zation names, lists of agentive verbs that reliably apply to persons, date atoms, and more.  For part-of-speech features, we either accepted the parts of speech provided with a data set, or generated them with our implementation of Brill?s method (Brill, 1994).  For the majority count features, we used document and corpus versions the token and entity features described by Krishnan and Manning, but did not re-implement their super-entity feature. 5.2 Experimental data We evaluated our approach on five different data sets: our current corpus of Web-harvested Reuters business news, as well as four archival data sets that have been reported on by other researchers.  The business news data consist of a training corpus of mergers and acquisition stories (M+A), devel-opment and evaluation test sets for M+A and test sets for three additional topics: hot stocks (HS), new initiatives (NI), and general business news (BN).   Table 3 provides an overview of our data sets and of some salient distinctions between them. All five extraction tasks require the reporting of three core entity types: persons, organizations, and locations; additional required types are noted in the table.  The reporting guidelines for the first four tasks are closely related: Reuters business and MUC-6 were annotated to the same original MUC-6 
standard, while MUC-7 and MNET extend the MUC-6 standard slightly.  The CoNLL standard alone calls for a catch-all (and troublesome) MISC entity. 5.3 Scoring metrics Previous results on these data sets have been re-ported using one of two scoring methods: strict match (CoNLL) or match with partial credit, as cal-culated by the MUC scorer (MUC-6, MUC-7, and MNET).  To enable comparisons to previously pub-lished work, we report our results with the metric appropriate to each data set (we use the MUC scorer for Reuters).  These scoring distinctions are perti-nent only to comparisons of absolute performance.  In this paper, the interest is with relative compari-sons across approaches to long-distance dependen-cies, for which the scorers are kept constant. 6 Experimental results Table 4 summarizes our experimental results for the seven test sets annotated to the MUC-6 standard or its close variants (we will consider the CoNLL task separately).  Along with F scores for our base-line CRF, the table presents F scores and baseline-relative error reduction (?E) for two approaches to long-distance name dependencies: feature dis-placement (disp) and the Krishnan and Manning strategy (K+M).  We were pleased to see that fea-ture displacement proved effective for all of the extraction tasks.  As the table shows, the addition of displaced features consistently reduced the re-sidual error term left by the baseline CRF trained only with local features.  For the English-language corpora, the error reduction ranged from a low of 11 % for the Reuters NI task to a high of 39% for the MUC-6 task.  The error reduction for the Span-ish-language MNET task was lowest of all, at 8.9%. For all the English tasks, we consistently achieved better results with feature displacement 
lexical unigrams w-2 ? w+2 lexical bigrams w-2,w-1 ? w+1,w+2 P-O-S unigrams p-2 ? p+2 P-O-S bigrams p-2,p-1 ? p+1,p+2 substrings .*s or s.* ||s||?4 linguistic word lists gazetteers, date atoms, ? regular expressions caps., digits, ? ?corp.? nearby also ?ltd.? ? Table 2: Baseline features; wi and pi respectively de-note lexeme and P-O-S in relative position i. 
Corpus Language NU TM MI  Topics MUC-6 English ? ?  mostly politics MUC-7 English ? ?r  mostly politics MNET Spanish ? ?r  mostly politics Reuters English ? ?  business CoNLL English   ? all news Table 3: Data set characteristics.  All include persons, organizations, and locations; some have nu-meric forms (NU), dates and times (TM) where r indicates relative dates, or misc (MI). 
197
than with our version of Krishnan and Manning?s approach (we were not able to obtain Spanish K+M results by publication time).  In each case, dis-placement produced a greater reduction in baseline error than did majority counts.  Furthermore, be-cause both approaches start from the same baseline CRF, the resulting raw performance was conse-quently also higher for displacement.  Note in par-ticular the Reuters M+A test set: these are the data for which Table 1 suggests that majority counts would be poor predictors of long-distance effects.  This prediction is in fact borne out by our results. 6.1 Effects of linguistic engineering  We were interested to note that the feature dis-placement method achieved both highest perform-ance and highest error reduction for the MUC-6 corpus (F=92.8, ?E=39.3%) and for two of the Reuters test sets: M+A (F=91.4, ?E=20.0%) and BN (F=91.8, ?E=21.6%).  The MUC-6 F-score, in par-ticular, is comparable to those of hand-built MUC-era systems; in fact, it exceeds the score of our own hand-built MUC-6 system (Aberdeen et al 1995). What is apparently happening is that these three data sets are well matched to a group of linguisti-cally inspired lexical features with which we trained our baseline CRF.  In particular, our base-line features include gazetteers and word lists hand-selected for identifying entities based on lo-cal context: first names, agentive verbs, date at-oms, etc.  This played out in two significant ways.  First, these linguistic features tended to elevate baseline performance (see Table 4).  Second, these same features also proved effective when dis-placed, as demonstrated by the substantial error reduction with displacement. Feature displacement thus further rewards sound feature engineering. 6.2 Other MUC-related results The MUC-7 and Reuters hot stocks data (HS) pro-vide informative contrasts.  For these data, feature displacement provided error reduction of ?E=13.9% and 13.4% respectively, which is less 
than for the top three data sets.  It is interesting to note that in both cases, the baseline score is also lower, suggesting again that the performance of feature copying follows the performance of base-line tagging.  In the case of Reuters HS, the evalua-tion data contained many out-of-training references to stock indices, which depressed baseline scores.  Similar development-to-evaluation divergences have also been noted with the MUC-7 corpus. 6.3 The CoNLL task Our results for the CoNLL task, reported in Table 5 below, provide a different point of contrast.  The middle two rows of the table present the same ex-perimental configurations as have been discussed so far.  For this data set, we note that feature dis-placement does not perform as well as our re-implementation of Krishnan and Manning?s strat-egy in terms of both absolute score and error re-duction.  Likewise, published results for other approaches mostly outperform displacement (see the first three rows in Table 5). One possible explanation lies with the linguistic features with which we approached CoNLL: these are the same ones we originally developed for MUC-6.  As noted earlier the CoNLL standard di-verges in several ways from MUC-6.  In particular, CoNLL calls for a MISC entity that covers a range of name-like entities, e.g., events. MISC also, how-ever, captures names that are trapped by tokeniza-tion (?London-based?), as well as some MUC organizations (sports leagues).  This suggests that adapting our features to the CONLL task might help. 
MUC-6 MUC-7 MNET Reuters M+A Reuters BN Reuters HS Reuters NI  F ?E F ?E F ?E F ?E F ?E F ?E F ?E baseline 88.2 ? 84.0 ? 88.9 ? 89.3 ? 89.5 ? 85.4 ? 88.8 ? disp. 92.8 39% 86.2 14% 89.9 8.9% 91.4 20% 91.8 22% 87.3 13% 90.1 11% K+M 91.5 28% 85.2 7.4% ? ? 90.4 11% 91.0 14% 86.3 6.2% 89.2 2.8%  Table 4: Performance on seven test sets annotated to variants of the MUC-6 standard (MUC scorer). 
  base F LDD F ?E Bunescu + Mooney 2004 80.09 82.30 11.1% Finkel et al2005 85.51 86.86 9.3% Krishnan + Manning 2006 85.29 87.34 13.3% K+M (re-impl, MUC feats.) 84.3 86.0 10.7% displacement (MUC feats.) 84.3 85.8 9.6% displ. (CoNLL feats.) 85.24 86.55 8.9% displ. (CoNLL feats. + DS) 86.57 87.39 6.1% Table 5: Performance on the CoNLL task; LDD designates  use of long-distance dependency method. 
198
The final two rows in Table 5 present attempts to tune our features to CoNLL.  This includes some features (the ?CoNLL feats? in Table 5) indicating story topic, all-caps headline contexts, presence in a sporting result table, and similar idiosyncrasies.  In addition, we also used features based on dis-tributional similarity word lists (DS in the table) provided with the Stanford NER package.2 While these feature engineering efforts proved effective, what we found surprised us.  As Table 5 shows, the CoNLL features do substantially raise baseline performance, with the full set of new fea-tures producing a baseline (F=86.6) that outper-forms previously published baselines by over a point of F score.  In keeping with our observations for the MUC-annotated text, we would then have expected to see a comparable increase in the per-formance of displaced features, i.e., a jump in error reduction relative to the baseline.  Instead, we found just the reverse.  Whereas displacement ac-counts for a 1.5 point gain in F (?E=9.6%) with the MUC baseline features, with the beter CoNLL fea-tures, the gain due to displacement falls to 0.82 points of F (?E=6.1%).  While the final result with displacement (F=87.39) slightly edges out the pre-vious high water mark of F=87.35 (Krishnan and Manning, 2005), the pattern is puzzling and not in keeping with our seven other data sets. One possible explanations lies again with the CoNLL standard.  The standard calls explicitly for inconsistent annotation of the same entity when used in different contexts.  Along with place names being called MISC in hyphenated contexts (noted above), some places must be called ORG when used to refer to sports teams ? except in results tables, where they are sometimes LOC.  Such inconsisten-cies subvert the notion of long-distance dependen-cies by making these dependencies contradictory, thereby reducing the potential value of displace-ment as a means for improving performance. 7 Conclusions Earlier in this paper, we introduced the notion of long-distance dependencies through their original codification in the context of phrase-structure grammars.  By an interesting historical twist, the original solution to these grammatical long-distance effects, known as gap threading (Pereira,                                                        2 http://nlp.stanford.edu/software/CRF-NER.shtml 
1981), involved what is essentially a feature-copying operation, namely unification of constitu-ent features.  It is gratifying to note that the method presented here has illustrious predecessors. Regarding the particular task of interest here, entity extraction, this paper conclusively shows that a simple feature-copying method provides an effective method for capturing long-distance de-pendencies between names.  For the MUC-6 task, in particular, this error reduction is enough to lift a middle-of-the-pack performance from our baseline CRF to a level that would have placed it among the handful of top performers at the MUC-6 evaluation. As noted, the method is also substantially more manageable than earlier approaches.  It avoids the intractability of graphical models and also avoids the approximations required by methods that rely on these models.  It also adds only minimal proc-essing time at training and run times.  This pro-vides a practical alternative to the method of Krishnan and Manning, who require twelve sepa-rate training runs to create their models, and fur-ther require a time-consuming run-time process to mediate between their first and second stage CRFs. We intend to take this work in two directions.  First, we would like to get to the bottom of why the method did not do better with the CoNLL and MNET tasks.  As noted earlier, our hypothesis is that we would expect greater exploitation of long-distance dependencies if we first improved the performance of the baseline CRF, especially by improving the acuity of task-related features.  While it is not a key interest of ours to achieve best-in-class per-formance on historical evaluations, it is the case that we seek a better understanding of the range of application of the feature copying method. Another direction of interest is to consider other problems that exhibit long-distance dependencies that might be addressed by feature copying.  Word sense disambiguation is one such case, especially given Yarowsky?s maxim regarding one sense per discourse, a consistency notion that seems tailor-made for treatment as long-distance dependencies (Yarowsky, 1995).  Likewise, we are curious about the applicability of the method to reference resolu-tion, another key task with long-distance effects. Meanwhile, we believe that this method pro-vides a practical approach for capturing long-distance effects in one of the most practical and useful application of human language technologies, entity extraction. 
199
References John Aberdeen, John Burger, David Day, Lynette Hirschman, Patricia Robinson, and Marc Vilain. 1995. Description of the Alembic system as used for MUC-6. Pcdgs of the 6th Message Understanding Conference (MUC-6). Eric Brill. 1994. Some advances in rule-based part-of-speech tagging. Pcdgs. AAAI-94. Razvan Bunescu and Raymond J. Mooney. 2004. Col-lective information extraction with relational Markov networks. Pcdgs. of the 42nd ACL.  Barcelona. Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sam-pling. Pcdgs. of the 43rd ACL.  Ann Arbor, MI. Han-Shen Huang, Yu-Ming Chang, and Chun-Nan Hsu. 2007. Training conditional random fields by periodic step size adaptation for large-scale text mining.  Pcdgs. 7th Intl. Conf. on Data Mining (ICDM-2007). Vijay Krishnan and Christopher D. Manning. 2006. An Effective Two-Stage Model for Exploiting Non-Local Dependencies in Named Entity Recognition. Pcdgs. of the 21st COLING and 44th ACL.  Sidney. Andrei Mikheev, Marc Moens, and Claire Grover. 1999. Named entity recognition without gazetteers.  Pcdgs. of the 9th EACL. Bergen. Fernando Pereira. 1981. Extraposition grammars.  American Jnl. of Computational Linguistics, 4(7). Karl-Michael Schneider. 2004. A new feature selection score for multinomial naive Bayes text classification based on KL-divergence.  In Companion to the Pcdgs. of the 42nd ACL.  Barcelona. Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields.  Pcdgs. of NAACL-HLT 2003. Edmonton, CA. Beth Sundheim, ed. 1995.  Pcdgs. of the 6th Message Understanding Conference (MUC-6). Columbia, MD. Charles Sutton and Andrew McCallum. 2004. Collec-tive segmentation and labeling of distant entities in information extraction.  Pcdgs. ICML Workshop on Statistical Relational Learning. Marc Vilain and David Day. 1996. Finite-state phrse parsing by rule sequences. Pcdgs. of the 16th Confer-ence on Computational Lingusitics (COLING-96). Ben Wellner, Matt Huyck, Scott Mardis, John Aber-deen, Alex Morgan, Leon Peskin, Alex Yeh, Janet Hitzeman, and Lynette Hirschman. 2007. Rapidly re-targetable approaches to de-identification.  Journal of the Americal Medical Informatics Association; 14(5). Ben Wellner and Marc Vilain. (2006). Leveraging ma-chine-readable dictionaries in discriminative se-quence models. In Pcdgs. of the 5th Language Resources and Evaluation Conf. (LREC 2006). Genoa. David Yarowsky. 1995.  Unsupervised word sense dis-ambiguation rivaling supervised methods.  Pcdgs. Of 33rd ACL.  Cambridge, MA. 
200
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 9?16, Detroit, June 2005. c?2005 Association for Computational Linguistics
Adaptive String Similarity Metrics for Biomedical Reference Resolution
Ben Wellner
 

The MITRE Corporation
202 Burlington Rd
Bedford MA 01730
wellner@mitre.org
Jose? Castan?o
 
and James Pustejovsky  
 
Computer Science Department
Brandeis University
Waltham MA 02454
 jcastano,jamesp  @cs.brandeis.edu
Abstract
In this paper we present the evaluation
of a set of string similarity metrics used
to resolve the mapping from strings to
concepts in the UMLS MetaThesaurus.
String similarity is conceived as a single
component in a full Reference Resolution
System that would resolve such a map-
ping. Given this qualification, we obtain
positive results achieving 73.6 F-measure
(76.1 precision and 71.4 recall) for the
task of assigning the correct UMLS con-
cept to a given string. Our results demon-
strate that adaptive string similarity meth-
ods based on Conditional Random Fields
outperform standard metrics in this do-
main.
1 Introduction
1.1 String Similarity and Reference Resolution
String similarity/matching algorithms are used as a
component in reference resolution algorithms. We
use reference resolution in a broad sense, which in-
cludes any of the following aspects:
a. Intra-document noun phrase reference resolu-
tion.
b. Cross-document or corpus reference resolution.
c. Resolution of entities found in a corpus with
databases, dictionaries or other external knowl-
edge sources. This is also called semantic inte-
gration, e.g., (Li et al, 2005), reference ground-
ing, e.g., (Kim and Park, 2004) or normaliza-
tion, e.g., (Pustejovsky et al, 2002; Morgan et
al., 2004).
The last two aspects of reference resolution are
particularly important for information extraction,
and the interaction of reference resolution with in-
formation extraction techniques (see for example
Bagga (1998)). The extraction of a particular set of
entities from a corpus requires reference resolution
for the set of entities extracted (e.g., the EDT task in
ACE1), and it is apparent that there is more variation
in the cross-document naming conventions than in a
single document.
The importance of edit distance algorithms has
already been noticed, (Mu?ller et al, 2002) and the
importance of string similarity techniques in the
biomedical domain has also been acknowledged,
e.g., (Yang et al, 2004).
String similarity/matching algorithms have also
been used extensively in related problems such as
Name databases and similar problems in structured
data, see (Li et al, 2005) and references mentioned
therein.
The problem of determining whether two similar
strings may denotate the same entity is particularly
challenging in the biomedical literature. It has al-
ready been noticed (Cohen et al, 2002) that there
is great variation in the naming conventions, and
noun phrase constructions in the literature. It has
also been noticed that bio-databases are hardly ever
updated with the names in the literature (Blaschke
1http://www.nist.gov/speech/tests/ace/
9
et al, 2003). A further complication is that the ac-
tual mentions found in text are more complex than
just names - including descriptors, in particular. Fi-
nally, ambiguity (where multiple entities have the
same name) is very pervasive in biomedicine.
In this paper we investigate the use of several
string similarity methods to group together string
mentions that might refer to the same entity or con-
cept. Specifically, we consider the sub-problem of
assigning an unseen mention to one of a set of exist-
ing unique entities or concepts, each with an associ-
ated set of known synonyms. As our aim here is fo-
cusing on improving string matching, we have pur-
posely factored out the problem of ambiguity (to the
extent possible) by using the UMLS MetaThesaurus
as our data source, which is largly free of strings that
refer to multiple entities. Thus, our work here can be
viewed an important piece in a larger normalization
or reference resolution system that resolves ambigu-
ity (which includes filtering out mentions that don?t
refer to any entity of interest).
The experiments reported on in this paper evalu-
ate a suite of robust string similarity techniques. Our
results demonstrate considerable improvement to be
gained by using adaptive string similarity metrics
based on Conditional Random Fields customized to
the domain at hand. The resulting best metric, we
term SoftTFIDF-CRF, achieves 73.6 F-measure on
the task of assigning a given string to the correct
concept. Additionally, our experiments demonstrate
a tradeoff between efficiency and recall based on  -
gram indexing.
2 Background
2.1 Entity Extraction and Reference
Resolution in the Biomedical Domain
Most of the work related to reference resolution in
this domain has been done in the following areas: a)
Intra-document Reference resolution, e.g (Castan?o
et al, 2002; Lin and Liang, 2004) b) Intra-document
Named entity recognition (e.g Biocreative Task 1A
(Blaschke et al, 2003), and others), also called clas-
sification of biological names (Torii et al, 2004) c)
Intra-document alias extraction d) cross-document
Acronym-expansion extraction, e.g., (Pustejovsky
et al, 2001). e) Protein names resolution against
database entries in SwissProt, protein name ground-
ing, in the context of a relation extraction task
(Kim and Park, 2004). One constraint in these ap-
proaches is that they use several patterns for the
string matching problem. The results of the protein
name grounding are 59% precision and 40% recall.
The Biocreative Task 1B task challenged systems
to ground entities found in article abstracts which
contain mentions of genes in Fly, Mouse and Yeast
databases. A central component in this task was re-
solving ambiguity as many gene names refer to mul-
tiple genes.
2.2 String Similarity and Ambiguity
In this subsection consider the string similarity is-
sues that are present in the biology domain in par-
ticular. The task we consider is to associate a string
with an existing entity, represented by a set of known
strings. Although the issue of ambiguity is present
in the examples we give, it cannot be resolved by
using string similarity methods alone, but instead by
methods that take into account the context in which
those strings occur.
The protein name p21 is ambiguous at least
between two entities, mentioned as p21-ras and
p21/Waf in the literature. A biologist can look at
a set of descriptions and decide whether the strings
are ambiguous or correspond to any of these two (or
any other entity).
The following is an example of such a mapping,
where R corresponds to p21-ras, W to p21(Waf) and
G to another entity (the gene). Also it can be noticed
that some of the mappings include subcases (e.g.,
R.1).2
String Form Entity
ras-p21 protein R
p21 R/W
p21(Waf1/Cip1) W
cyclin-dependent kinase-I p21(Waf-1) W
normal ras p21 protein R
pure v-Kirsten (Ki)-ras p21 R.1
wild type p21 R/W
synthetic peptide P21 R/W.2
p21 promoter G
transforming protein v-p21 R.3
v-p21 R.3
p21CIP1/WAF1 W
protein p21 WAF1/CIP1/Sd:1 W
Table 1: A possible mapping from strings to entities.
2All the examples were taken from the MEDLINE corpus.
10
If we want to use an external knowlege source to
produce such a mapping, we can try to map it to con-
cepts in the UMLS Methatesaurus and entries in the
SwissProt database.
These two entities correspond to the concepts
C0029007 (p21-Ras) and C0288472 (p21-Waf) in
the UMLS Methathesaurus. There are 27 strings or
names in the UMLS that map to C0288472 (Table
2):
oncoprotein p21 CAP20
CDK2-associated protein 20 kDa MDA 6
Cdk2 inhibitor WAF1 CIP1
Cdk-interacting protein cdn1 protein
CDK-Interacting Protein 1 CDKN1A
CDKN1 protein Cip1 protein
Cip-1 protein mda-6 protein
Cyclin-Dependent Kinase Inhibitor 1A p21
p21 cell cycle regulator p21(cip1)
p21 cyclin kinase inhibitor p21(waf1-cip1)
Pic-1 protein (cyclin) p21-WAF1
senescent cell-derived inhibitor protein 1 protein p21
CDKN1A protein WAF1 protein
WAF-1 Protein
Table 2: UMLS strings corresponding to C0288472
There are 8 strings that map to concept C0029007
(Table 3).
Proto-Oncogene Protein p21(ras) p21(c-ras)
p21 RAS Family Protein p21 RAS Protein
Proto-Oncogene Protein ras c-ras Protein
ras Proto-Oncogene Product p21 p21(ras)
Table 3: UMLS strings corresponding to C0029007
It can be observed that there is only one exact
match: p21 in C0288472 and Table 1. It should
be noted that p21, is not present in the UMLS as a
possible string for C0029007. There are other close
matches like p21(Waf1/Cip1) (which seems very
frequent) and p21(waf1-cip1).
An expression like The inhibitor of cyclin-
dependent kinases WAF1 gene product p21 has
a high similarity with Cyclin-Dependent Kinase
Inhibitor 1 A and The cyclin-dependent kinase-I
p21(Waf-1) partially matches Cyclin-Dependent Ki-
nase
However there are other mappings which look
quite difficult unless some context is given to pro-
vide additional clues (e.g., v-p21).
The SwissProt entries CDN1A FELCA,
CDN1A HUMAN and CDN1A MOUSE are
related to p21(Waf). They have the following set of
common description names:
Cyclin-dependent kinase inhibitor 1, p21, CDK-
interacting protein 1.3
There is only one entry in SwissProt related to p21-
ras: Q9PSS8 PLAFE: with the description name
P21-ras protein and a related gene name: Ki-ras.
It should be noted that SwissProt classifies, as dif-
ferent entities, the proteins that refer to different or-
ganisms. The UMLS MetaThesaurus, on the other
hand, does not make this distinction. Neither is this
distinction always present in the literature.
3 Methods for Computing String
Similarity
A central component in the process of normaliza-
tion or reference resolution is computing string sim-
ilarity between two strings. Methods for measuring
string similarity can generally be broken down into
character-based and token-based approaches.
Character-based approaches typically consist of
the edit-distance metric and variants thereof. Edit
distance considers the number of edit operations (ad-
dition, substitution and deletion) required to trans-
form a string  into another string 
	 . The Leven-
stein distance assigns unit cost to all edit operations.
Other variations allow arbitrary costs or special costs
for starting and continuing a ?gap? (i.e., a long se-
quence of adds or deletes).
Token-based approaches include the Jaccard sim-
ilarity metric and the TF/IDF metric. The meth-
ods consider the (possibly weighted) overlap be-
tween the tokens of two strings. Hybrid token and
character-based are best represented by SoftTFIDF,
which includes not only exact token matches but
also close matches (using edit-distance, for exam-
ple). Another approach is to perform the Jaccard
similarity (or TF/IDF) between the  -grams of the
two strings instead of the tokens. See Cohen et
al. (2003) for a detailed overview and comparison
of some of these methods on different data sets.
3There are two more description names for the human and
mouse entries. The SwissProt database has also associated
Gene names to those entries which are related to some of the
possible names that we find in the literature. Those gene names
are: CDKN1A, CAP20, CDKN1, CIP1, MDA6, PIC1, SDI1,
WAF1, Cdkn1a, Cip1, Waf1. It can be seen that those names are
incorporated in the UMLS as protein names.
11
Recent work has also focused on automatic meth-
ods for adapting these string similarity measures
to specific data sets using machine learning. Such
approaches include using classifiers to weight var-
ious fields for matching database records (Cohen
and Richman, 2001). (Belenko and Mooney, 2003)
presents a generative, Hidden Markov Model for
string similarity.
4 An Adaptive String Similarity Model
Conditional Random Fields (CRF) are a recent, in-
creasingly popular approach to sequence labeling
problems. Informally, a CRF bears resemblance to
a Hidden Markov Model (HMM) in which, for each
input position in a sequence, there is an observed
variable and a corresponding hidden variable. Like
HMMs, CRFs are able to model (Markov) depen-
dencies between the hidden (predicted) variables.
However, because CRFs are conditional, discrimina-
tively trained models, they can incorporate arbitrary
overlapping (non-independent) features over the en-
tire input space ? just like a discriminative classi-
fier.
CRFs are log-linear models that compute the
probability of a state sequence,    	
  ,
given an observed sequence,       	     as:
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 117?125,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Classification of Discourse Coherence Relations: An Exploratory Study
using Multiple Knowledge Sources
Ben Wellner
 
, James Pustejovsky   , Catherine Havasi   ,
Anna Rumshisky
 
and Roser Saur??
 
 
Department of Computer Science
Brandeis University
Waltham, MA USA

The MITRE Corporation
202 Burlington Road
Bedford, MA USA

wellner,jamesp,havasi,arum,roser  @cs.brandeis.edu
Abstract
In this paper we consider the problem of
identifying and classifying discourse co-
herence relations. We report initial re-
sults over the recently released Discourse
GraphBank (Wolf and Gibson, 2005). Our
approach considers, and determines the
contributions of, a variety of syntactic and
lexico-semantic features. We achieve 81%
accuracy on the task of discourse relation
type classification and 70% accuracy on
relation identification.
1 Introduction
The area of modeling discourse has arguably seen
less success than other areas in NLP. Contribut-
ing to this is the fact that no consensus has been
reached on the inventory of discourse relations
nor on the types of formal restrictions placed on
discourse structure. Furthermore, modeling dis-
course structure requires access to considerable
prior linguistic analysis including syntax, lexical
and compositional semantics, as well as the res-
olution of entity and event-level anaphora, all of
which are non-trivial problems themselves.
Discourse processing has been used in many
text processing applications, most notably text
summarization and compression, text generation,
and dialogue understanding. However, it is also
important for general text understanding, includ-
ing applications such as information extraction
and question answering.
Recently, Wolf and Gibson (2005) have pro-
posed a graph-based approach to representing in-
formational discourse relations.1 They demon-
strate that tree representations are inadequate for
1The relations they define roughly follow Hobbs (1985).
modeling coherence relations, and show that many
discourse segments have multiple parents (incom-
ing directed relations) and many of the relations
introduce crossing dependencies ? both of which
preclude tree representations. Their annotation of
135 articles has been released as the GraphBank
corpus.
In this paper, we provide initial results for the
following tasks: (1) automatically classifying the
type of discourse coherence relation; and (2) iden-
tifying whether any discourse relation exists on
two text segments. The experiments we report
are based on the annotated data in the Discourse
GraphBank, where we assume that the discourse
units have already been identified.
In contrast to a highly structured, compositional
approach to discourse parsing, we explore a sim-
ple, flat, feature-based methodology. Such an ap-
proach has the advantage of easily accommodat-
ing many knowledge sources. This type of de-
tailed feature analysis can serve to inform or aug-
ment more structured, compositional approaches
to discourse such as those based on Segmented
Discourse Representation Theory (SDRT) (Asher
and Lascarides, 2003) or the approach taken with
the D-LTAG system (Forbes et al, 2001).
Using a comprehensive set of linguistic fea-
tures as input to a Maximum Entropy classifier,
we achieve 81% accuracy on classifying the cor-
rect type of discourse coherence relation between
two segments.
2 Previous Work
In the past few years, the tasks of discourse seg-
mentation and parsing have been tackled from
different perspectives and within different frame-
works. Within Rhetorical Structure Theory (RST),
Soricut and Marcu (2003) have developed two
117
probabilistic models for identifying clausal ele-
mentary discourse units and generating discourse
trees at the sentence level. These are built using
lexical and syntactic information obtained from
mapping the discourse-annotated sentences in the
RST Corpus (Carlson et al, 2003) to their corre-
sponding syntactic trees in the Penn Treebank.
Within SDRT, Baldridge and Lascarides
(2005b) also take a data-driven approach to
the tasks of segmentation and identification of
discourse relations. They create a probabilistic
discourse parser based on dialogues from the Red-
woods Treebank, annotated with SDRT rhetorical
relations (Baldridge and Lascarides, 2005a). The
parser is grounded on headed tree representations
and dialogue-based features, such as turn-taking
and domain specific goals.
In the Penn Discourse TreeBank (PDTB) (Web-
ber et al, 2005), the identification of discourse
structure is approached independently of any lin-
guistic theory by using discourse connectives
rather than abstract rhetorical relations. PDTB
assumes that connectives are binary discourse-
level predicates conveying a semantic relationship
between two abstract object-denoting arguments.
The set of semantic relationships can be estab-
lished at different levels of granularity, depend-
ing on the application. Miltsakaki, et al (2005)
propose a first step at disambiguating the sense of
a small subset of connectives (since, while, and
when) at the paragraph level. They aim at distin-
guishing between the temporal, causal, and con-
trastive use of the connective, by means of syntac-
tic features derived from the Penn Treebank and a
MaxEnt model.
3 GraphBank
3.1 Coherence Relations
For annotating the discourse relations in text, Wolf
and Gibson (2005) assume a clause-unit-based
definition of a discourse segment. They define
four broad classes of coherence relations:
(1) 1. Resemblance: similarity (par), con-
trast (contr), example (examp), generaliza-
tion (gen), elaboration (elab);
2. Cause-effect: explanation (ce), violated
expectation (expv), condition (cond);
3. Temporal (temp): essentially narration;
4. Attribution (attr): reporting and evidential
contexts.
The textual evidence contributing to identifying
the various resemblance relations is heterogeneous
at best, where, for example, similarity and contrast
are associated with specific syntactic constructions
and devices. For each relation type, there are well-
known lexical and phrasal cues:
(2) a. similarity: and;
b. contrast: by contrast, but;
c. example: for example;
d. elaboration: also, furthermore, in addi-
tion, note that;
e. generalization: in general.
However, just as often, the relation is encoded
through lexical coherence, via semantic associa-
tion, sub/supertyping, and accommodation strate-
gies (Asher and Lascarides, 2003).
The cause-effect relations include conventional
causation and explanation relations (captured as
the label ce), such as (3) below:
(3) cause: SEG1: crash-landed in New Hope,
Ga.,
effect: SEG2: and injuring 23 others.
It also includes conditionals and violated expecta-
tions, such as (4).
(4) cause: SEG1: an Eastern Airlines Lockheed
L-1011 en route from Miami to the Bahamas
lost all three of its engines,
effect: SEG2: and land safely back in Miami.
The two last coherence relations annotated in
GraphBank are temporal (temp) and attribution
(attr) relations. The first corresponds generally to
the occasion (Hobbs, 1985) or narration (Asher
and Lascarides, 2003) relation, while the latter is
a general annotation over attribution of source.2
3.2 Discussion
The difficulty of annotating coherence relations
consistently has been previously discussed in the
literature. In GraphBank, as in any corpus, there
are inconsistencies that must be accommodated
for learning purposes. As perhaps expected, an-
notation of attribution and temporal sequence rela-
tions was consistent if not entirely complete. The
most serious concern we had from working with
2There is one non-rhetorical relation, same, which identi-
fies discontiguous segments.
118
the corpus derives from the conflation of diverse
and semantically contradictory relations among
the cause-effect annotations. For canonical cau-
sation pairs (and their violations) such as those
above, (3) and (4), the annotation was expectedly
consistent and semantically appropriate. Problems
arise, however when examining the treatment of
purpose clauses and rationale clauses. These are
annotated, according to the guidelines, as cause-
effect pairings. Consider (5) below.
(5) cause: SEG1: to upgrade lab equipment in
1987.
effect: SEG2: The university spent $ 30,000
This is both counter-intuitive and temporally false.
The rationale clause is annotated as the cause, and
the matrix sentence as the effect. Things are even
worse with purpose clause annotation. Consider
the following example discourse:3
(6) John pushed the door to open it, but it was
locked.
This would have the following annotation in
GraphBank:
(7) cause: to open it
effect: John pushed the door.
The guideline reflects the appropriate intuition
that the intention expressed in the purpose or ra-
tionale clause must precede the implementation of
the action carried out in the matrix sentence. In
effect, this would be something like
(8) [INTENTION TO SEG1] CAUSES SEG2
The problem here is that the cause-effect re-
lation conflates real event-causation with telos-
directed explanations, that is, action directed to-
wards a goal by virtue of an intention. Given that
these are semantically disjoint relations, which
are furthermore triggered by distinct grammatical
constructions, we believe this conflation should be
undone and characterized as two separate coher-
ence relations. If the relations just discussed were
annotated as telic-causation, the features encoded
for subsequent training of a machine learning al-
gorithm could benefit from distinct syntactic envi-
ronments. We would like to automatically gen-
erate temporal orderings from cause-effect rela-
tions from the events directly annotated in the text.
3This specific example was brought to our attention by
Alex Lascarides (p.c).
Splitting these classes would preserve the sound-
ness of such a procedure, while keeping them
lumped generates inconsistencies.
4 Data Preparation and Knowledge
Sources
In this section we describe the various linguistic
processing components used for classification and
identification of GraphBank discourse relations.
4.1 Pre-Processing
We performed tokenization, sentence tagging,
part-of-speech tagging, and shallow syntactic
parsing (chunking) over the 135 GraphBank docu-
ments. Part-of-speech tagging and shallow parsing
were carried out using the Carafe implementation
of Conditional Random Fields for NLP (Wellner
and Vilain, 2006) trained on various standard cor-
pora. In addition, full sentence parses were ob-
tained using the RASP parser (Briscoe and Car-
roll, 2002). Grammatical relations derived from
a single top-ranked tree for each sentence (head-
word, modifier, and relation type) were used for
feature construction.
4.2 Modal Parsing and Temporal Ordering
of Events
We performed both modal parsing and tempo-
ral parsing over events. Identification of events
was performed using EvITA (Saur?? et al, 2006),
an open-domain event tagger developed under the
TARSQI research framework (Verhagen et al,
2005). EvITA locates and tags all event-referring
expressions in the input text that can be tempo-
rally ordered. In addition, it identifies those gram-
matical features implicated in temporal and modal
information of events; namely, tense, aspect, po-
larity, modality, as well as the event class. Event
annotation follows version 1.2.1 of the TimeML
specifications.4
Modal parsing in the form of identifying sub-
ordinating verb relations and their type was per-
formed using SlinkET (Saur?? et al, 2006), an-
other component of the TARSQI framework. Slin-
kET identifies subordination constructions intro-
ducing modality information in text; essentially,
infinitival and that-clauses embedded by factive
predicates (regret), reporting predicates (say), and
predicates referring to events of attempting (try),
volition (want), command (order), among others.
4See http://www.timeml.org.
119
SlinkET annotates these subordination contexts
and classifies them according to the modality in-
formation introduced by the relation between the
embedding and embedded predicates, which can
be of any of the following types:
 factive: The embedded event is presupposed
or entailed as true (e.g., John managed to
leave the party).
 counter-factive: The embedded event is pre-
supposed as entailed as false (e.g., John was
unable to leave the party).
 evidential: The subordination is introduced
by a reporting or perception event (e.g., Mary
saw/told that John left the party).
 negative evidential: The subordination is a
reporting event conveying negative polarity
(e.g., Mary denied that John left the party).
 modal: The subordination creates an inten-
sional context (e.g., John wanted to leave the
party).
Temporal orderings between events were iden-
tified using a Maximum Entropy classifier trained
on the TimeBank 1.2 and Opinion 1.0a corpora.
These corpora provide annotated events along
with temporal links between events. The link
types included: before ( 
	 occurs before  ) , in-
cludes ( 

occurs sometime during 
	
), simultane-
ous ( 	 occurs over the same interval as  ), begins
(  	 begins at the same time as   ), ends (  	 ends at
the same time as 

).
4.3 Lexical Semantic Typing and Coherence
Lexical semantic types as well as a measure of
lexical similarity or coherence between words in
two discourse segments would appear to be use-
ful for assigning an appropriate discourse rela-
tionship. Resemblance relations, in particular, re-
quire similar entities to be involved and lexical
similarity here serves as an approximation to defi-
nite nominal coreference. Identification of lexical
relationships between words across segments ap-
pears especially useful for cause-effect relations.
In example (3) above, determining a (potential)
cause-effect relationship between crash and injury
is necessary to identify the discourse relation.
4.3.1 Corpus-based Lexical Similarity
Lexical similarity was computed using the
Word Sketch Engine (WSE) (Killgarrif et al,
2004) similarity metric applied over British Na-
tional Corpus. The WSE similarity metric imple-
ments the word similarity measure based on gram-
matical relations as defined in (Lin, 1998) with mi-
nor modifications.
4.3.2 The Brandeis Semantic Ontology
As a second source of lexical coherence, we
used the Brandeis Semantic Ontology or BSO
(Pustejovsky et al, 2006). The BSO is a lexically-
based ontology in the Generative Lexicon tradi-
tion (Pustejovsky, 2001; Pustejovsky, 1995). It fo-
cuses on contextualizing the meanings of words
and does this by a rich system of types and qualia
structures. For example, if one were to look up the
phrase RED WINE in the BSO, one would find its
type is WINE and its type?s type is ALCOHOLIC
BEVERAGE. The BSO contains ontological qualia
information (shown below). Using the BSO, one






wine
CONSTITUTIVE  Alcohol
HAS ELEMENT  Alcohol
MADE OF  Grapes
INDIRECT TELIC  drink activity
INDIRECT AGENTIVE  make alcoholic beverage






is able to find out where in the ontological type
system WINE is located, what RED WINE?s lexi-
cal neighbors are, and its full set of part of speech
and grammatical attributes. Other words have a
different configuration of annotated attributes de-
pending on the type of the word.
We used the BSO typing information to seman-
tically tag individual words in order to compute
lexical paths between word pairs. Such lexical as-
sociations are invoked when constructing cause-
effect relations and other implicatures (e.g. be-
tween crash and injure in Example 3).
The type system paths provide a measure of the
connectedness between words. For every pair of
head words in a GraphBank document, the short-
est path between the two words within the BSO
is computed. Currently, this metric only uses the
type system relations (i.e., inheritance) but prelim-
inary tests show that including qualia relations as
connections is promising. We also computed the
earliest common ancestor of the two words. These
metrics are calculated for every possible sense of
the word within the BSO.
120
The use of the BSO is advantageous compared
to other frameworks such as Wordnet because it
focuses on the connection between words and their
semantic relationship to other items. These con-
nections are captured in the qualia information and
the type system. In Wordnet, qualia-like informa-
tion is only present in the glosses, and they do
not provide a definite semantic path between any
two lexical items. Although synonymous in some
ways, synset members often behave differently in
many situations, grammatical or otherwise.
5 Classification Methodology
This section describes in detail how we con-
structed features from the various knowledge
sources described above and how they were en-
coded in a Maximum Entropy model.
5.1 Maximum Entropy Classification
For our experiments of classifying relation types,
we used a Maximum Entropy classifier5 in order
to assign labels to each pair of discourse segments
connected by some relation. For each instance (i.e.
pair of segments) the classifier makes its decision
based on a set of features. Each feature can query
some arbitrary property of the two segments, pos-
sibly taking into account external information or
knowledge sources. For example, a feature could
query whether the two segments are adjacent to
each other, whether one segment contains a dis-
course connective, whether they both share a par-
ticular word, whether a particular syntactic con-
struction or lexical association is present, etc. We
make strong use of this ability to include very
many, highly interdependent features6 in our ex-
periments. Besides binary-valued features, fea-
ture values can be real-valued and thus capture fre-
quencies, similarity values, or other scalar quanti-
ties.
5.2 Feature Classes
We grouped the features together into various
feature classes based roughly on the knowledge
source from which they were derived. Table 1
describes the various feature classes in detail and
provides some actual example features from each
class for the segment pair described in Example 5
in Section 3.2.
5We use the Maximum Entropy classifier included with
Carafe available at http://sourceforge.net/projects/carafe
6The total maximum number of features occurring in our
experiments is roughly 120,000.
6 Experiments and Results
In this section we provide the results of a set of
experiments focused on the task of discourse rela-
tion classification. We also report initial results on
relation identification with the same set of features
as used for classification.
6.1 Discourse Relation Classification
The task of discourse relation classification in-
volves assigning the correct label to a pair of dis-
course segments.7 The pair of segments to assign
a relation to is provided (from the annotated data).
In addition, we assume, for asymmetric links, that
the nucleus and satellite are provided (i.e., the di-
rection of the relation). For the elaboration rela-
tions, we ignored the annotated subtypes (person,
time, location, etc.). Experiments were carried out
on the full set of relation types as well as the sim-
pler set of coarse-grained relation categories de-
scribed in Section 3.1.
The GraphBank contains a total of 8755 an-
notated coherence relations. 8 For all the ex-
periments in this paper, we used 8-fold cross-
validation with 12.5% of the data used for test-
ing and the remainder used for training for each
fold. Accuracy numbers reported are the average
accuracies over the 8 folds. Variance was gener-
ally low with a standard deviation typically in the
range of 1.5 to 2.0. We note here also that the
inter-annotator agreement between the two Graph-
Bank annotators was 94.6% for relations when
they agreed on the presence of a relation. The
majority class baseline (i.e., the accuracy achieved
by calling all relations elaboration) is 45.7% (and
66.57% with the collapsed categories). These are
the upper and lower bounds against which these
results should be based.
To ascertain the utility of each of the various
feature classes, we considered each feature class
independently by using only features from a sin-
gle class in addition to the Proximity feature class
which serve as a baseline. Table 2 illustrates the
result of this experiment.
We performed a second set of experiments
shown in Table 3 that is essentially the converse
of the previous batch. We take the union of all the
7Each segment may in fact consist of a sequence of seg-
ments. We will, however, use the term segment loosely to
refer to segments or segment sequences.
8All documents are doubly annotated; we used the anno-
tator1 annotations.
121
Feature Description Example
Class
C Words appearing at beginning and end of the two discourse seg-
ments - these are often important discourse cue words.
first1-is-to; first2-is-The
P Proximity and direction between the two segments (in terms of
segments) - binary features such as distance less than 3, distance
greater than 10 were used in addition to the distance value itself;
the distance from beginning of the document using a similar bin-
ning approach
adjacent; dist-less-than-3; dist-less-
than-5; direction-reverse; samesentence
BSO Paths in the BSO up to length 10 between non-function words in the
two segments.
ResearchLab  EducationalActivity
 University
WSE WSE word-pair similarities between words in the two segments
were binned as (  0.05,  0.1,  0.2). We also computed sen-
tence similarity as the sum of the word similarities divided by the
sum of their sentence lengths.
WSE-greater-than-0.05; WSE-
sentence-sim = 0.005417
E Event head words and event head word pairs between segments as
identified by EvITA.
event1-is-upgrade; event2-is-spent;
event-pair-upgrade-spent
SlinkET Event attributes, subordinating links and their types between event
pairs in the two segments
seg1-class-is-occurrence; seg2-class-
is-occurrence; seg1-tense-is-infinitive;
seg2-tense-is-past; seg2-modal-seg1
C-E Cuewords of one segment paired with events in the other. first1-is-to-event2-is-spent; first2-is-
The-event1-is-upgrade
Syntax Grammatical dependency relations between two segments as iden-
tified by the RASP parser. We also conjoined the relation with one
or both of the headwords associated with the grammatical relation.
gr-ncmod; gr-ncmod-head1-equipment;
gr-ncmod-head-2-spent; etc.
Tlink Temporal links between events in the two segments. We included
both the link types and the number of occurrences of those types
between the segments
seg2-before-seg1
Table 1: Feature classes, their descriptions and example feature instances for Example 5 in Section 3.2.
Feature Class Accuracy Coarse-grained Acc.
Proximity 60.08% 69.43%
P+C 76.77% 83.50%
P+BSO 62.92% 74.40%
P+WSE 62.20% 70.10%
P+E 63.84% 78.16%
P+SlinkET 69.00% 75.91%
P+CE 67.18% 78.63%
P+Syntax 70.30% 80.84%
P+Tlink 64.19% 72.30%
Table 2: Classification accuracy over standard and
coarse-grained relation types with each feature
class added to Proximity feature class.
feature classes and perform ablation experiments
by removing one feature class at a time.
Feature Class Accuracy Coarse-grain Acc.
All Features 81.06% 87.51%
All-P 71.52% 84.88%
All-C 75.71% 84.69%
All-BSO 80.65% 87.04%
All-WSE 80.26% 87.14%
All-E 80.90% 86.92%
All-SlinkET 79.68% 86.89%
All-CE 80.41% 87.14%
All-Syntax 80.20% 86.89%
All-Tlink 80.30% 87.36%
Table 3: Classification accuracy with each fea-
ture class removed from the union of all feature
classes.
6.2 Analysis
From the ablation results, it is clear that overall
performance is most impacted by the cue-word
features (C) and proximity (P). Syntax and Slin-
kET also have high impact improving accuracy by
roughly 10 and 9 percent respectively as shown
in Table 2. From the ablation results in Table 3,
it is clear that the utility of most of the individ-
ual features classes is lessened when all the other
feature classes are taken into account. This indi-
cates that multiple feature classes are responsible
for providing evidence any given discourse rela-
tions. Removing a single feature class degrades
performance, but only slightly, as the others can
compensate.
Overall precision, recall and F-measure results
for each of the different link types using the set
of all feature classes are shown in Table 4 with the
corresponding confusion matrix in Table A.1. Per-
formance correlates roughly with the frequency of
the various relation types. We might therefore ex-
pect some improvement in performance with more
annotated data for those relations with low fre-
quency in the GraphBank.
122
Relation Precision Recall F-measure Count
elab 88.72 95.31 91.90 512
attr 91.14 95.10 93.09 184
par 71.89 83.33 77.19 132
same 87.09 75.00 80.60 72
ce 78.78 41.26 54.16 63
contr 65.51 66.67 66.08 57
examp 78.94 48.39 60.00 31
temp 50.00 20.83 29.41 24
expv 33.33 16.67 22.22 12
cond 45.45 62.50 52.63 8
gen 0.0 0.0 0.0 0
Table 4: Precision, Recall and F-measure results.
6.3 Coherence Relation Identification
The task of identifying the presence of a rela-
tion is complicated by the fact that we must con-
sider all ffProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 92?101, Prague, June 2007. c?2007 Association for Computational Linguistics
 
		Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 753?760,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Machine Learning of Temporal Relations 
Inderjeet Mani??, Marc Verhagen?, Ben Wellner?? 
Chong Min Lee? and James Pustejovsky? 
?The MITRE Corporation 
202 Burlington Road, Bedford, MA 01730, USA 
?Department of Linguistics, Georgetown University 
37th and O Streets, Washington, DC 20036, USA 
?Department of Computer Science, Brandeis University 
415 South St., Waltham, MA 02254, USA 
{imani, wellner}@mitre.org, {marc, jamesp}@cs.brandeis.edu, cml54@georgetown.edu 
Abstract 
This paper investigates a machine learn-
ing approach for temporally ordering and 
anchoring events in natural language 
texts. To address data sparseness, we 
used temporal reasoning as an over-
sampling method to dramatically expand 
the amount of training data, resulting in 
predictive accuracy on link labeling as 
high as 93% using a Maximum Entropy 
classifier on human annotated data. This 
method compared favorably against a se-
ries of increasingly sophisticated base-
lines involving expansion of rules de-
rived from human intuitions. 
1 Introduction 
The growing interest in practical NLP applica-
tions such as question-answering and text sum-
marization places increasing demands on the 
processing of temporal information. In multi-
document summarization of news articles, it can 
be useful to know the relative order of events so 
as to merge and present information from multi-
ple news sources correctly. In question-
answering, one would like to be able to ask when 
an event occurs, or what events occurred prior to 
a particular event.  
A wealth of prior research by (Passoneau 
1988), (Webber 1988), (Hwang and Schubert 
1992), (Kamp and Reyle 1993), (Lascarides and 
Asher 1993), (Hitzeman et al 1995), (Kehler 
2000) and others, has explored the different 
knowledge sources used in inferring the temporal 
ordering of events, including temporal adver-
bials, tense, aspect, rhetorical relations, prag-
matic conventions, and background knowledge. 
For example, the narrative convention of events 
being described in the order in which they occur 
is followed in (1), but overridden by means of a 
discourse relation, Explanation in (2).  
(1) Max stood up. John greeted him.  
(2) Max fell. John pushed him.  
In addition to discourse relations, which often 
require inferences based on world knowledge, 
the ordering decisions humans carry out appear 
to involve a variety of knowledge sources, in-
cluding tense and grammatical aspect (3a), lexi-
cal aspect (3b), and temporal adverbials (3c): 
(3a) Max entered the room. He had drunk a lot 
of wine.  
(3b) Max entered the room. Mary was seated 
behind the desk.  
(3c) The company announced Tuesday that 
third-quarter sales had fallen.  
Clearly, substantial linguistic processing may 
be required for a system to make these infer-
ences, and world knowledge is hard to make 
available to a domain-independent program. An 
important strategy in this area is of course the 
development of annotated corpora than can fa-
cilitate the machine learning of such ordering 
inferences. 
This paper 1  investigates a machine learning 
approach for temporally ordering events in natu-
ral language texts. In Section 2, we describe the 
annotation scheme and annotated corpora, and 
the challenges posed by them. A basic learning 
approach is described in Section 3. To address 
data sparseness, we used temporal reasoning as 
an over-sampling method to dramatically expand 
the amount of training data.  
As we will discuss in Section 5, there are no 
standard algorithms for making these inferences 
that we can compare against. We believe 
strongly that in such situations, it?s worthwhile 
for computational linguists to devote consider-
                                                 
1Research at Georgetown and Brandeis on this prob-
lem was funded in part by a grant from the ARDA 
AQUAINT Program, Phase II.  
753
able effort to developing insightful baselines. 
Our work is, accordingly, evaluated in compari-
son against four baselines: (i) the usual majority 
class statistical baseline, shown along with each 
result, (ii) a more sophisticated baseline that uses 
hand-coded rules (Section 4.1), (iii) a hybrid 
baseline based on hand-coded rules expanded 
with Google-induced rules (Section 4.2), and (iv) 
a machine learning version that learns from im-
perfect annotation produced by (ii) (Section 4.3).  
2 Annotation Scheme and Corpora 
2.1 TimeML 
TimeML (Pustejovsky et al 2005) 
(www.timeml.org) is an annotation scheme for 
markup of events, times, and their temporal rela-
tions in news articles. The TimeML scheme flags 
tensed verbs, adjectives, and nominals with 
EVENT tags with various attributes, including 
the class of event, tense, grammatical aspect, po-
larity (negative or positive), any modal operators 
which govern the event being tagged, and cardi-
nality of the event if it?s mentioned more than 
once. Likewise, time expressions are flagged and 
their values normalized, based on TIMEX3, an 
extension of the ACE (2004) (tern.mitre.org) 
TIMEX2 annotation scheme.  
For temporal relations, TimeML defines a 
TLINK tag that links tagged events to other 
events and/or times. For example, given (3a), a 
TLINK tag orders an instance of the event of 
entering to an instance of the drinking with the 
relation type AFTER. Likewise, given the sen-
tence (3c), a TLINK tag will anchor the event 
instance of announcing to the time expression 
Tuesday (whose normalized value will be in-
ferred from context), with the relation 
IS_INCLUDED. These inferences are shown (in 
slightly abbreviated form) in the annotations in 
(4) and (5). 
(4) Max <EVENT eventID=?e1? 
class=?occurrence? tense=?past? as-
pect=?none?>entered</EVENT> the room. 
He <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?>had drunk</EVENT>a 
lot of wine.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
 (5) The company <EVENT even-
tID=?e1? class=?reporting? 
tense=?past? as-
pect=?none?>announced</EVENT> 
<TIMEX3 tid=?t2? type=?DATE? tempo-
ralFunction=?false? value=?1998-01-
08?>Tuesday </TIMEX3> that third-
quarter sales <EVENT eventID=?e2? 
class=?occurrence? tense=?past? as-
pect=?perfect?> had fallen</EVENT>.  
<TLINK eventID=?e1? relatedToEven-
tID=?e2? relType=?AFTER?/> 
<TLINK eventID=?e1? relatedTo-
TimeID=?t2? relType=?IS_INCLUDED?/> 
 
The anchor relation is an Event-Time TLINK, 
and the order relation is an Event-Event TLINK. 
TimeML uses 14 temporal relations in the 
TLINK RelTypes, which reduce to a disjunctive 
classification of 6 temporal relations RelTypes = 
{SIMULTANEOUS, IBEFORE, BEFORE, BE-
GINS, ENDS, INCLUDES}. An event or time is 
SIMULTANEOUS with another event or time if 
they occupy the same time interval. An event or 
time INCLUDES another event or time if the 
latter occupies a proper subinterval of the former. 
These 6 relations and their inverses map one-to-
one to 12 of Allen?s 13 basic relations (Allen 
1984)2. There has been a considerable amount of 
activity related to this scheme; we focus here on 
some of the challenges posed by the TLINK an-
notation, the part that is directly relevant to the 
temporal ordering and anchoring problems. 
2.2 Challenges 
The annotation of TimeML information is on a 
par with other challenging semantic annotation 
schemes, like PropBank, RST annotation, etc., 
where high inter-annotator reliability is crucial 
but not always achievable without massive pre-
processing to reduce the user?s workload. In Ti-
meML, inter-annotator agreement for time ex-
pressions and events is 0.83 and 0.78 (average of 
Precision and Recall) respectively, but on 
TLINKs it is 0.55 (P&R average), due to the 
large number of event pairs that can be selected 
for comparison. The time complexity of the hu-
man TLINK annotation task is quadratic in the 
number of events and times in the document. 
Two corpora have been released based on Ti-
meML: the TimeBank (Pustejovsky et al 2003) 
(we use version 1.2.a) with 186 documents and 
                                                 
2Of the 14 TLINK relations, the 6 inverse relations are re-
dundant. In order to have a disjunctive classification, SI-
MULTANEOUS and IDENTITY are collapsed, since 
IDENTITY is a subtype of SIMULTANEOUS. (Specifi-
cally, X and Y are identical if they are simultaneous and 
coreferential.) DURING and IS_INCLUDED are collapsed 
since DURING is a subtype of IS_INCLUDED that anchors 
events to times that are durations. IBEFORE (immediately 
before) corresponds to Allen?s MEETS. Allen?s OVER-
LAPS relation is not represented in TimeML. More details 
can be found at timeml.org. 
754
64,077 words of text, and the Opinion Corpus 
(www.timeml.org), with 73 documents and 
38,709 words. The TimeBank was developed in 
the early stages of TimeML development, and 
was partitioned across five annotators with dif-
ferent levels of expertise. The Opinion Corpus 
was developed very recently, and was partitioned 
across just two highly trained annotators, and 
could therefore be expected to be less noisy. In 
our experiments, we merged the two datasets to 
produce a single corpus, called OTC. 
Table 1 shows the distribution of EVENTs and 
TIMES, and TLINK RelTypes3 in the OTC. The 
majority class percentages are shown in paren-
theses. It can be seen that BEFORE and SI-
MULTANEOUS together form a majority of 
event-ordering (Event-Event) links, whereas 
most of the event anchoring (Event-Time) links 
are INCLUDES.  
 
12750 Events, 2114 Times 
Relation Event-Event Event-Time 
IBEFORE 131 15 
BEGINS 160 112 
ENDS 208 159 
SIMULTANEOUS 1528 77 
INCLUDES 950 3001 (65.3%) 
BEFORE 3170 (51.6%) 1229 
TOTAL 6147 4593 
Table 1. TLINK Class Distributions in OTC 
Corpus 
 
The lack of TLINK coverage in human anno-
tation could be helped by preprocessing, pro-
vided it meets some threshold of accuracy. Given 
the availability of a corpus like OTC, it is natural 
to try a machine learning approach to see if it can 
be used to provide that preprocessing. However, 
the noise in the corpus and the sparseness of 
links present challenges to a learning approach. 
3 Machine Learning Approach 
3.1 Initial Learner 
There are several sub-problems related to in-
ferring event anchoring and event ordering. Once 
a tagger has tagged the events and times, the first 
task (A) is to link events and/or times, and the 
second task (B) is to label the links. Task A is 
hard to evaluate since, in the absence of massive 
preprocessing, many links are ignored by the 
human in creating the annotated corpora. In addi-
                                                 
3The number of TLINKs shown is based on the number of 
TLINK vectors extracted from the OTC. 
tion, a program, as a baseline, can trivially link 
all tagged events and times, getting 100% recall 
on Task A. We focus here on Task B, the label-
ing task. In the case of humans, in fact, when a 
TLINK is posited by both annotators between the 
same pairs of events or times, the inter-annotator 
agreement on the labels is a .77 average of P&R. 
To ensure replicability of results, we assume per-
fect (i.e., OTC-supplied) events, times, and links.  
Thus, we can consider TLINK inference as the 
following classification problem: given an or-
dered pair of elements X and Y, where X and Y 
are events or times which the human has related 
temporally via a TLINK, the classifier has to as-
sign a label in RelTypes. Using RelTypes instead 
of RelTypes ?  {NONE} also avoids the prob-
lem of heavily skewing the data towards the 
NONE class.  
To construct feature vectors for machine 
learning, we took each TLINK in the corpus and 
used the given TimeML features, with the 
TLINK class being the vector?s class feature.  
For replicability by other users of these corpora, 
and to be able to isolate the effect of components, 
we used ?perfect? features; no feature engineer-
ing was attempted. The features were, for each 
event in an event-ordering pair, the event-class, 
aspect, modality, tense and negation (all nominal 
features); event string, and signal (a preposi-
tion/adverb, e.g., reported on Tuesday), which 
are string features, and contextual features indi-
cating whether the same tense and same aspect 
are true of both elements in the event pair. For 
event-time links, we used the above event and 
signal features along with TIMEX3 time features. 
For learning, we used an off-the-shelf Maxi-
mum Entropy (ME) classifier (from Carafe, 
available at sourceforge.net/projects/carafe). As 
shown in the UNCLOSED (ME) column in Ta-
ble 24, accuracy of the unclosed ME classifier 
does not go above 77%, though it?s always better 
than the majority class (in parentheses). We also 
tried a variety of other classifiers, including the 
SMO support-vector machine and the na?ve 
Bayes tools in WEKA (www.weka.net.nz). SMO 
performance (but not na?ve Bayes) was compa-
rable with ME, with SMO trailing it in a few 
cases (to save space, we report just ME perform-
ance). It?s possible that feature engineering could 
improve performance, but since this is ?perfect? 
data, the result is not encouraging.  
                                                 
4All machine learning results, except for ME-C in Table 4, 
use 10-fold cross-validation. ?Accuracy? in tables is Predic-
tive Accuracy. 
755
 
 
 UNCLOSED (ME) CLOSED (ME-C) 
 Event-Event Event-Time Event-Event Event-Time 
Accuracy: 62.5 (51.6) 76.13 (65.3) 93.1 (75.2) 88.25 (62.3) 
Relation Prec Rec F Prec Rec F Prec Rec F Prec Rec F 
IBEFORE 50.00 27.27 35.39 0 0 0 77.78 60.86 68.29 0 0 0 
BEGINS 50.00 41.18 45.16 60.00 50.00 54.54 85.25 82.54 83.87 76.47 74.28 75.36 
ENDS 94.74 66.67 78.26 41.67 27.78 33.33 87.83 94.20 90.90 79.31 77.97 78.62 
SIMULTANEOUS 50.35 50.00 50.17 33.33 20.00 25.00 62.50 38.60 47.72 73.68 56.00 63.63 
INCLUDES 47.88 34.34 40.00 80.92 62.72 84.29 90.41 88.23 89.30 86.07 80.78 83.34 
BEFORE 68.85 79.24 73.68 70.47 62.72 66.37 94.95 97.26 96.09 90.16 93.56 91.83 
 
Table 2. Machine learning results using unclosed and closed data
 
3.2 Expanding Training Data using Tem-
poral Reasoning 
To expand our training set, we use a temporal  
closure component SputLink (Verhagen 2004), 
that takes known temporal relations in a text and  
derives new implied relations from them, in ef-
fect making explicit what was implicit. SputLink 
was inspired by (Setzer and Gaizauskas 2000) 
and is based on Allen?s interval algebra, taking 
into account the limitations on that algebra that 
were pointed out by (Vilain et al 1990). It is ba-
sically a constraint propagation algorithm that 
uses a transitivity table to model the composi-
tional behavior of all pairs of relations in a 
document. SputLink?s transitivity table is repre-
sented by 745 axioms. An example axiom:  
 
If relation(A, B) = BEFORE && 
   relation(B, C) = INCLUDES 
then infer relation(A, C) = BEFORE 
 
Once the TLINKs in each document in the 
corpus are closed using SputLink, the same vec-
tor generation procedure and feature representa-
tion described in Section 3.1 are used. The effect 
of closing the TLINKs on the corpus has a dra-
matic impact on learning. Table 2, in the 
CLOSED (ME-C) column shows that accura-
cies for this method (called ME-C, for Maximum 
Entropy learning with closure) are now in the 
high 80?s and low 90?s, and still outperform the 
closed majority class (shown in parentheses).  
What is the reason for the improvement?5 One 
reason is the dramatic increase in the amount of 
training data. The more connected the initial un-
                                                 
5Interestingly, performance does not improve for SIMUL-
TANEOUS.  The reason for this might be due to the rela-
tively modest increase in SIMULTANEOUS relations from 
applying closure (roughly factor of 2). 
closed graph for a document is in TLINKs, the 
greater the impact in terms of closure. When the 
OTC is closed, the number of TLINKs goes up 
by more than 11 times, from 6147 Event-Event 
and 4593 Event-Time TLINKs to 91,157 Event-
Event and 29,963 Event-Time TLINKs. The 
number of BEFORE links goes up from 3170 
(51.6%) Event-Event and 1229 Event-Time 
TLINKs (26.75%) to 68585 (75.2%) Event-
Event and 18665 (62.3%) Event-Time TLINKs, 
making BEFORE the majority class in the closed 
data for both Event-Event and Event-Time 
TLINKs. There are only an average of 0.84 
TLINKs per event before closure, but after clo-
sure it shoots up to 9.49 TLINKs per event. 
(Note that as a result, the majority class percent-
ages for the closed data have changed from the 
unclosed data.) 
Being able to bootstrap more training data is 
of course very useful. However, we need to dig 
deeper to investigate how the increase in data 
affected the machine learning. The improvement 
provided by temporal closure can be explained 
by three factors:  (1) closure effectively creates a 
new classification problem with many more in-
stances, providing more data to train on; (2) the 
class distribution is further skewed which results 
in a higher majority class baseline (3) closure 
produces additional data in such a way as to in-
crease the frequencies and statistical power of 
existing features in the unclosed data, as opposed 
to adding new features.  For example, with un-
closed data, given A BEFORE B and B BE-
FORE C, closure generates A BEFORE C which 
provides more significance for the features re-
lated to A and C appearing as first and second 
arguments, respectively, in a BEFORE relation.  
In order to help determine the effects of the 
above factors, we carried out two experiments in 
which we sampled 6145 vectors from the closed 
756
data ? i.e. approximately the number of Event-
Event vectors in the unclosed data.  This effec-
tively removed the contribution of factor (1) 
above. The first experiment (Closed Class Dis-
tribution) simply sampled 6145 instances uni-
formly from the closed instances, while the sec-
ond experiment (Unclosed Class Distribution) 
sampled instances according to the same distri-
bution as the unclosed data. Table 3 shows these 
results.  The greater class distribution skew in the 
closed data clearly contributes to improved accu-
racy. However, when using the same class distri-
bution as the unclosed data (removing factor (2) 
from above), the accuracy, 76%, is higher than 
using the full unclosed data.  This indicates that 
closure does indeed help according to factor (3). 
4 Comparison against Baselines 
4.1 Hand-Coded Rules 
Humans have strong intuitions about rules for 
temporal ordering, as we indicated in discussing 
sentences (1) to (3). Such intuitions led to the 
development of pattern matching rules incorpo-
rated in a TLINK tagger called GTag. GTag 
takes a document with TimeML tags, along with 
syntactic information from part-of-speech tag-
ging and chunking from Carafe, and then uses 
187 syntactic and lexical rules to infer and label 
TLINKs between tagged events and other tagged 
events or times. The tagger takes pairs of 
TLINKable items (event and/or time) and 
searches for the single most-confident rule to 
apply to it, if any, to produce a labeled TLINK 
between those items. Each (if-then) rule has a 
left-hand side which consists of a conjunction of 
tests based on TimeML-related feature combina-
tions (TimeML features along with part-of-
speech and chunk-related features), and a right-
hand side which is an assignment to one of the 
TimeML TLINK classes.  
The rule patterns are grouped into several dif-
ferent classes: (i) the event is anchored with or 
without a signal to a time expression within the 
same clause, e.g., (3c), (ii) the event is anchored 
without a signal to the document date (as is often 
the case for reporting verbs in news), (iii) an 
event is linked to another event in the same sen-
tence, e.g., (3c), and (iv) the event in a main 
clause of one sentence is anchored with a signal 
or tense/aspect cue to an event in the main clause 
of the previous sentence, e.g., (1-2), (3a-b). 
The performance of this baseline is shown in 
Table 4 (line GTag). The top most accurate rule 
(87% accuracy) was GTag Rule 6.6, which links 
a past-tense event verb joined by a conjunction to 
another past-tense event verb as being BEFORE 
the latter (e.g., they traveled and slept the 
night ..): 
 
If sameSentence=YES && 
 sentenceType=ANY && 
 conjBetweenEvents=YES && 
 arg1.class=EVENT && 
 arg2.class=EVENT && 
 arg1.tense=PAST && 
 arg2.tense=PAST && 
 arg1.aspect=NONE && 
 arg2.aspect=NONE && 
 arg1.pos=VB && 
 arg2.pos=VB && 
 arg1.firstVbEvent=ANY && 
 arg2.firstVbEvent=ANY  
then infer relation=BEFORE 
 
The vast majority of the intuition-bred rules 
have very low accuracy compared to ME-C, with 
intuitions failing for various feature combina-
tions and relations (for relations, for example, 
GTag lacks rules for IBEFORE, STARTS, and 
ENDS). The bottom-line here is that even when 
heuristic preferences are intuited, those prefer-
ences need to be guided by empirical data, 
whereas hand-coded rules are relatively ignorant 
of the distributions that are found in data. 
4.2 Adding Google-Induced Lexical Rules 
One might argue that the above baseline is too 
weak, since it doesn?t allow for a rich set of lexi-
cal relations. For example, pushing can result in 
falling, killing always results in death, and so 
forth. These kinds of defeasible rules have been 
investigated in the semantics literature, including 
the work of Lascarides and Asher cited in Sec-
tion 1.  
However, rather than hand-creating lexical 
rules and running into the same limitations as 
with GTag?s rules, we used an empirically-
derived resource called VerbOcean (Chklovski 
and Pantel 2004), available at 
http://semantics.isi.edu/ocean. This resource con-
sists of lexical relations mined from Google 
searches. The mining uses a set of lexical and 
syntactic patterns to test for pairs of verb 
strongly associated on the Web in an asymmetric 
?happens-before? relation. For example, the sys-
tem discovers that marriage happens-before di-
vorce, and that tie happens-before untie.  
We automatically extracted all the ?happens-
before? relations from the VerbOcean resource at 
the above web site, and then automatically con-
verted those relations to GTag format, producing 
4,199 rules. Here is one such converted rule: 
757
 
If arg1.class=EVENT && 
   arg2.class=EVENT && 
   arg1.word=learn && 
   arg2.word=forget && 
then infer relation=BEFORE 
 
Adding these lexical rules to GTag (with mor-
phological normalization being added for rule 
matching on word features) amounts to a consid-
erable augmentation of the rule-set, by a factor of 
22. GTag with this augmented rule-set might be 
a useful baseline to consider, since one would 
expect the gigantic size of the Google ?corpus? to 
yield fairly robust, broad-coverage rules.  
What if both a core GTag rule and a VerbO-
cean-derived rule could both apply? We assume 
the one with the higher confidence is chosen. 
However, we don?t have enough data to reliably 
estimate rule confidences for the original GTag 
rules; so, for the purposes of VerbOcean rule 
integration, we assigned either the original Ver-
bOcean rules as having greater confidence than 
the original GTag rules in case of a conflict (i.e., 
a preference for the more specific rule), or vice-
versa.  
 The results are shown in Table 4 (lines 
GTag+VerbOcean). The combined rule set, un-
der both voting schemes, had no statistically sig-
nificant difference in accuracy from the original 
GTag rule set. So, ME-C beat this baseline as 
well.  
The reason VerbOcean didn?t help is again 
one of data sparseness, due to most verbs occur-
ring rarely in the OTC. There were only 19 occa-
sions when a happens-before pair from VerbO-
cean correctly matched a human BEFORE 
TLINK, of which 6 involved the same rule being 
right twice (including learn happens-before for-
get, a rule which students are especially familiar 
with!), with the rest being right just once. There 
were only 5 occasions when a VerbOcean rule 
incorrectly matched a human BEFORE TLINK, 
involving just three rules. 
 
 
 Closed Class Distribution UnClosed Class Distribution 
Relation Prec Rec F Accuracy Prec Rec F Accuracy 
IBEFORE 100.0 100.0 100.0 83.33 58.82 68.96 
BEGINS 0 0 0 72.72 50.0 59.25 
ENDS 66.66 57.14 61.53 62.50 50.0 55.55 
SIMULTANEOUS 14.28 6.66 9.09 60.54 66.41 63.34 
INCLUDES 73.91 77.98 75.89 75.75 77.31 76.53 
BEFORE 90.68 92.60 91.63 
87.20  
(72.03) 
84.09 84.61 84.35 
76.0 
(40.95)  
Table 3. Machine Learning from subsamples of the closed data 
 
Accuracy Baseline 
Event-Event Event-Time 
GTag 63.43 72.46 
GTag+VerbOcean - GTag overriding VerbOcean 64.80 74.02 
GTag+VerbOcean - VerbOcean overriding GTag 64.22 73.37 
GTag+closure+ME-C 53.84 (57.00) 67.37 (67.59) 
Table 4. Accuracy of ?Intuition? Derived Baselines 
 
4.3 Learning from Hand-Coded Rules 
Baseline 
The previous baseline was a hybrid confi-
dence-based combination of corpus-induced 
lexical relations with hand-created rules for tem-
poral ordering. One could consider another obvi-
ous hybrid, namely learning from annotations 
created by GTag-annotated corpora. Since the 
intuitive baseline fares badly, this may not be 
that attractive. However, the dramatic impact of 
closure could help offset the limited coverage 
provided by human intuitions.   
Table 4 (line GTag+closure+ME-C) shows the 
results of closing the TLINKs produced by 
GTag?s annotation and then training ME from 
the resulting data. The results here are evaluated 
against a held-out test set. We can see that even 
after closure, the baseline of learning from un-
closed human annotations is much poorer than 
ME-C, and is in fact substantially worse than the  
majority class on event ordering.  
This means that for preprocessing new data 
sets to produce noisily annotated data for this 
classification task, it is far better to use machine-
learning from closed human annotations rather 
758
than machine-learning from closed annotations 
produced by an intuitive baseline. 
5 Related Work 
Our approach of classifying pairs independ-
ently during learning does not take into account 
dependencies between pairs.  For example, a 
classifier may label <X, Y> as BEFORE. Given 
the pair <X, Z>,  such a classifier has no idea if 
<Y, Z> has been classified as BEFORE, in 
which case, through closure, <X, Z> should be 
classified as BEFORE. This can result in the 
classifier producing an inconsistently annotated 
text. The machine learning approach of (Cohen 
et al 1999) addresses this, but their approach is 
limited to total orderings involving BEFORE, 
whereas TLINKs introduce partial orderings in-
volving BEFORE and five other relations. Future 
research will investigate methods for tighter in-
tegration of temporal reasoning and statistical 
classification. 
The only closely comparable machine-
learning approach to the problem of TLINK ex-
traction was that of (Boguraev and Ando 2005), 
who trained a classifier on Timebank 1.1 for 
event anchoring for events and times within the 
same sentence, obtaining an F-measure (for tasks 
A and B together) of 53.1. Other work in ma-
chine-learning and hand-coded approaches, 
while interesting, is harder to compare in terms 
of accuracy since they do not use common task 
definitions, annotation standards, and evaluation 
measures. (Li et al 2004) obtained 78-88% accu-
racy on ordering within-sentence temporal rela-
tions in Chinese texts. (Mani et al 2003) ob-
tained 80.2 F-measure training a decision tree on 
2069 clauses in anchoring events to reference 
times that were inferred for each clause. (Ber-
glund et al 2006) use a document-level evalua-
tion approach pioneered by (Setzer and Gai-
zauskas 2000), which uses a distinct evaluation 
metric. Finally, (Lapata and Lascarides 2004) use 
found data to successfully learn which (possibly 
ambiguous) temporal markers connect a main 
and subordinate clause, without inferring under-
lying temporal relations. 
In terms of hand-coded approaches, (Mani and 
Wilson 2000) used a baseline method of blindly 
propagating TempEx time values to events based 
on proximity, obtaining 59.4% on a small sample 
of 8,505 words of text. (Filatova and Hovy 2001) 
obtained 82% accuracy on ?timestamping? 
clauses for a single type of event/topic on a data 
set of 172 clauses. (Schilder and Habel 2001) 
report 84% accuracy inferring temporal relations 
in German data, and (Li et al 2001) report 93% 
accuracy on extracting temporal relations in Chi-
nese. Because these accuracies are on different 
data sets and metrics, they cannot be compared 
directly with our methods. 
Recently, researchers have developed other 
tools for automatically tagging aspects of Ti-
meML, including EVENT (Sauri et al 2005) at 
0.80 F-measure and TIMEX36 tags at 0.82-0.85 
F-measure. In addition, the TERN competition 
(tern.mitre.org) has shown very high (close to .95  
F-measures) for TIMEX2 tagging, which is fairly 
similar to TIMEX3. These results suggest the 
time is ripe for exploiting ?imperfect? features in 
our machine learning approach. 
6 Conclusion 
Our research has uncovered one new finding: 
semantic reasoning (in this case, logical axioms 
for temporal closure), can be extremely valuable 
in addressing data sparseness. Without it, per-
formance on this task of learning temporal rela-
tions is poor; with it, it is excellent. We showed 
that temporal reasoning can be used as an over-
sampling method to dramatically expand the 
amount of training data for TLINK labeling, re-
sulting in labeling predictive accuracy as high as 
93% using an off-the-shelf Maximum Entropy 
classifier. Future research will investigate this 
effect further, as well as examine factors that 
enhance or mitigate this effect in different cor-
pora. 
The paper showed that ME-C performed sig-
nificantly better than a series of increasingly so-
phisticated baselines involving expansion of 
rules derived from human intuitions. Our results 
in these comparisons confirm the lessons learned 
from the corpus-based revolution, namely that 
rules based on intuition alone are prone to in-
completeness and are hard to tune without access 
to the distributions found in empirical data.  
Clearly, lexical rules have a role to play in se-
mantic and pragmatic reasoning from language, 
as in the discussion of example (2) in Section 1. 
Such rules, when mined by robust, large corpus-
based methods, as in the Google-derived VerbO-
cean, are clearly relevant, but too specific to ap-
ply more than a few times in the OTC corpus.  
It may be possible to acquire confidence 
weights for at least some of the intuitive rules in 
GTag from Google searches, so that we have a 
                                                 
6http://complingone.georgetown.edu/~linguist/GU_TIME_
DOWNLOAD.HTML 
759
level field for integrating confidence weights 
from the fairly general GTag rules and the fairly 
specific VerbOcean-like lexical rules. Further, 
the GTag and VerbOcean rules could be incorpo-
rated as features for machine learning, along with 
features from automatic preprocessing.  
We have taken pains to use freely download-
able resources like Carafe, VerbOcean, and 
WEKA to help others easily replicate and 
quickly ramp up a system. To further facilitate 
further research, our tools as well as labeled vec-
tors (unclosed as well as closed) are available for 
others to experiment with. 
References 
James Allen. 1984. Towards a General Theory of Ac-
tion and Time.  Artificial Intelligence, 23, 2, 123-
154. 
Anders Berglund, Richard Johansson and Pierre 
Nugues. 2006. A Machine Learning Approach to 
Extract Temporal Information from Texts in Swed-
ish and Generate Animated 3D Scenes.  Proceed-
ings of EACL-2006. 
Branimir Boguraev and Rie Kubota Ando. 2005. Ti-
meML-Compliant Text Analysis for Temporal 
Reasoning. Proceedings of IJCAI-05, 997-1003. 
Timothy Chklovski and Patrick Pantel. 
2004.VerbOcean: Mining the Web for Fine-
Grained Semantic Verb Relations. Proceedings of 
EMNLP-04. http://semantics.isi.edu/ocean 
W. Cohen, R. Schapire, and Y. Singer. 1999. Learn-
ing to order things. Journal of Artificial Intelli-
gence Research, 10:243?270, 1999. 
Janet Hitzeman, Marc Moens and Clare Grover. 1995. 
Algorithms for Analyzing the Temporal Structure 
of Discourse. Proceedings of  EACL?95, Dublin, 
Ireland, 253-260. 
C.H. Hwang and L. K. Schubert. 1992. Tense Trees as 
the fine structure of discourse. Proceedings of 
ACL?1992, 232-240. 
Hans Kamp and Uwe Ryle. 1993. From Discourse to 
Logic (Part 2). Dordrecht: Kluwer. 
Andrew Kehler. 2000. Resolving Temporal Relations 
using Tense Meaning and Discourse Interpretation, 
in M. Faller, S. Kaufmann, and M. Pauly, (eds.), 
Formalizing the Dynamics of Information, CSLI 
Publications, Stanford. 
Mirella Lapata and Alex Lascarides. 2004. Inferring 
Sentence-internal Temporal Relations. In Proceed-
ings of the North American Chapter of the Assoca-
tion of Computational Linguistics, 153-160.  
Alex Lascarides and Nicholas Asher. 1993. Temporal 
Relations, Discourse Structure, and Commonsense 
Entailment. Linguistics and Philosophy 16, 437-
494. 
Wenjie Li, Kam-Fai Wong, Guihong Cao and Chunfa 
Yuan. 2004. Applying Machine Learning to Chi-
nese Temporal Relation Resolution. Proceedings of 
ACL?2004, 582-588. 
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.  
2003.  Inferring Temporal Ordering of Events in 
News. Short Paper. Proceedings of HLT-
NAACL'03, 55-57.  
Inderjeet Mani and George Wilson. 2000. Robust 
Temporal Processing of News.  Proceedings of 
ACL?2000. 
Rebecca J. Passonneau. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics, 14, 2, 1988, 44-60. 
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, David Day, Lisa Ferro, Robert Gai-
zauskas, Marcia Lazo, Andrea Setzer, and Beth 
Sundheim. 2003. The TimeBank Corpus. Corpus 
Linguistics, 647-656. 
James Pustejovsky, Bob Ingria, Roser Sauri, Jose 
Castano, Jessica Littman, Rob Gaizauskas, Andrea 
Setzer, G. Katz,  and I. Mani. 2005. The Specifica-
tion Language TimeML. In I. Mani, J. Pustejovsky, 
and R. Gaizauskas, (eds.), The Language of Time: 
A Reader. Oxford University Press.  
Roser Saur?, Robert Knippen, Marc Verhagen and 
James Pustejovsky. 2005. Evita: A Robust Event 
Recognizer for QA Systems. Short Paper. Proceed-
ings of HLT/EMNLP 2005: 700-707. 
Frank Schilder and Christof Habel. 2005. From tem-
poral expressions to temporal information: seman-
tic tagging of news messages. In I. Mani, J. Puste-
jovsky, and R. Gaizauskas, (eds.), The Language of 
Time: A Reader. Oxford University Press.  
Andrea Setzer and Robert Gaizauskas. 2000. Annotat-
ing Events and Temporal Information in Newswire 
Texts. Proceedings of LREC-2000, 1287-1294.  
Marc Verhagen. 2004. Times Between The Lines. 
Ph.D. Dissertation, Department of Computer Sci-
ence, Brandeis University. 
Marc Vilain, Henry Kautz, and Peter Van Beek. 1989. 
Constraint propagation algorithms for temporal 
reasoning: A revised report. In D. S. Weld and J. 
de Kleer (eds.), Readings in Qualitative Reasoning 
about Physical Systems, Morgan-Kaufman, 373-
381. 
Bonnie Webber. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14, 2, 1988, 61-73. 
760
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 1?8, Detroit, June 2005. c?2005 Association for Computational Linguistics
Weakly Supervised Learning Methods for Improving the Quality of 
Gene Name Normalization Data 
Ben Wellner 
wellner@mitre.org 
 
 The MITRE Corporation 
202 Burlington Rd 
Bedford MA 01730 
 
Computer Science Department 
Brandeis University 
Waltham MA 02454 
 
Abstract 
A pervasive problem facing many bio-
medical text mining applications is that of 
correctly associating mentions of entities 
in the literature with corresponding con-
cepts in a database or ontology.  Attempts 
to build systems for automating this proc-
ess have shown promise as demonstrated 
by the recent BioCreAtIvE Task 1B 
evaluation.  A significant obstacle to im-
proved performance for this task, how-
ever, is a lack of high quality training 
data. In this work, we explore methods for 
improving the quality of (noisy) Task 1B 
training data using variants of weakly su-
pervised learning methods. We present 
positive results demonstrating that these 
methods result in an improvement in 
training data quality as measured by im-
proved system performance over the same 
system using the originally labeled data. 
1 Introduction 
A primary set of tasks facing biomedical text proc-
essing systems is that of categorizing, identifying 
and classifying entities within the literature.  A key 
step in this process involves grouping mentions of 
entities together into equivalence classes that de-
note some underlying entity.  In the biomedical 
domain, however, we are fortunate to have struc-
tured data resources such as databases and ontolo-
gies with entries denoting these equivalence 
classes.  In biomedical text mining, then, this proc-
ess involves associating mentions of entities with 
known, existing unique identifiers for those entities 
in databases or ontologies ? a process referred to as 
normalization.  This ability is required for text 
processing systems to associate descriptions of 
concepts in free text with a grounded, organized 
system of knowledge more readily amenable to 
machine processing. 
The recent BioCreAtIvE Task 1B evaluation 
challenged a number of systems to identify genes 
associated with abstracts for three different organ-
isms: mouse, fly and yeast.  The participants were 
provided with a large set of noisy training data and 
a smaller set of higher quality development test 
data.  They were also provided with a lexicon con-
taining all the potential gene identifiers that might 
occur and a list of known, though incomplete, 
names and synonyms that refer to each of them.   
To prepare the training data, the list of unique 
gene identifiers associated with each full text arti-
cle was obtained from the appropriate model or-
ganism database.  However, the list had to be 
pruned to correspond to the genes mentioned in the 
abstract.  This was done by searching the abstract 
for each gene on the list or its synonyms, using 
exact string matching. This process has the poten-
tial to miss genes that were referred to in the ab-
stract using a phrase that does not appear in the 
synonym list.  Additionally, the list may be incom-
plete, because not all genes mentioned in the arti-
cle were curated, so there are mentions of genes in 
an abstract that did not have a corresponding iden-
tifier on the gene list. 
This paper explores a series of methods for at-
tempting to recover some of these missing gene 
1
identifiers from the Task 1B training data abstracts.  
We start with a robust, machine learning-based 
baseline system: a reimplementation of the system 
in [1].  Briefly, this system utilizes a classifier to 
select or filter matches made against the synonym 
list with a loose matching criterion.  From this 
baseline, we explore various methods for re-
labeling the noisy training data, resulting in im-
proved scores on the overall Task 1B development 
test and evaluation data.  Our methods are based on 
weakly supervised learning techniques such as co-
training [2] and self-training [3, 4] for learning 
with both labeled and unlabeled data.   
The setting here is different than the typical set-
ting for weakly supervised learning, however, in 
that we have a large amount of noisily labeled data, 
as opposed to completely unlabeled data.  The 
main contribution of this work is a framework for 
applying weakly supervised methods to this prob-
lem of re-labeling noisy training data.   
Our approach is based on partitioning the train-
ing data into two sets and viewing the problem as 
two mutually supporting weakly supervised learn-
ing problems. Experimental results demonstrate 
that these methods, carefully tuned, improve per-
formance for the gene name normalization task 
over those previously reported using machine 
learning-based techniques. 
2 Background and Related Work 
2.1 Gene Name Normalization and Extrac-
tion 
The task of normalizing and identifying biological 
entities, genes in particular, has received consider-
able attention in the biological text mining com-
munity.  The recent Task 1B from BioCreAtIvE 
[5] challenged systems to identify unique gene 
identifiers associated with paper abstracts from the 
literature for three organisms: mouse, fly and 
yeast.  Task 1A from the same workshop focused 
on identifying (i.e. tagging) mentions of genes in 
biomedical journal abstracts.  
2.2 NLP with Noisy and Un-labeled Training 
Data 
Within biomedical text processing, a number of 
approaches for both identification and normaliza-
tion of entities have attempted to make use of the 
many available structured biological resources to 
?bootstrap? systems by deriving noisy training data 
for the task at hand.  A novel method for using  
noisy (or ?weakly labeled?) training data from bio-
logical databases to learn to identify relations in 
biomedical texts is presented in [6].  Noisy training 
data was created in [7] to identify gene name men-
tions in text.  Similarly, [8] employed essentially 
the same approach using the FlyBase database to 
identify normalized genes within articles.   
2.3 Weakly Supervised Learning 
Weakly supervised learning remains an active area 
of research in machine learning.  Such methods are 
very appealing: they offer a way for a learning sys-
tem provided with only a small amount of labeled 
training data and a large amount of un-labeled data 
to perform better than using the labeled data alone.  
In certain situations (see [2]) the improvement can 
be substantial.   
Situations with small amounts of labeled data 
and large amounts of unlabeled data are very 
common in real-world applications where labeling 
large quantities of data is prohibitively expensive.  
Weakly supervised learning approaches can be 
broken down into multi-view and single-view 
methods.   
Multi-view methods [2] incrementally label 
unlabeled data as follows.  Two classifiers are 
trained on the training data with different ?views? 
of the data.  The different views are realized by 
splitting the set of features in such a way that the 
features for one classifier are conditionally inde-
pendent of features for the other given the class 
label.  Each classifier then selects the most confi-
dently classified instances from the unlabeled data 
(or some random subset thereof) and adds them to 
the training set.  The process is repeated until all 
data has been labeled or some other stopping crite-
rion is met.  The intuition behind the approach is 
that since the two classifiers have different views 
of the data, a new training instance that was classi-
fied with high confidence by one classifier (and 
thus is ?redundant? from that classifier?s point of 
view) will serve as an informative, novel, new 
training instance for the other classifier and vice-
versa.      
Single-view methods avoid the problem of find-
ing an appropriate feature split which is not possi-
ble or appropriate in many domains.  One common 
approach here [4] involves learning an ensemble of 
2
classifiers using bagging.  With bagging, the train-
ing data is randomly sampled, with replacement, 
with a separate classifier trained on each sample.  
Un-labeled instances are then labeled if all of the 
separate classifiers agree on the label for that in-
stance.  Other approaches are based on the expec-
tation maximization algorithm (EM) [9]. 
3 System Description 
The baseline version of our system is essentially a 
reproduction of the system described in [1] with a 
few modifications.   The great appeal of this sys-
tem is that, being machine learning based, it has no 
organism-specific aspects hard-coded in; moving 
to a new organism involves only re-training (as-
suming there is training data) and setting one or 
two parameters using a held-out data set or cross-
validation.   
The system is given a set of abstracts (and asso-
ciated gene identifiers at training time) and a lexi-
con.  The system first proposes candidate phrases 
based on all possible phrases up to 8 words in len-
gth with some constraints based on part-of-
speech1.  Matches against the lexicon are then car-
ried out by performing exact matching but ignoring 
case and removing punctuation from the both the 
lexical entries and candidate mentions.  Only maxi-
mal matching strings were used ? i.e. sub-strings of 
matching strings that match the same id are re-
moved.  
The resulting set of matches of candidate men-
tions with their matched identifiers results in a set 
of instances.  These instances are then provided 
with a label - ?yes? or ?no? depending on whether 
the match in the abstract is correct (i.e. if the gene 
identifier associated with the match was annotated 
with the abstract).  These instances are used to 
train a binary maximum entropy classifier that ul-
timately decides if a match is valid or not.   
Maximum entropy classifiers model the condi-
tional probability of a class, y, (in our setting, 
y=?yes? or y=?no?) given some observed data, x. 
The conditional probability has the following form 
in the binary case (where it is equivalent to logistic 
regression): 
                                                          
1
 Specifically, we excluded phrases that began with verbs 
prepositions, adverbs or determiners; we found this constraint 
did not affect recall while reducing the number of candidate 
mentions by more than 50%. 
)(
)),(exp(
)|(
xZ
yxf
xyP i
ii
 
=
?
 where Z(x) is the 
normalization function, the i? are real-valued 
model parameters and the if  are arbitrary real-
valued feature functions. 
One advantage of maximum entropy classifiers 
is the freedom to use large numbers of statistically 
non-independent features.  We used a number of 
different feature types in the classifier:  
 
? the matching phrase  
? the matched gene identifier  
? the previous and subsequent two words of the 
phrase 
? the number of words in the matching phrase  
? the total number of genes that matched against 
the phrase 
? all character prefixes and suffixes up to length 4 
for words within the phrase  
 
An example is shown below in Figure 1 below. 
 
Abstract Excerpt: 
 
?This new receptor, TOR (thymus or-
phan receptor)?? 
 
Feature Class Specific Feature 
Phrase TOR 
GENEID MGI104856 
Previous-1 , 
Previous-2 receptor 
Subsequent-1 ( 
Subsequent-2 thymus 
Number of Matches 2 
Number of Words 1 
Prefix-1 T 
Prefix-2 TO 
Prefix-3 TOR 
Suffix-1 R 
Suffix-2 OR 
Suffix-3 TOR 
 
Figure 1.  An abstract excerpt with the matching 
phrase ?TOR?.  The resulting features for the match 
are detailed in the table. 
 
In addition to these features we created addi-
tional features constituting conjunctions of some of 
these ?atomic? features.  For example, the con-
joined feature Phrase=TOR AND GE-
NEID=MGI104856 is ?on? when both conjuncts 
are true of the instance.   
To assign identifiers to a new abstract a set fea-
tures are extracted for each matching phrase and 
3
gene id pair just as in training (this constitutes an 
instance) and presented to the classifier for classi-
fication. As the classifier returns a probability for 
each instance, the gene id associated with the in-
stance with highest probability is returned as a 
gene id associated with the abstract, except in the 
case where the probability is less than some 
threshold 10, ?? TT  in which case no gene id is 
returned for that phrase. 
Training the model involves finding the pa-
rameters that maximize the log-likelihood of the 
training data.  As is standard with maximum en-
tropy models we employ a Gaussian prior over the 
parameters which bias them towards zero to reduce 
overfitting. 
Our model thus has just two parameters which 
need to be tuned to different datasets (i.e. different 
organisms): the Gaussian prior and the threshold, 
T .  Tuning the parameters can be done on a held 
out set (we used the Task 1B development data) or 
by cross validation:  
4 Weakly Supervised Methods for Re-
labeling Noisy Normalization Data 
The primary contribution of this work is a novel 
method for re-labeling the noisy training instances 
within the Task 1B training data sets.  Recall that 
the Task 1B training data were constructed by 
matching phrases in the abstract against the syno-
nym lists for the gene ids curated for the full text 
article for which the abstract was written.  In many 
cases, mentions of the gene in the abstract do not 
appear exactly as they do in the synonym list, 
which would result in a missed association of that 
gene id with the abstract.  In other cases, the data-
base curators simply did not curate a gene id men-
tioned in the abstract as it was not relevant to their 
particular line of interest.   
Our method for re-labeling potentially misla-
beled instances draws upon existing methods for 
weakly supervised learning.  We describe here the 
generic algorithm and include specific variations 
below in the experimental setup.   
The first step is to partition the training data 
into two disjoint sets, D1 and D2.2 We then create 
two instances of the weakly supervised learning 
                                                          
2
 Note that instances in D1 and D2 are also derived form dis-
joint sets of abstracts.  This helps ensure that very similar 
instances are unlikely to appear in different partitions. 
problem where in one instance, D1 is viewed as the 
labeled training data and D2 is viewed as the unla-
beled data, and in the other instance their roles are 
reversed.  Re-labeling of instances in D1 is carried 
out by a classifier or ensemble of classifiers, C2 
trained on D2.  Similarly, instances in D2 are re-
labeled by C1 trained on D1.  Those instances for 
which the classifier assigns high confidence (i.e. 
for which )|""( xyesyP = is high) but for which 
the existing label disagrees with the classifier are 
candidates for re-labeling.   Figure 2 diagrams this 
process below. 
 
 
 
Figure 2.  Diagram illustrating the method for re-
labeling instances.  The solid arrows indicate the 
training of a classifier from some set of data, while 
block arrows describe the data flow and re-labeling 
of instances. 
 
One assumption behind this approach is that not 
all of the errors in the training data labels are corre-
lated.  As such, we would expect that for a particu-
lar mislabeled instance in D1, there may be similar 
positive instances in D2 that provide evidence for 
re-labeling the mislabeled in D1.  
Initial experiments using this approach met 
with failure or negligible gains in performance.  
We initially attributed this to too many correlated 
errors.  Detailed error analysis revealed, however, 
that a significant portion of training instances be-
ing re-labeled were derived from matches against 
the lexicon that were not, in fact, references to 
genes ? i.e. they were other more common English 
words that happened to appear in the synonym lists 
for which the classifier mistakenly assigned them 
high probability.  
   D1     D2 
C2 C1 
   D2?    D1? 
  Final Classifier 
Original 
Training Data 
Modified Train-
ing Data 
Re-labeling 
classifiers 
4
Our solution to this problem was to impose a 
constraint on instances to be re-labeled:  The 
phrase in the abstract associated with the instance 
is required to have been tagged as a gene name by 
a gene name tagger in addition to the instance re-
ceiving a high probability by the re-labeling classi-
fier.  Use of a gene name tagger introduces a check 
against the classifier (trained on the noisy training 
data) and helps to reduce the chance of introducing 
false positives into the labeled data.   
We trained our entity tagger, Carafe, on a the 
Genia corpus [10] together with the BioCreative 
Task 1A gene name training corpus.  Not all of the 
entity types annotated in the Genia corpus are 
genes, however. Therefore we used an appropriate 
subset of the entity types found in the corpus.  Ca-
rafe is based on Conditional Random Fields [11] 
(CRFs) which, for this task, employed a similar set 
of features to the CRF described in [12].   
5 Experiments and Results 
The main goal of our experiments was to demon-
strate the benefits of re-labeling potentially noisy 
training instances in the task 1B training data.  In 
this work we focus the weakly supervised re-
labeling experiments on the mouse data set.  In the 
mouse data there is a strong bias towards false 
negatives in the training data ? i.e. many training 
instances have a negative label and should have a 
positive one.  Our reasons for focusing on this data 
are twofold: 1) we believe this situation is likely to 
be more common in practice since an organism 
may have impoverished synonym lists or ?gaps? in 
the curated databases and 2) the experiments and 
resulting analyses are made clearer by focusing on 
re-labeling instances in one direction only (i.e. 
from negative to positive). 
In this section, we first describe an initial ex-
periment comparing the baseline system (described 
above) using the original training data with a ver-
sion trained with an augmented data set where la-
bels changed based on a simple heuristic.  We then 
describe our main body of experiments using vari-
ous weakly supervised learning methods for re-
labeling the data.  Finally, we report our overall 
scores on the evaluation data for all three organ-
isms using the best system configurations derived 
from the development test data. 
5.1 Data and Methodology 
We used the BioCreative Task 1B data for all our 
experiments.  For the three data sets, there were 
5000 abstracts of training data and 250, 110 and 
108 abstracts of development test data for mouse, 
fly and yeast, respectively.  The final evaluation 
data consisted of 250 abstracts for each organism.  
In the training data, the ratios of positive to nega-
tive instances are the following: for mouse: 
40279/111967, for fly: 75677/493959 and for 
yeast: 25108/3856.  The number of features in each 
trained model range from 322110 for mouse,  
881398 for fly and 108948 for yeast.  
Given a classifier able to rank all the test in-
stances (in our case, the ranks derive from the 
probabilities output by the maximum entropy clas-
sifier), we return only the top n gene identifiers, 
where n is the number of correct identifiers in the 
development test data ? this results in a balanced 
F-measure score.  We use this metric for all ex-
periments on the development test data as it allows 
better comparison between systems by factoring 
out the need to tune the threshold.   
On the evaluation data, we do not know n. The 
system returns a number of identifiers based on the 
threshold, T.  For these experiments, we set T on 
the development test data and choose three appro-
priate values for three different evaluation ?sub-
missions?. 
5.2 Experiment Set 1: Effect of match-based 
re-labeling 
Our first set of experiments uses the baseline sys-
tem described earlier.  We compare the results of 
this system using the Task 1B training data ?as 
provided? with the results obtained by re-labeling 
some of the negative instances provided to the 
classifier as positive instances.   We re-labeled any 
instances as positive that matched a gene identifier 
associated with the abstract regardless of the (po-
tentially incorrect) label associated with the identi-
fier. The Task 1B dataset creators marked an 
identifier ?no? if an exact lexicon match wasn?t 
found in the abstract.  As our system matching 
phase is a bit different (i.e. we remove punctuation 
and ignore case), this amounts to re-labeling the 
training data using this looser criterion. The results 
of this match-based re-labeling are shown in Table 
1 below. 
 
5
 Baseline Re-labeled 
Mouse 68.8 72.0 
Fly 70.8 75.3 
Yeast 89.7 90.0 
 
Table 1 Balanced F-measure scores comparing the 
baseline vs. a system trained with the match-based 
re-labeled instances on the development test data. 
5.3 Experiment Set 2: Effect of Weakly Su-
pervised Re-labeling 
In our next set of experiments we tested a number 
of different weakly supervised learning configura-
tions.  These different methods simply amount to 
different rankings of the instances to re-label 
(based on confidence and the gene name tags).  
The basic algorithm (outlined in Figure 1) remains 
the same in all cases. Specifically, we investigated 
three methods for ranking the instances to re-label: 
1) na?ve self-training, 2) self-training with bagging, 
and 3) co-training.  
Na?ve self-training consisted of training a single 
maximum entropy classifier with the full feature 
set on each partition and using it to re-label in-
stances from the other partition based on confi-
dence.   
Self training with bagging followed the same 
idea but used bagging.  For each partition, we 
trained 20 separate classifiers on random subsets of 
the training data using the full feature set. The con-
fidence assigned to a test instance was then defined 
as the product of the confidences of the individual 
classifiers.   
Co-training involved training two classifiers for 
each partition with feature split.  We split the fea-
tures into context-based features such as the sur-
rounding words and the number of gene ids 
matching the current phrase, and lexically-based 
features that included the phrase itself, affixes, the 
number of tokens in the phrase, etc.  We computed 
the aggregated confidences for each instance as the 
product of the confidences assigned by the result-
ing context-based and lexically-based classifiers. 
We ran experiments for each of these three op-
tions both with the gene tagger and without the 
gene tagger.  The systems that included the gene 
tagger ranked all instances derived from tagged 
phrases above all instances derived from phrases 
that were not tagged regardless of the classifier 
confidence.  
A final experimental condition we explored was 
comparing batch re-labeling vs. incremental re-
labeling.  Batch re-labeling involved training the 
classifiers once and re-labeling all k instances us-
ing the same classifier.  Incremental re-labeling 
consisted of iteratively re-labeling n instances over 
k/n epochs where the classifiers were re-trained on 
each epoch with the newly re-labeled training data. 
Interestingly, incremental re-labeling did not per-
form better than batch re-labeling in our experi-
ments.  All results reported here, therefore, used 
batch re-labeling. 
After the training data was re-labeled, a single 
maximum entropy classifier was trained on the 
entire (now re-labeled) training set.  This resulting 
classifier was then applied to the development set 
in the manner described in Section 3. 
 
MAX With Tagger Without Tagger 
Self-Na?ve 74.4 (4000) 72.3 (5000) 
Self-Bagging 74.8 (4000) 73.5 (6000) 
Co-Training 74.6 (4000) 72.7 (6000) 
 
AVG With Tagger Without Tagger 
Self-Na?ve 72.2 71.2 
Self-Bagging 72.2 71.5 
Co-Training 71.9 71.2 
 
Table 2.  Maximum and average balanced f-measure 
scores on the mouse data set for each of the six sys-
tem configurations for all values of k ? the number of 
instances re-labeled.  The numbers in parentheses 
indicate for which value of k the maximum value was 
achieved. 
 
We tested each of these six configurations for 
different values of k, where k is the total number of 
instances re-labeled3.  Table 2 highlights the maxi-
mum and average balanced f-measure scores 
across all values of k for the different system con-
figurations. Both the maximum and averaged 
scores appear noticeably higher when constraining 
the instances to re-label with the tagger.  The three 
weakly supervised methods perform comparably 
with bagging performing slightly better.  
                                                          
3
 The values of k considered here were: 0, 10, 20, 50, 100, 
200, 300, 500, 800, 1000, 2000, 3000, 4000, 5000, 6000, 
7000, 8000, 9000, 10000, 12000 and 15000. 
6
  
 
Figure 3.  The top graph shows balanced F-measure 
scores against the number of instances re-labeled 
when using the tagger as a constraint.  The bottom 
graph compares the re-labeling of instances with the 
gene tagger as a constraint and without.  
 
In order to gain further insight into re-labeling in-
stances, we have plotted the balanced F-measure 
performance on the development test for various 
values of k.  The upper graph indicates that the 
three different methods correlate strongly.  The 
bottom graph makes apparent the benefits of tag-
ging as a constraint.  It also points to the weakness 
of the tagger, however.  At k=7000 and k=8000, 
the system tends to perform worse when using the 
tags as a constraint.  This indicates that tagger re-
call errors have the potential to filter out good can-
didates for re-labeling. 
Another observation from the graphs is that per-
formance actually drops for small values of k.  This 
would imply that many of the instances the classi-
fiers are most confident about re-labeling are in 
fact spurious.  To support this hypothesis, we 
trained the baseline system on the entire training 
set and computed its calibration error on the de-
velopment test data. The calibration error measures 
how ?realistic? the probabilities output by the clas-
sifier are.  See [13] for details. 
 
Figure 4.  Classifier calibration error on the devel-
opment test data. 
 
Figure 4 illustrates the estimated calibration er-
ror at different thresholds.  As can be seen, the er-
ror is greatest for high confidence values indicating 
that the classifier is indeed very confidently pre-
dicting an instance as positive when it is negative.  
Extrapolating this calibration error to the re-labling 
classifiers (each trained on one half of the training 
data) offers some explanation as to why re-labeling 
starts off so poorly.  The error mass is exactly 
where we do not want it - at the highest confidence 
values.  This also offers an explanation as to why 
incremental re-labeling did not help. Fortunately, 
introducing a gene tagger as a constraint mitigates 
this problem. 
5.4 Experiment Set 3: Final Evaluation 
We report our results using the best overall system 
configurations on the Task 1B evaluation data.  We 
?submitted? 3 runs for two different mouse con-
figurations and one for both fly and yeast.  The 
highest scores over the 3 runs are reported in Table 
3.  MouseWS used the best weakly supervised 
method as determined on the development test 
data: bagging with k=4000.  MouseMBR, Ye-
astMBR and FlyMBR used match-based re-labeling 
described in Section 5.2. The Gaussian prior was 
set to 2.0 for all runs and the 3 submissions for 
each configuration only varied in the threshold 
value T.  
 
 F-measure Precision Recall 
MouseWS 0.784 0.81 0.759 
MouseMBR 0.768 0.795 0.743 
FlyMBR 0.767 0.767 0.767 
YeastMBR 0.902 0.945 0.902 
 
Table 3.  Final evaluation results.  
 
7
These results are competitive compared with 
the BioCreAtIvE Task 1B results where the highest 
F-measures for mouse, fly and yeast were 79.1, 
81.5 and 92.1 with the medians at 73.8, 66.1 and 
85.8, respectively.  The results for mouse and fly 
improve upon previous best reported results with 
an organism invariant, automatic system [1]. 
6 Conclusions 
The quality of training data is paramount to the 
success of fully automatic, organism invariant ap-
proaches to the normalization problem.  In this pa-
per we have demonstrated the utility of weakly 
supervised learning methods in conjunction with a 
gene name tagger for re-labeling noisy training 
data for gene name normalization.  The result be-
ing higher quality data with corresponding higher 
performance on the BioCreAtIvE Task 1B gene 
name normalization task.   
Future work includes applying method outlined 
here for correcting noisy data to other classifica-
tion problems. Doing so generally requires an in-
dependent ?filter? to restrict re-labeling ? the 
equivalent of the gene tagger used here.  We also 
have plans to improve classifier calibration. Inte-
grating confidence estimates produced by the gene 
name tagger, following [14], is another avenue for 
investigation. 
Acknowledgements 
We thank Alex Morgan, Lynette Hirschman, Marc Colosimo, 
Jose Castano and James Pustejovsky for helpful comments 
and encouragement.  This work was supported under MITRE 
Sponsored Research 51MSR123-A5. 
References 
 
1. Crim, J., R. McDonald, and F. Pereira. Auto-
matically Annotating Documents with Normal-
ized Gene Lists. in EMBO Workshop - A 
critical assessment of text mining methods in 
molecular biology. 2004. Granada, Spain. 
2. Blum, A. and T. Mitchell. Combining Labeled 
and Unlabeled Data with Co-training. 1998. 
Proceedings of the Workshop on Computa-
tional Learning Theory: Morgan Kaufmann. 
3. Banko, M. and E. Brill. Scaling to very very 
large corpora for natural language disam-
biguation. in ACL/EACL. 2001. 
4. Ng, V. and C. Cardie. Weakly Supervised 
Natural Language Learning Without Redun-
dant Views. in Human Language Technology 
Conference of the North American Chapter of 
the Association for Computational Linguistics 
(HLT/NAACL). 2003. 
5. Hirschman, L., et al, Overview of BioCreAtIvE 
task 1B: Normalized Gene Lists. BioMed Cen-
tral BioInformatics, 2005(Special Issue on 
BioCreAtIvE). 
6. Craven, M. and J. Kumlien, Constructing Bio-
logical Knowledge Bases by Extracting Infor-
mation from Text Sources. 1999: p. 77-86. 
7. Morgan, A., et al, Gene Name Extraction Us-
ing FlyBase Resources. ACL Workshop on 
Natural Language Processing in Biomedicine, 
2003. 
8. Morgan, A.A., et al, Gene name identification 
and normalization using a model organism da-
tabase. J Biomed Inform, 2004. 37(6): p. 396-
410. 
9. Nigam, K. and R. Ghani. Analyzing the effec-
tiveness and applicability of co-training. in In-
formation and Knowledge Management. 2000. 
10. Kim, J.-D., et al, GENIA Corpus -- a semanti-
cally annotated corpus for bio-text mining. 
Bioinformatics, 2003. 19((Supppl 1)): p. 180-
182. 
11. Lafferty, J., A. McCallum, and F. Pereira. 
Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence 
Data. in 18th International Conf. on Machine 
Learning. 2001. San Francisco, CA: Morgan 
Kaufmann. 
12. McDonald, R. and F. Pereira. Identifying Gene 
and Protein Mentions in Text Using Condi-
tional Random Fields. in A critical assessment 
of text mining methods in molecular biology, 
BioCreative 2004. 2004. Grenada, Spain. 
13. Cohen, I. and M. Goldszmidt. Properties and 
Benefits of Calibrated Classifiers. in 
EMCL/PKDD. 2004. Pisa, Italy. 
14. Culotta, A. and A. McCallum. Confidence Es-
timation for Information Extraction. in Pro-
ceedings of Human Language Technology 
Conference and North American Chapter of 
the Association for Computational Linguistics 
(HLT-NAACL). 2004. Boston, MA. 
 
 
8
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 23?29,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Pilot Study on Acquiring Metric Temporal Constraints for Events 
Inderjeet Mani and Ben Wellner 
The MITRE Corporation 
202 Burlington Road, Bedford, MA 01730, USA 
and 
Department of Computer Science, Brandeis University 
415 South St., Waltham, MA 02254, USA 
{imani, wellner}@mitre.org 
  
 
Abstract 
Previous research on temporal anchoring 
and ordering has focused on the annota-
tion and learning of temporal relations 
between events. These qualitative rela-
tions can be usefully supplemented with 
information about metric constraints, 
specifically as to how long events last. 
This paper describes the first steps in ac-
quiring metric temporal constraints for 
events. The work is carried out in the 
context of the TimeML framework for 
marking up events and their temporal re-
lations. This pilot study examines the fea-
sibility of acquisition of metric temporal 
constraints from corpora.  
1 Introduction 
The growing interest in practical NLP applica-
tions such as question-answering and text sum-
marization places increasing demands on the 
processing of temporal information. In multi-
document summarization of news articles, it can 
be useful to know the relative order of events so 
as to merge and present information from multi-
ple news sources correctly. In question-
answering, one would like to be able to ask when 
an event occurs, or what events occurred prior to 
a particular event. A wealth of prior research by 
(Passoneau 1988), (Webber 1988), (Hwang and 
Schubert 1992), (Kamp and Reyle 1993), (Las-
carides and Asher 1993), (Hitzeman et al 1995), 
(Kehler 2000) and others, has explored the dif-
ferent knowledge sources used in inferring the 
temporal ordering of events, including temporal 
adverbials, tense, aspect, rhetorical relations, 
pragmatic conventions, and background knowl-
edge. For example, the narrative convention of 
events being described in the order in which they 
occur is followed in (1), but overridden by means 
of a discourse relation, Explanation in (2).  
 
(1) Max stood up. John greeted him.  
(2) Max fell. John pushed him. 
 
While there has been a spurt of recent research 
addressing the event ordering problem, e.g., 
(Mani and Wilson 2000) (Filatova and Hovy 
2001) (Schilder and Habel 2001) (Li et al 2001) 
(Mani et al 2003) (Li et al 2004) (Lapata and 
Lascarides 2004) (Boguraev and Ando 2005) 
(Mani et al 2006), that research relies on qualita-
tive temporal relations. Qualitative relations (e.g., 
event A BEFORE event B, or event A DURING 
time T) are certainly of interest in developing 
timelines of events in news and other genres. 
However, metric constraints can also be poten-
tially useful in this ordering problem. For exam-
ple, in (3), it can be crucial to know whether the 
bomb landed a few minutes to hours or several 
years BEFORE the hospitalization. While hu-
mans have strong intuitions about this from 
commonsense knowledge, machines don?t. 
 
 (3) An elderly Catholic man was 
hospitalized from cuts after a Prot-
estant gasoline bomb landed in his 
back yard.  
 
Fortunately, there are numerous instances such 
as (4), where metric constraints are specified ex-
plicitly: 
 
 (4) The company announced Tuesday 
that third quarter sales had fallen. 
  
In (4), the falling of sales occurred over the 
three-month period of time inferable from the 
speech time. However, while the announcement 
is anchored to a day inferable from the speech 
23
time, the length of the announcement is not 
specified.  
These examples suggest that it may be possi-
ble to mine information from a corpus to fill in 
extents for the time intervals of and between 
events, when these are either unspecified or par-
tially specified. Metric constraints can also po-
tentially lead to better qualitative links, e.g., 
events with long durations are more likely to 
overlap with other events.  
This paper describes some preliminary ex-
periments to acquire metric constraints. The ap-
proach extends the TimeML representation 
(Pustejovsky et al 2005) to include such con-
straints. We first translate a TimeML representa-
tion with qualitative relations into one where 
metric constraints are added. This representation 
may tell us how long certain events last, and the 
length of the gaps between them, given the in-
formation in the text. However, the information 
in the text may be incomplete; some extents may 
be unknown. We therefore need an external 
source of knowledge regarding the typical ex-
tents of events, which we can use when the text 
doesn?t provide it. We accordingly describe an 
initial attempt to bootstrap event durations from 
raw corpora as well as corpora annotated with 
qualitative relations.   
2 Annotation Scheme and Corpora 
TimeML (Pustejovsky et al 2005) 
(www.timeml.org) is an annotation scheme for 
markup of events, times, and their qualitative 
temporal relations in news articles. The TimeML 
scheme flags tensed verbs, adjectives, and nomi-
nals with EVENT tags with various attributes, 
including the class of event, tense, grammatical 
aspect, polarity (negative or positive), any modal 
operators which govern the event being tagged, 
and cardinality of the event if it?s mentioned 
more than once. Likewise, time expressions are 
flagged and their values normalized, based on an 
extension of the ACE (2004) (tern.mitre.org) 
TIMEX2 annotation scheme (called TIMEX3).  
For temporal relations, TimeML defines a 
TLINK tag that links tagged events to other 
events and/or times. For example, given sentence 
(4), a TLINK tag will anchor the event instance 
of announcing to the time expression Tuesday 
(whose normalized value will be inferred from 
context), with the relation IS_INCLUDED. This 
is shown in (5). 
 
(5) The company <EVENT even-
tID=e1>announced</EVENT> <TIMEX3 
tid=t1 value=1998-01-08>Tuesday 
</TIMEX3> that <TIMEX3 tid=t2 
value=P1Q3 beginPoint=t3 end-
Point=t4>third-quarter</TIMEX3> 
sales <EVENT eventID=e2> had 
fallen</EVENT>.  
<TLINK eventID=e1 relatedToEven-
tID=e2 relType=AFTER/> 
<TLINK eventID=e1 relatedToTimeID=t1 
relType=IS_INCLUDED/> 
<TIMEX3 tid=t3 value=1997-07/> 
<TIMEX3 tid=t4 value=1997-09/> 
 
The representation of time expressions in Ti-
meML uses TIMEX2, which is an extension of 
the TIMEX2 scheme (Ferro et al 2005). It repre-
sents three different kinds of time values: points 
in time (answering the question ?when??), dura-
tions (answering ?how long??), and frequencies 
(answering ?how often??)1.  
TimeML uses 14 temporal relations in the 
TLINK relTypes. Among these, the 6 inverse 
relations are redundant. In order to have a non-
hierarchical classification, SIMULTANEOUS 
and IDENTITY are collapsed, since IDENTITY 
is a subtype of SIMULTANEOUS. (An event or 
time is SIMULTANEOUS with another event or 
time if they occupy the same time interval. X and 
Y are IDENTICAL if they are simultaneous and 
coreferential). DURING and IS_INCLUDED are 
collapsed since DURING is a subtype of 
IS_INCLUDED that anchors events to times that 
are durations. (An event or time INCLUDES an-
other event or time if the latter occupies a proper 
subinterval of the former.) IBEFORE (immedi-
ately before) corresponds to MEETS in Allen?s 
interval calculus (Allen 1984). Allen?s OVER-
LAPS relation is not represented in TimeML.  
The above considerations allow us to collapse 
the TLINK relations to a disjunctive classifica-
tion of 6 temporal relations TRels = {SIMUL-
TANEOUS, IBEFORE, BEFORE, BEGINS, 
ENDS, INCLUDES}. These 6 relations and their 
inverses map one-to-one to 12 of Allen?s 13 ba-
sic relations (Allen 1984).  
Formally, each TLINK is a constraint of the 
general form x R y, where x and y are intervals, 
and R is a disjunct ?i=1,..,6(ri), where  ri is a rela-
tion in TRels. In annotating a document for Ti-
                                                 
1 Our representation (using t3 and t4) grounds the fuzzy 
primitive P1Q3 (i.e., a period of one 3rd-quarter) to specific 
months, though this is an application-specific step. In ana-
lyzing our data, we normalize P1Q3 as P3M (i.e., a period 
of 3 months). For conciseness, we omit TimeML EVENT 
and TIMEX3 attributes that aren?t relevant to the discus-
sion. 
24
meML, the annotator adds a TLINK iff she can 
commit to the TLINK relType being unambigu-
ous, i.e., having exactly one relType r. 
Two human-annotated corpora have been re-
leased based on TimeML2: TimeBank 1.2 (Puste-
jovsky et al 2003) with 186 documents and 
64,077 words of text, and the Opinion Corpus 
(www.timeml.org), with 73 documents and 
38,709 words. TimeBank 1.2 (we use 1.2.a) was 
created in the early stages of TimeML develop-
ment, and was partitioned across five annotators 
with different levels of expertise. The Opinion 
Corpus was developed recently, and was parti-
tioned across just two highly trained annotators, 
and could therefore be expected to be less noisy. 
In our experiments, we merged the two datasets 
to produce a single corpus, called OTC. 
3 Translation 
3.1 Introduction 
The first step is to translate a TimeML rep-
resentation with qualitative relations into one 
where metric constraints are added. This transla-
tion needs to produce a consistent metric repre-
sentation. The temporal extents of events, and 
between events, can be read off, when there are 
no unknowns, from the metric representation. 
The problem, however is that the representation 
may have unknowns, and the extents may not be 
minimal. 
3.2 Mapping to Metric Representation 
Let each event or time interval x be repre-
sented as a pair of start and end time points <x1, 
x2>. For example, given sentence (4), and the 
TimeML representation shown in (5), let x be 
fall and y be announce. Then, we have x1 = 
19970701T00, x2 = 19970930T23:59, y1 = 
19980108Tn1, y2 = 19980108Tn2 (here T repre-
sents time of day in hours). 
To add metric constraints, given a pair of 
events or times x and y, where x=<x1, x2> and 
y=<y1, y2>, we need to add, based on the quali-
tative relation between x and y, constraints of the 
general form (xi-yj) ? n, for 1 ? i, j ?2. We fol-
low precisely the method ?Allen-to-metric? of 
(Kautz and Ladkin 1991) which defines metric 
constraints for each relation R in TRels. For ex-
ample, here is a qualitative relation and its metric 
constraints: 
 
(6) x is BEFORE y iff (x2-y1) < 0.  
                                                 
2More details can be found at timeml.org. 
 
In our example, where x is fall and y is the 
announce, we are given the qualitative relation-
ship that x is BEFORE Y, so the metric con-
straint (x2-y1) < 0 can be asserted. 
Consider another qualitative relation and its 
metric constraint:  
 
(7) z INCLUDES y iff (z1-y1) < 0 and 
(y2-z2) < 0. 
 
Let y be announce in (4), as before, and let 
z=<z1, z2> be the time of Tuesday, where z1 = 
19980108T00, and z2 = 19980108T23:59. Since 
we are given the qualitative relation y 
IS_INCLUDED z, the metric constraints (z1-y1) 
< 0 and (y2-z2) < 0 can be asserted. 
3.3 Consistency Checking 
We now turn to the general problem of check-
ing consistency. The set of TLINKs for a docu-
ment constitutes a graph, where the nodes are 
events or times, and the edges are TLINKs. 
Given such a TimeML-derived graph for a 
document, a temporal closure algorithm (Verha-
gen 2005) carries out a transitive closure of the 
graph. The transitive closure algorithm was in-
spired by (Setzer and Gaizauskas 2000) and is 
based on Allen?s interval algebra, taking into 
account the limitations on that algebra that were 
pointed out by (Vilain et al 1990). It is basically 
a constraint propagation algorithm that uses a 
transitivity table to model the compositional be-
havior of all pairs of relations in a document. The 
algorithm?s transitivity table is represented by 
745 axioms. An example axiom is shown in (8):  
 
(8) If relation(A, B) = BEFORE && 
   relation(B, C) = INCLUDES 
then infer relation(A, C) = BEFORE. 
 
In propagating constraints, links added by clo-
sure can have a disjunction of one or more rela-
tions in TRels. When the algorithm terminates, 
any TLINK with more than one disjunct is dis-
carded. Thus, a closed graph is consistent and 
has a single relType r in TRels for each TLINK 
edge. The algorithm runs in O(n3) time, where n 
is the number of intervals.  
The closed graph is augmented so that when-
ever input edges a r1 b and b r2 c are composed to 
yield the output edge a r3 c, where r1, r2, and r3 
are in TRels, the metric constraints for r3 are 
added to the output edge. To continue our exam-
ple, since the fall x is BEFORE the Tuesday z 
25
and z INCLUDES y (announce), we can infer, 
using rule 8, that x is BEFORE y, i.e., that fall 
precedes announce. Using rule (6), we can again 
assert that (x2-y1) < 0. 
3.4 Reading off Temporal Extents 
 
Figure 1. Metric Constraints 
 
We now have the metric constraints added to 
the graph in a consistent manner. It remains to 
compute, given each event or time x=<x1, x2>, 
the values for x1 and x2. In our example, we 
have fall x=<19970701T00, 19970930T23:59>, 
announce y=<19980108Tn1, 19980108Tn2>, 
and Tuesday z=<19980108T00, 
19980108T23:59>, and the added metric con-
straints that (x2-y1), (z1-y1), and (y2-z2) are all 
negative. Graphically, this can be pictured as in 
Figure 1.  
As can be see in Figure 1, there are still un-
knowns (n1 and n2): we aren?t told exactly how 
long announce lasted -- it could be anywhere up 
to a day. We therefore need to acquire informa-
tion about how long events last when the exam-
ple text doesn?t tell us. We now turn to this prob-
lem. 
4 Acquisition 
We started with the 4593 event-time TLINKs 
we found in the unclosed human-annotated OTC. 
From these, we restricted ourselves to those 
where the times involved were of type TIMEX3 
DURATION. We augmented the TimeBank data 
with information from the raw (un-annotated) 
British National Corpus.  We tried a variety of 
search patterns to try and elicit durations, finally 
converging on the single pattern ?lasted?. There 
were 1325 hits for this query in the BNC. (The 
public web interface to the BNC only shows 50 
random results at a time, so we had to iterate.) 
The retrieved hits (sentences and fragments of 
sentences) were then processed with components 
from the TARSQI toolkit (Verhagen et al 2005) 
to provide automatic TimeML annotations. The 
TLINKs between events and times that were 
TIMEX3 DURATIONS were then extracted. 
These links were then corrected and validated by 
hand and then added to the OTC data to form an 
integrated corpus. An example from the BNC is 
shown in (9). 
 
(9) The <EVENT>storm</EVENT> 
<EVENT>lasted</EVENT> <TIMEX3 
VAL="P5D">five days</TIMEX3>.   
 
Next, the resulting data was subject to mor-
phological normalization in a semi-automated 
fashion to generate more counts for each event. 
Hyphens were removed, plurals were converted 
to singular forms, finite verbs to infinitival forms, 
and gerundive nominals to verbs. Derivational 
ending on nominals were stripped and the corre-
sponding infinitival verb form generated. These 
normalizations are rather aggressive and can lead 
to loss of important distinctions. For example, 
sets of events (e.g., storms or bombings) as a 
whole can have much longer durations compared 
to individual events. In addition, no word-sense 
disambiguation was carried out, so different 
senses of a given verb or event nominal may be 
confounded together.  
5 Results 
 
Number of 
durations 
Number of 
Events 
Normalized 
Form of 
Event 
26 1 lose 
16 1 earn 
10 1 fall 
9 1 rise 
8 1 drop 
7 2 decline, in-
crease 
6 4 end, grow, 
say, sell 
5 2 income, 
stop 
4 9 ? 
3 17 ? 
2 40 ? 
1 176 ? 
Table 1. Frequencies of event durations 
 
The resulting dataset had 255 distinct events 
with the number of durations for the events as 
shown in the frequency distribution in Table 1. 
The granularities found in news corpora such as 
OTC and mixed corpora such as BNC are domi-
26
nated by quarterly reports, which reflect the in-
fluence of specific information pinpointing the 
durations of financial events. This explains the 
fact that 12 of the top 13 events in Table 1 are 
financial ones, with the reporting verb say being 
the only non-financial event in the top 13.  
The durations for the most frequent event, rep-
resented by the verb to lose, is shown in Table 2. 
Most losses are during a quarter, or a year, be-
cause financial news tends to quantize losses for 
those periods. 
 
Duration Frequency
1 day (P1D) 1 
2 months (P2M) 1 
unspecified weeks (PXW) 1 
unspecified months (PXM) 1 
3 months (P3M) 9 
9 months (P9M) 3 
1 year (P1Y) 6 
5 years (P5Y) 1 
1 decade (P1Y) 1 
TOTAL 26 
Table 2. Distribution of durations for event 
 to lose 
 
Ideally, we would be able to generalize over 
the duration values, grouping them into classes. 
Table 3 shows some hand-aggregated duration 
classes for the data. These classes are ranges of 
durations. It can be seen that the temporal span 
of events across the data is dominated by 
granularities of weeks and months, extending 
into small numbers of years.  
 
Duration Class Count
<1 min 1 
5-15 min 12 
1-<24 hr 20 
1 day 14 
2-14 days 49 
1-3 months 120 
7-9 months 48 
1-6 years 97 
1 decade - < 1 century 30 
1-2 centuries 2 
vague (unspecified 
mins/days/months, continu-
ous present, indefinite fu-
ture, etc.) 
69 
Table 3. Distribution of aggregated durations 
 
   Interestingly, 67 events in the data correspond 
to ?achievement? verbs, whose main characteris-
tic is that they can have a near-instantaneous du-
ration (though of course they can be iterated or 
extended to have other durations). We obtained a 
list of achievement verbs from the LCS lexicon 
of (Dorr and Olsen 1997)3. Achievements can be 
marked as having durations of PTXS, i.e., an 
unspecified number of seconds. Such values 
don?t reinforce any of the observed values, in-
stead extending the set of durations to include 
much smaller durations. As a result, these hidden 
values are not shown in our data 
6 Estimating Duration Probabilities 
Given a distribution of durations for events 
observed in corpora, one of the challenges is to 
arrive at an appropriate value for a given event 
(or class of events). Based on data such as Table 
2, we could estimate the probability P(lose, P3M) 
? 0.346, while P(lose, P1D) ? 0.038, which is 
nearly ten times less likely. Table 2 reveals peak-
ing at 3 months, 6 months, and 9 months, with 
uniform probabilities for all others. Further, we 
can estimate the probability that losses will be 
during periods of 2 months, 3 months, or 9 
months as ? 0.46. Of course, we would prefer a 
much large sample to get more reliable estimates.  
One could also infer a max-min time range, 
but the maximum or minimum may not always 
be likely, as in the case of lose, which has rela-
tively low probability of extending for ?P1D? or 
?P1E?.  Turning to earnings, we find that P(earn, 
P9M) ? 4/16 = 0.25, P(earn, P1Y) ? 0.31, but 
P(earn, P3M) ? 0.43, since most earnings are 
reported for a quarter. 
 
Figure 2. Distribution of durations 
of event to lose 
 
So far, we have considered durations to be 
discrete, falling into a fixed number of categories. 
These categories could be atomic TimeML DU-
                                                 
3See  www.umiacs.umd.edu/ ~bonnie/ LCS_ Data-
base_Documentation.html. 
27
RATION values, as in the examples of durations 
in Table 2, or they could be aggregated in some 
fashion, as in Table 3. In the discrete view, 
unless we have a category of 4 months, the prob-
ability of a loss extending over 4 months is unde-
fined. Viewed this way, the problem is one of 
classification, namely providing the probability 
that an event has a particular duration category.  
The second view takes duration to be continu-
ous, so the duration of an event can have any 
subinterval as a value. The problem here is one 
of regression. We can re-plot the data in Table 2 
as Figure 2, where we have plotted durations in 
days on the x-axis in a natural log scale, and fre-
quency on the y-axis. Since we have plotted the 
durations as a curve, we can interpolate and ex-
trapolate durations, so that we can obtain the 
probability of a loss for 4 months. Of course, we 
would like to fit the best curve possible, and, as 
always, the more data points we have, the better.  
7 Possible Enhancements 
    One of the basic problems with this approach 
is data sparseness, with few examples for each 
event. This makes it difficult to generalize about 
durations. In this section, we discuss enhance-
ments that can address this problem. 
7.1 Converting points to durations 
More durations can be inferred from the OTC 
by coercing TIMEX3 DATE and TIME expres-
sions to DURATIONS; for example, if someone 
announced something in 1997, the maximum 
duration would be one year. Whether this leads 
to reliable heuristics or not remains to be seen.  
7.2 Event class aggregation 
A more useful approach might be to aggregate 
events into classes, as we have done implicitly 
with financial events. Reporting verbs are al-
ready identified as a TimeML subclass, as are 
aspectual verbs such as begin, continue and fin-
ish. Arriving at an appropriate set of classes, 
based on distributional data or resource-derived 
classes (e.g., TimeML, VerbNet, WordNet, etc.) 
remains to be explored. 
7.3 Expanding the corpus sample 
Last but not least, we could expand substan-
tially the search patterns and size of the corpus 
searched against. In particular, we could emulate 
the approach used in VerbOcean (Chklovski and 
Pantel 2004). This resource consists of lexical 
relations mined from Google searches. The min-
ing uses a set of lexical and syntactic patterns to 
test for pairs of verbs strongly associated on the 
Web in a particular semantic relation. For exam-
ple, the system discovers that marriage happens-
before divorce, and that tie happens-before untie. 
Such results are based on estimating the prob-
ability of the joint occurrence of the two verbs 
and the pattern. One can imagine a similar ap-
proach being used for durations. Bootstrapping 
of patterns may also be possible. 
8 Conclusion 
This paper describes the first steps in acquir-
ing metric temporal constraints for events. The 
work is carried out in the context of the TimeML 
framework for marking up events and their tem-
poral relations. We have identified a method for 
enhancing TimeML annotations with metric con-
straints. Although the temporal reasoning re-
quired to carry that out has been described in the 
prior literature, e.g., (Kautz and Ladkin 1991), 
this is a first attempt at lexical acquisition of 
metrical constraints. As a pilot study, it does 
suggest the feasibility of acquisition of metric 
temporal constraints from corpora. In follow-on 
research, we will explore the enhancements de-
scribed in Section 7. 
However, this work is limited by the lack of 
evaluation, in terms of assessing how valid the 
durations inferred by our method are compared 
with human annotations. In ongoing work, Jerry 
Hobbs and his colleagues (Pan et al 2006) have 
developed an annotation scheme for humans to 
mark up event durations in documents. Once 
such enhancements are carried out, it will cer-
tainly be fruitful to compare the duration prob-
abilities obtained with the ranges of durations 
provided in that corpus.  
In future, we will explore both regression and 
classification models for duration learning. In the 
latter case, we will investigate the use of con-
structive induction e.g., (Bloedorn and Michalski 
1998). In particular, we will avail of operators to 
implement attribute abstraction that will cluster 
durations into coarse-grained classes, based on 
distributions of atomic durations observed in the 
data. We will also investigate the extent to which 
learned durations can be used to constrain 
TLINK ordering. 
References 
James Allen. 1984. Towards a General Theory of Ac-
tion and Time.  Artificial Intelligence, 23, 2, 123-
154. 
28
Eric Bloedorn  and Ryszard S. Michalski. 1998. Data-
Driven Constructive Induction. IEEE Intelligent 
Systems, 13, 2.  
Branimir Boguraev and Rie Kubota Ando. 2005. 
TimeML-Compliant Text Analysis for Temporal 
Reasoning. Proceedings of IJCAI-05, 997-1003. 
Timothy Chklovski and Patrick Pantel. 
2004.VerbOcean: Mining the Web for Fine-
Grained Semantic Verb Relations. Proceedings of 
EMNLP-04. http://semantics.isi.edu/ocean 
B. Dorr and M. B. Olsen. Deriving Verbal and Com-
positional Lexical Aspect for NLP Applications. 
ACL'1997, 151-158.   
Janet Hitzeman, Marc Moens and Clare Grover. 1995. 
Algorithms for Analyzing the Temporal Structure 
of Discourse. Proceedings of  EACL?95, Dublin, 
Ireland, 253-260. 
Feng Pan, Rutu Mulkar, and Jerry Hobbs.  Learning 
Event Durations from Event Descriptions. Proceed-
ings of Workshop on Annotation and Reasoning 
about Time and Events (ARTE?2006),  ACL?2006.  
C.H. Hwang and L. K. Schubert. 1992. Tense Trees as 
the fine structure of discourse. Proceedings of 
ACL?1992, 232-240. 
Hans Kamp and Uwe Ryle. 1993. From Discourse to 
Logic (Part 2). Dordrecht: Kluwer. 
Andrew Kehler. 2000. Resolving Temporal Relations 
using Tense Meaning and Discourse Interpretation, 
in M. Faller, S. Kaufmann, and M. Pauly, (eds.), 
Formalizing the Dynamics of Information, CSLI 
Publications, Stanford. 
Henry A. Kautz and Peter B. Ladkin. 1991. Integrat-
ing Metric and Qualitative Temporal Reasoning. 
AAAI'91. 
Mirella Lapata and Alex Lascarides. 2004. Inferring 
Sentence-internal Temporal Relations. In Proceed-
ings of the North American Chapter of the Assoca-
tion of Computational Linguistics, 153-160.  
Alex Lascarides and Nicholas Asher. 1993. Temporal 
Relations, Discourse Structure, and Commonsense 
Entailment. Linguistics and Philosophy 16, 437-
494. 
Wenjie Li, Kam-Fai Wong, Guihong Cao and Chunfa 
Yuan. 2004. Applying Machine Learning to Chi-
nese Temporal Relation Resolution. Proceedings of 
ACL?2004, 582-588. 
Inderjeet Mani and George Wilson. 2000. Robust 
Temporal Processing of News.  Proceedings of 
ACL?2000. 
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.  
2003.  Inferring Temporal Ordering of Events in 
News. Short Paper. Proceedings of HLT-
NAACL'03, 55-57.  
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong 
Min Lee, and James Pustejovsky.  2006.  Machine 
Learning of Temporal Relations. Proceedings of 
ACL?2006.  
Rebecca J. Passonneau. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics, 14, 2, 1988, 44-60. 
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, David Day, Lisa Ferro, Robert Gai-
zauskas, Marcia Lazo, Andrea Setzer, and Beth 
Sundheim. 2003. The TimeBank Corpus. Corpus 
Linguistics, 647-656. 
James Pustejovsky, Bob Ingria, Roser Sauri, Jose 
Castano, Jessica Littman, Rob Gaizauskas, Andrea 
Setzer, G. Katz,  and I. Mani. 2005. The Specifica-
tion Language TimeML. In I. Mani, J. Pustejovsky, 
and R. Gaizauskas, (eds.), The Language of Time: 
A Reader. Oxford University Press.  
Roser Saur?, Robert Knippen, Marc Verhagen and 
James Pustejovsky. 2005. Evita: A Robust Event 
Recognizer for QA Systems. Short Paper. Proceed-
ings of HLT/EMNLP 2005: 700-707. 
Frank Schilder and Christof Habel. 2005. From tem-
poral expressions to temporal information: seman-
tic tagging of news messages. In I. Mani, J. Puste-
jovsky, and R. Gaizauskas, (eds.), The Language of 
Time: A Reader. Oxford University Press.  
Andrea Setzer and Robert Gaizauskas. 2000. Annotat-
ing Events and Temporal Information in Newswire 
Texts. Proceedings of LREC-2000, 1287-1294.  
Marc Verhagen. 2004. Times Between The Lines. 
Ph.D. Dissertation, Department of Computer Sci-
ence, Brandeis University. 
Marc Verhagen, Inderjeet Mani, Roser Saur?, Robert 
Knippen, Jess Littman and James Pustejovsky. 
2005. Automating Temporal Annotation with 
TARSQI. Demo Session, ACL 2005. 
Marc Vilain, Henry Kautz, and Peter Van Beek. 1989. 
Constraint propagation algorithms for temporal 
reasoning: A revised report. In D. S. Weld and J. 
de Kleer (eds.), Readings in Qualitative Reasoning 
about Physical Systems, Morgan-Kaufman, 373-
381. 
Bonnie Webber. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14, 2, 1988, 61-73. 
29
