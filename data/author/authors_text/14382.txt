Proceedings of the 12th Conference of the European Chapter of the ACL, pages 763?771,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Semi-supervised Training for the Averaged Perceptron POS Tagger
Drahom??ra ?johanka? Spoustova? Jan Hajic? Jan Raab Miroslav Spousta
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics,
Charles University Prague, Czech Republic
{johanka,hajic,raab,spousta}@
ufal.mff.cuni.cz
Abstract
This paper describes POS tagging exper-
iments with semi-supervised training as
an extension to the (supervised) averaged
perceptron algorithm, first introduced for
this task by (Collins, 2002). Experiments
with an iterative training on standard-sized
supervised (manually annotated) dataset
(106 tokens) combined with a relatively
modest (in the order of 108 tokens) un-
supervised (plain) data in a bagging-like
fashion showed significant improvement
of the POS classification task on typo-
logically different languages, yielding bet-
ter than state-of-the-art results for English
and Czech (4.12 % and 4.86 % relative er-
ror reduction, respectively; absolute accu-
racies being 97.44 % and 95.89 %).
1 Introduction
Since 2002, we have seen a renewed interest in
improving POS tagging results for English, and
an inflow of results (initial or improved) for many
other languages. For English, after a relatively big
jump achieved by (Collins, 2002), we have seen
two significant improvements: (Toutanova et al,
2003) and (Shen et al, 2007) pushed the results
by a significant amount each time.1
1In our final comparison, we have also included the re-
sults of (Gime?nez and Ma`rquez, 2004), because it has sur-
passed (Collins, 2002) as well and we have used this tag-
ger in the data preparation phase. See more details below.
Most recently, (Suzuki and Isozaki, 2008) published their
Semi-supervised sequential labelling method, whose results
on POS tagging seem to be optically better than (Shen et al,
2007), but no significance tests were given and the tool is not
available for download, i.e. for repeating the results and sig-
nificance testing. Thus, we compare our results only to the
tools listed above.
Even though an improvement in POS tagging
might be a questionable enterprise (given that its
effects on other tasks, such as parsing or other
NLP problems are less than clear?at least for En-
glish), it is still an interesting problem. Moreover,
the ?ideal?2 situation of having a single algorithm
(and its implementation) for many (if not all) lan-
guages has not been reached yet. We have cho-
sen Collins? perceptron algorithm because of its
simplicity, short training times, and an apparent
room for improvement with (substantially) grow-
ing data sizes (see Figure 1). However, it is clear
that there is usually little chance to get (substan-
tially) more manually annotated data. Thus, we
have been examining the effect of adding a large
monolingual corpus to Collins? perceptron, appro-
priately extended, for two typologically different
languages: English and Czech. It is clear however
that the features (feature templates) that the tag-
gers use are still language-dependent.
One of the goals is also to have a fast im-
plementation for tagging large amounts of data
quickly. We have experimented with various clas-
sifier combination methods, such as those de-
scribed in (Brill and Wu, 1998) or (van Halteren et
al., 2001), and got improved results, as expected.
However, we view this only as a side effect (yet, a
positive one)?our goal was to stay on the turf of
single taggers, which are both the common ground
for competing on tagger accuracy today and also
significantly faster at runtime.3 Nevertheless, we
have found that it is advantageous to use them to
(pre-)tag the large amounts of plain text data dur-
2We mean easy to use for further research on problems
requiring POS tagging, especially multilingual ones.
3And much easier to (re)implement as libraries in proto-
type systems, which is often difficult if not impossible with
other people?s code.
763
Training data size (thousands of tokens)
Accu
racy
 on d
evel
opm
ent d
ata
100 200 300 400 500 600 700 800 900
96.0
96.5
97.0
97.5
98.0
Figure 1: Accuracy of the original averaged per-
ceptron, supervised training on PTB/WSJ (En-
glish)
ing the training phase.
Apart from feeding the perceptron by various
mixtures of manually tagged (?supervised?) and
auto-tagged (?unsupervised?)4 data, we have also
used various feature templates extensively; for ex-
ample, we use lexicalization (with the added twist
of lemmatization, useful especially for Czech, an
inflectionally rich language), ?manual? tag clas-
sification into large classes (again, useful espe-
cially for Czech to avoid the huge, still-to-be-
overcome data sparseness for such a language5),
and sub-lexical features mainly targeted at OOV
words. Inspired i.a. by (Toutanova et al, 2003)
and (Hajic? and Vidova?-Hladka?, 1998), we also use
?lookahead? features (however, we still remain
in the left-to-right HMM world ? in this respect
our solution is closer to the older work of (Hajic?
and Vidova?-Hladka?, 1998) than to (Toutanova et
al., 2003), who uses bidirectional dependencies
to include the right-hand side disambiguated tags,
4For brevity, we will use the terms ?supervised? and ?un-
supervised? data for ?manually annotated? and ?(automat-
ically annotated) plain (raw) text? data, respectively, even
though these adjectives are meant to describe the process of
learning, not the data themselves.
5As (Hajic?, 2004) writes, Czech has 4400 plausible tags,
of which we have observed almost 2000 in the 100M cor-
pus we have used in our experiments. However, only 1100
of them have been found in the manually annotated PDT 2.0
corpus (the corpus on which we have based the supervised
experiments). The situation with word forms (tokens) is even
worse: Czech has about 20M different word forms, and the
OOV rate based on the 1.5M PDT 2.0 data and measured
against the 100M raw corpus is almost 10 %.
which we cannot.)
To summarize, we can describe our system as
follows: it is based on (Votrubec, 2006)?s imple-
mentation of (Collins, 2002), which has been fed
at each iteration by a different dataset consisting
of the supervised and unsupervised part: precisely,
by a concatenation of the manually tagged training
data (WSJ portion of the PTB 3 for English, mor-
phologically disambiguated data from PDT 2.0 for
Czech) and a chunk of automatically tagged unsu-
pervised data. The ?parameters? of the training
process (feature templates, the size of the unsu-
pervised chunks added to the trainer at each itera-
tion, number of iterations, the combination of tag-
gers that should be used in the auto-tagging of the
unsupervised chunk, etc.) have been determined
empirically in a number of experiments on a de-
velopment data set. We should also note that as a
result of these development-data-based optimiza-
tions, no feature pruning has been employed (see
Section 4 for details); adding (even lexical) fea-
tures from the auto-tagged data did not give signif-
icant accuracy improvements (and only made the
training very slow).
The final taggers have surpassed the current
state-of-the-art taggers by significant margins (we
have achieved 4.12 % relative error reduction for
English and 4.86 % for Czech over the best pre-
viously published results, single or combined),
using a single tagger. However, the best En-
glish tagger combining some of the previous state-
of-the-art ones is still ?optically? better (yet not
significantly?see Section 6).
2 The perceptron algorithm
We have used the Morc?e6 tagger (Votrubec, 2006)
as a main component in our experiments. It is a
reimplementation of the averaged perceptron de-
scribed in (Collins, 2002), which uses such fea-
tures that it behaves like an HMM tagger and thus
the standard Viterbi decoding is possible. Collins?
GEN(x) set (a set of possible tags at any given
position) is generated, in our case, using a mor-
phological analyzer for the given language (essen-
6The name ?Morc?e? stands for ?MORfologie C?Es?tiny?
(?Czech morphology?, see (Votrubec, 2006)), since it
has been originally developed for Czech. We keep this
name in this paper as the generic name of the aver-
aged perceptron tagger for the English-language experi-
ments as well. We have used the version available at
http://ufal.mff.cuni.cz/morce/.
764
tially, a dictionary that returns all possible tags7
for an input word form). The transition and out-
put scores for the candidate tags are based on a
large number of binary-valued features and their
weights, which are determined during iterative
training by the averaged perceptron algorithm.
The binary features describe the tag being pre-
dicted and its context. They can be derived from
any information we already have about the text at
the point of decision (respecting the HMM-based
overall setting). Every feature can be true or false
in a given context, so we can consider the true fea-
tures at the current position to be the description
of a tag and its context.
For every feature, the perceptron keeps its
weight coefficient, which is (in its basic version)
an integer number, (possibly) changed at every
training sentence. After its final update, this in-
teger value is stored with the feature to be later
retrieved and used at runtime. Then, the task of
the perceptron algorithm is to sum up all the co-
efficients of true features in a given context. The
result is passed to the Viterbi algorithm as a tran-
sition and output weight for the current state.8 We
can express it as
w(C, T ) =
n?
i=1
?i.?i(C, T ) (1)
where w(C, T ) is the transition weight for tag T
in context C, n is the number of features, ?i is the
weight coefficient of the ith feature and ?i(C, T )
is the evaluation of the ith feature for context C
and tag T . In the averaged perceptron, the val-
ues of every coefficient are added up at each up-
date, which happens (possibly) at each training
sentence, and their arithmetic average is used in-
stead.9 This trick makes the algorithm more re-
sistant to weight oscillations during training (or,
more precisely, at the end of it) and as a result, it
substantially improves its performance.10
7And lemmas, which are then used in some of the fea-
tures. A (high recall, low precision) ?guesser? is used for
OOV words.
8Which identifies unambiguously the corresponding tag.
9Implementation note: care must be taken to avoid inte-
ger overflows, which (at 100 iterations through millions of
sentences) can happen for 32bit integers easily.
10Our experiments have shown that using averaging helps
tremendously, confirming both the theoretical and practical
results of (Collins, 2002). On Czech, using the best feature
set, the difference on the development data set is 95.96 % vs.
95.02 %. Therefore, all the results presented in the following
text use averaging.
The supervised training described in (Collins,
2002) uses manually annotated data for the esti-
mation of the weight coefficients ?. The train-
ing algorithm is very simple?only integer num-
bers (counts and their sums for the averaging) are
updated for each feature at each sentence with
imperfect match(es) found against the gold stan-
dard. Therefore, it can be relatively quickly re-
trained and thus many different feature sets and
other training parameters, such as the number of
iterations, feature thresholds etc. can be con-
sidered and tested. As a result of this tuning,
our (fully supervised) version of the Morc?e tag-
ger gives the best accuracy among all single tag-
gers for Czech and also very good results for En-
glish, being beaten only by the tagger (Shen et al,
2007) (by 0.10 % absolute) and (not significantly)
by (Toutanova et al, 2003).
3 The data
3.1 The ?supervised? data
For English, we use the same data division of Penn
Treebank (PTB) parsed section (Marcus et al,
1994) as all of (Collins, 2002), (Toutanova et al,
2003), (Gime?nez and Ma`rquez, 2004) and (Shen
et al, 2007) do; for details, see Table 1.
data set tokens sentences
train (0-18) 912,344 38,220
dev-test (19-21) 131,768 5,528
eval-test (22-24) 129,654 5,463
Table 1: English supervised data set ? WSJ part
of Penn Treebank 3
For Czech, we use the current standard Prague
Dependency Treebank (PDT 2.0) data sets (Hajic?
et al, 2006); for details, see Table 2.
data set tokens sentences
train 1,539,241 91,049
dev-test 201,651 11,880
eval-test 219,765 13,136
Table 2: Czech supervised data set ? Prague De-
pendency Treebank 2.0
3.2 The ?unsupervised? data
For English, we have processed the North Amer-
ican News Text corpus (Graff, 1995) (without the
765
WSJ section) with the Stanford segmenter and to-
kenizer (Toutanova et al, 2003). For Czech, we
have used the SYN2005 part of Czech National
Corpus (CNC, 2005) (with the original segmenta-
tion and tokenization).
3.3 GEN(x): The morphological analyzers
For English, we perform a very simple morpholog-
ical analysis, which reduces the full PTB tagset to
a small list of tags for each token on input. The re-
sulting list is larger than such a list derived solely
from the PTB/WSJ, but much smaller than a full
list of tags found in the PTB/WSJ.11 The English
morphological analyzer is thus (empirically) opti-
mized for precision while keeping as high recall
as possible (it still overgenerates). It consists of a
small dictionary of exceptions and a small set of
general rules, thus covering also a lot of OOV to-
kens.12
For Czech, the separate morphological analyzer
(Hajic?, 2004) usually precedes the tagger. We use
the version from April 2006 (the same as (Spous-
tova? et al, 2007), who reported the best previous
result on Czech tagging).
4 The perceptron feature sets
The averaged perceptron?s accuracy is determined
(to a large extent) by the set of features used. A
feature set is based on feature templates, i.e. gen-
eral patterns, which are filled in with concrete val-
ues from the training data. Czech and English
are morphosyntactically very different languages,
therefore each of them needs a different set of
feature templates. We have empirically tested
hundreds of feature templates on both languages,
taken over from previous works for direct compar-
ison, inspired by them, or based on a combination
of previous experience, error analysis and linguis-
tic intuition.
In the following sections, we present the best
performing set of feature templates as determined
on the development data set using only the super-
vised training setting; our feature templates have
thus not been influenced nor extended by the un-
supervised data.13
11The full list of tags, as used by (Shen et al, 2007), also
makes the underlying Viterbi algorithm unbearably slow.
12The English morphology tool is also downloadable as a
separate module on the paper?s accompanying website.
13Another set of experiments has shown that there is not,
perhaps surprisingly, a significant gain in doing so.
4.1 English feature templates
The best feature set for English consists of 30 fea-
ture templates. All templates predict the current
tag as a whole. A detailed description of the En-
glish feature templates can be found in Table 3.
Context predicting whole tag
Tags
Previous tag
Previous two tags
First letter of previous tag
Word forms
Current word form
Previous word form
Previous two word forms
Following word form
Following two word forms
Last but one word form
Current word affixes
Prefixes of length 1-9
Suffixes of length 1-9
Current word features
Contains number
Contains dash
Contains upper case letter
Table 3: Feature templates for English
A total of 1,953,463 features has been extracted
from the supervised training data using the tem-
plates from Table 3.
4.2 Czech feature templates
The best feature set for Czech consists of 63 fea-
ture templates. 26 of them predict current tag as
a whole, whereas the rest predicts only some parts
of the current tag separately (e.g., detailed POS,
gender, case) to avoid data sparseness. Such a fea-
ture is true, in an identical context, for several dif-
ferent tags belonging to the same class (e.g., shar-
ing a locative case). The individual grammatical
categories used for such classing have been cho-
sen on both linguistic grounds (POS, detailed fine-
grained POS) and also such categories have been
used which contribute most to the elimination of
the tagger errors (based on an extensive error anal-
ysis of previous results, the detailed description of
which can be found in (Votrubec, 2006)).
Several features can look ahead (to the right
of the current position) - apart from the obvious
word form, which is unambiguous, we have used
(in case of ambiguity) a random tag and lemma of
the first position to the right from the current po-
sition which might be occupied with a verb (based
on dictionary and the associated morphological
guesser restrictions).
A total of 8,440,467 features has been extracted
from the supervised training data set. A detailed
description is included in the distribution down-
loadable from the Morc?e website.
766
5 The (un)supervised training setup
We have extended the averaged perceptron setup
in the following way: the training algorithm is
fed, in each iteration, by a concatenation of the
supervised data (the manually tagged corpus) and
the automatically pre-tagged unsupervised data,
different for each iteration (in this order). In
other words, the training algorithm proper does
not change at all: it is the data and their selection
(including the selection of the way they are auto-
matically tagged) that makes all the difference.
The following ?parameters? of the (unsuper-
vised part of the) data selection had to be deter-
mined experimentally:
? the tagging process for tagging the selected
data
? the selection mechanism (sequential or ran-
dom with/without replacement)
? the size to use for each iteration
? and the use and order of concatenation with
the manually tagged data.
We have experimented with various settings to
arrive at the best performing configuration, de-
scribed below. In each subsection, we compare
the result of our ,,winning? configuration with re-
sults of the experiments which have the selected
attributes omitted or changed; everything is mea-
sured on the development data set.
5.1 Tagging the plain data
In order to simulate the labeled training events,
we have tagged the unsupervised data simply by
a combination of the best available taggers. For
practical reasons (to avoid prohibitive training
times), we have tagged all the data in advance, i.e.
no re-tagging is performed between iterations.
The setup for the combination is as follows (the
idea is simplified from (Spoustova? et al, 2007)
where it has been used in a more complex setting):
1. run N different taggers independently;
2. join the results on each position in the data
from the previous step ? each token thus
ends up with between 1 and N tags, a union
of the tags output by the taggers at that posi-
tion;
3. do final disambiguation (by a single tag-
ger14).
Tagger Accuracy
Morc?e 97.21
Shen 97.33
Combination 97.44
Table 4: Dependence on the tagger(s) used to tag
the additional plain text data (English)16
Table 4 illustrates why it is advantageous to go
through this (still)16 complicated setup against a
single-tagger bootstrapping mechanism, which al-
ways uses the same tagger for tagging the unsu-
pervised data.
For both English and Czech, the selection of
taggers, the best combination and the best over-
all setup has been optimized on the development
data set. A bit surprisingly, the final setup is very
similar for both languages (two taggers to tag the
data in Step 1, and a third one to finish it up).
For English, we use three state-of-the-art tag-
gers: the taggers of (Toutanova et al, 2003) and
(Shen et al, 2007) in Step 1, and the SVM tag-
ger (Gime?nez and Ma`rquez, 2004) in Step 3. We
run the taggers with the parameters which were
shown to be the best in the corresponding papers.
The SVM tagger needed to be adapted to accept
the (reduced) list of possible tags.17
For Czech, we use the Feature-based tagger
(Hajic?, 2004) and the Morc?e tagger (with the new
feature set as described in section 4) in Step 1, and
an HMM tagger (Krbec, 2005) in Step 3. This
combination outperforms the results in (Spoustova?
et al, 2007) by a small margin.
5.2 Selection mechanism for the plain data
We have found that it is better to feed the training
with different chunks of the unsupervised data at
each iteration. We have then experimented with
14This tagger (possibly different from any of theN taggers
from Step 1) runs as usual, but it is given a minimal list of (at
most N ) tags that come from Step 2 only.
15?Accuracy? means accuracy of the semi-supervised
method using this tagger for pre-tagging the unsupervised
data, not the accuracy of the tagger itself.
16In fact, we have experimented with other tagger
combinations and configurations as well?with the TnT
(Brants, 2000), MaxEnt (Ratnaparkhi, 1996) and TreeTag-
ger (Schmid, 1994), with or without the Morc?e tagger in the
pack; see below for the winning combination.
17This patch is available on the paper?s website (see Sec-
tion 7).
767
three methods of unsupervised data selection, i.e.
generating the unsupervised data chunks for each
training iteration from the ,,pool? of sentences.
These methods are: simple sequential chopping,
randomized data selection with replacement and
randomized selection without replacement. Ta-
ble 5 demonstrates that there is practically no dif-
ference in the results. Thus, we use the sequential
chopping mechanism, mainly for its simplicity.
Method of data selection English Czech
Sequential chopping 97.44 96.21
Random without replacement 97.44 96.20
Random with replacement 97.44 96.21
Table 5: Unsupervised data selection
5.3 Joining the data
We have experimented with various sizes of the
unsupervised parts (from 500k tokens to 5M) and
also with various numbers of iterations. The best
results (on the development data set) have been
achieved with the unsupervised chunks containing
approx. 4 million tokens for English and 1 million
tokens for Czech. Each training process consists
of (at most) 100 iterations (Czech) or 50 iterations
(English); therefore, for the 50 (100) iterations we
needed only about 200,000,000 (100,000,000) to-
kens of raw texts. The best development data set
results have been (with the current setup) achieved
on the 44th (English) and 33th (Czech) iteration.
The development data set has been also used to
determine the best way to ?merge? the manually
labeled data (the PTB/WSJ and the PDT 2.0 train-
ing data) and the unsupervised parts of the data.
Given the properties of the perceptron algorithm,
it is not too surprising that the best solution is to
put (the full size of) the manually labeled data first,
followed by the (four) million-token chunk of the
automatically tagged data (different data in each
chunk but of the same size for each iteration). It
corresponds to the situation when the trainer is pe-
riodically ?returned to the right track? by giving it
the gold standard data time to time.
Figure 2 (English) and especially Figure 3
(Czech) demonstrate the perceptron behavior in
cases where the supervised data precede the un-
supervised data only in selected iterations. A sub-
set of these development results is also present in
Table 6.
0 10 20 30 40 509
7.20
97.2
5
97.3
0
97.3
5
97.4
0
Iteration
Accu
racy
 on d
evel
opm
ent d
ata
Every iterationEvery 4th iterationEvery 8th iterationEvery 16th iterationOnce at the beginning    No supervised data
Figure 2: Dependence on the inclusion of the su-
pervised training data (English)
English Czech
No supervised data 97.37 95.88
Once at the beginning 97.40 96.00
Every training iteration 97.44 96.21
Table 6: Dependence on the inclusion of the su-
pervised training data
5.4 The morphological analyzers and the
perceptron feature templates
The whole experiment can be performed with
the original perceptron feature set described in
(Collins, 2002) instead of the feature set described
in this article. The results are compared in Table 7
(for English only).
Also, for English it is not necessary to use our
morphological analyzer described in section 3.3
(other variants are to use the list of tags derived
solely from the WSJ training data or to give each
token the full list of tags found in WSJ). It is
practically impossible to perform the unsupervised
training with the full list of tags (it would take sev-
eral years instead of several days with the default
setup), thus we compare only the results with mor-
phological analyzer to the results with the list of
tags derived from the training data, see Table 8.
It can be expected (some approximated exper-
iments were performed) that the results with the
full list of tags would be very similar to the results
with the morphological analyzer, i.e. the morpho-
logical analyzer is used mainly for technical rea-
sons. Our expectations are based mainly (but not
768
0 10 20 30 40 50
95.6
95.7
95.8
95.9
96.0
96.1
96.2
Iteration
Accu
racy
 on d
evel
opm
ent d
ata
Every iterationEvery 4th iterationEvery 8th iterationEvery 16th iterationOnce at the beginning    No supervised data
Figure 3: Dependence on the inclusion of the su-
pervised training data (Czech)
only) on the supervised training results, where the
performance of the taggers using the morpholog-
ical analyzer output and using the full list of tags
are nearly the same, see Table 9.
Feature set Accuracy
Collins? 97.38
Our?s 97.44
Table 7: Dependence on the feature set used by the
perceptron algorithm (English)
GEN(x) Accuracy
List of tags derived from train 97.13
Our morphological analyzer 97.44
Table 8: Dependence on the GEN(x)
6 Results
In Tables 10 and 11, the main results (on the eval-
test data sets) are summarized. The state-of-the
art taggers are using feature sets discribed in the
corresponding articles ((Collins, 2002), (Gime?nez
and Ma`rquez, 2004), (Toutanova et al, 2003) and
(Shen et al, 2007)), Morc?e supervised and Morc?e
semi-supervised are using feature set desribed in
section 4.
For significance tests, we have used the paired
Wilcoxon signed rank test as implemented in the
R package (R Development Core Team, 2008)
GEN(x) Accuracy
List of tags derived from train 95.89
Our morphological analyzer 97.17
Full tagset 97.15
Table 9: Supervised training results: dependence
on the GEN(x)
Tagger accuracy
Collins 97.07 %
SVM 97.16 %
Stanford 97.24 %
Shen 97.33 %
Morc?e supervised 97.23 %
combination 97.48 %
Morc?e semi-supervised 97.44 %
Table 10: Evaluation of the English taggers
Tagger accuracy
Feature-based 94.04 %
HMM 94.82 %
Morc?e supervised 95.67 %
combination 95.70 %
Morc?e semi-supervised 95.89 %
Table 11: Evaluation of the Czech taggers
in wilcox.test(), dividing the data into 100
chunks (data pairs).
6.1 English
The combination of the three existing English tag-
gers seems to be best, but it is not significantly
better than our semi-supervised approach.
The combination is significantly better than
(Shen et al, 2007) at a very high level, but more
importantly, Shen?s results (currently represent-
ing the replicable state-of-the-art in POS tagging)
have been significantly surpassed also by the semi-
supervised Morc?e (at the 99 % confidence level).
In addition, the semi-supervised Morc?e per-
forms (on single CPU and development data set)
77 times faster than the combination and 23 times
faster than (Shen et al, 2007).
6.2 Czech
The best results (Table 11) are statistically signif-
icantly better than the previous results: the semi-
supervised Morc?e is significantly better than both
769
the combination and the supervised (original) vari-
ant at a very high level.
7 Download
We decided to publish our system for wide use un-
der the name COMPOST (Common POS Tagger).
All the programs, patches and data files are avail-
able at the website http://ufal.mff.cuni.cz/compost
under either the original data provider license, or
under the usual GNU General Public License, un-
less they are available from the widely-known and
easily obtainable sources (such as the LDC, in
which case pointers are provided on the download
website).
The Compost website also contains easy-to-run
Linux binaries of the best English and Czech sin-
gle taggers (based on the Morc?e technology) as de-
scribed in Section 6.
8 Conclusion and Future Work
We have shown that the ?right?18 mixture of su-
pervised and unsupervised (auto-tagged) data can
significantly improve tagging accuracy of the av-
eraged perceptron on two typologically different
languages (English and Czech), achieving the best
known accuracy to date.
To determine what is the contribution of the in-
dividual ?dimensions? of the system setting, as
described in Sect. 5, we have performed exper-
iments fixing all but one of the dimensions, and
compared their contribution (or rather, their loss
when compared to the best ?mix? overall). For
English, we found that excluding the state-of-the-
art-tagger (in fact, a carefully selected combina-
tion of taggers yielding significantly higher qual-
ity than any of them has) drops the resulting ac-
curacy the most (0.2 absolute). Significant yet
smaller drop (less than 0.1 percent) appears when
the manually tagged portion of the data is not used
or used only once (or infrequently) in the input
to the perceptron?s learner. The difference in us-
ing various feature templates (yet al largely sim-
ilar to what state-of-the-art taggers currently use)
is not significant. Similarly, the way the unsuper-
vised data is selected plays no role, either; this dif-
fers from the bagging technique (Breiman, 1996)
where it is significant. For Czech, the drop in ac-
curacy appears in all dimensions, except the unsu-
pervised data selection one. We have used novel
features inspired by previous work but not used in
18As empirically determined on the development data set.
the standard perceptron setting yet (linguistically
motivated tag classes in features, lookahead fea-
tures). Interestingly, the resulting tagger is better
than even a combination of the previous state-of-
the-art taggers (for English, this comparison is in-
conclusive).
We are working now on parallelization of the
perceptron training, which seems to be possible
(based i.a. on small-scale preliminary experiments
with only a handful of parallel processes and
specific data sharing arrangements among them).
This would further speed up the training phase, not
just as a nice bonus per se, but it would also allow
for a semi-automated feature template selection,
avoiding the (still manual) feature template prepa-
ration for individual languages. This would in turn
facilitate one of our goals to (publicly) provide
single-implementation, easy-to-maintain state-of-
the-art tagging tools for as many languages as pos-
sible (we are currently preparing Dutch, Slovak
and several other languages).19
Another area of possible future work is more
principled tag classing for languages with large
tagsets (in the order of 103), and/or adding
syntactically-motivated features; it has helped
Czech tagging accuracy even when only the ?in-
trospectively? defined classes have been added. It
is an open question if a similar approach helps
English as well (certain grammatical categories
can be generalized from the current WSJ tagset as
well, such as number, degree of comparison, 3rd
person present tense).
Finally, it would be nice to merge some of the
approaches by (Toutanova et al, 2003) and (Shen
et al, 2007) with the ideas of semi-supervised
learning introduced here, since they seem orthog-
onal in at least some aspects (e.g., to replace the
rudimentary lookahead features with full bidirec-
tionality).
Acknowledgments
The research described here was supported by the
projects MSM0021620838 and LC536 of Ministry
of Education, Youth and Sports of the Czech Re-
public, GA405/09/0278 of the Grant Agency of the
Czech Republic and 1ET101120503 of Academy
of Sciences of the Czech Republic.
19Available soon also on the website.
770
References
Thorsten Brants. 2000. TnT - a Statistical Part-of-
Speech Tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference, pages
224?231, Seattle, WA. ACL.
Leo Breiman. 1996. Bagging predictors. Mach.
Learn., 24(2):123?140.
Eric Brill and Jun Wu. 1998. Classifier Combination
for Improved Lexical Disambiguation. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics, pages 191?195, Montreal, Que-
bec, Canada. Association for Computational Lin-
guistics.
CNC, 2005. Czech National Corpus ? SYN2005. In-
stitute of Czech National Corpus, Faculty of Arts,
Charles University, Prague, Czech Republic.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In EMNLP ?02:
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing, volume 10,
pages 1?8, Philadelphia, PA.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool:
A General POS Tagger Generator Based on Support
Vector Machines. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation, pages 43?46, Lisbon, Portugal.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium, Cat. LDC95T21,
Philadelphia, PA.
Jan Hajic? and Barbora Vidova?-Hladka?. 1998. Tag-
ging Inflective Languages: Prediction of Morpho-
logical Categories for a Rich, Structured Tagset.
In Proceedings of the 17th international conference
on Computational linguistics, pages 483?490. Mon-
treal, Quebec, Canada.
Jan Hajic?. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv?? Karolinum, Prague.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, and Marie
Mikulova?. 2006. Prague Dependency Treebank
v2.0, CDROM, LDC Cat. No. LDC2006T01. Lin-
guistic Data Consortium, Philadelphia, PA.
Pavel Krbec. 2005. Language Modelling for Speech
Recognition of Czech. Ph.D. thesis, UK MFF,
Prague, Malostranske? na?me?st?? 25, 118 00 Praha 1.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1994. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
R Development Core Team, 2008. R: A Language and
Environment for Statistical Computing. R Foun-
dation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the 1st EMNLP, pages 133?142, New Brunswick,
NJ. ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, page 9pp., Manchester, GB.
Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007.
Guided Learning for Bidirectional Sequence Classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Drahom??ra ?johanka? Spoustova?, Jan Hajic?, Jan
Votrubec, Pavel Krbec, and Pavel Kve?ton?. 2007.
The Best of Two Worlds: Cooperation of Statistical
and Rule-Based Taggers for Czech. In Proceedings
of the Workshop on Balto-Slavonic Natural Lan-
guage Processing 2007, pages 67?74, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL-
08: HLT, pages 665?673, Columbus, Ohio, June.
Association for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In NAACL ?03: Proceedings of the 2003 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 173?180, Edmonton,
Canada. Association for Computational Linguistics.
Hans van Halteren, Walter Daelemans, and Jakub Za-
vrel. 2001. Improving accuracy in word class
tagging through the combination of machine learn-
ing systems. Computational Linguistics, 27(2):199?
229.
Jan Votrubec. 2006. Morphological Tagging Based
on Averaged Perceptron. In WDS?06 Proceedings of
Contributed Papers, pages 191?195, Prague, Czech
Republic. Matfyzpress, Charles University.
771
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 67?74,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Best of Two Worlds: Cooperation of Statistical
and Rule-Based Taggers for Czech
Drahom??ra ?johanka? Spoustova?
Jan Hajic?
Jan Votrubec
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics,
Charles University Prague, Czech Republic
{johanka,hajic,votrubec}@
ufal.mff.cuni.cz
Pavel Krbec
IBM Czech Republic,
Voice Technologies and Systems,
Prague, Czech Republic,
pavel krbec@cz.ibm.com
Pavel Kve?ton?
Institute of the Czech Language,
Academy of Sciences of the Czech Republic
Pavel.Kveton@seznam.cz
Abstract
Several hybrid disambiguation methods are
described which combine the strength of
hand-written disambiguation rules and sta-
tistical taggers. Three different statistical
(HMM, Maximum-Entropy and Averaged
Perceptron) taggers are used in a tagging
experiment using Prague Dependency Tree-
bank. The results of the hybrid systems are
better than any other method tried for Czech
tagging so far.
1 Introduction
Inflective languages pose a specific problem in tag-
ging due to two phenomena: highly inflective na-
ture (causing sparse data problem in any statistically
based system), and free word order (causing fixed-
context systems, such as n-gram HMMs, to be even
less adequate than for English).
The average tagset contains about 1,000 ? 2,000
distinct tags; the size of the set of possible and plau-
sible tags can reach several thousands. There have
been attempts at solving this problem for some of
the highly inflective European languages, such as
(Daelemans, 1996), (Erjavec, 1999) for Slovenian
and (Hajic?, 2000) for five Central and Eastern Euro-
pean languages.
Several taggers already exist for Czech, e.g.
(Hajic? et al, 2001b), (Smith, 2005), (Hajic? et al,
2006) and (Votrubec, 2006). The last one reaches
the best accuracy for Czech so far (95.12 %). Hence
no system has reached ? in the absolute terms ? a
performance comparable to English tagging (such as
(Ratnaparkhi, 1996)), which stands above 97 %.
We are using the Prague Dependency Treebank
(Hajic? et al, 2006) (PDT) with about 1.8 million
hand annotated tokens of Czech for training and test-
ing. The tagging experiments in this paper all use
the Czech morphological (pre)processor, which in-
cludes a guesser for ?unknown? tokens and which is
available from the PDT website (PDT Guide, 2006)
to disambiguate only among those tags which are
morphologically plausible.
The meaning of the Czech tags (each tag has 15
positions) we are using is explained in Table 1. The
detailed linguistic description of the individual posi-
tions can be found in the documentation to the PDT
(Hajic? et al, 2006).
67
Name Description
1 POS Part of Speech
2 SUBPOS Detailed POS
3 GENDER Gender
4 NUMBER Number
5 CASE Case
6 POSSGENDER Possessor?s Gender
7 POSSNUMBER Possessor?s Number
8 PERSON Person
9 TENSE Tense
10 GRADE Degree of comparison
11 NEGATION Negation
12 VOICE Voice
13 RESERVE1 Unused
14 RESERVE2 Unused
15 VAR Variant
Table 1: Czech Morphology and the Positional Tags
2 Components of the hybrid system
2.1 The HMM tagger
The HMM tagger is based on the well known for-
mula of HMM tagging:
T? = arg max
T
P (T )P (W | T ) (1)
where
P (W |T ) ?
?n
i=1 P (wi | ti, ti?1)
P (T ) ?
?n
i=1 P (ti | ti?1, ti?2).
(2)
The trigram probability P (W | T ) in formula 2
replaces (Hajic? et al, 2001b) the common (and less
accurate) bigram approach. We will use this tagger
as a baseline system for further improvements.
Initially, we change the formula 1 by introduc-
ing a scaling mechanism1: T? = arg maxT (?T ?
logP (T ) + logP (W | T )).
We tag the word sequence from right to left, i.e.
we change the trigram probability P (W | T ) from
formula 2 to P (wi | ti, ti+1).
Both the output probability P (wi | ti, ti+1) and
the transition probability P (T ) suffer a lot due to
the data sparseness problem. We introduce a com-
ponent P (endingi | ti, ti+1), where ending con-
sists of the last three characters of wi. Also, we in-
troduce another component P (t?i | t
?
i+1, t
?
i+2) based
on a reduced tagset T ? that contains positions POS,
GENDER, NUMBER and CASE only (chosen on
linguistic grounds).
1The optimum value of the scaling parameter ?T can be
tuned using held-out data.
We upgrade all trigrams to fourgrams; the
smoothing mechanism for fourgrams is history-
based bucketing (Krbec, 2005).
The final fine-tuned HMM tagger thus uses all
the enhancements and every component contains its
scaling factor which has been computed using held-
out data. The total error rate reduction is 13.98 %
relative on development data, measured against the
baseline HMM tagger.
2.2 Morc?e
TheMorc?e2 tagger assumes some of the HMMprop-
erties at runtime, namely those that allow the Viterbi
algorithm to be used to find the best tag sequence for
a given text. However, the transition weights are not
probabilities. They are estimated by an Averaged
Perceptron described in (Collins, 2002). Averaged
Perceptron works with features which describe the
current tag and its context.
Features can be derived from any information we
already have about the text. Every feature can be
true or false in a given context, so we can regard
current true features as a description of the current
tag context.
For every feature, the Averaged Perceptron stores
its weight coefficient, which is typically an integer
number. The whole task of Averaged Perceptron is
to sum all the coefficients of true features in a given
context. The result is passed to the Viterbi algorithm
as a transition weight for a given tag. Mathemati-
cally, we can rewrite it as:
w(C, T ) =
n?
i=1
?i.?i(C, T ) (3)
where w(C, T ) is the transition weight for tag T in
context C, n is number of features, ?i is the weight
coefficient of ith feature and ?(C, T )i is evaluation
of ith feature for context C and tag T .
Weight coefficients (?) are estimated on training
data, cf. (Votrubec, 2006). The training algorithm
is very simple, therefore it can be quickly retrained
and it gives a possibility to test many different sets of
features (Votrubec, 2005). As a result, Morc?e gives
the best accuracy from the standalone taggers.
2The name Morc?e stands for ?MORfologie C?Es?tiny?
(?Czech morphology?).
68
2.3 The Feature-Based Tagger
The Feature-based tagger, taken also from the PDT
(Hajic? et al, 2006) distribution used in our exper-
iments uses a general log-linear model in its basic
formulation:
pAC(y | x) =
exp(
?n
i=1 ?ifi(y, x))
Z(x)
(4)
where fi(y, x) is a binary-valued feature of the event
value being predicted and its context, ?i is a weight
of the feature fi, and the Z(x) is the natural normal-
ization factor.
The weights ?i are approximated by Maximum
Likelihood (using the feature counts relative to all
feature contexts found), reducing the model essen-
tially to Naive Bayes. The approximation is nec-
essary due to the millions of the possible features
which make the usual entropy maximization infeasi-
ble. The model makes heavy use of single-category
Ambiguity Classes (AC)3, which (being indepen-
dent on the tagger?s intermediate decisions) can be
included in both left and right contexts of the fea-
tures.
2.4 The rule-based component
The approach to tagging (understood as a stand-
alone task) using hand-written disambiguation rules
has been proposed and implemented for the first
time in the form of Constraint-Based Grammars
(Karlsson, 1995). On a larger scale, this aproach was
applied to English, (Karlsson, 1995) and (Samuels-
son, 1997), and French (Chanod, 1995). Also (Bick,
2000) uses manually written disambiguation rules
for tagging Brazilian Portuguese, (Karlsson, 1985)
and (Koskenniemi, 1990) for Finish and (Oflazer,
1997) reports the same for Turkish.
2.4.1 Overview
In the hybrid tagging system presented in this pa-
per, the rule-based component is used to further re-
duce the ambiguity (the number of tags) of tokens
in an input sentence, as output by the morphological
processor (see Sect. 1). The core of the component
is a hand-written grammar (set of rules).
Each rule represents a portion of knowledge of
the language system (in particular, of Czech). The
3If a token can be a N(oun), V(erb) or A(djective), its (major
POS) Ambiguity Class is the value ?ANV?.
knowledge encoded in each rule is formally defined
in two parts: a sequence of tokens that is searched
for in an input sentence and the tags that can be
deleted if the sequence of tokens is found.
The overall strategy of this ?negative? grammar is
to keep the highest recall possible (i.e. 100 %) and
gradually improve precision. In other words, when-
ever a rule deletes a tag, it is (almost) 100% safe that
the deleted tag is ?incorrect? in the sentence, i.e. the
tag cannot be present in any correct tagging of the
sentence.
Such an (virtually) ?error-free? grammar can par-
tially disambiguate any input and prevent the subse-
quent taggers (stochastic, in our case) to choose tags
that are ?safely incorrect?.
2.4.2 The rules
Formally, each rule consists of the description of
the context (sequence of tokens with some special
property), and the action to be performed given the
context (which tags are to be discarded). The length
of context is not limited by any constant; however,
for practical purposes, the context cannot cross over
sentence boundaries.
For example: in Czech, two finite verbs cannot
appear within one clause. This fact can be used to
define the following disambiguation rule:
? context: unambiguous finite verb, fol-
lowed/preceded by a sequence of tokens
containing neither a comma nor a coordinat-
ing conjunction, at either side of a word x
ambiguous between a finite verb and another
reading;
? action: delete the finite verb reading(s) at the
word x.
It is obvious that no rule can contain knowledge
of the whole language system. In particular, each
rule is focused on at most a few special phenomena
of the language. But whenever a rule deletes a tag
from a sentence, the information about the sentence
structure ?increases?. This can help other rules to be
applied and to delete more and more tags.
For example, let?s have an input sentence with two
finite verbs within one clause, both of them ambigu-
ous with some other (non-finite-verbal) tags. In this
situation, the sample rule above cannot be applied.
69
On the other hand, if some other rule exists in the
grammar that can delete non-finite-verbal tags from
one of the tokens, then the way for application of the
sample rule is opened.
The rules operate in a loop in which (theoreti-
cally) all rules are applied again whenever a rule
deletes a tag in the partially disambiguated sentence.
Since deletion is a monotonic operation, the algo-
rithm is guaranteed to terminate; effective imple-
mentation has also been found in (Kve?ton?, 2006).
2.4.3 Grammar used in tests
The grammar is being developed since 2000 as
a standalone module that performs Czech morpho-
logical disambiguation. There are two ways of rule
development:
? the rules developed by syntactic introspection:
such rules are subsequently verified on the cor-
pus material, then implemented and the imple-
mented rules are tested on a testing corpus;
? the rules are derived from the corpus by intro-
spection and subsequently implemented.
In particular, the rules are not based on examina-
tion of errors of stochastic taggers.
The set of rules is (manually) divided into two
(disjoint) reliability classes ? safe rules (100% re-
liable rules) and heuristics (highly reliable rules, but
obscure exceptions can be found). The safe rules re-
flect general syntactic regularities of Czech; for in-
stance, no word form in the nominative case can fol-
low an unambiguous preposition. The less reliable
heuristic rules can be exemplified by those account-
ing for some special intricate relations of grammati-
cal agreement in Czech.
The grammar consists of 1727 safe rules and 504
heuristic rules. The system has been used in two
ways:
? safe rules only: in this mode, safe rules are ex-
ecuted in the loop until some tags are being
deleted. The system terminates as soon as no
rule can delete any tag.
? all rules: safe rules are executed first (see safe
rules only mode). Then heuristic rules start
to operate in the loop (similarly to the safe
rules). Any time a heuristic rule deletes a tag,
the safe rules only mode is entered as a sub-
procedure. When safe rules? execution termi-
nates, the loop of heuristic rules continues. The
disambiguation is finished when no heuristic
rule can delete any tag.
The rules are written in the fast LanGR formalism
(Kve?ton?, 2006) which is a subset of more general
LanGR formalism (Kve?ton?, 2005). The LanGR for-
malism has been developed specially for writing and
implementing disambiguation rules.
3 Methods of combination
3.1 Serial combination
The simplest way of combining a hand-written dis-
ambiguation grammar with a stochastic tagger is to
let the grammar reduce the ambiguity of the tagger?s
input. Formally, an input text is processed as fol-
lows:
1. morphological analysis (every input token gets
all tags that are plausible without looking at
context);
2. rule-based component (partially disambiguates
the input, i.e. deletes some tags);
3. the stochastic tagger (gets partially disam-
biguated text on its input).
This algorithm was already used in (Hajic? et
al., 2001b), only components were changed ? the
ruled-based component was significantly improved
and two different sets of rules were tried, as well
as three different statistical taggers. The best result
was (not surprisingly) achieved with set of safe rules
followed by the Morc?e tagger.
An identical approach was used in (Tapanainen,
1994) for English.
3.2 Serial combination with SUBPOS
pre-processing
Manual inspection of the output of the application of
the hand-written rules on the development data (as
used in the serial combination described in the pre-
vious section) discovered that certain types of dead-
locked (?cross-dependent?) rules prevent successful
disambiguation.
70
Cross-dependence means that a rule A can not
apply because of some remaining ambiguity, which
could be resolved by a ruleB, but the operation ofB
is still dependent on the application of A. In particu-
lar, ambiguity in the Part-of-Speech category is very
problematic. For example, only a few safe rules can
apply to a three-word sentence where all three words
are ambiguous between finite verbs and something
else.
If the Part-of-Speech ambiguity of the input is al-
ready resolved, precision of the rule-based compo-
nent and also of the final result after applying any of
the statistical taggers improves. Full Part-of-Speech
information is represented by the first two categories
of the Czech morphology tagset ? POS and SUB-
POS, which deals with different types of pronouns,
adverbs etc. As POS is uniquely determined by
SUBPOS (Hajic? et al, 2006), it is sufficient to re-
solve the SUBPOS ambiguity only.
All three taggers achieve more than 99% accuracy
in SUBPOS disambiguation. For SUBPOS disam-
biguation, we use the taggers in usual way (i.e. they
determine the whole tag) and then we put back all
tags having the same SUBPOS as the tag chosen by
the tagger.
Thus, the method with SUBPOS pre-processing
operates in four steps:
1. morphological analysis;
2. SUBPOS disambiguation (any tagger);
3. rule-based component;
4. final disambiguation (the same tagger4).
The best results were again achieved with the tag-
ger Morc?e and set of safe rules.
3.3 Combining more taggers in parallel
This method is quite different from previous ones,
because it essentially needs more than one tagger. It
consists of the following steps:
1. morphological analysis;
4This limitation is obviously not necessary, but we treat this
combination primarily as a one-tagger method. Results of em-
ploying two different taggers are only slightly better, but still
much worse than results of other methods presented later be-
low.
2. running N taggers independently;
3. merging the results from the previous step ?
each token ends up with between 1 and N tags,
a union of the taggers? outputs;
4. (optional: the rule-based component;)
5. final disambiguation (single tagger).
The best results were achieved with two taggers
in Step 1 (Feature-based and Morc?e), set of all rules
in Step 3 and the HMM tagger in Step 4.
This method is based on an assumption that dif-
ferent stochastic taggers make complementary mis-
takes, so that the recall of the ?union? of taggers
is almost 100 %. Several existing language mod-
els are based on this assumption ? (Brill, 1998)
for tagging English, (Borin, 2000) for tagging Ger-
man and (Vidova?-Hladka?, 2000) for tagging inflec-
tive languages. All these models perform some kind
of ?voting? ? for every token, one tagger is selected
as the most appropriate to supply the correct tag.
The model presented in this paper, however, entrusts
the selection of the correct tag to another tagger that
already operates on the partially disambiguated in-
put.
4 Results
All the methods presented in this paper have been
trained and tested on the PDT version 2.05. Tag-
gers were trained on PDT 2.0 training data set
(1,539,241 tokens), the results were achieved on
PDT 2.0 evaluation-test data set (219,765 tokens),
except Table 6, where PDT 2.0 development-test
data set (201,651 tokens) was used. The morpholog-
ical analysis processor and all the taggers were used
in versions from April 2006 (Hajic? et al, 2006), the
rule-based component is from September 2006.
For evaluation, we use both precision and recall
(and the corresponding F-measure) and accuracy,
since we also want to evaluate the partial disam-
biguation achieved by the hand-written rules alone.
Let t denote the number of tokens in the test data,
let c denote the number of tags assigned to all to-
kens by a disambiguation process and let h denote
5The results cannot be simply (number-to-number) com-
pared to previous results on Czech tagging, because different
training and testing data (PDT 2.0 instead of PDT 1.0) are used
since 2006.
71
the number of tokens where the manually assigned
tag is present in the output of the process.
? In case of the morphological analysis processor
and the standalone rule-based component, the
output can contain more than one tag for ev-
ery token. Then precision (p), recall (r) and F-
measure (f ) characteristics are defined as fol-
lows:
p = h/c r = h/t f = 2pr/(p + r).
? The output of the stochastic taggers contains al-
ways exactly one tag for every token ? then
p = r = f = h/t holds and this ratio is de-
noted as accuracy.
Table 2 shows the performance of the morpholog-
ical analysis processor and the standalone rule-based
component. Table 3 shows the performance of the
standalone taggers. The improvement of the combi-
nation methods is presented in Table 4.
Table 5 shows the relative error rate reduction.
The best method presented by this paper (parallel
combination of taggers with all rules) reaches the
relative error rate decrease of 11.48 % in compari-
son with the tagger Morc?e (which achieves the best
results for Czech so far).
Table 6 shows error rate (100 % ? accuracy) of
various methods6 on particular positions of the tags
(13 and 14 are omitted). The most problematic posi-
tion is CASE (5), whose error rate was significantly
reduced.
5 Conclusion
We have presented several variations of a novel
method for combining statistical and hand-written
rule-based tagging. In all cases, the rule-based
component brings an improvement ? the smaller
the involvement of the statistical component(s) is,
the bigger. The smallest gain can be observed
in the case of the parallel combination of taggers
(which by itself brings an expected improvement).
The best variation improved the accuracy of the
best-performing standalone statistical tagger by over
6F-b stands for feature-based taggeer, Par for parallel com-
bination without rules and Par+Rul for parallel combination
with rules.
11 % (in terms of relative error rate reduction), and
the inclusion of the rule-component itself improved
the best statistical-only combination by over 3.5 %
relative.
This might actually lead to pessimism regarding
the rule-based component. Most other inflective lan-
guages however have much smaller datasets avail-
able than Czech has today; in those cases, we expect
that the contribution of the rule-based component
(which does not depend on the training data size, ob-
viously) will be much more substantial.
The LanGR formalism, now well-developed,
could be used for relatively fast development for
other languages. We are, of course, unable to give
exact figures of what will take less effort ? whether
to annotate more data or to develop the rule-based
component for a particular language. Our feeling is
that the jury is actually still out on this issue, de-
spite some people saying that annotation is always
cheaper: annotation for morphologically complex
(e.g., inflective) languages is not cheap, and rule-
based development efforts have not been previously
using (unannotated) corpora so extensively (which
is what LanGR supports for ?testing? the developed
rules, leading to more reliable rules and more effec-
tive development cycle).
On the other hand, the rule-based component has
also two obvious and well-known disadvantages: it
is language dependent, and the application of the
rules is slower than even the baseline HMM tagger
despite the ?fast? version of the LanGR implemen-
tation we are using7.
In any case, our experiments produced a software
suite which gives the all-time best results in Czech
tagging, and we have offered to apply it to re-tag the
existing 200 mil. word Czech National Corpus. It
should significantly improve the user experience (for
searching the corpus) and allow for more precise ex-
periments with parsing and other NLP applications
that use that corpus.
7In the tests presented in this paper, the speed of the op-
eration of each stochastic tagger (and the parallel combination
without rules) is several hundreds of tokens processed per sec-
ond (running on a 2.2GHz Opteron processor). The operation of
the standalone rule-based component, however, is cca 10 times
slower ? about 40 tokens per second. The parallel combination
with all rules processes about 60 tokens per second ? the rules
operate faster here because their input in parallel combination
is already partially disambiguated.
72
Method p r f
Morphology 25.72 % 99.39 % 40.87 %
Safe rules 57.90 % 98.83 % 73.02 %
All rules 66.35 % 98.03 % 79.14 %
Table 2: Evaluation of rules alone
Tagger accuracy
Feature-based 94.04 %
HMM 94.82 %
Morc?e 95.12 %
Table 3: Evaluation of the taggers alone
Combination method accuracy
Serial (safe rules+Morc?e) 95.34 %
SUBPOS serial (safe rules+Morc?e) 95.44 %
Parallel without rules 95.52 %
Parallel with all rules 95.68 %
Table 4: Evaluation of the combinations
Method Morc?e Parallel
without
rules
Parallel without rules 8.20 % ?
Parallel with all rules 11.48 % 3.57 %
Table 5: Relative error rate reduction
F-b HMM Morc?e Par Par+Rul
1 0.61 0.70 0.66 0.57 0.57
2 0.69 0.78 0.75 0.64 0.64
3 1.82 1.49 1.66 1.39 1.37
4 1.56 1.30 1.38 1.18 1.15
5 4.03 3.53 3.08 2.85 2.62
6 0.02 0.03 0.03 0.02 0.02
7 0.01 0.01 0.01 0.01 0.01
8 0.06 0.07 0.08 0.06 0.05
9 0.05 0.08 0.07 0.05 0.04
10 0.29 0.28 0.30 0.26 0.27
11 0.29 0.31 0.33 0.28 0.28
12 0.05 0.08 0.06 0.05 0.04
15 0.31 0.31 0.31 0.28 0.29
Table 6: Error rate [%] on particular positions of tags
Acknowledgements
The research described here was supported by the
projects MSM0021620838 and LC536 of Ministry of
Eduation, Youth and Sports of the Czech Republic,
GA405/06/0589 of the Grant Agency of the Czech
Republic and 1ET100610409 Diagnostic and Eval-
uation Tools for Linguistic Software of the Informa-
tion Society Programme of the National Research
Programme of the Czech Republic.
References
Eckhard Bick. 2000. The parsing system ?Palavras?
? automatic grammatical analysis of Portuguese in a
constraint grammar framework. In: Proceedings of the
2nd International Conference on Language Resources
and Evaluation, TELRI. Athens
Lars Borin. 2000. Something borrowed, something blue:
Rule-based combination of POS taggers. In: Proceed-
ings of the 2nd International Conference on Language
Resources and Evaluation, Vol. 1, pp. 21?26. Athens
Eric Brill and Jun Wu. 1998. Classifier combination
for improved lexical disambiguation. In: Proceedings
of the 17th international conference on Computational
linguistics, Vol. 1, pp. 191?195. Montreal, Quebec
Jean-Pierre Chanod and Pasi Tapanainen. 1995. Tagging
French ? comparing a statistical and a constraint-
based method. In: Proceedings of EACL-95, pp. 149?
157. Dublin
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In: Proceedings
of EMNLP?02, July 2002, pp. 1?8. Philadelphia
W. Daelemans and Jakub Zavrel and Peter Berck and
Steven Gillis. 1996. MBT: A memory-based part of
speech tagger-generator. In: Proceedings of the 4th
WVLC, pp. 14?27. Copenhagen
Tomaz Erjavec and Saso Dzeroski and Jakub Zavrel.
1999. Morphosyntactic Tagging of Slovene: Evaluat-
ing PoS Taggers and Tagsets. Technical Report, Dept.
for Intelligent Systems, Jozef Stefan Institute. Ljubl-
jana
Jan Hajic? and Barbora Hladka?. 1997. Tagging of in-
flective languages: a comparison. In: Proceedings of
ANLP ?97, pp. 136?143. Washington, DC.
Jan Hajic? 2000. Morphological tagging: Data vs. dic-
tionaries. In: Proceedings of the 6th ANLP / 1st
NAACL?00, pp. 94?101. Seattle, WA
73
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva and
Vladim??r Petkevic?. 2001. Serial Combination of
Rules and Statistics: A Case Study in Czech Tag-
ging. In: Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics. CNRS
? Institut de Recherche en Informatique de Toulouse
and Universite? des Sciences Sociales, pp. 260?267.
Toulouse
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka
and Marie Mikulova?. 2006. Prague De-
pendency Treebank v2.0. CDROM. Linguis-
tic Data Consortium, Cat. LDC2006T01. Philadel-
phia. ISBN 1-58563-370-4. Documentation also at
http://ufal.ms.mff.cuni.cz/pdt2.0.
Fred Karlsson. 1985. Parsing Finnish in terms of a pro-
cess grammar. In: Fred Karlsson (ed.): Computational
Morphosyntax: Report on Research 1981-84, Univer-
sity of Helsinki, Department of General Linguistics
Publications No. 13, pp. 137?176.
Fred Karlsson and Atro Voutilainen and Juha Heikkila?
and Arto Anttila (eds.). 1995. Constraint Grammar: a
language-independent system for parsing unrestricted
text. Natural Language Processing. Vol. 4, Mouton
de Gruyter, Berlin and New York.
Kimmo Koskenniemi. 1990. Finite-State Parsing and
Disambiguation. In: Proceedings of Coling-90, Uni-
versity of Helsinki, 1990, pp. 229?232. Helsinki
Pavel Krbec. 2005. Language Modelling for Speech
Recognition of Czech. PhD Thesis, MFF, Charles Uni-
versity Prague.
Pavel Kve?ton?. 2005. Rule-based Morphological Dis-
ambiguation. PhD Thesis, MFF, Charles University
Prague.
Pavel Kve?ton?. 2006. Rule-based morphological dis-
ambiguation: On computational complexity of the
LanGR formalism. In: The Prague Bulletin of Mathe-
matical Linguistics, Vol. 85, pp. 57?72. Prague
Kemal Oflazer and Go?khan Tu?r. 1997. Morphological
disambiguation by voting constraints. In: Proceedings
of the 8th conference on European chapter of the As-
sociation for Computational Linguistics, pp. 222?229.
Madrid
Karel Oliva, Milena Hna?tkova?, Vladim??r Petkevic? and
Pavel Kve?ton?. 2000. The Linguistic Basis of a Rule-
Based Tagger of Czech. In: Sojka P., Kopec?ek I.,
Pala K. (eds.): Proceedings of the Conference ?Text,
Speech and Dialogue 2000?, Lecture Notes in Artifi-
cial Intelligence, Vol. 1902. Springer-Verlag, pp. 3?8.
Berlin-Heidelberg
PDTGuide. http://ufal.ms.mff.cuni.cz/pdt2.0
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In: Proceedings of the 1st
EMNLP, May 1996, pp. 133?142. Philadelphia
Christer Samuelsson and Atro Voluntainen. 1997. Com-
paring a linguistic and a stochastic tagger. In: Pro-
ceedings of ACL/EACL Joint Converence, pp. 246?
252. Madrid
Noah A. Smith and David A. Smith and Roy W.
Tromble. 2005. Context-Based Morphological Dis-
ambiguation with Random Fields. In: Proceedings of
HLT/EMNLP, pp. 475?482. Vancouver
Drahom??ra ?johanka? Spoustova?. in prep. Kombino-
vane? statisticko-pravidlove? metody znac?kova?n?? c?es?tiny.
(Combining Statistical and Rule-Based Approaches to
Morphological Tagging of Czech Texts). PhD Thesis,
MFF UK, in prep.
Pasi Tapanainen and Atro Voutilainen. 1994. Tagging
accurately: don?t guess if you know. In: Proceedings
of the 4th conference on Applied Natural Language
Processing, pp. 47?52. Stuttgart
Barbora Vidova?-Hladka?. 2000. Czech Language Tag-
ging. PhD thesis, U?FAL MFF UK. Prague
Jan Votrubec. 2005. Volba vhodny?ch rysu? pro morfolog-
icke? znac?kova?n?? c?es?tiny. (Feature Selection for Mor-
phological Tagging of Czech.) Master thesis, MFF,
Charles University, Prague.
Jan Votrubec. 2006. Morphological Tagging Based on
Averaged Perceptron. In: WDS?06 Proceedings of
Contributed Papers, MFF UK, pp. 191?195. Prague
74
Comparable Fora
Johanka Spoustova? Miroslav Spousta
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics,
Charles University Prague, Czech Republic
{johanka,spousta}@ufal.mff.cuni.cz
Abstract
As the title suggests, our paper deals with web
discussion fora, whose content can be consid-
ered to be a special type of comparable cor-
pora. We discuss the potential of this vast
amount of data available now on the World
Wide Web nearly for every language, regard-
ing both general and common topics as well
as the most obscure and specific ones. To il-
lustrate our ideas, we propose a case study
of seven wedding discussion fora in five lan-
guages.
1 Introduction to comparable corpora
Nearly every description of comparable corpora be-
gins with the EAGLES (Expert Advisory Group on
Language Engineering Standards) definition:1
?A comparable corpus is one which selects simi-
lar texts in more than one language or variety. The
possibilities of a comparable corpus are to com-
pare different languages or varieties in similar cir-
cumstances of communication, but avoiding the in-
evitable distortion introduced by the translations of
a parallel corpus.?
(Maia, 2003), which also became nearly standard
during the recent years, emphasizes the fact that
comparable monolingual corpora usually provide us
with much better linguistic quality and representa-
tiveness than translated parallel corpora. The other
advantages over the parallel corpora, i.e. amount
and availability, are obvious.
Nowadays, the most popular usage of compara-
ble corpora is improving machine translation, more
1http://www.ilc.cnr.it/EAGLES96/corpustyp/node21.html
precisely, compensating the lack of parallel train-
ing data. The articles (Munteanu et al, 2004),
(Munteanu and Marcu, 2005) and (Munteanu and
Marcu, 2006) are introducing algorithms for ex-
tracting parallel sentences and sub-sententional frag-
ments from comparable corpora and using the auto-
matically extracted parallel data for improving sta-
tistical machine translation algorithms performance.
Present day most popular comparable corpora
come either from the newswire resources (AFP,
Reuters, Xinhua), leading to data sets like LDC
English, Chinese and Arabic Gigaword, or from
Wikipedia. Mining Wikipedia became very popu-
lar in the recent years. For example, (Toma?s et al,
2008) is exploring both parallel and comparable po-
tential of Wikipedia, (Filatova, 2009) examines mul-
tilingual aspects of a selected subset of Wikipedia
and (Gamallo and Lo?pez, 2010) describes convert-
ing Wikipedia into ?CorpusPedia?.
2 Introduction to fora
Just to avoid confusion: In this article, we focus only
on fora or boards, i.e. standalone discussion sites on
a stated topic. We are not talking about comments
accompanying news articles or blog posts.
The internet discussion fora cover, in surprisingly
big amounts of data and for many languages, the
most unbelievable topics (real examples from the
authors? country). People, who eat only uncooked
(?raw?) food. People, who eat only cooked food.
Mothers with young children, women trying to con-
ceive, communities of people absolutely avoiding
sex. Fans of Volvo, BMW, Maserati, Trabant cars.
Probably also in your country mothers like to talk
96
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 96?101,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
about their children and men like to compare their
engine?s horse power.
Everyone who has any specific interest or hobby
and is friendly with the web, probably knows at least
one discussion forum focused on his/her favourite
topic, inhabited by intelligent, friendly debaters pro-
ducing interesting, on-topic content. These types of
fora often have very active administrators, who clean
the discussions from off-topics, vulgarities, move
the discussion threads into correct thematic cate-
gories etc. The administrators? ?tidying up? effort
can be even regarded as a kind of annotation.
The rapidly growing amount of web discussion
fora was until now linguistically exploited only in a
strictly monolingual manner. To the best of our (and
Google Scholar) knowledge, nobody has published
any work regarding the possibility of using internet
discussion fora as a multilingual source of data for
linguistic or machine translation purposes.
2.1 Forum structure
A typical forum is divided into thematic categories
(larger fora split into boards and boards into cate-
gories). Every category usually contains from tens
to thousands of separate discussions. A discussion
consists of messages (posts) and sometimes its con-
tent is further arranged using threads.
A discussion should be placed in appropriate cat-
egory and messages in the discussion should hold
onto the discussion topic, otherwise the administra-
tor removes the inappropriate messages or even the
whole discussion.
Fora usually have an entire off-topic category
where their members can talk about anything ?out-
of-domain?.
To avoid spam, usually only registered members
can contribute. Some fora keep their memberlist vis-
ible to the public, some do not.
3 Why comparable fora?
Besides their amount and availability, comparable
fora have a few other advantages over other types
of comparable corpora.
They contain ?spontaneous writing? ? an original,
previously unpublished content, which is almost cer-
tainly not a translation of other language original.
This is obviously not the case of parallel corpora,
and we cannot be sure even for other popular com-
parable corpora. A journalist may be inspired by a
news agency report or by another media source, and
a Wikipedia author must also reconcile his claims
with existing resources, which more or less affects
his writing style.
The other advantage is easier domain classifi-
cation, or more effective pre-selection before run-
ning an automatic parallel sentences alignment. A
generic newspaper article is provided only with a
title, language and release date. A Wikipedia en-
try has a title, history and is classified into a the-
matic category. Fora messages have both dates, titles
and category classifications and they are available in
much larger amounts than Wikipedia entries and are
covering more thematic domains than news articles.
4 A case study: wedding sites
As a topic of our case study, we have chosen an event
which occurs to most of the people at least once in
their life ? a wedding.
4.1 General overview
We looked over five language mutations of the
same forum operated by Asmira Company ? Fi-
nalstitch.co.uk (EN), Braupunkt.de (DE), Faire-
lanoce.fr (FR), Mojasvadba.sk (SK), Beremese.cz
(CZ); and two other fora, Brides.com/forums (EN2)
and Organisation-mariage.net (FR2), which seem to
be larger and more popular in the target countries.
We have manually examined fora sizes and possi-
bilities of their alignment on the category level.
Tables 1 and 2 summarize the total number of dis-
cussions and messages contained in selected cate-
gories, shared by most of the fora. For the Asmira
fora, we omitted the discussions accessible both
from CZ and SK sites.
If we assume average length of a message to be
about 60 words (see below), the proposed sites give
us a few millions of words of multilingual compa-
rable corpora in each category (focussed on very re-
stricted topic, such as wedding clothes, or hairdress-
ing & make-up) even for ?non-mainstream? lan-
guages, such as Czech or Slovak.
4.2 Quantitative characteristics
In order to learn more about the amount and textual
quality of the data, we have downloaded all the con-
97
EN DE FR CZ SK EN2 FR2
Ceremony and reception 389 280 232 1 532 2 345 N/A 1 536
Wedding-preparations 474 417 654 916 1270 13632 1 873
Date & location 63 119 154 839 529 371 N/A
Beauty 68 47 74 472 794 2 858 2 452Wedding clothing 291 166 200 715 1 108 10 832
After the wedding 37 47 47 236 245 1 530 390
Table 1: Total number of discussions in the selected wedding fora.
EN DE FR CZ SK EN2 FR2
Ceremony and reception 3 863 3 947 4 174 43 436 64 273 N/A 19 002
Wedding-preparations 4 908 4 987 8 867 51 880 27 837 130 408 24 585
Date & location 1 004 1 988 3 178 550 969 279 091 24 513 N/A
Beauty 692 852 1 462 32 118 32 620 15 946 38 582Wedding clothing 2 634 2 336 3 588 27 624 28 048 75 331
After the wedding 527 1 012 1 065 30 588 18 090 23 612 6 286
Table 2: Total number of messages in the selected wedding fora.
tent of the five Asmira fora, extracted their messages
into five monolingual corpora and measured some
basic characteristics of the texts. The downloading
and extracting task needed about 20 minutes of cod-
ing and a few days of waiting for the result (we did
not want to overload the fora webservers).
Table 3 shows us average messages lengths (in
words) for particular categories of these fora.
In graphs 1, 2 and 3, we present normalized sen-
tence length distributions for particular fora. For
English and Czech, we added for comparison sen-
tence length distributions of reference corpora of
comparable sizes, i.e. The Penn Treebank, train-
ing set (Marcus et al, 1994), for English and The
Czech National Corpus, SYN2005 (CNC, 2005), for
Czech.
4.3 Examples of similar discussion topics
The category distinction may be still too coarse for
potential alignment. The site FR2 has a joint cate-
gory for Beauty and Wedding clothing, and on the
contrary, it has separate categories for Wedding and
Reception. Therefore, we tried to examine the fora
on a deeper level. In table 4, we present some exam-
ples of discussions on the same topic.
As you can guess, fully automatic alignment of
the discussion titles will not be an easy task. On the
other side, every machine translation specialist must
0 20 40 60 80 100
Sentence length
R
el
at
ive
 F
re
qu
en
cy
0
Forum
PTB
Figure 1: The EN forum and The Penn Treebank - sen-
tence length distributions.
shiver with pleasure when seeing some of the dis-
cussion titles to be almost translations of each other,
and it would be a sin to leave these data unexploited.
98
EN DE FR CZ SK
Ceremony and reception 70.0 68.7 51.9 59.7 56.9
Wedding-preparations 73.8 62.5 55.1 63.7 62.3
Date & location 59.2 56.4 61.7 52.0 48.8
Beauty 67.7 61.3 53.4 65.8 56.6
Wedding clothing 61.1 60.4 42.1 57.0 50.0
After the wedding 71.8 69.5 52.0 66.8 68.6
Table 3: Average messages lengths (in words) for the selected wedding fora categories.
0 20 40 60 80 100
Sentence length
R
el
at
ive
 F
re
qu
en
cy
0
Forum
CNC
Figure 2: The CZ forum and The Czech National Corpus
- sentence length distributions.
5 Technical issues
Of course, language mutations of the same forum
(sharing the same category structure and running on
the same forum engine) are a ?researcher?s dream?
and not the case of the majority of potential compa-
rable fora.
You will probably ask two questions: 1) How to
effectively extract messages from a site with undoc-
umented structure? 2) How to put together compara-
ble fora in multiple languages and how to align their
category hierarchy?
5.1 Messages mining
According to an internet source 2, about 96 % of in-
ternet discussion fora are powered by two most pop-
2http://www.qualityposts.com/ForumMarketShare.php
0 20 40 60 80 100
Sentence length
R
el
at
ive
 F
re
qu
en
cy
0
brautpunkt.de
fairelanoce.fr
mojasvatba.sk
Figure 3: The DE, FR and SK fora - sentence length dis-
tributions.
ular forum systems, phpBB and vBulletin, and an-
other 3 % are powered by Simple Machines Forum,
MyBB and Invision Power Board.
Our observation is, that small hobby fora run
mostly on unadapted (?as is?) phpBB or another free
system, while large commercial fora often have their
own systems.
If you intend to automatically process only a few
selected fora, you will probably use XPath queries
on the HTML Document Object Model. According
to our experience, it is very easy and straightforward
task to write a single wrapper for a particular forum.
But it would be nice, of course, to have a general
solution which does not rely on a fixed forum struc-
ture. Unfortunately, general web page cleaning al-
gorithms, e.g. Victor (Spousta et al, 2008), are not
99
EN2 How to set up a budget
DE Budget?
FR2 Financement mariage
CZ Jaky? ma?te rozpoc?et na svatbu???
SK Svadobny rozpocet
EN Mobile hair and makeup
DE Friseur und Kosmetik daheim?
FR2 Estheticienne a domicile?
CZ Nal??c?en?? plus u?c?es doma - Praha
SK Licenie a uces - v den svadby a doma
EN Hair extensions?
DE Echthaar-Clip-Extensions
FR2 Extensions pour cheveux
CZ Prodluz?ova?n?? vlasu?
SK Predlzovanie vlasov
EN Where should we go for our honeymoon?
DE Habt ihr Tipps fu?r eine scho?ne
Hochzeitsreise???
FR2 Quelle destination pour le voyage de noce?
CZ Svatebn?? cesta
SK Kam idete na svadobnu? cestu?
Table 4: Examples of similar discussions.
very succesfull with this type of input (i.e. ten to
fifty rather small textual portions on one page).
However, there are some invariants shared among
all types of fora 3. The content is automatically gen-
erated and therefore all the messages on one page
(can be generalized to one site) usually ?look simi-
lar?, in terms of HTML structure. (Limanto et al,
2005) exploits this fact and introduces a subtree-
matching algorithm for detecting messages on a dis-
cussion page. (Li et al, 2009) proposes more com-
plex algorithm which extracts not only the messages
content but also the user profile information.
5.2 Fora coupling
The task of optimal fora, categories, discussions,
sentences and phrases alignment remains open. Our
article is meant to be an inspiration, thus for now,
we will not provide our reader with any surprising
practical solutions, only with ideas.
The sentence and sub-sentence level can be main-
tained by existing automatic aligners. For the rest,
we believe that combined use of hierarchical struc-
3and some other types of web sites, eg. e-shops or blogs
ture of the fora together with terms, named entities
or simple word translations can help. For example,
nearly every EU top level domain hosts a ?Volvo Fo-
rum? or ?Volvo Club?, and each Volvo Forum con-
tains some portion of discussions mentioning model
names, such as V70 or S60, in their titles.
Besides, according to our case study, the amount
of acquired data compared to the amount of hu-
man effort should be reasonable even when cou-
pling the fora sites and their top categories manu-
ally. Present day approaches to acquiring compara-
ble corpora also require some human knowledge and
effort, e.g. you need to pick out manually the most
reliable and appropriate news resources.
6 Conclusion
We have proposed an idea of using co-existent web
discussion fora in multiple languages addressing the
same topic as comparable corpora. Our case study
shows that using this approach, one can acquire large
portions of comparable multilingual data with min-
imal effort. We also discussed related technical is-
sues.
You may ask, whether the forum language is the
right (addition to a) training set for a machine trans-
lation system. The answer may depend on, what
type of system it is and what type of input do you
want to translate. If you need to translate parliamen-
tary proceedings, you will surely be more satisfied
with parliament-only training data. But do you want
an anything-to-speech machine translation system to
talk to you like a parliamentary speaker, or like a
Wikipedia author, or like a friend of yours from your
favourite community of interest?
We hope that our article drew the attention of the
linguistic audience to this promising source of com-
parable texts and we are looking forward to seeing
some interesting resources and applications.
Acknowledgments
The research described here was supported by the
project GA405/09/0278 of the Grant Agency of the
Czech Republic.
100
References
CNC, 2005. Czech National Corpus ? SYN2005. In-
stitute of Czech National Corpus, Faculty of Arts,
Charles University, Prague, Czech Republic.
Elena Filatova. 2009. Directions for exploiting asymme-
tries in multilingual wikipedia. In Proceedings of the
Third International Workshop on Cross Lingual Infor-
mation Access: Addressing the Information Need of
Multilingual Societies, CLIAWS3 ?09, pages 30?37,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Pablo Gamallo and Isaac Gonza?lez Lo?pez. 2010.
Wikipedia as multilingual source of comparable cor-
pora. In Proceedings of the LREC Workshop on Build-
ing and Using Comparable Corpora, pages 30?37.
Suke Li, Liyong Tang, Jianbin Hu, and Zhong Chen.
2009. Automatic data extraction from web discussion
forums. Frontier of Computer Science and Technol-
ogy, Japan-China Joint Workshop on, 0:219?225.
Hanny Yulius Limanto, Nguyen Ngoc Giang, Vo Tan
Trung, Jun Zhang, Qi He, and Nguyen Quang Huy.
2005. An information extraction engine for web dis-
cussion forums. In Special interest tracks and posters
of the 14th international conference on World Wide
Web, WWW ?05, pages 978?979, New York, NY,
USA. ACM.
Belinda Maia. 2003. What are comparable corpora?
In Proceedings of the Workshop on Multilingual Cor-
pora: Linguistic requirements and technical perspec-
tives, at the Corpus Linguistics 2003, pages 27?34,
Lancaster, UK, March.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4).
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 81?88, Sydney, Australia,
July. Association for Computational Linguistics.
Dragos Stefan Munteanu, Alexander Fraser, and Daniel
Marcu. 2004. Improved machine translation perfor-
mance via parallel sentence extraction from compara-
ble corpora. In HLT-NAACL 2004: Main Proceedings,
pages 265?272, Boston, Massachusetts, USA, May.
Association for Computational Linguistics.
Miroslav Spousta, Michal Marek, and Pavel Pecina.
2008. Victor: the web-page cleaning tool. In Pro-
ceedings of the Web as Corpus Workshop (WAC-4),
Marrakech, Morocco.
Jesu?s Toma?s, Jordi Bataller, Francisco Casacuberta, and
Jaime Lloret. 2008. Mining wikipedia as a parallel
and comparable corpus. Language Forum, 34.
101
