Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 48?56,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Engineering of Syntactic Features for Shallow Semantic Parsing
Alessandro Moschitti?
? DISP - University of Rome ?Tor Vergata?, Rome, Italy
{moschitti, pighin, basili}@info.uniroma2.it
? ITC-Irst, ? DIT - University of Trento, Povo-Trento, Italy
coppolab@itc.it
Bonaventura Coppola?? Daniele Pighin? Roberto Basili?
Abstract
Recent natural language learning research
has shown that structural kernels can be
effectively used to induce accurate models
of linguistic phenomena.
In this paper, we show that the above prop-
erties hold on a novel task related to predi-
cate argument classification. A tree kernel
for selecting the subtrees which encodes
argument structures is applied. Experi-
ments with Support Vector Machines on
large data sets (i.e. the PropBank collec-
tion) show that such kernel improves the
recognition of argument boundaries.
1 Introduction
The design of features for natural language process-
ing tasks is, in general, a critical problem. The inher-
ent complexity of linguistic phenomena, often char-
acterized by structured data, makes difficult to find
effective linear feature representations for the target
learning models.
In many cases, the traditional feature selection
techniques (Kohavi and Sommerfield, 1995) are not
so useful since the critical problem relates to feature
generation rather than selection. For example, the
design of features for a natural language syntactic
parse-tree re-ranking problem (Collins, 2000) can-
not be carried out without a deep knowledge about
automatic syntactic parsing. The modeling of syn-
tactic/semantic based features should take into ac-
count linguistic aspects to detect the interesting con-
text, e.g. the ancestor nodes or the semantic depen-
dencies (Toutanova et al, 2004).
A viable alternative has been proposed in (Collins
and Duffy, 2002), where convolution kernels were
used to implicitly define a tree substructure space.
The selection of the relevant structural features was
left to the voted perceptron learning algorithm. An-
other interesting model for parsing re-ranking based
on tree kernel is presented in (Taskar et al, 2004).
The good results show that tree kernels are very
promising for automatic feature engineering, espe-
cially when the available knowledge about the phe-
nomenon is limited.
Along the same line, automatic learning tasks that
rely on syntactic information may take advantage of
a tree kernel approach. One of such tasks is the au-
tomatic boundary detection of predicate arguments
of the kind defined in PropBank (Kingsbury and
Palmer, 2002). For this purpose, given a predicate p
in a sentence s, we can define the notion of predicate
argument spanning trees (PAST s) as those syntac-
tic subtrees of s which exactly cover all and only
the p?s arguments (see Section 4.1). The set of non-
spanning trees can be then associated with all the
remaining subtrees of s.
An automatic classifier which recognizes the
spanning trees can potentially be used to detect the
predicate argument boundaries. Unfortunately, the
application of such classifier to all possible sen-
tence subtrees would require an exponential execu-
tion time. As a consequence, we can use it only to
decide for a reduced set of subtrees associated with
a corresponding set of candidate boundaries. Notice
how these can be detected by previous approaches
48
(e.g. (Pradhan et al, 2004)) in which a traditional
boundary classifier (tbc) labels the parse-tree nodes
as potential arguments (PA). Such classifiers, gen-
erally, are not sensitive to the overall argument struc-
ture. On the contrary, a PAST classifier (pastc) can
consider the overall argument structure encoded in
the associated subtree. This is induced by the PA
subsets.
The feature design for the PAST representation
is not simple. Tree kernels are a viable alternative
that allows the learning algorithm to measure the
similarity between two PAST s in term of all pos-
sible tree substructures.
In this paper, we designed and experimented a
boundary classifier for predicate argument labeling
based on two phases: (1) a first annotation of po-
tential arguments by using a high recall tbc and
(2) a PAST classification step aiming to select the
correct substructures associated with potential argu-
ments. Both classifiers are based on Support Vector
Machines learning. The pastc uses the tree kernel
function defined in (Collins and Duffy, 2002). The
results show that the PAST classification can be
learned with high accuracy (the f-measure is about
89%) and the impact on the overall boundary detec-
tion accuracy is good.
In the remainder of this paper, Section 2 intro-
duces the Semantic Role Labeling problem along
with the boundary detection subtask. Section 3 de-
fines the SVMs using the linear kernel and the parse
tree kernel for boundary detection. Section 4 de-
scribes our boundary detection algorithm. Section 5
shows the preliminary comparative results between
the traditional and the two-step boundary detection.
Finally, Section 7 summarizes the conclusions.
2 Automated Semantic Role Labeling
One of the largest resources of manually annotated
predicate argument structures has been developed in
the PropBank (PB) project. The PB corpus contains
300,000 words annotated with predicative informa-
tion on top of the Penn Treebank 2 Wall Street Jour-
nal texts. For any given predicate, the expected ar-
guments are labeled sequentially from Arg0 to Arg9,
ArgA and ArgM. Figure 1 shows an example of
the PB predicate annotation of the sentence: John
rented a room in Boston.
Predicates in PB are only embodied by verbs
whereas most of the times Arg0 is the subject, Arg1
is the direct object and ArgM indicates locations, as
in our example.
 
 
 
 
 
 
 
 
 
Predicate 
Arg. 0 
Arg. M 
S 
N 
NP 
D N 
VP 
V John 
in 
 rented 
a 
  room 
PP 
IN N 
Boston 
Arg. 1 
Figure 1: A predicate argument structure in a parse-tree rep-
resentation.
Several machine learning approaches for auto-
matic predicate argument extraction have been de-
veloped, e.g. (Gildea and Jurasfky, 2002; Gildea and
Palmer, 2002; Gildea and Hockenmaier, 2003; Prad-
han et al, 2004). Their common characteristic is
the adoption of feature spaces that model predicate-
argument structures in a flat feature representation.
In the next section, we present the common parse
tree-based approach to this problem.
2.1 Predicate Argument Extraction
Given a sentence in natural language, all the predi-
cates associated with the verbs have to be identified
along with their arguments. This problem is usually
divided in two subtasks: (a) the detection of the tar-
get argument boundaries, i.e. the span of its words
in the sentence, and (b) the classification of the argu-
ment type, e.g. Arg0 or ArgM in PropBank or Agent
and Goal in FrameNet.
The standard approach to learn both the detection
and the classification of predicate arguments is sum-
marized by the following steps:
1. Given a sentence from the training-set, gener-
ate a full syntactic parse-tree;
2. let P and A be the set of predicates and the
set of parse-tree nodes (i.e. the potential argu-
ments), respectively;
3. for each pair < p, a >? P ?A:
? extract the feature representation set, Fp,a;
49
? if the subtree rooted in a covers exactly
the words of one argument of p, put Fp,a
in T+ (positive examples), otherwise put
it in T? (negative examples).
For instance, in Figure 1, for each combination of
the predicate rent with the nodes N, S, VP, V, NP,
PP, D or IN the instances Frent,a are generated. In
case the node a exactly covers ?John?, ?a room? or
?in Boston?, it will be a positive instance otherwise
it will be a negative one, e.g. Frent,IN .
The T+ and T? sets are used to train the bound-
ary classifier. To train the multi-class classifier T+
can be reorganized as positive T+argi and negative
T?argi examples for each argument i. In this way,
an individual ONE-vs-ALL classifier for each argu-
ment i can be trained. We adopted this solution, ac-
cording to (Pradhan et al, 2004), since it is simple
and effective. In the classification phase, given an
unseen sentence, all its Fp,a are generated and clas-
sified by each individual classifier Ci. The argument
associated with the maximum among the scores pro-
vided by the individual classifiers is eventually se-
lected.
2.2 Standard feature space
The discovery of relevant features is, as usual, a
complex task. However, there is a common con-
sensus on the set of basic features. These stan-
dard features, firstly proposed in (Gildea and Juras-
fky, 2002), refer to unstructured information de-
rived from parse trees, i.e. Phrase Type, Predicate
Word, Head Word, Governing Category, Position
and Voice. For example, the Phrase Type indicates
the syntactic type of the phrase labeled as a predicate
argument, e.g. NP for Arg1 in Figure 1. The Parse
Tree Path contains the path in the parse tree between
the predicate and the argument phrase, expressed as
a sequence of nonterminal labels linked by direction
(up or down) symbols, e.g. V ? VP ? NP for Arg1 in
Figure 1. The Predicate Word is the surface form of
the verbal predicate, e.g. rent for all arguments.
In the next section we describe the SVM approach
and the basic kernel theory for the predicate argu-
ment classification.
3 Learning predicate structures via
Support Vector Machines
Given a vector space in <n and a set of positive and
negative points, SVMs classify vectors according to
a separating hyperplane, H(~x) = ~w ? ~x + b = 0,
where ~w ? <n and b ? < are learned by applying
the Structural Risk Minimization principle (Vapnik,
1995).
To apply the SVM algorithm to Predicate Argu-
ment Classification, we need a function ? : F ? <n
to map our features space F = {f1, .., f|F|} and our
predicate/argument pair representation, Fp,a = Fz ,
into <n, such that:
Fz ? ?(Fz) = (?1(Fz), .., ?n(Fz))
From the kernel theory we have that:
H(~x) =
( ?
i=1..l
?i~xi
)
? ~x+ b =
?
i=1..l
?i~xi ? ~x+ b =
?
i=1..l
?i?(Fi) ? ?(Fz) + b.
where, Fi ?i ? {1, .., l} are the training instances
and the product K(Fi, Fz) =<?(Fi) ??(Fz)> is the
kernel function associated with the mapping ?.
The simplest mapping that we can apply is
?(Fz) = ~z = (z1, ..., zn) where zi = 1 if fi ? Fz
and zi = 0 otherwise, i.e. the characteristic vector
of the set Fz with respect to F . If we choose the
scalar product as a kernel function we obtain the lin-
ear kernel KL(Fx, Fz) = ~x ? ~z.
An interesting property is that we do not need to
evaluate the ? function to compute the above vector.
Only the K(~x, ~z) values are in fact required. This al-
lows us to derive efficient classifiers in a huge (pos-
sible infinite) feature space, provided that the ker-
nel is processed in an efficient way. This property
is also exploited to design convolution kernel like
those based on tree structures.
3.1 The tree kernel function
The main idea of the tree kernels is the modeling of
a KT (T1, T2) function which computes the number
of common substructures between two trees T1 and
T2.
Given the set of substructures (fragments)
{f1, f2, ..} = F extracted from all the trees of the
training set, we define the indicator function Ii(n)
50
 S 
NP VP 
VP VP CC 
VB NP 
took DT NN 
the book 
and VB NP 
read PRP$ NN 
its title 
PRP 
John 
S 
NP VP 
VP 
VB NP 
read 
Sentence Parse-Tree 
S 
NP VP 
VP 
VB NP 
  took 
took{ARG0, ARG1} 
PRP 
John 
PRP 
John 
DT NN 
the book 
PRP$ NN 
its title 
read{ARG0, ARG1} 
Figure 2: A sentence parse tree with two predicative tree structures (PAST s)
which is equal 1 if the target fi is rooted at node n
and 0 otherwise. It follows that:
KT (T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2) (1)
where NT1 and NT2 are the sets of the T1?s
and T2?s nodes, respectively and ?(n1, n2) =?|F|
i=1 Ii(n1)Ii(n2). This latter is equal to the num-
ber of common fragments rooted at the n1 and n2
nodes. We can compute ? as follows:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminals symbols) then ?(n1, n2) =
1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) =
nc(n1)?
j=1
(1 + ?(cjn1 , cjn2)) (2)
where nc(n1) is the number of the children of n1
and cjn is the j-th child of the node n. Note that, as
the productions are the same, nc(n1) = nc(n2).
The above kernel has the drawback of assigning
higher weights to larger structures1. In order to over-
come this problem we scale the relative importance
of the tree fragments imposing a parameter ? in con-
ditions 2 and 3 as follows: ?(nx, nz) = ? and
?(nx, nz) = ?
?nc(nx)
j=1 (1 + ?(cjn1 , cjn2)).
1In order to approach this problem and to map similarity
scores in the [0,1] range, a normalization in the kernel space,
i.e. K?T (T1, T2) = KT (T1,T2)?KT (T1,T1)?KT (T2,T2) . is always applied
4 Boundary detection via argument
spanning
Section 2 has shown that traditional argument
boundary classifiers rely only on features extracted
from the current potential argument node. In or-
der to take into account a complete argument struc-
ture information, the classifier should select a set of
parse-tree nodes and consider them as potential ar-
guments of the target predicate. The number of all
possible subsets is exponential in the number of the
parse-tree nodes of the sentence, thus, we need to
cut the search space. For such purpose, a traditional
boundary classifier can be applied to select the set
of potential arguments PA. The reduced number of
PA subsets can be associated with sentence subtrees
which in turn can be classified by using tree kernel
functions. These measure if a subtree is compatible
or not with the subtree of a correct predicate argu-
ment structure.
4.1 The Predicate Argument Spanning Trees
(PAST s)
We consider the predicate argument structures an-
notated in PropBank along with the corresponding
TreeBank data as our object space. Given the target
predicate p in a sentence parse tree T and a subset
s = {n1, .., nk} of the T?s nodes, NT , we define as
the spanning tree root r the lowest common ancestor
of n1, .., nk. The node spanning tree (NST ), ps is
the subtree rooted in r, from which the nodes that
are neither ancestors nor descendants of any ni are
removed.
Since predicate arguments are associated with
tree nodes, we can define the predicate argu-
51
 S 
NP VP 
VB NP 
read 
John 
DT NN 
the title 
NP PP 
DT NN 
the book 
NP IN 
of 
Arg. 1 
Arg. 0 
S 
NP VP 
VB NP 
read 
John 
DT NN 
the title 
NP PP 
DT NN 
the book 
NP IN 
of 
S 
NP VP 
VB NP 
read 
John 
DT NN 
the title 
NP PP 
DT NN 
the book 
NP IN 
of 
S 
NP-0 VP 
John 
PP 
DT NN 
the book 
NP IN 
of 
S 
NP-0 VP 
VB NP 
read 
John 
DT NN 
the title 
NP-1 PP-2 
DT NN 
the book 
IN 
of 
NP 
(a) (b) (c) 
Correct PAST 
Incorrect  PAST 
Correct PAST 
Incorrect  PAST 
DT NN 
the title 
NP 
NP-1 VB 
read 
 
 
 
Figure 3: Two-step boundary classifier.
ment spanning tree (PAST ) of a predicate ar-
gument set, {a1, .., an}, as the NST over such
nodes, i.e. p{a1,..,an}. A PAST corresponds
to the minimal subparse tree whose leaves are
all and only the word sequence compounding
the arguments. For example, Figure 2 shows
the parse tree of the sentence "John took the
book and read its title". took{ARG0,ARG1}
and read{ARG0,ARG1} are two PAST structures
associated with the two predicates took and read,
respectively. All the other NST s are not valid
PAST s.
Notice that, labeling ps, ?s ? NT with a PAST
classifier (pastc) corresponds to solve the boundary
problem. The critical points for the application of
this strategy are: (1) how to design suitable features
for the PAST characterization. This new problem
requires a careful linguistic investigation about the
significant properties of the argument spanning trees
and (2) how to deal with the exponential number of
NST s.
For the first problem, the use of tree kernels over
the PAST s can be an alternative to the manual fea-
tures design as the learning machine, (e.g. SVMs)
can select the most relevant features from a high di-
mensional feature space. In other words, we can use
Eq. 1 to estimate the similarity between two PAST s
avoiding to define explicit features. The same idea
has been successfully applied to the parse-tree re-
ranking task (Taskar et al, 2004; Collins and Duffy,
2002) and predicate argument classification (Mos-
chitti, 2004).
For the second problem, i.e. the high computa-
tional complexity, we can cut the search space by us-
ing a traditional boundary classifier (tbc), e.g. (Prad-
han et al, 2004), which provides a small set of po-
tential argument nodes. Let PA be the set of nodes
located by tbc as arguments. We may consider the
set P of the NST s associated with any subset of
PA, i.e. P = {ps : s ? PA}. However, also
the classification ofP may be computationally prob-
lematic since theoretically there are |P| = 2|PA|
members.
In order to have a very efficient procedure, we
applied pastc to only the PA sets associated with
incorrect PAST s. A way to detect such incor-
rect NST s is to look for a node pair <n1, n2>?
PA ? PA of overlapping nodes, i.e. n1 is ances-
tor of n2 or viceversa. After we have detected such
nodes, we create two node sets PA1 = PA? {n1}
and PA2 = PA ? {n2} and classify them with the
pastc to select the correct set of argument bound-
aries. This procedure can be generalized to a set of
overlapping nodes O greater than 2 as reported in
Appendix 1.
Note that the algorithm selects a maximal set of
non-overlapping nodes, i.e. the first that is gener-
ated. Additionally, the worst case is rather rare thus
the algorithm is very fast on average.
The Figure 3 shows a working example of the
multi-stage classifier. In Frame (a), tbc labels as
potential arguments (gray color) three overlapping
nodes (in Arg.1). The overlap resolution algorithm
proposes two solutions (Frame (b)) of which only
one is correct. In fact, according to the second so-
lution the propositional phrase ?of the book? would
incorrectly be attached to the verbal predicate, i.e.
in contrast with the parse tree. The pastc, applied
52
to the two NST s, should detect this inconsistency
and provide the correct output. Note that, during the
learning, we generate the non-overlapping structures
in the same way to derive the positive and negative
examples.
4.2 Engineering Tree Fragment Features
In the Frame (b) of Figure 3, we show one of the
possible cases which pastc should deal with. The
critical problem is that the two NST s are perfectly
identical, thus, it is not possible to discern between
them using only their parse-tree fragments.
The solution to engineer novel features is to sim-
ply add the boundary information provided by the
tbc to the NST s. We mark with a progressive num-
ber the phrase type corresponding to an argument
node, starting from the leftmost argument. For ex-
ample, in the first NST of Frame (c), we mark
as NP-0 and NP-1 the first and second argument
nodes whereas in the second NST we have an hy-
pothesis of three arguments on the NP, NP and PP
nodes. We trasform them in NP-0, NP-1 and
PP-2.
This simple modification enables the tree ker-
nel to generate features useful to distinguish be-
tween two identical parse trees associated with dif-
ferent argument structures. For example, for the first
NST the fragments [NP-1 [NP][PP]], [NP
[DT][NN]] and [PP [IN][NP]] are gener-
ated. They do not match anymore with the [NP-0
[NP][PP]], [NP-1 [DT][NN]] and [PP-2
[IN][NP]] fragments of the second NST .
In order to verify the relevance of our model, the
next section provides empirical evidence about the
effectiveness of our approach.
5 The Experiments
The experiments were carried out with
the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes the tree kernels in the SVM-light
software (Joachims, 1999). For tbc, we used the
linear kernel with a regularization parameter (option
-c) equal to 1 and a cost-factor (option -j) of 10 to
have a higher Recall. For the pastc we used ? = 0.4
(see (Moschitti, 2004)).
As referring dataset, we used the PropBank cor-
pora available at www.cis.upenn.edu/?ace,
along with the Penn TreeBank 2
(www.cis.upenn.edu/?treebank) (Marcus et
al., 1993). This corpus contains about 53,700
sentences and a fixed split between training and
testing which has been used in other researches, e.g.
(Pradhan et al, 2004; Gildea and Palmer, 2002).
We did not include continuation and co-referring
arguments in our experiments.
We used sections from 02 to 07 (54,443 argu-
ment nodes and 1,343,046 non-argument nodes) to
train the traditional boundary classifier (tbc). Then,
we applied it to classify the sections from 08 to
21 (125,443 argument nodes vs. 3,010,673 non-
argument nodes). As results we obtained 2,988
NST s containing at least an overlapping node pair
out of the total 65,212 predicate structures (accord-
ing to the tbc decisions). From the 2,988 over-
lapping structures we extracted 3,624 positive and
4,461 negative NST s, that we used to train the
pastc.
The performance was evaluated with the F1 mea-
sure2 over the section 23. This contains 10,406 ar-
gument nodes out of 249,879 parse tree nodes. By
applying the tbc classifier we derived 235 overlap-
ping NSTs, from which we extracted 204 PAST s
and 385 incorrect predicate argument structures. On
such test data, the performance of pastc was very
high, i.e. 87.08% in Precision and 89.22% in Recall.
Using the pastc we removed from the tbc the PA
that cause overlaps. To measure the impact on the
boundary identification performance, we compared
it with three different boundary classification base-
lines:
? tbc: overlaps are ignored and no decision is
taken. This provides an upper bound for the
recall as no potential argument is rejected for
later labeling. Notice that, in presence of over-
lapping nodes, the sentence cannot be anno-
tated correctly.
? RND: one among the non-overlapping struc-
tures with maximal number of arguments is
randomly selected.
2F1 assigns equal importance to Precision P and Recall R,
i.e. F1 = 2P?RP+R .
53
tbc tbc+RND tbc+Heu tbc+pastc
P R F P R F P R F P R F
All Struct. 92.21 98.76 95.37 93.55 97.31 95.39 92.96 97.32 95.10 94.40 98.42 96.36
Overl. Struct. 98.29 65.8 78.83 74.00 72.27 73.13 68.12 75.23 71.50 89.61 92.68 91.11
Table 1: Two-steps boundary classification performance using the traditional boundary classifier tbc, the random selection of
non-overlapping structures (RND), the heuristic to select the most suitable non-overlapping node set (Heu) and the predicate
argument spanning tree classifier (pastc).
? Heu (heuristic): one of the NST s which con-
tain the nodes with the lowest overlapping
score is chosen. This score counts the number
of overlapping node pairs in the NST . For ex-
ample, in Figure 3.(a) we have a NP that over-
laps with two nodes NP and PP, thus it is as-
signed a score of 2.
The third row of Table 1 shows the results of tbc,
tbc + RND, tbc + Heu and tbc + pastc in the
columns 2,3,4 and 5, respectively. We note that:
? The tbc F1 is slightly higher than the result ob-
tained in (Pradhan et al, 2004), i.e. 95.37%
vs. 93.8% on same training/testing conditions,
i.e. (same PropBank version, same training and
testing split and same machine learning algo-
rithm). This is explained by the fact that we
did not include the continuations and the co-
referring arguments that are more difficult to
detect.
? Both RND and Heu do not improve the tbc re-
sult. This can be explained by observing that in
the 50% of the cases a correct node is removed.
? When, to select the correct node, the pastc is
used, the F1 increases of 1.49%, i.e. (96.86 vs.
95.37). This is a very good result considering
that to increase the very high baseline of tbc is
hard.
In order to give a fairer evaluation of our approach
we tested the above classifiers on the overlapping
structures only, i.e. we measured the pastc improve-
ment on all and only the structures that required its
application. Such reduced test set contains 642 ar-
gument nodes and 15,408 non-argument nodes. The
fourth row of Table 1 reports the classifier perfor-
mance on such task. We note that the pastc im-
proves the other heuristics of about 20%.
6 Related Work
Recently, many kernels for natural language applica-
tions have been designed. In what follows, we high-
light their difference and properties.
The tree kernel used in this article was proposed
in (Collins and Duffy, 2002) for syntactic parsing re-
ranking. It was experimented with the Voted Percep-
tron and was shown to improve the syntactic parsing.
A refinement of such technique was presented in
(Taskar et al, 2004). The substructures produced by
the proposed tree kernel were bound to local prop-
erties of the target parse tree and more lexical infor-
mation was added to the overall kernel function.
In (Zelenko et al, 2003), two kernels over syn-
tactic shallow parser structures were devised for
the extraction of linguistic relations, e.g. person-
affiliation. To measure the similarity between two
nodes, the contiguous string kernel and the sparse
string kernel (Lodhi et al, 2000) were used. The
former can be reduced to the contiguous substring
kernel whereas the latter can be transformed in the
non-contiguous string kernel. The high running time
complexity, caused by the general form of the frag-
ments, limited the experiments on data-set of just
200 news items.
In (Cumby and Roth, 2003), it is proposed a de-
scription language that models feature descriptors
to generate different feature type. The descriptors,
which are quantified logical prepositions, are instan-
tiated by means of a concept graph which encodes
the structural data. In the case of relation extraction
the concept graph is associated with a syntactic shal-
low parse and the extracted propositional features
express fragments of a such syntactic structure. The
experiments over the named entity class categoriza-
tion show that when the description language selects
an adequate set of tree fragments the Voted Percep-
tron algorithm increases its classification accuracy.
In (Culotta and Sorensen, 2004) a dependency
54
tree kernel is used to detect the Named Entity classes
in natural language texts. The major novelty was
the combination of the contiguous and sparse ker-
nels with the word kernel. The results show that
the contiguous outperforms the sparse kernel and the
bag-of-words.
7 Conclusions
The feature design for new natural language learn-
ing tasks is difficult. We can take advantage from
the kernel methods to model our intuitive knowledge
about the target linguistic phenomenon. In this pa-
per we have shown that we can exploit the properties
of tree kernels to engineer syntactic features for the
predicate argument boundary detection task.
Preliminary results on gold standard trees suggest
that (1) the information related to the whole predi-
cate argument structure is important and (2) tree ker-
nel can be used to generate syntactic features.
In the future, we would like to use an approach
similar to the PAST classifier on parses provided
by different parsing models to detect boundary and
to classify semantic role more accurately .
Acknowledgements
We wish to thank Ana-Maria Giuglea for her help in
the design and implementation of the basic Seman-
tic Role Labeling system that we used in the experi-
ments.
References
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In ACL02.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of ICML 2000.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 423?429,
Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel methods for
relational learning. In Proceedings of the Twentieth
International Conference (ICML 2003), Washington,
DC, USA.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, Sapporo,
Japan.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA, USA.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2002), Las Palmas, Spain.
Ron Kohavi and Dan Sommerfield. 1995. Feature sub-
set selection using the wrapper model: Overfitting and
dynamic search space topology. In The First Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 192?197. AAAI Press, Menlo Park,
California, August. Journal version in AIJ.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Christopher Watkins. 2000. Text clas-
sification using string kernels. In NIPS, pages 563?
569.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The Penn Treebank. Computational Linguistics,
19:313?330.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow semantic parsing. In proceedings of
the 42th Conference on Association for Computational
Linguistic (ACL-2004), Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. to appear in Machine Learning Journal.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Dekang Lin and Dekai Wu, editors, Proceedings of
EMNLP 2004, pages 1?8, Barcelona, Spain, July. As-
sociation for Computational Linguistics.
Kristina Toutanova, Penka Markova, and Christopher D.
Manning. 2004. The leaf projection path view of
parse trees: Exploring string kernels for hpsg parse se-
lection. In Proceedings of EMNLP 2004.
55
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research.
Appendix 1: Generalized Boundary
Selection Algorithm
Let O be the set of overlapping nodes of PA, and
NO the set of non overlapping nodes of PA.
Let subs(?1)(A) = {B|B ? 2A, |B| = |A| ? 1}.
Let O? = subs(?1)(O).
while(true)
begin
1. H = ?
2. ?o ? O?:
(a) If o does not include any overlapping node
pair
then H = H ? {o}
3. If H 6= ? then:
(a) Let s? =argmaxo?H pastc(pNO?o),
where pNO?o represents the node span-
ning tree compatible with o, and the
pastc(pNO?o) is the score provided by the
PAST SVM categorizer on it
(b) If pastc(s?) > 0 then RETURN( s?)
4. If O? = {?} then RETURN( NO )
5. Else:
(a) O? = O? ?H
(b) O? = ?o?O? subs(?1)(o)
end
56
Tree Kernel Engineering in Semantic Role Labeling Systems
Alessandro Moschitti and Daniele Pighin and Roberto Basili
University of Rome, Tor Vergata
{moschitti,basili}@info.uniroma2.it
daniele.pighin@gmail.com
Abstract
Recent work on the design of automatic
systems for semantic role labeling has
shown that feature engineering is a com-
plex task from a modeling and implemen-
tation point of view. Tree kernels alleviate
such complexity as kernel functions gener-
ate features automatically and require less
software development for data extraction.
In this paper, we study several tree kernel
approaches for both boundary detection
and argument classification. The compar-
ative experiments on Support Vector Ma-
chines with such kernels on the CoNLL
2005 dataset show that very simple tree
manipulations trigger automatic feature
engineering that highly improves accuracy
and efficiency in both phases. Moreover,
the use of different classifiers for internal
and pre-terminal nodes maintains the same
accuracy and highly improves efficiency.
1 Introduction
A lot of attention has been recently devoted to
the design of systems for the automatic label-
ing of semantic roles (SRL) as defined in two
important projects: FrameNet (Johnson and Fill-
more, 2000), inspired by Frame Semantics, and
PropBank (Kingsbury and Palmer, 2002) based
on Levin?s verb classes. In general, given a sen-
tence in natural language, the annotation of a pred-
icate?s semantic roles requires (1) the detection of
the target word that embodies the predicate and
(2) the detection and classification of the word se-
quences constituting the predicate?s arguments. In
particular, step (2) can be divided into two differ-
ent phases: (a) boundary detection, in which the
words of the sequence are detected and (b) argu-
ment classification, in which the type of the argu-
ment is selected.
Most machine learning models adopted for the
SRL task have shown that (shallow or deep) syn-
tactic information is necessary to achieve a good
labeling accuracy. This research brings a wide
empirical evidence in favor of the linking theories
between semantics and syntax, e.g. (Jackendoff,
1990). However, as no theory provides a sound
and complete treatment of such issue, the choice
and design of syntactic features for the automatic
learning of semantic structures requires remark-
able research efforts and intuition.
For example, the earlier studies concerning lin-
guistic features suitable for semantic role labeling
were carried out in (Gildea and Jurasfky, 2002).
Since then, researchers have proposed diverse syn-
tactic feature sets that only slightly enhance the
previous ones, e.g. (Xue and Palmer, 2004) or
(Carreras and Ma`rquez, 2005). A careful analy-
sis of such features reveals that most of them are
syntactic tree fragments of training sentences, thus
a natural way to represent them is the adoption of
tree kernels as described in (Moschitti, 2004). The
idea is to associate with each argument the mini-
mal subtree that includes the target predicate with
one of its arguments, and to use a tree kernel func-
tion to evaluate the number of common substruc-
tures between two such trees. Such approach is in
line with current research on the use of tree kernels
for natural language learning, e.g. syntactic pars-
ing re-ranking (Collins and Duffy, 2002), relation
extraction (Zelenko et al, 2003) and named entity
recognition (Cumby and Roth, 2003; Culotta and
Sorensen, 2004).
Regarding the use of tree kernels for SRL, in
(Moschitti, 2004) two main drawbacks have been
49
pointed out:
? Highly accurate boundary detection cannot
be carried out by a tree kernel model since
correct and incorrect arguments may share a
large portion of the encoding trees, i.e. they
may share many substructures.
? Manually derived features (extended with a
polynomial kernel) have been shown to be su-
perior to tree kernel approaches.
Nevertheless, we believe that modeling a com-
pletely kernelized SRL system is useful for the fol-
lowing reasons:
? We can implement it very quickly as the fea-
ture extractor module only requires the writ-
ing of the subtree extraction procedure. Tra-
ditional SRL systems are, in contrast, based
on the extraction of more than thirty features
(Pradhan et al, 2005), which require the writ-
ing of at least thirty different procedures.
? Combining it with a traditional attribute-
value SRL system allows us to obtain a more
accurate system. Usually the combination of
two traditional systems (based on the same
machine learning model) does not result in
an improvement as their features are more
or less equivalent as shown in (Carreras and
Ma`rquez, 2005).
? The study of the effective structural features
can inspire the design of novel linear fea-
tures which can be used with a more efficient
model (i.e. linear SVMs).
In this paper, we carry out tree kernel engineer-
ing (Moschitti et al, 2005) to increase both ac-
curacy and speed of the boundary detection and
argument classification phases. The engineering
approach relates to marking the nodes of the en-
coding subtrees in order to generate substructures
more strictly correlated with a particular argu-
ment, boundary or predicate. For example, mark-
ing the node that exactly covers the target ar-
gument helps tree kernels to generate different
substructures for correct and incorrect argument
boundaries.
The other technique that we applied to engineer
different kernels is the subdivision of internal and
pre-terminal nodes. We show that designing dif-
ferent classifiers for these two different node types
slightly increases the accuracy and remarkably de-
creases the learning and classification time.
An extensive experimentation of our tree ker-
nels with Support Vector Machines on the CoNLL
2005 data set provides interesting insights on the
design of performant SRL systems entirely based
on tree kernels.
In the remainder of this paper, Section 2 intro-
duces basic notions on SRL systems and tree ker-
nels. Section 3 illustrates our new kernels for both
boundary and classification tasks. Section 4 shows
the experiments of SVMs with the above tree ker-
nel based classifiers.
2 Preliminary Concepts
In this section we briefly define the SRL model
that we intend to design and the kernel function
that we use to evaluate the similarity between sub-
trees.
2.1 Basic SRL approach
The SRL approach that we adopt is based on the
deep syntactic parse (Charniak, 2000) of the sen-
tence that we intend to annotate semantically. The
standard algorithm is to classify the tree node pair
?p, a?, where p and a are the nodes that exactly
cover the target predicate and a potential argu-
ment, respectively. If ?p, a? is labeled with an ar-
gument, then the terminal nodes dominated by a
will be considered as the words constituting such
argument. The number of pairs for each sentence
can be hundreds, thus, if we consider training cor-
pora of thousands of sentences, we have to deal
with millions of training instances.
The usual solution to limit such complexity is to
divide the labeling task in two subtasks:
? Boundary detection, in which a single clas-
sifier is trained on many instances to detect
if a node is an argument or not, i.e. if the
sequence of words dominated by the target
node constitutes a correct boundary.
? Argument classification: only the set of
nodes corresponding to correct boundaries
are considered. These can be used to train a
multiclassifier that, for such nodes, only de-
cides the type of the argument. For example,
we can train n classifiers in the style One-vs-
All. At classification time, for each argument
node, we can select the argument type asso-
ciated with the maximum among the n scores
provided by the single classifiers.
50
We adopt this solution as it enables us to use
only one computationally expensive classifier, i.e.
the boundary detection one. This, as well as the
argument classifiers, requires a feature represen-
tation of the predicate-argument pair. Such fea-
tures are mainly extracted from the parse trees of
the target sentence, e.g. Phrase Type, Predicate
Word, Head Word, Governing Category, Position
and Voice proposed in (Gildea and Jurasfky, 2002).
As most of the features proposed in literature
are subsumed by tree fragments, tree-kernel func-
tions are a natural way to produce them automati-
cally.
2.2 Tree kernel functions
Tree-kernel functions simply evaluate the number
of substructures shared between two trees T1 and
T2. Such functions can be seen as a scalar product
in the huge vector space constituted by all possi-
ble substructures of the training set. Thus, kernel
functions implicitly define a large feature space.
Formally, given a tree fragment space
{f1, f2, ..} = F , we can define an indica-
tor function Ii(n), which is equal to 1 if the
target fi is rooted at node n and equal to
0 otherwise. Therefore, a tree-kernel func-
tion K over T1 and T2 can be defined as
K(T1, T2) =
?
n1?NT1
?
n2?NT2 ?(n1, n2),
where NT1 and NT2 are the sets of the
T1?s and T2?s nodes, respectively and
?(n1, n2) =
?|F|
i=1 Ii(n1)Ii(n2). This latter
is equal to the number of common fragments
rooted at nodes n1 and n2 and, according to
(Collins and Duffy, 2002), it can be computed as
follows:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the
same, and n1 and n2 have only leaf chil-
dren (i.e. they are pre-terminal symbols) then
?(n1, n2) = ?;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminal then
?(n1, n2) = ?
?nc(n1)
j=1 (1 + ?(cjn1 , cjn2)).
where ? is the decay factor to scale down the im-
pact of large structures, nc(n1) is the number of
the children of n1 and cjn is the j-th child of the
node n. Note that, as the productions are the same,
nc(n1) = nc(n2). Additionally, to map similar-
ity scores in the [0,1] range, we applied a nor-
Figure 1: The PAF subtree associated with A1.
Figure 2: Example of CMST.
malization in the kernel space, i.e. K ?(T1, T2) =
K(T1,T2)?
K(T1,T1)?K(T2,T2)
.
Once a kernel function is defined, we need to
characterize the predicate-argument pair with a
subtree. This allows kernel machines to generate a
large number of syntactic features related to such
pair. The approach proposed in (Moschitti, 2004)
selects the minimal subtree that includes a predi-
cate with its argument. We follow such approach
by studying and proposing novel, interesting solu-
tions.
3 Novel Kernels for SRL
The basic structure used to characterize the predi-
cate argument relation is the smallest subtree that
includes a predicate with one of its argument. For
example, in Figure 1, the dashed line encloses a
predicate argument feature (PAF) over the parse
tree of the sentence: ?Paul delivers a talk in for-
mal style?. This PAF is a subtree that characterizes
the predicate to deliver with its argument a talk.
In this section, we improve PAFs, propose dif-
ferent kernels for internal and pre-terminal nodes
and new kernels based on complete predicate ar-
51
Figure 3: Differences between PAF (a) and MPAF (b) structures.
gument structures.
3.1 Improving PAF
PAFs have shown to be very effective for argu-
ment classification but not for boundary detection.
The reason is that two nodes that encode correct
and incorrect boundaries may generate very sim-
ilar PAFs. For example, Figure 3.A shows two
PAFs corresponding to a correct (PAF+) and an
incorrect (PAF-) choice of the boundary for A1:
PAF+ from the NP vs. PAF- from the N nodes. The
number of their common substructures is high, i.e.
the four subtrees shown in Frame C. This prevents
the algorithm from making different decisions for
such cases.
To solve this problem, we specify which is the
node that exactly covers the argument (also called
argument node) by simply marking it with the la-
bel B denoting the boundary property. Figure 3.B
shows the two new marked PAFs (MPAFs). The
features generated from the two subtrees are now
very different so that there is only one substructure
in common (see Frame D). Note that, each markup
strategy impacts on the output of a kernel function
in terms of the number of structures common to
two trees. The same output can be obtained us-
ing unmarked trees and redefining consistently the
kernel function, e.g. the algorithm described in
Section 2.2.
An alternative way to partially solve the struc-
ture overlapping problem is the use of two differ-
ent classifiers, one for the internal nodes and one
for the pre-terminal nodes, and combining their
decisions. In this way, the negative example of
Figure 3 would not be used to train the same clas-
sifier that uses PAF+. Of course, similar structures
can both be rooted on internal nodes, therefore
they can belong to the training data of the same
classifier. However, the use of different classi-
fiers is motivated also by the fact that many ar-
gument types can be found mostly in pre-terminal
nodes, e.g. modifier or negation arguments, and
do not necessitate training data extracted from in-
ternal nodes. Consequently, it is more convenient
(at least from a computational point of view) to
use two different boundary classifiers, hereinafter
referred to as combined classifier.
3.2 Kernels on complete predicate argument
structures
The type of a target argument strongly depends on
the type and number of the predicate?s arguments1
(Punyakanok et al, 2005; Toutanova et al, 2005).
Consequently, to correctly label an argument, we
should extract features from the complete predi-
cate argument structure it belongs to. In contrast,
PAFs completely neglect the information (i.e. the
tree portions) related to non-target arguments.
One way to use this further information with
tree kernels is to use the minimum subtree that
spans all the predicate?s arguments. The whole
parse tree in Figure 1 is an example of such Min-
imum Spanning Tree (MST) as it includes all and
only the argument structures of the predicate ?to
deliver?. However, MSTs pose some problems:
? We cannot use them for the boundary detec-
tion task since we do not know the predi-
cate?s argument structure yet. However, we
can derive the MST (its approximation) from
the nodes selected by a boundary classifier,
i.e. the nodes that correspond to potential ar-
guments. Such approximated MSTs can be
easily used in the argument type classifica-
tion phase. They can also be used to re-rank
the most probable m sequences of arguments
for both labeling phases.
? Obviously, an MST is the same for all the
arguments it includes, thus we need a way
to differentiate it for each target argument.
1This is true at least for core arguments.
52
Again, we can mark the node that exactly
covers the target argument as shown in the
previous section. We refer to this subtree as
marked MST (MMST). However, for large
arguments (i.e. spread on a large part of the
sentence tree) the substructures? likelihood of
being part of other arguments is quite high.
To address this latter problem, we can mark all
nodes that descend from the target argument node.
Figure 2 shows a MST in which the subtree as-
sociated with the target argument (AM) has the
nodes marked. We refer to this structure as a
completely marked MST (CMST). CMSTs may
be seen as PAFs enriched with new information
coming from the other arguments (i.e. the non-
marked subtrees). Note that if we consider only
the PAF subtree from a CMST we obtain a differ-
ently marked subtree which we refer to as CPAF.
In the next section we study the impact of the
proposed kernels on the boundary detection and
argument classification performance.
4 Experiments
In these experiments we evaluate the impact of our
proposed kernels in terms of accuracy and effi-
ciency. The accuracy improvement confirms that
the node marking approach enables the automatic
engineering of effective SRL features. The effi-
ciency improvement depends on (a) the less train-
ing data used when applying two distinct type clas-
sifiers for internal and pre-terminal nodes and (b) a
more adequate feature space which allows SVMs
to converge faster to a model containing a smaller
number of support vectors, i.e. faster training and
classification.
4.1 Experimental set up
The empirical evaluations were carried out within
the setting defined in the CoNLL-2005 Shared
Task (Carreras and Ma`rquez, 2005). We
used as a target dataset the PropBank corpus
available at www.cis.upenn.edu/?ace, along
with the Penn TreeBank 2 for the gold trees
(www.cis.upenn.edu/?treebank) (Marcus et al,
1993), which includes about 53,700 sentences.
Since the aim of this study was to design a real
SRL system we adopted the Charniak parse trees
from the CoNLL 2005 Shared Task data (available
at www.lsi.upc.edu/?srlconll/).
We used Section 02, 03 and 24 from the Penn
TreeBank in most of the experiments. Their char-
acteristics are shown in Table 1. Pos and Neg in-
dicate the number of nodes corresponding or not
to a correct argument boundary. Rows 3 and 4 re-
port such number for the internal and pre-terminal
nodes separately. We note that the latter are much
fewer than the former; this results in a very fast
pre-terminal classifier.
As the automatic parse trees contain errors,
some arguments cannot be associated with any
covering node. This prevents us to extract a tree
representation for them. Consequently, we do not
consider them in our evaluation. In sections 2, 3
and 24 there are 454, 347 and 731 such cases, re-
spectively.
The experiments were carried out with
the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes fast tree kernel evaluation (Mos-
chitti, 2006) in the SVM-light software (Joachims,
1999). We used a regularization parameter (option
-c) equal to 1 and ? = 0.4 (see (Moschitti,
2004)).
4.2 Boundary Detection Results
In these experiments, we used Section 02 for train-
ing and Section 24 for testing. The results using
the PAF and the MPAF based kernels are reported
in Table 2 in rows 2 and 3, respectively. Columns
3 and 4 show the CPU testing time (in seconds)
and the F1 of the monolithic boundary classifier.
The next 3 columns show the CPU time for the in-
ternal (Int) and pre-terminal (Pre) node classifiers,
as well as their total (All). The F1 measures are
reported in the 3 rightmost columns. In particular,
the third column refers to the F1 of the combined
classifier. This has been computed by summing
correct, incorrect and not retrieved examples of the
two distinct classifiers.
We note that: first, the monolithic classifier ap-
plied to MPAF improves both the efficiency, i.e.
about 3,131 seconds vs. 5,179, of PAF and the
F1, i.e. 82.07 vs. 75.24. This suggests that mark-
ing the argument node simplifies the generaliza-
tion process.
Second, by dividing the boundary classifica-
tion in two tasks, internal and pre-terminal nodes,
we furthermore improve the classification time for
both PAF and MPAF kernels, i.e. 5,179 vs. 1,851
(PAF) and 3,131 vs. 1,471 (MPAF). The sepa-
rated classifiers are much faster, especially the pre-
terminal one (about 61 seconds to classify 81,075
nodes).
53
Section 2 Section 3 Section 24
Nodes pos neg tot pos neg tot pos neg tot
Internal 11,847 71,126 82,973 6,403 53,591 59,994 7,525 50,123 57,648
Pre-terminal 894 114,052 114,946 620 86,232 86,852 709 80,366 81,075
Both 12,741 185,178 197,919 7,023 139,823 146,846 8,234 130,489 138,723
Table 1: Tree nodes of the sentences from sections 2, 3 and 24 of the PropBank. pos and neg are the
nodes that exactly cover arguments and all the other nodes, respectively.
Monolithic Combined
Tagging strategy CPUtime F1 CPUtime F1Int Pre All Int Pre All
PAF 5,179.18 75.24 1,794.92 56.72 1,851.64 79.93 79.39 79.89
MPAF 3,131.56 82.07 1,410.10 60.99 1,471.09 82.20 79.14 81.96
Table 2: F1 comparison between PAF and MPAF based kernels using different classification strategies.
Int, Pre and ALL are the internal, pre-terminal and combined classifiers. The CPU time refers to the
classification time in seconds of all Section 24.
Figure 4: Learning curve comparison between the
PAF and MPAF F1 measures using the combined
classifier.
Third, the combined classifier approach seems
quite feasible as its F1 is almost equal to the mono-
lithic one (81.96 vs. 82.07) in case of MPAF and
even superior when using PAF (79.89 vs. 75.34).
This result confirms the observation given in Sec-
tion 3.1 about the importance of reducing the num-
ber of substructures common to PAFs associated
with correct and incorrect boundaries.
Finally, we trained the combined boundary clas-
sifiers with sets of increasing size to derive the
learning curves of the PAF and MPAF models.
To have more significant results, we increased the
training set by using also sections from 03 to 07.
Figure 4 shows that the MPAF approach is con-
stantly over the PAF. Consider also that the mark-
ing strategy has a lesser impact on the combined
classifier.
4.3 Argument Classification Results
In these experiments we tested different kernels
on the argument classification task. As some ar-
guments have a very small number of training in-
stances in a single section, we also used Section
03 for training and we continued to test on only
Section 24.
The results of the multiclassifiers on 59 argu-
ment types2 (e.g. constituted by 59 binary clas-
sifiers in the monolithic approach) are reported in
Table 3. The rows from 3 to 5 report the accuracy
when using the PAF,MPAF and CPAFwhereas the
rows from 6 to 8 show the accuracy for the com-
plete argument structure approaches, i.e. MST,
MMST and CMST.
More in detail, Column 2 shows the accuracy of
the monolithic multi-argument classifiers whereas
Columns 3, 4 and 5 report the accuracy of the in-
ternal, pre-terminal and combined multi-argument
classifiers, respectively.
We note that:
First, the two classifier approach does not im-
prove the monolithic approach accuracy. Indeed,
the subtrees describing different argument types
are quite different and this property holds also for
the pre-terminal nodes. However, we still mea-
sured a remarkable improvement in efficiency.
Second, MPAF is the best kernel. This con-
firms the outcome on boundary detection ex-
periments. The fact that it is more accu-
rate than CPAF reveals that we need to distin-
27 for the core arguments (A0...AA), 13 for the adjunct
arguments (AM-*), 19 for the argument references (R-*) and
20 for the continuations (C-*).
54
Monolithic CombinedTagging strategy Internal nodes Pre-terminals Overall
PAF 75.06 74.16 85.61 75.15
MPAF 77.17 76.25 85.76 77.07
CPAF 76.79 75.68 85.76 76.54
MST 34.80 36.52 78.14 40.10
MMST 72.55 71.59 86.32 72.86
CMST 73.21 71.93 86.32 73.17
Table 3: Accuracy produced by different tree kernels on argument classification. We trained on sections
02 and 03 and tested on Section 24.
guish the argument node from the other nodes.
To explain this, suppose that two argument
nodes, NP1 and NP2, dominate the follow-
ing structures: [NP1 [NP [DT NN]][PP]]
and [NP2 [DT NN]]. If we mark only the
argument node we obtain [NP-B [NP [DT
NN]][PP]] and [NP-B [DT NN]] which
have no structure in common. In contrast, if
we mark them completely, i.e. [NP-B [NP-B
[DT-B NN-B]][PP-B]] and [NP-B [DT-B
NN-B]], they will share the subtree [NP-B
[DT-B NN-B]]. Thus, although it may seem
counterintuitive, by marking only one node, we
obtain more specific substructures. Of course, if
we use different labels for the argument nodes and
their descendants, we obtain the same specializa-
tion effect.
Finally, if we do not mark the target argument
in the MSTs, we obtain a very low result (i.e.
40.10%) as expected. When we mark the cover-
ing node or the complete argument subtree we ob-
tain an acceptable accuracy. Unfortunately, such
accuracy is lower than the one produced by PAFs,
e.g. 73.17% vs. 77.07%, thus it may seem that
the additional information provided by the whole
argument structure is not effective. A more care-
ful analysis can be carried out by considering a
CMST as composed by a PAF and the rest of the
argument structure. We observe that some pieces
of information provided by a PAF are not deriv-
able by a CMST (or a MMST). For example, Fig-
ure 1 shows that the PAF contains the subtree [VP
[V NP]] while the associated CMST (see Figure
2) contains [VP [V NP PP]]. The latter struc-
ture is larger and more sparse and consequently,
the learning machine applied to CMSTs (or MM-
STs) performs a more difficult generalization task.
This problem is emphasized by our use of the ad-
juncts in the design of MSTs. As adjuncts tend to
be the same for many predicates they do not pro-
vide a very discriminative information.
5 Discussions and Conclusions
The design of automatic systems for the labeling
of semantic roles requires the solution of complex
problems. Among others, feature engineering is
made difficult by the structural nature of the data,
i.e. features should represent information con-
tained in automatic parse trees. This raises two
problems: (1) the modeling of effective features,
partially solved in the literature work and (2) the
implementation of the software for the extraction
of a large number of such features.
A system completely based on tree kernels al-
leviate both problems as (1) kernel functions au-
tomatically generate features and (2) only a pro-
cedure for subtree extraction is needed. Although
some of the manual designed features seem to be
superior to those derived with tree kernels, their
combination seems still worth applying.
In this paper, we have improved tree kernels
by studying different strategies: MPAF and the
combined classifier (for internal and pre-terminal
nodes) highly improve efficiency and accuracy in
both the boundary detection and argument classi-
fication tasks. In particular, MPAF improves the
old PAF-based tree kernel of about 8 absolute per-
cent points in the boundary classification task, and
when used along the combined classifier approach
the speed of the model increases of 3.5 times. In
case of argument classification the improvement is
less evident but still consistent, about 2%.
We have also studied tree representations based
on complete argument structures (MSTs). Our
preliminary results seem to suggest that additional
information extracted from other arguments is not
effective. However, such findings are affected by
two main problems: (1) We used adjuncts in the
tree representation. They are likely to add more
noise than useful information for the recognition
of the argument type. (2) The traditional PAF
contains subtrees that cannot be derived by the
55
MMSTs, thus we should combine these structures
rather than substituting one with the other.
In the future, we plan to extend this study as
follows:
First, our results are computed individually for
boundary and classification tasks. Moreover, in
our experiments, we removed arguments whose
PAF or MST could not be extracted due to errors
in parse trees. Thus, we provided only indicative
accuracy to compare the different tree kernels. A
final evaluation of the most promising structures
using the CoNLL 2005 evaluator should be carried
out to obtain a sound evaluation.
Second, as PAFs and MSTs should be com-
bined to generate more information, we are go-
ing to carry out a set of experiments that com-
bine different kernels associated with different
subtrees. Moreover, as shown in (Basili and Mos-
chitti, 2005; Moschitti, 2006), there are other tree
kernel functions that generate different fragment
types. The combination of such functions with the
marking strategies may provide more general and
effective kernels.
Third, once the final set of the most promising
kernels is established, we would like to use all the
available CoNLL 2005 data. This would allow us
to study the potentiality of our approach by exactly
comparing with literature work.
Next, our fast tree kernel function along with
the combined classification approach and the im-
proved tree representation make the learning and
classification much faster so that the overall run-
ning time is comparable with polynomial kernels.
However, when these latter are used with SVMs
the running time is prohibitive when very large
datasets (e.g. millions of instances) are targeted.
Exploiting tree kernel derived features in a more
efficient way is thus an interesting line of future
research.
Finally, as CoNLL 2005 has shown that the
most important contribution relates on re-ranking
predicate argument structures based on one single
tree (Toutanova et al, 2005) or several trees (Pun-
yakanok et al, 2005), we would like to use tree
kernels for the re-ranking task.
Acknowledgments
This research is partially supported by the Euro-
pean project, PrestoSpace (FP6-IST-507336).
References
Roberto Basili and Alessandro Moschitti. 2005. Automatic
Text Categorization: from Information Retrieval to Sup-
port Vector Learning. Aracne Press, Rome, Italy.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL?05.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the NACL?00.
Michael Collins and Nigel Duffy. 2002. New ranking al-
gorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In ACL?02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. In Proceedings of ICML?03.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic
labeling of semantic roles. Computational Linguistic,
28(3):496?530.
R. Jackendoff. 1990. Semantic Structures, Current Studies in
Linguistics series. Cambridge, Massachusetts: The MIT
Press.
T. Joachims. 1999. Making large-scale SVM learning prac-
tical. In B. Scho?lkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods - Support Vector Learning.
Christopher R. Johnson and Charles J. Fillmore. 2000. The
framenet tagset for frame-semantic and syntactic coding
of predicate-argument structure. In In the Proceedings
ANLP-NAACL.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC?02.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn
Treebank. Computational Linguistics, 19:313?330.
Alessandro Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of ACL?04,
Barcelona, Spain.
Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,
and Roberto Basili. 2005. Engineering of syntactic fea-
tures for shallow semantic parsing. In of the ACL05 Work-
shop on Feature Engineering for Machine Learning in
Natural Language Processing, USA.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings of EACL?06,
Trento, Italy.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005. Sup-
port vector learning for semantic argument classification.
Machine Learning Journal.
V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity of
syntactic parsing for semantic role labeling. In Proceed-
ings of IJCAI?05.
Kristina Toutanova, Aria Haghighi, and Christopher Man-
ning. 2005. Joint learning improves semantic role label-
ing. In Proceedings of ACL?05.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP
2004.
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel methods for relation extraction. Journal of Machine
Learning Research.
56
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 61?68, New York City, June 2006. c?2006 Association for Computational Linguistics
Semantic Role Labeling via Tree Kernel Joint Inference
Alessandro Moschitti, Daniele Pighin and Roberto Basili
Department of Computer Science
University of Rome ?Tor Vergata?
00133 Rome, Italy
{moschitti,basili}@info.uniroma2.it
daniele.pighin@gmail.com
Abstract
Recent work on Semantic Role Labeling
(SRL) has shown that to achieve high
accuracy a joint inference on the whole
predicate argument structure should be ap-
plied. In this paper, we used syntactic sub-
trees that span potential argument struc-
tures of the target predicate in tree ker-
nel functions. This allows Support Vec-
tor Machines to discern between correct
and incorrect predicate structures and to
re-rank them based on the joint probabil-
ity of their arguments. Experiments on the
PropBank data show that both classifica-
tion and re-ranking based on tree kernels
can improve SRL systems.
1 Introduction
Recent work on Semantic Role Labeling (SRL)
(Carreras and Ma`rquez, 2005) has shown that to
achieve high labeling accuracy a joint inference on
the whole predicate argument structure should be
applied. For this purpose, we need to extract fea-
tures from the sentence?s syntactic parse tree that
encodes the target semantic structure. This task is
rather complex since we do not exactly know which
are the syntactic clues that capture the relation be-
tween the predicate and its arguments. For exam-
ple, to detect the interesting context, the modeling
of syntax/semantics-based features should take into
account linguistic aspects like ancestor nodes or se-
mantic dependencies (Toutanova et al, 2004).
A viable approach to generate a large number of
features has been proposed in (Collins and Duffy,
2002), where convolution kernels were used to im-
plicitly define a tree substructure space. The selec-
tion of the relevant structural features was left to the
Voted Perceptron learning algorithm. Such success-
ful experimentation shows that tree kernels are very
promising for automatic feature engineering, espe-
cially when the available knowledge about the phe-
nomenon is limited.
In a similar way, we can model SRL systems with
tree kernels to generate large feature spaces. More
in detail, most SRL systems split the labeling pro-
cess into two different steps: Boundary Detection
(i.e. to determine the text boundaries of predicate
arguments) and Role Classification (i.e. labeling
such arguments with a semantic role, e.g. Arg0 or
Arg1 as defined in (Kingsbury and Palmer, 2002)).
The former relates to the detection of syntactic parse
tree nodes associated with constituents that corre-
spond to arguments, whereas the latter considers the
boundary nodes for the assignment of the suitable
label. Both steps require the design and extraction
of features from parse trees. As capturing the tightly
interdependent relations among a predicate and its
arguments is a complex task, we can apply tree ker-
nels on the subtrees that span the whole predicate
argument structure to generate the feature space of
all the possible subtrees.
In this paper, we apply the traditional bound-
ary (TBC) and role (TRC) classifiers (Pradhan
et al, 2005a), which are based on binary predi-
cate/argument relations, to label all parse tree nodes
corresponding to potential arguments. Then, we ex-
61
tract the subtrees which span the predicate-argument
dependencies of such arguments, i.e. Argument
Spanning Trees (AST s). These are used in a tree
kernel function to generate all possible substructures
that encode n-ary argument relations, i.e. we carry
out an automatic feature engineering process.
To validate our approach, we experimented with
our model and Support Vector Machines for the clas-
sification of valid and invalid AST s. The results
show that this classification problem can be learned
with high accuracy. Moreover, we modeled SRL as a
re-ranking task in line with (Toutanova et al, 2005).
The large number of complex features provided by
tree kernels for structured learning allows SVMs to
reach the state-of-the-art accuracy.
The paper is organized as follows: Section 2 intro-
duces the Semantic Role Labeling based on SVMs
and the tree kernel spaces; Section 3 formally de-
fines the AST s and the algorithm for their classifi-
cation and re-ranking; Section 4 shows the compara-
tive results between our approach and the traditional
one; Section 5 presents the related work; and finally,
Section 6 summarizes the conclusions.
2 Semantic Role Labeling
In the last years, several machine learning ap-
proaches have been developed for automatic role
labeling, e.g. (Gildea and Jurasfky, 2002; Prad-
han et al, 2005a). Their common characteristic is
the adoption of attribute-value representations for
predicate-argument structures. Accordingly, our ba-
sic system is similar to the one proposed in (Pradhan
et al, 2005a) and it is hereby described.
We use a boundary detection classifier (for any
role type) to derive the words compounding an ar-
gument and a multiclassifier to assign the roles (e.g.
Arg0 or ArgM) described in PropBank (Kingsbury
and Palmer, 2002)). To prepare the training data for
both classifiers, we used the following algorithm:
1. Given a sentence from the training-set, generate
a full syntactic parse tree;
2. Let P and A be respectively the set of predicates
and the set of parse-tree nodes (i.e. the potential ar-
guments);
3. For each pair ?p, a? ? P ?A:
- extract the feature representation set, Fp,a;
- if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in the T+
set (positive examples), otherwise put it in the
T? set (negative examples).
The outputs of the above algorithm are the T+ and
T? sets. These sets can be directly used to train a
boundary classifier (e.g. an SVM). Regarding the
argument type classifier, a binary labeler for a role r
(e.g. an SVM) can be trained on the T+r , i.e. its pos-
itive examples and T?r , i.e. its negative examples,
where T+ = T+r ? T?r , according to the ONE-vs-
ALL scheme. The binary classifiers are then used
to build a general role multiclassifier by simply se-
lecting the argument associated with the maximum
among the SVM scores.
Regarding the design of features for predicate-
argument pairs, we can use the attribute-values de-
fined in (Gildea and Jurasfky, 2002) or tree struc-
tures (Moschitti, 2004). Although we focus on
the latter approach, a short description of the for-
mer is still relevant as they are used by TBC and
TRC. They include the Phrase Type, Predicate
Word, Head Word, Governing Category, Position
and Voice features. For example, the Phrase Type
indicates the syntactic type of the phrase labeled as
a predicate argument and the Parse Tree Path con-
tains the path in the parse tree between the predicate
and the argument phrase, expressed as a sequence of
nonterminal labels linked by direction (up or down)
symbols, e.g. V ? VP ? NP.
A viable alternative to manual design of syntac-
tic features is the use of tree-kernel functions. These
implicitly define a feature space based on all possi-
ble tree substructures. Given two trees T1 and T2, in-
stead of representing them with the whole fragment
space, we can apply the kernel function to evaluate
the number of common fragments.
Formally, given a tree fragment space F =
{f1, f2, . . . , f|F|}, the indicator function Ii(n)
is equal to 1 if the target fi is rooted at
node n and equal to 0 otherwise. A tree-
kernel function over t1 and t2 is Kt(t1, t2) =?
n1?Nt1
?
n2?Nt2 ?(n1, n2), where Nt1 and Nt2
are the sets of the t1?s and t2?s nodes, respectively. In
turn ?(n1, n2) =
?|F|
i=1 ?l(fi)Ii(n1)Ii(n2), where
0 ? ? ? 1 and l(fi) is the height of the subtree
fi. Thus ?l(fi) assigns a lower weight to larger frag-
62
S
NP VP
PRP
John
VP CC VP
VB NP
and
VB NP
took
DT NN
the book read
PRP$ NN
its title
Sentence Parse-Tree
S
NP VP
PRP
John
VP
VB NP
took
DT NN
the book
took{ARG0, ARG1}
S
NP VP
PRP
John
VP
VB NP
read
PRP$ NN
its title
read{ARG0, ARG1}
Figure 1: A sentence parse tree with two argument spanning trees (AST s)
ments. When ? = 1, ? is equal to the number of
common fragments rooted at nodes n1 and n2. As
described in (Collins and Duffy, 2002), ? can be
computed in O(|Nt1 | ? |Nt2 |).
3 Tree kernel-based classification of
Predicate Argument Structures
Traditional semantic role labeling systems extract
features from pairs of nodes corresponding to a
predicate and one of its argument, respectively.
Thus, they focus on only binary relations to make
classification decisions. This information is poorer
than the one expressed by the whole predicate ar-
gument structure. As an alternative we can select
the set of potential arguments (potential argument
nodes) of a predicate and extract features from them.
The number of the candidate argument sets is ex-
ponential, thus we should consider only those cor-
responding to the most probable correct argument
structures.
The usual approach (Toutanova et al, 2005) uses
a traditional boundary classifier (TBC) to select the
set of potential argument nodes. Such set can be as-
sociated with a subtree which in turn can be classi-
fied by means of a tree kernel function. This func-
tion intuitively measures to what extent a given can-
didate subtree is compatible with the subtree of a
correct predicate argument structure. We can use it
to define two different learning problems: (a) the
simple classification of correct and incorrect pred-
icate argument structures and (b) given the best m
structures, we can train a re-ranker algorithm able to
exploit argument inter-dependencies.
3.1 The Argument Spanning Trees (AST s)
We consider predicate argument structures anno-
tated in PropBank along with the corresponding
TreeBank data as our object space. Given the target
predicate node p and a node subset s = {n1, .., nk}
of the parse tree t, we define as the spanning tree
root r the lowest common ancestor of n1, .., nk and
p. The node set spanning tree (NST ) ps is the sub-
tree of t rooted in r from which the nodes that are
neither ancestors nor descendants of any ni or p are
removed.
Since predicate arguments are associated with
tree nodes (i.e. they exactly fit into syntactic
constituents), we can define the Argument Span-
ning Tree (AST ) of a predicate argument set,
{p, {a1, .., an}}, as the NST over such nodes,
i.e. p{a1,..,an}. An AST corresponds to the min-
imal subtree whose leaves are all and only the
words compounding the arguments and the predi-
cate. For example, Figure 1 shows the parse tree
of the sentence "John took the book and read
its title". took{Arg0,Arg1} and read{Arg0,Arg1}
are two AST structures associated with the two
predicates took and read, respectively. All the other
possible subtrees, i.e. NST s, are not valid AST s
for these two predicates. Note that classifying ps in
AST or NST for each node subset s of t is equiva-
lent to solve the boundary detection problem.
The critical points for the AST classification are:
(1) how to design suitable features for the charac-
terization of valid structures. This requires a careful
linguistic investigation about their significant prop-
erties. (2) How to deal with the exponential number
of NST s.
The first problem can be addressed by means of
tree kernels over the AST s. Tree kernel spaces are
an alternative to the manual feature design as the
learning machine, (e.g. SVMs) can select the most
relevant features from a high dimensional space. In
other words, we can use a tree kernel function to
estimate the similarity between two AST s (see Sec-
63
Figure 2: Two-step boundary classification. a) Sentence tree; b) Two candidate ASTs; c) Extended AST -
Ord labeling
tion 2), hence avoiding to define explicit features.
The second problem can be approached in two
ways:
(1) We can increase the recall of TBC to enlarge the
set of candidate arguments. From such set, we can
extract correct and incorrect argument structures. As
the number of such structures will be rather small,
we can apply the AST classifier to detect the cor-
rect ones.
(2) We can consider the classification probability
provided by TBC and TRC (Pradhan et al, 2005a)
and select the m most probable structures. Then, we
can apply a re-ranking approach based on SVMs and
tree kernels.
The re-ranking approach is the most promising
one as suggested in (Toutanova et al, 2005) but it
does not clearly reveal if tree kernels can be used
to learn the difference between correct or incorrect
argument structures. Thus it is interesting to study
both the above approaches.
3.2 NST Classification
As we cannot classify all possible candidate argu-
ment structures, we apply the AST classifier just to
detect the correct structures from a set of overlap-
ping arguments. Given two nodes n1 and n2 of an
NST , they overlap if either n1 is ancestor of n2 or
vice versa. NST s that contain overlapping nodes
are not valid AST s but subtrees of NSTs may be
valid ASTs. Assuming this, we define s as the set
of potential argument nodes and we create two node
sets s1 = s ? {n1} and s2 = s ? {n2}. By classi-
fying the two new NST s ps1 and ps2 with the AST
classifier, we can select the correct structures. Of
course, this procedure can be generalized to a set of
overlapping nodes greater than 2. However, consid-
ering that the Precision of TBC is generally high,
the number of overlapping nodes is usually small.
Figure 2 shows a working example of the multi-
stage classifier. In Frame (a), TBC labels as po-
tential arguments (circled nodes) three overlapping
nodes related to Arg1. This leads to two possible
non-overlapping solutions (Frame (b)) but only the
first one is correct. In fact, according to the second
one the propositional phrase ?of the book? would be
incorrectly attached to the verbal predicate, i.e. in
contrast with the parse tree. The AST classifier, ap-
plied to the two NST s, is expected to detect this
inconsistency and provide the correct output.
3.3 Re-ranking NST s with Tree Kernels
To implement the re-ranking model, we follow the
approach described in (Toutanova et al, 2005).
First, we use SVMs to implement the boundary
TBC and role TRC local classifiers. As SVMs do
not provide probabilistic output, we use the Platt?s
algorithm (Platt, 2000) and its revised version (Lin
et al, 2003) to trasform scores into probabilities.
Second, we combine TBC and TRC probabil-
ities to obtain the m most likely sequences s of
tree nodes annotated with semantic roles. As argu-
ment constituents of the same verb cannot overlap,
we generate sequences that respect such node con-
straint. We adopt the same algorithm described in
(Toutanova et al, 2005). We start from the leaves
and we select the m sequences that respect the con-
straints and at the same time have the highest joint
probability of TBC and TRC.
Third, we extract the following feature represen-
tation:
(a) The AST s associated with the predicate argu-
ment structures. To make faster the learning process
and to try to only capture the most relevant features,
we also experimented with a compact version of the
64
AST which is pruned at the level of argument nodes.
(b) Attribute value features (standard features) re-
lated to the whole predicate structure. These include
the features for each arguments (Gildea and Juras-
fky, 2002) and global features like the sequence of
argument labels, e.g. ?Arg0, Arg1, ArgM?.
Finally, we prepare the training examples for the
re-ranker considering the m best annotations of each
predicate structure. We use the approach adopted
in (Shen et al, 2003), which generates all possible
pairs from the m examples, i.e. (m2
)
pairs. Each pair
is assigned to a positive example if the first mem-
ber of the pair has a higher score than the second
member. The score that we use is the F1 measure
of the annotated structure with respect to the gold
standard. More in detail, given training/testing ex-
amples ei = ?t1i , t2i , v1i , v2i ?, where t1i and t2i are two
AST s and v1i and v2i are two feature vectors associ-
ated with two candidate predicate structures s1 and
s2, we define the following kernels:
1) Ktr(e1, e2) = Kt(t11, t12) +Kt(t21, t22)
?Kt(t11, t22)?Kt(t21, t12),
where tji is the j-th AST of the pair ei, Kt is the
tree kernel function defined in Section 2 and i, j ?
{1, 2}.
2) Kpr(e1, e2) = Kp(v11, v12) +Kp(v21, v22)
?Kp(v11, v22)?Kp(v21, v12),
where vji is the j-th feature vector of the pair ei and
Kp is the polynomial kernel applied to such vectors.
The final kernel that we use for re-ranking is the
following:
K(e1, e2) = Ktr(e1, e2)|Ktr(e1, e2)| +
Kpr(e1, e2)
|Kpr(e1, e2)|
Regarding tree kernel feature engineering, the
next section show how we can generate more effec-
tive features given an established kernel function.
3.4 Tree kernel feature engineering
Consider the Frame (b) of Figure 2, it shows two
perfectly identical NST s, consequently, their frag-
ments will also be equal. This prevents the algorithm
to learn something from such examples. To solve the
problem, we can enrich the NSTs by marking their
argument nodes with a progressive number, starting
from the leftmost argument. For example, in the first
NST of Frame (c), we mark as NP-0 and NP-1 the
first and second argument nodes whereas in the sec-
ond NST we trasform the three argument node la-
bels in NP-0, NP-1 and PP-2. We will refer to the
resulting structure as a AST -Ord (ordinal number).
This simple modification allows the tree kernel to
generate different argument structures for the above
NST s. For example, from the first NST in Fig-
ure 2.c, the fragments [NP-1 [NP][PP]], [NP
[DT][NN]] and [PP [IN][NP]] are gener-
ated. They do not match anymore with the [NP-0
[NP][PP]], [NP-1 [DT][NN]] and [PP-2
[IN][NP]] fragments generated from the second
NST in Figure 2.c.
Additionally, it should be noted that the semantic
information provided by the role type can remark-
ably help the detection of correct or incorrect predi-
cate argument structures. Thus, we can enrich the ar-
gument node label with the role type, e.g. the NP-0
and NP-1 of the correct AST of Figure 2.c become
NP-Arg0 and NP-Arg1 (not shown in the figure).
We refer to this structure as AST -Arg. Of course,
to apply the AST -Arg classifier, we need that TRC
labels the arguments detected by TBC.
4 The experiments
The experiments were carried out within the set-
ting defined in the CoNLL-2005 Shared Task
(Carreras and Ma`rquez, 2005). In particular,
we adopted the Charniak parse trees available at
www.lsi.upc.edu/?srlconll/ along with the of-
ficial performance evaluator.
All the experiments were performed with
the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes ST and SST kernels in SVM-light
(Joachims, 1999). For TBC and TRC, we used the
linear kernel with a regularization parameter (option
-c) equal to 1. A cost factor (option -j) of 10 was
adopted for TBC to have a higher Recall, whereas
for TRC, the cost factor was parameterized accord-
ing to the maximal accuracy of each argument class
on the validation set. For the AST -based classifiers
we used a ? equal to 0.4 (see (Moschitti, 2004)).
65
Section 21 Section 23
AST Class. P. R. F1 P. R. F1
? 69.8 77.9 73.7 62.2 77.1 68.9
Ord 73.7 81.2 77.3 63.7 80.6 71.2
Arg 73.6 84.7 78.7 64.2 82.3 72.1
Table 1: AST , AST -Ord, and AST -Arg perfor-
mance on sections 21 and 23.
4.1 Classification of whole predicate argument
structures
In these experiments, we trained TBC on sections
02-08 whereas, to achieve a very accurate role clas-
sifier, we trained TRC on all sections 02-21. To
train the AST , AST -Ord (AST with ordinal num-
bers in the argument nodes), and AST -Arg (AST
with argument type in the argument nodes) clas-
sifiers, we applied the TBC and TRC over sec-
tions 09-20. Then, we considered all the structures
whose automatic annotation showed at least an ar-
gument overlap. From these, we extracted 30,220
valid AST s and 28,143 non-valid AST s, for a total
of 183,642 arguments.
First, we evaluate the accuracy of the AST -based
classifiers by extracting 1,975 AST s and 2,220 non-
AST s from Section 21 and the 2,159 AST s and
3,461 non-AST s from Section 23. The accuracy
derived on Section 21 is an upperbound for our clas-
sifiers since it is obtained using an ideal syntactic
parser (the Charniak?s parser was trained also on
Section 21) and an ideal role classifier.
Table 1 shows Precision, Recall and F1 mea-
sures of the AST -based classifiers over the above
NSTs. Rows 2, 3 and 4 report the performance of
AST , AST -Ord, and AST -Arg classifiers, respec-
tively. We note that: (a) The impact of parsing ac-
curacy is shown by the gap of about 6% points be-
tween sections 21 and 23. (b) The ordinal number-
ing of arguments (Ord) and the role type informa-
tion (Arg) provide tree kernels with more meaning-
ful fragments since they improve the basic model
of about 4%. (c) The deeper semantic information
generated by the Arg labels provides useful clues to
select correct predicate argument structures since it
improves the Ord model on both sections.
Second, we measured the impact of the AST -
based classifiers on the accuracy of both phases of
semantic role labeling. Table 2 reports the results
on sections 21 and 23. For each of them, Precision,
Recall and F1 of different approaches to bound-
ary identification (bnd) and to the complete task,
i.e. boundary and role classification (bnd+class)
are shown. Such approaches are based on differ-
ent strategies to remove the overlaps, i.e. with the
AST , AST -Ord and AST -Arg classifiers and using
the baseline (RND), i.e. a random selection of non-
overlapping structures. The baseline corresponds to
the system based on TBC and TRC1.
We note that: (a) for any model, the boundary de-
tection F1 on Section 21 is about 10 points higher
than the F1 on Section 23 (e.g. 87.0% vs. 77.9%
for RND). As expected the parse tree quality is very
important to detect argument boundaries. (b) On the
real test (Section 23) the classification introduces la-
beling errors which decrease the accuracy of about
5% (77.9 vs 72.9 for RND). (c) The Ord and Arg
approaches constantly improve the baseline F1 of
about 1%. Such poor impact does not surprise as
the overlapping structures are a small percentage of
the test set, thus the overall improvement cannot be
very high.
Third, the comparison with the CoNLL 2005 re-
sults (Carreras and Ma`rquez, 2005) can only be
carried out with respect to the whole SRL task
(bnd+class in table 2) since boundary detection ver-
sus role classification is generally not provided in
CoNLL 2005. Moreover, our best global result, i.e.
73.9%, was obtained under two severe experimental
factors: a) the use of just 1/3 of the available train-
ing set, and b) the usage of the linear SVM model
for the TBC classifier, which is much faster than the
polynomial SVMs but also less accurate. However,
we note the promising results of the AST meta-
classifier, which can be used with any of the best
figure CoNLL systems.
Finally, the overall results suggest that the tree
kernel model is robust to parse tree errors since pre-
serves the same improvement across trees derived
with different accuracy, i.e. the semi-automatic trees
of Section 21 and the automatic tree of Section 23.
Moreover, it shows a high accuracy for the classi-
fication of correct and incorrect AST s. This last
property is quite interesting as the best SRL systems
1We needed to remove the overlaps from the baseline out-
come in order to apply the CoNLL evaluator.
66
(Punyakanok et al, 2005; Toutanova et al, 2005;
Pradhan et al, 2005b) were obtained by exploit-
ing the information on the whole predicate argument
structure.
Next section shows our preliminary experiments
on re-ranking using the AST kernel based approach.
4.2 Re-ranking based on Tree Kernels
In these experiments, we used the output of TBC
and TRC2 to provide an SVM tree kernel with a
ranked list of predicate argument structures. More in
detail, we applied a Viterbi-like algorithm to gener-
ate the 20 most likely annotations for each predicate
structure, according to the joint probabilistic model
of TBC and TRC. We sorted such structures based
on their F1 measure and used them to learn the SVM
re-ranker described in 3.3.
For training, we used Sections 12, 14, 15, 16
and 24, which contain 24,729 predicate structures.
For each of them, we considered the 5 annotations
having the highest F1 score (i.e. 123,674 NST s)
on the span of the 20 best annotations provided by
Viterbi algorithm. With such structures, we ob-
tained 294,296 pairs used to train the SVM-based
re-ranker. As the number of such structures is very
large the SVM training time was very high. Thus,
we sped up the learning process by using only the
AST s associated with the core arguments. From the
test sentences (which contain 5,267 structures), we
extracted the 20 best Viterbi annotated structures,
i.e. 102,343 (for a total of 315.531 pairs), which
were used for the following experiments:
First, we selected the best annotation (according
to the F1 provided by the gold standard annotations)
out of the 20 provided by the Viterbi?s algorithm.
The resulting F1 of 88.59% is the upperbound of our
approach.
Second, we selected the top ranked annotation in-
dicated by the Viterbi?s algorithm. This provides our
baseline F1 measure, i.e. 75.91%. Such outcome is
slightly higher than our official CoNLL result (Mos-
chitti et al, 2005) obtained without converting SVM
scores into probabilities.
Third, we applied the SVM re-ranker to select
2With the aim of improving the state-of-the-art, we applied
the polynomial kernel for all basic classifiers, at this time.
We used the models developed during our participation to the
CoNLL 2005 shared task (Moschitti et al, 2005).
the best structures according to the core roles. We
achieved 80.68% which is practically equal to the
result obtained in (Punyakanok et al, 2005; Car-
reras and Ma`rquez, 2005) for core roles, i.e. 81%.
Their overall F1 which includes all the arguments
was 79.44%. This confirms that the classification of
the non-core roles is more complex than the other
arguments.
Finally, the high computation time of the re-
ranker prevented us to use the larger structures
which include all arguments. The major complexity
issue was the slow training and classification time
of SVMs. The time needed for tree kernel function
was not so problematic as we could use the fast eval-
uation proposed in (Moschitti, 2006). This roughly
reduces the computation time to the one required by
a polynomial kernel. The real burden is therefore the
learning time of SVMs that is quadratic in the num-
ber of training instances. For example, to carry out
the re-ranking experiments required approximately
one month of a 64 bits machine (2.4 GHz and 4Gb
Ram). To solve this problem, we are going to study
the impact on the accuracy of fast learning algo-
rithms such as the Voted Perceptron.
5 Related Work
Recently, many kernels for natural language applica-
tions have been designed. In what follows, we high-
light their difference and properties.
The tree kernel used in this article was proposed
in (Collins and Duffy, 2002) for syntactic parsing
re-ranking. It was experimented with the Voted
Perceptron and was shown to improve the syntac-
tic parsing. In (Cumby and Roth, 2003), a feature
description language was used to extract structural
features from the syntactic shallow parse trees asso-
ciated with named entities. The experiments on the
named entity categorization showed that when the
description language selects an adequate set of tree
fragments the Voted Perceptron algorithm increases
its classification accuracy. The explanation was that
the complete tree fragment set contains many irrel-
evant features and may cause overfitting. In (Pun-
yakanok et al, 2005), a set of different syntactic
parse trees, e.g. the n best trees generated by the
Charniak?s parser, were used to improve the SRL
accuracy. These different sources of syntactic infor-
mation were used to generate a set of different SRL
67
Section 21 Section 23
bnd bnd+class bnd bnd+class
AST Classifier RND AST Classifier RND AST Classifier RND AST Classifier RND
- Ord Arg - Ord Arg - Ord Arg - Ord Arg
P. 87.5 88.3 88.3 86.9 85.5 86.3 86.4 85.0 78.6 79.0 79.3 77.8 73.1 73.5 73.4 72.3
R. 87.3 88.1 88.3 87.1 85.7 86.5 86.8 85.6 78.1 78.4 78.7 77.9 73.8 74.1 74.4 73.6
F1 87.4 88.2 88.3 87.0 85.6 86.4 86.6 85.3 78.3 78.7 79.0 77.9 73.4 73.8 73.9 72.9
Table 2: Semantic Role Labeling performance on automatic trees using AST -based classifiers.
outputs. A joint inference stage was applied to re-
solve the inconsistency of the different outputs. In
(Toutanova et al, 2005), it was observed that there
are strong dependencies among the labels of the se-
mantic argument nodes of a verb. Thus, to approach
the problem, a re-ranking method of role sequences
labeled by a TRC is applied. In (Pradhan et al,
2005b), some experiments were conducted on SRL
systems trained using different syntactic views.
6 Conclusions
Recent work on Semantic Role Labeling has shown
that to achieve high labeling accuracy a joint in-
ference on the whole predicate argument structure
should be applied. As feature design for such task is
complex, we can take advantage from kernel meth-
ods to model our intuitive knowledge about the n-
ary predicate argument relations.
In this paper we have shown that we can exploit
the properties of tree kernels to engineer syntactic
features for the semantic role labeling task. The ex-
periments suggest that (1) the information related
to the whole predicate argument structure is impor-
tant as it can improve the state-of-the-art and (2)
tree kernels can be used in a joint model to gen-
erate relevant syntactic/semantic features. The real
drawback is the computational complexity of work-
ing with SVMs, thus the design of fast algorithm is
an interesting future work.
Acknowledgments
This research is partially supported by the
PrestoSpace EU Project#: FP6-507336.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL05.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL02.
Chad Cumby and Dan Roth. 2003. Kernel methods for re-
lational learning. In Proceedings of ICML03, Washington,
DC, USA.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic label-
ing of semantic roles. Computational Linguistic, 28(3):496?
530.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC?02), Las Palmas, Spain.
H.T. Lin, C.J. Lin, and R.C. Weng. 2003. A note on platt?s
probabilistic outputs for support vector machines. Technical
report, National Taiwan University.
Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,
and Roberto Basili. 2005. Hierarchical semantic role label-
ing. In Proceedings of CoNLL05 shared task, Ann Arbor
(MI), USA.
Alessandro Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of ACL?04,
Barcelona, Spain.
Alessandro Moschitti. 2006. Making tree kernels practical
for natural language learning. In Proceedings of EACL?06,
Trento, Italy.
J. Platt. 2000. Probabilistic outputs for support vector ma-
chines and comparison to regularized likelihood methods.
MIT Press.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005a. Support vec-
tor learning for semantic argument classification. Machine
Learning Journal.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin,
and Daniel Jurafsky. 2005b. Semantic role labeling using
different syntactic views. In Proceedings ACL?05.
V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity of
syntactic parsing for semantic role labeling. In Proceedings
of IJCAI 2005.
Libin Shen, Anoop Sarkar, and Aravind Joshi. 2003. Using
ltag based features in parse reranking. In Conference on
EMNLP03, Sapporo, Japan.
Kristina Toutanova, Penka Markova, and Christopher D. Man-
ning. 2004. The leaf projection path view of parse trees:
Exploring string kernels for hpsg parse selection. In In Pro-
ceedings of EMNLP04.
Kristina Toutanova, Aria Haghighi, and Christopher Manning.
2005. Joint learning improves semantic role labeling. In
Proceedings of ACL05.
68
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 111?120,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Reverse Engineering of Tree Kernel Feature Spaces
Daniele Pighin
FBK-Irst, HLT
Via di Sommarive, 18 I-38100 Povo (TN) Italy
pighin@fbk.eu
Alessandro Moschitti
University of Trento, DISI
Via di Sommarive, 14 I-38100 Povo (TN) Italy
moschitti@disi.unitn.it
Abstract
We present a framework to extract the
most important features (tree fragments)
from a Tree Kernel (TK) space according
to their importance in the target kernel-
based machine, e.g. Support Vector Ma-
chines (SVMs). In particular, our min-
ing algorithm selects the most relevant fea-
tures based on SVM estimated weights
and uses this information to automatically
infer an explicit representation of the in-
put data. The explicit features (a) improve
our knowledge on the target problem do-
main and (b) make large-scale learning
practical, improving training and test time,
while yielding accuracy in line with tradi-
tional TK classifiers. Experiments on se-
mantic role labeling and question classifi-
cation illustrate the above claims.
1 Introduction
The last decade has seen a massive use of Support
Vector Machines (SVMs) for carrying out NLP
tasks. Indeed, their appealing properties such as
1) solid theoretical foundations, 2) robustness to
irrelevant features and 3) outperforming accuracy
have been exploited to design state-of-the-art lan-
guage applications.
More recently, kernel functions, which im-
plicitly represent data in some high dimensional
space, have been employed to study and fur-
ther improve many natural language systems, e.g.
(Collins and Duffy, 2002), (Kudo and Matsumoto,
2003), (Cumby and Roth, 2003), (Cancedda et al,
2003), (Culotta and Sorensen, 2004), (Toutanova
et al, 2004), (Kazama and Torisawa, 2005), (Shen
et al, 2003), (Gliozzo et al, 2005), (Kudo et al,
2005), (Moschitti et al, 2008), (Diab et al, 2008).
Unfortunately, the benefit to easily and effectively
model the target linguistic phenomena is reduced
by the the implicit nature of the kernel space,
which prevents to directly observe the most rele-
vant features. As a consequence, even very accu-
rate models generally fail in providing useful feed-
back for improving our understanding of the prob-
lems at study. Moreover, the computational bur-
den induced by high dimensional kernels makes
the application of SVMs to large corpora still more
problematic.
In (Pighin and Moschitti, 2009), we proposed a
feature extraction algorithm for Tree Kernel (TK)
spaces, which selects the most relevant features
(tree fragments) according to the gradient compo-
nents (weight vector) of the hyperplane learnt by
an SVM, in line with current research, e.g. (Rako-
tomamonjy, 2003; Weston et al, 2003; Kudo and
Matsumoto, 2003). In particular, we provided al-
gorithmic solutions to deal with the huge dimen-
sionality and, consequently, high computational
complexity of the fragment space. Our experimen-
tal results showed that our approach reduces learn-
ing and classification processing time leaving the
accuracy unchanged.
In this paper, we present a new version of such
algorithm which, under the same parameteriza-
tion, is almost three times as fast while produc-
ing the same results. Most importantly, we ex-
plored tree fragment spaces for two interesting
natural language tasks: Semantic Role Labeling
(SRL) and Question Classification (QC). The re-
sults show that: (a) on large data sets, our ap-
proach can improve training and test time while
yielding almost unaffected classification accuracy,
and (b) our framework can effectively exploit the
ability of TKs and SVMs to, respectively, gener-
ate and recognize relevant structured features. In
particular, we (i) study in more detail the relevant
fragments identfied for the boundary classification
task of SRL, (ii) closely observe the most relevant
fragments for each QC class and (iii) look at the di-
verse syntactic patterns characterizing each ques-
111
tion category.
The rest of the paper is structured as follows:
Section 2 will briefly review SVMs and TK func-
tions; Section 3 will detail our proposal for the lin-
earization of a TK feature space; Section 4 will
review previous work on related subjects; Section
5 will detail the outcome of our experiments, and
Section 6 will discuss some relevant aspects of the
evaluation; finally, in Section 7 we will draw our
conclusions.
2 Tree Kernel Functions
The decision function of an SVM is:
f(~x) = ~w ? ~x + b =
n
?
i=1
?
i
y
i
~x
i
? ~x + b (1)
where ~x is a classifying example and ~w and b are
the separating hyperplane?s gradient and its bias,
respectively. The gradient is a linear combination
of the training points ~x
i
, their labels y
i
and their
weights ?
i
. Applying the so-called kernel trick it
is possible to replace the scalar product with a ker-
nel function defined over pairs of objects:
f(o) =
n
?
i=1
?
i
y
i
k(o
i
, o) + b
with the advantage that we do not need to provide
an explicit mapping ?(?) of our examples in a vec-
tor space.
A Tree Kernel function is a convolution ker-
nel (Haussler, 1999) defined over pairs of trees.
Practically speaking, the kernel between two trees
evaluates the number of substructures (or frag-
ments) they have in common, i.e. it is a measure
of their overlap. The function can be computed re-
cursively in closed form, and quite efficient imple-
mentations are available (Moschitti, 2006). Dif-
ferent TK functions are characterized by alterna-
tive fragment definitions, e.g. (Collins and Duffy,
2002) and (Kashima and Koyanagi, 2002). In the
context of this paper we will be focusing on the
SubSet Tree (SST) kernel described in (Collins
and Duffy, 2002), which relies on a fragment defi-
nition that does not allow to break production rules
(i.e. if any child of a node is included in a frag-
ment, then also all the other children have to). As
such, it is especially indicated for tasks involving
constituency parsed texts.
Implicitly, a TK function establishes a corre-
spondence between distinct fragments and dimen-
sions in some fragment space, i.e. the space of all
Fragment space
A
B A
A
B A
B A
A
B A
C
A
B A
B A
C
A
C
D
B A
D
B A
C
1 2 3 4 5 6 7
T1
A
B A
B A
C
T2
D
B A
C
?(T1) = [2, 1, 1, 1, 1, 0, 0]
?(T2) = [0, 0, 0, 0, 1, 1, 1]
K(T1, T2) = ??(T1), ?(T2)? = 1
Figure 1: Esemplification of a fragment space and
the kernel product between two trees.
the possible fragments. To simplify, a tree t can
be represented as a vector whose attributes count
the occurrences of each fragment within the tree.
The kernel between two trees is then equivalent to
the scalar product between pairs of such vectors,
as exemplified in Figure 1.
3 Linearization of a TK function
Our objective is to efficiently mine the most rele-
vant fragments from the huge fragment space, so
that we can explicitly represent our input trees in
terms of these fragments and learn fast and accu-
rate linear classifiers.
The framework defines five distinct activities,
detailed in the following paragraphs.
3.1 Kernel Space Learning (KSL)
The first step involves the generation of an approx-
imation of the whole fragment space, i.e. we can
consider only the trees that encode the most rele-
vant fragments. To this end, we can partition our
training data into S smaller sets, and use the SVM
and the SST kernel to learn S models. We will
only consider the fragments encoded by the sup-
port vectors of the S models. In the next stage, we
will use the SVM estimated weights to drive our
feature selection process.
Since time complexity of SVM training is ap-
proximately quadratic in the number of examples,
by breaking training data into smaller sets we
can considerably accelerate the process of filtering
trees and estimating support vector weights. Ac-
cording to statistical learning theory, being trained
on smaller subsets of the available data these mod-
els will be less robust with respect to the min-
imization of the empirical risk (Vapnik, 1998).
112
Algorithm 3.1: MINE MODEL(M,L, ?)
global maxexp
prev? ? ; CLEAR INDEX()
for each ??y, t? ?M
do
?
?
?
?
?
?
?
T
i
? ? ? y/?t?
for each n ? N
t
do
{
f ? FRAG(n) ; rel = ? ? T
i
prev? prev ? {f, rel}
PUT(f, rel)
best pr? BEST(L) ;
while true
do
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
next? ?
for each ?f, rel? ? prev if f ? best pr
do
?
?
?
?
?
?
?
?
?
?
X = EXPAND(f,maxexp)
rel exp? ? ? rel
for each frag ? X
do
{
temp = {frag, rel exp}
next? next ? temp
PUT(frag, rel exp)
best? BEST(L)
if not CHANGED()
then break
best pr ? best
prev? next
F
L
? best pr
return (F
L
)
Nonetheless, since we do not need to employ them
for classification (but just to direct our feature se-
lection process, as we will describe shortly), we
can accept to rely on sub-optimal weights. Fur-
thermore, research results in the field of SVM par-
allelization using cascades of SVMs (Graf et al,
2004) suggest that support vectors collected from
locally learnt models can encode many of the rel-
evant features retained by models learnt globally.
Henceforth, let M
s
be the model associated with
the s-th split, and F
s
the fragment space that can
describe all the trees in M
s
.
3.2 Fragment Mining and Indexing (FMI)
In Equation 1 it is possible to isolate the gradient
~w =
?
n
i=1
?
i
y
i
~x
i
, with ~x
i
= [x
(1)
i
, . . . , x
(N)
i
], N
being the dimensionality of the feature space. For
a tree kernel function, we can rewrite x(j)
i
as:
x
(j)
i
=
t
i,j
?
?(f
j
)
?t
i
?
=
t
i,j
?
?(f
j
)
?
?
N
k=1
(t
i,k
?
?(f
k
)
)
2
(2)
where: t
i,j
is the number of occurrences of the
fragment f
j
, associated with the j-th dimension of
the feature space, in the tree t
i
; ? is the kernel de-
cay factor; and ?(f
j
) is the depth of the fragment.
The relevance |w(j)| of the fragment f
j
can be
measured as:
|w
(j)
| =
?
?
?
?
?
n
?
i=1
?
i
y
i
x
(j)
i
?
?
?
?
?
=
?
?
?
?
n
i=1
?
i
y
i
t
i,j
?
?(f
j
)
?
?
?
?t
i
?
.
(3)
We fix a threshold L and from each model M
s
(learnt during KSL) we select the L most relevant
fragments, i.e. we build the set F
s,L
= ?
k
{f
k
} so
that:
|F
s,L
| = L and |w(k)| ? |w(i)|?f
i
? F \ F
s,L
.
To generate all the fragments encoded in a
model, we adopt the greedy strategy described in
Algorithm 3.1. Its arguments are: an SVM model
M represented as ??y, t? pairs, where t is a tree
structure; the threshold value L; and the kernel de-
cay factor ?.
The function FRAG(n) generates the smallest
fragment rooted in node n (i.e. for an SST kernel,
the fragment consisting of n and its direct chil-
dren). We call such fragment a base fragment. The
function EXPAND(f,maxexp) generates all the
fragments that can be derived from the fragment
f by expanding, i.e. including in the fragment the
direct children of some of its nodes. These frag-
ments are derived from f . The parameter maxexp
limits fragment proliferation by setting the maxi-
mum number of nodes which can be expanded in
a fragment expansion operation. For example, if
there are 10 nodes which can be expanded in frag-
ment f , then only the fragments where at most 3
of the 10 nodes are expanded will be generated by
a call to EXPAND(f, 3).
Every time we generate a fragment f , the func-
tion PUT(f, rel) saves the fragment along with its
relevance rel in an index. The index keeps track
of the cumulative relevance of a fragment, and its
implementation has been optimized for fast inser-
tions and spatial compactness.
A whole cycle of expansions is considered as
an iteration of the mining process: we take into
account all the fragments that have undergone k
expansions and produce all the fragments that re-
sult from a further expansion, i.e. all the fragments
expanded k + 1 times.
We keep iterating until we reach a stop crite-
rion, which we base on the threshold value L, i.e.
the limit on the number of fragments that we are
interested in mining from a model. During each it-
eration k+1, we only expand the best L fragments
identified during the previous iteration k. When
113
the iteration is complete we re-evaluate the set of
L best fragments in the index, and we stop only if
the worst of them, i.e. the L-th ranked fragment
at the step k + 1, and its score are the same as at
the end of the previous iteration. That is, we as-
sume that if none of the fragments mined during
the (k + 1)-th iteration managed to affect the bot-
tom of the pool of the L most relevant fragments,
then none of their expansions is likely to succeed.
In the algorithm, N
t
is the set of nodes of the tree
t; BEST(L) returns the L highest ranked fragments
in the index; CHANGED() verifies whether the bot-
tom of the L-best set has been affected by the last
iteration or not.
We call MINE MODEL(?) on each of the mod-
els M
s
that we learnt from the S initial splits. For
each model, the function returns the set of L-best
fragments in the model. The union of all the frag-
ments harvested from each model is then saved
into a dictionary D
L
which will be used by the next
stage.
3.2.1 Discussion on FMI algorithm
With respect to the algorithm presented in (Pighin
and Moschitti, 2009), the one presented here has
the following advantages:
? the process of building fragments is strictly
small-to-large: fragments that span n+1 lev-
els of the tree may be generated only after all
those spanning n levels;
? the threshold value L is a parameter of the
mining process, and it is used to prevent the
algorithm from generating more fragments
than necessary, thus making it more efficient;
? it has one less parameter (maxdepth) which
was used to force fragments to span at-most
a given number of levels. The new algorithm
does not need it since the maximum number
of iterations is implicitly set via L.
These differences result in improved efficiency for
the FMI stage. For example, on the data for the
boundary classification task (see Section 5), using
comparable parameters the old algorithm required
85 minutes to mine the most relevant fragments,
whereas the new one only takes 31, i.e. it is 2.74
times as fast.
3.3 Tree Fragment Extraction (TFX)
During this phase we actually linearize our data:
a file encoding label-tree pairs ?y
i
, t
i
? is trans-
formed to encode label-vector pairs ?y
i
, ~v
i
?. To
do so, we generate the fragment space of t
i
, us-
ing a variant of the mining algorithm described in
Algorithm 3.1, and encode in ~v
i
all and only the
fragments t
i,j
so that t
i,j
? D
L
. The algorithm
exploits labels and production rules found in the
fragments listed in the dictionary to generate only
the fragments that may be in the dictionary. For
example, if the dictionary does not contain a frag-
ment whose root is labeled N , then if a node N is
encountered during TFX neither its base fragment
nor its expansions are generated. The process is
applied to the whole training (TFX-train) and test
(TFX-test) sets. The fragment space is now ex-
plicit, as there is a mapping between the input vec-
tors and the fragments they encode.
3.4 Explicit Space Learning (ESL)
Linearized training data is used to learn a very fast
model by using all the available data and a linear
kernel.
3.5 Explicit Space Classification (ESC)
The linear model is used to classify linearized test
data and evaluate the accuracy of the resulting
classifier.
4 Previous work
A rather comprehensive overview of feature se-
lection techniques is carried out in (Guyon and
Elisseeff, 2003). Non-filter approaches for SVMs
and kernel machines are often concerned with
polynomial and Gaussian kernels, e.g. (Weston et
al., 2001) and (Neumann et al, 2005). Weston
et al (2003) use the ?
0
norm in the SVM opti-
mizer to stress the feature selection capabilities
of the learning algorithm. In (Kudo and Mat-
sumoto, 2003), an extension of the PrefixSpan al-
gorithm (Pei et al, 2001) is used to efficiently
mine the features in a low degree polynomial ker-
nel space. The authors discuss an approximation
of their method that allows them to handle high
degree polynomial kernels.
Suzuki and Isozaki (2005) present an embed-
ded approach to feature selection for convolution
kernels based on ?2-driven relevance assessment.
To our knowledge, this is the only published work
clearly focusing on feature selection for tree ker-
nel functions, and indeed has been one of the
major sources of inspiration for our methodol-
ogy. With respect to their work, the difference
114
in our approach is that we want to exploit the
SVM optimizer to select the most relevant fea-
tures instead of a relevance assessment measure
that moves from different statistical assumptions
than the learning algorithm.
In (Graf et al, 2004), an approach to SVM
parallelization is presented which is based on a
divide-et-impera strategy to reduce optimization
time. The idea of using a compact graph rep-
resentation to represent the support vectors of a
TK function is explored in (Aiolli et al, 2006),
where a Direct Acyclic Graph (DAG) is employed.
In (Moschitti, 2006; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b; Mos-
chitti et al, 2007), the SST kernel along with other
tree and combined kernels are employed for ques-
tion classification and semantic role labeling with
interesting results.
5 Experiments
We evaluated the capability of our model to ex-
tract relevant features on two data sets: the
CoNLL 2005 shared task on Semantic Role Label-
ing (SRL) (Carreras and Ma`rquez, 2005), and the
Question Classification (QC) task based on data
from the TREC 10 QA competition (Voorhees,
2001). The next sections will detail the setup and
outcome of the two sets of experiments.
All the experiments were run on a machine
equipped with 4 Intel R? Xeon R? CPUs clocked at
1.6 GHz and 4 GB of RAM. As a supervised learn-
ing framework we used SVM-Light-TK1, which
extends the SVM-Light optimizer (Joachims,
2000) with tree kernel support. For each classi-
fication task, we compare the accuracy of a vanilla
SST classifier against the corresponding linearized
SST classifier (SST
?
). For KSL and SST training
we used the default decay factor ? = 0.4. For
ESL, we use a non-normalized, linear kernel. No
further parametrization of the learning algorithms
is carried out. Indeed, our focus is on showing
that, under the same conditions, our linearized tree
kernel can be as accurate as the original kernel,
and choosing of parameters may just bias such
test.
5.1 Semantic Role Labeling
For our experiments on semantic role labeling we
used PropBank annotations (Palmer et al, 2005)
1http://disi.unitn.it/
?
moschitt/
Tree-Kernel.htm
S
NP
NNP
Mary
VP
VB
bought
NP
D
a
NN
cat
(A1)
(A0)
?
VP
VB-P
bought
NP
D-B
a
VP
VB-P
bought
NP-B
D
a
NN
cat
-1: BC +1: BC,A1
-1: A0,A2,A3,A4,A5
Figure 2: Examples of AST
m
structured features.
and automatic Charniak parse trees (Charniak,
2000) as provided for the CoNLL 2005 evaluation
campaign (Carreras and Ma`rquez, 2005). SRL can
be decomposed into two tasks: boundary detec-
tion, where the word sequences that are arguments
of a predicate word w are identified, and role clas-
sification, where each argument is assigned the
proper role. The former task requires a binary
Boundary Classifier (BC), whereas the second in-
volves a Role Multi-class Classifier (RM).
5.1.1 Setup
If the constituency parse tree t of a sentence s
is available, we can look at all the pairs ?p, n
i
?,
where n
i
is any node in the tree and p is the node
dominating w, and decide whether n
i
is an argu-
ment node or not, i.e. whether it exactly dominates
all and only the words encoding any of w?s argu-
ments. The objects that we classify are subsets
of the input parse tree that encompass both p and
n
i
. Namely, we use the AST
m
structure defined
in (Moschitti et al, 2008), which is the minimal
tree that covers all and only the words of p and n
i
.
In the AST
m
, p and n
i
are marked so that they can
be distinguished from the other nodes. An AST
m
is regarded as a positive example for BC if n
i
is an
argument node, otherwise it is considered a nega-
tive example. Positive BC examples can be used to
train an efficient RM: for each role r we can train
a classifier whose positive examples are argument
nodes whose label is exactly r, whereas negative
examples are argument nodes labeled r? 6= r. Two
AST
m
s extracted from an example parse tree are
shown in Figure 2: the first structure is a negative
example for BC and is not part of the data set of
RM, whereas the second is a positive instance for
BC and A1.
To train BC we used PropBank sections 1
through 6, extracting AST
m
structures out of the
first 1 million ?p, n
i
? pairs from the corresponding
parse trees. As a test set we used the 149,140 in-
stance collected from the annotations in Section
24. There are 61,062 positive examples in the
training set (i.e. 6.1%) and 8,515 in the test set
115
(i.e. 5.7%).
For RM we considered all the argument nodes
of any of the six PropBank core roles (i.e. A0,
. . . , A5) from all the available training sections,
i.e. 2 through 21, for a total of 179,091 train-
ing instances. Similarly, we collected 5,928 test
instances from the annotations of Section 24.
Columns Tr+ and Te+ of Table 1 show the num-
ber of positive training and test examples, respec-
tively, for BC and the role classifiers.
For all the linearized classifiers, we used 50
splits for the FMI stage and we set the threshold
value L = 50k and maxexp = 1 during FMI and
TFX. We did not validate these parameters, which
we know to be sub-optimal. These values were
selected during the development of the software
because, on a very small test bed, they resulted in
a responsive and accurate system.
We should point out that other experiments have
shown that linearization is very robust with re-
spect to parametrization: due to the huge num-
ber and variety of fragments in the TK space, dif-
ferent choices of the parameters result in differ-
ent explicit spaces and more or less efficient solu-
tions, but in most cases the final accuracy of the
linearized classifiers is affected only marginally.
For example, it could be expected that reducing
the number of splits during KSL would improve
the final accuracy of a linearized classifier, as the
weights used for FMI would then converge to the
global optimum. Instead, we have observed that
increasing the number of splits does not necessar-
ily decrease the accuracy of the linearized classi-
fier.
The evaluation on the whole SRL task using
the official CoNLL?05 evaluator was not carried
out because producing complete annotations re-
quires several steps (e.g. overlap resolution, OvA
or Pairwise combination of individual role classi-
fiers) that would shade off the actual impact of the
methodology on classification.
5.1.2 Results
The left side of Table 1 shows the distribution of
positive data points in the training and test sets of
each classifier. Columns SST and SST
?
compare
side by side the F
1
measure of the non-linearized
and linearized classifier for each class. The accu-
racy of the RM classifier is the percentage of cor-
rect class assignments.
We can see that the accuracy of linearized clas-
sifiers is always in line with vanilla SST, even
Data set Accuracy
Class Tr+ Te+ SST SST
?
BC 61,062 8,515 81.8 81.3
A0 60,900 2,014 91.6 91.1
A1 90,636 3,041 89.0 89.4
A2 21,291 697 73.1 73.0
A3 3,481 105 56.8 53.0
A4 2,713 69 69.1 67.9
A5 69 2 66.7 0.0
RM 87.8 87.8
Table 1: Number of positive training (Tr+) and test
(Te+) examples in the SRL dataset. Accuracy of
the non-linearized (SST) and linearized (SST
?
) bi-
nary classifiers (i.e. BC, A0, . . . A5) is F
1
measure.
Accuracy of RM is the percentage of correct class
assignments.
if the selected linearization parameters generate
a very rough approximation of the original frag-
ment space, generally consisting of billions of
fragments. BC
?
(i.e. the linearized BC) has an
F
1
of 81.3, just 0.5% less than BC, i.e. 81.8. Con-
cerning RM
?
, its accuracy is the same as the non
linearized classifier, i.e. 87.8.
We should consider that the linearization frame-
work can drastically improve the efficiency of
learning and classification when dealing with large
amounts of data. For a linearized classifier, we
consider training time to be the overall time re-
quired to carry out the following activities: KSL,
FMI, TFX on training data and ESL. Similarly,
we consider test time the time necessary to per-
form TFX on test data and ESC. Training BC took
more than two days of CPU time and testing about
4 hours, while training and testing the linearized
boundary classifier required only 381 and 25 min-
utes, respectively. That is, on the same amount
of data we can train a linearized classifier about
8 times as fast, and test it in about 1 tenth of the
time. Concerning RM, sequential training of the
6 models took 2,596 minutes, while testing took
27 minutes. The linearized role multi classifier re-
quired 448 and 24 minutes for training and test-
ing, respectively, i.e. training is about 5 times as
fast while testing time is about the same. If com-
pared with the boundary classifier, the improve-
ment in efficiency is less evident: indeed, the rel-
atively small size of the role classifiers data sets
limits the positive effect of splitting training data
into smaller chunks.
SRL fragment space. Table 3 lists the best frag-
ments identified for the Boundary Classifier. We
should remember that we are using AST
m
struc-
116
tures as input to our classifiers: nodes whose la-
bel end with ?-P? are predicate nodes, while nodes
whose label ends with ?-B? are candidate argu-
ment nodes.
All the most relevant fragments encode the min-
imum sub-tree encompassing the predicate and the
argument node. This kind of structured feature
subsumes several features traditionally employed
for explicit SRL models: the Path (i.e. the se-
quence of nodes connecting the predicate and the
candidate argument node), Phrase Type (i.e. the
label of the candidate argument node), Predicate
POS (i.e. the POS of the predicate word), Posi-
tion (i.e. whether the argument is to the left or to
the right of the predicate) and Governing Category
(i.e. the label of the common ancestor) defined
in (Gildea and Jurafsky, 2002).
The linearized model for BC contains about 160
thousand fragments. Of these, about 70 and 33
thousand encompass the candidate argument or the
predicate node, respectively. About 16 thousand
fragments contain both.
5.2 Question Classification
For question classification we used the data set
from the TREC 10 QA evaluation campaign2, con-
sisting of 5,500 training and 500 test questions.
5.2.1 Setup
Given a question, the QC task consists in selecting
the most appropriate expected answer type from a
given set of possibilities. We adopted the question
taxonomy known as coarse grained, which has
been described in (Zhang and Lee, 2003) and (Li
and Roth, 2006), consisting of six non overlap-
ping classes: Abbreviations (ABBR), Descrip-
tions (DESC, e.g. definitions or explanations), En-
tity (ENTY, e.g. animal, body or color), Human
(HUM, e.g. group or individual), Location (LOC,
e.g. cities or countries) and Numeric (NUM, e.g.
amounts or dates).
For each question, we generate the full parse
of the sentence and use it to train SST and (lin-
earized) SST
?
models. The automatic parses are
obtained with the Stanford parser3 (Klein and
Manning, 2003). We actually have only 5,483 sen-
tences in our training set, due to parsing issues
with a few of them.
2http://l2r.cs.uiuc.edu/cogcomp/Data/
QA/QC/
3http://nlp.stanford.edu/software/
lex-parser.shtml
Data set Accuracy
Class Tr+ Te+ SST SST
?
ABBR 89 9 80.0 87.5
DESC 1,164 138 96.0 94.5
ENTY 1,269 94 63.9 63.5
HUM 1,231 65 88.1 87.2
LOC 834 81 77.6 77.9
NUM 896 113 80.4 80.8
Overall 86.2 86.6
Table 2: Number of positive training (Tr+) and test
(Te+) examples in the QA dataset. Accuracy of
the non-linearized (SST) and linearized (SST
?
) bi-
nary classifiers is F
1
measure. Overall accuracy is
the percentage of correct class assignments.
The classifiers are arranged in a one-vs.-all
(OvA) configuration, where each sentence is a
positive example for one of the six classes, and
negative for the other five. Given the very small
size of the data set, we used S = 1 during KSL
for the linearized classifier (i.e. we didn?t parti-
tion training data). We carried out no validation of
the parameters, and we used maxexp = 4 and
L = 50k in order to generate a rich fragment
space.
5.2.2 Results
Table 2 shows the number of positive examples
in the training and test set of each individual bi-
nary classifiers. Columns SST and SST
?
compare
the F
1
measure of the vanilla and linearized classi-
fiers on the individual classes, and the accuracy of
the complete QC task (Row Overall) in terms of
percentage of correct class assignments. Also in
this case, we can notice that the accuracy of the
linearized classifiers is always in line with non-
linearized ones, e.g. 86.6 vs. 86.2 for the multi-
classifiers. These results are lower than those de-
rived in (Moschitti, 2006; Moschitti et al, 2007),
i.e. 88.2 and 90.4, respectively, where the param-
eters for each classifier were carefully optimized.
QC Fragment space. Tables from 4 to 9 list the
top fragments identified for each class 4.
As expected, for all the categories the domain
lexical information is very relevant. For example,
film, color, book, novel and sport for ENTY or
city, country, state and capital for LOC. Of the six
classes, ENTY (Table 6) is mostly characterized
by lexical features. Interestingly, function words,
which would have been eliminated by a pure In-
formation Retrieval approach (i.e. by means of
4Some categories show meaningful syntactic fragments
after the first 10, so for them we report more subtrees.
117
standard stop-list), are in the top positions, e.g.:
why and how for DESC, what for ENTY, who for
HUM, where for LOC and when for NUM. For the
latter, also how seems to be important suggesting
that features may strongly characterize more than
one given class.
Characteristic syntactic features appear in the
top positions for each class, for example: (VP (VB
(stand)) (PP)), which suggests that stand should
be followed by a prepositional phrase to character-
ize ABBR; or (NP (NP (DT) (NN (abbreviation)))
(PP)), which suggests that, to be in a relevant pat-
tern, abbreviation should be preceded by an article
and followed by a PP. Also, the syntactic struc-
ture is useful to differentiate the use of the same
important words, e.g. (SBARQ (WHADVP (WRB
(How))) (SQ) (.)) for DESC better characterizes
the use of how with respect to NUM, in which a
relevant use is (WHADJP (WRB (How)) (JJ)).
In (Moschitti et al, 2007) it was shown that the
use of TK improves QC of 1.2 percent points, i.e.
from 90.6 to 91.8: further analysis of these frag-
ments may help us to device compact, less sparse
syntactic features and design more accurate mod-
els for the task.
6 Discussion
The fact that our model doesn?t always improve
the accuracy of a standard SST model might be
related to the process of splitting training data and
employing locally estimated weights during FMI.
Concerning the experiments presented in this
paper, this objection might apply to the results on
SRL, where we used 50 splits to identify the most
relevant fragments, but not to those on QC, where
given the limited size of the data set we decided
not to split training data at all as explained in Sec-
tion 5.2. Furthermore, as we already discussed,
we have evidence that there is no direct correlation
between the number of splits used for KSL and
the accuracy of the resulting classifier. After all,
the optimization carried out during ESL is global,
and we can assume that, if we mined enough frag-
ments during FMI, than those actually retained by
the global linear model would be by and large the
same, regardless of the split configuration.
More in general, feature selection may give an
improvement to some learning algorithm but if it
can help SVMs is debatable, since its related the-
ory show that they are robust to irrelevant fea-
tures. In our specific case, we remove features
(ADJP(RB-B)(VBN-P))
(NP(VBN-P)(NNS-B))
(S(NP-B)(VP))
(VP(VBD-P(said))(SBAR))
(VP(VB-P)(NP-B))
(NP(VBG-P)(NNS-B))
(VP(VBD-P)(NP-B))
(VP(VBG-P)(NP-B))
(VP(VBZ-P)(NP-B))
(VP(VBN-P)(NP-B))
(VP(VBP-P)(NP-B))
(NP(NP-B)(VP))
(NP(VBG-P)(NN-B))
(S(S(VP(VBG-P)))(NP-B))
Table 3: Best fragments for SRL BC.
(NN(abbreviation))
(NP(DT)(NN(abbreviation)))
(NP(DT(the))(NN(abbreviation)))
(IN(for))
(VB(stand))
(VBZ(does))
(PP(IN))
(VP(VB(stand))(PP))
(NP(NP(DT)(NN(abbreviation)))(PP))
(SQ(VBZ)(NP)(VP(VB(stand))(PP)))
(SBARQ(WHNP)(SQ(VBZ)(NP)(VP(VB(stand))(PP)))(.))
(SQ(VBZ(does))(NP)(VP(VB(stand))(PP)))
(VP(VBZ)(NP(NP(DT)(NN(abbreviation)))(PP)))
Table 4: Best fragments for the ABBR class.
(WRB(Why))
(WHADVP(WRB(Why)))
(WHADVP(WRB(How)))
(WHADVP(WRB))
(VB(mean))
(VBZ(causes))
(VB(do))
(ROOT(SBARQ(WHADVP(WRB(How)))(SQ)(.)))
(ROOT(SBARQ(WHADVP(WRB(How)))(SQ)(.(?))))
(SBARQ(WHADVP(WRB(How)))(SQ))
(WRB(How))
(SBARQ(WHADVP(WRB(How)))(SQ)(.))
(SBARQ(WHADVP(WRB(How)))(SQ)(.(?)))
(SBARQ(WHADVP(WRB(Why)))(SQ))
(ROOT(SBARQ(WHADVP(WRB(Why)))(SQ)))
(SBARQ(WHADVP(WRB))(SQ))
Table 5: Best fragments for the DESC class.
(NN(film))
(NN(color))
(NN(book))
(NN(novel))
(NN(sport))
(WP(What))
(NN(fear))
(NN(movie))
(NN(word))
(VP(VBN(called)))
(NN(game))
(NP(DT)(NN(fear)))
(NP(NP(DT)(NN(fear)))(PP))
Table 6: Best fragments for the ENTY class.
118
(NN(company))
(WP(Who))
(WHNP(WP(Who)))
(NN(name))
(NN(team))
(NN(baseball))
(WHNP(WP))
(NN(character))
(NNP(President))
(NN(leader))
(NN(actor))
(NN(president))
(JJ(Whose))
(VP(VBD)(NP))
(NP(NP)(JJ)(NN(name)))
(VP(VBD)(VP))
(NN(organization))
(VP(VBD)(NP)(PP(IN)(NP)))
(SBARQ(WHNP(WP(Who)))(SQ)(.))
(ROOT(SBARQ(WHNP(WP(Who)))(SQ)(.)))
(ROOT(SBARQ(WHNP(WP(Who)))(SQ)(.(?))))
(SBARQ(WHNP(WP(Who)))(SQ)(.(?)))
Table 7: Best fragments for the HUM class.
(NN(city))
(NN(country))
(WRB(Where))
(NN(state))
(WHADVP(WRB(Where)))
(NN(capital))
(NP(NN(city)))
(NNS(countries))
(NP(NN(state)))
(PP(IN(in)))
(SBARQ(WHADVP(WRB(Where)))(SQ)(.(?)))
(SBARQ(WHADVP(WRB(Where)))(SQ)(.))
(ROOT(SBARQ(WHADVP(WRB(Where)))(SQ)(.)))
(ROOT(SBARQ(WHADVP(WRB(Where)))(SQ)(.(?))))
(NN(island))
(NN(address))
(NN(river))
(NN(mountain))
(ROOT(SBARQ(WHADVP(WRB(Where)))(SQ)))
(SBARQ(WHADVP(WRB(Where)))(SQ))
Table 8: Best fragments for the LOC class.
(WRB(How))
(WHADVP(WRB(When)))
(WRB(When))
(JJ(many))
(NN(year))
(WHADJP(WRB)(JJ))
(NP(NN(year)))
(WHADJP(WRB(How))(JJ))
(NN(date))
(SBARQ(WHADVP(WRB(When)))(SQ)(.(?)))
(SBARQ(WHADVP(WRB(When)))(SQ)(.))
(NN(day))
(NN(population))
(ROOT(SBARQ(WHADVP(WRB(When)))(SQ)(.)))
(ROOT(SBARQ(WHADVP(WRB(When)))(SQ)(.(?))))
(JJ(average))
(NN(number))
Table 9: Best fragments for the NUM class.
whose SVM weights are the lowest, i.e. those
that are (almost) irrelevant for the SVM. There-
fore, the chance of this resulting in an improve-
ment is rather low.
With respect to cases where our model is less
accurate than a standard SST, we should consider
that our choice of parameters is sub-optimal and
we adopt a very aggressive feature selection strat-
egy, that only retains a few thousand features from
a space where there are hundreds of millions of
different features.
7 Conclusions
We introduced a novel framework for support vec-
tor classification that combines advantages of con-
volution kernels, i.e. the generation of a very high
dimensional structure space, with the efficiency
and clarity of explicit representations in a linear
space.
For this paper, we focused on the SubSet Tree
kernel and verified the potential of the proposed
solution on two NLP tasks, i.e. semantic role
labeling and question classification. The exper-
iments show that our framework drastically re-
duces processing time, e.g. boundary classifica-
tion for SRL, while preserving the accuracy.
We presented a selection of the most relevant
fragments identified for the SRL boundary classi-
fier as well as for each class of the coarse grained
QC task. Our analysis shows that our frame-
work can discover state-of-the-art features, e.g.
the Path feature for SRL. We believe that shar-
ing these fragments with the NLP community and
studying them in more depth will be useful to
identify new, relevant features for the character-
ization of several learning problems. For this
purpose, we made available the fragment spaces
at http://danielepighin.net and we will keep
them updated with new set of experiments on new
tasks, e.g. SRL based on FrameNet and VerbNet,
e.g. (Giuglea and Moschitti, 2004).
In our future work, we plan to widen the list
of covered tasks and to extend our algorithm to
cope with different kernel families, such as the
partial tree kernel and kernels defined over pairs
of trees, e.g. the ones used for textual entailment
in (Moschitti and Zanzotto, 2007). We also plan to
move from mining fragments to mining classes of
fragments, i.e. to identify prototypical fragments
in the fragment space that generalize topological
sub-classes of the most relevant fragments.
119
References
Fabio Aiolli, Giovanni Da San Martino, Alessandro Sper-
duti, and Alessandro Moschitti. 2006. Fast on-line kernel
learning for trees. In Proceedings of ICDM?06.
Stephan Bloehdorn and Alessandro Moschitti. 2007a. Com-
bined syntactic and semantic kernels for text classification.
In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b. Struc-
ture and semantics for expressive text kernels. In In Pro-
ceedings of CIKM ?07.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of CoNLL?05.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00.
Michael Collins and Nigel Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over Discrete
Structures, and the Voted Perceptron. In Proceedings of
ACL?02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceedings of
ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for Re-
lational Learning. In Proceedings of ICML 2003.
Mona Diab, Alessandro Moschitti, and Daniele Pighin. 2008.
Semantic role labeling systems for Arabic using kernel
methods. In Proceedings of ACL-08: HLT, pages 798?
806.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics, 28:245?
288.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge discovery using framenet, verbnet and prop-
bank. In A. Meyers, editor, Workshop on Ontology and
Knowledge Discovering at ECML 2004, Pisa, Italy.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation. In
Proceedings of ACL?05, pages 403?410.
Hans P. Graf, Eric Cosatto, Leon Bottou, Igor Durdanovic,
and Vladimir Vapnik. 2004. Parallel support vector ma-
chines: The cascade svm. In Neural Information Process-
ing Systems.
Isabelle Guyon and Andre? Elisseeff. 2003. An introduc-
tion to variable and feature selection. Journal of Machine
Learning Research, 3:1157?1182.
David Haussler. 1999. Convolution kernels on discrete struc-
tures. Technical report, Dept. of Computer Science, Uni-
versity of California at Santa Cruz.
T. Joachims. 2000. Estimating the generalization perfor-
mance of a SVM efficiently. In Proceedings of ICML?00.
Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for
semi-structured data. In Proceedings of ICML?02.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding up
training with tree kernels for node relation labeling. In
Proceedings of HLT-EMNLP?05.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL?03, pages
423?430.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-
based parse reranking with subtree features. In Proceed-
ings of ACL?05.
Xin Li and Dan Roth. 2006. Learning question classifiers:
the role of semantic information. Natural Language En-
gineering, 12(3):229?249.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from
texts. In ICML?07.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and
Suresh Manandhar. 2007. Exploiting syntactic and shal-
low semantic kernels for question/answer classification.
In Proceedings of ACL?07.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2008. Tree kernels for semantic role labeling. Compu-
tational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Pro-
ceedings of ECML?06, pages 318?329.
Julia Neumann, Christoph Schnorr, and Gabriele Steidl.
2005. Combined SVM-Based Feature Selection and Clas-
sification. Machine Learning, 61(1-3):129?150.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of semantic
roles. Comput. Linguist., 31(1):71?106.
J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U. Dayal,
and M. C. Hsu. 2001. PrefixSpan Mining Sequential Pat-
terns Efficiently by Prefix Projected Pattern Growth. In
Proceedings of ICDE?01.
Daniele Pighin and Alessandro Moschitti. 2009. Efficient
linearization of tree kernel functions. In Proceedings of
CoNLL?09.
Alain Rakotomamonjy. 2003. Variable selection using SVM
based criteria. Journal of Machine Learning Research,
3:1357?1370.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Us-
ing LTAG Based Features in Parse Reranking. In Proceed-
ings of EMNLP?06.
Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree
Kernels with Statistical Feature Mining. In Proceedings
of NIPS?05.
Kristina Toutanova, Penka Markova, and Christopher Man-
ning. 2004. The Leaf Path Projection View of Parse
Trees: Exploring String Kernels for HPSG Parse Selec-
tion. In Proceedings of EMNLP 2004.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience.
Ellen M. Voorhees. 2001. Overview of the trec 2001 ques-
tion answering track. In In Proceedings of the Tenth Text
REtrieval Conference (TREC, pages 42?51.
Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimil-
iano Pontil, Tomaso Poggio, and Vladimir Vapnik. 2001.
Feature Selection for SVMs. In Proceedings of NIPS?01.
Jason Weston, Andre? Elisseeff, Bernhard Scho?lkopf, and
Mike Tipping. 2003. Use of the zero norm with lin-
ear models and kernel methods. J. Mach. Learn. Res.,
3:1439?1461.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of SI-
GIR?03, pages 26?32.
120
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 141?144,
Prague, June 2007. c?2007 Association for Computational Linguistics
FBK-IRST: Kernel Methods for Semantic Relation Extraction
Claudio Giuliano and Alberto Lavelli and Daniele Pighin and Lorenza Romano
FBK-IRST, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Povo (TN), ITALY
{giuliano,lavelli,pighin,romano}@itc.it
Abstract
We present an approach for semantic rela-
tion extraction between nominals that com-
bines shallow and deep syntactic processing
and semantic information using kernel meth-
ods. Two information sources are consid-
ered: (i) the whole sentence where the re-
lation appears, and (ii) WordNet synsets and
hypernymy relations of the candidate nom-
inals. Each source of information is rep-
resented by kernel functions. In particu-
lar, five basic kernel functions are linearly
combined and weighted under different con-
ditions. The experiments were carried out
using support vector machines as classifier.
The system achieves an overall F1 of 71.8%
on the Classification of Semantic Relations
between Nominals task at SemEval-2007.
1 Introduction
The starting point of our research is an approach
for identifying relations between named entities ex-
ploiting only shallow linguistic information, such as
tokenization, sentence splitting, part-of-speech tag-
ging and lemmatization (Giuliano et al, 2006). A
combination of kernel functions is used to represent
two distinct information sources: (i) the global con-
text where entities appear and (ii) their local con-
texts. The whole sentence where the entities appear
(global context) is used to discover the presence of
a relation between two entities. Windows of limited
size around the entities (local contexts) provide use-
ful clues to identify the roles played by the entities
within a relation (e.g., agent and target of a gene in-
teraction). In the task of detecting protein-protein
interactions, we obtained state-of-the-art results on
two biomedical data sets. In addition, promising re-
sults have been recently obtained for relations such
as work for and org based in in the news domain1.
In this paper, we investigate the use of the above
approach to discover semantic relations between
nominals. In addition to the original feature rep-
resentation, we have integrated deep syntactic pro-
cessing of the global context and semantic informa-
tion for each candidate nominals using WordNet as
external knowledge source. Each source of informa-
tion is represented by kernel functions. A tree kernel
(Moschitti, 2004) is used to exploit the deep syn-
tactic processing obtained using the Charniak parser
(Charniak, 2000). On the other hand, bag of syn-
onyms and hypernyms is used to enhance the repre-
sentation of the candidate nominals. The final sys-
tem is based on five basic kernel functions (bag-of-
words kernel, global context kernel, tree kernel, su-
persense kernel, bag of synonyms and hypernyms
kernel) linearly combined and weighted under dif-
ferent conditions. The experiments were carried out
using support vector machines (Vapnik, 1998) as
classifier.
We present results on the Classification of Seman-
tic Relations between Nominals task at SemEval-
2007, in which sentences containing ordered pairs
of marked nominals, possibly semantically related,
have to be classified. On this task, we achieve an
overall F1 of 71.8% (B category evaluation), largely
outperforming all the baselines.
1These results appear in a paper currently under revision.
141
2 Kernel Methods for Relation Extraction
In order to implement the approach based on syntac-
tic and semantic information, we employed a linear
weighted combination of kernels, using support vec-
tor machines as classifier. We designed two families
of basic kernels: syntactic kernels and semantic ker-
nels. These basic kernels are combined by exploit-
ing the closure properties of kernels. We define our
composite kernel KC(x1, x2) as follows
n
?
i=1
wi
Ki(x1, x2)
?
Ki(x1, x1)Ki(x2, x2)
, (1)
where each basic kernel Ki is normalized and wi ?
{0, 1} is the kernel weight. The normalization factor
plays an important role in allowing us to integrate in-
formation from heterogeneous knowledge sources.
All basic kernels, but the tree kernel (see Section
2.1.3), are explicitly calculated as follows
Ki(x1, x2) = ??(x1), ?(x2)?, (2)
where ?(?) is the embedding vector. Even though
the resulting feature space has high dimensionality,
an efficient computation of Equation 2 can be carried
out explicitly since the input representations defined
below are extremely sparse.
2.1 Syntactic Kernels
Syntactic kernels are defined over the whole sen-
tence where the candidate nominals appear.
2.1.1 Global Context Kernel
Bunescu and Mooney (2005) and Giuliano et al
(2006) successfully exploited the fact that relations
between named entities are generally expressed us-
ing only words that appear simultaneously in one of
the following three contexts.
Fore-Between Tokens before and between the two
entities, e.g. ?the head of [ORG], Dr. [PER]?.
Between Only tokens between the two entities, e.g.
?[ORG] spokesman [PER]?.
Between-After Tokens between and after the two
entities, e.g. ?[PER], a [ORG] professor?.
Here, we investigate whether this assumption is
also correct for semantic relations between nomi-
nals. Our global context kernel operates on the con-
texts defined above, where each context is repre-
sented using a bag-of-words. More formally, given
a) S1
S
NP
PRP
I
VP
VBD
found
NP
DT
some
NN
candy
PP
IN
in
NP
PRP$
my
NN
underwear
.
.
b) S
VP
VBD
found
NP
NNS
agent
PP
IN
in
NP
NN
target
Figure 1: A content-container relation test sentence
parse tree (a) and the corresponding RT structure (b).
a relation example R, we represent a context C as a
row vector
?C(R) = (tf(t1, C), tf(t2, C), . . . , tf(tl, C)) ? Rl, (3)
where the function tf(ti, C) records how many
times a particular token ti is used in C . Note that
this approach differs from the standard bag-of-words
as punctuation and stop words are included in ?C ,
while the nominals are not. To improve the classi-
fication performance, we have further extended ?C
to embed n-grams of (contiguous) tokens (up to n =
3). By substituting ?C into Equation 2, we obtain
the n-gram kernel Kn, which counts uni-grams, bi-
grams, . . . , n-grams that two patterns have in com-
mon2. The Global Context kernel KGC(R1, R2) is
then defined as
KF B(R1, R2) +KB(R1, R2) +KBA(R1, R2), (4)
where KFB , KB and KBA are n-gram kernels
that operate on the Fore-Between, Between and
Between-After patterns respectively.
2.1.2 Bag-of-Words Kernel
The bag-of-words kernel is defined as the previ-
ous kernel but it operates on the whole sentence.
2.1.3 Tree Kernel
Tree kernels can trigger automatic feature selec-
tion and represent a viable alternative to the man-
2In the literature, it is also called n-spectrum kernel.
142
ual design of attribute-value syntactic features (Mos-
chitti, 2004). A tree kernel KT (t1, t2) evaluates
the similarity between two trees t1 and t2 in terms
of the number of fragments they have in common.
Let Nt be the set of nodes of a tree t and F =
{f1, f2, . . . , f|F|} be the fragment space of t1 and
t2. Then
KT (t1, t2) =
P
ni?Nt1
P
nj?Nt2
?(ni, nj) , (5)
where ?(ni, nj) =
?|F|
k=1 Ik(ni) ? IK(nj) and
Ik(n) = 1 if k is rooted in n, 0 otherwise.
For this task, we defined an ad-hoc class of struc-
tured features (Moschitti et al, 2006), the Reduced
Tree (RT), which can be derived from a sentence
parse tree t by the following steps: (1) remove all the
terminal nodes but those labeled as relation entities
and those POS tagged as verbs, auxiliaries, prepo-
sitions, modals or adverbs; (2) remove all the in-
ternal nodes not covering any remaining terminal;
(3) replace the entity words with placeholders that
indicate the direction in which the relation should
hold. Figure 1 shows a parse tree and the resulting
RT structure.
2.2 Semantic Kernels
In (Giuliano et al, 2006), we used the local context
kernel to infer semantic information on the candi-
date entities (i.e., roles played by the entities). As
the task organizers provide the WordNet sense and
role for each nominal, we directly use this informa-
tion to enrich the feature space and do not include
the local context kernel in the combination.
2.2.1 Bag of Synonyms and Hypernyms Kernel
By using the WordNet sense key provided, each
nominal is represented by the bag of its synonyms
and hypernyms (direct and inherited hypernyms).
Formally, given a relation example R, each nominal
N is represented as a row vector
?N(R) = (f(t1, N), f(t2, N), . . . , f(tl, N)) ? Rl, (6)
where the binary function f(ti, N) records if a par-
ticular lemma ti is contained into the bag of syn-
onyms and hypernyms of N. The bag of synonyms
and hypernyms kernel KS&H(R1, R2) is defined as
Ktarget(R1, R2) +Kagent(R1, R2), (7)
where Ktarget and Kagent are defined by substitut-
ing the embedding of the target and agent nominals
into Equation 2 respectively.
2.2.2 Supersense Kernel
WordNet synsets are organized into 45 lexicogra-
pher files, based on syntactic category and logical
groupings. E.g., noun.artifact is for nouns denoting
man-made objects, noun.attribute for nouns denot-
ing attributes for people and objects etc. The super-
sense kernel KSS(R1, R2) is a variant of the previ-
ous kernel that uses the names of the lexicographer
files (i.e., the supersense) to index the feature space.
3 Experimental Setup and Results
Sentences have been tokenized, lemmatized, and
POS tagged with TextPro3. We considered each re-
lation as a different binary classification task, and
each sentence in the data set is a positive or negative
example for the relation. The direction of the rela-
tion is considered labelling the first argument of the
relation as agent and the second as target.
All the experiments were performed using the
SVM package SVMLight-TK4, customized to em-
bed our own kernels. We optimized the linear com-
bination weights wi and regularization parameter c
using 10-fold cross-validation on the training set.
We set the cost-factor j to be the ratio between the
number of negative and positive examples.
Table 1 shows the performance on the test set. We
achieve an overall F1 of 71.8% (B category evalua-
tion), largely outperforming all the baselines, rang-
ing from 48.5% to 57.0%. The average training plus
test running time for a relation is about 10 seconds
on a Intel Pentium M755 2.0 GHz. Figure 2 shows
the learning curves on the test set. For all relations
but theme-tool, accurate classifiers can be learned
using a small fraction of training.
4 Discussion and Conclusion
Experimental results show that our kernel-based ap-
proach is appropriate also to detect semantic rela-
tions between nominals. However, differently from
relation extraction between named entities, there is
not a common kernel setup for all relations. E.g.,
3http://tcc.itc.it/projects/textpro/
4http://ai-nlp.info.uniroma2.it/moschitti/
143
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 30  40  50  60  70  80  90  100
F 1
Percentage of Training
Learning Curve
Cause-Effect
Instrument-Agency
Product-Producer
Origin-Entity
Theme-Tool
Part-Whole
Content-Container
Figure 2: Learning curves on the test set.
Relation P R F1 Acc
Cause-Effect 67.3 90.2 77.1 72.5
Instrument-Agency 76.9 78.9 77.9 78.2
Product-Producer 76.2 77.4 76.8 68.8
Origin-Entity 62.2 63.9 63.0 66.7
Theme-Tool 69.2 62.1 65.5 73.2
Part-Whole 65.5 73.1 69.1 76.4
Content-Container 78.8 68.4 73.2 74.3
Avg 70.9 73.4 71.8 72.9
Table 1: Results on the test set.
for content-container we obtain the best perfor-
mance combining the tree kernel and the bag of syn-
onyms and hypernyms kernel; on the other hand, for
instrument-agency the best performance is obtained
by combining the global kernel and the supersense
kernel. Surprisingly, the supersense kernel alone
works quite well and obtains results comparable to
the bag of synonyms and hypernyms kernel. This
result is particularly interesting as a supersense tag-
ger can easily provide a satisfactory accuracy (Cia-
ramita and Altun, 2006). On the other hand, ob-
taining an acceptable accuracy in word sense disam-
biguation (required for a realistic application of the
bag of synonyms and hypernyms kernel) is imprac-
tical as a sufficient amount of training for at least all
nouns is currently not available. Hence, the super-
sense could play a crucial role to improve the perfor-
mance when approaching this task without the nomi-
nals disambiguated. To model the global context us-
ing the Fore-Between, Between and Between-After
contexts did not produce a significant improvement
with respect to the bag-of-words model. This is
mainly due to the fact that examples have been col-
lected from the Web using heuristic patterns/queries,
most of which implying Between patterns/contexts
(e.g., for the cause-effect relation ?* comes from *?,
?* out of *? etc.).
5 Acknowledgements
Claudio Giuliano, Alberto Lavelli and Lorenza Ro-
mano are supported by the X-Media project (http:
//www.x-media-project.org), sponsored
by the European Commission as part of the Infor-
mation Society Technologies (IST) programme un-
der EC grant number IST-FP6-026978.
References
Razvan Bunescu and Raymond J. Mooney. 2005. Subse-
quence kernels for relation extraction. In Proceedings
of the 19th Conference on Neural Information Pro-
cessing Systems, Vancouver, British Columbia.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132?139, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602,
Sydney, Australia, July.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of the Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-2006), Trento, Italy, 5-7 April.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
CoNLL-X.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages 335?
342, Barcelona, Spain, July.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York, NY.
144
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 30?38,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Linearization of Tree Kernel Functions
Daniele Pighin
FBK-Irst, HLT
Via di Sommarive, 18 I-38100 Povo (TN) Italy
pighin@fbk.eu
Alessandro Moschitti
University of Trento, DISI
Via di Sommarive, 14 I-38100 Povo (TN) Italy
moschitti@disi.unitn.it
Abstract
The combination of Support Vector Machines
with very high dimensional kernels, such as
string or tree kernels, suffers from two ma-
jor drawbacks: first, the implicit representa-
tion of feature spaces does not allow us to un-
derstand which features actually triggered the
generalization; second, the resulting compu-
tational burden may in some cases render un-
feasible to use large data sets for training. We
propose an approach based on feature space
reverse engineering to tackle both problems.
Our experiments with Tree Kernels on a Se-
mantic Role Labeling data set show that the
proposed approach can drastically reduce the
computational footprint while yielding almost
unaffected accuracy.
1 Introduction
The use of Support Vector Machines (SVMs)
in supervised learning frameworks is spreading
across different communities, including Computa-
tional Linguistics and Natural Language Processing,
thanks to their solid mathematical foundations, ef-
ficiency and accuracy. Another important reason
for their success is the possibility of using kernel
functions to implicitly represent examples in some
high dimensional kernel space, where their similar-
ity is evaluated. Kernel functions can generate a very
large number of features, which are then weighted
by the SVM optimization algorithm obtaining a fea-
ture selection side-effect. Indeed, the weights en-
coded by the gradient of the separating hyperplane
learnt by the SVM implicitly establish a ranking be-
tween features in the kernel space. This property has
been exploited in feature selection models based on
approximations or transformations of the gradient,
e.g. (Rakotomamonjy, 2003), (Weston et al, 2003)
or (Kudo and Matsumoto, 2003).
However, kernel based systems have two major
drawbacks: first, new features may be discovered
in the implicit space but they cannot be directly ob-
served. Second, since learning is carried out in the
dual space, it is not possible to use the faster SVM or
perceptron algorithms optimized for linear spaces.
Consequently, the processing of large data sets can
be computationally very expensive, limiting the use
of large amounts of data for our research or applica-
tions.
We propose an approach that tries to fill in the
gap between explicit and implicit feature represen-
tations by 1) selecting the most relevant features in
accordance with the weights estimated by the SVM
and 2) using these features to build an explicit rep-
resentation of the kernel space. The most innovative
aspect of our work is the attempt to model and im-
plement a solution in the context of structural ker-
nels. In particular we focus on Tree Kernel (TK)
functions, which are especially interesting for the
Computational Linguistics community as they can
effectively encode rich syntactic data into a kernel-
based learning algorithm. The high dimensionality
of a TK feature space poses interesting challenges in
terms of computational complexity that we need to
address in order to come up with a viable solution.
We will present a number of experiments carried
out in the context of Semantic Role Labeling, show-
ing that our approach can noticeably reduce training
time while yielding almost unaffected classification
accuracy, thus allowing us to handle larger data sets
at a reasonable computational cost.
The rest of the paper is structured as follows: Sec-
30
Fragment space
A
B A
A
B A
B A
A
B A
C
A
B A
B A
C
A
C
D
B A
D
B A
C
1 2 3 4 5 6 7
T1
A
B A
B A
C
T2
D
B A
C
?(T1) = [2, 1, 1, 1, 1, 0, 0]
?(T2) = [0, 0, 0, 0, 1, 1, 1]
K(T1, T2) = ??(T1), ?(T2)? = 1
Figure 1: Esemplification of a fragment space and the
kernel product between two trees.
tion 2 will briefly review SVMs and Tree Kernel
functions; Section 3 will detail our proposal for the
linearization of a TK feature space; Section 4 will
review previous work on related subjects; Section 5
will describe our experiments and comment on their
results; finally, in Section 6 we will draw our con-
clusions.
2 Tree Kernel Functions
The decision function of an SVM is:
f(~x) = ~w ? ~x+ b =
n?
i=1
?iyi ~xi ? ~x+ b (1)
where ~x is a classifying example and ~w and b are
the separating hyperplane?s gradient and its bias,
respectively. The gradient is a linear combination
of the training points ~xi, their labels yi and their
weights ?i. These and the bias are optimized at
training time by the learning algorithm. Applying
the so-called kernel trick it is possible to replace the
scalar product with a kernel function defined over
pairs of objects:
f(o) =
n?
i=1
?iyik(oi, o) + b
with the advantage that we do not need to provide
an explicit mapping ?(?) of our examples in a vector
space.
A Tree Kernel function is a convolution ker-
nel (Haussler, 1999) defined over pairs of trees.
Practically speaking, the kernel between two trees
evaluates the number of substructures (or fragments)
they have in common, i.e. it is a measure of their
overlap. The function can be computed recursively
in closed form, and quite efficient implementations
are available (Moschitti, 2006). Different TK func-
tions are characterized by alternative fragment defi-
nitions, e.g. (Collins and Duffy, 2002) and (Kashima
and Koyanagi, 2002). In the context of this paper
we will be focusing on the SubSet Tree (SST) ker-
nel described in (Collins and Duffy, 2002), which
relies on a fragment definition that does not allow to
break production rules (i.e. if any child of a node is
included in a fragment, then also all the other chil-
dren have to). As such, it is especially indicated for
tasks involving constituency parsed texts.
Implicitly, a TK function establishes a correspon-
dence between distinct fragments and dimensions in
some fragment space, i.e. the space of all the pos-
sible fragments. To simplify, a tree t can be repre-
sented as a vector whose attributes count the occur-
rences of each fragment within the tree. The ker-
nel between two trees is then equivalent to the scalar
product between pairs of such vectors, as exempli-
fied in Figure 1.
3 Mining the Fragment Space
If we were able to efficiently mine and store in a
dictionary all the fragments encoded in a model,
we would be able to represent our objects explicitly
and use these representations to train larger models
and very quick and accurate classifiers. What we
need to devise are strategies to make this approach
convenient in terms of computational requirements,
while yielding an accuracy comparable with direct
tree kernel usage.
Our framework defines five distinct activities,
which are detailed in the following paragraphs.
Fragment Space Learning (FSL) First of all, we
can partition our training data into S smaller sets,
and use the SVM and the SST kernel to learn S mod-
els. We will use the estimated weights to drive our
feature selection process. Since the time complexity
of SVM training is approximately quadratic in the
number of examples, this way we can considerably
accelerate the process of estimating support vector
weights.
According to statistical learning theory, being
trained on smaller subsets of the available data
these models will be less robust with respect to the
31
minimization of the empirical risk (Vapnik, 1998).
Nonetheless, since we do not need to employ them
for classification (but just to direct our feature se-
lection process, as we will describe shortly), we can
accept to rely on sub-optimal weights. Furthermore,
research results in the field of SVM parallelization
using cascades of SVMs (Graf et al, 2004) suggest
that support vectors collected from locally learnt
models can encode many of the relevant features re-
tained by models learnt globally. Henceforth, letMs
be the model associated with the s-th split, and Fs
the fragment space that can describe all the trees in
Ms.
Fragment Mining and Indexing (FMI) In Equa-
tion 1 it is possible to isolate the gradient ~w =?n
i=1 ?iyi ~xi, with ~xi = [x(1)i , . . . , x(N)i ], N being
the dimensionality of the feature space. For a tree
kernel function, we can rewrite x(j)i as:
x(j)i = ti,j?
`(fj)
?ti?
= ti,j?
`(fj)
??N
k=1(ti,k?`(fk))2
(2)
where: ti,j is the number of occurrences of the frag-
ment fj , associated with the j-th dimension of the
feature space, in the tree ti; ? is the kernel decay
factor; and `(fj) is the depth of the fragment.
The relevance |w(j)| of the fragment fj can be
measured as:
|w(j)| =
?????
n?
i=1
?iyix(j)i
????? . (3)
We fix a threshold L and from each model Ms
(learnt during FSL) we select the L most relevant
fragments, i.e. we build the set Fs,L = ?k{fk} so
that:
|Fs,L| = L and |w(k)| ? |w(i)|?fi ? F \ Fs,L .
In order to do so, we need to harvest all the frag-
ments with a fast extraction function, store them in
a compact data structure and finally select the frag-
ments with the highest relevance. Our strategy is ex-
emplified in Figure 2. First, we represent each frag-
ment as a sequence as described in (Zaki, 2002). A
sequence contains the labels of the fragment nodes
in depth-first order. By default, each node is the
child of the previous node in the sequence. A spe-
cial symbol (?) indicates that the next node in the
R1
A B
Z W
R2
X Y B
Z W
B
Z W
weight: w1 weight: w2 weight: w3
R1, A, ?, B, Z, ?, W
R2, X, ?, Y, ?, B, Z, ?, W
B, Z, ?, W
R1
w1A
? W
X R2w2
Z
Y
B
w3
1,1
1,2
2,11,1
1,1 1,-
1,1
1,3
3,1
1,2
1
2
Figure 2: Fragment indexing. Each fragment is repre-
sented as a sequence 1 and then encoded as a path in the
index 2 which keeps track of its cumulative relevance.
sequence should be attached after climbing one level
in the tree. For example, the tree (B (Z W)) in figure
is represented as the sequence [B, Z, ?, W]. Then, we
add the elements of the sequence to a graph (which
we call an index of fragments) where each sequence
becomes a path. The nodes of the index are the la-
bels of the fragment nodes, and each arc is associ-
ated with a pair of values ?d, n?: d is a node identi-
fier, which is unique with respect to the source node;
n is the identifier of the arc that must be selected at
the destination node in order to follow the path as-
sociated with the sequence. Index nodes associated
with a fragment root also have a field where the cu-
mulative relevance of the fragment is stored.
As an example, the index node labeled B in fig-
ure has an associated weight of w3, thus identify-
ing the root of a fragment. Each outgoing edge
univocally identifies an indexed fragment. In this
case, the only outgoing edge is labeled with the pair
?d = 1, n = 1?, meaning that we should follow it
to the next node, i.e. Z, and there select the edge la-
beled 1, as indicated by n. The edge with d = 1 in Z
is ?d = 1, n = 1?, so we browse to ? where we se-
lect the edge ?d = 1, n = ??. The missing value for
n tells us that the next node, W, is the last element
of the sequence. The complete sequence is then [B,
Z, ?, W], which encodes the fragment (B (Z W)).
The index implementation has been optimized for
fast insertions and has the following features: 1)
each node label is represented exactly once; 2) each
distinct sequence tail is represented exactly once.
The union of all the fragments harvested from each
model is then saved into a dictionary DL which will
be used by the next stage.
To mine the fragments, we apply to each tree in
each model the algorithm shown in Algorithm 3.1.
In this context, we call fragment expansion the pro-
32
Algorithm 3.1: MINE TREE(tree)
global maxdepth,maxexp
main
mined? ?; indexed? ?; MINE(FRAG(tree), 0)
procedure MINE(frag, depth)
if frag ? indexed
then return
indexed? indexed ? {frag}
INDEX(frag)
for each node ? TO EXPAND(frag)
do
?
?
?
if node 6? mined
then
{
mined? mined ? {node}
MINE(FRAG(node), 0)
if depth < maxdepth
then
{for each fragment ? EXPAND(frag,maxexp)
do MINE(fragment, depth+ 1)
cess by which tree nodes are included in a frag-
ment. Fragment expansion is achieved via node ex-
pansions, where expanding a node means includ-
ing its direct children in the fragment. The func-
tion FRAG(n) builds the basic fragment rooted in a
given node n, i.e. the fragment consisting only of n
and its direct children. The function TO EXPAND(f)
returns the set of nodes in a fragment f that can
be expanded (i.e. internal nodes in the origin tree),
while the function EXPAND(f,maxexp) returns all
the possible expansions of a fragment f . The pa-
rameter maxexp is a limit to the number of nodes
that can be expanded at the same time when a new
fragment is generated, while maxdepth sets a limit
on the number of times that a base fragment can be
expanded. The function INDEX(f) adds the frag-
ment f to the index. To keep the notation simple,
here we assume that a fragment f contains all the
necessary information to calculate its relevance (i.e.
the weight, label and norm of the support vector ?i,
yi, and ?ti?, the depth of the fragment `(f) and the
decay factor ?, see equations 2 and 3).
Performing in a different order the same node ex-
pansions on the same fragment f results in the same
fragment f ?. To prevent the algorithm from entering
circular loops, we use the set indexed so that the
very same fragment in each tree cannot be explored
more than once. Similarly, the mined set is used
so that the base fragment rooted in a given node is
considered only once.
Tree Fragment Extraction (TFX) During this
phase, a data file encoding label-tree pairs ?yi, ti? is
S
NP
NNP
Mary
VP
VB
bought
NP
D
a
NN
cat
(A1)
(A0)
?
VP
VB-P
bought
NP
D-B
a
VP
VB-P
bought
NP-B
D
a
NN
cat
-1: BC +1: BC,A1
-1: A0,A2,A3,A4,A5
Figure 3: Examples of ASTm structured features.
transformed to encode label-vector pairs ?yi, ~vi?. To
do so, we generate the fragment space of ti, using
a variant of the mining algorithm described in Fig-
ure 3.1, and encode in ~vi all and only the fragments
ti,j so that ti,j ? DL, i.e. we perform feature extrac-
tion based on the indexed fragments. The process is
applied to the whole training and test sets. The al-
gorithm exploits labels and production rules found
in the fragments listed in the dictionary to generate
only the fragments that may be in the dictionary. For
example, if the dictionary does not contain a frag-
ment whose root is labeled N , then if a node N is
encountered during TFX neither its base fragment
nor its expansions are generated.
Explicit Space Learning (ESL) After linearizing
the training data, we can learn a very fast model by
using all the available data and a linear kernel. The
fragment space is now explicit, as there is a mapping
between the input vectors and the fragments they en-
code.
Explicit Space Classification (ESC) After learn-
ing the linear model, we can classify our linearized
test data and evaluate the accuracy of the resulting
classifier.
4 Previous work
A rather comprehensive overview of feature selec-
tion techniques is carried out in (Guyon and Elis-
seeff, 2003). Non-filter approaches for SVMs and
kernel machines are often concerned with polyno-
mial and Gaussian kernels, e.g. (Weston et al, 2001)
and (Neumann et al, 2005). Weston et al (2003) use
the `0 norm in the SVM optimizer to stress the fea-
ture selection capabilities of the learning algorithm.
In (Kudo and Matsumoto, 2003), an extension of the
PrefixSpan algorithm (Pei et al, 2001) is used to ef-
ficiently mine the features in a low degree polyno-
mial kernel space. The authors discuss an approx-
imation of their method that allows them to handle
high degree polynomial kernels.
33
Data set Non-linearized classifiers Linearized classifiers (Thr=10k)
Task Pos Neg Train Test P R F1 Train Test P R F1
A0 60,900 118,191 521 7 90.26 92.95 91.59 209 3 88.95 91.91 90.40
A1 90,636 88,455 1,206 11 89.45 88.62 89.03 376 3 89.39 88.13 88.76
A2 21,291 157,800 692 7 84.56 64.42 73.13 248 3 81.23 68.29 74.20
A3 3,481 175,610 127 2 97.67 40.00 56.76 114 3 97.56 38.10 54.79
A4 2,713 176,378 47 1 92.68 55.07 69.10 92 2 95.00 55.07 69.72
A5 69 179,022 3 0 100.00 50.00 66.67 63 2 100.00 50.00 66.67
BC 61,062 938,938 3,059 247 82.57 80.96 81.76 916 39 83.36 78.95 81.10
RM - - 2,596 27 89.37 86.00 87.65 1,090 16 88.50 85.81 87.13
Table 1: Accuracy (P, R, F1), training (Train) and test (Test) time of non-linearized (center) and linearized (right)
classifiers. Times are in minutes. For each task, columns Pos and Neg list the number of positive and negative training
examples, respectively. The accuracy of the role multiclassifiers is the micro-average of the individual classifiers
trained to recognize core PropBank roles.
Suzuki and Isozaki (2005) present an embedded
approach to feature selection for convolution ker-
nels based on ?2-driven relevance assessment. To
our knowledge, this is the only published work
clearly focusing on feature selection for tree ker-
nel functions. In (Graf et al, 2004), an approach
to SVM parallelization is presented which is based
on a divide-et-impera strategy to reduce optimiza-
tion time. The idea of using a compact graph rep-
resentation to represent the support vectors of a TK
function is explored in (Aiolli et al, 2006), where a
Direct Acyclic Graph (DAG) is employed.
Concerning the use of kernels for NLP, inter-
esting models and results are described, for exam-
ple, in (Collins and Duffy, 2002), (Moschitti et al,
2008), (Kudo and Matsumoto, 2003), (Cumby and
Roth, 2003), (Shen et al, 2003), (Cancedda et al,
2003), (Culotta and Sorensen, 2004), (Daume? III
and Marcu, 2004), (Kazama and Torisawa, 2005),
(Kudo et al, 2005), (Titov and Henderson, 2006),
(Moschitti et al, 2006), (Moschitti and Bejan, 2004)
or (Toutanova et al, 2004).
5 Experiments
We tested our model on a Semantic Role La-
beling (SRL) benchmark, using PropBank annota-
tions (Palmer et al, 2005) and automatic Charniak
parse trees (Charniak, 2000) as provided for the
CoNLL 2005 evaluation campaign (Carreras and
Ma`rquez, 2005). SRL can be decomposed into
two tasks: boundary detection, where the word se-
quences that are arguments of a predicate word w
are identified, and role classification, where each ar-
gument is assigned the proper role. The former task
requires a binary Boundary Classifier (BC), whereas
the second involves a Role Multi-class Classifier
(RM).
Setup. If the constituency parse tree t of a sen-
tence s is available, we can look at all the pairs
?p, ni?, where ni is any node in the tree and p is
the node dominating w, and decide whether ni is an
argument node or not, i.e. whether it exactly dom-
inates all and only the words encoding any of w?s
arguments. The objects that we classify are sub-
sets of the input parse tree that encompass both p
and ni. Namely, we use the ASTm structure defined
in (Moschitti et al, 2008), which is the minimal tree
that covers all and only the words of p and ni. In
the ASTm, p and ni are marked so that they can be
distinguished from the other nodes. An ASTm is
regarded as a positive example for BC if ni is an ar-
gument node, otherwise it is considered a negative
example. Positive BC examples can be used to train
an efficient RM: for each role r we can train a clas-
sifier whose positive examples are argument nodes
whose label is exactly r, whereas negative examples
are argument nodes labeled r? 6= r. Two ASTms
extracted from an example parse tree are shown in
Figure 3: the first structure is a negative example for
BC and is not part of the data set of RM, whereas
the second is a positive instance for BC and A1.
To train BC we used PropBank sections 1 through
6, extracting ASTm structures out of the first 1 mil-
lion ?p, ni? pairs from the corresponding parse trees.
As a test set we used the 149,140 instance collected
from the annotations in Section 24. There are 61,062
positive examples in the training set (i.e. 6.1%) and
8,515 in the test set (i.e. 5.7%).
For RM we considered all the argument nodes of
any of the six PropBank core roles (i.e. A0, . . . ,
34
1k 2k 5k 10k 20k30k 50k 100k
0
200
400
600
800
1,000
1,200
929 916 1,037
1,104
Threshold (log10)
Le
arn
ing
tim
e(
mi
nu
tes
)
Overall TFX ESL
FMI FSL
Figure 4: Training time decomposition for the linearized
BC with respect to its main components when varying the
threshold value.
A5) from all the available training sections, i.e. 2
through 21, for a total of 179,091 training instances.
Similarly, we collected 5,928 test instances from the
annotations of Section 24.
In the remainder, we will mark with an ` the lin-
earized classifiers, i.e. BC` and RM` will refer to
the linearized boundary and role classifiers, respec-
tively. Their traditional, vanilla SST counterparts
will be simply referred to as BC and RM.
We used 10 splits for the FMI stage and we set
maxdepth = 4 and maxexp = 5 during FMI and
TFX. We didn?t carry out an extensive validation of
these parameters. These values were selected dur-
ing the development of the software because, on a
very small development set, they resulted in a very
responsive system.
Since the main topic of this paper is the assess-
ment of the efficiency and accuracy of our lineariza-
tion technique, we did not carry out an evaluation
on the whole SRL task using the official CoNLL?05
evaluator. Indeed, producing complete annotations
requires several steps (e.g. overlap resolution, OvA
or Pairwise combination of individual role classi-
fiers) that would shade off the actual impact of the
methodology on classification.
Platform. All the experiments were run on a ma-
chine equipped with 4 Intel R? Xeon R? CPUs clocked
at 1.6 GHz and 4 GB of RAM running on a Linux
2.6.9 kernel. As a supervised learning framework
we used SVM-Light-TK 1, which extends the SVM-
Light optimizer (Joachims, 2000) with tree kernel
1http://disi.unitn.it/?moschitt/Tree-Kernel.htm
1k 2k 5k 10k 20k30k 50k 100k
72
74
76
78
80
82
84
Threshold (log10)
Ac
cu
rac
y
BC` Prec BC Prec
BC` Rec BC Rec
BC` F1 BC F1
Figure 5: BC` accuracy for different thresholds.
support. During FSL, we learn the models using a
normalized SST kernel and the default decay factor
? = 0.4. The same parameters are used to train
the models of the non linearized classifiers. During
ESL, the classifier is trained using a linear kernel.
We did not carry out further parametrization of the
learning algorithm.
Results. The left side of Table 1 shows the distri-
bution of positive (Column Pos) and negative (Neg)
data points in each classifier?s training set. The cen-
tral group of columns lists training and test effi-
ciency and accuracy of BC and RM, i.e. the non-
linearized classifiers, along with figures for the indi-
vidual role classifiers that make up RM.
Training BC took more than two days of CPU
time and testing about 4 hours. The classifier
achieves an F1 measure of 81.76, with a good bal-
ance between precision and recall. Concerning RM,
sequential training of the 6 models took 2,596 min-
utes, while classification took 27 minutes. The slow-
est of the individual role classifiers happens to be
A1, which has an almost 1:1 ratio between posi-
tive and negative examples, i.e. they are 90,636 and
88,455 respectively.
We varied the threshold value (i.e. the number of
fragments that we mine from each model, see Sec-
tion 3) to measure its effect on the resulting classi-
fier accuracy and efficiency. In this context, we call
training time all the time necessary to obtain a lin-
earized model, i.e. the sum of FSL, FMI and TFX
time for every split, plus the time for ESL. Similarly,
we call test time the time necessary to classify a lin-
earized test set, i.e. the sum of TFX and ESC on test
data.
In Figure 4 we plot the efficiency of BC` learn-
35
ing with respect to different threshold values. The
Overall training time is shown alongside with par-
tial times coming from FSL (which is the same for
every threshold value and amounts to 433 minutes),
FMI, training data TFX and ESL. The plot shows
that TFX has a logarithmic behaviour, and that quite
soon becomes the main player in total training time
after FSL. For threshold values lower than 10k, ESL
time decreases as the threshold increases: too few
fragments are available and adding new ones in-
creases the probability of including relevant frag-
ments in the dictionary. After 10k, all the relevant
fragments are already there and adding more only
makes computation harder. We can see that for a
threshold value of 100k total training time amounts
to 1,104 minutes, i.e. 36% of BC. For a threshold
value of 10k, learning time further decreases to 916
minutes, i.e. less than 30%. This threshold value
was used to train the individual linearized role clas-
sifiers that make up RM`.
These considerations are backed by the trend of
classification accuracy shown in Figure 5, where the
Precision, Recall and F1 measure of BC`, evaluated
on the test set, are shown in comparison with BC.
We can see that BC` precision is almost constant,
while its recall increases as we increase the thresh-
old, reaches a maximum of 78.95% for a threshold
of 10k and then settles around 78.8%. The F1 score
is maximized for a threshold of 10k, where it mea-
sures 81.10, i.e. just 0.66 points less than BC. We
can also see that BC` is constantly more conserva-
tive than BC, i.e. it always has higher precision and
lower recall.
Table 1 compares side to side the accuracy
(columns P, R and F1), training (Train) and test
(Test) times of the different classifiers (central block
of columns) and their linearized counterparts (block
on the right). Times are measured in minutes. For
the linearized classifiers, test time is the sum of
TFX and ESC time, but the only relevant contribu-
tion comes from TFX, as the low dimensional linear
space and fast linear kernel allow us to classify test
instances very efficiently 2. Overall, BC` test time is
39 minutes, which is more than 6 times faster than
BC (i.e. 247 minutes). It should be stressed that we
2Although ESC is not shown in table, the classification of all
149k test instances with BC` took 5 seconds with a threshold of
1k and 17 seconds with a threshold of 100k.
Learning parallelization
Task Non Lin. Linearized (Thr=10k)1 cpu 5 cpus 10 cpus
BC 3,059 916 293 215
RM 2,596 1,090 297 198
Table 2: Learning time when exploiting the framework?s
parallelization capabilities. Column Non Lin. lists non-
linearized training time.
are comparing against a fast TK implementation that
is almost linear in time with respect to the number of
tree nodes (Moschitti, 2006).
Concerning RM`, we can see that the accuracy
loss is even less than with BC`, i.e. it reaches an F1
measure of 87.13 which is just 0.52 less than RM.
It is also interesting to note how the individual lin-
earized role classifiers manage to perform accurately
regardless of the distribution of examples in the data
set: for all the six classifiers the final accuracy is
in line with that of the corresponding non-linearized
classifier. In two cases, i.e. A2 and A4, the accuracy
of the linearized classifier is even higher, i.e. 74.20
vs. 73.13 and 69.72 vs. 69.10, respectively. As for
the efficiency, total training time for RM` is 37% of
RM, i.e. 1,190 vs. 2,596 minutes, while test time
is reduced to 60%, i.e. 16 vs 27 minutes. These
improvements are less evident than those measured
for boundary detection. The main reason is that
the training set for boundary classification is much
larger, i.e. 1 million vs. 179k instances: therefore,
splitting training data during FSL has a reduced im-
pact on the overall efficiency of RM`.
Parallelization. All the efficiency improvements
that have been discussed so far considered a com-
pletely sequential process. But one of the advan-
tages of our approach is that it allows us to paral-
lelize some aspect of SVM training. Indeed, every
activity (but ESL) can exploit some degree of par-
allelism: during FSL, all the models can be learnt
at the same time (for this activity, the maximum de-
gree of parallelization is conditioned by the number
of training data splits); during FMI, models can be
mined concurrently; during TFX, the data-set to be
linearized can be split arbitrarily and individual seg-
ments can be processed in parallel. Exploiting this
possibility we can drastically improve learning ef-
ficiency. As an example, in Table 2 we show how
the total learning of the BC` can be cut to as low as
215 seconds when exploiting ten CPUs and using a
36
1 2 3 4 5 6 7 8 9 10
20
40
60
80
100
Models
Cu
mu
lat
ive
co
ntr
ibu
tio
n(
%)
1k 5k 10k
50k 100k
Figure 6: Growth of dictionary size when including frag-
ments from more splits at different threshold values.
When a low threshold is used, the contribution of indi-
vidual dictionaries tends to be more marginal.
threshold of 10k. Even running on just 5 CPUs, the
overall computational cost of BC` is less than 10%
of BC (Column Non Lin.). Similar considerations
can be drawn concerning the role multi-classifier.
Fragment space. In this section we take a look at
the fragments included in the dictionary of the BC`
classifier. During FMI, we incrementally merge the
fragments mined from each of the models learnt dur-
ing FSL. Figure 6 plots, for different threshold val-
ues, the percentage of new fragments (on the y axis)
that the i-th model (on the x axis) contributes with
respect to the number of fragments mined from each
model (i.e. the threshold value).
If we consider the curve for a threshold equal to
100k, we can see that each model after the first ap-
proximately contributes with the same number of
fragments. On the other hand, if the threshold is set
to 1k than the contribution of subsequent models is
increasingly more marginal. Eventually, less than
10% of the fragments mined from the last model are
new ones. This behaviour suggests that there is a
core set of very relevant fragments which is com-
mon across models learnt on different data, i.e. they
are relevant for the task and do not strictly depend
on the training data that we use. When we increase
the threshold value, the new fragments that we index
are more and more data specific.
The dictionary compiled with a threshold of 10k
lists 62,760 distinct fragments. 15% of the frag-
ments contain the predicate node (which generally
is the node encoding the predicate word?s POS tag),
more than one third contain the candidate argument
node and, of these, about one third are rooted in it.
This last figure strongly suggests that the internal
structure of an argument is indeed a very powerful
feature not only for role classification, as we would
expect, but also for boundary detection. About 10%
of the fragments contain both the predicate and the
argument node, while about 1% encode the Path fea-
ture traditionally used in explicit semantic role label-
ing models (Gildea and Jurafsky, 2002). About 5%
encode a sort of extended Path feature, where the ar-
gument node is represented together with its descen-
dants. Overall, about 2/3 of the fragments contain at
least some terminal symbol (i.e. words), generally a
preposition or an adverb.
6 Conclusions
We presented a supervised learning framework for
Support Vector Machines that tries to combine the
power and modeling simplicity of convolution ker-
nels with the advantages of linear kernels and ex-
plicit feature representations. We tested our model
on a Semantic Role Labeling benchmark and ob-
tained very promising results in terms of accuracy
and efficiency. Indeed, our linearized classifiers
manage to be almost as accurate as non linearized
ones, while drastically reducing the time required to
train and test a model on the same amounts of data.
To our best knowledge, the main points of nov-
elty of this work are the following: 1) it addresses
the problem of feature selection for tree kernels, ex-
ploiting SVM decisions to guide the process; 2) it
provides an effective way to make the kernel space
observable; 3) it can efficiently linearize structured
data without the need for an explicit mapping; 4) it
combines feature selection and SVM parallelization.
We began investigating the fragments generated
by a TK function for SRL, and believe that study-
ing them in more depth will be useful to identify
new, relevant features for the characterization of
predicate-argument relations.
In the months to come, we plan to run a set of ex-
periments on a wider list of tasks so as to consolidate
the results we obtained so far. We will also test the
generality of the approach by testing with different
high-dimensional kernel families, such as sequence
and polynomial kernels.
37
References
Fabio Aiolli, Giovanni Da San Martino, Alessandro Sper-
duti, and Alessandro Moschitti. 2006. Fast on-line
kernel learning for trees. In Proceedings of ICDM?06.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL?05.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceedings
of ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28:245?288.
Hans P. Graf, Eric Cosatto, Leon Bottou, Igor Dur-
danovic, and Vladimir Vapnik. 2004. Parallel support
vector machines: The cascade svm. In Neural Infor-
mation Processing Systems.
Isabelle Guyon and Andre? Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157?1182.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, Dept. of Computer Sci-
ence, University of California at Santa Cruz.
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proceedings of
ICML?00.
Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for
semi-structured data. In Proceedings of ICML?02.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding
up training with tree kernels for node relation labeling.
In Proceedings of HLT-EMNLP?05.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Alessandro Moschitti and Cosmin Bejan. 2004. A se-
mantic kernel for predicate argument classification. In
CoNLL-2004, Boston, MA, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree ker-
nel joint inference. In Proceedings of CoNLL-X, New
York City.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proccedings of
EACL?06.
Julia Neumann, Christoph Schnorr, and Gabriele Steidl.
2005. Combined SVM-Based Feature Selection and
Classification. Machine Learning, 61(1-3):129?150.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Comput. Linguist., 31(1):71?106.
J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U.
Dayal, and M. C. Hsu. 2001. PrefixSpan Mining Se-
quential Patterns Efficiently by Prefix Projected Pat-
tern Growth. In Proceedings of ICDE?01.
Alain Rakotomamonjy. 2003. Variable selection using
SVM based criteria. Journal of Machine Learning Re-
search, 3:1357?1370.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Proceedings of EMNLP?06.
Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree
Kernels with Statistical Feature Mining. In Proceed-
ings of the 19th Annual Conference on Neural Infor-
mation Processing Systems (NIPS?05).
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience.
Jason Weston, Sayan Mukherjee, Olivier Chapelle, Mas-
similiano Pontil, Tomaso Poggio, and Vladimir Vap-
nik. 2001. Feature Selection for SVMs. In Proceed-
ings of NIPS?01.
Jason Weston, Andre? Elisseeff, Bernhard Scho?lkopf, and
Mike Tipping. 2003. Use of the zero norm with lin-
ear models and kernel methods. J. Mach. Learn. Res.,
3:1439?1461.
Mohammed J Zaki. 2002. Efficiently mining frequent
trees in a forest. In Proceedings of KDD?02.
38
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 219?227,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
New Features for FrameNet ? WordNet Mapping
Sara Tonelli and Daniele Pighin
FBK-Irst, Human Language Technologies
Via di Sommarive, 18 I-38100 Povo (TN) Italy
{satonelli,pighin}@fbk.eu
Abstract
Many applications in the context of natural
language processing or information retrieval
may be largely improved if they were able to
fully exploit the rich semantic information an-
notated in high-quality, publicly available re-
sources such as the FrameNet and the Word-
Net databases. Nevertheless, the practical use
of similar resources is often biased by the
limited coverage of semantic phenomena that
they provide.
A natural solution to this problem would be to
automatically establish anchors between these
resources that would allow us 1) to jointly use
the encoded information, thus possibly over-
coming limitations of the individual corpora,
and 2) to extend each resource coverage by ex-
ploiting the information encoded in the others.
In this paper, we present a supervised learn-
ing framework for the mapping of FrameNet
lexical units onto WordNet synsets based on
a reduced set of novel and semantically rich
features. The automatically learnt mapping,
which we call MapNet, can be used 1) to ex-
tend frame sets in the English FrameNet, 2)
to populate frame sets in the Italian FrameNet
via MultiWordNet and 3) to add frame labels
to the MultiSemCor corpus. Our evaluation on
these tasks shows that the proposed approach
is viable and can result in accurate automatic
annotations.
1 Introduction
In recent years, the integration of manually-built
lexical resources into NLP systems has received
growing interest. In particular, resources annotated
with the surface realization of semantic roles, like
FrameNet (Baker et al, 1998) or PropBank (Palmer
et al, 2005) have shown to convey an improve-
ment in several NLP tasks, from question answer-
ing (Shen and Lapata, 2007) to textual entailment
(Burchardt et al, 2007) and shallow semantic pars-
ing (Giuglea and Moschitti, 2006). Nonetheless, the
main limitation of such resources is their poor cov-
erage, particularly as regards FrameNet. Indeed, the
latest FrameNet release (v. 1.3) contains 10,195 lex-
ical units (LUs), 3,380 of which are described only
by a lexicographic definition without any example
sentence. In order to cope with this lack of data, it
would be useful to map frame information onto other
lexical resources with a broader coverage. We be-
lieve that WordNet (Fellbaum, 1998), with 210,000
entries in version 3.0, can represent a suitable re-
source for this task. In fact, both FrameNet and
WordNet group together semantically similar words,
and provide a hierarchical representation of the lex-
ical knowledge (in WordNet the relations between
synsets, in FrameNet between frames, see Ruppen-
hofer et al (2006)). On the other hand, WordNet
provides a more extensive coverage particularly for
adjectives and nouns denoting artifacts and natural
kinds, that are mostly neglected in FrameNet.
In this paper, we present an approach using Sup-
port Vector Machines (SVM) to map FrameNet lex-
ical units to WordNet synsets. The proposed ap-
proach addresses some of the limitations of previous
works on the same task (see for example De Cao
et al (2008) and Johansson and Nugues (2007)).
Most notably, as we do not train the SVM on a per-
219
frame basis, our model is able to cope also with
those frames that have little or no annotated sen-
tences to support the frame description. After learn-
ing a very fast model on a small set of annotated
lexical unit-synset pairs, we can automatically es-
tablish new mappings in never-seen-before pairs and
use them for our applications. We will evaluate the
effect of the induced mappings on two tasks: the au-
tomatic enrichment of lexical unit sets in the English
and Italian FrameNet via MultiWordNet (Pianta et
al., 2002), and the annotation of the MultiSemCor
corpus (Bentivogli and Pianta, 2005) with frame la-
bels.
The discussion is structured as follows: in
Section 2 we review the main characteristics of
FrameNet and WordNet; in Section 3 we discuss
previous attempts to establish a mapping between
them; in Section 4 we describe our supervised ap-
proach to map lexical units onto synsets; Section 5
details the dataset that we employed for our experi-
ments; Section 6 describes the novel features that we
used to characterize the mapping; Section 7 details
the results of our experiments; in Section 8 we ap-
ply the mapping to three resource annotation tasks;
finally, in Section 9 we draw our conclusions.
2 FrameNet and WordNet
The FrameNet database (Baker et al (1998), Fill-
more et al (2003)) is an English lexical resource
based on the description of some prototypical sit-
uations, the frames, and the frame-evoking words
or expressions associated to them, the lexical units
(LU). Every frame corresponds to a scenario involv-
ing a set of participants, the frame elements (FEs),
that are typically the semantic arguments shared by
all LUs in a frame.
We report in Table 1 the information recorded
in FrameNet for the CAUSE TO WAKE frame. In
the first row there is the frame definition with the
relevant frame elements, namely AGENT, CAUSE,
SLEEPER and SLEEP STATE. Then there is the list
of all lexical units evoking the frame and the corre-
sponding part of speech. Note that, differently from
WordNet synsets, a frame can contain LUs with dif-
ferent PoS as well as antonymous words. In the
last row, an example for each frame element is re-
ported. The lexical unit is underlined, while the con-
Frame: CAUSE TO WAKE
De
f. An AGENT or CAUSE causes a SLEEPER totransition from the SLEEP STATE to wakeful
consciousness.
LU
s awaken.v, get up.v, rouse.v, wake.v, wake up.v
singe.v, sizzle.v, stew.v
FE
s AGENT We tried to rouse Peter.
CAUSE The rain woke the children.
SLEEPER Neighbors were awakened by screams.
SL STATE He woke Constance from her doze.
Table 1: Frame CAUSE TO WAKE
stituent bearing the FE label is written in italics. The
FrameNet resource is corpus-based, i.e. every lexi-
cal unit should be instantiated by at least one ex-
ample sentence. Besides, every lexical unit comes
with a manual lexicographic definition. The latest
database release contains 795 frame definitions and
10,195 lexical units, instantiated through approxi-
mately 140.000 example sentences. Despite this, the
database shows coverage problems when exploited
for NLP tasks, and is still being extended by the
Berkeley group at ICSI.
WordNet (Fellbaum, 1998) is a lexical resource
for English based on psycholinguistics principles
and developed at Princeton University. It has been
conceived as a computational resource aimed at im-
proving some drawbacks of traditional dictionaries
such as the circularity of definitions and the ambigu-
ity of sense references. At present, it covers the ma-
jority of nouns, verbs, adjectives and adverbs in the
English language, organized in synonym sets called
synsets, which correspond to concepts. WordNet
also includes a rich set of semantic relations across
concepts, such as hyponymy, entailment, antonymy,
similar-to, etc. Each synset is encoded as a set of
synomyms having the same part of speech and de-
scribed by a definition or gloss. In some cases, one
or more example sentences may also be reported.
The Princeton English WordNet has also been aug-
mented with domain labels (Magnini and Cavaglia`,
2000) that group synsets into homogeneous clusters
in order to reduce polysemy in the database.
We believe that mapping FrameNet LUs to Word-
Net synsets would have at least three different ad-
vantages: 1) for the English FrameNet, it would au-
tomatically increase the number of LUs for frame by
220
importing all synonyms from the mapped synset(s),
and would allow to exploit the semantic and lex-
ical relations in WordNet to enrich the informa-
tion encoded in FrameNet. This would help cop-
ing with coverage problems and disambiguating the
LU senses. 2) For WordNet, it would be possible
to add a semantic layer between the synset level
and the domain level represented by frame rela-
tions, and to enrich the synsets with a computa-
tional description of the situation they refer to to-
gether with the semantic roles involved. 3) Since
frames are mostly defined at conceptual level, the
FrameNet model is particularly suitable for cross-
lingual induction (Boas, 2005). In this framework,
the FrameNet-WordNet mapping could help mod-
elling frame-based resources for new languages us-
ing minimal supervision. In fact, the availability of
multilingual resources like MultiWordNet (Pianta et
al., 2002) and EuroWordNet (Vossen, 1998) allows
to easily populate frame sets for new languages with
reduced human effort and near-manual quality by
importing all lemmas from the mapped synsets.
3 Related work
Several experiments have been carried out to de-
velop a FrameNet-WordNet mapping and test its
applications. Shi and Mihalcea (2005) described
a semi-automatic approach to exploit VerbNet as a
bridge between FrameNet and WordNet for verbs,
using synonym and hyponym relations and simi-
larity between Levin?s verb classes and FrameNet
frames. Their mapping was used to develop a rule-
based semantic parser (Shi and Mihalcea, 2004) as
well as to detect target words and assign frames for
verbs in an open text (Honnibal and Hawker, 2005).
Burchardt et al (2005) presented a rule-based
system for the assignment of FrameNet frames by
way of a ?detour via WordNet?. They applied
a WordNet-based WSD system to annotate lexical
units in unseen texts with their contextually de-
termined WordNet synsets and then exploited syn-
onyms and hypernyms information to assign the best
frame to the lexical units. The system was inte-
grated into the SALSA RTE system for textual en-
tailment (Burchardt et al, 2007) to cope with sparse-
data problems in the automatic assignment of frame
labels.
Johansson and Nugues (2007) created a feature
representation for every WordNet lemma and used
it to train an SVM classifier for each frame that tells
whether a lemma belongs to the frame or not. The
best-performing feature representation was built us-
ing the sequence of unique identifiers for each synset
in its hypernym tree and weigthing the synsets ac-
cording to their relative frequency in the SemCor
corpus. They used the mapping in the Semeval-2007
task on frame-semantic structure extraction (Baker
et al, 2007) in order to find target words in open
text and assign frames.
Crespo and Buitelaar (2008) carried out an auto-
matic mapping of medical-oriented frames to Word-
Net synsets applying a Statistical Hypothesis Test-
ing to select synsets attached to a lexical unit that
were statistically significant using a given refer-
ence corpus. The mapping obtained was used
to expand Spanish FrameNet using EuroWordNet
(Vossen, 1998) and evaluation was carried out on the
Spanish lexical units obtained after mapping.
Given a set of lexical units, De Cao et al (2008)
propose a method to detect the set of suitable Word-
Net senses able to evoke a frame by applying a simi-
larity function that exploits different WordNet infor-
mation, namely conceptual density for nouns, syn-
onymy and co-hyponymy for verbs and synonymy
for adjectives. The mapping approach was applied
also to LU induction for the English FrameNet and
for Italian frames via MultiWordNet.
4 Problem formulation
Our objective is to be able to assign to every lex-
ical unit l, belonging to a frame Fi defined in the
FrameNet database, one or more WordNet senses
that best express the meaning of l. More specifically,
for every l ? Fi, we consider the set of all WordNet
senses where l appears, CandSet, and then find the
best WordNet sense(s) bests ? CandSet that express
the meaning of l.
For example, the lexical unit rouse.v belonging to
the CAUSE TO WAKE frame, is defined in FrameNet
as ?bring out of sleep; awaken?. Its CandSet com-
prises 4 senses1: 1# bestir, rouse (become active);
2# rout out, drive out, force out, rouse (force or
drive out); #3 agitate, rouse, turn on, charge, com-
1The gloss is reported between parenthesis
221
move, excite, charge up (cause to be agitated, ex-
cited or roused); #4 awaken, wake, waken, rouse,
wake up, arouse (cause to become awake or con-
scious). In this example, bests = {#4} for rouse.v
in CAUSE TO WAKE.
We aim at creating a mapping system that
can achieve a good accuracy also with poorly-
documented lexical units and frames. In fact, we be-
lieve that under real-usage conditions, the automatic
induction of LUs is typically required for frames
with a smaller LU set, especially for those with only
one element. In the FrameNet database (v. 1.3), 33
frames out of 720 are described only by one lex-
ical unit, and 63 are described by two. Further-
more, more than 3,000 lexical units are character-
ized only by the lexicographic definition and are
not provided with example sentences. For this rea-
son, we suggest an approach that makes also use
of usually unexploited information in the FrameNet
database, namely the definition associated to every
lexical unit, and disregards example sentences.
This is the main point of difference between
our and some previous works, e.g. Johansson and
Nugues (2007) and De Cao et al (2008), where un-
supervised approaches are proposed which strongly
rely either on the number of lexical units in a frame
or on the example sentences available for l in the
FrameNet corpus. We claim that the relative short
time necessary to annotate a small dataset of frame-
synset pairs will result in a more reliable mapping
system and, as a consequence, in consistent time
savings when we actually try to use the mappings
for some tasks. The ability to cope with different
cases while retaining a good accuracy will allow to
bootstrap the mapping process in many cases where
other approaches would have failed due to lack of
training data.
To this end, we can train a binary classifier
that, given l and CandSet, for each pair ?l, s?,
s ? CandSet, delivers a positive answer if s ?
bests, and a negative one otherwise. To follow on
the previous example, for rouse.v we would have
4 classifier examples, i.e. the pairs ?rouse.v,#1?,
?rouse.v,#2?, ?rouse.v,#3? and ?rouse.v,#4?. Of
these, only the last would be considered a positive
instance. As a learning framework, we decided to
use SVMs due to their classification accuracy and
robustness to noisy data (Vapnik, 1998).
5 Dataset description
In order to train and test the classifier, we created
a gold standard by manually annotating 2,158 LU-
synset pairs as positive or negative examples. We
don?t have data about inter-annotator agreement be-
cause the dataset was developed only by one annota-
tor, but De Cao et al (2008) report 0.90 as Cohen?s
Kappa computed over 192 LU-synset pairs for the
same mapping task. This confirms that senses and
lexical units are highly correlated and that the map-
ping is semantically motivated.
The annotation process can be carried out in rea-
sonable time. It took approximately two work days
to an expert annotator to manually annotate the
2,158 pairs that make up our gold standard. The lexi-
cal units were randomly selected from the FrameNet
database regardless of their part of speech or amount
of annotated data in the FrameNet database. For
each lexical unit, we extracted from WordNet the
synsets where the LU appears, and for each of them
we assigned a positive label in case the LU-synset
pairs share the same meaning, and a negative label
otherwise. Statistics about the dataset are reported
in Table 2.
N. of LU-synset pairs 2,158
N. of lexical units 617
Verbal lexical units 39%
Nominal lexical units 51%
Adjectival lexical units 9%
Adverbial lexical units <1%
Targeted frames 386
Pairs annotated as positive 32%
Pairs annotated as negative 68%
Average polysemy 3.49
LUs with one candidate synset 204
LUs with 10 or more cand. synsets 32
Table 2: Statistics on the dataset
The 386 frames that are present in the dataset rep-
resent about one half of all lexicalized frames in
the FrameNet database. This proves that, despite
the limited size of the dataset, it is well representa-
tive of FrameNet characteristics. This is confirmed
by the distribution of the part of speech. In fact,
in the FrameNet database about 41% of the LUs
222
are nouns, 40% are verbs, 17% are adjectives and
<1% are adverbs (the rest are prepositions, which
are not included in our experiment because they are
not present in WordNet). In our dataset, the per-
centage of nouns is higher, but the PoS ranking by
frequency is the same, with nouns being the most
frequent PoS and adverbs the less represented. The
average polysemy corresponds to the average num-
ber of candidate synsets for every LU in the dataset.
Note that the high number of lexical units with only
one candidate does not imply a more straightforward
mapping, because in some cases the only candidate
represents a negative example. In fact, a LU could
be encoded in a frame that does not correspond to
the sense expressed by the synset.
6 Feature description
For every LU-synset pair in the gold standard, we
extracted a set of features that characterize different
aspects of the mapping. In the remainder, we detail
the meaning as well as the feature extraction proce-
dure of each of them.
Stem overlap Both WordNet glosses and LU def-
initions in FrameNet are manually written by lex-
icographers. We noticed that when they share the
same sense, they show high similarity, and some-
times are even identical. For example, the defini-
tion of thicken in the Change of consistency frame
is ?become thick or thicker?, which is identical to
the WordNet gloss of synset n. v#00300319. The
thicken lemma occurs in three WordNet synsets, and
in each of them it is the only lemma available, so no
other information could be exploited for the sense
disambiguation.
We believe that this information could help in the
choice of the best candidate synset, so we stemmed
all the words in the synset gloss and in the lexical
unit definition and measured their overlap. As fea-
tures, we use the ratio between the number of over-
lapping words and the number of words in the defi-
nition, both for the gloss and the LU description.
Prevalent Domain and Synset Since a frame rep-
resents a prototypical situation evoked by the set
of its lexical units, our intuition is that it should
be possible to assign it to a WordNet domain, that
groups homogeneous clusters of semantically simi-
lar synsets (see Section 2).
Given the LU-synset pair ?l, s?, l ? Fi, s ?
CandSet, we extract all the lexical units in Fi and
then build a set AllCandSet of pairs ?sj , cj?, where
sj is a synset in which at least one li ? Fi appears,
and cj is the count of lexical units that are found in
sj .
We exploit the information conveyed by AllCan-
dSet in two ways: i) if there is a prevalent Word-
Net domain that characterizes the majority of the
synsets in AllCandSet, and s ? CandSet belongs
to that same domain, we add a boolean feature to
the feature vector representing ?l, s?; ii) if s is the
synset with the highest count in AllCandSet, i.e. if
s = sj and cj > ci??sj , cj? ? AllCandSet, i 6= j,
then we add another boolean feature to encode this
information.
Cross-lingual parallelism Our idea is that, if an
English lexical unit and its Italian translation belong
to the same frame, they are likely to appear also in
the same MultiWordNet synset, and the latter would
be a good candidate for mapping. In fact, in Multi-
WordNet the Italian WordNet is strictly aligned with
the Princeton WordNet 1.6, with synsets having the
same id for both languages, and also semantic re-
lations are preserved in the multilingual hierarchy.
Since no Italian FrameNet is available yet, we ex-
tended the parallel English-Italian corpus annotated
on both sides with frame information described in
Tonelli and Pianta (2008) by adding and annotating
400 new parallel sentences. The final corpus con-
tains about 1,000 pairs of parallel sentences where
the English and the Italian lexical unit belong to the
same frame.
Given a pair ?l, s?, we check if l appears also in
the corpus with the frame label Fi and extract its
Italian translation lit. If lit appears also in the Italian
version of synset s in MultiWordNet, we consider s
as a good candidate for the mapping of l and encode
this information as a binary feature.
Simple synset-frame overlap Intuitively, the
more lemmas a frame and a synset have in common,
the more semantically similar they are. In order to
take into account this similarity in our feature vec-
tor, given the pair ?l, s?, l ? Fi, we extract all lexical
units in Fi and all lemmas in s and we compute the
number of overlapping elements. Then we divide
223
the value by the number of synsets where the same
overlapping element(s) occur.
As an example, the words tank and tank car in
the Vehicle frame, occur together only in the fourth
synset related to tank, which therefore will have a
higher value for this feature.
Extended synset-frame overlap This feature is a
generalization of overlapping value described above.
In fact, we noticed that the hypernym information
in WordNet can help disambiguating the synsets.
Therefore, we take into account not only the over-
laps according to the previous criterion, but also the
number of overlapping words between the lexical
units in a frame and the hypernyms of a synset. For
example, the party.n lexical unit in the AGGREGATE
frame has 5 senses in WordNet. According to the
previous criterion, there is no overlap between the
LUs in the frame and the lemmas in any of the five
synsets. Instead, if we look at the direct hypernym
relation of party, we find that sense #3 is also de-
scribed as set, circle, band, that are also lexical units
of AGGREGATE.
In those cases where the hypernym relation is not
defined, e.g. adjectives, we used the similar-to rela-
tion.
7 Experimental setup and evaluation
To evaluate our methodology we carried out a 10-
fold cross validation using the available data, split-
ting them in 10 non-overlapping sets. For each itera-
tion, 70% of the data was used for training, 30% for
testing. All the splits were generated so as to main-
tain a balance between positive and negative exam-
ples in the training and test sets.
We used the SVM optimizer SVM-
Light2 (Joachims, 1999), and applied polynomial
kernels (poly) of different degrees (i.e. 1 through
4) in order to select the configuration with the best
generalization capabilities. The accuracy is mea-
sured in terms of Precision, Recall and F1 measure,
i.e. the harmonic average between Precision and
Recall. For the sake of annotation, it is important
that an automatic system be very precise, thus not
producing wrong annotations. On the other hand,
the higher the recall, the larger the amount of data
that the system will be able to annotate.
2Available at http://svmlight.joachims.org/
The macro-average of the classifier accuracy for
the different configurations is shown in Table 3. We
report results for linear kernel (i.e. poly 1), maxi-
mizing recall and f-measure, and for polynomial ker-
nel of degree 2 (i.e. poly 2), scoring the highest pre-
cision. In general , we notice that all our models
have a higher precision than recall, but overall are
quite balanced. Different polynomial kernels (i.e.
conjunction of features) do not produce very rele-
vant differences in the results, suggesting that the
features that we employed encode significant infor-
mation and have a relevance if considered indepen-
dently.
As a comparison, we also carried out the same
evaluation by setting a manual threshold and con-
sidering a LU-synset pair as a positive example if
the sum of the feature values was above the thresh-
old. We chose two different threshold values, the
first (Row 1 in Table 3) selected so as to have com-
parable precision with the most precise SVM model
(i.e. poly2), the second (Row 2) selected to have
recall comparable with poly1, i.e. the SVM model
with highest recall. In the former case, the model has
a recall that is less than half than poly2, i.e. 0.214
vs. 0.569, meaning that such model would establish
a half of the mappings while making the same per-
centage of mistakes. In the latter, the precision of
the SVM classifier is 0.114 points higher, i.e. 0.794
vs. 0.680, meaning the SVM can retrieve as many
mappings but making 15% less errors.
In order to investigate the impact of different fea-
tures on the classifier performance, we also consid-
ered three different groups of features separately:
the ones based on stem overlap, those computed
for prevalent domain and synset, and the features
for simple and extended frame ? synset overlap.
We did not take into account cross-lingual paral-
lelism because it is one single feature whose cover-
age strongly relies on the parallel corpus available.
As a consequence, it is not possible to test the fea-
ture in isolation due to data sparseness.
Results are shown in Table 3, in the second group
of rows. Also in this case, we carried out a 10-
fold cross validation using a polynomial kernel of
degree 2. The stem overlap features, which to our
best knowledge are an original contribution of our
approach, score the highest recall among the three
groups. This confirms our intuition that LU defini-
224
tions and WordNet glosses can help extending the
number of mapped LUs, including those that are
poorly annotated. For instance, if we consider the
KNOT CREATION frame, having only tie.v as LU,
the features about prevalent domain & synset and
about synset-frame overlap would hardly be infor-
mative, while stem overlap generally achieves a con-
sistent performance regardless of the LU set. In
fact, tie.v is correctly mapped to synset v#00095054
based on their similar definition (respectively ?to
form a knot? and ?form a knot or bow in?). Best
precision was scored by the feature group consider-
ing prevalent domain & synset, which are also new
features introduced by our approach. The positive
effect of combining all features is clearly shown by
comparing the results obtained with individual fea-
ture groups against the figures in the row labeled
poly2.
Prec. Recall F1
Man. thresh. (P) 0.789 0.214 0.337
Man. thresh. (F1) 0.680 0.662 0.671
Stem Overlap 0.679 0.487 0.567
Prev.Dom.& Syn. 0.756 0.434 0.551
Syn.- Frame Overlap 0.717 0.388 0.504
poly1 0.761 0.613 0.679
poly2 0.794 0.569 0.663
Table 3: Mapping evaluation
8 MapNet and its applications
Since we aim at assigning at least one synset to ev-
ery lexical unit in FrameNet, we considered all the
frames and for every LU in the database we created
a list of LU-synset pairs. We re-trained the clas-
sifier using the whole annotated gold standard and
classified all the candidate pairs. The mapping pro-
duced between the two resources, that we call Map-
Net, comprises 5,162 pairs. Statistics on MapNet are
reported in table 4.
About one thousand lexical units in FrameNet
have no candidate synsets because the lemma is not
present in WordNet. The remaining LUs have 3.69
candidate synsets each on average, similarly to the
average polysemy reported for the gold standard (see
Table 2). This confirms our hypothesis that the data
used for training are well representative of the char-
N. of LUs in FrameNet 10,100
N. of LUs with at least one syn.cand. 9,120
N. of LU-synset candidate pairs 33,698
N. of mapped pairs 5,162
Table 4: Statistics on the mapping
acteristics of the whole resource. We expect about
80% of these mappings to be correct, i.e. in line
with the precision of the classifier.
8.1 Automatic FrameNet extension
MapNet can be easily exploited to automatically ex-
tend FrameNet coverage, in particular to extend the
set of lexical units for each frame. In fact, we can
assume that all lemmas in the mapped synsets have
the same meaning of the LUs in the corresponding
frames. We use MapNet to extract from WordNet
the lemmas in the mapped synsets and add them to
the frames.
For English FrameNet, we can acquire 4,265 new
lexical units for 521 frames. In this way, we would
extend FrameNet size by almost 42%. In the ran-
dom evaluation of 100 newly acquired LUs belong-
ing to 100 different frames, we assessed a precision
of 78%. For the Italian side, we extract 6,429 lexi-
cal units for 561 frames. Since no Italian FrameNet
has been developed yet, this would represent a first
attempt to create this resource by automatically pop-
ulating the frames. We evaluate the content of 15
complete frames containing 191 Italian LUs. The
assigned LUs are correct in 88% of the considered
cases, which represent a promising result w.r.t. the
unsupervised creation of Italian FrameNet.
The difference in the evaluation for the two lan-
guages most likely lies in the smaller number of
synsets on the Italian side of MultiWordNet if com-
pared to the English, which results in less ambigu-
ity. Furthermore, we should consider that the task
for Italian is easier than for English, since in the for-
mer case we are building a resource from scratch,
while in the latter we are extending an already exist-
ing resource with lexical units which are most likely
peripheral with respect to those already present in
the database.
225
8.2 Frame annotation of MultiSemCor
MultiSemCor (Bentivogli and Pianta, 2005) is an
English/Italian parallel corpus, aligned at word
level and annotated with PoS, lemma and Word-
Net synsets. The parallel corpus was created start-
ing from the SemCor corpus, which is a subset of
the English Brown corpus containing about 700,000
running words. The corpus was first manually trans-
lated into Italian. Then, the procedure of transferring
word sense annotations from English to Italian was
carried out automatically.
We apply MapNet to enrich the corpus with frame
information. We believe that this procedure would
be interesting from different point of views. Not
only we would enrich the resource with a new anno-
tation layer, but we would also automatically acquire
a large set of English and Italian sentences having a
lexical unit with a frame label. For the English side,
it is a good solution to automatically extract a dataset
with frame information and train, for example, a ma-
chine learning system for frame identification. For
the Italian side, it represents a good starting point for
the creation of a large annotated corpus with frame
information, the base for a future Italian FrameNet.
MultiSemCor contains 12,843 parallel sentences.
If we apply MapNet to the corpus, we produce
27,793 annotated instances in English and 23,872 in
Italian, i.e. about two lexical units per sentence. The
different amount of annotated sentences depends on
the fact that in MultiSemCor some synset annota-
tions have not been transferred from English to Ital-
ian. From both sides of the resulting corpus, we
randomly selected 200 sentences labeled with 200
different frames, and evaluated the annotation qual-
ity. As for the English corpus, 75% of the sen-
tences was annotated with the correct frame label,
while on the Italian side they were 70%. This re-
sult is in line with the expectations, since Map-
Net was developed with 0.79 precision. Besides,
synset annotation on the English side of MultiSem-
Cor was carried out by hand, while annotation in
Italian was automatically acquired by transferring
the information from the English corpus (precision
0.86). This explains why the resulting annotation
for English is slightly better than for Italian. In some
cases, the wrongly annotated frame was strictly con-
nected to the right one, i.e. APPLY HEAT instead
of COOKING CREATION and ATTACHING instead
of INCHOATIVE ATTACHING.
9 Conclusions
We proposed a new method to map FrameNet LUs
to WordNet synsets using SVM with minimal super-
vision effort.
To our best knowledge, this is the only approach
to the task that exploits features based on stem over-
lap between LU definition and synset gloss and
that makes use of information about WordNet do-
mains. Differently from other models, the SVM
is not trained on a per-frame basis and we do not
rely on the number of the annotated sentences for a
LU in the FrameNet corpus, thus our mapping al-
gorithm performs well also with poorly-annotated
LUs. After creating MapNet, the mapping be-
tween FrameNet and WordNet, we applied it to three
tasks: the automatic induction of new LUs for En-
glish FrameNet, the population of frames for Italian
FrameNet and the annotation of the MultiSemCor
corpus with frame information. A preliminary eval-
uation shows that the mapping can significantly re-
duce the manual effort for the development and the
extension of FrameNet-like resources, both in the
phase of corpus annotation and of frame population.
In the future, we plan to improve the algorithm
by introducing syntactic features for assessing simi-
larity between LU definitions and WordNet glosses.
We also want to merge all information extracted and
collected for Italian FrameNet and deliver a seed
version of the resource to be validated. Finally, we
plan to extend the mapping to all languages included
in MultiWordNet, i.e. Spanish, Portuguese, Hebrew
and Romanian.
Acknowledgements
We thank Roberto Basili and Diego De Cao for shar-
ing with us their gold standard of frame ? synset
mappings.
226
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 36th ACL Meeting and 17th ICCL Confer-
ence. Morgan Kaufmann.
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 10: Frame Semantic Struc-
ture Extraction. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 99?104, Prague, CZ, June.
Luisa Bentivogli and Emanuele Pianta. 2005. Exploiting
Parallel Texts in the Creation of Multilingual Seman-
tically Annotated Resources: The MultiSemCor Cor-
pus. Natural Language Engineering, Special Issue on
Parallel Texts, 11(03):247?261, September.
Hans C. Boas. 2005. Semantic frames as interlingual
representations for multilingual lexical databases. In-
ternational Journal of Lexicography, 18(4):445?478.
Aljoscha Burchardt, Katrin Erk, and Annette Frank.
2005. A WordNet detour to FrameNet. In B. Fis-
seni, H. Schmitz, B. Schro?der, and P. Wagner, editors,
Sprachtechnologie, mobile Kommunikation und lingis-
tische Resourcen, Frankfurt am Main, Germany. Peter
Lang.
Aljoscha Burchardt, Nils Reiter, Stefan Thater, and An-
nette Frank. 2007. A semantic approach to textual
entailment: System evaluation and task analysis. In
Proceedings of Pascal RTE-3 Challenge, Prague, CZ.
Diego De Cao, Danilo Croce, Marco Pennacchiotti, and
Roberto Basili. 2008. Combining Word Sense and
Usage for modeling Frame Semantics. In Proceedings
of STEP 2008, Venice, Italy.
Mario Crespo and Paul Buitelaar. 2008. Domain-specific
English-to-Spanish Translation of FrameNet. In Proc.
of LREC 2008, Marrakech.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
C.J. Fillmore, C.R. Johnson, and M. R. L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16:235?250, September.
Ana-Maria Giuglea and Alessandro Moschitti. 2006. Se-
mantic role labeling via FrameNet, VerbNet and Prop-
Bank. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th an-
nual ACL meeting, pages 929?936, Morristown, NJ,
US. Association for Computational Linguistics.
Matthew Honnibal and Tobias Hawker. 2005. Identify-
ing FrameNet frames for verbs from a real-text corpus.
In Proceedings of Australasian Language Technology
Workshop 2005.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Scho?lkopf, Christopher J. C. Burges, and Alexander J
Smola, editors, Advances in kernel methods: support
vector learning, pages 169?184. MIT Press, Cam-
bridge, MA, USA.
R. Johansson and P. Nugues. 2007. Using WordNet to
extend FrameNet coverage. In Proc. of the Workshop
on Building Frame-semantic Resources for Scandina-
vian and Baltic Languages, at NODALIDA, Tartu.
Bernardo Magnini and Gabriela Cavaglia`. 2000. Inte-
grating Subject Field Codes into WordNet. In Pro-
ceedings of LREC 2000, pages 1413?1418, Athens,
Greece.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Computational Linguistics, 31.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: developing an aligned
multilingual database. In First International Confer-
ence on Global WordNet, pages 292?302, Mysore, In-
dia.
Josef Ruppenhofer, Michael Ellsworth, Miriam R.L.
Petruck, Christopher R. Johnson, and Jan
Scheffczyk. 2006. FrameNet II: Ex-
tended Theory and Practice. Available at
http://framenet.icsi.berkeley.edu/book/book.html.
Dan Shen and Mirella Lapata. 2007. Using Semantic
Roles to Improve Question Answering. In Proceed-
ings of EMNLP and CONLL, pages 12?21, Prague,
CZ.
Lei Shi and Rada Mihalcea. 2004. Open Text Semantic
Parsing Using FrameNet and WordNet. In Proceed-
ings of HLT-NAACL 2004.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and WordNet
for Robust Semantic Parsing. In Proceedings of CI-
CLing 2005, pages 100?111. Springer.
Sara Tonelli and Emanuele Pianta. 2008. Frame Infor-
mation Transfer from English to Italian. In European
Language Resources Association (ELRA), editor, Pro-
ceedings of LREC 2008, Marrakech, Morocco.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience.
Piek Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Springer,
October.
227
Tree Kernels for Semantic Role Labeling
Alessandro Moschitti?
University of Trento
Daniele Pighin??
University of Trento
Roberto Basili?
University of Rome ?Tor Vergata?
The availability of large scale data sets of manually annotated predicate?argument struc-
tures has recently favored the use of machine learning approaches to the design of automated
semantic role labeling (SRL) systems. The main research in this area relates to the design choices
for feature representation and for effective decompositions of the task in different learning models.
Regarding the former choice, structural properties of full syntactic parses are largely employed as
they represent ways to encode different principles suggested by the linking theory between syntax
and semantics. The latter choice relates to several learning schemes over global views of the
parses. For example, re-ranking stages operating over alternative predicate?argument sequences
of the same sentence have shown to be very effective.
In this article, we propose several kernel functions to model parse tree properties in kernel-
based machines, for example, perceptrons or support vector machines. In particular, we define
different kinds of tree kernels as general approaches to feature engineering in SRL. Moreover, we
extensively experiment with such kernels to investigate their contribution to individual stages
of an SRL architecture both in isolation and in combination with other traditional manually
coded features. The results for boundary recognition, classification, and re-ranking stages provide
systematic evidence about the significant impact of tree kernels on the overall accuracy, especially
when the amount of training data is small. As a conclusive result, tree kernels allow for a general
and easily portable feature engineering method which is applicable to a large family of natural
language processing tasks.
1. Introduction
Much attention has recently been devoted to the design of systems for the automatic
labeling of semantic roles (SRL) as defined in two important projects: FrameNet (Baker,
Fillmore, and Lowe 1998), based on frame semantics, and PropBank (Palmer, Gildea,
? Department of Information Engineering and Computer Science, Via Sommarive, 14 I-38050 Povo (TN).
E-mail: moschitti@dit.unitn.it.
?? Fondazione Bruno Kessler, Center for Scientific and Technological Research, Department of Information
Engineering and Computer Science, Via Sommarive, 18 I-38050 Povo (TN). E-mail: pighin@itc.it.
? Department of Computer Science, Systems and Production, Via del Politecnico, 1 I-00133 RM.
E-mail: basili@info.uniroma2.it.
Submission received: 15 July 2006; revised submission received: 1 May 2007; accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
and Kingsbury 2005), inspired by Levin?s verb classes. To annotate natural language
sentences, such systems generally require (1) the detection of the target word em-
bodying the predicate and (2) the detection and classification of the word sequences
constituting the predicate?s arguments.
Previous work has shown that these steps can be carried out by applying machine
learning techniques (Carreras and Ma`rquez 2004, 2005; Litkowski 2004), for which the
most important features encoding predicate?argument relations are derived from (shal-
low or deep) syntactic information. The outcome of this research brings wide empirical
evidence in favor of the linking theories between semantics and syntax, for example,
Jackendoff (1990). Nevertheless, as no such theory provides a sound and complete
treatment, the choice and design of syntactic features to represent semantic structures
requires remarkable research effort and intuition.
For example, earlier studies on feature design for semantic role labeling were car-
ried out by Gildea and Jurafsky (2002) and Thompson, Levy, and Manning (2003). Since
then, researchers have proposed several syntactic feature sets, where the more recent
sets slightly enhanced the older ones.
A careful analysis of such features reveals that most of them are syntactic tree
fragments of training sentences, thus a viable way to alleviate the feature design com-
plexity is the adoption of syntactic tree kernels (Collins and Duffy 2002). For example, in
Moschitti (2004), the predicate?argument relation is represented by means of the min-
imal subtree that includes both of them. The similarity between two instances is eval-
uated by a tree kernel function in terms of common substructures. Such an approach
is in line with current research on kernels for natural language learning, for example,
syntactic parsing re-ranking (Collins and Duffy 2002), relation extraction (Zelenko,
Aone, and Richardella 2003), and named entity recognition (Cumby and Roth 2003;
Culotta and Sorensen 2004).
Furthermore, recent work (Haghighi, Toutanova, and Manning 2005; Punyakanok
et al 2005) has shown that, to achieve high labeling accuracy, joint inference should
be applied on the whole predicate?argument structure. For this purpose, we need to
extract features from the sentence syntactic parse tree that encodes the relationships
governing complex semantic structures. This task is rather difficult because we do
not exactly know which syntactic clues effectively capture the relation between the
predicate and its arguments. For example, to detect the interesting context, themodeling
of syntax-/semantics-based features should take into account linguistic aspects like
ancestor nodes or semantic dependencies (Toutanova, Markova, and Manning 2004).
In this scenario, the automatic feature generation/selection carried out by tree kernels
can provide useful insights into the underlying linguistic phenomena. Other advantages
coming from the use of tree kernels are the following.
First, we can implement them very quickly as the feature extractor module only
requires the writing of a general procedure for subtree extraction. In contrast, traditional
SRL systems use more than thirty features (e. g., Pradhan, Hacioglu, Krugler et al 2005),
each of which requires the writing of a dedicated procedure.
Second, their combination with traditional attribute?value models produces more
accurate systems, also when using the same machine learning algorithm in the combi-
nation, because the feature spaces are very different.
Third, we can carry out feature engineering using kernel combinations andmarking
strategies (Moschitti et al 2005a; Moschitti, Pighin, and Basili 2006). This allows us to
boost the SRL accuracy in a relatively simple way.
Next, tree kernels generate large tree fragment sets which constitute back-off
models for important syntactic features. Using them, the learning algorithm generalizes
194
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
better and produces a more accurate classifier, especially when the amount of training
data is scarce.
Finally, once the learning algorithm using tree kernels has converged, we can iden-
tify the most important structured features of the generated model. One approach for
such a reverse engineering process relies on the computation of the explicit feature
space, at least for the highest-weighted features (Kudo and Matsumoto 2003). Once
the most relevant fragments are available, they can be used to design novel effective
attribute?value features (which in turn can be used to design more efficient classifiers,
e. g., with linear kernels) and inspire new linguistic theories.
These points suggest that tree kernels should always be applied, at least for an initial
study of the problem. Unfortunately, they suffer from two main limitations: (a) poor
impact on boundary detection as, in this task, correct and incorrect arguments may
share a large portion of the encoding trees (Moschitti 2004); and (b) more expensive
running time and limited contribution to the overall accuracy if compared with manu-
ally derived features (Cumby and Roth 2003). Point (a) has been addressed byMoschitti,
Pighin, and Basili (2006) by showing that a strategy ofmarking relevant parse-tree nodes
makes correct and incorrect subtrees for boundary detection quite different. Point (b)
can be tackled by studying approaches to kernel engineering that allow for the design
of efficient and effective kernels.
In this article, we provide a comprehensive study of the use of tree kernels for se-
mantic role labeling. For this purpose, we define tree kernels based on the composition
of two different feature functions: canonical mappings, which map sentence-parse trees
in tree structures encoding semantic information, and feature extraction functions,
which encode these trees in the actual feature space. The latter functions explode the
canonical trees into all their substructures and, in the literature, are usually referred to as
tree kernels. For instance, in Collins and Duffy (2002), Vishwanathan and Smola (2002),
and Moschitti (2006a) different tree kernels extract different types of tree fragments.
Given the heuristic nature of canonical mappings, we studied their properties
by experimenting with them within support vector machines and with the data set
provided by CoNLL shared tasks (Carreras and Ma`rquez 2005). The results show that
carefully engineered tree kernels always boost the accuracy of the basic systems. Most
importantly, in complex tasks such as the re-ranking of semantic role annotations, they
provide an easy way to engineer new features which enhance the state-of-the-art in SRL.
In the remainder of this article, Section 2 presents traditional architectures for SRL
and the typical features proposed in literature. Tree kernels are formally introduced
in Section 3, and Section 4 describes our modular architecture employing support
vector machines along with manually designed features, tree kernels (feature extraction
functions), and their combinations. Section 5 presents our structured features (canonical
mappings) inducing different kernels that we used for different SRL subtasks. The
extensive experimental results obtained on the boundary recognition, role classification,
and re-ranking stages are presented in Section 6. Finally, Section 7 summarizes the
conclusions.
2. Automatic Shallow Semantic Parsing
The recognition of semantic structures within a sentence relies on lexical and syntactic
information provided by early stages of anNLP process, such as lexical analysis, part-of-
speech tagging, and syntactic parsing. The complexity of the SRL task mostly depends
on two aspects: (a) the information is generally noisy, that is, in a real-world scenario
the accuracy and reliability of NLP subsystems are generally not very high; and (b) the
195
Computational Linguistics Volume 34, Number 2
lack of a sound and complete linguistic or cognitive theory about the links between
syntax and semantics does not allow an informed, deductive approach to the problem.
Nevertheless, the large amount of available lexical and syntactic information favors the
application of inductive approaches to the SRL task, which indeed is generally treated
as a combination of statistical classification problems.
The next sections define the SRL task more precisely and summarize the most
relevant work carried out to address these two problems.
2.1 Problem Definition
The most well-known shallow semantic theories are studied in two different projects:
PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and
Lowe 1998). The former is based on a linguistic model inspired by Levin?s verb classes
(Levin 1993), focusing on the argument structure of verbs and on the alternation pat-
terns that describe movements of verbal arguments within a predicate structure. The
latter refers to the application of frame semantics (Fillmore 1968) in the annotation of
predicate?argument structures based on frame elements (semantic roles). These theories
have been investigated in two CoNLL shared tasks (Carreras and Ma`rquez 2004, 2005)
and a Senseval-3 evaluation (Litkowski 2004), respectively.
Given a sentence and a predicate word, an SRL system outputs an annotation of the
sentence in which the sequences of words that make up the arguments of the predicate
are properly labeled, for example:
[Arg0 He] got [Arg1 his money] [C-V back]
1
in response to the input He got his money back. This processing requires that: (1) the
predicates within the sentence are identified and (2) the word sequences that span the
boundaries of each predicate argument are delimited and assigned the proper role label.
The first sub-task can be performed either using statistical methods or hand-crafted
lexical and syntactic rules. In the case of verbal predicates, it is quite easy to write
simple rules matching regular expressions built on POS tags. The second task is more
complex and is generally viewed as a combination of statistical classification problems:
The learning algorithms are trained to recognize the extension of predicate arguments
and the semantic role they play.
2.2 Models for Semantic Role Labeling
An SRL model and the resulting architecture are largely influenced by the kind of data
available for the task. As an example, a model relying on a shallow syntactic parser
would assign roles to chunks, whereas with a full syntactic parse of the sentence it
would be straightforward to establish a correspondence between nodes of the parse tree
and semantic roles. We focused on the latter as it has been shown to be more accurate
by the CoNLL 2005 shared task results.
According to the deep syntactic formulation, the classifying instances are pairs
of parse-tree nodes which dominate the exact span of the predicate and the target
argument. Such pairs are usually represented in terms of attribute?value vectors, where
1 In PropBank notation, Arg0 and Arg1 represent the logical subject and the logical object of the target
verbal predicate, respectively. C-V represents the particle of a phrasal-verb predicate.
196
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
the attributes describe properties of predicates, arguments, and theway they are related.
There is large agreement on an effective set of linguistic features (Gildea and Jurafsky
2002; Pradhan, Hacioglu, Krugler, et al 2005) that have been employed in the vast
majority of SRL systems. The most relevant features are summarized in Table 1.
Once the representation for the predicate?argument pairs is available, a multi-
classifier is used to recognize the correct node pairs, namely, nodes associated with
correct arguments (given a predicate), and assign them a label (which is the label of the
argument). This can be achieved by training a multi-classifier on n+ 1 classes, where
the first n classes correspond to the different roles and the (n+ 1)th is a NARG (non-
argument) class to which non-argument nodes are assigned.
A more efficient solution consists in dividing the labeling process into two steps:
boundary detection and argument classification. A Boundary Classifier (BC) is a binary
classifier that recognizes the tree nodes that exactly cover a predicate argument, that
is, that dominate all and only the words that belong to target arguments. Then, such
nodes are classified by a Role Multi-classifier (RM) that assigns to each example the
most appropriate label. This two-step approach (Gildea and Jurafsky 2002) has the
advantage of only applying BC on all parse-tree nodes. RM can ignore non-boundary
nodes, resulting in a much faster classification. Other approaches have extended this
solution and suggested other multi-stage classification models (e. g., Moschitti et al
2005b in which a four-step hierarchical SRL architecture is described).
After node labeling has been carried out, it is possible that the output of the argu-
ment classifier does not result in a consistent annotation, as the labeling scheme may
not be compatible with the underlying linguistic model. As an example, PropBank-style
annotations do not allow arguments to be nested. This happens when two or more
Table 1
Standard linguistic features employed by most SRL systems.
Feature Name Description
Predicate Lemmatization of the predicate word
Path Syntactic path linking the predicate and an argument,
e. g., NN?NP?VP?VBX
Partial path Path feature limited to the branching of the argument
No-direction path Like Path, but without traversal directions
Phrase type Syntactic type of the argument node
Position Relative position of the argument with respect to the predicate
Voice Voice of the predicate, i. e., active or passive
Head word Syntactic head of the argument phrase
Verb subcategorization Production rule expanding the predicate parent node
Named entities Classes of named entities that appear in the argument node
Head word POS POS tag of the argument node head word (less sparse than
Head word)
Verb clustering Type of verb? direct object relation
Governing Category Whether the candidate argument is the verb subject or object
Syntactic Frame Position of the NPs surrounding the predicate
Verb sense Sense information for polysemous verbs
Head word of PP Enriched POS of prepositional argument nodes (e. g., PP-for, PP-in)
First and last word/POS First and last words and POS tags of candidate argument phrases
Ordinal position Absolute offset of a candidate argument within a proposition
Constituent tree distance Distance from the predicate with respect to the parse tree
Constituent features Description of the constituents surrounding the argument node
Temporal Cue Words Temporal markers which are very distinctive of some roles
197
Computational Linguistics Volume 34, Number 2
overlapping tree nodes, namely, one dominating the other, are classified as positive
boundaries.
The simplest solution relies on the application of heuristics that take into account
the whole predicate?argument structure to remove the incorrect labels (e. g., Moschitti
et al 2005a; Tjong Kim Sang et al 2005). A much more complex solution consists in the
application of some joint inference model to the whole predicate?argument structure,
as in Pradhan et al (2004). As an example, Haghighi, Toutanova, and Manning (2005)
associate a posterior probability with each argument node role assignment, estimate the
likelihood of the alternative labeling schemes, and employ a re-ranking mechanism to
select the best annotation.
Additionally, the most accurate systems participating in CoNLL 2005 shared task
(Pradhan, Hacioglu, Ward et al 2005; Punyakanok et al 2005) use different syntactic
views of the same input sentence. This allows the SRL system to recover from syntactic
parser errors; for example, a prepositional phrase specifying the direct object of the
predicate would be attached to the verb instead of the argument. This kind of error
prevents some arguments of the proposition from being recognized, as: (1) there may
not be a node of the parse tree dominating (all and only) the words of the correct se-
quence; (2) a badly attached tree node may invalidate other argument nodes, generating
unexpected overlapping situations.
The manual design of features which capture important properties of complete
predicate?argument structures (also coming from different syntactic views) is quite
complex. Tree kernels are a valid alternative to manual design as the next section
points out.
3. Tree Kernels
Tree kernels have been applied to reduce the feature design effort in the context of
several natural language tasks, for example, syntactic parsing re-ranking (Collins and
Duffy 2002), relation extraction (Zelenko, Aone, and Richardella 2003), named entity
recognition (Cumby and Roth 2003; Culotta and Sorensen 2004), and semantic role
labeling (Moschitti 2004).
On the one hand, these studies show that the kernel ability to generate large feature
sets is useful to quickly model new and not well understood linguistic phenomena
in learning machines. On the other hand, they show that sometimes it is possible to
manually design features for linear kernels that produce higher accuracy and faster
computation time. One of the most important causes of such mixed behavior is the
inappropriate choice of kernel functions. For example, in Moschitti, Pighin, and Basili
(2006) andMoschitti (2006a), several kernels have been designed and shown to produce
different impacts on the training algorithms.
In the next sections, we briefly introduce the kernel trick and describe the subtree
(ST) kernel devised in Vishwanathan and Smola (2002), the subset tree (SST) kernel
defined in Collins and Duffy (2002), and the partial tree (PT) kernel proposed in
Moschitti (2006a).
3.1 Kernel Trick
The main concept underlying machine learning for classification tasks is the automatic
learning of classification functions based on examples labeled with the class informa-
tion. Such examples can be described by means of feature vectors in an n dimensional
198
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
space over the real numbers, namely, n. The learning algorithm uses space metrics
over vectors, for example, the scalar product, to learn an abstract representation of all
instances belonging to the target class.
For example, support vector machines (SVMs) are linear classifiers which learn a
hyperplane f (x) = w? x+ b = 0, separating positive from negative examples. x is the
feature vector representation of a classifying object o, whereas w ? n and b ?  are
parameters learned from the data by applying the Structural Risk Minimization principle
(Vapnik 1998). The object o is mapped to x via a feature function ? : O ? n, O being
the set of the objects that we want to classify. o is categorized in the target class only if
f (x) ? 0.
The kernel trick allows us to rewrite the decision hyperplane as:
f (x) =
(
?
i=1..l
yi?ixi
)
? x+ b =
?
i=1..l
yi?ixi ? x+ b =
?
i=1..l
yi?i?(oi) ??(o)+ b = 0
where yi is equal to 1 for positive examples and ?1 for negative examples, ?i ?  with
?i ? 0, oi ?i ? {1, .., l} are the training instances and the product K(oi, o) = ??(oi) ??(o)?
is the kernel function associated with the mapping ?.
Note that we do not need to apply the mapping ?; we can use K(oi, o) directly.
This allows us, underMercer?s conditions (Shawe-Taylor and Cristianini 2004), to define
abstract kernel functions which generate implicit feature spaces. A traditional example
is given by the polynomial kernel: Kp(o1, o2) = (c+ x1 ? x2)d, where c is a constant and d
is the degree of the polynomial. This kernel generates the space of all conjunctions of
feature groups up to d elements.
Additionally, we can carry out two interesting operations:
 kernel combinations, for example, K1 + K2 or K1 ? K2
 feature mapping compositions, for example,
K(o1, o2) = ??(o1) ??(o2)? = ??B(?A(o1)) ??B(?A(o2))?
Kernel combinations are very useful for integrating the knowledge provided by the
manually defined features with the knowledge automatically obtained with structural
kernels; feature mapping compositions are useful methods to describe diverse kernel
classes (see Section 5). In this perspective, we propose to split themapping? by defining
our tree kernel as follows:
 Canonical Mapping, ?M(), in which a linguistic object (e. g., a syntactic
parse tree) is transformed into a more meaningful structure (e. g., the
subtree corresponding to a verb subcategorization frame).
 Feature Extraction, ?S(), which maps the canonical structure in all its
fragments according to different fragment spaces S (e. g., ST, SST, and PT).
For example, given the kernel KST = ?ST(o1) ??ST(o2), we can apply a canonical
mapping ?M(), obtaining K
M
ST = ?ST(?M(o1)) ??ST(?M(o2)) =
(
?ST ??M
)
(o1) ?
(
?ST ?
?M
)
(o2), which is a noticeably different kernel, which is induced by the mapping
(
?ST ??M
)
.
199
Computational Linguistics Volume 34, Number 2
In the remainder of this section we start the description of our engineered kernels
by defining three different feature extraction mappings based on three different kernel
spaces (i. e., ST, SST, and PT).
3.2 Tree Kernel Spaces
The kernels that we consider represent trees in terms of their substructures (fragments).
The kernel function detects if a tree subpart (common to both trees) belongs to the fea-
ture space that we intend to generate. For this purpose, the desired fragments need to be
described. We consider three main characterizations: the subtrees (STs) (Vishwanathan
and Smola 2002), the subset trees (SSTs) or all subtrees (Collins and Duffy 2002), and the
partial trees (PTs) (Moschitti 2006a).
As we consider syntactic parse trees, each node with its children is associated with
a grammar production rule, where the symbol on the left-hand side corresponds to the
parent and the symbols on the right-hand side are associated with the children. The
terminal symbols of the grammar are always associated with tree leaves.
A subtree (ST) is defined as a tree rooted in any non-terminal node along with
all its descendants. For example, Figure 1a shows the parse tree of the sentence Mary
brought a cat together with its six STs. A subset tree (SST) is a more general structure
because its leaves can be non-terminal symbols. For example, Figure 1(b) shows ten
SSTs (out of 17) of the subtree in Figure 1a rooted in VP. SSTs satisfy the constraint that
grammatical rules cannot be broken. For example, [VP [V NP]] is an SST which has
two non-terminal symbols, V and NP, as leaves. On the contrary, [VP [V]] is not an
SST as it violates the production VP?V NP. If we relax the constraint over the SSTs, we
obtain a more general form of substructures that we call partial trees (PTs). These can be
generated by the application of partial production rules of the grammar; consequently
[VP [V]] and [VP [NP]] are valid PTs. It is worth noting that PTs consider the position
of the children as, for example, [A [B][C][D]] and [A [D][C][B]] only share single
children, i.e., [A [B]], [A [C]], and [A [D]].
Figure 1c shows that the number of PTs derived from the same tree as before is still
higher (i. e., 30 PTs). These numbers provide an intuitive quantification of the different
degrees of information encoded by each representation.
Figure 1
Example of (a) ST, (b) SST, and (c) PT fragments.
200
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
3.3 Feature Extraction Functions
The main idea underlying tree kernels is to compute the number of common substruc-
tures between two trees T1 and T2 without explicitly considering the whole fragment
space. In the following, we report on the Subset Tree (SST) kernel proposed in Collins
and Duffy (2002). The algorithms to efficiently compute it along with the ST and PT
kernels can be found in Moschitti (2006a).
Given two trees T1 and T2, let { f1, f2, ..} = F be the set of substructures (fragments)
and Ii(n) be equal to 1 if fi is rooted at node n, 0 otherwise. Collins and Duffy?s kernel is
defined as
K(T1,T2) =
?
n1?NT1
?
n2?NT2
?(n1,n2) (1)
where NT1 and NT2 are the sets of nodes in T1 and T2, respectively, and ?(n1,n2) =
?|F|
i=1 Ii(n1)Ii(n2). The latter is equal to the number of common fragments rooted in nodes
n1 and n2.? can be computed as follows:
1. If the productions (i.e. the nodes with their direct children) at n1 and n2
are different, then?(n1,n2) = 0.
2. If the productions at n1 and n2 are the same, and n1 and n2 only have
leaf children (i.e., they are pre-terminal symbols), then ?(n1,n2) = 1.
3. If the productions at n1 and n2 are the same, and n1 and n2 are
not pre-terminals, then?(n1,n2) =
?nc(n1 )
j=1 (1+?(c
j
n1 , c
j
n2 )), where
nc(n1) is the number of children of n1 and c
j
n is the j-th child of n.
Such tree kernels can be normalized and a ? factor can be added to reduce the
weight of large structures (refer to Collins and Duffy [2002] for a complete description).
3.4 Related Work
Although the literature on SRL is extensive, there is almost no study of the use of tree
kernels for its solution. Consequently, the reported research is mainly based on diverse
natural language learning problems tackled by means of tree kernels.
In Collins and Duffy (2002), the SST kernel was experimented with using the voted
perceptron for the parse tree re-ranking task. A combination with the original PCFG
model improved the syntactic parsing. Another interesting kernel for re-ranking was
defined in Toutanova, Markova, andManning (2004). This represents parse trees as lists
of paths (leaf projection paths) from leaves to the top level of the tree. It is worth noting
that the PT kernel includes tree fragments identical to such paths.
In Kazama and Torisawa (2005), an interesting algorithm that speeds up the average
running time is presented. This algorithm looks for node pairs in which the rooted
subtrees share many substructures (malicious nodes) and applies a transformation to
the trees rooted in such nodes to make the kernel computation faster. The results show
a several-hundred-fold speed increase with respect to the basic implementation.
201
Computational Linguistics Volume 34, Number 2
In Zelenko, Aone, and Richardella (2003), two kernels over syntactic shallow
parser structures were devised for the extraction of linguistic relations, for example,
person-affiliation. To measure the similarity between two nodes, the contiguous string
kernel and the sparse string kernelwere used. In Culotta and Sorensen (2004) such kernels
were slightly generalized by providing a matching function for the node pairs. The
time complexity for their computation limited the experiments to a data set of just
200 news items.
In Shen, Sarkar, and Joshi (2003), a tree kernel based on lexicalized tree adjoining
grammar (LTAG) for the parse re-ranking task was proposed. The subtrees induced by
this kernel are built using the set of elementary trees as defined by LTAG.
In Cumby and Roth (2003), a feature description language was used to extract struc-
tured features from the syntactic shallow parse trees associated with named entities.
Their experiments on named entity categorization showed that when the description
language selects an adequate set of tree fragments the voted perceptron algorithm
increases its classification accuracy. The explanationwas that the complete tree fragment
set contains many irrelevant features and may cause overfitting.
In Zhang, Zhang, and Su (2006), convolution tree kernels for relation extraction
were applied in a way similar to the one proposed in Moschitti (2004). The combina-
tion of standard features along with several tree subparts, tailored according to their
importance for the task, produced again an improvement on the state of the art.
Such previous work, as well as that described previously, show that tree kernels
can efficiently represent syntactic objects, for example, constituent parse trees, in huge
feature spaces. The next section describes our SRL system adopting tree kernels within
SVMs.
4. A State-of-the-Art Architecture for Semantic Role Labeling
Ameaningful study of tree kernels for SRL cannot be carried out without a comparison
with a state-of-the-art architecture: Kernel models that improve average performing
systems are just a technical exercise whose findings would have a reduced value. A
state-of-the-art architecture, instead, can be used as a basic system upon which tree
kernels should improve. Because kernel functions in general introduce a sensible slow-
down with respect to the linear approach, we also have to consider efficiency issues.
These aims drove us in choosing the following components for our SRL system:
 SVMs as our learning algorithm; these provide both a state-of-the-art
learning model (in terms of accuracy) and the possibility of using
kernel functions
 a two-stage role labeling module to improve learning and classification
efficiency; this comprises:
? a feature extractor that can represent candidate arguments using
both linear and structured features
? a boundary classifier (BC)
? a role multi-classifier (RM), which is obtained by applying the OVA
(One vs. All) approach
 a conflict resolution module, that is, a software component that resolves
inconsistencies in the annotations using either a rule-based approach
or a tree kernel classifier; the latter allows experimentation with
202
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
the classification of complete predicate?argument annotations in correct
and incorrect structures
 a joint inference re-ranking module, which employs a combination of
standard features and tree kernels to rank alternative candidate labeling
schemes for a proposition; this module, as shown in Gildea and Jurafsky
(2002), Pradhan et al (2004), and Haghighi, Toutanova, and Manning
(2005), is mandatory in order to achieve state-of-the-art accuracy
We point out that we did not use any heuristic to filter out the nodes which are
likely to be incorrect boundaries, for example, as done in Xue and Palmer (2004). On the
one hand, this makes the learning and classification phases more complex because they
involve more instances. On the other hand, our results are not biased by the quality of
the heuristics, leading to more meaningful findings.
In the remainder of this section, we describe the main functional modules of our
architecture for SRL and introduce some basic concepts about the use of structured
features for SRL. Specific feature engineering for the above SRL subtasks is described
and discussed in Section 5.
4.1 A Basic Two-Stage Role Labeling System
Given a sentence in natural language, our SRL system identifies all the verb predicates
and their respective arguments. We divide this step into three subtasks: (a) predicate
detection, which can be carried out by simple heuristics based on part-of-speech infor-
mation, (b) the detection of predicate?argument boundaries (i. e., the span of their words
in the sentence), and (c) the classification of the argument type (e. g., Arg0 or ArgM in
PropBank).
The standard approach to learning both the detection and the classification of
predicate arguments is summarized by the following steps:
1. Given a sentence from the training set, generate a full syntactic parse tree;
2. let P and A be the set of predicates and the set of parse-tree nodes (i. e., the
potential arguments), respectively;
3. for each pair ?p, a? ? P ?A:
 extract the feature representation, ?(p, a), (e. g., attribute?values or
tree fragments [see Section 3.1]);
 if the leaves of the subtree rooted in a correspond to all and
only the words of one argument of p (i. e., a exactly covers an
argument), add ?(p, a) in E+ (positive examples), otherwise
add it in E? (negative examples).
For instance, given the example in Figure 2(a), we would consider all the pairs ?p, a?
where p is the node associated with the predicate took and a is any other tree node not
overlapping with p. If the node a exactly covers the word sequences John or the book,
then ?(p, a) is added to the set E+, otherwise it is added to E?, as in the case of the node
(NN book).
The E+ and E? sets are used to train the boundary classifier. To train the role
multiclassifier, the elements of E+ can be reorganized as positive E+argi and negative E
?
argi
examples for each role type i. In this way, a binary OVA classifier for each argument
203
Computational Linguistics Volume 34, Number 2
Figure 2
Positive (framed) and negative (unframed) examples of candidate argument nodes for the
propositions (a) [Arg0 John] took [Arg1 the book] and read its title and (b) [Arg0 John] took the
book and read [Arg1 its title].
i can be trained. We adopted this solution following Pradhan, Hacioglu, Krugler et al
(2005) because it is simple and effective. In the classification phase, given an unseen
sentence, all the pairs ?p, a? are generated and classified by each individual role classifier
Ci. The argument label associated with the maximum among the scores provided by Ci
is eventually selected.
The feature extraction function ? can be implemented according to different lin-
guistic theories and intuitions. From a technical point of view, we can use ? to map
?p, a? in feature vectors or in structures to be used in a tree kernel function. The next
section describes our choices in more detail.
4.2 Linear and Structured Representation
Our feature extractor module and our learning algorithms are designed to cope with
both linear and structured features, used for the different stages of the SRL process.
The standard features that we adopted are shown in Table 1. They include:
 the Phrase Type, Predicate Word, Head Word, Governing Category, Position,
and Voice defined in Gildea and Jurafsky (2002);
 the Partial Path, No Direction Path, Constituent Tree Distance, Head Word
POS, First and Last Word/POS, Verb Subcategorization, and Head Word of the
Noun Phrase in the Prepositional Phrase proposed in Pradhan, Hacioglu,
Krugler et al (2005); and
 the Syntactic Frame defined in Xue and Palmer (2004).
We indicate with structured features the basic syntactic structures extracted from
the sentence-parse tree or their canonical transformation (see Section 3.1). In particular,
we focus on the minimal spanning tree that includes the predicate along with all of its
arguments.
More formally, given a parse tree t, a node set spanning tree (NST) over a set of
nodes Nt = {n1, . . . ,nk} is a partial tree of t that (1) is rooted at the deepest level and (2)
contains all and only the nodes ni ? Nt, along with their ancestors and descendants. An
NST can be built as follows. For any choice of Nt, we call r the lowest common ancestor
204
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Figure 3
(a) A sentence parse tree, the correct ASTns associated with two different predicates (b,c), and (d)
a correct AST1 relative to the argument Arg1 its title of the predicate read.
of n1, . . . ,nk. Then, from the set of all the descendants of r, we remove all the nodes nj
that: (1) do not belong to Nt and (2) are neither ancestors nor descendants of any node
belonging to Nt.
Because predicate arguments are associatedwith tree nodes, we can define the pred-
icate argument spanning tree (ASTn) of a predicate argument node setAp = {a1, . . . , an}
as the NST over these nodes and the predicate node, that is, the node exactly covering
the predicate p.2 An ASTn corresponds to the minimal parse subtree whose leaves are
all and only the word sequences belonging to the arguments and the predicate. For
example, Figure 3a shows the parse tree of the sentence: John took the book and read its
title. took{ARG0,ARG1} and read{ARG0,ARG1} are two ASTn structures associated with the two
predicates took and read, respectively, and are shown in Figure 3b and 3c.
For each predicate, only one NST is a valid ASTn. Careful manipulations of an ASTn
can be employed for those tasks that require a representation of the whole predicate?
argument structure, for example, overlap resolution or proposition re-ranking.
It is worth noting that the predicate?argument feature, or PAF in Moschitti (2004),
is a canonical transformation of the ASTn in the subtree including the predicate p and
only one of its arguments. For the sake of uniform notation, PAF will be referred to as
AST1 (argument spanning tree), the subscript 1 stressing the fact that the structure only
encompasses one of the predicate arguments. An example AST1 is shown in Figure 3d.
Manipulations of an AST1 structure can lead to interesting tree kernels for local learning
tasks, such as boundary detection and argument classification.
Regardless of the adopted feature space, our multiclassification approach suffers
from the problem of selecting both boundaries and argument roles independently of
the whole structures. Thus, it is possible that (a) two labeled nodes refer to the same
arguments (node overlaps) and (b) invalid role sequences are generated (e. g., Arg0,
Arg0, Arg0, . . . ). Next, we describe our approach to solving such problems.
4.3 Conflict Resolution
We call a conflict, or ambiguity, or overlap resolution a stage of the SRL process
which resolves annotation conflicts that invalidate the underlying linguistic model. This
2 The ASTn of a predicate p and its argument nodes {a1, . . . , an}, will also be referred to as p{a1,..., an}.
205
Computational Linguistics Volume 34, Number 2
happens, for example, when both a node and one of its descendants are classified as
positive boundaries, namely, they received a role label. We say that such nodes are
overlapping as their leaf (i. e., word) sequences overlap. Because this situation is not
allowed by the PropBank annotation definition, we need a method to select the most
appropriate word sequence. Our system architecture can employ one of three different
disambiguation strategies:
 a basic solution which, given two overlapping nodes, randomly selects
one to be removed;
 the following heuristics:
1. The node causing the major number of overlaps is removed, for
example, a node which dominates two nodes labeled as arguments
2. Core arguments (i. e., arguments associated with the
subcategorization frame of the target verb) are always preferred
over adjuncts (i. e., arguments that are not specific to verbs or
verb senses)
3. In case the two previous rules do not eliminate all conflicts, the
nodes located deeper in the tree are discarded; and
 a tree kernel?based overlap resolution strategy consisting of an SVM
trained to recognize non-clashing configurations that often correspond
to correct propositions.
The latter approach consists of: (1) a softwaremodule that generates all the possible non-
overlapping configurations of nodes. These are built using the output of the local node
classifiers by generating all the permutations of argument nodes of a predicate and re-
moving the configurations that contain at least one overlap; (2) an SVM trained on such
non-overlapping configurations, where the positive examples are correct predicate?
argument structures (although eventually not complete) and negative ones are not. At
testing time, we classify all the alternative non-clashing configurations. In case more
than one structure is selected as correct, we choose the one associated with the highest
SVM score.
These disambiguationmodules can be invoked after either the BC or the RM classifi-
cation. The different information available after each phase can be used to design differ-
ent kinds of features. For example, the knowledge of the candidate role of an argument
node can be a key issue in the design of effective conflict resolution methodologies, for
example, by eliminating ArgX, ArgX, ArgX, . . . sequences. These different approaches
are discussed in Section 5.2.
The next section describes a more advanced approach that can eliminate overlaps
and choose the most correct annotation for a proposition among a set of alternative
labeling schemes.
4.4 A Joint Model for Re-Ranking
The heuristics considered in the previous sections only act when a conflict is detected.
In a real situation, many incorrect annotations are generated with no overlaps. To deal
with such cases, we need a re-ranking module based on a joint BC and RM model as
suggested in Haghighi, Toutanova, and Manning (2005). Such a model is based on (1)
206
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
an algorithm to evaluate the most likely labeling schemes for a given predicate, and (2)
a re-ranker that sorts the labeling schemes according to their correctness.
Step 1 uses the probabilities associated with each possible annotation of parse tree
nodes, hence requiring a probabilistic output from BC and RM. As the SVM learning
algorithm produces metric values, we applied Platt?s algorithm (Platt 1999) to convert
them into probabilities, as already proposed in Pradhan, Ward et al (2005). These
posterior probabilities are then combined to generate the n labelings that maximize a
likelihood measure. Step 2 requires the training of an automatic re-ranker. This can be
designed using a binary classifier that, given two annotations, decides which one is
more accurate. We modeled such a classifier by means of three different kernels based
on standard features, structured features, and their combination.
4.4.1 Evaluation of the N-best Annotations. First, we converted the output of each node-
classifier into a posterior probability conditioned by its output scores (Platt 1999).
This method uses a parametric model to fit onto a sigmoid distribution the posterior
probability P(y = 1, f ), where f is the output of the classifier and the parameters are
dynamically adapted to give the best probability output.3 Second, we selected the n
most likely sequences of node labelings. Given a predicate, the likelihood of a labeling
scheme (or state) s for the K candidate argument nodes is given by:
p(s) =
K
?
i=1
p?i (l), p
?
i (l) =
{
pi(li)pi(ARG) if li = NARG
(1? pi(ARG))2 otherwise
(2)
where pi(l) is the probability of node i being assigned the label l, and p
?
i (l) is the same
probability weighted by the probability pi(ARG) of the node being an argument. If l =
NARG (not an argument) then both terms evaluate to (1? pi(ARG)) and the likelihood
of the NARG label assignment is given by (1? pi(ARG))2.
To select the n states associated with the highest probability, we cannot evaluate
the likelihood of all possible states because they are exponential in number. In order
to reduce the search space we (a) limit the number of possible labelings of each node
to m and (b) avoid traversing all the states by applying a Viterbi algorithm to search
for the most likely labeling schemes. From each state we generate the states in which
a candidate argument is assigned different labels. This operation is bound to output at
most n states which are generated by traversing a maximum of n?m states. Therefore,
in the worst case scenario the number of traversed states is V = n?m? k, k being the
number of candidate argument nodes in the tree.
During the search we also enforce overlap resolution policies. Indeed, for any given
state in which a node nj is assigned a label l = NARG, we generate all; and only the
states in which all the nodes that are dominated by nj are assigned the NARG label.
4.4.2 Modeling an Automatic Re-Ranker. The Viterbi algorithm generates the nmost likely
annotations for the proposition associated with a predicate p. These can be used to build
annotation pairs, ?si, sj?, which, in turn, are used to train a binary classifier that decides if
3 We actually implemented the pseudo-code proposed in Lin, Lin, and Weng (2003) which, with respect
to Platt?s original formulation, is theoretically demonstrated to converge and avoids some numerical
difficulties that may arise.
207
Computational Linguistics Volume 34, Number 2
si is more accurate that sj. Each candidate proposition si can be described by a structured
feature ti and a vector of standard features vi. As a whole, an example ei is described by
the tuple ?t1i , t
2
i , v
1
i , v
2
i ?, where t
1
i and v
1
i refer to the first candidate annotation, whereas t
2
i
and v2i refer to the second one. Given such data, we can define the following re-ranking
kernels:
Ktr(e1, e2) = Kt(t
1
1, t
1
2)+ Kt(t
2
1, t
2
2)? Kt(t
1
1, t
2
2)? Kt(t
2
1, t
1
2)
Kpr(e1, e2) = Kp(v
1
1, v
1
2)+ Kp(v
2
1, v
2
2)? Kp(v
1
1, v
2
2)? Kp(v
2
1, v
1
2)
where Kt is one of the tree kernel functions defined in Section 3 and Kp is a polynomial
kernel applied to the feature vectors. The final kernel that we use is the following
combination:
K(e1, e2) =
Ktr(e1, e2)
|Ktr(e1, e2)|
+
Kpr(e1, e2)
|Kpr(e1, e2)|
Previous sections have shown how our SRL architecture exploits tree kernel func-
tions to a large extent. In the next section, we describe in more detail our structured
features and the engineering methods applied for the different subtasks of the SRL
process.
5. Structured Feature Engineering
Structured features are an effective alternative to standard features in many aspects. An
important advantage is that the target feature space can be completely changed even
by small modifications of the applied kernel function. This can be exploited to identify
features relevant to learning problems lacking a clear and sound linguistic or cognitive
justification.
As shown in Section 3.1, a kernel function is a scalar product ?(o1) ??(o2), where
? is a mapping in an Euclidean space, and o1 and o2 are the target data, for example,
parse trees. To make the engineering process easier, we decompose ? into a canonical
mapping, ?M, and a feature extraction function, ?S, over the set of incoming parse
trees. ?M transforms a tree into a canonical structure equivalent to an entire class of
input parses and ?S shatters an input tree into its subparts (e. g., subtrees, subset trees,
or partial trees as described in Section 3). A large number of different feature spaces can
thus be explored by suitable combinations ? = ?S ??M of mappings.
We study different canonical mappings to capture syntactic/semantic aspects useful
for SRL. In particular, we define structured features for the different phases of the SRL
process, namely, boundary detection, argument classification, conflict resolution, and
proposition re-ranking.
5.1 Structures for Boundary Detection and Argument Classification
The AST1 or PAF structures, already mentioned in Section 4.2, have shown to be very
effective for argument classification but not for boundary detection. The reason is that
two nodes that encode correct and incorrect boundaries may generate very similar
AST1s and, consequently, have many fragments in common. To solve this problem, we
208
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Figure 4
Parse tree of the example proposition [Arg0 Paul] delivers [Arg1 a talk in formal style].
Figure 5
(a) AST1, (b) AST
m
1 , and (c) AST
cm
1 structures relative to the argument Arg1 a talk in formal style of
the predicate delivers of the example parse tree shown in Figure 4.
specify the node that exactly covers the target argument node by simply marking it (or
marking all its descendants) with the label B, denoting the boundary property.
For example, Figure 4 shows the parse tree of the sentence Paul delivers a talk in
formal style, highlighting the predicate with its two arguments, that is, Arg0 and Arg1.
Figure 5 shows the AST1, AST
m
1 , and AST
cm
1 , that is, the basic structure, the structure
with the marked argument node, and the completely marked structure, respectively.
To understand the usefulness of node-marking strategies, we can examine Figure 6.
This reports the case in which a correct and an incorrect argument node are chosen by
also showing the corresponding AST1 and AST
m
1 representations ((a) and (b)). Figure 6c
shows that the number of common fragments of two AST1 structures is 14. This is
much larger than the number of common ASTm1 fragments, that is, only 3 substructures
(Figure 6d).
Additionally, because the type of a target argument strongly depends on the type
and number of the other predicate arguments4 (Punyakanok et al 2005; Toutanova,
4 This is true at least for core arguments.
209
Computational Linguistics Volume 34, Number 2
Figure 6
(a) AST1s and (b) AST
m
1 s extracted for the same target argument with their respective (c,b)
common fragment spaces.
Haghighi, and Manning 2005), we should extract features from the whole predicate
argument structure. In contrast, AST1s completely neglect the information (i. e., the tree
portions) related to non-target arguments.
One way to use this further information with tree kernels is to use the minimum
subtree that spans all the predicate?argument structures, that is, the ASTn defined in
Section 4.2.
However, ASTns pose two problems. First, we cannot use them for the boundary
detection task since we do not know the predicate?argument structure yet. We can
derive the ASTn (its approximation) from the nodes selected by a boundary classifier,
that is, the nodes that correspond to potential arguments. Such approximated ASTns
can be easily used in the argument classification stage.
Second, an ASTn is the same for all the arguments in a proposition, thus we need a
way to differentiate it for each target argument. Again, we canmark the target argument
node as shown in the previous section. We refer to this subtree as a marked target
ASTn (AST
mt
n ). However, for large arguments (i. e., spread over a large part of the
sentence tree) the substructures? likelihood of being part of different arguments is quite
high.
To address this problem, we can mark all the nodes that descend from the target
argument node. We refer to this structure as a completely marked targetASTn (AST
cmt
n ).
ASTcmtn s may be seen as AST1s enriched with new information coming from the other
arguments (i. e., the non-marked subtrees). Note that if we only consider the AST1
subtree from a ASTcmtn , we obtain AST
cm
1 .
210
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
5.2 Structured Features for Conflict Resolution
This section describes structured features employed by the tree kernel?based conflict
resolution module of the SRL architecture described in Section 4.3. This subtask is
performed by means of:
1. A first annotation of potential arguments using a high recall boundary
classifier and, eventually, the role information provided by a role
multiclassifier (RM).
2. An ASTn classification step aiming at selecting, among the substructures
that do not contain overlaps, those that are more likely to encode the
correct argument set.
The set of argument nodes recognized by BC can be associated with a subtree of the
corresponding sentence parse, which can be classified using tree kernel functions. These
should evaluate whether a subtree encodes a correct predicate?argument structure or
not. As it encodes features from the whole predicate?argument structure, the ASTn that
we introduced in Section 4.2 is a structure that can be employed for this task.
Let Ap be the set of potential argument nodes for the predicate p output by BC; the
classifier examples are built as follows: (1) we look for node pairs ?n1,n2? ? Ap ? Ap
where n1 is the ancestor of n2 or vice versa; (2) we create two node sets A1 = A? {n1}
and A2 = A? {n2} and classify the two NSTs associated with A1 and A2 with the tree
kernel classifier to select the most correct set of argument boundaries. This procedure
can be generalized to a set of overlapping nodes Owith more than two elements, as we
simply need to generate all and only the permutations of A?s nodes that do not contain
overlapping pairs.
Figure 7 shows a working example of such amulti-stage classifier. In (Figure 7a), the
BC labels as potential arguments four nodes (circled), three of which are overlapping
Figure 7
An overlap situation (a) and the candidate solutions resulting from the employment of the
different marking strategies.
211
Computational Linguistics Volume 34, Number 2
(in bold circles). The overlap resolution algorithm proposes two solutions (Figure 7b)
of which only one is correct. In fact, according to the second solution, the preposi-
tional phrase of the book would incorrectly be attached to the verbal predicate, that
is, in contrast with the parse tree. The ASTn classifier, applied to the two NSTs,
should detect this inconsistency and provide the correct output. Figure 7 also high-
lights a critical problem the ASTn classifier has to deal with: as the two NSTs are
perfectly identical, it is not possible to distinguish between them using only their
fragments.
In order to engineer novel features, we simply add the boundary information pro-
vided by BC to the NSTs. We mark with a progressive number the phrase type cor-
responding to an argument node, starting from the leftmost argument. We call the
resulting structure an ordinal predicate?argument spanning tree (ASTordn ). For example,
in the first NST of Figure 7c, we mark as NP-0 and NP-1 the first and second argument
nodes, whereas in the second NST, we have a hypothesis of three arguments on three
nodes that we transform as NP-0, NP-1, and PP-2.
This simple modification enables the tree kernel to generate features useful for dis-
tinguishing between two identical parse trees associated with different argument struc-
tures. For example, for the first NST the fragments [NP-1 [NP PP]], [NP [DT NN]], and
[PP [IN NP]] are generated. They no longer match with the fragments of the second
NST [NP-0 [NP PP]], [NP-1 [DT NN]], and [PP-2 [IN NP]].
We also experimented with another structure, the marked predicate?argument
spanning tree (ASTmn ), in which each argument node is marked with a role label as-
signed by a role multi-classifier (RM). Of course, this model requires a RM to classify all
the nodes recognized by BC first. An example ASTmn is shown in Figure 7d.
5.3 Structures for Proposition Re-Ranking
In Section 4.4, we presented our re-ranking mechanism, which is inspired by the joint
inference model described in Haghighi, Toutanova, and Manning (2005). Designing
structured features for the re-ranking classifier is complex in many aspects. Unlike
the other structures that we have discussed so far, the defined mappings should:
(1) preserve as much information as possible about the whole predicate?argument
structure; (2) focus the learning algorithm on the whole structure; and (3) be able
to identify those small differences that distinguish more or less accurate labeling
schemes. Among the possible solutions that we have explored, three are especially
interesting in terms of accuracy improvement or linguistic properties, and are described
hereinafter.
The ASTcmn (completely marked ASTn, see Figure 8a) is an ASTn in which each
argument node label is enriched with the role assigned to the node by RM. The la-
bels of the descendants of each argument node are modified accordingly, down to
pre-terminal nodes. The ASTcmtn is a variant of AST
cm
n in which only the target is
marked. Marking a node descendant is meant to force substructures matching only
among homogeneous argument types. This representation should provide rich syn-
tactic and lexical information about the parse tree encoding the predicate?argument
structure.
The PAS (predicate?argument structure, see Figure 8b) is a completely different
structure that preserves the parse subtrees associated with each argument node while
discarding the intra-argument syntactic parse information. Indeed, the syntactic links
between the argument nodes are represented as a dummy 1-level tree, which appears
in any PAS and therefore does not influence the evaluation of similarity between pairs
212
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Figure 8
Different representations of the same proposition.
of structures. This structure accommodates the predicate and all the arguments of an
annotation in a sequence of seven slots.5 To each slot is attached an argument label to
which in turn is attached the subtree rooted in the argument node. The predicate is
represented by means of a pre-terminal node labeled rel to which the lemmatization of
the predicate word is attached as a leaf node. In general, a proposition consists of m
arguments, with m ? 6, where m varies according to the predicate and the context. To
guarantee that predicate structures with a different number of arguments are matched
in the SST kernel function, we attach a dummy descendant marked null to the slots not
filled by an argument.
The PAStl (type-only, lemmatized PAS, see Figure 8c) is a specialization of the PAS
that only focuses on the syntax of the predicate?argument structure, namely, the type
and relative position of each argument, minimizing the amount of lexical and syntactic
information derived from the parse tree. The differences with the PAS are that: (1) each
slot is attached to a pre-terminal node representing the argument type and a terminal
node whose label indicates the syntactic type of the argument; and (2) the predicate
word is lemmatized.
The next section presents the experiments used to evaluate the effectiveness of the
proposed canonical structures in SRL.
5 We assume that predicate?argument structures cannot be composed by more than six arguments, which
is generally true.
213
Computational Linguistics Volume 34, Number 2
6. Experiments
The experiments aim to measure the contribution and the effectiveness of our proposed
kernel engineering models and of the diverse structured features that we designed
(Section 5). From this perspective, the role of feature extraction functions is not
fundamental because the study carried out in Moschitti (2006a) strongly suggests that
the SST (Collins and Duffy 2002) kernel produces higher accuracy than the PT kernel
when dealing with constituent parse trees, which are adopted in our study.6 We then
selected the SST kernel and designed the following experiments:
(a) A study of canonical functions based on node marking for boundary detection
and argument classification, that is, ASTm1 (Section 6.2). Moreover, as the standard
features have shown to be effective, we combined them with ASTm1 based kernels on
the boundary detection and classification tasks (Section 6.2).
(b) We varied the amount of training data to demonstrate the higher generalization
ability of tree kernels (Section 6.3).
(c) Given the promising results of kernel engineering, we also applied it to solve a more
complex task, namely, conflict resolution in SRL annotations (see Section 6.4). As this
involves the complete predicate?argument structure, we could test advanced canonical
functions generating ASTn, AST
ord
n , and AST
m
n .
(d) Previous work has shown that re-ranking is very important in boosting the accuracy
of SRL. Therefore, we tested advanced canonical mappings, that is, those based on
ASTcmn , PAS, and PAS
tl, on such tasks (Section 6.5).
6.1 General Setup
The empirical evaluations were mostly carried out within the setting defined in the
CoNLL 2005 shared task (Carreras and Ma`rquez 2005). As a target data set, we
used the PropBank7 and the automatic Charniak parse trees of the sentences of Penn
TreeBank 2 corpus8 (Marcus, Santorini, and Marcinkiewicz 1993) from the CoNLL 2005
shared-task data.9 We employed the SVM-light-TK software10, which encodes fast tree
kernel evaluation (Moschitti 2006b), and combinations betweenmultiple feature vectors
and trees in the SVM-light software (Joachims 1999). We used the default regularization
parameter (option -c) and ? = 0.4 (see Moschitti [2004]).
6.2 Testing Canonical Functions Based on Node Marking
In these experiments, we measured the impact of node marking strategies on boundary
detection (BD) and the complete SRL task, that is, BD and role classification (RC). We
employed a configuration of the architecture described in Section 4 and previously
6 Of course the PT kernel may be much more accurate in processing PAS and PAStl because these are not
simply constituent parse trees. Nevertheless, a study of the PT kernel potential is beyond the purpose
of this article.
7 http://www.cis.upenn.edu/?ace.
8 http://www.cis.upenn.edu/?treebank.
9 http://www.lsi.upc.edu/?srlconll/.
10 http://ai-nlp.info.uniroma2.it/moschitti/.
214
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Table 2
Number of arguments (Arguments) and of unrecoverable arguments (Unrecoverable) due to
parse tree errors in Sections 2, 3, and 24 of the Penn TreeBank/PropBank.
Sec. Arguments Unrecoverable
2 198,373 454 (0.23%)
3 147,193 347 (0.24%)
24 139,454 731 (0.52%)
Table 3
Comparison between different models on Boundary Detection and the complete Semantic Role
Labeling tasks. The training set is constituted by the first 1 million instances from Sections 02?06
for the boundary classifier and all arguments from Sections 02?21 for the role multiclassifier
(253,129 instances). The performance is measured against Section 24 (149,140 instances).
Boundary Detection Semantic Role Labeling
Kernels P R F1 P R F1
AST1 75.75% 71.68% 73.66 64.71% 61.71% 63.17
ASTm1 77.32% 74.80% 76.04 66.58% 64.87% 65.71
Poly 82.18% 79.19% 80.66 75.86% 72.60% 73.81
Poly+AST1 81.74% 80.71% 81.22 74.23% 73.62% 73.92
Poly+ASTm1 81.64% 80.73% 81.18 74.36% 73.87% 74.11
adopted in Moschitti et al (2005b), in which the simple conflict resolution heuristic is
applied. The results were derived within the CoNLL setting by means of the related
evaluator.
In more detail, in the BD experiments, we used the first million instances from the
Penn TreeBank Sections 2?6 for training11 and Section 24 for testing. Our classification
model applied to this data replicates the results obtained in the CoNLL 2005 shared
task, that is, the highest accuracy in BD among the systems using only one parse
tree and one learning algorithm. For the complete SRL task, we used the previous
BC and all the available data, that is, the sections from 2 to 21, for training the role
multiclassifier.
It is worth mentioning that, as the automatic parse trees contain errors, some
arguments cannot be associated with any covering node; thus we cannot extract a
tree representation for them. In particular, Table 2 shows the number of arguments
(column 2) for sections 2, 3, and 24 as well as the number of arguments that we could not
take into account (Unrecoverable) due to the lack of parse tree nodes exactly covering
their word spans. Note how Section 24 of the Penn TreeBank (which is not part of the
Charniak training set) is much more affected by this problem.
Given this setting, the impact of node marking can be measured by comparing the
AST1 and the AST
m
1 based kernels. The results are reported in the rows AST1 and AST
m
1
of Table 3. Columns 2, 3, and 4 show their Precision, Recall, and F1 measure on BD and
columns 5, 6, and 7 report the performance on SRL. We note that marking the argument
11 This was the most expensive process in terms of training time, requiring more than one week.
215
Computational Linguistics Volume 34, Number 2
node simplifies the generalization process as it improves both tasks by about 3.5 and 2.5
absolute percentage points, respectively.
However, Row Poly shows that the polynomial kernel using state-of-the-art fea-
tures (Moschitti et al 2005b) outperforms ASTm1 by about 4.5 percentage points in BD
and 8 points in the SRL task. The main reason is that the employed tree structures
do not explicitly encode very important features like the passive voice or predicate
position. In Moschitti (2004), these are shown to be very effective especially when used
in polynomial kernels. Of course, it is possible to engineer trees including these and
other standard features with a canonical mapping, but the aim here is to provide new
interesting representations rather than to abide by the simple exercise of representing
already designed features within tree kernel functions. In other words, we follow the
idea presented in Moschitti (2004), where tree kernels were suggested as a means to
derive new features rather than generate a stand-alone feature set.
Rows Poly+AST1 and Poly+AST
m
1 investigate this possibility by presenting the
combination of polynomial and tree kernels. Unfortunately, the results on both BD and
SRL do not show enough improvement to justify the use of tree kernels; for example,
Poly+ASTm1 improves Poly by only 0.52 in BD and 0.3 in SRL. The small improvement
is intuitively due to the use of (1) a state-of-the-art model as a baseline and (2) a very
large amount of training data which decreases the contribution of tree features. In the
next section an analysis in terms of training data will shed some light on the role of tree
kernels for BD and RC in SRL.
6.3 The Role of Tree Kernels for Boundary Detection and Argument Classification
The previous section has shown that if a state-of-the-art model12 is adopted, then the
tree kernel contribution is marginal. On the contrary, if a non state-of-the-art model is
adopted tree kernels can play a significant role. To verify this hypothesis, we tested the
polynomial kernel over the standard feature vector proposed in Gildea and Jurafsky
(2002) obtaining an F1 of 67.3, which is comparable with the ASTm1 model, that is 65.71.
Moreover, a kernel combination produced a significant improvement of both models
reaching an F1 of 70.4.
Thus, the role of tree kernels relates to the design of features for novel linguistic
tasks for which the optimal data representation has not yet been developed. For exam-
ple, although SRL has been studied for many years and many effective features have
been designed, representations for languages like Arabic are still not very well under-
stood and raise challenges in the design of effective predicate?argument descriptions.
However, this hypothesis on the usefulness of tree kernels is not completely satis-
factory as the huge feature space produced by them should play a more important role
in predicate?argument representation. For example, the many fragments extracted by
an AST1 provide a very promising back-off model for the Path feature, which should
improve the generalization process of SVMs.
As back-off models show their advantages when the amount of training data
is small, we experimented with Poly, AST1, AST
m
1 , Poly+AST1, and Poly+AST
m
1 and
12 The adopted model is the same as used in Moschitti et al (2005b), which is the most accurate among the
systems that use a single learning model, a single source of syntactic information, and no accurate
inference mechanism. If tree kernels improved this basic model they would likely improve the
accuracy of more complex systems as well.
216
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
different bins of training data, starting from a very small set, namely, 10,000 instances
(1%) to 1 million (100%) of instances. The results from the BD classifiers and the
complete SRL task are very interesting and are illustrated by Figure 9. We note several
things.
First, Figure 9a shows that with only 1% of data (i.e., 640 arguments) as positive
examples, the F1 on BD of the ASTm1 kernel is surprisingly about 3 percentage points
higher than the one obtained by the polynomial kernel (Poly) (i. e., the state of the art).
When ASTm1 is combined with Poly the improvement reaches 5 absolute percentage
points. This suggests that tree kernels should always be used when small training data
sets are available.
Second, although the performance of AST1 is much lower than all the other models,
its combination with Poly produces results similar to Poly+ASTm1 , especially when
the amount of training data increases. This, in agreement with the back-off property,
indicates that the number of tree fragments is more relevant than their quality.
Third, Figure 9b shows that as we increase training data, the advantage of using
tree kernels decreases. This is rather intuitive as (i) in general less accurate data machine
learning models trained with enough data can reach the accuracy of the most accurate
models, and (ii) if the hypothesis that tree kernels provide back-off models is true, a lot
of training data makes them less critical, for example, the probability of finding the Path
feature of a test instance in the training set becomes high.
Figure 9
Learning curves for BD (a and b) and the SRL task (c and d), where 100% of data corresponds to
1 million candidate argument nodes for boundary detection and 64,000 argument nodes for role
classification.
217
Computational Linguistics Volume 34, Number 2
Table 4
Boundary detection accuracy (F1) on gold-standard parse trees and ambiguous structures
employing the different conflict resolution methodologies described in Section 4.3.
RND HEU ASTordn
73.13 71.50 91.11
Finally, Figures 9c and 9d show learning curves13 similar to Figures 9a and 9b, but
with a reduced impact of tree kernels on the Poly model. This is due to the reduced
impact of ASTm1 on role classification. Such findings are in agreement with the results
in Moschitti (2004), which show that for argument classification the SCF structure (a
variant of the ASTmn ) is more effective. Thus a comparison between learning curves of
Poly and SCF on RC may show a behavior similar to Poly and ASTm1 for BD.
6.4 Conflict Resolution Results
In these experiments, we are interested in (1) the evaluation of the accuracy of our
tree kernel?based conflict resolution strategy and (2) studying the most appropriate
structured features for the task.
A first evaluation was carried out over gold-standard Penn TreeBank parses and
PropBank annotations. We compared the alternative conflict resolution strategies imple-
mented by our architecture (see Section 4.3), namely the random (RND), the heuristic
(HEU), and a tree kernel?based disambiguator working with ASTordn structures. The
disambiguators were run on the output of BC, that is, without any information about the
candidate arguments? roles. BC was trained on Sections 2 to 7 with a high-recall linear
kernel. We applied it to classify Sections 8 to 21 and obtained 2,988 NSTs containing at
least one overlapping node. These structures generated 3,624 positive NSTs (i. e., correct
structures) and 4,461 negative NSTs (incorrect structures) in which no overlap is present.
We used them to train the ASTordn classifier. The F1 measure on the boundary detection
task was evaluated on the 385 overlapping annotations of Section 23, consisting of 642
argument and 15,408 non-argument nodes.
The outcome of this experiment is summarized in Table 4. We note two points.
(1) The RND disambiguator (slightly) outperforms the HEU. This suggests that the
heuristics that we implemented were inappropriate for solving the problem. It also
underlines how difficult it is to explicitly choose the aspects that are relevant for a
complex, non-local task such as overlap resolution. (2) The ASTordn classifier outperforms
the other strategies by about 20 percentage points, that is, 91.11 vs. 73.13 and 71.50.
This datum along with the previous one is a good demonstration of how tree kernels
can be effectively exploited to describe phenomena whose relevant features are largely
unknown or difficult to represent explicitly. It should be noted that a more accurate
baseline can be provided by using the Viterbi-style search (see Section 4.4.1). However,
the experiments in Section 6.5 show that the heuristics produce the same accuracy (at
least when the complete task is carried out).
13 Note that using all training data, all the models reach lower F1s than the respective values shown in
Table 3. This happens because the data for training the role multiclassifier is restricted to the first
million instances, in other words, about 64,000 out of the total 253,129 arguments.
218
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
Table 5
SRL accuracy on different PropBank target sections in terms of F1 measure of the different
structured features employed for conflict resolution.
Target section ASTn AST
ord
n AST
m
n
21 73.7 77.3 78.7
23 68.9 71.2 72.1
These experiments suggest that tree kernels are promising methods for resolving
annotation conflicts; thus, we tried to also select the most representative structured
features (i. e., ASTn, AST
ord
n , or AST
m
n ) when automatic parse trees are used. We trained
BC on Sections 2?8, whereas, to achieve a very accurate argument classifier, we trained
a role multi-classifier (RM) on Sections 2?21. Then, we trained the ASTn, AST
ord
n , and
ASTmn classifiers on the output of BC. To test BC, RM, and the tree kernel classifiers, we
ran two evaluations on Section 23 and Section 21.14
Table 5 shows the F1 measure for the different tree kernels (columns 2, 3, and 4) for
conflict resolution over the NSTs of Sections 21 and 23. Several points should be noted.
(1) The general performance is much lower than that achieved on gold-standard
trees, as shown in Table 4. This datum and the gap of about 6 percentage points between
Sections 21 and 23 confirm the impact of parsing accuracy on the subtasks of the SRL
process.
(2) The ordinal numbering of arguments (ASTordn ) and the role type information
(ASTmn ) provide tree kernels with more meaningful fragments because they improve
the basic model by about 4 percentage points.
(3) The deeper semantic information generated by the argument labels provides
useful clues for selecting correct predicate?argument structures because the ASTmn
model improves ASTordn performance on both sections.
6.5 Proposition Re-Ranking Results
In these experiments, Section 23 was used for testing our proposition re-ranking. We
employed a BC trained on Sections 2 to 8, whereas RMwas trained on Sections 2?12.15 In
order to provide a probabilistic interpretation of the SVM output (see Section 4.4.1), we
evaluated each classifier distribution parameter based on its output on Section 12. For
computational complexity reasons, we decided to consider the five most likely labelings
for each node and the five first alternatives output by the Viterbi algorithm (i. e., m = 5
and n = 5).
With this set-up, we evaluated the accuracy lower and upper bounds of our system.
As our baseline, we consider the accuracy of a re-ranker that always chooses the first
alternative output from the Viterbi algorithm, that is, the most likely according to the
joint inference model. This accuracy has been measured as 75.91 F1 percentage points;
this is practically identical to the 75.89 obtained by applying heuristics to remove
overlaps generated by BC.
14 As Section 21 of the Penn TreeBank is part of the Charniak parser training set, the performance derived
on its parse trees represents an upper bound for our classifiers, i. e., the results using a nearly ideal
syntactic parser and role multiclassifier.
15 In these experiments we did not use tree kernels for BC and RM as we wanted to measure the impact of
tree kernels only on the re-ranking stage.
219
Computational Linguistics Volume 34, Number 2
This does not depend on the bad quality of the five top labelings. Indeed, we
selected the best alternative produced by the Viterbi algorithm according to the gold-
standard score, and we obtained an F1 of 84.76 for n = 5. Thus, the critical aspect resides
in the selection of the best annotations, which should be carried out by an automatic
re-ranker.
Rows 2 and 3 of Table 6 show the number of distinct propositions and alternative
annotations output by the Viterbi algorithm for each of the employed sections. In row
3, the number of pair comparisons (i. e., the number of training/test examples for the
classifier) is shown.
Using this data, we carried out a complete SRL experiment, which is summarized in
Table 7. First, we compared the accuracy of the ASTcmn , PAS, and PAS
tlclassifiers trained
on Section 24 (in row 3, columns 2, 3, and 4) and discovered that the latter structure
produces a noticeable F1 improvement, namely, 78.15 vs. 76.47 and 76.77, whereas the
accuracy gap between the PAS and the ASTcmn classifiers is very small, namely, 76.77
vs. 76.47 percentage points. We selected the most interesting structured feature, that
is, the PAStl, and extended it with the local (to each argument node) standard features
commonly employed for the boundary detection and argument classification tasks, as
in Haghighi, Toutanova, and Manning (2005). This richer kernel (PAStl+STD, column 5)
was compared with the PAStl one. The comparison was performed on two different
training sets (rows 2 and 3): In both cases, the introduction of the standard features
produced a performance decrement, most notably in the case of Section 12 (i. e., 82.07
vs. 75.06). Our best re-ranking kernel (i. e., the PAStl) was then employed in a larger
experiment, using both Sections 12 and 24 for testing (row 4), achieving an F1 measure
of 78.44.
First, we note that the accuracy of the ASTcmn and PAS classifiers is very similar (i. e.,
76.77 vs. 76.47). This datum suggests that the intra-argument syntactic information is
not critical for the re-ranking task, as including it or not in the learning algorithm does
not lead to noticeable differences.
Second, we note that the PAStl kernel is much more effective than those based on
ASTcmn and PAS, which are always outperformed. This may be due to the fact that
Table 6
Number of propositions, alternative annotations (as output by the Viterbi algorithm), and pair
comparisons (i. e., re-ranker input examples) for the PropBank sections used for the experiments.
Section 12 Section 23 Section 24
Propositions 4,899 5,267 3,248
Alternatives 24,494 26,325 16,240
Comparisons 74,650 81,162 48,582
Table 7
Summary of the proposition re-ranking experiments with different training sets.
Training Section ASTcmn PAS PAS
tl PAStl+STD
12 ? ? 78.27 77.61
24 76.47 76.77 78.15 77.77
12+24 ? ? 78.44 ?
220
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
two ASTcmn s (or PASs) always share a large number of substructures, because most
alternative annotations tend to be very similar and the small differences among them
only affect a small part of the encoding of syntactic information; on the other hand,
the small amount of local parsing information encoded in the PAStls enables a good
generalization process.
Finally, the introduction of the standard, local standard features in our re-ranking
model caused a performance loss of about 0.5 percentage points on both Sections 12 and
24. This fact, which is in contrast with what has been shown in Haghighi, Toutanova,
and Manning (2005), might be the consequence of the small training sets that we
employed. Indeed, local standard features tend to be very sparse and their effectiveness
should be evaluated against a larger data set.
7. Discussions and Conclusions
The design of automatic systems for the labeling of semantic roles requires the solution
of complex problems. Among other issues, feature engineering is made difficult by the
structured nature of the data, that is, features should represent information expressed
by automatically generated parse trees. This raises two main problems: (1) the mod-
eling of effective features, partially solved for some subtasks in previous works, and
(2) the implementation of the software for the extraction of a large number of such
features.
A system completely (or largely) based on tree kernels alleviates both problems
as (1) kernel functions automatically generate features and (2) only a procedure for the
extraction of subtrees is needed. Although some of themanually designed features seem
to be superior to those derived with tree kernels, their combination still seems worth
applying. Moreover, tree kernels provide a back-off model that greatly outperforms
state-of-the-art SRL models when the amount of training data is small.
To demonstrate these points, we carried out a comprehensive study of the use of
tree kernels for semantic role labeling by designing several canonical mappings. These
correspond to the application of innovative tree kernel engineering techniques tailored
to different stages of an SRL process. The experiments with these methods and SVMs on
the data set provided by the CoNLL 2005 shared task (Carreras andMa`rquez 2005) show
that, first, tree kernels are a valid support tomanually designed features for many stages
of the SRL process. We have shown that our improved tree kernel (i.e., the one based
on ASTm1 ) highly improves accuracy in both boundary detection and the SRL task when
the amount of training data is small (e.g., 5 absolute percentage points over a state-of-
the-art boundary classifier). In the case of argument classification the improvement is
less evident but still consistent, at about 3%.
Second, appropriately engineered tree kernels can replace standard features in
many SRL subtasks. For example, in complex tasks such as conflict resolution or re-
ranking, they provide an easy way to build new features that would be difficult to
describe explicitly. More generally, tree kernels can be used to combine different sources
of information for the design of complex learning models.
Third, in the specific re-ranking task, our structured features show a noticeable im-
provement over our baseline (i. e., about 2.5 percentage points). This could be increased
considering that we have not been able to fully exploit the potential of our re-ranking
model, whose theoretical upper bound is 6 percentage points away. Still, although we
only used a small fraction of the available training data (i. e., only 2 sections out of 22
were used to train the re-ranker) our system?s accuracy is in line with state-of-the-art
systems (Carreras and Ma`rquez 2005) that do not employ tree kernels.
221
Computational Linguistics Volume 34, Number 2
Finally, although the study carried out in this article is quite comprehensive, several
issues should be considered in more depth in the future:
(a) The tree feature extraction functions ST, SST, and PT should be studied in com-
bination with the proposed canonical mappings. For example, as the PT kernel seems
more suitable for the processing of dependency information, it would be interesting
to apply it in an architecture using these kinds of syntactic parse trees (e. g., Chen
and Rambow 2003). In particular, the combination of different extraction functions on
different syntactic views may lead to very good results.
(b) Once the set of the most promising kernels is established, it would be interesting
to use all the available CoNLL 2005 data. This would allow us to estimate the potential
of our approach by comparing it with previous work on a fairer basis.
(c) The use of fast tree kernels (Moschitti 2006a) along with the proposed tree repre-
sentations makes the learning and classification much faster, so that the overall running
time is comparable with polynomial kernels. However, when used with SVMs their
running time on very large data sets (e. g., millions of instances) becomes prohibitive.
Exploiting tree kernel?derived features in a more efficient way (e. g., by selecting the
most relevant fragments and using them in an explicit space) is thus an interesting
line of future research. Note that such fragments would be the product of a reverse
engineering process useful to derive linguistic insights on semantic role theory.
(d) As CoNLL 2005 (Punyakanok et al 2005) has shown that multiple parse trees
provide the most important boost to the accuracy of SRL systems, we would like to
extend our model to work with multiple syntactic views of each input sentence.
Acknowledgments
This article is the result of research on kernel
methods for Semantic Role Labeling which
started in 2003 and went under the review of
several program committees of different
scientific communities, from which it highly
benefitted. In this respect, we would like to
thank the reviewers of the SRL special issue
as well as those of the ACL, CoNLL, EACL,
ECAI, ECML, HLT-NAACL, and ICML
conferences. We are indebted to Silvia
Quarteroni for her help in reviewing the
English formulation of an earlier version of
this article.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In COLING-ACL ?98:
Proceedings of the Conference, pages 86?90,
Montre?al, Canada.
Carreras, Xavier and Llu??s Ma`rquez.
2004. Introduction to the CoNLL-2004
shared task: Semantic role labeling.
In HLT-NAACL 2004 Workshop: Eighth
Conference on Computational Natural
Language Learning (CoNLL-2004),
pages 89?97, Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez.
2005. Introduction to the CoNLL-2005
shared task: Semantic role labeling.
In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 152?164,
Ann Arbor, MI.
Chen, John and Owen Rambow. 2003.
Use of deep linguistic features for
the recognition and labeling of
semantic arguments. In Proceedings
of the 2003 Conference on Empirical
Methods in Natural Language Processing,
pages 41?48, Sapporo, Japan.
Collins, Michael and Nigel Duffy. 2002.
New ranking algorithms for parsing and
tagging: Kernels over discrete structures,
and the voted perceptron. In ACL02,
pages 263?270, Philadelphia, PA.
Culotta, Aron and Jeffrey Sorensen. 2004.
Dependency tree kernels for relation
extraction. In ACL04, pages 423?429,
Barcelona, Spain.
Cumby, Chad and Dan Roth. 2003. Kernel
methods for relational learning. In
Proceedings of ICML 2003, pages 107?114,
Washington, DC.
Fillmore, Charles J. 1968. The case for case. In
Emmon Bach and Robert T. Harms,
222
Moschitti, Pighin, and Basili Tree Kernels for Semantic Role Labeling
editors, Universals in Linguistic Theory.
Holt, Rinehart, and Winston, New York,
pages 1?210.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3): 245?288.
Haghighi, Aria, Kristina Toutanova, and
Christopher Manning. 2005. A joint model
for semantic role labeling. In Proceedings of
the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 173?176, Ann Arbor, MI.
Jackendoff, Ray. 1990. Semantic Structures,
Current Studies in Linguistics Series. The
MIT Press, Cambridge, MA.
Joachims, Thorsten. 1999. Making large-scale
SVM learning practical. In B. Scho?lkopf,
C. Burges, and A. Smola, editors,
Advances in Kernel Methods?Support Vector
Learning. MIT Press, Cambridge, MA,
pages 169?184.
Kazama, Jun?ichi and Kentaro Torisawa.
2005. Speeding up training with tree
kernels for node relation labeling.
In Proceedings of EMNLP 2005,
pages 137?144, Toronto, Canada.
Kudo, Taku and Yuji Matsumoto. 2003. Fast
methods for kernel-based text analysis. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 24?31, Sapporo, Japan.
Levin, Beth. 1993. English Verb Classes
and Alternations. The University of
Chicago Press, Chicago, IL.
Lin, H.-T., C.-J. Lin, and R. C. Weng. 2003.
A note on Platt?s probabilistic outputs
for support vector machines. Technical
report, National Taiwan University.
Litkowski, Kenneth. 2004. Senseval-3 task:
Automatic labeling of semantic roles.
In Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 9?12,
Barcelona, Spain.
Marcus, M. P., B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large
annotated corpus of English: The Penn
treebank. Computational Linguistics,
19:313?330.
Moschitti, Alessandro. 2004. A study
on convolution kernels for shallow
semantic parsing. In Proceedings of
the 42th Conference on Association for
Computational Linguistic (ACL-2004),
pages 335?342, Barcelona, Spain.
Moschitti, Alessandro. 2006a. Efficient
convolution kernels for dependency
and constituent syntactic trees.
In Proceedings of The 17th European
Conference on Machine Learning,
pages 318?329, Berlin, Germany.
Moschitti, Alessandro. 2006b. Making tree
kernels practical for natural language
learning. In Proceedings of 11th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL2006),
pages 113?120, Treato, Italy.
Moschitti, Alessandro, Bonaventura
Coppola, Daniele Pighin, and Roberto
Basili. 2005a. Engineering of syntactic
features for shallow semantic parsing.
In Proceedings of the ACL Workshop on
Feature Engineering for Machine Learning in
Natural Language Processing, pages 48?56,
Ann Arbor, MI.
Moschitti, Alessandro, Ana-Maria Giuglea,
Bonaventura Coppola, and Roberto
Basili. 2005b. Hierarchical semantic
role labeling. In Proceedings of the
Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 201?204, Ann Arbor, MI.
Moschitti, Alessandro, Daniele Pighin,
and Roberto Basili. 2006. Tree kernel
engineering in semantic role labeling
systems. In Proceedings of the Workshop on
Learning Structured Information in Natural
Language Applications, EACL 2006,
pages 49?56, Trento, Italy.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1): 71?106.
Platt, J. 1999. Probabilistic outputs
for support vector machines and
comparison to regularized likelihood
methods. In A. J. Smola, P. Bartlett,
B. Schoelkopf, and D. Schuurmans,
editors, Advances in Large Margin
Classifiers. MIT Press, Cambridge, MA,
pages 61?74.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Daniel Jurafsky. 2005a. Support
vector learning for semantic argument
classification.Machine Learning,
60(1?3):11?39.
Pradhan, Sameer, Kadri Hacioglu, Wayne
Ward, James H. Martin, and Daniel
Jurafsky. 2005b. Semantic role chunking
combining complementary syntactic
views. In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 217?220,
Ann Arbor, MI.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Daniel
Jurafsky. 2005c. Semantic role labeling
223
Computational Linguistics Volume 34, Number 2
using different syntactic views. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 581?588,
Ann Arbor, MI.
Pradhan, Sameer S., Wayne H. Ward,
Kadri Hacioglu, James H. Martin, and
Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines.
In HLT-NAACL 2004: Main Proceedings,
pages 233?240, Boston, MA.
Punyakanok, Vasin, Peter Koomen,
Dan Roth, and Wen-tau Yih. 2005.
Generalized inference with multiple
semantic role labeling systems. In
Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 181?184,
Ann Arbor, MI.
Shawe-Taylor, John and Nello Cristianini.
2004. Kernel Methods for Pattern
Analysis. Cambridge University Press,
Cambridge, UK.
Shen, Libin, Anoop Sarkar, and Aravind K.
Joshi. 2003. Using LTAG based features in
parse reranking. In Empirical Methods for
Natural Language Processing (EMNLP),
pages 89?96, Sapporo, Japan.
Thompson, Cynthia A., Roger Levy, and
Christopher Manning. 2003. A generative
model for semantic role labeling. In 14th
European Conference on Machine Learning,
pages 397?408, Cavtat, Croatia.
Tjong Kim Sang, Erik, Sander Canisius,
Antal van den Bosch, and Toine Bogers.
2005. Applying spelling error correction
techniques for improving semantic
role labelling. In Proceedings of the
Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 229?232, Ann Arbor, MI.
Toutanova, Kristina, Aria Haghighi, and
Christopher Manning. 2005. Joint learning
improves semantic role labeling. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 589?596,
Ann Arbor, MI.
Toutanova, Kristina, Penka Markova, and
Christopher Manning. 2004. The leaf path
projection view of parse trees: Exploring
string kernels for HPSG parse selection. In
Proceedings of EMNLP 2004, pages 166?173,
Barcelona, Spain.
Vapnik, Vladimir N. 1998. Statistical Learning
Theory. John Wiley and Sons, New York.
Vishwanathan, S. V. N. and A. J. Smola.
2002. Fast kernels on strings and trees.
In Proceedings of Neural Information
Processing Systems, pages 569?576,
Vancouver, British Columbia.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of EMNLP 2004,
pages 88?94, Barcelona, Spain.
Zelenko, D., C. Aone, and A. Richardella.
2003. Kernel methods for relation
extraction. Journal of Machine Learning
Research, 3:1083?1106.
Zhang, Min, Jie Zhang, and Jian Su. 2006.
Exploring syntactic features for relation
extraction using a convolution tree
kernel. In Proceedings of the Human
Language Technology Conference of the
NAACL, Main Conference, pages 288?295,
New York, NY.
224
Proceedings of ACL-08: HLT, pages 798?806,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semantic Role Labeling Systems for Arabic using Kernel Methods
Mona Diab
CCLS, Columbia University
New York, NY 10115, USA
mdiab@ccls.columbia.edu
Alessandro Moschitti
DISI, University of Trento
Trento, I-38100, Italy
moschitti@disi.unitn.it
Daniele Pighin
FBK-irst; DISI, University of Trento
Trento, I-38100, Italy
pighin@fbk.eu
Abstract
There is a widely held belief in the natural lan-
guage and computational linguistics commu-
nities that Semantic Role Labeling (SRL) is
a significant step toward improving important
applications, e.g. question answering and in-
formation extraction. In this paper, we present
an SRL system for Modern Standard Arabic
that exploits many aspects of the rich mor-
phological features of the language. The ex-
periments on the pilot Arabic Propbank data
show that our system based on Support Vector
Machines and Kernel Methods yields a global
SRL F1 score of 82.17%, which improves the
current state-of-the-art in Arabic SRL.
1 Introduction
Shallow approaches to semantic processing are mak-
ing large strides in the direction of efficiently and
effectively deriving tacit semantic information from
text. Semantic Role Labeling (SRL) is one such ap-
proach. With the advent of faster and more power-
ful computers, more effective machine learning al-
gorithms, and importantly, large data resources an-
notated with relevant levels of semantic information,
such as the FrameNet (Baker et al, 1998) and Prob-
Bank (Kingsbury and Palmer, 2003), we are seeing
a surge in efficient approaches to SRL (Carreras and
Ma`rquez, 2005).
SRL is the process by which predicates and their
arguments are identified and their roles are defined
in a sentence. For example, in the English sen-
tence, ?John likes apples.?, the predicate is ?likes?
whereas ?John? and ?apples?, bear the semantic role
labels agent (ARG0) and theme (ARG1). The cru-
cial fact about semantic roles is that regardless of
the overt syntactic structure variation, the underly-
ing predicates remain the same. Hence, for the sen-
tence ?John opened the door? and ?the door opened?,
though ?the door? is the object of the first sentence
and the subject of the second, it is the ?theme? in
both sentences. Same idea applies to passive con-
structions, for example.
There is a widely held belief in the NLP and com-
putational linguistics communities that identifying
and defining roles of predicate arguments in a sen-
tence has a lot of potential for and is a significant
step toward improving important applications such
as document retrieval, machine translation, question
answering and information extraction (Moschitti et
al., 2007).
To date, most of the reported SRL systems are for
English, and most of the data resources exist for En-
glish. We do see some headway for other languages
such as German and Chinese (Erk and Pado, 2006;
Sun and Jurafsky, 2004). The systems for the other
languages follow the successful models devised for
English, e.g. (Gildea and Jurafsky, 2002; Gildea and
Palmer, 2002; Chen and Rambow, 2003; Thompson
et al, 2003; Pradhan et al, 2003; Moschitti, 2004;
Xue and Palmer, 2004; Haghighi et al, 2005). In the
same spirit and facilitated by the release of the Se-
mEval 2007 Task 18 data1, based on the Pilot Arabic
Propbank, a preliminary SRL system exists for Ara-
bic2 (Diab and Moschitti, 2007; Diab et al, 2007a).
However, it did not exploit some special character-
istics of the Arabic language on the SRL task.
In this paper, we present an SRL system for MSA
that exploits many aspects of the rich morphological
features of the language. It is based on a supervised
model that uses support vector machines (SVM)
technology (Vapnik, 1998) for argument boundary
detection and argument classification. It is trained
and tested using the pilot Arabic Propbank data re-
leased as part of the SemEval 2007 data. Given the
lack of a reliable Arabic deep syntactic parser, we
1http://nlp.cs.swarthmore.edu/semeval/
2We use Arabic to refer to Modern Standard Arabic (MSA).
798
use gold standard trees from the Arabic Tree Bank
(ATB) (Maamouri et al, 2004).
This paper is laid out as follows: Section 2
presents facts about the Arabic language especially
in relevant contrast to English; Section 3 presents
the approach and system adopted for this work; Sec-
tion 4 presents the experimental setup, results and
discussion. Finally, Section 5 draws our conclu-
sions.
2 Arabic Language and Impact on SRL
Arabic is a very different language from English in
several respects relevant to the SRL task. Arabic is a
semitic language. It is known for its templatic mor-
phology where words are made up of roots and af-
fixes. Clitics agglutinate to words. Clitics include
prepositions, conjunctions, and pronouns.
In contrast to English, Arabic exhibits rich mor-
phology. Similar to English, Arabic verbs explic-
itly encode tense, voice, Number, and Person fea-
tures. Additionally, Arabic encodes verbs with Gen-
der, Mood (subjunctive, indicative and jussive) in-
formation. For nominals (nouns, adjectives, proper
names), Arabic encodes syntactic Case (accusative,
genitive and nominative), Number, Gender and Def-
initeness features. In general, many of the morpho-
logical features of the language are expressed via
short vowels also known as diacritics3 .
Unlike English, syntactically Arabic is a pro-drop
language, where the subject of a verb may be im-
plicitly encoded in the verb morphology. Hence, we
observe sentences such as ?A?KQ. ? @ ?? @ Akl AlbrtqAl
?ate-[he] the-oranges?, where the verb Akl encodes
the third Person Masculine Singular subject in the
verbal morphology. It is worth noting that in the
ATB 35% of all sentences are pro-dropped for sub-
ject (Maamouri et al, 2006). Unless the syntactic
parse is very accurate in identifying the pro-dropped
case, identifying the syntactic subject and the under-
lying semantic arguments are a challenge for such
pro-drop cases.
Arabic syntax exhibits relative free word order.
Arabic allows for both subject-verb-object (SVO)
and verb-subject-object (VSO) argument orders.4 In
3Diacritics encode the vocalic structure, namely the short
vowels, as well as the gemmination marker for consonantal dou-
bling, among other markers.
4MSA less often allows for OSV, or OVS.
the VSO constructions, the verb agrees with the syn-
tactic subject in Gender only, while in the SVO con-
structions, the verb agrees with the subject in both
Number and Gender. Even though, in the ATB, an
equal distribution of both VSO and SVO is observed
(each appearing 30% of the time), it is known that
in general Arabic is predominantly in VSO order.
Moreover, the pro-drop cases could effectively be
perceived as VSO orders for the purposes of SRL.
Syntactic Case is very important in the cases of VSO
and pro-drop constructions as they indicate the syn-
tactic roles of the object arguments with accusative
Case. Unless the morphology of syntactic Case is
explicitly present, such free word order could run
the SRL system into significant confusion for many
of the predicates where both arguments are semanti-
cally of the same type.
Arabic exhibits more complex noun phrases than
English mainly to express possession. These con-
structions are known as idafa constructions. Mod-
ern standard Arabic does not have a special parti-
cle expressing possession. In these complex struc-
tures a surface indefinite noun (missing an explicit
definite article) may be followed by a definite noun
marked with genitive Case, rendering the first noun
syntactically definite. For example, I
J. ? @ ?g. P rjl
Albyt ?man the-house? meaning ?man of the house?,
?g. P becomes definite. An adjective modifying the
noun ?g. P will have to agree with it in Number,
Gender, Definiteness, and Case. However, with-
out explicit morphological encoding of these agree-
ments, the scope of the arguments would be con-
fusing to an SRL system. In a sentence such as
?K
???@ I
J. ? @ ?g. P rjlu Albyti AlTwylu meaning ?the
tall man of the house?: ?man? is definite, masculine,
singular, nominative, corresponding to Definiteness,
Gender, Number and Case, respectively; ?the-house?
is definite, masculine, singular, genitive; ?the-tall? is
definite, masculine, singular, nominative. We note
that ?man? and ?tall? agree in Number, Gender, Case
and Definiteness. Syntactic Case is marked using
short vowels u, and i at the end of the word. Hence,
rjlu and AlTwylu agree in their Case ending5 With-
out the explicit marking of the Case information,
5The presence of the Albyti is crucial as it renders rjlu defi-
nite therefore allowing the agreement with AlTwylu to be com-
plete.
799
SVP
VBDpredicate
@YK.
started
NPARG0
NP
NN
?
KP
president
NP
NN
Z @P 	P??@
ministers
JJ
?

	?J
??@
Chinese
NP
NNP
? 	P
Zhu
NNP
?
m.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 133?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
CUNIT: A Semantic Role Labeling System for Modern Standard Arabic
Mona Diab
Columbia University
mdiab@cs.columbia.edu
Alessandro Moschitti
University of Trento, DIT
moschitti@dit.unitn.it
Daniele Pighin
FBK-irst; University of Trento, DIT
pighin@itc.it
Abstract
In this paper, we present a system for Ara-
bic semantic role labeling (SRL) based on
SVMs and standard features. The system is
evaluated on the released SEMEVAL 2007
development and test data. The results show
an F?=1 score of 94.06 on argument bound-
ary detection and an overall F?=1 score of
81.43 on the complete semantic role label-
ing task using gold parse trees.
1 Introduction
There is a widely held belief in the computational
linguistics field that identifying and defining the
roles of predicate arguments, semantic role label-
ing (SRL), in a sentence has a lot of potential for
and is a significant step towards the improvement of
important applications such as document retrieval,
machine translation, question answering and infor-
mation extraction. However, effective ways for see-
ing this belief come to fruition require a lot more
research investment.
Since most of the available data resources are for
the English language, most of the reported SRL sys-
tems to date only deal with English. Nevertheless,
we do see some headway for other languages, such
as German and Chinese (Erk and Pado, 2006; Sun
and Jurafsky, 2004; Xue and Palmer, 2005). The
systems for non-English languages follow the suc-
cessful models devised for English, e.g. (Gildea and
Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et
al., 2003). However, no SRL system exists for Ara-
bic.
In this paper, we present a system for semantic
role labeling for modern standard Arabic. To our
knowledge, it is the first SRL system for a semitic
language in the literature. It is based on a supervised
model that uses support vector machines (SVM)
technology for argument boundary detection and ar-
gument classification. It is trained and tested using
the pilot Arabic PropBank data released as part of
the SEMEVAL 2007 data. Given the lack of a re-
liable deep syntactic parser, in this research we use
gold trees.
The system yields an F-score of 94.06 on the sub
task of argument boundary detection and an F-score
of 81.43 on the complete task, i.e. boundary plus
classification.
2 SRL system for Arabic
The design of an optimal model for an Arabic SRL
systems should take into account specific linguis-
tic aspects of the language. However, a remarkable
amount of research has already been done in SRL
and we can capitalize from it to design a basic and
effective SRL system. The idea is to use the technol-
ogy developed for English and verify if it is suitable
for Arabic.
Our adopted SRL models use Support Vector Ma-
chines (SVM) to implement a two steps classifica-
tion approach, i.e. boundary detection and argument
classification. Such models have already been in-
vestigated in (Pradhan et al, 2003; Moschitti et al,
2005) and their description is hereafter reported.
2.1 Predicate Argument Extraction
The extraction of predicative structures is carried out
at the sentence level. Given a predicate within a
natural language sentence, its arguments have to be
properly labeled. This problem is usually divided
in two subtasks: (a) the detection of the boundaries,
i.e. the word spans of the arguments, and (b) the
classification of their type, e.g. Arg0 and ArgM in
133
SNP
NN
  
 /project
NP
NNP

  	/nations
JJ


 




	/United
VP
VBP




 /instated
NP
NN



 /grace-period
JJ




  Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 288?291,
Prague, June 2007. c?2007 Association for Computational Linguistics
RTV: Tree Kernels for Thematic Role Classification
Daniele Pighin
FBK-irst; University of Trento, DIT
pighin@itc.it
Alessandro Moschitti
University of Trento, DIT
moschitti@dit.unitn.it
Roberto Basili
University of Rome Tor Vergata, DISP
basili@info.uniroma2.it
Abstract
We present a simple, two-steps supervised
strategy for the identification and classifica-
tion of thematic roles in natural language
texts. We employ no external source of in-
formation but automatic parse trees of the in-
put sentences. We use a few attribute-value
features and tree kernel functions applied to
specialized structured features. The result-
ing system has an F1 of 75.44 on the Se-
mEval2007 closed task on semantic role la-
beling.
1 Introduction
In this paper we present a system for the labeling
of semantic roles that produces VerbNet (Kipper et
al., 2000) like annotations of free text sentences us-
ing only full syntactic parses of the input sentences.
The labeling process is modeled as a cascade of two
distinct classification steps: (1) boundary detection
(BD), in which the word sequences that encode a
thematic role for a given predicate are recognized,
and (2) role classification (RC), in which the type
of thematic role with respect to the predicate is as-
signed. After role classification, a set of simple
heuristics are applied in order to ensure that only
well formed annotations are output.
We designed our system on a per-predicate basis,
training one boundary classifier and a battery of role
classifiers for each predicate word. We clustered all
the senses of the same verb together and ended up
with 50 distinct boundary classifiers (one for each
target predicate word) and 619 role classifiers to rec-
ognize the 47 distinct role labels that appear in the
training set.
The remainder of this paper is structured as fol-
lows: Section 2 describes in some detail the archi-
tecture of our labeling system; Section 3 describes
the features that we use to represent the classifier
examples; Section 4 describes the experimental set-
ting and reports the accuracy of the system on the
SemEval2007 semantic role labeling closed task; fi-
nally, Section 5 discusses the results and presents
our conclusions.
2 System Description
Given a target predicate word in a natural language
sentence, a SRL system is meant to correctly iden-
tify all the arguments of the predicate. This problem
is usually divided in two sub-tasks: (a) the detection
of the boundaries (i. e. the word span) of each argu-
ment and (b) the classification of the argument type,
e.g. Arg0 or ArgM in PropBank or Agent and Goal
in FrameNet or VerbNet.
The standard approach to learn both the detection
and the classification of predicate arguments is sum-
marized by the following steps:
1 Given a sentence from the training-set, gener-
ate a full syntactic parse-tree;
2 let P and A be the set of predicates and the
set of parse-tree nodes (i.e. the potential argu-
ments), respectively;
3 for each pair ?p, a? ? P ?A:
3.1 extract the feature representation set, Fp,a;
3.2 if the sub-tree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
For instance, in Figure 1.a, for each combination
of the predicate approve with any other tree node a
288
that does not overlap with the predicate, a classifier
example Fapprove,a is generated. If a exactly covers
one of the predicate arguments (in this case: ?The
charter?, ?by the EC Commission? or ?on Sept. 21?)
it is regarded as a positive instance, otherwise it will
be a negative one, e. g. Fapprove,(NN charter).
The T+ and T? sets are used to train the bound-
ary classifier. To train the role multi-class classifier,
T+ can be reorganized as positive T+argi and nega-
tive T?argi examples for each argument i. In this way,
an individual ONE-vs-ALL classifier for each argu-
ment i can be trained. We adopted this solution, ac-
cording to (Pradhan et al, 2005), since it is simple
and effective. In the classification phase, given an
unseen sentence, all its Fp,a are generated and clas-
sified by each individual role classifier. The role la-
bel associated with the maximum among the scores
provided by the individual classifiers is eventually
selected.
To make the annotations consistent with the un-
derlying linguistic model, we employ a few simple
heuristics to resolve the overlap situations that may
occur, e. g. both ?charter? and ?the charter? in Figure
1 may be assigned a role:
? if more than two nodes are involved, i. e. a node
d and two or more of its descendants ni are
classified as arguments, then assume that d is
not an argument. This choice is justified by pre-
vious studies (Moschitti et al, 2006b) showing
that the accuracy of classification is higher for
lower nodes;
? if only two nodes are involved, i. e. they dom-
inate each other, then keep the one with the
highest classification score.
3 Features for Semantic Role Labeling
We explicitly represent as attribute-value pairs the
following features of each Fp,a pair:
? Phrase Type, Predicate Word, Head Word, Po-
sition and Voice as defined in (Gildea and Juras-
fky, 2002);
? Partial Path, No Direction Path, Head Word
POS, First and Last Word/POS in Constituent
and SubCategorization as proposed in (Pradhan
et al, 2005);
a) S
NP
DT
The
NN
charter
VP
AUX
was
VP
VBN
approved
PP
IN
by
NP
DT
the
NNP
EC
NNP
Commission
PP
IN
on
NP
NNP
Sept.
CD
21
.
.
b) S
NP-B
DT
The
NN
charter
VP
VP
VBN-P
approved
VP
VBN-P
approved
PP-B
IN
by
NP
DT
the
NNP
EC
NNP
Commission
Cause
Experiencer ARGM-TMP
Figure 1: A sentence parse tree (a) and two example ASTm1
structures relative to the predicate approve (b).
Set Props T T+ T?
Train 15,838 793,104 45,157 747,947
Dev 1,606 75,302 4,291 71,011
Train - Dev 14,232 717,802 40,866 676,936
Table 1: Composition of the dataset in terms of: number of
annotations (Props); number of candidate argument nodes (T );
positive (T+) and negative (T?) boundary classifier examples.
? Syntactic Frame as designed in (Xue and
Palmer, 2004).
We also employ structured features derived by the
full parses in an attempt to capture relevant aspects
that may not be emphasized by the explicit feature
representation. (Moschitti et al, 2006a) and (Mos-
chitti et al, 2006b) defined several classes of struc-
tured features that were successfully employed with
tree kernels for the different stages of an SRL pro-
cess. Figure 1 shows an example of the ASTm1 struc-
tures that we used for both the boundary detection
and the role classification stages.
4 Experiments
In this section we discuss the setup and the results
of the experiments carried out on the dataset of the
SemEval2007 closed task on SRL.
289
Task Kernel(s) Precision Recall F?=1
BD poly 94.34% 71.26% 81.19poly + TK 92.89% 76.09% 83.65
BD + RC poly 88.72% 68.76% 77.47poly + TK 86.60% 72.40% 78.86
Table 2: SRL accuracy on the development test for the bound-
ary detection (BD) and the complete SRL task (BD+RC) using
the polynomial kernel alone (poly) or combined with a tree ker-
nel function (poly + TK).
4.1 Setup
The training set comprises 15,8381 training annota-
tions organized on a per-verb basis. In order to build
a development set (Dev), we sampled about one
tenth, i. e. 1,606 annotations, of the original train-
ing set. For the final evaluation on the test set (Test),
consisting of 3,094 annotations, we trained our clas-
sifiers on the whole training data. Statistics on the
dataset composition are shown in Table 1.
The evaluations were carried out with the SVM-
Light-TK2 software (Moschitti, 2004) which ex-
tends the SVM-Light package (Joachims, 1999)
with tree kernel functions. We used the default
polynomial kernel (degree=3) for the linear features
and a SubSet Tree (SST) kernel (Collins and Duffy,
2002) for the comparison of ASTm1 structured fea-
tures. The kernels are normalized and summed by
assigning a weight of 0.3 to the TK contribution.
Training all the 50 boundary classifiers and the
619 role classifiers on the whole dataset took about
4 hours on a 64 bits machine (2.2GHz, 1GB RAM)3.
4.2 Evaluation
All the evaluations were carried out using
the CoNLL2005 evaluator tool available at
http://www.lsi.upc.es/?srlconll/soft.html.
Table 2 shows the aggregate results on boundary
detection (BD) and the complete SRL task (BD+RC)
on the development set using the polynomial kernel
alone (poly) or in conjunction with the tree kernels
and structured features (poly+TK). For both tasks,
tree kernel functions do trigger automatic feature se-
1A bunch of unaligned annotations were removed from the
dataset.
2http://ai-nlp.info.uniroma2.it/moschitti/
3In order to have a faster development cycle, we only used
60k training examples to train the boundary classifier of the verb
say. The accuracy on this relation is still very high, as we mea-
sured an overall F1 of 87.18 on the development set and of 85.13
on the test set.
Role #TI Precision Recall F?=1
Ov(BD) 6931 87.09% 72.96% 79.40Ov(BD+RC) 81.58% 70.16% 75.44
ARG2 4 100.00% 25.00% 40.00
ARG3 17 61.11% 64.71% 62.86
ARG4 4 0.00% 0.00% 0.00
ARGM-ADV 188 55.14% 31.38% 40.00
ARGM-CAU 13 50.00% 23.08% 31.58
ARGM-DIR 4 100.00% 25.00% 40.00
ARGM-EXT 3 0.00% 0.00% 0.00
ARGM-LOC 151 51.66% 51.66% 51.66
ARGM-MNR 85 41.94% 15.29% 22.41
ARGM-PNC 28 38.46% 17.86% 24.39
ARGM-PRD 9 83.33% 55.56% 66.67
ARGM-REC 1 0.00% 0.00% 0.00
ARGM-TMP 386 55.65% 35.75% 43.53
Actor1 12 85.71% 50.00% 63.16
Actor2 1 100.00% 100.00% 100.00
Agent 2551 91.38% 77.34% 83.78
Asset 21 42.42% 66.67% 51.85
Attribute 17 60.00% 70.59% 64.86
Beneficiary 24 65.00% 54.17% 59.09
Cause 48 75.56% 70.83% 73.12
Experiencer 132 86.49% 72.73% 79.01
Location 12 83.33% 41.67% 55.56
Material 7 100.00% 14.29% 25.00
Patient 37 76.67% 62.16% 68.66
Patient1 20 72.73% 40.00% 51.61
Predicate 181 63.75% 56.35% 59.82
Product 106 70.79% 59.43% 64.62
R-ARGM-LOC 2 0.00% 0.00% 0.00
R-ARGM-MNR 2 0.00% 0.00% 0.00
R-ARGM-TMP 4 0.00% 0.00% 0.00
R-Agent 74 70.15% 63.51% 66.67
R-Experiencer 5 100.00% 20.00% 33.33
R-Patient 2 0.00% 0.00% 0.00
R-Predicate 1 0.00% 0.00% 0.00
R-Product 2 0.00% 0.00% 0.00
R-Recipient 8 100.00% 87.50% 93.33
R-Theme 7 75.00% 42.86% 54.55
R-Theme1 7 100.00% 85.71% 92.31
R-Theme2 1 50.00% 100.00% 66.67
R-Topic 14 66.67% 42.86% 52.17
Recipient 48 75.51% 77.08% 76.29
Source 25 65.22% 60.00% 62.50
Stimulus 21 33.33% 19.05% 24.24
Theme 650 79.22% 68.62% 73.54
Theme1 69 77.42% 69.57% 73.28
Theme2 60 74.55% 68.33% 71.30
Topic 1867 84.26% 82.27% 83.25
Table 3: Evaluation of the semantic role labeling accuracy on
the SemEval2007 - Task 17 test set using the poly + TK kernel.
Column #TI reports the number of instances of each role label
in the test set. Rows Ov(BD) and Ov(BD + RC) show the overall
accuracy on the boundary detection and the complete SRL task,
respectively.
lection and improve the polynomial kernel by 2.46
and 1.39 F1 points, respectively.
The SRL accuracy for each one of the 47 dis-
tinct role labels is shown in Table 3. Column 2 lists
290
the number of instances of each role in the test set.
Many roles have very few positive examples both in
the training and the test sets, and therefore have little
or no impact on the overall accuracy which is domi-
nated by the few roles which are very frequent, such
as Theme, Agent, Topic and ARGM-TMP which ac-
count for almost 80% of all the test roles.
5 Final Remarks
In this paper we presented a system that employs
tree kernels and a basic set of flat features for the
classification of thematic roles.
We adopted a very simple approach that is meant
to be as general and fast as possible. The issue
of generality is addressed by training the bound-
ary and role classifiers on a per-predicate basis and
by employing tree kernel and structured features in
the learning algorithm. The resulting architecture
can indeed be used to learn the classification of
roles of non-verbal predicates as well, and the au-
tomatic feature selection triggered by the tree kernel
should compensate for the lack of ad-hoc, well es-
tablished explicit features for some classes of non-
verbal predicates, e. g. adverbs or prepositions.
Splitting the learning problem also has the clear
advantage of noticeably improving the efficiency of
the classifiers, thus reducing training and classifica-
tion time. On the other hand, this split results in
some classifiers having too few training instances
and therefore being very inaccurate. This is espe-
cially true for the boundary classifiers, which con-
versely need to be very accurate in order to posi-
tively support the following stages of the SRL pro-
cess. The solution of a monolithic boundary classi-
fier that we previously employed (Moschitti et al,
2006b) is noticeably more accurate though much
less efficient, especially for training. Indeed, after
the SemEval2007 evaluation period was over, we
ran another experiment using a monolithic boundary
classifier. On the test set, we measured F1 values of
82.09 vs 79.40 and 77.17 vs 75.44 for the boundary
detection and the complete SRL tasks, respectively.
Although it was provided as part of both the train-
ing and test data, we chose not to use the verb sense
information. This choice is motivated by our in-
tention to depend on as less external resources as
possible in order to be able to port our SRL system
to other linguistic models and languages, for which
such resources may not exist. Still, identifying the
predicate sense is a key issue especially for role clas-
sification, as the argument structure of a predicate is
largely determined by its sense. In the near feature
we plan to use larger structured features, i. e. span-
ning all the potential arguments of a predicate, to
improve the accuracy of our role classifiers.
Acknowledgments
The development of the SRL system was carried out
at the University of Rome Tor Vergata and financed
by the EU project PrestoSpace4 (FP6-507336).
References
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL02.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic label-
ing of semantic roles. Computational Linguistic, 28(3):496?
530.
T. Joachims. 1999. Making large-scale SVM learning practical.
In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In Proceedings
of AAAI-2000 Seventeenth National Conference on Artificial
Intelligence, Austin, TX.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2006a. Semantic role labeling via tree kernel joint inference.
In Proceedings of the Tenth Conference on Computational
Natural Language Learning, CoNLL-X.
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2006b. Tree kernel engineering in semantic role labeling
systems. In Proceedings of the Workshop on Learning Struc-
tured Information in Natural Language Applications, EACL
2006, pages 49?56, Trento, Italy, April. European Chapter
of the Association for Computational Linguistics.
Alessandro Moschitti. 2004. A study on convolution kernel
for shallow semantic parsing. In proceedings of ACL-2004,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Support vector
learning for semantic argument classification. to appear in
Machine Learning Journal.
Nianwen Xue and Martha Palmer. 2004. Calibrating features
for semantic role labeling. In Proceedings of EMNLP 2004,
pages 88?94, Barcelona, Spain, July.
4http://www.prestospace.org
291
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1243?1253,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
HEADY: News headline abstraction through event pattern clustering
Enrique Alfonseca
Google Inc.
ealfonseca@google.com
Daniele Pighin
Google Inc.
biondo@google.com
Guillermo Garrido?
NLP & IR Group at UNED
ggarrido@lsi.uned.es
Abstract
This paper presents HEADY: a novel, ab-
stractive approach for headline generation
from news collections. From a web-scale
corpus of English news, we mine syntac-
tic patterns that a Noisy-OR model gener-
alizes into event descriptions. At inference
time, we query the model with the patterns
observed in an unseen news collection,
identify the event that better captures the
gist of the collection and retrieve the most
appropriate pattern to generate a head-
line. HEADY improves over a state-of-the-
art open-domain title abstraction method,
bridging half of the gap that separates
it from extractive methods using human-
generated titles in manual evaluations, and
performs comparably to human-generated
headlines as evaluated with ROUGE.
1 Introduction
Motivation. News events are rarely reported
only in one way, from a single point of view. Dif-
ferent news agencies will interpret the event in dif-
ferent ways; various countries or locations may
highlight different aspects of it depending on how
they are affected; and opinions and in-depth anal-
yses will be written after the fact.
The variety of contents and styles is both an op-
portunity and a challenge. On the positive side, we
have the same events described in different ways;
this redundancy is useful for summarization, as
the information content reported by the majority
of news sources most likely represents the central
part of the event. On the other hand, variability
and subjectivity can be difficult to isolate. For
some applications it is important to understand,
given a collection of related news articles and re-
?Work done during an internship at Google Zurich.
? Carmelo and La La Party It Up with Kim and Ciara
? La La Vazquez and Carmelo Anthony: Wedding
Day Bliss
? Carmelo Anthony, actress LaLa Vazquez wed in
NYC
? Stylist to the Stars
? LaLa, Carmelo Set Off Celebrity Wedding Weekend
? Ciara rocks a sexy Versace Spring 2010 mini to
LaLa Vasquez and Carmelo Anthony?s wedding
(photos)
? Lala Vasquez on her wedding dress, cake, reality tv
show and fiance?, Carmelo Anthony (video)
? VAZQUEZ MARRIES SPORTS STAR AN-
THONY
? Lebron Returns To NYC For Carmelo?s Wedding
? Carmelo Anthony?s stylist dishes on the wedding
? Paul pitching another Big Three with ?Melo in
NYC?
? Carmelo Anthony and La La Vazquez Get Married
at Star-Studded Wedding Ceremony
Table 1: Headlines observed for a news collection
reporting the same wedding event.
ports, how to formulate in an objective way what
has happened.
As a motivating example, Table 1 shows the dif-
ferent headlines observed in news reporting the
wedding between basketball player Carmelo An-
thony and actress LaLa Vazquez. As can be seen,
there is a wide variety of ways to report the same
event, including different points of view, high-
lighted aspects, and opinionated statements on the
part of the reporter. When presenting this event to
a user in a news-based information retrieval or rec-
ommendation system, different event descriptions
may be more appropriate. For example, a user may
only be interested in objective, informative sum-
maries without any interpretation on the part of
the reporter. In this case, Carmelo Anthony, ac-
1243
tress LaLa Vazquez wed in NYC would be a good
choice.
Goal. Our final goal in this research is to build a
headline generation system that, given a news col-
lection, is able to describe it with the most com-
pact, objective and informative headline. In par-
ticular, we want the system to be able to:
? Generate headlines in an open-domain, unsu-
pervised way, so that it does not need to rely
on training data which is expensive to pro-
duce.
? Generalize across synonymous expressions
that refer to the same event.
? Do so in an abstractive fashion, to enforce
novelty, objectivity and generality.
In order to advance towards this goal, this paper
explores the following questions:
? What is a good way of using syntactic pat-
terns to represent events for generating head-
lines?
? Can we have satisfactory readability with an
open-domain abstractive approach, not rely-
ing on training data nor on manually pre-
defined generation templates?
? How far can we get in terms of informative-
ness, compared to the human-produced head-
lines, i.e., extractive approaches?
Contributions. In this paper we present
HEADY, which is at the same time a novel system
for abstractive headline generation, and a smooth
clustering of patterns describing the same events.
HEADY is fully open-domain and can scale to
web-sized data. By learning to generalize events
across the boundaries of a single news story or
news collection, HEADY produces compact and
effective headlines that objectively convey the
relevant information.
When compared to a state-of-the-art open-
domain headline abstraction system (Filippova,
2010), the new headlines are statistically signifi-
cantly better both in terms of readability and in-
formativeness. Also, automatic evaluations using
ROUGE, having objective headlines for the news
as references, show that the abstractive headlines
are on par with human-produced headlines.
2 Related work
Headline generation and summarization.
Most headline generation work in the past has
focused on the problem of single-document sum-
marization: given the main passage of a single
news article, generate a very short summary of
the article. From early in the field, it was pointed
out that a purely extractive approach is not good
enough to generate headlines from the body
text (Banko et al, 2000). Sometimes the most
important information is distributed across several
sentences in the document. More importantly,
quite often, the single sentence selected as the
most informative for the news collection is already
longer than the desired headline size. For this
reason, most early headline generation work fo-
cused on either extracting and reordering n-grams
from the document to be summarized (Banko et
al., 2000), or extracting one or two informative
sentences from the document and performing
linguistically-motivated transformations to them
in order to reduce the summary length (Dorr et
al., 2003). The first approach is not guaranteed
to produce grammatical headlines, whereas the
second approach is tightly tied to the actual
wording found in the document. Single-document
headline generation was also explored at the
Document Understanding Conferences between
2002 and 20041.
In later years, there has been more interest in
problems such as sentence compression (Galley
and McKeown, 2007; Clarke and Lapata, 2008;
Cohn and Lapata, 2009; Napoles et al, 2011;
Berg-Kirkpatrick et al, 2011), text simplification
(Zhu et al, 2010; Coster and Kauchak, 2011;
Woodsend and Lapata, 2011) and sentence fu-
sion (Barzilay and McKeown, 2005; Wan et al,
2007; Filippova and Strube, 2008; Elsner and San-
thanam, 2011). All of them have direct applica-
tions for headline generation, as it can be con-
strued as selecting one or a few sentences from
the original document(s), and then reducing them
to the target title size. For example, Wan et al
(2007) generate novel utterances by combining
Prim?s maximum-spanning-tree algorithm with an
n-gram language model to enforce fluency. Un-
like HEADY, the method by Wan and colleagues
is an extractive method that can summarize single
documents into a sentence, as opposed to generat-
ing a sentence that can stand for a whole collec-
1http://duc.nist.gov/
1244
tion of news. Filippova (2010) reports a system
that is very close to our settings: the input is a
collection of related news articles, and the system
generates a headline that describes the main event.
This system uses sentence compression techniques
and benefits from the redundancy in the collection.
One difference with respect to HEADY is that it
does not use any syntactic information aside from
part-of-speech tags, and it does not require a train-
ing step. We have used this approach as a baseline
for comparison.
There are not many fully abstractive systems for
news summarization. The few that exist, such as
the work by Genest and Lapalme (2012), rely on
manually written generation templates. In con-
trast, HEADY automatically learns the templates
or headline patterns automatically, which allows it
to work in open-domain settings without relying
on supervision or manual annotations.
Open-domain pattern learning. Pattern learn-
ing for relation extraction is an active area of re-
search that is very related to our problem of event
pattern learning for headline generation. TextRun-
ner (Yates et al, 2007), ReVerb (Fader et al, 2011)
and NELL (Carlson et al, 2010; Mohamed et al,
2011) are some examples of open-domain systems
that learn surface patterns that express relations
between pairs of entities. PATTY (Nakashole et
al., 2012) generalizes the patterns to also include
syntactic information and ontological (class mem-
bership) constraints. Our patterns are more similar
to the ones used by PATTY, which also produces
clusters of synonymous patterns. The main differ-
ences are that (a) HEADY is not limited to con-
sider patterns expressing relations between pairs
of entities; (b) we identify synonym patterns us-
ing a probabilistic, Bayesian approach that takes
advantage of the multiplicity of news sources re-
porting the same events. Chambers and Jurafsky
(2009) present an unsupervised method for learn-
ing narrative schemas from news, i.e., coherent
sets of events that involve specific entity types (se-
mantic roles). Similarly to them, we move from
the assumptions that 1) utterances involving the
same entity types within the same document (in
our case, a collection of related documents) are
likely describing aspects of the same event, and
2) meaningful representations of the underlying
events can be learned by clustering these utter-
ances in a principled way.
Noisy-OR networks. Noisy-OR Bayesian net-
works (Pearl, 1988) have been applied in the
past to a wide class of large-scale probabilis-
tic inference problems, from the medical do-
main (Middleton et al, 1991; Jaakkola and Jor-
dan, 1999; Onisko et al, 2001), to synthetic
image-decomposition and co-citation data analy-
sis (S?ingliar and Hauskrecht, 2006). By assum-
ing independence between the causes of the hid-
den variables, noisy-OR models tend to be reli-
able (Friedman and Goldszmidt, 1996) as they re-
quire a relatively small number of parameters to
be estimated (linear with the size of the network).
3 Headline generation
In this section, we describe the HEADY system for
news headline abstraction. Our approach takes as
input, for training, a corpus of news articles or-
ganized in news collections. Once the model is
trained, it can generate headlines for new collec-
tions. An outline of HEADY?s main components
follows (details of each component are provided
in Sections 3.1, 3.2 and 3.3):
Pattern extraction. Identify, in each of the news
collections, syntactic patterns connecting k enti-
ties, for k ? 1. These will be the candidate pat-
terns expressing events.
Training. Train a Noisy-OR Bayesian network
on the co-occurrence of syntactic patterns. Each
pattern extracted in the previous step is added as
an observed variable, and latent variables are used
to represent the hidden events that generate pat-
terns. An additional noise variable links to every
terminal node, allowing every terminal to be gen-
erated by language background (noise) instead of
by an actual event.
Inference. Generate a headline from an unseen
news collection. First, patterns are extracted using
the pattern extraction procedure mentioned above.
Given the patterns, the posterior probability of the
hidden event variables is estimated. Then, from
the activated hidden events, the likelihood of ev-
ery pattern can be estimated, even if they do not
appear in the collection. The single pattern with
the maximum probability is selected to generate a
new headline from it. Being the product of extra-
news collection generalization, the retrieved pat-
tern is more likely to be objective and informative
than patterns directly observed in the news collec-
tion.
1245
Algorithm 1 COLLECTIONTOPATTERNS?(N ):
N is a repository of news collections, ? is a set
of parameters controlling the extraction process.
R ? {}
for all N ? N do
PREPROCESSDATA(N)
E ? GETRELEVANTENTITIES(N ?)
for all Ei ? COMBINATIONS?(E) do
for all n ? N do
P ? EXTRACTPATTERNS?(n, Ei)
R{N,Ei} ? R{N,Ei} ? P
returnR
3.1 Pattern extraction
In this section we detail the process for obtain-
ing the event patterns that constitute the building
blocks of learning and inference.
Patterns are extracted from a large repository
N of news collections N1, . . . , N|N |. Each news
collection N = {ni} is an unordered collec-
tion of related news, each of which can be seen
as an ordered sequence of sentences, i.e.: n =
[s0, . . . s|n|].
Algorithm 1 presents a high-level view of the
pattern extraction process. The different steps are
described below:
PREPROCESSDATA: We start by preprocess-
ing all the news in the news collections with a
standard NLP pipeline: tokenization and sentence
boundary detection (Gillick, 2009), part-of-speech
tagging, dependency parsing (Nivre, 2006), co-
reference resolution (Haghighi and Klein, 2009)
and entity linking based on Wikipedia and Free-
base. Using the Freebase dataset, each entity is
annotated with all its Freebase types (class labels).
In the end, for each entity mentioned in the docu-
ment we have a unique identifier, a list with all its
mentions in the document and a list of class labels
from Freebase.
As a result of this process, we obtain for each
sentence in the corpus a representation as exem-
plified in Figure 1 (1). In this example, the men-
tions of three distinct entities have been identified,
i.e., e1, . . . , e3. In the Freebase list of types (class
labels), e1 is a person and a celebrity, and e3 is a
state and a location.
GETRELEVANTENTITIES: For each news col-
lection N we collect the set E of the entities men-
tioned most often within the collection. Next, we
generate the set COMBINATIONS?(E) consisting
NNP CC NNP TO VB IN NNP
Portia and Helen to marry in California
e1 e2 e3
person actress statecelebrity location
root
cc conj
nsubj
aux prep pobj
1
NNP NNP
e1 e2
person actresscelebrity
conj 2
NNP CC NNP TO VB
e1 and e2 to marry
person actresscelebrity
cc conj
nsubj
aux
3
NNP CC NNP TO VBperson and actress to marry
cc conj
nsubj
aux
4
NNP CC NNP TO VBcelebrity and actress to marry
cc conj
nsubj
aux
Figure 1: Pattern extraction process from an anno-
tated dependency parse. (1): an MST is extracted
from the entity pair e1, e2 (2); nodes are heuristi-
cally added to the MST to enforce grammaticality
(3); entity types are recombined to generate the fi-
nal patterns (4).
of non-empty subsets of E, without repeated en-
tities. The number of entities to consider in each
collection, and the maximum size for the subsets
of entities to consider are meta-parameters embed-
ded in ?.2
EXTRACTPATTERNS: For each subset of rel-
evant entities Ei, event patterns are mined from
the articles in the news collection. The process
by which patterns are extracted from a news is
explained in Algorithm 2 and exemplified graphi-
cally in Figure 1 (2?4).
GETMENTIONNODES: Using the dependency
parse T for a sentence s, we first identify the set
of nodes Mi that mention the entities in Ei. If
T does not contain exactly one mention of each
target entity in Ei, then the sentence is ignored.
Otherwise, we obtain the minimum spanning tree
for the nodeset Pi, i.e., the shortest path in the de-
pendency tree connecting all the nodes inMi (Fig-
ure 1, 2). Pi is the set of nodes around which the
patterns will be constructed.
APPLYHEURISTICS: With very high probabil-
ity, the MST Pi that we obtain does not constitute
a grammatical or useful extrapolation of the origi-
nal sentence s. For example, the MST for the en-
2As our objective is to generate very short titles (under
10 words), we only consider combinations of up to three ele-
ments of E.
1246
Algorithm 2 EXTRACTPATTERNS?(n, Ei): n is
the list of sentences in a news article. Sentences
are POS-tagged, dependency parsed and annotated
with respect to a set of entities E ? Ei
P ? ?
for all s ? n[0 : 2) do
T ? DEPPARSE(s)
Mi ? GETMENTIONNODES(t, Ei)
if ?e ? Ei, count(e,Mi) 6= 1 then continue
Pi ? GETMINIMUMSPANNINGTREE?(Mi)
APPLYHEURISTICS?(Pi) or continue
P ? P ? COMBINEENTITYTYPES?(Pi)
return P
tity pair ?e1, e2? in the example does not provide a
good description of the event as it is neither ade-
quate nor fluent. For this reason, we apply a set of
post-processing heuristic transformations that aim
at including a minimal set of meaningful nodes.
These include making sure that both the root of the
clause and its subject appear in the extracted pat-
tern, and that conjunctions between entities should
not be dropped (Figure 1, 3).
COMBINEENTITYTYPES: Finally, a distinct
pattern is generated from each possible combina-
tion of entity type assignments for the participat-
ing entities. (Figure 1, 4).
It is important to note that both at training and
test time, for pattern extraction we only consider
the title and the first sentence of the article body.
The reason is that we want to limit ourselves, in
each news collection, to the most relevant event
reported in the collection, which appears most of
the times in these two sentences. Unlike titles, first
sentences do not extensively use puns or rhetorics
as they tend to be grammatical and informative
rather than catchy.
The patterns mined from the same news collec-
tion and for the same set of entities are grouped
together, and constitute the building blocks of the
clustering algorithm which is described below.
3.2 Training
The extracted patterns are used to learn a Noisy-
OR (Pearl, 1988) model by estimating the prob-
ability that each (observed) pattern activates one
or many (hidden) events. Figure 2 represents the
two levels: the hidden event variables at the top,
and the observed pattern variables at the bottom.
An additional noise variable links to every termi-
e1 ... en noise
p3p2p1 ... pm
Figure 2: Probabilistic model. The associations
between latent event variables and observed pat-
tern variables are modeled by noisy-OR gates.
Events are assumed to be marginally independent,
and patterns conditionally independent given the
events.
nal node, allowing all terminals to be generated by
language background (noise) instead of by an ac-
tual event. The associations between latent events
and observed patterns are modeled by noisy-OR
gates.
In this model, the conditional probability of a
hidden event ei given a configuration of observed
patterns p ? {0, 1}|P| is calculated as:
P (ei = 0 | p) = (1? qi0)
?
j?pij
(1? qij)pj
= exp
?
???i0 ?
?
j?pii
?ijpj
?
? ,
where pii is the set of active events (i.e., pii =
?j{pj} | pj = 1), and qij = P (ei = 1 | pj = 1)
is the estimated probability that the observed pat-
tern pi can, in isolation, activate the event e. The
term qi0 is the so-called ?noise? term of the model,
and it accounts for the fact that an observed event
ei might be activated by some pattern that has
never been observed (Jaakkola and Jordan, 1999).
In Algorithm 1, at the end of the process we
group in R[N,Ei] all the patterns extracted from
the same news collection N and entity sub-set Ei.
These groups represent rough clusters of patterns,
that we can use to bootstrap the optimization of
the model parameters ?ij = ? log(1 ? qij). We
initiate the training process by randomly selecting
100,000 of these groups, and optimize the weights
of the model through 40 EM (Dempster et al,
1977) iterations.
3.3 Inference (generation of new headlines)
Given an unseen news collection N , the inference
component of HEADY generates a single headline
that captures the main event reported by the news
in N . In order to do so, we first need to select a
1247
single event-pattern p? that is especially relevant
for N . Having selected p?, in order to generate a
headline it is sufficient to replace the entity place-
holders in p? with the surface forms observed in
N .
To identify p?, we start from the assumption that
the most descriptive event encoded by N must de-
scribe an important situation in which some subset
of the relevant entities E in N are involved.
The basic inference algorithm is a two-
step random walk in the Bayesian network.
Given a set of entities E and sentences n,
EXTRACTPATTERNS?(n, E) collects patterns in-
volving those entities. By normalizing the fre-
quency of the extracted patterns, we get a prob-
ability distribution over the observed variables in
the network. A two-step random walk traversing
to the latent event nodes and back to the pattern
nodes allows us to generalize across events. We
call this algorithm INFERENCE(n, E).
In order to decide which is the most relevant set
of events that should appear in the headline, we
use the following procedure:
1. Given the set of entities E mentioned in the
news collection, we consider each entity sub-
set Ei ? E including up to three entities3.
For each Ei, we run INFERENCE(n, Ei),
which computes a distribution wi over pat-
terns involving the entities in Ei.
2. We invoke again INFERENCE, now using at
the same time all the patterns extracted for
every subset of Ei ? E. This computes a
probability distribution w over all patterns in-
volving any admissible subset of the entities
mentioned in the collection.
3. Third, we select the entity-specific distribu-
tion that approximates better the overall dis-
tribution
w? = arg max
i
cos(w,wi)
We assume that the corresponding set of en-
tities Ei are the most central entities in the
collection and therefore any headline should
make sure to mention them all.
3As we noted before, we impose this limitation to keep the
generated headlines relatively short and to limit data sparsity
issues.
4. Finally, we select the pattern with the highest
weight in w? as the pattern that better cap-
tures the main event reported in the news col-
lection:
p? = pj | wj = arg maxj w
?
j
The headline is then produced from p?, replac-
ing placeholders with the entities in the document
from which the pattern was extracted.
While in many cases information about entity
types would be sufficient to decide about the or-
der of the entities in the generated sentences (e.g.,
?[person] married in [location]? for the entity
set {ea = ?Mr. Brown?, eb = ?Los Angeles?}),
in other cases class assignment can be ambigu-
ous (e.g., ?[person] killed [person]? for {ea =
?Mr. A?, eb = ?Mr. B?}). To handle these cases,
when extracting patterns for an entity set {ea, eb},
we keep track of the alphabetical ordering of
the entities, e.g., from a news collection about
?Mr. B? killing ?Mr. A? we would produce
patterns such as ?[person:2] killed [person:1]? or
?[person:1] was killed by [person:2]? since ea =
?Mr. A? < eb = ?Mr. B?. At inference time,
when we query the model with such patterns we
can only activate events whose assignments are
compatible with the entities observed in the text,
making the replacement straightforward and un-
ambiguous.
4 Experiment settings
In our method we use patterns that are fully lex-
icalized (with the exception of entity placehold-
ers) and enriched with syntactic data. Under these
circumstances, the Noisy-OR can effectively gen-
eralize and learn meaningful clusters only if pro-
vided with large amounts of data. To our best
knowledge, available data sets for headline gen-
eration are not large enough to support this kind
of inference.
For this reason, we rely on a corpus of news
crawled from the web between 2008 and 2012
which have been clustered based on closeness in
time and cosine similarity, using the vector-space
model and tf.idf weights. News collections with
less than 5 documents are discarded4, and those
4There is a very long tail of singleton articles, which do
not offer useful examples of lexical or syntactic variation, and
many very small collections that tend to be especially noisy,
hence the decision to consider only collections with at least 5
documents.
1248
larger than 50 documents are capped, by randomly
picking 50 documents from the collection5. The
total number of news collections after clustering
is 1.7 million. From this set, we have set aside
a few hundred collections that will remain unseen
until the final evaluation.
As we have no development set, we have done
no tuning of the parameters for pattern extraction
nor for the Bayesian network training (100,000 la-
tent variables to represent the different events, 40
EM iterations, as mentioned in Section 3.2). The
EM iterations on the noisy-OR were distributed
across 30 machines with 16 GB of memory each.
4.1 Systems used
One of the questions we wanted to answer in
this research was whether it was possible to ob-
tain the same quality with automatically abstracted
headlines as with human-generated headlines. For
every news collection we have as many human-
generated headlines as documents. To decide
which human-generated headline should be used
in this comparison, we used three different meth-
ods that pick one of the collection headlines:
? Latest headline: selects the headline from
the latest document in the collection. Intu-
itively this should be the most relevant one
for news about sport matches and competi-
tions, where the earlier headlines offer pre-
views and predictions, and the later headlines
report who won and the final scores.
? Most frequent headline: some headlines
are repeated across the collection, and this
method chooses the most frequent one. If
there are several with the same frequency,
one is taken at random6.
? TopicSum: we use TopicSum (Haghighi and
Vanderwende, 2009), a 3-layer hierarchical
topic model, to infer the language model that
is most central for the collection. The news
title that has the smallest Kullback-Leibler
5Even though we did not run any experiment to find an
optimal value for this parameter, 50 documents seems like
a reasonable choice to avoid redundancy while allowing for
considerable lexical and syntactic variation.
6The most frequent headline only has a tie in 6 collections
in the whole test set. In 5 cases two headlines are tied at fre-
quencies around 4, and in one case three headlines are tied at
frequency 2. All six are large collections with 50 news arti-
cles, so this baseline is significantly different from a random
baseline.
R-1 R-2 R-SU4
HEADY 0.3565 0.1903 0.1966
Most frequent pattern 0.3560 0.1864 0.1959
TopicSum 0.3594 0.1821 0.1935
MSC 0.3470 0.1765 0.1855
Most frequent headline 0.3177 0.1401 0.1668
Latest headline 0.2814 0.1191 0.1425
Table 2: Results from the automatic evaluation,
sorted according to the ROUGE-2 and ROUGE-
SU4 scores.
divergence with respect the collection lan-
guage model is the one chosen.
A headline generation system that addresses
the same application as ours is (Filippova, 2010),
which generates a graph from the collection sen-
tences and selects the shortest path between the
begin and the end node traversing words in the
same order in which they were found in the orig-
inal documents. We have used this system, called
Multi-Sentence Compression (MSC), for compar-
isons.
Finally, in order to understand whether the
noisy-OR Bayesian network is useful for general-
izing across patterns into latent events, we added a
baseline that extracts all patterns from the test col-
lection following the same COLLECTIONTOPAT-
TERNS algorithm (including the application of the
linguistically motivated heuristics), and then pro-
duces a headline straightaway from the most fre-
quent pattern extracted. In other words, the only
difference with respect to HEADY is that in this
case no generalization through the Noisy-OR net-
work is carried out, and that headlines are gen-
erated from patterns directly observed in the test
news collections. We call this system Most fre-
quent pattern.
4.2 Annotation activities
In order to evaluate HEADY?s performance, we
carried out two annotation activities.
First, from the set of collections that we had
set aside at the beginning, we randomly chose 50
collections for which all the systems could gen-
erate an output, and we asked raters to manually
write titles for them. As this is not a simple task
to be crowdsourced, for this evaluation we relied
on eight trained raters. We collected between four
and five reference titles for each of the fifty news
collections, to be used to compare the headline
1249
Readability Informativeness
TopicSum 4.86 4.63
Most freq. headline ??4.61 ??34.43
Latest headline ??4.55 ? 4.00
HEADY ? 4.28 ? 3.75
Most freq. pattern ? 3.95 ? 3.82
MSC 3.00 3.05
Table 3: Results from the manual evaluation. At
95% confidence, TopicSum is significantly better
than all others for readability, and only indistin-
guishable from the most frequent pattern for in-
formativeness. For the rest, 3 means being signifi-
cantly better than HEADY, ? than the most frequent
pattern, and ? than MSC.
generation methods using automatic summariza-
tion metrics.
Then, we took the output of the systems for the
50 test collections and asked human raters to eval-
uate the headlines:
1. Raters were shown one headline and asked to
rate it in terms of readability on a 5-point
Likert scale. In the instructions, the raters
were provided with examples of ungrammat-
ical and grammatical titles to guide them in
this annotation.
2. After the previous rating is done, raters were
shown a selection of five documents from the
collection, and they were asked to judge the
informativeness of the previous headline for
the news in the collection, again on a 5-point
Likert scale.
This second annotation was carried out by inde-
pendent raters in a crowd-sourcing setting. The
raters did not have any involvement with the in-
ception of the model or the writing of the pa-
per. They did not know that the headlines they
were rating were generated according to differ-
ent methods. We measured inter-judge agreement
on the Likert-scale annotations using their Intra-
Class Correlation (ICC) (Cicchetti, 1994). The
ICC for readability is 0.76 (0.95 confidence in-
terval [0.71, 0.80]), and for informativeness it is
0.67 (0.95 confidence interval [0.60, 0.73]). This
means strong agreement for readability, and mod-
erate agreement for informativeness.
5 Results
The COLLECTIONTOPATTERNS algorithm was
run on the training set, producing a 230 million
event patterns. Patterns that were obtained from
the same collection and involving the same entities
were grouped together, for a total of 1.7 million
pattern collections. The pattern groups are used to
bootstrap the Noisy-OR model training. Training
the HEADY model that we used for the evaluation
took around six hours on 30 cores.
Table 2 shows the results of the comparison
of the headline generation systems using ROUGE
(R-1, R-2 and R-SU4) (Lin, 2004) with the col-
lected references. According to Owczarzak et
al. (2012), ROUGE is still a competitive met-
ric that correlates well with human judgements
for ranking summarizers. The significance tests
for ROUGE are performed using bootstrap resam-
pling and a graphical significance test (Minka,
2002). The human annotators that created the
references for this evaluation were explicitly in-
structed to write objective titles, which is the kind
of headlines that the abstractive systems aim at
generating. It is common to see real headlines
that are catchy, joking, or with a double mean-
ing, and therefore they use a different vocabulary
than objective titles that simply mention what hap-
pened. TopicSum sometimes selects objective ti-
tles amongst the human-made titles and that is
why it also scores very well with the ROUGE
scores. But the other two criteria for choosing
human-made headlines select non-objective titles
much more often, and this lowers their perfor-
mance when measured with ROUGE with respect
to the objective references.
Table 3 lists the results of the manual evaluation
of readability and informativeness of the generated
headlines. The first result that we can see is the
difference in the rankings between the two evalu-
ations. Part of this difference might be due to the
fact that ROUGE is not as good for discriminating
between human-made and automatic summaries.
In fact, in the DUC competitions, the gap between
human summaries and automatic summaries was
also more apparent in the manual evaluations than
using ROUGE. Another part of the observed dif-
ference may be due to the design of the evalua-
tion. The manual evaluation is asking raters to
judge whether real, human-written titles that were
actually used for those news are grammatical and
informative. As could be expected, as these are
published titles, the real titles score very good on
the manual evaluation.
Some other interesting results are:
1250
Model Generated title
TopicSum Modern Family?s Eric Stonestreet laughs off
Charlize Theron rumours
MSC Modern Family star Eric Stonestreet is dating
Charlize Theron.
Latest headline Eric laughs off Theron dating rumours
Frequent pattern Eric Stonestreet jokes about Charlize relationship
Frequent headline Charlize Theron dating Modern Family star
HEADY Eric Stonestreet not dating Charlize Theron
TopicSum McFadzean rescues point for Crawley Town
MSC Crawley side challenging for a point against Old-
ham Athletic.
Latest headline Reds midfielder victim of racist tweet
Frequent pattern Kyle McFadzean fired a equaliser Crawley were
made
Frequent headline Latics halt Crawley charge
HEADY Kyle McFadzean rescues point for Crawley Town
F.C.
TopicSum UCI to strip Lance Armstrong of his 7 Tour titles
MSC The international cycling union said today.
Latest headline Letters: elderly drivers and Lance Armstrong
Frequent pattern Lance Armstrong stripped of Tour de France ti-
tles
Frequent headline Today in the news: third debate is tonight
HEADY Lance Armstrong was stripped of Tour de France
titles
Table 4: A comparison of the titles generated by
the different models for three news collections.
? Amongst the automatic systems, HEADY per-
formed better than MSC, with statistical sig-
nificance at 95% for all the metrics. Head-
lines based on the most frequent patterns
were better than MSC for all metrics but
ROUGE-2.
? The most frequent pattern baseline and
HEADY have comparable performance across
all the metrics (not statistically significantly
different), although HEADY has slightly bet-
ter scores for all metrics except for informa-
tiveness.
While we do not take any step to explicitly
model stylistic variation, estimating the weights
of the Noisy-OR network turns out to be a very
effective way of filtering out sensational wording
to the advantage of plainer, more objective style.
This may not clearly emerge from the evaluation,
as we did not explicitly ask the raters to annotate
the items based on their objectivity, but a manual
inspection of the clusters suggests that the gener-
alization is working in the right direction.
Table 4 presents a selection of outputs produced
by the six models for three different news collec-
tions. The first example shows a news collection
containing news about a rumour that was imme-
diately denied. In the second example, HEADY
generalization improves over the most frequent
pattern. In the third case, HEADY generates a
good title from a noisy collection (containing dif-
ferent but related events). The examples also
show that TopicSum is very effective in selecting
a good human-generated headline for each collec-
tion. This opens the possibility of using TopicSum
to automatically generate ROUGE references for
future evaluations of abstractive methods.
6 Conclusions
We have presented HEADY, an abstractive head-
line generation system based on the generaliza-
tion of syntactic patterns by means of a Noisy-OR
Bayesian network. We evaluated the model both
automatically and through human annotations.
HEADY performs significantly better than a state-
of-the-art open domain abstractive model (Filip-
pova, 2010) in all evaluations, and is in par with
human-generated headlines in terms of ROUGE
scores. We have shown that it is possible to
achieve high quality generation of news headlines
in an open-domain, unsupervised setting by suc-
cessfully exploiting syntactic and ontological in-
formation. The system relies on a standard NLP
pipeline, requires no manual data annotation and
can effectively scale to web-sized corpora.
For feature work, we plan to improve all compo-
nents of HEADY in order to fill in the gap with the
human-generated titles in terms of readability and
informativeness. One of the directions in which
we plan to move is the removal of the syntac-
tic heuristics that currently enforce pattern well-
formedness and to automatically learn the neces-
sary transformations from the data.
Two other lines of work that we plan to explore
are the possibility of personalizing the headlines
to user interests (as stored in user profiles or ex-
pressed as user queries), and to investigate further
applications of the Bayesian network of event pat-
terns, such as its use for relation extraction and
knowledge base population.
Acknowledgments
The research leading to these results has received
funding from: the EU?s 7th Framework Pro-
gramme (FP7/2007-2013) under grant agreement
number 257790; the Spanish Ministry of Science
and Innovation?s project Holopedia (TIN2010-
21128-C02); and the Regional Government of
Madrid?s MA2VICMR (S2009/TIC1542). We
would like to thank Katja Filippova and the anony-
mous reviewers for their insightful comments.
1251
References
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statis-
tical translation. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?00, pages 318?325. Association for
Computational Linguistics.
Regina Barzilay and Kathleen R McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297?
328.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 481?490. Association for
Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel,
Burr Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010), pages 3?3.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised Learning of Narrative Schemas and Their
Participants. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2 - Volume
2, pages 602?610.
Domenic V Cicchetti. 1994. Guidelines, criteria, and
rules of thumb for evaluating normed and standard-
ized assessment instruments in psychology. Psycho-
logical Assessment, 6(4):284.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31(1):399?429.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34:637?674.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using Wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1?9. Association for Computational
Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B.
Rubi. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, Series B, 39(1):1?38.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 03 on Text summarization workshop-Volume
5, pages 1?8. Association for Computational Lin-
guistics.
Micha Elsner and Deepak Santhanam. 2011. Learn-
ing to fuse disparate sentences. In Proceedings of
the Workshop on Monolingual Text-To-Text Gener-
ation, pages 54?63. Association for Computational
Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535?1545. Association for Computational
Linguistics.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 177?185. As-
sociation for Computational Linguistics.
Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 322?330. Association
for Computational Linguistics.
Nir Friedman and Moises Goldszmidt. 1996. Learning
Bayesian networks with local structure. In Proceed-
ings of the Twelfth Conference Annual Conference
on Uncertainty in Artificial Intelligence (UAI-96),
pages 252?262, San Francisco, CA. Morgan Kauf-
mann.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. Proceedings of the North American Chap-
ter of the Association for Computational Linguistics,
pages 180?187.
Pierre-Etienne Genest and Guy Lapalme. 2012. Fully
abstractive approach to guided summarization. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics, short papers.
Association for Computational Linguistics.
Dan Gillick. 2009. Sentence boundary detection and
the problem with the us. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 241?244. Association for
Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1152?1161. Asso-
ciation for Computational Linguistics.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362?370. Association for
Computational Linguistics.
1252
Tommi S. Jaakkola and Michael I. Jordan. 1999.
Variational probabilistic inference and the QMR-
DT Network. Journal of Artificial Intelligence Re-
search, 10:291?322.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81.
Blackford Middleton, Michael Shwe, David Hecker-
man, Max Henrion, Eric Horvitz, Harold Lehmann,
and Gregory Cooper. 1991. Probabilistic diag-
nosis using a reformulation of the INTERNIST-
1/QMR knowledge base. I. The probabilistic model
and inference algorithms. Methods of information in
medicine, 30(4):241?255, October.
Tom Minka. 2002. Judging Significance from Error
Bars. CM U Tech R eport.
Thahir P Mohamed, Estevam R Hruschka Jr, and
Tom M Mitchell. 2011. Discovering relations be-
tween noun categories. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1447?1455. Association for Com-
putational Linguistics.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphras-
tic sentence compression with a character-based
metric: Tightening without deletion. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation, pages 84?90. Association for Computa-
tional Linguistics.
Joakim Nivre. 2006. Inductive Dependency Parsing,
volume 34 of Text, Speech and Language Technol-
ogy. Springer.
Agnieszka Onisko, Marek J. Druzdzel, and Hanna Wa-
syluk. 2001. Learning Bayesian network parame-
ters from small data sets: application of Noisy-OR
gates. International Journal of Approximated Rea-
soning, 27(2):165?182.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proceedings of the NAACL-HLT 2012 Work-
shop on Evaluation Metrics and System Comparison
for Automatic Summarization, pages 1?9. Associa-
tion for Computational Linguistics.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Mor-
gan Kaufmann.
Toma?s? S?ingliar and Milos? Hauskrecht. 2006. Noisy-or
component analysis and its application to link analy-
sis. J. Mach. Learn. Res., 7:2189?2213, December.
Stephen Wan, Robert Dale, Mark Dras, and Ce?cile
Paris. 2007. Global Revision in Summarisation:
Generating Novel Sentences with Prim?s Algorithm.
In Proceedings of PACLING 2007 - 10th Conference
of the Pacific Association for Computational Lin-
guistics.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 409?420. Association
for Computational Linguistics.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. TextRunner: Open information
extraction on the web. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics: Demonstrations, pages
25?26. Association for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics, pages 1353?1361.
1253
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1446?1455,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
BRAINSUP: Brainstorming Support for Creative Sentence Generation
Go?zde O?zbal
FBK-irst
Trento, Italy
gozbalde@gmail.com
Daniele Pighin
Google Inc.
Zu?rich, Switzerland
daniele.pighin@gmail.com
Carlo Strapparava
FBK-irst
Trento, Italy
strappa@fbk.eu
Abstract
We present BRAINSUP, an extensible
framework for the generation of creative
sentences in which users are able to
force several words to appear in the sen-
tences and to control the generation pro-
cess across several semantic dimensions,
namely emotions, colors, domain related-
ness and phonetic properties. We evalu-
ate its performance on a creative sentence
generation task, showing its capability of
generating well-formed, catchy and effec-
tive sentences that have all the good qual-
ities of slogans produced by human copy-
writers.
1 Introduction
A variety of real-world scenarios involve talented
and knowledgable people in a time-consuming
process to write creative, original sentences gen-
erated according to well-defined requisites. For
instance, to advertise a new product it could be
desirable to have its name appearing in a punchy
sentence together with some keywords relevant for
marketing, e.g. ?fresh?, or ?thirst? for the adver-
tisement of a drink. Besides, it could be interesting
to characterize the sentence with respect to a spe-
cific color, like ?blue? to convey the idea of fresh-
ness, or to a color more related to the brand of the
company, e.g. ?red? for a new Ferrari. Moreover,
making the slogan evoke ?joy? or ?satisfaction?
could make the advertisement even more catchy
for customers. On the other hand, there are many
examples of provocative slogans in which copy-
writers try to impress their readers by suscitating
strong negative feelings, as in the case of anti-
smoke campaigns (e.g., ?there are cooler ways to
die than smoking? or ?cancer cures smoking?), or
the famous beer motto ?Guinness is not good for
you?. As another scenario, creative sentence gen-
eration is also a useful teaching device. For ex-
ample, the keyword or linkword method used for
second language learning links the translation of
a foreign (target) word to one or more keywords
in the native language which are phonologically
or lexically similar to the target word (Sagarra and
Alba, 2006). To illustrate, for teaching the Ital-
ian word ?tenda?, which means ?curtain? in En-
glish, the learners are asked to imagine ?rubbing
a tender part of their leg with a curtain?. These
words should co-occur in the same sentence, but
constructing such sentences by hand can be a dif-
ficult and very time-consuming process. O?zbal
and Strapparava (2011), who attempted to auto-
mate the process, conclude that the inability to re-
trieve from the web a good sentence for all cases
is a major bottleneck.
Although state of the art computational mod-
els of creativity often produce remarkable results,
e.g., Manurung et al (2008), Greene et al (2010),
Guerini et al (2011), Colton et al (2012) just to
name a few, to our best knowledge there is no at-
tempt to develop an unified framework for the gen-
eration of creative sentences in which users can
control all the variables involved in the creative
process to achieve the desired effect.
In this paper, we advocate the use of syntactic
information to generate creative utterances by de-
scribing a methodology that accounts for lexical
and phonetic constraints and multiple semantic di-
mensions at the same time. We present BRAIN-
SUP, an extensible framework for creative sen-
tence generation in which users can control all the
parameters of the creative process, thus generat-
ing sentences that can be used for practical ap-
plications. First, users can define a set of key-
words which must appear in the final sentence.
Second, they can slant the output towards a spe-
1446
Domain Keywords BRAINSUP output examples
coffee waking,
cup
Between waking and doing there
is a wondrous cup.
coke drink, ex-
haustion
The physical exhaustion wants
the dark drink.
health day, juice,
sunshine
With juice and cereal the normal
day becomes a summer sunshine.
beauty kiss,lips
Passionate kiss, perfect lips. ?
Lips and eyes want the kiss.
mascara drama,lash
Lash your drama to the stage. ?
A mighty drama, a biting lash.
pickle crunch, bite Crunch your bite to the top. ?
Crunch of a savage byte. ? A
large byte may crunch a little at-
tention.
soap
skin,
love,
touch
A touch of love is worth a fortune
of skin. ? The touch of froth is
the skin of love. ? A skin of water
is worth a touch of love.
Table 1: A selection of sentences automatically
generated by BRAINSUP for specific domains.
cific emotion, color or domain. At the same time,
they can require a sentence to include desired pho-
netic properties, such as rhymes, alliteration or
plosives. The combination of these features al-
lows for the generation of potentially catchy and
memorable sentences by establishing connections
between linguistic, emotional (LaBar and Cabeza,
2006), echoic and visual (Borman et al, 2005)
memory, as exemplified by the system outputs
showcased in Table 1. Other creative dimensions
can easily be plugged in, due to the inherently
modular structure of the system.
BRAINSUP supports the creative process by
greedily exploring a huge solution space to pro-
duce completely novel utterances responding to
user requisites. It exploits syntactic constraints to
dramatically cut the size of the search space, thus
making it possible to focus on the creative aspects
of sentence generation.
2 Related work
Research in creative language generation has
bloomed in recent years. In this section, we pro-
vide a necessarily succint overview of a selection
of the studies that most heavily inspired and influ-
enced the development of BRAINSUP.
Humor generators are a notable class of sys-
tems exploring new venues in computational cre-
ativity (Binsted and Ritchie, 1997; McKay, 2002;
Manurung et al, 2008). Valitutti et al (2009)
present an interactive system which generates hu-
morous puns obtained through variation of famil-
iar expressions with word substitution. The varia-
tion takes place considering the phonetic distance
and semantic constraints such as semantic similar-
ity, semantic domain opposition and affective po-
larity difference. Possibly closer to slogan genera-
tion, Guerini et al (2011) slant existing textual ex-
pressions to obtain more positively or negatively
valenced versions using WordNet (Miller, 1995)
semantic relations and SentiWordNet (Esuli and
Sebastiani, 2006) annotations. Stock and Strap-
parava (2006) generate acronyms based on lexical
substitution via semantic field opposition, rhyme,
rythm and semantic relations. The model is lim-
ited to the generation of noun phrases.
Poetry generation systems face similar chal-
lenges to BRAINSUP as they struggle to combine
semantic, lexical and phonetic features in a unified
framework. Greene et al (2010) describe a model
for poetry generation in which users can control
meter and rhyme scheme. Generation is modeled
as a cascade of weighted Finite State Transduc-
ers that only accept strings conforming to the de-
sired rhyming scheme. Toivanen et al (2012) at-
tempt to generate novel poems by replacing words
in existing poetry with morphologically compat-
ible words that are semantically related to a tar-
get domain. Content control and the inclusion of
phonetic features are left as future work and syn-
tactic information is not taken into account. The
Electronic Text Composition project1 is a corpus
based approach to poetry generation which recur-
sively combines automatically generated linguistic
constituents into grammatical sentences. Colton et
al. (2012) propose another data-driven approach to
poetry generation based on simile transformation.
The mood and theme of the poems are influenced
by daily news. Constraints about phonetic proper-
ties of the selected words or their frequencies can
be enforced during retrieval. Unlike these exam-
ples, BRAINSUP makes heavy use of syntactic in-
formation to enforce well-formed sentences and to
constraint the search for a solution, and provides
an extensible framework in which various forms
of linguistic creativity can easily be incorporated.
Several slogan generators are available on the
web2, but their capabilities are very limited as they
can only replace single words or word sequences
within existing slogan. This often results in syn-
tactically incorrect outputs. Furthermore, they do
not allow for other forms of user control.
1http://slought.org/content/11199
2E.g.: http://www.procato.com/slogan+
generator, http://www.sloganizer.net/en/,
http://www.sloganmania.com/index.htm.
1447
3 Architecture of BRAINSUP
To effectively support the creative process with
useful suggestions, we must be able to generate
sentences conforming to the user needs. First of
all, users can select the target words that need to
appear in the sentence. In the context of second
language learning, these might be the words that a
learner must associate in order to expand her vo-
cabulary. For slogan generation, the target words
could be the key features of a product, or target-
defining keywords that copywriters want to explic-
itly mention. On top of that, a user can character-
ize the generated sentences according to several
dimensions, namely: 1) a specific semantic do-
main, e.g.: ?sports? or ?blankets?; 2) a specific
emotion, e.g., ?joy?, ?anger? or just ?negative?; 3)
a specific color, e.g., ?red? or ?blue?; 4) a com-
bination of phonetic properties of the words that
will appear in the sentence, i.e., rhymes, allitera-
tions and plosives. More formally, the user input
is a tuple: U = ?t,d, c, e, p,w? , where t is the
set of target words, d is a set of words defining the
target domain, c and p are, respectively, the color
and the emotion towards which the user wants to
slant the sentence, p represents the desired pho-
netic features, and w is a set of weights that control
the influence of each dimension on the generative
process, as detailed in Section 3.3. For target and
domain words, users can explicitly select one or
more POSes to be considered, e.g., ?drink/verb?
or ?drink/verb,noun?.
The sentence generation process is based on
morpho-syntactic patterns which we automati-
cally discover from a corpus of dependency parsed
sentences P . These patterns represent very gen-
eral skeletons of well-formed sentences that we
employ to generate creative sentences by only
focusing on the lexical aspects of the process.
Candidate fillers for each empty position (slot)
in the patterns are chosen according to the lexi-
cal and syntactic constraints enforced by the de-
pendency relations in the patterns. These con-
straints are learned from relation-head-modifier
co-occurrence counts estimated from a depen-
dency treebank L. A beam search in the space of
all possible lexicalizations of a syntactic pattern
promotes the words with the highest likelihood of
satisfying the user specification.
Algorithm 1 provides a high-level description of
the creative sentence generation process. Here, ?
is a set of meta-parameters that affect search com-
plexity and running time of the algorithm, such
as the minimum/maximum number of solutions to
Algorithm 1 SentenceGeneration(U,?,P,L): U is the
user specification, ? is a set of meta-parameters; P and L are
two dependency treebanks.
O ? ?
for all p ? CompatiblePatterns?(U,P) dowhile NotEnoughSolutions?(O) do
O ? O ? FillInPattern?(U, p,L)
return SelectBestSolutions?(O)
DT NNS VBD DT JJ NN IN DT NN
The * * a * * in the *
det nsubj
dobj
det
amod
prep
pobj
det
Figure 1: Example of a syntactic pattern. A ?*?
represents an empty slot to be filled with a filler.
be generated, the maximum number of patterns to
consider, or the maximum size of the generated
sentences. CompatiblePatterns(?) finds the most
frequent syntactic patterns in P that are compat-
ible with the user specification, as explained in
Section 3.1; FillInPattern(?) carries out the beam
search, and returns the best solutions generated for
each pattern p given U . The algorithm terminates
when at least a minimum number of solutions have
been generated, or when all the compatible pat-
terns have been exhausted. Finally, only the best
among the generated solutions are shown to the
user. More details about the search in the solution
space are provided in Section 3.2.
3.1 Pattern selection
We generate creative sentences starting from
morpho-syntactic patterns which have been au-
tomatically learned from a large corpus P . The
choice of the corpus from which the patterns
are extracted constitutes the first element of the
creative sentence generation process, as differ-
ent choices will generate sentences with different
styles. For example, a corpus of slogans or punch-
lines can result in short, catchy and memorable
sentences, whereas a corpus of simplified English
would be a better choice to learn a second lan-
guage or to address low reading level audiences.
A pattern is the syntactic skeleton of a class
of sentences observed in P . Within a pattern, a
second element of creativity involves the selec-
tion of original combinations of words (fillers) that
do not violate the grammaticality of the sentence.
The patterns that we employ are automatic de-
pendency trees from which all content-words have
been removed, as exemplified in Figure 1. After
selecting the target corpus, we parse all the sen-
tences with the Stanford Parser (Klein and Man-
1448
ning, 2003) and produce the patterns by stripping
away all content words from the parses. Then,
for each pattern we count how many times it has
been observed in the corpus. Additionally, we
keep track of what kind of empty slots, i.e., empty
positions, are available in each pattern. For ex-
ample, the pattern in Figure 1 can accommodate
up to two singular nouns (NN), one plural noun
(NNS), one adjective (JJ) and one verb in the past
tense (VBD). This information is needed to se-
lect the patterns which are compatible with the
target words t in the user specification U . For
example, this pattern is not compatible with t =
[heading/VBG, edge/NN] as the pattern does not
have an empty slot for a gerundive verb, while it
satisfies t = [heading/NN, edge/NN] as it can
accommodate the two singular nouns. While re-
trieving patterns, we also need to enforce that a
pattern be not completely filled just by adding the
target words t, as under these conditions there
would be no room to achieve any kind of creative
effect. Therefore, we also require that the pat-
terns retrieved by CompatiblePatterns(?) have
more empty slots than the size of t. The mini-
mum and maximum number of excess slots in the
pattern are two other meta-parameters controlled
by ?. CompatiblePatterns(?) returns compati-
ble patterns ordered by their frequency, i.e. when
generating solutions the first patterns that are ex-
plored are the most frequently observed ones. In
this way, we achieve the following two objectives:
1) we compensate for the unavoidable errors intro-
duced by the automatic parser, as frequently ob-
served parses are less likely to be the result of
an erroneous interpretation of a sentence; and 2)
we generate sentences that are most likely to be
catchy and memorable, being based on syntactic
constructs that are used more frequently. To avoid
always selecting the same patterns for the same
kinds of inputs, we add a small random compo-
nent (also controlled by ?) to the pattern sorting
algorithm, thus allowing for sentences to be gen-
erated also from non-top ranked patterns.
3.2 Searching the solution space
With the compatible patterns selected, we can ini-
tiate a beam search in the space of all possible
lexicalizations of the patterns, i.e., the space of
all sentences that can be generated by respect-
ing the syntactic constraints encoded by each pat-
tern. The process starts with a syntactic pattern
p containing only stop words, syntactic relations
and morphologic constraints (i.e., part-of-speech
DT NNS VBD DT JJ NN IN DT NN
The fires X a * smoke in the *
det nsubj
dobj
det
amod
prep
pobj
det
Figure 2: A partially lexicalized sentence with a
highlighted empty slot marked with X. The rele-
vant dependencies to fill in the slot are shown in
boldface.
tags) for the empty slots. The search advances to-
wards a complete solution by selecting an empty
slot to fill and trying to place candidate fillers in
the selected position. Each partially lexicalized
solution is scored by a battery of scoring func-
tions that compete to generate creative sentences
respecting the user specificationU , as explained in
Section 3.3. The most promising solutions are ex-
tended by filling another slot, until completely lex-
icalized sentences, i.e., sentences without empty
slots, are generated.
To limit the number of words that can occupy
a given position in a sentence, we define a set of
operators that return a list of candidate fillers for
a slot solely based on syntactic clues. To achieve
that, we analyze a large corpus of parsed sentences
L3 and store counts of observed head-relation-
modifier (?h, r,m?) dependency relations. Let
?r(h) be an operator that, when applied to a head
word h in a relation r, returns the set of words in
L which have been observed as modifiers for h in
r with a specific POS. To simplify the notation,
we assume that the relation r also carries along
the POS of the head and modifier slots. As an
example, with respect to the tree depicted in Fig-
ure 2, ?amod(smoke) would return all the words
with POS equal to ?JJ? that have been observed as
adjective modifiers for the singular noun ?smoke?.
We will refer to ?r(?) as the dependency operator
for r. For every ?r(?), we also define an inverse
dependency operator ??1r (?), which returns the list
of the possible heads in r when applied to a mod-
ifier word m. For instance, with respect to Fig-
ure 2, ??1nsubj(fires) would return the set of verbs inthe past tense of which ?fires? as a plural noun can
be a subject.
While filling in a given slot X , the dependency
operators can be combined to obtain a list of words
which are likely to occupy that position given the
syntactic constraints induced by the structure of
the pattern. Let W = ?i{wi} be the set of words
which are directly connected to the empty slot by
3Distinct from the corpus used for pattern selection, P .
1449
a dependency relation. Each word wi implies a
constraint that candidate fillers for X must satisfy.
If wi is the head of X , then a direct operator is
used to retrieve a list of fillers that satisfy the ith
constraint. Conversely, if wi is a modifier of X ,
an inverse operator is employed.
As an example, let us consider the partially
completed sentence shown in Figure 2 having
an empty slot marked with X . Here, the word
?smoke? is a modifier for X , to which it is con-
nected by a dobj relation. Therefore, we can ex-
ploit ??1dobj(smoke) to obtain a ranked list of wordsthat can occupy X according to this constraint.
Similarly, the ??1nsubj(fires) operator can be used toretrieve a list of verbs in the past tense that ac-
cept ?fires? as nsubj modifier. Finally ??1prep(in)
can further restrict our options to verbs that ac-
cepts complements introduced by the preposition
?in?. For example, the words ?generated?, ?pro-
duced?, ?caused? or ?formed? would be good can-
didates to fill in the slot considering all the pre-
vious constraints. More formally, we can de-
fine the set of candidate fillers for a slot X , CX ,
as: CX = ??1rhX,X(hX) ? (
?
wi|wi?MX ?rwi,X(wi)),where rwi,X is the type of relation between wi and
X , MX is the set of modifiers of X and hX is the
syntactic head of X .4
Concerning the order in which slots are filled,
we start from those that have the highest num-
ber of dependencies (both head or modifiers) that
have been already instantiated in the sentence, i.e.,
we start from the slots that are connected to the
highest number of non-empty slots. In doing so
we maximize the constraints that we can rely on
when inserting a new word, and eventually gener-
ate more reliable outputs.
3.3 Filler selection and solution scoring
We have devised a set of feature functions that ac-
count for different aspects of the creative sentence
generation process. By changing the weight w of
the feature functions in U , users can control the
extent to which each creativity component will af-
fect the sentence generation process, and tune the
output of the system to better match their needs.
As explained in the remainder of this section, fea-
ture functions are responsible for ranking the can-
didate slot fillers to be used during sentence gen-
eration and for selecting the best solutions to be
4An empty slot does not generate constraints for X . In
addition, there might be cases in which it is not possible to
find a filler that satisfies all the constraints at the same time.
In such cases, all the fillers that satisfy the maximum number
of constraints are considered.
Algorithm 2 RankCandidates(U, f , c1, c2, s,X): c1
and c2 are two candidate fillers for the slot X in the sentence
s = [s0, . . . sn]; f is the set of feature functions; U is the user
specification.
sc1 ? s, sc2 ? s, sc1 [X]? c1, sc2 [X]? c2
for all f ? SortFeatureFunctions?(U, f) do
if f(sc1 , U) > f(sc2 , U) then return c1  c2
else if f(sc1 , U) < f(sc2 , U) then
return c1 ? c2
return c1 ? c2
shown to the users.
Algorithm 2 details the process of ranking can-
didate fillers. To compare two candidates c1 and c2
for the slot X in the sentence s, we first generate
two sentences sc1 and sc2 in which the empty slot
X is occupied by c1 and c2, respectively. Then, we
sort the feature functions based on their weights
in descending order, and in turn we apply them
to score the two sentences. As soon as we find
a scorer for which one sentence is better than the
other, we can take a decision about the ranking of
the fillers. This approach makes it possible to es-
tablish a strict order of precedence among feature
functions and to select fillers that have a highest
chance of maximizing the user satisfaction.
Concerning the scoring of partial solutions and
complete sentences, we adopt a simple linear com-
bination of scoring functions. Let s be a (partial)
sentence, f = [f0, . . . , fk] be the vector of scor-
ing functions and w = [w0, . . . , wk] the associ-
ated vector of weights in U . The overall score of s
is calculated as score(s, U) = ?ki=0wifi(s, U) .Solutions that do not contain all the required target
words are discarded and not shown to the user.
Currently, the model employs the following 12
feature functions:
Chromatic and emotional connota-
tion. The chromatic connotation of a
sentence s = [s0, . . . , sn] is computed as
f(s, U) =
?
si(sim(si, c) ?
?
cj 6=c sim(si, cj)),where c is the user selected target color and
sim(si, cj) is the degree of association between
the word si and the color cj as calculated by
Mohammad (2011). All the words in the sentence
which have an association with the target color
c give a positive contribution, while those that
are associated with a color ci 6= c contribute
negatively. Emotional connotation works exactly
in the same way, but in this case word-emotion
associations are taken from (Mohammad and
Turney, 2010).
Domain relatedness. This feature function uses
an LSA (Deerwester et al, 1990) vector space
1450
model to measure the similarity between the words
in the sentence and the target domain d speci-
fied by the user. It is calculated as: f(s, U) =?
di v(di)?
?
si v(si)
??di v(di)???
?
si v(si)?
where v(?) returns the rep-
resentation of a word in the vector space.
Semantic cohesion. This feature behaves ex-
actly like domain relatedness, with the only dif-
ference that it measures the similarity between the
words in the sentence and the target words t.
Target-words scorer. This feature function
simply counts what fraction of the target
words t is present in a partial solution:
f(s, U) = (
?
si|si?t 1)/|t|. The target word scorertakes care of enforcing the presence of the target
words in the sentences. Letting beam search find
the best placement for the target words comes at
no extra cost and results in a simple and elegant
model.
Phonetic features (plosives, alliteration and
rhyme). All the phonetic features are based on
the phonetic representation of English words of
the Carnegie Mellon University pronouncing dic-
tionary (Lenzo, 1998). The plosives feature is cal-
culated as the ratio between the number of plo-
sive sounds in a sentence and the overall num-
ber of phonemes. For the alliteration scorer, we
store the phonetic representation of each word in
s in a trie (i.e., prefix tree), and count how many
times each node ni of the trie (corresponding to a
phoneme) is traversed. Let ci be the value of the
counts for ni. The alliteration score is then cal-
culated as f(s, U) = (?i|ci>1 ci)/
?
i ci. Moresimply put, we count how many of the phonetic
prefixes of the words in the sentence are repeated,
and then we normalize this value by the total num-
ber of phonemes in s. The rhyme feature works
exactly in the same way, with the only difference
that we invert the phonetic representation of each
word before adding it to the TRIE. Thus, we give
higher scores to sentences in which several words
share the same phonetic ending.
Variety scorer. This feature function promotes
sentences that contain as many different words as
possible. It is calculated as the number of distinct
words in the sentence over the size of the sentence.
Unusual-words scorer. To increase the ability
of the model to generate sentences containing non-
trivial word associations, we may want to prefer
solutions in which relatively uncommon words are
employed. Inversely, we may want to lower lex-
ical complexity to generate sentences more ap-
propriate for certain education or reading levels.
We define ci as the number of times each word
si ? s is observed in a corpus V . Accord-
ingly, the value of this feature is calculated as:
f(s, U) = (1/|s|)(?si 1/ci).
N-gram likelihood. This is simply the likeli-
hood of a sentence estimated by an n-gram lan-
guage model, to enforce the generation of well-
formed word sequences. When a solution is not
complete, in the computation we include only the
sequences of contiguous words (i.e., not inter-
rupted by empty slots) having length greater than
or equal to the order of the n-gram model.
Dependency likelihood. This feature is re-
lated to the dependency operators introduced
in Section 3.2 and it enforces sentences in
which dependency chains are well formed. We
estimate the probability of a modifier word
m and its head h to be in the relation r
as pr(h,m) = cr(h,m)/(?hi
?
mi cr(hi,mi)),where cr(?) is the number of times that m
depends on h in the dependency treebank
L and hi,mi are all the head/modifier pairs
observed in L. The dependency-likelihood
of a sentence s can then be calculated as
f(s, U) = exp(
?
?h,m,r??r(s) log pr(h,m)), r(s)being the set of dependency relations in s.
4 Evaluation
We evaluated our model on a creative sentence
generation task. The objective of the evaluation
is twofold: we wanted to demonstrate 1) the effec-
tiveness of our approach for creative sentence gen-
eration, in general, and 2) the potential of BRAIN-
SUP to support the brainstorming process behind
slogan generation. To this end, the annotation tem-
plate included one question asking the annotators
to rate the quality of the generated sentences as
slogans.
Five experienced annotators were asked to rate
432 creative sentences according to the follow-
ing criteria, namely: 1) Catchiness: is the sen-
tence attractive, catchy or memorable? [Yes/No]
2) Humor: is the sentence witty or humorous?
[Yes/No]; 3) Relatedness: is the sentence seman-
tically related to the target domain? [Yes/No]; 4)
Correctness: is the sentence grammatically cor-
rect? [Ungrammatical/Slightly disfluent/Fluent];
5) Success: could the sentence be a good slogan
for the target domain? [As it is/With minor edit-
ing/No]. In these last two cases, the annotators
1451
were instructed to select the middle option only
in cases where the gap with a correct/successful
sentence could be filled just by performing minor
editing. The annotation form had no default val-
ues, and the annotators did not know how the eval-
uated sentences were generated, or whether they
were the outcome of one or more systems.
We started by collecting slogans from an on-
line repository of slogans5. Then, we randomly
selected a subset of these slogans and for each of
them we generated an input specification U for the
system. We used the commercial domain of the
advertised product as the target domain d. Two
or three content words appearing in each slogan
were randomly selected as the target words t. We
did so to simulate the brainstorming phase behind
the slogan generation process, where copywriters
start with a set of relevant keywords to come up
with a catchy slogan. In all cases, we set the tar-
get emotion to ?positive? as we could not estab-
lish a generally valid criteria to associate a spe-
cific emotion to a product. Concerning chromatic
slanting, for target domains having a strong chro-
matic correlation we allowed the system to slant
the generated sentences accordingly. In the other
cases, a random color association was selected. In
this manner, we produced 10 tuples ?t,d, c, e, p?.
Then, from each tuple we produced 5 complete
user specifications by enabling or disabling differ-
ent feature function combinations6. The four com-
binations of features are: base: Target-word scorer
+ N-gram likelihood + Dependency likelihood +
Variety scorer + Unusual-words scorer + Seman-
tic cohesion; base+D: all the scorers in base +
Domain relatedness; base+D+C: all the scorers in
base+D + Chromatic connotation; base+D+E: all
the scorers in base+D + Emotional connotation;
base+D+P: all the scorers in base+D + Phonetic
features. For each of the resulting 50 input config-
urations, we generated up to 10 creative sentences.
As the system could not generate exactly 10 solu-
tions in all the cases, we ended up with a set of
432 items to annotate. The weights of the feature
functions were set heuristically, due to the lack
of an annotated dataset suitable to learn an opti-
5http://www.tvacres.com/advertising_
slogans.htm
6An alternative strategy to keep the annotation effort un-
der control would have been to generate fewer sentences from
a larger number of inputs. We adopted the former setting
since we regarded it as more similar to a brainstorming ses-
sion, where the system proposes different alternatives to in-
spire human operators. Forcing BRAINSUP to only output
one or two sentences would have limited its ability to explore
and suggest potentially valuable outputs.
MC Cat. Hum. Corr. Rel. Succ. RND2 RND3
2 - - 16.67 - 22.22 - 37.043 47.45 39.58 43.52 13.66 44.21 62.50 49.384 33.10 37.73 32.18 21.99 22.22 31.25 12.355 19.44 22.69 07.64 64.35 11.34 06.25 01.23
Table 2: Majority classes (%) for the five dimen-
sions of the annotation.
mal weight configuration. We started by assign-
ing the highest weight to the Target Word scorer
(i.e., 1.0), followed by the Variety and Unusual
Word scorers (0.99), the Phonetic Features, Chro-
matic/Emotional Connotation and Semantic Co-
hesion scorers (0.98) and finally the Domain, N -
gram and Dependency Likelihood scorers (0.97).
These settings allow us to enforce an order of
precedence among the scorers during slot-filling,
while giving them virtually equal relevance for so-
lution ranking.
As discussed in Section 3 we use two differ-
ent treebanks to learn the syntactic patterns (P)
and the dependency operators (L). For these ex-
periments, patterns were learned from a corpus
of 16,000 proverbs (Mihalcea and Strapparava,
2006), which offers a good selection of short sen-
tences with a good potential to be used for slo-
gan generation. This choice seemed to be a good
compromise as, to our best knowledge, there is
no published slogan dataset with an adequate size.
Besides, using existing slogans might have legal
implications that we might not be aware of. De-
pendency operators were learned by dependency
parsing the British National Corpus7. To reduce
the amount of noise introduced by the automatic
parses, we only considered sentences having less
than 20 words. Furthermore, we only considered
sentences in which all the content words are listed
in WordNet (Miller, 1995) with the observed part
of speech.8 The LSA space used for the semantic
feature functions was also learned on BNC data,
but in this case no filtering was applied.
4.1 Results
To measure the agreement among the annota-
tors, similarly to Mohammad (2011) and Ozbal
and Strapparava (2012) we calculated the majority
class for each dimension of the annotation task. A
7http://www.natcorp.ox.ac.uk/
8Since the CMU pronouncing dictionary used by the pho-
netic scorers is based on the American pronunciation of
words, we actually pre-processed the whole BNC by replac-
ing all British-English words with their American-English
counterparts. To this end, we used the mapping available at
http://wordlist.sourceforge.net/.
1452
Cat. Rel. Hum. Succ. Corr.
Yes 67.59 93.98 12.73 32.41 64.35
Partly - - - 23.15 31.71
No 32.41 06.02 87.27 44.44 03.94
Table 3: Majority decisions (%) for each annota-
tion dimension.
majority class greater than or equal to 3 means that
the absolute majority of the 5 annotators agreed
on the same decision9. Table 2 shows the ob-
served agreement for each dimension. The column
labeled RND2 (RND3) shows the random agree-
ment for a given number of annotators and a binary
(ternary) decision. For example, all five annotators
(MC=5) agreed on the annotation of the catchiness
of the slogans in 19.44% of the cases. The random
chance of agreement for 5 annotators on the binary
decision problem is 6.25%. The figures for MC ?
4 are generally high, confirming a good agreement
among the annotators. The agreement on the relat-
edness of the slogans is especially high, with all 5
annotators taking the same decision in almost two
cases out of three, i.e., 64.35%.
Table 3 lists the distribution of answers for each
dimension in the cases where a decision can be
taken by majority vote. The generated slogans
are found to be catchy in more than 2/3 of the
cases, (i.e., 67.59%), completely successful in 1/3
of the cases (32.41%) and completely correct in
2/3 of the cases (64.35%). These figures demon-
strate that BRAINSUP is very effective in gener-
ating grammatical utterances that have all the ap-
pealing properties of a successful slogan. As for
humor, the sentences are found to have this prop-
erty in only 12.73% of cases. Even though the
figure is not very high, we should also consider
that BRAINSUP is not explicitly trying to gener-
ate amusing utterances. Concerning success, we
should point out that in 23.15% of the cases the
annotators have found that the generated slogans
have the potential to be turned into successful ones
only with minor editing. This is a very important
piece of result, as it corroborates our claim that
BRAINSUP can indeed be a valuable tool for copy-
writing, even when it does not manage to output a
perfectly good sentence. Similar conclusions can
be drawn concerning the correctness of the output,
as in almost one third of the cases the slogans are
9For the binary decisions (i.e., catchiness, relatedness and
humor), at least 3 annotators out of 5 must necessarily agree
on the same option.
only affected by minor disfluencies.
The relatedness figure is especially high, as in
almost 94% of the cases the majority of annota-
tors found the slogans to be pertinent to the tar-
get domain. This result is not surprising, as all
the slogans are generated by considering keywords
that already exist in real slogans for the same do-
main. Anyhow, this is exactly the kind of setting in
which we expect BRAINSUP to be employed, i.e.,
to support creative sentence generation starting
from a good set of relevant keywords. Nonethe-
less, it is very encouraging to observe that the gen-
eration process does not deteriorate the positive
impact of the input keywords.
We would also like to mention that in 63 cases
(14.58%) the majority of the annotators have la-
beled the slogans favorably across all 5 dimen-
sions. The examples listed in Table 1 are selected
from this set. It is interesting to observe how
the word associations established by BRAINSUP
can result in pertinent yet unintentional rhetori-
cal devices such as metaphors (?a summer sun-
shine?), puns (?lash your drama?) and personifica-
tions (?lips and eyes want?). Some examples show
the effect of the phonetic features, e.g. plosives in
?passionate kiss, perfect lips?, alliteration in ?the
dark drink? and rhyming in ?lips and eyes want
the kiss?. In some cases, the output of BRAINSUP
seems to be governed by mysterious philosophical
reasoning, as in the delicate examples generated
for ?soap?.
For comparison, Table 4 lists a selection of
the examples that have been labeled as unsuc-
cessful by the majority of raters. In some cases,
BRAINSUP is improperly selecting attributes that
highlight undesirable properties in the target do-
main, e.g., ?A pleasant tasting, a heady wine?. To
avoid similar errors, it would be necessary to rea-
son about the valence of an attribute for a spe-
cific domain. In other cases, the N -gram and the
Dependency Likelihood features may introduce
phrases which are very cohesive but unrelated to
the rest of the sentence, e.g., ?Unscrupulous doc-
tors smoke armored units?. Many of these errors
could be solved by increasing the weight of the
Semantic Cohesion and Domain Relatedness scor-
ers. In other cases, such as ?A sixth calorie may
taste an own good? or ?A same sunshine is fewer
than a juice of day?, more sophisticated reason-
ing about syntactic and semantic relations in the
output might be necessary in order to enforce the
generation of sound and grammatical sentences.
We could not find a significant correlation be-
1453
Domain Keywords BRAINSUP output examples
pleasure wine, tast-
ing
A pleasant tasting, a heady wine.
? A fruity tasting may drink a
sparkling wine.
healthy day, juice,
sunshine
Drink juice of your sunshine, and
your weight will choose day of
you. ? A same sunshine is fewer
than a juice of day.
cigarette doctors,
smoke
Unscrupulous doctors smoke ar-
mored units. ? Doctors smoke no
arrow.
mascara drama,
lash
The such drama is the lash.
soap skin, love,
touch
The touch of skin is the love of
cacophony. ? You love an own
skin for a first touch.
coke calorie,
taste, good
A sixth calorie may taste an own
good.
coffee waking,
cup
You cannot cup hands without
waking some fats.
Table 4: Unsuccessful BRAINSUP outputs.
tween the input variables (e.g., presence or ab-
sence of phonetic features or chromatic slanting)
and the outcome of the annotation, i.e. the sys-
tem by and large produces correct, catchy, related
and (at least potentially) successful outputs regard-
less of the specific input configurations. In this re-
spect, it should be noted that we did not carry out
any kind of optimization of the feature weights,
which might be needed to obtain more heavily
characterized sentences. Furthermore, to better
appreciate the contribution of the individual fea-
tures, comparative experiments in which the users
evaluate the system before and after triggering a
feature function might be necessary. Concern-
ing the correlation among output dimensions, we
only observed relatively high Spearman correla-
tion between correctness and relatedness (0.65),
and catchiness and success (0.68).
5 Conclusion
We have presented BRAINSUP, a novel system
for creative sentence generation that allows users
to control many aspects of the creativity process,
from the presence of specific target words in the
output, to the selection of a target domain, and
to the injection of phonetic and semantic proper-
ties in the generated sentences. BRAINSUP makes
heavy use of dependency parsed data and statistics
collected from dependency treebanks to ensure the
grammaticality of the generated sentences, and to
trim the search space while seeking the sentences
that maximize the user satisfaction.
The system has been designed as a support-
ing tool for a variety of real-world applications,
from advertisement to entertainment and educa-
tion, where at the very least it can be a valu-
able support for time-consuming and knowledge-
intensive sentence generation needs. To demon-
strate this point, we carried out an evaluation on a
creative sentence generation benchmark showing
that BRAINSUP can effectively produce catchy,
memorable and successful sentences that have the
potential to inspire the work of copywriters.
To our best knowledge, this is the first system-
atic attempt to build an extensible framework that
allows for multi-dimensional creativity while at
the same time relying on syntactic constraints to
enforce grammaticality. In this regard, our ap-
proach is dual with respect to previous work based
on lexical substitution, which suffers from limited
expressivity and creativity latitude. In addition, by
acquiring the lexicon and the sentence structure
from two distinct corpora, we can guarantee that
the sentences that we generate have never been
observed. We believe that our contribution con-
stitutes a valid starting point for other researchers
to deal with unexplored dimensions of creativity.
As future work, we plan to use machine learn-
ing techniques to estimate optimal weights for the
feature functions in different use cases. We would
also like to consider syntactic clues while reason-
ing about semantic properties of the sentence, e.g.,
color and emotion associations, instead on relying
solely on lexical semantics. Concerning the exten-
sion of the capabilities of BRAINSUP, we want to
include common-sense knowledge and reasoning
to profit from more sophisticated semantic rela-
tions and to inject humor on demand. Further tun-
ing of BRAINSUP to build a dedicated system for
slogan generation is also part of our future plans.
After these improvements, we would like to con-
duct a more focused evaluation on slogan genera-
tion involving human copywriters and domain ex-
perts in an interactive setting.
We would like to conclude this paper with a pearl
of BRAINSUP?s wisdom:
It is wiser to believe in science
than in everlasting love.
Acknowledgments
Go?zde O?zbal and Carlo Strapparava were partially
supported by the PerTe project (Trento RISE).
1454
References
Kim Binsted and Graeme Ritchie. 1997. Computa-
tional rules for generating punning riddles. Humor -
International Journal of Humor Research, 10(1):25?
76, January.
Andy Borman, Rada Mihalcea, and Paul Tarau. 2005.
Pic-net: Pictorial representations for illustrated se-
mantic networks. In Proceedings of the AAAI Spring
Symposium on Knowledge Collection from Volun-
teer Contributors.
Simon Colton, Jacob Goodwin, and Tony Veale. 2012.
Full-FACE Poetry Generation. In Proceedings of
the 3rd International Conference on Computational
Creativity, pages 95?102.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Journal
Of The American Society for Information Science,
41(6):391?407.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC?06), pages 417?422.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry
with applications to generation and translation. In
EMNLP, pages 524?533.
Marco Guerini, Carlo Strapparava, and Oliviero Stock.
2011. Slanting existing text with valentino. In Pro-
ceedings of the 16th international conference on In-
telligent user interfaces, IUI ?11, pages 439?440,
New York, NY, USA. ACM.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kevin S. LaBar and Roberto Cabeza. 2006. Cognitive
neuroscience of emotional memory. Nature reviews.
Neuroscience, 7(1):54?64, January.
Kevin Lenzo. 1998. The cmu pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
Ruli Manurung, Graeme Ritchie, Helen Pain, An-
nalu Waller, Dave O?Mara, and Rolf Black. 2008.
The Construction of a Pun Generator for Language
Skills Development. Applied Artificial Intelligence,
22(9):841?869, October.
J McKay. 2002. Generation of idiom-based witticisms
to aid second language learning. In Twente Work-
shop on Language Technology 20, pages 70?74.
R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Journal of Computational In-
telligence, 22(2):126?142, May.
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, 38:39?41.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: using
mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ?10, pages 26?
34, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Saif Mohammad. 2011. Even the abstract have color:
Consensus in word-colour associations. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 368?373, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Go?zde O?zbal and Carlo Strapparava. 2011. Autom-
atized Memory Techniques for Vocabulary Acquisi-
tion in a Second Language. In Alexander Verbraeck,
Markus Helfert, Jose? Cordeiro, and Boris Shishkov,
editors, CSEDU, pages 79?87. SciTePress.
Gozde Ozbal and Carlo Strapparava. 2012. A compu-
tational approach to the automation of creative nam-
ing. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 703?711, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
N. Sagarra and M. Alba. 2006. The key is in the
keyword: L2 vocabulary learning methods with be-
ginning learners of spanish. The Modern Language
Journal, 90(2):228?243.
Oliviero Stock and Carlo Strapparava. 2006. Laughing
with hahacronym, a computational humor system.
In proceedings of the 21st national conference on
Artificial intelligence - Volume 2, pages 1675?1678.
AAAI Press.
J. M. Toivanen, H. Toivonen, A. Valitutti, and O. Gross.
2012. Corpus-based Generation of Content and
Form in Poetry. In International Conference on
Computational Creativity, pages 175?179.
A. Valitutti, C. Strapparava, , and O. Stock. 2009.
Graphlaugh: a tool for the interactive generation of
humorous puns. In Proceedings of ACII-2009, Third
Conference on Affective Computing and Intelligent
Interaction, Demo track.
1455
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 892?901,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Modelling Events through Memory-based, Open-IE Patterns
for Abstractive Summarization
Daniele Pighin
Google Inc.
biondo@google.com
Marco Cornolti
?
University of Pisa, Italy
cornolti@di.unipi.it
Enrique Alfonseca
Google Inc.
ealfonseca@google.com
Katja Filippova
Google Inc.
katjaf@google.com
Abstract
Abstractive text summarization of news
requires a way of representing events, such
as a collection of pattern clusters in which
every cluster represents an event (e.g.,
marriage) and every pattern in the clus-
ter is a way of expressing the event (e.g.,
X married Y, X and Y tied the knot). We
compare three ways of extracting event
patterns: heuristics-based, compression-
based and memory-based. While the for-
mer has been used previously in multi-
document abstraction, the latter two have
never been used for this task. Compared
with the first two techniques, the memory-
based method allows for generating sig-
nificantly more grammatical and informa-
tive sentences, at the cost of searching a
vast space of hundreds of millions of parse
trees of known grammatical utterances. To
this end, we introduce a data structure and
a search method that make it possible to
efficiently extrapolate from every sentence
the parse sub-trees that match against any
of the stored utterances.
1 Introduction
Text summarization beyond extraction requires a
semantic representation that abstracts away from
words and phrases and from which a summary can
be generated (Mani, 2001; Sp?arck-Jones, 2007).
Following and extending recent work in semantic
parsing, information extraction (IE), paraphrase
generation and summarization (Titov and Klemen-
tiev, 2011; Alfonseca et al, 2013; Zhang and
Weld, 2013; Mehdad et al, 2013), the represen-
tation we consider in this paper is a large collec-
?
Work done during an internship at Google Zurich.
[John Smith] and 
[Mary Brown] wed 
in [Baltimore]...
[Smith] tied the 
knot with [Brown] 
this Monday...
#21: death
#22: divorce
#23: marriage
PER married PER
PER and PER wed
PER tied the knot with PER
PER has married PER
John Smith married Mary Brown
e1: J. Smith (PER)
e2: M. Brown (PER)
e3: Baltimore, MD (LOC)
Figure 1: An example of abstracting from input
sentences to an event representation and genera-
tion from that representation.
tion of clusters of event patterns. An abstractive
summarization system relying on such a represen-
tation proceeds by (1) detecting the most relevant
event cluster for a given sentence or sentence col-
lection, and (2) using the most representative pat-
tern from the cluster to generate a concise sum-
mary sentence. Figure 1 illustrates the summa-
rization architecture we are assuming in this pa-
per. Given input text(s) with resolved and typed
entity mentions, event mentions and the most rele-
vant event cluster are detected (first arrow). Then,
a summary sentence is generated from the event
and entity representations (second arrow).
However, the utility of such a representation for
summarization depends on the quality of pattern
clusters. In particular, event patterns must cor-
respond to grammatically correct sentences. In-
troducing an incomplete or incomprehensible pat-
tern (e.g., PER said PER) may negatively affect
both event detection and sentence generation. Re-
lated work on paraphrase detection and relation
extraction is mostly heuristics-based and has re-
lied on hand-crafted rules to collect such patterns
(see Sec. 2). A standard approach is to focus
on binary relations between entities and extract
892
EYent 
moGel
Pattern 
FlXVterinJ
1ewV 
FlXVterV
Pattern 
e[traFtion
1ewV 
artiFle
Pattern 
e[traFtion
,nIerenFe
$EVtraFtiYe
VXmmar\
Figure 2: A generic pipeline for event-driven ab-
stractive headline generation.
the dependency path between the two entities as
an event representation. An obvious limitation
of this approach is there is no guarantee that the
extracted pattern corresponds to a grammatically
correct sentence, e.g., that an essential preposi-
tional phrase is retained like in file for a divorce.
In this paper we explore two novel, data-driven
methods for event pattern extraction. The first,
compression-based method uses a robust sentence
compressor with an aggressive compression rate
to get to the core of the sentence (Sec. 3). The
second, memory-based method relies on a vast
collection of human-written headlines and sen-
tences to find a substructure which is known to
be grammatically correct (Sec. 4). While the lat-
ter method comes closer to ensuring perfect gram-
maticality, it introduces a problem of efficiently
searching the vast space of known well-formed
patterns. Since standard iterative approaches com-
paring every pattern with every sentence are pro-
hibitive here, we present a search strategy which
scales well to huge collections (hundreds of mil-
lions) of sentences.
In order to evaluate the three methods, we con-
sider an abstractive summarization task where the
goal is to get the gist of single sentences by recog-
nizing the underlying event and generating a short
summary sentence. To the best of our knowledge,
this is the first time that this task has been pro-
posed; it can be considered as abstractive sentence
compression, in contrast to most existing sentence
compression systems which are based on selecting
words from the original sentence or rewriting with
simpler paraphrase tables. An extensive evalua-
tion with human raters demonstrates the utility of
the new pattern extraction techniques. Our analy-
sis highlights advantages and disadvantages of the
three methods.
To better isolate the qualities of the three ex-
traction methodologies, all three methods use the
same training data and share components of the
Algorithm 1 HEURISTICEXTRACTOR(T,E): heuristi-
cally extract relational patterns for the dependency parse T
and the set of entities E.
1: /* Global constants /*
2: global V
p
, V
c
, N
p
, N
c
3: V
c
? {subj, nsubj, nsubjpass, dobj, iobj, xcomp,
4: acomp, expl, neg, aux, attr, prt}
5: V
p
? {xcomp}
6: N
c
? {det, predet, num, ps, poss, nc, conj}
7: N
p
? {ps, poss, subj, nsubj, nsubjpass, dobj, iobj}
8: /* Entry point /*
9: P ? ?
10: for all C ? COMBINATIONS(E) do
11: N ? MENTIONNODES(T,C)
12: N
?
? APPLYHEURISTICS(T, BUILDMST(T,N))
13: P ? P ? {BUILDPATTERN(T,N
?
)}
14: return P
15: /* Procedures /*
16: procedure APPLYHEURISTICS(T,N )
17: N
?
? N
18: while |N
?
| > 0 do
19: N
??
? ?
20: for all n ? N
?
do
21: if n.ISVERB() then
22: N
??
? N
??
? INCLUDECHILDREN(n, V
c
)
23: N
??
? N
??
? INCLUDEPARENT(n, V
p
)
24: else if n.ISNOUN() then
25: N
??
? N
??
? INCLUDECHILDREN(n,N
c
)
26: N
??
? N
??
? INCLUDEPARENT(n,N
p
)
27: N
?
? N
??
\N
?
28: procedure INCLUDECHILDREN(n,L)
29: R? ?
30: for all c ? n.CHILDREN() do
31: if c.PARENTEDGELABEL() ? L then
32: R? R ? {c}
33: return R
34: procedure INCLUDEPARENT(n,L)
35: if n.PARENTEDGELABEL() ? L then
36: return {n}
37: else return ?
very same summarization architecture, as shown
in Figure 2: an event model is constructed by clus-
tering the patterns extracted according to the se-
lected extraction method. Then, the same extrac-
tion method is used to collect patterns from sen-
tences in never-seen-before news articles. Finally,
the patterns are used to query the event model and
generate an abstractive summary. The three differ-
ent pattern extractors are detailed in the next three
sections.
2 Heuristics-based pattern extraction
In order to be able to work in an Open-IE man-
ner, applicable to different domains, most existing
pattern extraction systems are based on linguisti-
cally motivated heuristics. Zhang and Weld (2013)
is based on REVERB (Fader et al, 2011), which
uses a regular expression on part-of-speech tags
to produce the extractions. An alternative system,
893
OLLIE (Schmitz et al, 2012), uses syntactic de-
pendency templates to guide the pattern extraction
process.
The heuristics used in this paper are inspired by
Alfonseca et al (2013), who built well formed re-
lational patterns by extending minimum spanning
trees (MST) which connect entity mentions in a
dependency parse. Algorithm 1 details our re-
implementation of their method and the specific
set of rules that we rely on to enforce pattern gram-
maticality. We use the standard Stanford-style set
of dependency labels (de Marneffe et al, 2006).
The input to the algorithm are a parse tree T and
a set of target entities E. We first generate com-
binations of 1-3 elements of E (line 10), then for
each combination C we identify all the nodes in
T that mention any of the entities in C. We con-
tinue by constructing the MST of these nodes, and
finally apply our heuristics to the nodes in the
MST. The procedure APPLYHEURISTICS (:16) re-
cursively grows a nodeset N
?
by including chil-
dren and parents of noun and verb nodes in N
?
based on dependency labels. For example, we in-
clude all children of verbs in N
?
whose label is
listed in V
c
(:3), e.g., active or passive subjects,
direct or indirect objects, particles and auxiliary
verbs. Similarly, we include the parent of a noun
in N
?
if the dependency relation between the node
and its parent is listed in N
p
.
3 Pattern extraction by sentence
compression
Sentence compression is a summarization tech-
nique that shortens input sentences preserving the
most important content (Grefenstette, 1998; Mc-
Donald, 2006; Clarke and Lapata, 2008, inter
alia). While first attempts at integrating a com-
pression module into an extractive summarization
system were not particularly successful (Daum?e
III and Marcu, 2004, inter alia), recent work
has been very promising (Berg-Kirkpatrick et al,
2011; Wang et al, 2013). It has shown that drop-
ping constituents of secondary importance from
selected sentences ? e.g., temporal modifiers or
relative clauses ? results in readable and more in-
formative summaries. Unlike this related work,
our goal here is to compress sentences to obtain
an event pattern ? the minimal grammatical struc-
ture expressing an event. To our knowledge, this
application of sentence compressors is novel. As
in Section 2, we only consider sentences mention-
ing entities and require the compression (pattern)
to retain at least one such mention.
Sentence compression methods are abundant
but very few can be configured to produce out-
put satisfying certain constraints. For example,
most compression algorithms do not accept com-
pression rate as an argument. In our case, sen-
tence compressors which formulate the compres-
sion task as an optimization problem and solve it
with integer linear programming (ILP) tools un-
der a number of constraints are particularly attrac-
tive (Clarke and Lapata, 2008; Filippova and Al-
tun, 2013). They can be extended relatively easily
with both the length constraint and the constraint
on retaining certain words. The method of Clarke
and Lapata (2008) uses a trigram language model
(LM) to score compressions. Since we are inter-
ested in very short outputs, a LM trained on stan-
dard, uncompressed text would not be suitable. In-
stead, we chose to modify the method of Filippova
and Altun (2013) because it relies on dependency
parse trees and does not use any LM scoring.
Like other syntax-based compressors, the sys-
tem of Filippova and Altun (2013) prunes depen-
dency structures to obtain compression trees and
hence sentences. The objective function to maxi-
mize in an ILP problem (Eq. 1) is formulated over
weighted edges in a transformed dependency tree
and is subject to a number of constraints. Edge
weight is defined as a linear function over a fea-
ture set: w(e) = w ? f(e).
F (X) =
?
e?E
x
e
? w(e) (1)
In our reimplementation we followed the algo-
rithm as described by Filippova and Altun (2013).
The compression tree is obtained in two steps.
First, the input tree is transformed with determin-
istic rules, most of which aim at collapsing indis-
pensable modifiers with their heads (determiners,
auxiliary verbs, negation, multi-word expressions,
etc.). Then a sub-tree maximizing the objective
function is found under a number of constraints.
Apart from the structural constrains from the
original system which ensure that the output is a
valid tree, the constraints we add state that:
1. tree size in edges must be in [3, 6],
2. entity mentions must be retained,
3. subject of the clause must be retained,
4. the sub-tree must be covered by a single
clause ? exactly one finite verb must used.
894
Since we consider compressions with different
lengths as candidates, from this set we select the
one with the maximum averaged edge weight as
the final compression. Figure 3 illustrates the
use of the compressor for obtaining event pat-
terns. Dashed edges are dropped as a result of
constrained compression so that the output is John
Smith married Mary Brown and the event pattern
is PER married PER. Note that the root of a sub-
clause is allowed to be the top-level node in the
extracted compression.
Compared with patterns obtaines with heuris-
tics, compression patterns should retain preposi-
tional verb arguments whose removal would ren-
der the pattern ungrammatical. As an example
consider [C. Zeta-Jones] and [M. Douglas] filed
for divorce. The heuristics-based pattern is PER
and PER filed which is incomplete. Unlike it,
the compression-based method keeps the essential
prepositional phrase for divorce in the pattern be-
cause the average edge weight is greater for the
tree with the prepositional phrase.
4 Memory-based pattern extraction
Neither heuristics-based, nor compression-based
methods provide a guarantee that the extracted
pattern is grammatically correct. In this sec-
tion we introduce an extraction technique which
makes it considerably more likely because it only
extracts patterns which have been observed as
full sentences in a human-written text (Sec. 4.1).
However, this memory-based method also poses
a problem not encountered by the two previous
methods: how to search over the vast space of ob-
served headlines and sentences to extract a pattern
from a given sentence? Our trie-based solution,
which we present in the remainder of this sec-
tion, makes it possible to compare a dependency
graph against millions of observed grammatical
utterances in a fraction of a second.
4.1 A tree-trie to store them all. . .
Our objective is to construct a compact representa-
tion of hundreds of millions of observed sentences
that can fit in the memory of a standard worksta-
tion. This data structure should make it possible
to efficiently identify the sub-trees of a sentence
that match any complete utterance previously ob-
served. To this end, we build a trie of depen-
dency trees (which we call a tree-trie) by scan-
ning all the dependency parses in the news training
Algorithm 2 STORE(T, I): store the dependency tree T
in the tree-trie I .
1: /* Entry point /*
2: L? T.LINEARIZE()
3: STORERECURSION(I.ROOT(), L, 0)
4: return M
5: /* Procedures /*
6: procedure STORERECURSION(n,L, o)
7: if o == L.LENGTH() then
8: n.ADDTREESTRUCTURE(L.STRUCTURE())
9: return
10: if not n.HASCHILD(L.TOKEN(o)) then
11: n.ADDCHILD(L.TOKEN(o))
12: n
?
? n.GETCHILD(L.TOKEN(o))
13: STORERECURSION(n
?
, L, o+ 1)
data, and index each tree in the tree-trie accord-
ing to Algorithm 2. For better clarity, the process
is also described graphically in Figure 4. First,
each dependency tree (a) is linearized, resulting
in a data structure that consists of two aligned se-
quences (b). The first sequence (tokens) encodes
word/parent-relation pairs, while the second se-
quence (structure) encodes the offsets of parent
nodes in the linearized tree. As an example, the
first word ?The? is a determiner (?det?) for the sec-
ond node (offset 1) in the sequence, which is ?cat?.
In turn, ?cat? is the subject (?nsubj?) of the node
in position 2, i.e., ?sleeps?. As described in Algo-
rithm 2, we recursively store the token sequence
in the trie, each word/relation pair being stored in
a node. When the token sequence is completely
consumed, we store in the current trie node the
structure of the linearized tree. Combining struc-
tural information with the sequential information
encoded by each path in the trie makes it possi-
ble to rebuild a complete dependency graph. Fig-
ure 4(c) shows an example trie encoding 4 differ-
ent sentences. We highlighted in bold the path cor-
responding to the linearized form (b) of the exam-
ple parse tree (a).
The figure shows that the tree contains two
kinds of nodes: end-of-sentence (EOS) nodes
(red) and non-terminal nodes (in blue). EOS nodes
do not necessarily coincide with trie leaves, as it
is possible to observe complete sentences embed-
ded in longer ones. EOS nodes differ from non-
terminal nodes in that they store one or more struc-
tural sequences corresponding to different syntac-
tic representations of observed sentences with the
same tokens.
Space-complexity and generalization. Storing
all the observed sentences in a single trie requires
huge amounts of memory. To make it possible to
895
root Our sources report John Smith married Mary Brown in Baltimore yesterday
root
root
subj
subj
obj
in
tmod
Figure 3: Transformed dependency tree with a sub-tree expressing an event pattern.
The cat sleeps under the table
root
nsubj
det
prep
pobj
det
(a)
The
det
cat
nsubj
sleeps
ROOT
under
prep
the
det
table
pobj
1 2 -1 2 5 3
(b)
The
det
dog
nsubj
barked
ROOT
1,2,-1
cat
nsubj
sleeps
ROOT
1,2,-1
soundly
advmod
1,2,-1,2
under
prep
the
det
table
pobj
1,2,-1,2,5,3
(c)
Figure 4: A dependency tree (a), its linearized form (b) and the resulting path in a trie (c), in bold.
store a complete tree-trie in memory, we adopt the
following strategy. We replace the surface form of
entity nodes with the coarse entity type (e.g., PER,
LOC, ORG) of the entity. Similarly, we replace
proper nouns with the placeholder ?[P]?, thus sig-
nificantly reducing lexical sparsity. Then, we en-
code each distinct word/relation pair as a 32-bit
unsigned integer. Assuming a maximum tree size
of 255 nodes, we represent structure sequences as
vectors of type unsigned char (8 bit per element).
Finally, we store trie-node children as sorted vec-
tors instead of hash maps to reduce memory foot-
print. As a result, we are able to load a trie encod-
ing 400M input dependency parses, 170M distinct
nodes and 48M distinct sentence structures in un-
der 10GB of RAM.
4.2 . . . and in the vastness match them
At lookup time, we want to use the tree-trie to
identify all sub-graphs of an input dependency tree
T that match at least a complete observed sen-
tence. To do so, we need to identify all paths in
the trie that match any sub-sequence s of the lin-
earized sequence of T nodes. Whenever we en-
counter an EOS node e, we verify if any of the
structures stored at e matches the sub-tree gener-
ated by s. If so, then we have a positive match.
As a sentence might embed many shorter utter-
ances, each input T will generally yield multiple
matches. For example, querying the tree-trie in
Figure 4(c) with the input tree shown in (a) would
yield two results, as both The cat sleeps and The
cat sleeps under the table are complete utterances
stored in the trie.
Algorithm 3 LOOKUP(T, I): Lookup for matches of sub-
set of tree T in the trie index I .
1: /* Entry point /*
2: L? T.LINEARIZE()
3: M ? ?
4: LOOKUPRECURSIVE(T,L, 0, I.ROOT(), ?,M)
5: return M
6: /* Procedures /*
7: procedure LOOKUPRECURSIVE(T,L, o, n, P,M )
8: for all i ? [o, L.LENGTH()) do
9: if n.HASCHILD(L.TOKEN(i)) then
10: n
?
? n.GETCHILD(L.TOKEN(i))
11: P
?
? P ? {i}
12: for all S ? n
?
.TREESTRUCTURES() do
13: if L.ISCOMPATIBLE(S, P
?
) then
14: M ?M ? {T.GETNODES(P
?
)}
15: LOOKUPRECURSIVE(L, i, o+ 1, n
?
, P
?
,M)
Algorithm 3 describes the lookup process in
more detail. The first step consists in the lineariza-
tion of the input tree T . Then, we recursively tra-
verse the trie calling LOOKUPRECURSIVE. The
inputs of this procedure are: the input tree T , its
linearization L and an offset o (starting at 0), the
trie node currently being traversed n (starting with
the root), the set of offsets in L that constitute a
partial match P (initially empty) and the set of
complete matches found M . We recursively tra-
verse all the nodes in the trie that yield a partial
match with any sub-sequence of the linearized to-
kens of T . At each step, we scan all the tokens
in L in the range [o, L.LENGTH()) looking for to-
kens matching any of the children of n. If a match-
ing node is found, a new partial match P
?
is con-
structed by extending P with the matching token
896
Sheet6
Page 1
0 10 20 30 40 50 60 70 80 901.0E-05
1.0E-04
1.0E-03
1.0E-02
1.0E-01
1.0E+00
1.0E+01
f(x) = 1.9E-07 x^3.3E+00
Tree size (number of nodes)
Time (seconds)
Figure 5: Time complexity of lookup operations
for inputs of different sizes.
offset i (line 11), and the recursion continues from
the matching trie node n
?
and offset i (line 15).
Every time a partial match is found, we verify if
the partial match is compatible with any of the
tree structures stored in the matching node. If that
is the case, we identify the corresponding set of
matching nodes in T and add it to the result M
(lines 12-14). A pattern is generated from each
complete match returned by LOOKUP after apply-
ing a simple heuristic: for each verb node v in the
match, we enforce that negations and auxiliaries in
T depending from x are also included in the pat-
tern.
Time complexity of lookups. Let k be the max-
imum fan-out of trie nodes, d be the depth of
the trie and n be the size of an input tree (num-
ber of nodes). If trie node children are hashed
(which has a negative effect on space complex-
ity), then worst case complexity of LOOKUP() is
O(nk)
d?1
. If they are stored as sorted lists, as in
our memory-efficient implementation, theoretical
complexity becomes O(nk log(k))
d?1
. It should
be noted that worst case complexity can only be
observed under extremely unlikely circumstances,
i.e., that at every step of the recursion all the nodes
in the tail of the linearized tree match a child of
the current node. Also, in the actual trie used in
our experiments the average branching factor k is
very small. We observed that a trie storing 400M
sentences (170M nodes) has an average branching
factor of 1.02. While the root of the trie has unsur-
prisingly many children (210K, all the observed
first sentence words), already at depth 2 the aver-
age fan-out is 13.7, and at level 3 it is 4.9.
For an empirical analysis of lookup complexity,
Figure 5 plots, in black, wall-clock lookup time
as a function of tree size n for a random sample
of 1,600 inputs. As shown by the polynomial re-
gression curve (red), observed lookup complexity
is approximately cubic with a very small constant
factor. In general, we can see that for sentences of
common length (20-50 words) a lookup operation
can be completed in well under one second.
5 Evaluation
5.1 Experimental settings
All the models for the experiments that we present
have been trained using the same corpus of
news crawled from the web between 2008 and
2013. The news have been processed with a to-
kenizer, a sentence splitter (Gillick and Favre,
2009), a part-of-speech tagger and dependency
parser (Nivre, 2006), a co-reference resolution
module (Haghighi and Klein, 2009) and an entity
linker based on Wikipedia and Freebase (Milne
and Witten, 2008). We use Freebase types as fine-
grained named entity types, so we are also able to
label e.g. instances of sports teams as such instead
of the coarser label ORG.
Next, the news have been grouped based on
temporal closeness (Zhang and Weld, 2013) and
cosine similarity (using tf?idf weights). For each
of the three pattern extraction methods we used the
same summarization pipeline (as shown above in
Figure 2):
1. Run pattern extraction on the news.
2. For every news collection Coll and entity set
E, generate a set containing all the extracted
patterns from news in Coll mentioning all
the entities in E. These are patterns that are
likely to be paraphrasing each other.
3. Run a clustering algorithm to group together
patterns that typically co-occur in the sets
generated in the previous step. There are
many choices for clustering algorithms (Al-
fonseca et al, 2013; Zhang and Weld, 2013).
Following Alfonseca et al (2013) we use in
this work a Noisy-OR Bayesian Network be-
cause it has already been applied for abstrac-
tive summarization (albeit multi-document),
it provides an easily interpretable probabilis-
tic clustering, and training can be easily par-
allelized to be able to handle large training
sets. The hidden events in the Bayesian net-
work represent pattern clusters. When train-
ing is done, for each extraction pattern p
j
897
Original sentence Abstractive summary (method)
Two-time defending overall World Cup champion Marcel Hirscher won the
challenging giant slalom on the Gran Risa course with two solid runs Sunday
and attributed his victory to a fixed screw in his equipment setup.
Marcel Hirscher has won the giant
slalom. (C)
Zodiac Aerospace posted a 7.9 percent rise in first-quarter revenue, below mar-
ket expectations, but reaffirmed its full-year financial targets.
Zodiac Aerospace has reported a rise in
profits. (C)
Australian free-agent closer Grant Balfour has agreed to terms with the Balti-
more Orioles on a two-year deal, the Baltimore Sun reported on Tuesday citing
multiple industry sources.
Balfour will join the Baltimore Orioles.
(H)
Paul Rudd is ?Ant-Man?: 5 reasons he needs an ?Agents of SHIELD? appear-
ance.
Paul Rudd to play Ant-Man. (H)
Millwall defender Karleigh Osborne has joined Bristol City on a two-and-a-half
year deal after a successful loan spell.
Bristol City have signed Karleigh Os-
borne. (M)
Simon Hoggart, one of the Spectator?s best-loved columnists, died yesterday
after fighting pancreatic cancer for over three years.
Simon Hoggart passed away yesterday.
(M)
Table 1: Abstraction examples from compression (C), heuristic (H) and memory-based (M) patterns.
Method Extractions Abstractions
HEURISTIC 24,630 956
COMPRESSION 15,687 657
MEMORY-BASED 11,459 967
Table 2: Patterns extracted in each method, before
Noisy-OR inference.
and pattern cluster c
i
, the network provides
p(p
j
|c
i
) ?the probability that c
i
will gener-
ate p
j
? and p(c
i
|p
j
) ?the probability that,
given a pattern p
j
, c
i
was the hidden event
that generated it.
At generation time we proceed in the following
way:
1. Given the title or first sentence of a news ar-
ticle, run the same pattern extraction method
that was used in training and, if possible, ob-
tain a pattern p involving some entities.
2. Find the model clusters that contain this pat-
tern, C
p
= {c
i
such that p(c
i
|p) > 0}.
3. Return a ranked list of model patterns
output = {(p
j
, score(p
j
))}, scored as fol-
lows:
score(p
j
) =
?
c
i
?C
p
p(p
j
|c
i
)p(c
i
|p)
where p was the input pattern.
4. Replace the entity placeholders in the top-
scored patterns p
j
with the entities that were
actually mentioned in the input news article.
In all cases the parameters of the network were
predefined as 20,000 nodes in the hidden layer
(model clusters) and 40 Expectation Maximization
(EM) training iterations. Training was distributed
across 20 machines with 10 GB of memory each.
For testing we used 37,584 news crawled dur-
ing December 2013, which had not been used for
training the models. Table 3 shows one pattern
cluster example from each of the three trained
models. The table shows only the surface form
of the pattern for simplicity.
Pattern cluster (MEMORY-BASED)
organization
1
gets organization
0
nod for drug
organization
1
gets organization
0
nod for tablets
organization
0
approves organization
1
drug
organizations
0
approves organization
1
?s drug
organization
1
gets organization
0
nod for capsules
Pattern cluster (HEURISTIC)
organization
0
to buy organization
1
organization
0
to acquire organization
1
organization
0
buys organization
1
organization
0
acquires organization
1
organization
0
to acquire organizations
1
organization
0
buys organizations
1
organization
0
acquires organizations
1
organization
0
agrees to buy organization
1
organization
0
snaps up organization
1
organization
0
to purchase organizations
1
organization
0
is to acquire organization
1
organization
0
has agreed to buy organization
1
organization
0
announces acquisition of organizations
1
organization
0
may bid for organization
1
organization
1
sold to organization
0
organization
1
acquired by organization
0
Pattern cluster (COMPRESSION)
the sports team
1
have acquired person
0
from the sports team
2
the sports team
1
acquired person
0
from the sports team
2
the sports team
2
have traded person
0
to the sports team
1
sports team
1
acquired the rights to person
0
from sports team
2
sports team
2
acquired from sports team
1
in exchange for person
0
sports team
2
have acquired from the sports team
1
in exchange for person
0
Table 3: Examples of pattern clusters. In each
cluster c
i
, patterns are sorted by p(p
j
|c
i
).
898
5.2 Results
Table 2 shows the number of extracted patterns
from the test set, and the number of abstractive
event descriptions produced.
As expected, the number of extracted patterns
using the memory-based model is smaller than
with the two other models, which are based on
generic rules and are less restricted in what they
can generate. As mentioned, the memory-based
model can only extract previously-seen structures.
Compared to this model, with heuristics we can
obtain patterns for more than twice more news ar-
ticles. At the same time, looking at the number
of summary sentences generated they are com-
parable, meaning that a larger proportion of the
memory-based patterns actually appeared in the
pattern clusters and could be used to produce sum-
maries. This is also consistent with the fact that us-
ing heuristics the space of extracted patterns is ba-
sically unbounded and many new patterns can be
generated that were previously unseen ?and these
cannot generate abstractions. A positive outcome
is that restricting the syntactic structure of the ex-
tracted patterns to what has been observed in past
news does not negatively affect end-to-end cover-
age when generating the abstractive summaries.
Table 1 shows some of the abstractive sum-
maries generated with the different methods. For
manually evaluating their quality, a random sam-
ple of 100 original sentences was selected for each
method. The top ranked summary for each origi-
nal sentence was sent to human raters for evalua-
tion, and received three different ratings. None of
the raters had any involvement in the development
of the work or the writing of the paper, and a con-
straint was added that no rater could rate more than
50 abstractions. Raters were presented with the
original sentence and the compressed abstraction,
and were asked to rate it along two dimensions, in
both cases using a 5-point Likert scale:
? Readability: whether the abstracted com-
pression is grammatically correct.
? Informativeness: whether the abstracted
compression conveys the most important in-
formation from the original sentence.
Inter-judge agreement was measured using the
Intra-Class Correlation (ICC) (Shrout and Fleiss,
1979; Cicchetti, 1994). The ICC for readability
was 0.37 (95% confidence interval [0.32, 0.41]),
Method Readability Informativeness
HEURISTIC 3.95 3.07
COMPRESSION 3.98 2.35
MEMORY-BASED 4.20 3.70
Table 4: Results for the three methods when rating
the top-ranked abstraction.
and for informativeness it was 0.64 (95% confi-
dence interval [0,60, 0.67]), representing fair and
substantial reliability.
Table 4 shows the results when rating the top
ranked abstraction using either of the three dif-
ferent models for pattern extraction. The abstrac-
tions produced with the memory-based method are
more readable than those produced with the other
two methods (statistically significant with 95%
confidence).
Regarding informativeness, the differences be-
tween the methods are bigger, because the first two
methods have a proportionally larger number of
items with a high readability but a low informa-
tiveness score. For each method, we have man-
ually reviewed the 25 items where the difference
between readability and informativeness was the
largest, to understand in which cases grammatical,
yet irrelevant compressions are produced. The re-
sults are shown in Table 5. Be+adjective includes
examples where the pattern is of the form Entity is
Adjective, which the compression-based systems
extracts often represents an incomplete extraction.
Wrong inference contains the cases where patterns
that are related but not equivalent are clustered,
e.g. Person arrived in Country and Person arrived
in Country for talks. Info. missing represents cases
where very relevant information has been dropped
and the summary sentence is not complete. Pos-
sibility contains cases where the original sentence
described a possibility and the compression states
it as a fact, or vice versa. Disambiguation are en-
tity disambiguations errors, and Opposite contains
cases of patterns clustered together that are op-
posite along some dimension, e.g. Person quits
TV Program and Person to return to TV Program.
The method with the largest drop between the
readability and informativeness scores is COM-
PRESSION. As can be seen, many of these mis-
takes are due to relevant information being miss-
ing in the summary sentence. This is also the
largest source of errors for the HEURISTIC system.
For the MEMORY-BASED system, the drop in read-
899
Method Be+adjective Wrong inference Info. missing Possibility Disambiguation Opposite
HEURISTIC 0 7 14 3 1 0
COMPRESSION 3 10 10 0 0 2
MEMORY-BASED 0 17 4 2 0 2
Table 5: Sources of errors for the top 25 items with high readability and low informativeness.
Original sentence Pattern extracted (method) Abstraction
David Moyes is happy to use tough love on Adnan Januzaj
to ensure the Manchester United youngster fulfils his mas-
sive potential.
David Moyes is happy. (C)
Fortune will start to favour
David Moyes.
The Democratic People?s Republic of Korea will ?achieve
nothing by making threats or provocation,? the United
States said Friday.
The United States said Fri-
day. (C, H)
United States officials said
Friday.
EU targets Real and Barca over illegal state aid.
EU targets Real Madrid.
(H)
EU is going after Real
Madrid.
EU warns Israel over settlement construction
EU warns Israel. (M)
EU says Israel needs re-
forms.
Table 6: Examples of compression (C), heuristic (H) and memory-based (M) patterns that led to abstrac-
tions with high readability but a low informativeness score. Both incomplete summary sentences and
wrong inferences can be observed.
ability score is much smaller, so there were less of
these examples. And most of these examples be-
long to the class of wrong inferences (patterns that
are related but not equivalent, so we should not
abstract one of them from the other, but they were
clustered together in the model). Our conclusion
is that the examples with missing information are
not such a big problem with the MEMORY-BASED
system, as using the trie is an additional safeguard
that the generated titles are complete statements,
but the method is not preventing the wrong infer-
ence errors so this class of errors become the dom-
inant class by a large margin.
Some examples with high readability but low
informativeness are shown in Table 6.
6 Conclusions
Most Open-IE systems are based on linguistically-
motivated heuristics for learning patterns that ex-
press relations between entities or events. How-
ever, it is common for these patterns to be incom-
plete or ungrammatical, and therefore they are not
suitable for abstractive summary generation of the
relation or event mentioned in the text.
In this paper, we describe a memory-based ap-
proach in which we use a corpus of past news
to learn valid syntactic sentence structures. We
discuss the theoretical time complexity of look-
ing up extraction patterns in a large corpus of
syntactic structures stored as a trie and demon-
strate empirically that this method is effective in
practice. Finally, the evaluation shows that sum-
mary sentences produced by this method outper-
form heuristics and compression-based ones both
in terms of readability and informativeness. The
problem of generating incomplete summary sen-
tences, which was the main source of informative-
ness errors for the alternative methods, becomes a
minor problem with the memory-based approach.
Yet, there are some cases in which also the mem-
ory based approach extracts correct but misleading
utterances, e.g., a pattern like PER passed away
from the sentence PER passed the ball away. To
solve this class of problems, a possible research
direction would be the inclusion of more complex
linguistic features in the tree-trie, such as verb sub-
categorization frames.
As another direction for future work, more ef-
fort is needed in making sure that no incorrect in-
ferences are made with this model. These happen
when a more specific pattern is clustered together
with a less specific pattern, or when two non-
equivalent patterns often co-occur in news as two
events are somewhat correlated in real life, but it is
generally incorrect to infer one from the other. Im-
provements in the pattern-clustering model, out-
side the scope of this paper, will be required.
900
References
Enrique Alfonseca, Daniele Pighin, and Guillermo
Garrido. 2013. HEADY: News headline abstraction
through event pattern clustering. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, Sofia, Bulgaria, 4?9 August
2013, pages 1243?1253.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
OR, 19?24 June 2011.
Domenic V Cicchetti. 1994. Guidelines, criteria, and
rules of thumb for evaluating normed and standard-
ized assessment instruments in psychology. Psycho-
logical Assessment, 6(4):284.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:399?429.
Hal Daum?e III and Daniel Marcu. 2004. A tree-
position kernel for document compression. In Pro-
ceedings of the 2004 Document Understanding Con-
ference held at the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics,, Boston,
Mass., 6?7 May 2004.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006, pages 449?454.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, Edinburgh, UK, 27?29 July 2011, pages 1535?
1545.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, Seattle,
WA, USA, 18?21 October 2013, pages 1481?1491.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the ILP
for NLP Workshop, Boulder, CO, June 4 2009, pages
10?18.
Gregory Grefenstette. 1998. Producing intelligent
telegraphic text reduction to provide an audio scan-
ning service for the blind. In Working Notes of the
Workshop on Intelligent Text Summarization, Palo
Alto, Cal., 23 March 1998, pages 111?117.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, Singapore, 6-7 August 2009, pages 1152?1161.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins, Amsterdam, Philadelphia.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Trento, Italy, 3?7 April 2006, pages 297?304.
Yashar Mehdad, Giuseppe Carenini, and Frank W.
Tompa. 2013. Abstractive meeting summariza-
tion with entailment and fusion. In Proceedings
of the 14th European Workshop on Natural Lan-
guage Generation, Sofia, Bulgaria, 8?9 August,
2013, pages 136?146.
David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the AAAI
2008 Workshop on Wikipedia and Artificial Intelli-
gence, Chicago, IL, 13-14 July, 2008.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Michael Schmitz, Robert Bart, Stephen Soderland,
Oren Etzioni, et al 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Conference on Empirical Methods in Natu-
ral Language Processing, Jeju Island, Korea, 12?14
July 2012, pages 523?534.
Patrick E Shrout and Joseph L Fleiss. 1979. Intraclass
correlations: uses in assessing rater reliability. Psy-
chological bulletin, 86(2):420.
Karen Sp?arck-Jones. 2007. Automatic summaris-
ing: A review and discussion of the state of the art.
Technical Report UCAM-CL-TR-679, University of
Cambridge, Computer Laboratory, Cambridge, U.K.
Ivan Titov and Alexandre Klementiev. 2011. A
Bayesian model for unsupervised semantic parsing.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, Portland,
OR, 19?24 June 2011, pages 1445?1455.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of the
51st Annual Meeting of the Association for Com-
putational Linguistics, Sofia, Bulgaria, 4?9 August
2013, pages 1384?1394.
Congle Zhang and Daniel S. Weld. 2013. Harvest-
ing parallel news streams to generate paraphrases of
event relations. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, WA, USA, 18?21 October 2013,
pages 1776?1786.
901
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 352?357,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Automation and Evaluation of the Keyword Method
for Second Language Learning
G
?
ozde
?
Ozbal
Trento RISE
Trento, Italy
gozbalde@gmail.com
Daniele Pighin
Google
Z?urich, Switzerland
biondo@google.com
Carlo Strapparava
FBK-irst
Trento, Italy
strappa@fbk.eu
Abstract
In this paper, we combine existing
NLP techniques with minimal supervi-
sion to build memory tips according to
the keyword method, a well established
mnemonic device for second language
learning. We present what we believe to be
the first extrinsic evaluation of a creative
sentence generator on a vocabulary learn-
ing task. The results demonstrate that NLP
techniques can effectively support the de-
velopment of resources for second lan-
guage learning.
1 Introduction
The keyword method is a mnemonic device (Co-
hen, 1987; Thompson, 1987) that is especially
suitable for vocabulary acquisition in second lan-
guage learning (Mizumoto and Kansai, 2009;
Hummel, 2010; Shen, 2010; Tavakoli and Gerami,
2013). In this method, a target word in a foreign
language L2 can be learned by a native speaker of
another language L1 in two main steps: 1) one or
more L1 words, possibly referring to a concrete
entity, are chosen based on orthographic or pho-
netic similarity with the target word; 2) an L1 sen-
tence is constructed in which an association be-
tween the translation of the target word and the
keyword(s) is established, so that the learner, when
seeing or hearing the word, immediately recalls
the keyword(s). To illustrate, for teaching the Ital-
ian word cuore which means heart in English, the
learner might be asked to imagine ?a lonely heart
with a hard core?.
The keyword method has already been proven
to be a valuable teaching device. However, the
preparation of the memorization tips for each new
word is an activity that requires considerable time,
linguistic competence and creativity. To the best
of our knowledge, there is only one study which
attempts to automate the mechanism of the key-
word method. In (
?
Ozbal and Strapparava, 2011),
we proposed to automate the keyword method by
retrieving sentences from the Web. However, we
did not provide any evaluation to demonstrate the
effectiveness of our approach in a real life sce-
nario. In addition, we observed that retrieval poses
severe limitations in terms of recall and sentence
quality, and it might incur copyright violations.
In this paper, we overcome these limitations by
introducing a semi-automatic system implement-
ing the keyword method that builds upon the key-
word selection mechanism of
?
Ozbal and Strappar-
ava (2011) and combines it with a state-of-the-art
creative sentence generation framework (
?
Ozbal et
al., 2013). We set up an experiment to simulate
the situation in which a teacher needs to prepare
material for a vocabulary teaching resource. Ac-
cording to our scenario, the teacher relies on au-
tomatic techniques to generate relatively few, high
quality mnemonics in English to teach Italian vo-
cabulary. She only applies a very light supervi-
sion in the last step of the process, in which the
most suitable among the generated sentences are
selected before being presented to the learners. In
this stage, the teacher may want to consider factors
which are not yet in reach of automatic linguistic
processors, such as the evocativeness or the mem-
orability of a sentence. We show that the automat-
ically generated sentences help learners to estab-
lish memorable connections which augment their
ability to assimilate new vocabulary. To the best of
our knowledge, this work is the first documented
extrinsic evaluation of a creative sentence genera-
tor on a real-world application.
2 Related work
The effectiveness of the keyword method (KM)
is a well-established fact (Sar?c?oban and Bas??bek,
2012). Sommer and Gruneberg (2002) found that
using KM to teach French made learning easier
and faster than conventional methods. Sagarra
and Alba (2006) compared the effectiveness of
352
three learning methods including the semantic
mapping, rote memorization (i.e., memorization
by pure repetition, with no mnemonic aid) and
keyword on beginner learners of a second lan-
guage. Their results show that using KM leads
to better learning of second language vocabulary
for beginners. Similar results have been reported
by Sar?c?oban and Bas??bek (2012) and Tavakoli
and Gerami (2013). Besides all the experimental
results demonstrating the effectiveness of KM, it
is worthwhile to mention about the computational
efforts to automate the mechanism. In (
?
Ozbal and
Strapparava, 2011) we proposed an automatic vo-
cabulary teaching system which combines NLP
and IR techniques to automatically generate mem-
ory tips for vocabulary acquisition. The system
exploits orthographic and phonetic similarity met-
rics to find the best L2 keywords for each target L1
word. Sentences containing the keywords and the
translation of the target word are retrieved from
the Web, but we did not carry out an evaluation
of the quality or the coverage of the retrieved sen-
tences. In
?
Ozbal et al (2013) we proposed an ex-
tensible framework for the generation of creative
sentences in which users are able to force sev-
eral words to appear in the sentences. While we
had discussed the potentiality of creative sentence
generation as a useful teaching device, we had not
validated our claim experimentally yet. As a previ-
ous attempt at using NLP for education, Manurung
et al (2008) employ a riddle generator to create
a language playground for children with complex
communication needs.
3 Memory tip generation
Preparing memory tips based on KM includes two
main ingredients: one or more keywords which are
orthographically or phonetically similar to the L2
word to be learned; and a sentence in which the
keywords and the translation of the target L2 word
are combined in a meaningful way. In this section,
we detail the process that we employed to generate
such memory tips semi-automatically.
3.1 Target word selection and keyword
generation
We started by compiling a collection of Ital-
ian nouns consisting of three syllables from var-
ious resources for vocabulary teaching includ-
ing http://didattica.org/italiano.
htm and http://ielanguages.com, and
produced a list of 185 target L2 words. To gen-
erate the L1 keywords for each target word, we
adopted a similar strategy to
?
Ozbal and Strappa-
rava (2011). For each L2 target word t, the key-
word selection module generates a list of possi-
ble keyword pairs, K. A keyword pair k ? K
can either consist of two non-empty strings, i.e.,
k = [w
0
, w
1
], or of one non-empty and one empty
string, i.e., w
1
= . Each keyword pair has the
property that the concatenation of its elements is
either orthographically or phonetically similar to
the target word t. Orthographic and phonetic sim-
ilarity are evaluated by means of the Levenshtein
distance (Levenshtein, 1966). For orthographic
similarity, the distance is calculated over the char-
acters in the words, while for phonetic similarity
it is calculated over the phonetic representations
of t and w
0
+ w
1
. We use the CMU pronuncia-
tion dictionary
1
to retrieve the phonetic represen-
tation of English words. For Italian words, instead,
their phonetic representation is obtained from an
unpublished phonetic lexicon developed at FBK-
irst.
3.2 Keyword filtering and ranking
Unlike in (
?
Ozbal and Strapparava, 2011), where
we did not enforce any constraints for selecting
the keywords, in this case we applied a more so-
phisticated filtering and ranking strategy. We re-
quire at least one keyword in each pair to be a
content word; then, we require that at least one
keyword has length ? 3; finally, we discard pairs
containing at least one proper noun. We allowed
the keyword generation module to consider all the
entries in the CMU dictionary, and rank the key-
word pairs based on the following criteria in de-
creasing order of precedence: 1) Keywords with
a smaller orthographic/phonetic distance are pre-
ferred; 2) Keywords consisting of a single word
are preferred over two words (e.g., for the target
word lavagna, which means blackboard, lasagna
takes precedence over love and onion); 3) Key-
words that do not contain stop words are preferred
(e.g., for the target word pettine, which means
comb, the keyword pair pet and inn is ranked
higher than pet and in, since in is a stop word); 4)
Keyword pairs obtained with orthographic similar-
ity are preferred over those obtained with phonetic
similarity, as learners might be unfamiliar with the
phonetic rules of the target language. For example,
for the target word forbice, which means scissors,
1
http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
353
Group Target Sentence
A1 campagna a company runs the country
A1 isola an island of remote isolated communities
A1 fabbrica a fabric worker in a factory
A1 bagnino lifeguards carry no bag
A1 inverno the inferno started, winter left
A1 cielo the sky has no ceiling
A1 marrone blood and marrow in a brown water
A1 cuore the lonely heart has hard core
A1 coperta a piece of copper in the corner of a blanket
A1 locanda an inn oak door with lock and key
A2 piazza a square building serves a free pizza
A2 calzino big bloke with sock in the casino
A2 scatola a cardboard box sat in a scuttle of a house
A2 ragazzo boys also have rag dolls
A2 angolo a corner kick came at an angle
A2 cestino a teen movie uses basket to play the chess
A2 carbone the coal is the form of carbon
A2 cassetto a blank cassette tape is in a drawer
A2 farfalla the butterflies are far in the fall
A2 tovaglia a damp cloth towel
B1 duomo the old cathedral has a dome
B1 aceto a vinegar sauce contains the acid
B1 nuvola the sophisticated novel depicts the cloud
B1 chiesa the Catholic church has Swiss cheese
B1 bacino the explosion in the back broke the pelvis
B1 maiale a pork meat comes in the mail
B1 minestra Chinese ministries have soup
B1 estate this estate is for summer
B1 bozzolo a buzz comes wrapped in the cocoon
B1 arnese harness a technology to develop a tool
B2 asino an Asian elephant is riding a donkey
B2 miele do not make honey to walk a mile
B2 polmone crowded pullmans stop the lungs
B2 fagiolo a topical facial bean cream
B2 fiore a fire in a flower market
B2 compressa the clay tablet is in the compressed form
B2 cavallo horse running fast in cavalry
B2 fiume the muddy river has smoke and fumes
B2 pittore a famous painter has precious pictures
B2 manico manic people have broken necks
Table 1: Sentences used in the vocabulary acqui-
sition experiment.
the keyword pair for and bid is preferred to for and
beach.
We selected up to three of the highest ranked
keyword pairs for each target word, obtaining 407
keyword combinations for the initial 185 Italian
words, which we used as the input for the sentence
generator.
3.3 Sentence generation
In this step, our goal was to generate, for each Ital-
ian word, sentences containing its L1 translation
and the set of orthographically (or phonetically)
similar keywords that we previously selected. For
each keyword combination, starting from the top-
ranked ones, we generated up to 10 sentences by
allowing any known part-of-speech for the key-
words. The sentences were produced by the state
of the art sentence generator of
?
Ozbal et al (2013).
The system relies on two corpora of automatic
parses as a repository of sentence templates and
lexical statistics. As for the former, we combined
two resources: a corpus of 16,000 proverbs (Mi-
halcea and Strapparava, 2006) and a collection of
5,000 image captions
2
collected by Rashtchian et
al. (2010). We chose these two collections since
they offer a combination of catchy or simple sen-
tences that we expect to be especially suitable
for second language learning. As for the sec-
ond corpus, we used LDC?s English GigaWord 5th
Edition
3
. Of the 12 feature functions described
in (
?
Ozbal et al, 2013), we only implemented the
following scorers: Variety (to prevent duplicate
words from appearing in the sentences); Seman-
tic Cohesion (to enforce the generation of sentence
as lexically related to the target words as possi-
ble); Alliteration, Rhyme and Plosive (to intro-
duce hooks to echoic memory in the output); De-
pendency Operator andN -gram (to enforce output
grammaticality).
We observed that the sentence generation mod-
ule was not able to generate a sentence for 24%
of the input configurations. For comparison, when
we attempted to retrieve sentences from the Web
as suggested in
?
Ozbal and Strapparava (2011), we
could collect an output for less than 10% of the in-
put configurations. Besides, many of the retrieved
sentences were exceedingly long and complex to
be used in a second language learning experiment.
3.4 Sentence selection
For each L1 keyword pair obtained for each L2
target word, we allowed the system to output up to
10 sentences. We manually assessed the quality of
the generated sentences in terms of meaningful-
ness, evocativeness and grammaticality to select
the most appropriate sentences to be used for the
task. In addition, for keyword pairs not containing
the empty string, we prioritized the sentences in
which the keywords were closer to each other. For
example, let us assume that we have the keywords
call and in for the target word collina. Among
the sentences ?The girl received a call in the bath-
room? and ?Call the blond girl in case you need?,
the first one is preferred, since the keywords are
closer to each other. Furthermore, we gave pri-
ority to the sentences that included the keywords
2
http://vision.cs.uiuc.edu/
pascal-sentences/
3
http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2011T07
354
in the right order. To illustrate, for the same key-
words and the target words, we would prefer the
sentence ?I called him in the morning yesterday?
over ?You talk a lot in a call?.
Accordingly, for each target word in random or-
der, we sequentially scanned the outputs generated
for each keyword pair. As soon as a sentence of
adequate quality was found, we added it to our
evaluation data and moved on to the next keyword.
We continued this process until we selected a sen-
tence for 40 distinct target words, which we set
as the target size of the experiment. We had to
inspect the outputs generated for 48 target words
before we were able to select 40 good examples,
meaning that for 17% of the target words the sen-
tence generator could not produce a sentence of
acceptable quality.
4 Experiment setup
For our experiment, we drew inspiration from
Sagarra and Alba (2006). We compared the re-
tention error rate of learners who tried to memo-
rize new words with or without the aid of the auto-
matically generated sentences. Through academic
channels, we recruited 20 native English speakers
with no prior knowledge of Italian.
4
After obtaining the sentences as explained in
Section 3, we shuffled and then divided the whole
set including 40 target words together with their
translation, the generated keywords and sentences
into 2 batches (A, B) and further divided each
batch into 2 groups consisting of 10 elements (A1,
A2, B1 and B2). The set of sentences assigned
to each group is listed in Table 1: Column ?Tar-
get? reports the Italian target word being taught;
Column ?Sentence? shows the automatically gen-
erated sentence, where the translation of the tar-
get word is shown in bold and the keyword(s) in
italic. For the experiments, we randomly assigned
each subject to one of the batches (A or B). Then,
each subject was asked to memorize all the word
pairs in a batch, but they would see the memory
tips only for one of the two groups, which was
again randomly assigned. This approach resulted
in 4 different memorization exercises, namely 1)
A1 with tips and A2 without, 2) A2 with tips and
A1 without, 3) B1 with tips and B2 without, 4) B2
with tips and B1 without.
4
We preferred to select the experiment subjects in person
as opposed to crowdsourcing the evaluation to be able to ver-
ify the proficiency of the subjects in the two languages and to
ensure the reliability of the outcome of the evaluation.
Error rate (%) Reduction
Group Rote KW ?
e
%
e
A1 4.08 3.39 0.69 16.95
A2 12.07 10.42 1.65 13.69
B1 12.77 10.00 2.77 21.67
B2 22.50 12.50 10.00 44.44
Macro-average 12.85 9.08 3.78 29.39
Micro-average 11.27 8.25 3.02 26.76
Table 2: Per-group and overall retention error rate
when using rote or keyword-aided (KW) memo-
rization.
When memorizing the translations without the
aid of memory tips, the subjects were instructed
to focus only on the Italian word and its English
translation and to repeat them over and over in
their mind. Conversely, when relying on the au-
tomatic memory tips the subjects were shown the
word, its translation and the generated sentence in-
cluding the keywords. In this case, the subjects
were instructed to read the sentence over and over
trying to visualize it.
After going through each set of slides, we dis-
tracted the subjects with a short video in order to
reset their short term memory. After that, their re-
tention was tested. For each Italian word in the ex-
ercise, they were asked to select the English trans-
lation among 5 alternatives, including the correct
translation and 4 other words randomly selected
from the same group. In this way, the subjects
would always have to choose among the words
that they encountered during the exercise.
5
We
also added an extra option ?I already knew this
word? that the subjects were instructed to select
in case they already knew the Italian word prior to
taking part in the experiment.
5 Experiment results
Table 2 summarizes the outcome of the experi-
ment. The contribution of the automatically gen-
erated sentences to the learning task is assessed
in terms of error rate-reduction, which we mea-
sure both within each group (rows 1-4) and on the
whole evaluation set (rows 5-6). Due to the pres-
ence of the ?I already knew this word? option in
the learning-assessment questionnaire, the number
of the actual answers provided by each subject can
be slightly different, hence the difference between
macro- and micro-average.
5
Otherwise, they could easily filter out the wrong answers
just because they were not exposed to them recently.
355
The error rate for each memorization technique
t (where t = R for ?Rote memorization? and
t = K for ?keyword-aided memorization?) is cal-
culated as: e
t
=
i
t
c
t
+i
t
, where c
t
and i
t
are the
number of correct and incorrect answers provided
by the subjects, respectively. The absolute error
rate reduction ?e is calculated as the absolute dif-
ference in error rate between rote and keyword-
aided memorization, i.e.: ?
e
= e
R
? e
K
. Finally,
the relative error rate reduction %
e
is calculated as
the the ratio between the absolute error rate reduc-
tion ?e and the error rate of rote memorization e
R
,
i.e.,: %
e
=
?
e
e
R
=
e
R
?e
K
e
R
.
The overall results (rows 5 and 6 in Table 2)
show that vocabulary learning noticeably im-
proves when supported by the generated sen-
tences, with error rates dropping by almost 30%
in terms of macro-average (almost 27% for micro-
average). The breakdown of the error rate across
the 4 groups shows a clear pattern. The results
clearly indicate that one group (A1) by chance
contained easier words to memorize as shown by
the low error rate (between 3% and 4%) obtained
with both methods. Similarly, groups A2 and B1
are of average difficulty, whereas group B2 ap-
pears to be the most difficult, with an error rate
higher than 22% when using only rote memoriza-
tion. Interestingly, there is a strong correlation
(Pearson?s r = 0.85) between the difficulty of
the words in each group (measured as the error
rate on rote memorization) and the positive contri-
bution of the generated sentences to the learning
process. In fact, we can see how the relative er-
ror rate reduction %
e
increases from?17% (group
A1) to almost 45% (group B2). Based on the re-
sults obtained by Sagarra and Alba (2006), who
showed that the keyword method results in bet-
ter long-term word retention than rote memoriza-
tion, we would expect the error rate reduction to be
even higher in a delayed post-test. All in all, these
findings clearly support the claim that a state-of-
the-art sentence generator can be successfully em-
ployed to support keyword-based second language
learning. After completing their exercise, the sub-
jects were asked to provide feedback about their
experience as learners. We set up a 4-items Lik-
ert scale (Likert, 1932) where each item consisted
of a statement and a 5-point scale of values rang-
ing from (1) [I strongly disagree] to (5) [I strongly
agree]. The distribution of the answers to the ques-
tions is shown in Table 3. 60% of the subjects ac-
knowledged that the memory tips helped them in
Rating (%)
Question 1 2 3 4 5
Sentences helped 5 20 15 35 25
Sentences are grammatical - 25 30 35 10
Sentences are catchy - 25 10 50 15
Sentences are witty - 25 25 50 -
Table 3: Evaluation of the generated sentences on
a 5-point Likert scale.
the memorization process; 45% found that the sen-
tences were overall correct; 65% confirmed that
the sentences were catchy and easy to remember;
and 50% found the sentences to be overall witty
although the sentence generator does not include a
mechanism to generate humor. Finally, it is worth
mentioning that none of the subjects noticed that
the sentences were machine generated, which we
regard as a very positive assessment of the qual-
ity of the sentence generation framework. From
their comments, it emerges that the subjects ac-
tually believed that they were just comparing two
memorization techniques.
6 Conclusion and Future Work
In this paper, we have presented a semi-automatic
system for the automation of the keyword method
and used it to teach 40 Italian words to 20 En-
glish native speakers. We let the system select
appropriate keywords and generate sentences au-
tomatically. For each Italian word, we selected the
most suitable among the 10 highest ranked sug-
gestions and used it for the evaluation. The sig-
nificant reduction in retention error rate (between
17% and 45% on different word groups) for the
words learned with the aid of the automatically
generated sentences shows that they are a viable
low-effort alternative to human-constructed exam-
ples for vocabulary teaching.
As future work, it would be interesting to in-
volve learners in an interactive evaluation to un-
derstand the extent to which learners can bene-
fit from ad-hoc personalization. Furthermore, it
should be possible to use frameworks similar to
the one that we presented to automate other teach-
ing devices based on sentences conforming to spe-
cific requirements (Dehn, 2011), such as verbal
chaining and acrostic.
Acknowledgements
This work was partially supported by the PerTe
project (Trento RISE).
356
References
Andrew D. Cohen. 1987. The use of verbal and
imagery mnemonics in second-language vocabulary
learning. Studies in Second Language Acquisition,
9:43?61, 2.
M.J. Dehn. 2011. Working Memory and Academic
Learning: Assessment and Intervention. Wiley.
K. M. Hummel. 2010. Translation and short-term L2
vocabulary retention: Hindrance or help? Language
Teaching Research, 14(1):61?74.
V. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10:707?710.
R. Likert. 1932. A technique for the measurement of
attitudes. Archives of Psychology, 22(140):1?55.
Ruli Manurung, Graeme Ritchie, Helen Pain, Annalu
Waller, Dave O?Mara, and Rolf Black. 2008. The
Construction of a Pun Generator for Language Skill
Development. Appl. Artif. Intell., 22(9):841?869,
October.
R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Journal of Computational In-
telligence, 22(2):126?142, May.
A. Mizumoto and O. T. Kansai. 2009. Examining
the effectiveness of explicit instruction of vocabu-
lary learning strategies with Japanese EFL university
students. Language Teaching Research 13, 4.
G?ozde
?
Ozbal and Carlo Strapparava. 2011. MEANS:
Moving Effective Assonances for Novice Students.
In Proceedings of the 16th International Confer-
ence on Intelligent User Interfaces (IUI 2011), pages
449?450, New York, NY, USA. ACM.
G?ozde
?
Ozbal, Daniele Pighin, and Carlo Strapparava.
2013. BRAINSUP: Brainstorming Support for Cre-
ative Sentence Generation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013), pages 1446?1455,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using amazon?s mechanical turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
N. Sagarra and M. Alba. 2006. The key is in the
keyword: L2 vocabulary learning methods with be-
ginning learners of spanish. The Modern Language
Journal, 90(2):228?243.
A. Sar?c?oban and N. Bas??bek. 2012. Mnemonics tech-
nique versus context method in teaching vocabulary
at upper-intermediate level. Journal of Education
and Science, 37(164):251?266.
Helen H. Shen. 2010. Imagery and verbal coding ap-
proaches in Chinese vocabulary instruction. Lan-
guage Teaching Research, 14(4):485?499.
Steffen Sommer and Michael Gruneberg. 2002. The
use of linkword language computer courses in a
classroom situation: a case study at rugby school.
?Language Learning Journal, 26(1):48?53.
M. Tavakoli and E. Gerami. 2013. The effect of key-
word and pictorial methods on EFL learners? vocab-
ulary learning and retention. PORTA LINGUARUM,
19:299?316.
G. Thompson. 1987. Using bilingual dictionar-
ies. ELT Journal, 41(4):282?286. cited By (since
1996)6.
357
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 223?233,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
On Reverse Feature Engineering of Syntactic Tree Kernels
Daniele Pighin
FBK-irst, DISI, University of Trento
Via di Sommarive, 14
I-38123 Povo (TN) Italy
daniele.pighin@gmail.com
Alessandro Moschitti
DISI, University of Trento
Via di Sommarive, 14
I-38123 Povo (TN) Italy
moschitti@disi.unitn.it
Abstract
In this paper, we provide a theoretical
framework for feature selection in tree ker-
nel spaces based on gradient-vector com-
ponents of kernel-based machines. We
show that a huge number of features can
be discarded without a significant decrease
in accuracy. Our selection algorithm is as
accurate as and much more efficient than
those proposed in previous work. Com-
parative experiments on three interesting
and very diverse classification tasks, i.e.
Question Classification, Relation Extrac-
tion and Semantic Role Labeling, support
our theoretical findings and demonstrate
the algorithm performance.
1 Introduction
Kernel functions are very effective at modeling
diverse linguistic phenomena by implicitly rep-
resenting data in high dimensional spaces, e.g.
(Cumby and Roth, 2003; Culotta and Sorensen,
2004; Kudo et al, 2005; Moschitti et al, 2008).
However, the implicit nature of the kernel space
causes two major drawbacks: (1) high computa-
tional costs for learning and classification, and (2)
the impossibility to identify the most important
features. A solution to both problems is the ap-
plication of feature selection techniques.
In particular, the problem of feature selection
in Tree Kernel (TK) spaces has already been ad-
dressed by previous work in NLP, e.g. (Kudo
and Matsumoto, 2003; Suzuki and Isozaki, 2005).
However, these approaches lack a theoretical char-
acterization of the problem that could support and
justify the design of more effective algorithms.
In (Pighin and Moschitti, 2009a) and (Pighin
and Moschitti, 2009b) (P&M), we presented a
heuristic framework for feature selection in kernel
spaces that selects features based on the compo-
nents of the weight vector, ~w, optimized by Sup-
port Vector Machines (SVMs). This method ap-
pears to be very effective, as the model accuracy
does not significantly decrease even when a large
number of features are filtered out. Unfortunately,
we could not provide theoretical or intuitive moti-
vations to justify our proposed apporach.
In this paper, we present and empirically val-
idate a theory which aims at filling the above-
mentioned gaps. In particular we provide: (i) a
proof of the equation for the exact computation of
feature weights induced by TK functions (Collins
and Duffy, 2002); (ii) a theoretical characteriza-
tion of feature selection based on ?~w?. We show
that if feature selection does not sensibly reduces
?~w?, the margin associated with ~w does not sen-
sibly decrease as well. Consequently, the theoret-
ical upperbound to the probability error does not
sensibly increases; (iii) a proof that the convolu-
tive nature of TK allows for filtering out an expo-
nential number of features with a small ?~w? de-
crease. The combination of (ii) with (iii) suggests
that an extremely aggressive feature selection can
be applied. We describe a greedy algorithm that
exploits these results. Compared to the one pro-
posed in P&M, the new version of the algorithm
has only one parameter (instead of 3), it is more
efficient and can be more easily connected with the
amount of gradient norm that is lost after feature
selection.
In the remainder: Section 2 briefly reviews
SVMs and TK functions; Section 3 describes the
problem of selecting and projecting features from
very high onto lower dimensional spaces, and pro-
vides the theoretical foundation to our approach;
Section 4 presents a selection of related work; Sec-
tion 5 describes our approach to tree fragment se-
lection; Section 6 details the outcome of our ex-
periments; finally, in Section 7 we draw our con-
clusions.
223
2 Fragment Weights in TK Spaces
The critical step for feature selection in tree ker-
nel spaces is the computation of the weights of
features (tree fragments) in the kernel machines?
gradient. The basic parameters are the fragment
frequencies which are combined with a decay fac-
tor used to downscale the weight of large sub-
trees (Collins and Duffy, 2002). In this section, af-
ter introducing basic kernel concepts, we describe
a theorem that establishes the correct weight1 of
features in the STK space.
2.1 Kernel Based-Machines
Typically, a kernel machine is a linear classifier
whose decision function can be expressed as:
c(~x) = ~w ? ~x+ b =
?`
i=1
?iyi ~xi ? ~x+ b (1)
where ~x ? <N is a classifying example and
~w ? <N and b ? < are the separating hyper-
plane?s gradient and its bias, respectively. The
gradient is a linear combination of ` training
points ~xi ? <N multiplied by their labels
yi ? {?1,+1} and their weights ?i ? <+.
Different optimizers use different strategies to
learn the gradient. For instance, an SVM learns
to maximize the distance between positive and
negative examples, i.e. the margin ?. Applying
the so-called kernel trick, it is possible to replace
the scalar product with a kernel function defined
over pairs of objects, which can more efficiently
compute it:
c(o) =
?`
i=1
?iyik(oi, o) + b,
where k(oi, o) = ?(oi) ? ?(o), with the advantage
that we do not need to provide an explicit mapping
? : O ? <N of our example objects O in a vec-
tor space. In the next section, we show a kernel
directly working on syntactic trees.
2.2 Syntactic Tree Kernel (STK)
Tree Kernel (TK) functions are convolution ker-
nels (Haussler, 1999) defined over pairs of trees.
Different TKs are characterized by alternative
fragment definitions, e.g. (Collins and Duffy,
2002; Kashima and Koyanagi, 2002; Moschitti,
2006). We will focus on the syntactic tree kernel
described in (Collins and Duffy, 2002), which re-
lies on a fragment definition that does not allow to
1In P&M we provided an approximation of the real
weight.
break production rules (i.e. if any child of a node is
included in a fragment, then also all the other chil-
dren have to). As such, it is especially indicated
for tasks involving constituency parsed texts.
Tree kernels compute the number of common
substructures between two trees T1 and T2
without explicitly considering the whole feature
(fragment) space. Let F = {f1, f2, . . . , f|F|}
be the set of tree fragments, i.e. the explicit
representation for the components of the fragment
space, and ?i(n) be an indicator function2, equal
to 1 if the target fi is rooted at node n and equal
to 0 otherwise. A tree kernel function over T1 and
T2 is defined as
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), (2)
whereNT1 andNT2 are the sets of nodes in T1 and
T2, respectively and
?(n1, n2) =
|F|?
i=1
?i(n1)?i(n2). (3)
The ? function counts the number of common
subtrees rooted in n1 and n2 and weighs them
according to their size. It can be evaluated as
follows (Collins and Duffy, 2002):
1. if the productions at n1 and n2 are different,
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminal symbols) then ?(n1, n2) = ?;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) = ?
l(n1)?
j=1
(1 + ?(cjn1 , c
j
n2)), (4)
where l(n1) is the number of children of n1, cjn
is the j-th child of node n and ? is a decay factor
penalizing larger structures.
2.3 Tree Fragment Weights
Eq. 3 shows that ? counts the shared fragments
rooted in n1 and n2 in the form of scalar product,
as evaluated by Eq. 2. However, when ? is used in
? as in Eq. 4, it changes the weight of the product
?i(n1)?i(n2). As ? multiplies ? in each recur-
sion step, we may be induced to assume3 that the
2We will consider it as a weighting function.
3In (Collins and Duffy, 2002), there is a short note about
the correct value weight of lambda for each product compo-
nents (i.e. pairs of fragments). This is in line with the formu-
lation we provide.
224
weight of a fragment is ?d, where d is the depth of
the fragment. On the contrary, we show the actual
weight by providing the following:
Theorem 1. Let T and f be a tree and one of
its fragments, respectively, induced by STK. The
weight of f accounted by STK is ?
s(f)
2 , where
lf (n) is the number of children of n in f and
s(f) = |{n ? T : lf (n) > 0}| is the number
of nodes that have active productions in the frag-
ment, i.e. the size of the fragment.
In other words, the exponent of ? is the number
of fragment nodes that have at least one child (i.e.
active productions), divided by 2.
Proof. The thesis can be proven by induction on
the depth d of f . The base case is f of depth
1. Fragments of depth 1 are matched by step 2
of ?(n1, n2) computation, which assigns a value
? = ?i(n1)?i(n2) independently of the number of
children (where fi = f ). It follows that the weight
of f is ?i(n1) = ?i(n2) = ?1/2.
Suppose that the thesis is valid for depth d and
let us consider a fragment f of depth d+ 1, rooted
in r. Without loss of generality, we can assume
that f is in the set of the fragments rooted in n1
and n2, as evaluated by Eq. 4. It follows that
the production rules associated with n1 and n2 are
identical to the production rule in r. Let us con-
sider M = {i ? {1, .., l(n1)} : l(cir) > 0},
i.e. the set of child indices of r which have at
least a child. Thus, for j ? M , cir has a pro-
duction shared by cjn1 and c
j
n2 . Conversely, for
j /? M , there is no match and ?(cjn1 , c
j
n2) = 0.
Therefore, the product in Eq. 4 can be rewrit-
ten as ?
?
j?M ?(c
j
n1 , c
j
n2), where the term 1 in
(1 + ?(cjn1 , c
j
n2)) is not considered since it ac-
counts for those cases where there are no common
productions in the children, i.e. cjn1 6= c
j
n2?j ?
M .
We can now substitute ?(cjn1 , c
j
n2) with the
weight of the subtree tj of f rooted in cjr (and ex-
tended until its leaves), which is ?s(tj) by induc-
tive hypothesis (since tj has depth lower than d).
Thus, the weight of f is s(f) = ?
?
j?M ?
s(tj) =
?1+
?
j?M
s(tj), where
?
j?M s(tj) is the num-
ber of nodes in f ?s subtrees rooted in r?s chil-
dren and having at least one child; by adding
1, i.e. the root of f , we obtain s(f). Finally,
?s(f) = ?i(n1)?i(n2), which satisfies our thesis:
?i(n1) = ?i(n2) = ?
s(f)
2 .
2.4 Weights in Feature Vectors
In the light of this result, we can use the definition
of a TK function to project a tree t onto a linear
space by recognizing that t can be represented as a
vector ~xi = [x
(1)
i , . . . , x
(N)
i ] whose attributes are
the counts of the occurrences for each fragment,
weighed with respect to the decay factor ?.
For a normalized STK kernel, the value of the
j-th attribute of the example ~xi is therefore:
x(j)i =
ti,j?
s(fj)
2
?~xi?
=
ti,j?
s(fj)
2
??N
k=1 t
2
i,k?
s(fk)
(5)
where: ti,j is the number of occurrences of the
fragment fj , associated with the j-th dimension
of the feature space, in the tree ti. It follows that
the components of ~w (see Eq. 1) can be rewritten
as:
w(j) =
?`
i=1
?iyix
(j)
i =
?`
i=1
?iyiti,j?
s(fj)
2
??N
k=1 t
2
i,k?
s(fk)
(6)
3 Projecting Exponentially Large Spaces
In order to provide a theoretical background to our
feature selection technique and to develop effec-
tive algorithms, we want to relate our approach to
statistical learning and, in particular, support vec-
tor classification theory. Since we select features
with respect to their weight w(j), we can use the
following theorem that establishes a general bound
for margin-based classifiers.
Theorem 2. (Bartlett and Shawe-Taylor, 1998)
Let C = {~x ? ~w ? ~x : ?~w? ? 1, ?~x? ? R}
be the class of real-valued functions defined in a
ball of radius R in <N . Then there is a con-
stant k such that ?c ? C having a margin ?, i.e.
|~w ? ~x| ? ?,?~x ? X (training set), the error of c
is bounded by b/` +
?
k
`
(
R2
?2 log
2`+ log 1?
)
with
a probability 1 ? ?, where ` = |X | and b is the
number of examples with margin less than ?.
In other words, if X is separated with a margin
? by a linear classifier, then the error has a bound
depending on ?. Another conclusion is that a fea-
ture selection algorithm that wants to preserve the
accuracy of the original space should not affect the
margin.
Since we would like to exploit the availability
of the initial gradient ~w derived by the applica-
tion of SVMs, it makes sense to try to quantify the
percentage of ? reduction after feature selection,
which we indicate by ?. We found out that ? is
225
linked to the reduction of ||~w||, as illustrated by
the next lemma.
Lemma 1. Let X be a set of points in a vector
space and ~w be the gradient vector which sepa-
rates them with a margin ?. If the selection de-
creases ||~w|| of a ? rate, then the resulting hyper-
plane separates X by a margin larger than ?in =
? ? ?R||~w||.
Proof. Let ~w = ~win+ ~wout, where ~win and ~wout ?
<N are constituted by the components of ~w that
are selected in and out, respectively, and have zero
values in the remaining positions. By hypothesis
|~w ? ~x| ? ?; without loss of generality, we can
consider just the case ~w ? ~x ? ?, and write ~w ?
~x = ~win ? ~x + ~wout ? ~x ? ? ? ~win ? ~x ? ? ?
~wout ? ~x ? ? ? |~wout ? ~x| ? ? ? ||~wout|| ? ||~x||,
where the last inequality holds owing to Cauchy-
Schwarz inequality. The margin associated with
~win, i.e. ?in, is therefore ? ? ||~wout|| ? ||~x|| ?
? ? ||~wout||R = ? ? ?R||~w||.
Remark 1. The lemma suggests that, even in case
of very aggressive feature selection, if a small per-
centage ? of ||~w|| is lost, the margin reduction is
small. Consequently, through Theorem 2, we can
conclude that the accuracy of the model is by and
large preserved.
Remark 2. We prefer to show the lemma in the
more general form, but if we use normalized ~x and
classifiers with ||~w|| ? 1, then ?in = ??||~w||? >
? ? ?.
The last result that we present justifies our se-
lection approach as it demonstrates that most of
the gradient norm is concentrated in relatively few
features, with respect to the huge space induced
by tree kernels. The selection of these few fea-
tures allows us to preserve most of the norm and
the margin.
Lemma 2. Let ~w be a linear separator of a set of
points X , where each ~xi ? X is an explicit vector
representations of a tree ti in the space induced by
STK and let ? be the largest s(ti), i.e. the max-
imum tree size. Then, if we discard fragments of
size greater than ?, ||~wout|| ? ??2
?
(??)??(??)?
1??? .
Proof. By applying simple norm proper-
ties, ||~wout|| =
?
?
?
?`
i=1 ?iyi~xouti
?
?
? ?
?`
i=1
||?iyi~xouti || =
?`
i=1 ?i||~xouti ||. To evaluate
the latter, we first re-organize the summation in
Eq. 5 (with no normalization) such that ?~xi?2
=
??
k=1
?
j:s(fj)=k t
2
i,j?
s(fj). Since a fragment
fj can be at maximum rooted in ? nodes, then
ti,j ? ?. Therefore, by replacing the number of
trees of size k with the upperbound ?k, we have
?~xi? <
???
k=1 ?2?k?k =
???
k=1 ?2(??)k =
?
?2 1??
?
1?? , where we applied geometric series
summation. Now if we assume that our algorithm
selects out (i.e. discards) fragments with size
s(f) > ?, ?~xouti? <
?
?2 ?
????
1?? . It follows that
||~wout|| <
?`
i=1 ?i
?
?2 ?
????
1?? . In case of hard-
margin SVMs, we have
?`
i=1 ?i = 1/?
2. Thus,
||~wout|| < ??2
?
?????
1?? =
?
?2
?
(??)??(??)?
1??? .
Remark 3. The lemma shows that for an enough
large ? and ? < 1/?, ||~wout|| can be very small,
even though it includes an exponential number of
features, i.e. all the subtrees whose size ranges
from ? to ?. Therefore, according to Lemma 1 and
Theorem 2, we can discard an exponential number
of features with a limited loss in accuracy.
Remark 4. Regarding the proposed norm bound,
we observe that ?k is a rough overestimation of the
the real number of fragments having size k rooted
in the nodes of the target tree t. This suggests that
we don?t really need ? < 1/?. Moreover, in case
of soft-margin SVMs, we can bound ?i with the
value of the trade-off parameter C.
4 Previous Work
Initial work on feature selection for text, e.g.
(Yang and Pedersen, 1997), has shown that it may
improve the accuracy or, at least, improve effi-
ciency while preserving accuracy. Our context for
feature selection is different for several important
reasons: (i) we focus on structured features with
a syntactic nature, which show different behaviour
from lexical ones, e.g. they tend to be more sparse;
(ii) in the TK space, the a-priori weights are very
skewed, and large fragments receive exponentially
lower scores than small ones; (iii) there is high re-
dundancy and inter-dependency between such fea-
tures; (iv) we want to be able to observe the most
relevant features automatically generated by TKs;
and (v) the huge number of features makes it im-
possible to evaluate the weight of each feature in-
dividually.
Guyon and Elisseeff (2003) carries out a very
informative survey of feature selection techniques.
Non-filter approaches for SVMs and kernel ma-
chines are often concerned with polynomial and
226
Gaussian kernels, e.g. (Weston et al, 2001; Neu-
mann et al, 2005). In (Kudo and Matsumoto,
2003), an extension of the PrefixSpan algo-
rithm (Pei et al, 2001) is used to efficiently mine
the features in a low degree polynomial kernel
space. The authors discuss an approximation
of their method that allows them to handle high
degree polynomial kernels. Suzuki and Isozaki
(2005) present an embedded approach to feature
selection for convolution kernels based on ?2-
driven relevance assessment. With respect to their
work, the main differences in the approach that we
propose are that we want to exploit the SVM op-
timizer to select the most relevant features, and to
be able to observe the relevant fragments.
Regarding work that may directly benefit from
reverse kernel engineering is worthwhile mention-
ing: (Cancedda et al, 2003; Shen et al, 2003;
Daume? III and Marcu, 2004; Giuglea and Mos-
chitti, 2004; Toutanova et al, 2004; Kazama and
Torisawa, 2005; Titov and Henderson, 2006; Kate
and Mooney, 2006; Zhang et al, 2006; Bloehdorn
et al, 2006; Bloehdorn and Moschitti, 2007; Mos-
chitti and Zanzotto, 2007; Surdeanu et al, 2008;
Moschitti, 2008; Moschitti and Quarteroni, 2008;
Martins et al, 2009; Nguyen et al, 2009a)
5 Mining Fragments Efficiently
The high-level description of our feature selection
technique is as follows: we start by learning an
STK model and we greedily explore the support
vectors in search for the most relevant fragments.
We store them in an index, and then we decode (or
linearize) all the trees in the dataset, i.e. we repre-
sent them as vectors in a linear space where only a
very small subset of the fragments in the original
space are accounted for. These vectors are then
employed for learning and classification in the lin-
ear space.
To explore the fragment space defined by a set
of support vectors, we adopt the greedy strategy
described in Algorithm 5.1. Its arguments are a
model M , and the threshold factor L. The greedy
algorithm explores the fragment space in a small to
large fashion. The first step is the generation of the
all base fragments F encoded in each tree, i.e. the
smallest possible fragments according to the defi-
nition of the kernel function. For STK, such frag-
ments are all those consisting of a node and all its
direct children (i.e. production rules of the gram-
mar). We assess the cumulative relevance of each
Algorithm 5.1: GREEDY MODEL MINER(M,L)
B ? BASE FRAGS(model)
B ? REL(BEST(B))
? ? B/L
Dprev ? FILTER(B, ?)
UPDATE(Dprev)
while Dprev 6= ?
do
?
????????????????????
????????????????????
Dnext ? ?
? ? 1/ ? widthfactor ? /
Wprev ? Dprev
whileWprev 6= ?
do
?
???????????
???????????
Wnext ? ?
for each f ? Wprev
do
?
?????
?????
Ef ? EXPAND(f, ?)
F ? FILTER(Ef , ?)
if F 6= ?
then
{
Wnext ?Wnext ? {f}
Dnext ? Dnext ? F
UPDATE(F )
? ? ? + 1
Wprev ?Wnext
Dprev ? Dnext
return (result)
base fragment according to Eq. 6 and then use the
relevanceB of the heaviest fragment, i.e. the frag-
ment with the highest relevance in absolute value,
as a criterion to set our fragment mining threshold
? to B/L. We then apply the FILTER(?) operator
which discards all the fragments whose cumula-
tive score is less than ?. Then, the UPDATE(?) op-
erator stores the ramaining fragments in the index.
The exploration of the kernel space is carried
out via the process of fragment expansion, by
which each fragment retained at the previous step
is incrementally grown to span more levels of the
tree and to include more nodes at each level. These
two directions of growth are controlled by the
outer and the inner while loops, respectively. Frag-
ment expansion is realized by the EXPAND(f, n)
operator, that grows the fragment f by including
the children of n expandable nodes in the frag-
ment. Expandable nodes are nodes which are
leaves in f but that have children in the tree that
originated f .
After each expansion, the FILTER(?) operator is
invoked on the set of generated fragments. If the
filtered set is empty, i.e. no fragments more rele-
vant than ? have been found during the previous
iteration, then the loop is terminated.
Unlike previous attempts, this algorithm relies
on just one parameter, i.e. L. As it revolves around
the weight of the most relevant fragment, it oper-
ates according to the norm-preservation principle
described in the previous sections. In fact, if we
call N the number of fragments mined for a given
value of L, the norm after feature selection can be
227
bounded by BL
?
N ? ?win? ? B
?
N .
The choice of B, i.e. the highest relevance of
a base fragment, as an upper bound for fragment
relevance is motivated as follows. In Eq. 6, we can
identify a term Ti = ?iyi/?ti? that is the same for
all the fragments in the tree ti. For 0 < ? ? 1,
if fj is an expansion of fk, then from our defini-
tion of fragment expansion it follows that ?
s(fj)
2 <
?
s(fk)
2 . It can also be observed that ti,j ? ti,k. In-
deed, if ti,k is a subset of ti,j , then it will occur at
least as many times as its expansion ti,k, possibly
occurring as a seed fragment for different expan-
sions in other parts of the tree as well. Therefore,
if Ef is the set of expansions of f , for every two
fragments fi,j , fi,k coming from the same tree ti,
we can conclude that x(j)i < x
(k)
i ?fi,j ? Efi,k . In
other words, for each tree in the model, base frag-
ments are the most relevant, and we can assume
that the relevance of the heaviest fragment is an
upper bound for the relevance of any fragment 4.
6 Experiments
We ran a set of thorough experiments to sup-
port our claims with empirical evidence. We
show our results on three very different bench-
marks: Question Classification (QC) using TREC
10 data (Voorhees, 2001), Relation Extraction
(RE) based on the newswire and broadcast news
domain of the ACE 2004 English corpus (Dod-
dington et al, 2004) and Semantic Role Labeling
(SRL) on the CoNLL 2005 shared task data (Car-
reras and Ma`rquez, 2005). In the next sections we
elaborate on the setup and outcome of each set
of experiments. As a supervised learning frame-
work we used SVM-Light-TK5, which extends the
SVM-Light optimizer (Joachims, 2000) with sup-
port for tree kernel functions.
Unless differently stated, all the classifiers are
parametrized for optimal Precision and Recall on
a development set, obtained by selecting one ex-
ample in ten from the training set with the same
positive-to-negative example ratio. The results
that we show are obtained on the test sets by using
all the available data for training. For multi-class
scenarios, the classifiers are arranged in a one vs.
4In principle, the weight of some fragment encoded in the
model M may be greater than B. However, as an empirical
justification, we report that in all our experiments we have
never been able to observe such case. Thus, with a certain
probability, we can assume that the highest weight will be
obtained from the heaviest of the base fragments.
5
http://disi.unitn.it/?moschitt/Tree-Kernel.htm
all configuration, where each sentence is a positive
example for one of the classes, and negative for
the others. While binary classifiers are evaluated
in terms of F1 measure, for multi-class classifiers
we show the final accuracy.
The next paragraphs describe the datasets used
for the experiments.
Question Classification (QC) Given a question,
the task consists in selecting the most appropriate
expected answer type from a given set of possibil-
ities. We adopted the question taxonomy known
as coarse grained, which has been described
in (Zhang and Lee, 2003) and (Li and Roth, 2006),
consisting of six non overlapping classes: Abbre-
viations (ABBR), Descriptions (DESC, e.g. def-
initions or explanations), Entity (ENTY, e.g. an-
imal, body or color), Human (HUM, e.g. group
or individual), Location (LOC, e.g. cities or coun-
tries) and Numeric (NUM, e.g. amounts or dates).
The TREC 10 QA data set accounts for 6,000
questions. For each question, we generate the
full parse of the sentence and use it to train our
models. Automatic parses are obtained with the
Stanford parser6 (Klein and Manning, 2003), and
we actually have only 5,953 sentences in our data
set due to parsing issues. During preliminary ex-
periments, we observed an uneven distribution of
examples in the traditional training/test split (the
same used in P&M). Therefore, we used a ran-
dom selection to generate an unbiased split, with
5,468 sentences for training and 485 for testing.
The resulting data set is available for download
at http://danielepighin.net/cms/research/
QC_dataset.tgz.
Relation Extraction (RE) The corpus
consists of 348 documents, and contains
seven relation classes defined over pairs of
mentions: Physical, Person/Social, Employ-
ment/Membership/Subsidiary, Agent-Artifact,
PER/ORG Affiliation, GPE Affiliation, and
Discourse. There are 4,400 positive and 38,696
negative examples when the potential relations
are generated using all the entity/mention pairs in
the same sentence.
Documents are parsed using the Stanford
Parser, where the nodes of the entities are enriched
with information about the entity type. Overall,
we used the setting and data defined in (Nguyen et
al., 2009b).
6
http://nlp.stanford.edu/software/lex-parser.shtml
228
Semantic Role Labeling (SRL) SRL can be de-
composed into two tasks: boundary detection,
where the word sequences that are arguments of
a predicate word w are identified, and role clas-
sification, where each argument is assigned the
proper role. For these experiments we concen-
trated on this latter task and used exactly the same
setup as P&M. We considered all the argument
nodes of any of the six PropBank (Palmer et al,
2005) core roles7 (i.e. A0, . . . , A5) from all the
available training sections, i.e. 2 through 21, for a
total of 179,091 training instances. Similarly, we
collected 9,277 test instances from the annotations
of Section 23.
6.1 Model Comparison
To show the validity of Lemma 1 in practical sce-
narios, we compare the accuracy of our linearized
models against vanilla STK classifiers. We de-
signed two types of classifiers:
LIN, a linearized STK model, which uses the
weights estimated by the learner in the STK space
and linearized examples; in other words LIN uses
~wIN . It allows us to measure exactly the loss in
accuracy with respect to the reduction of ||~w||.
OPT, a linearized STK model that is re-
optimized in the linear space, i.e. for which we
retrained an SVM using the linearized training ex-
amples as input data. Since the LIN solution is
part of the candidate solutions from which OPT is
selected, we always expect higher accuracy from
it.
Additionally, we compare selection based on
gradient ~w (as detailed in Section 2.4) against to
?2 selection, which evaluates the relevance of fea-
tures, in a similar way to (Suzuki and Isozaki,
2005). The relevance of a fragment is calculated
as
?2 =
N(yN ?Mx)2
x(N ? x)M(N ?M)
,
where N is the number of support vectors, M is
the number of positive vectors (i.e. ?i > 0), and x
and y are the fractions ofN andM where the frag-
ment is instantiated, respectively. We specify the
selection models by means of Grad for the former
and Chi for the latter. For example, a model called
OPT/Grad is a re-trained model using the features
selected according the highest gradient weights,
while LIN/Chi would be a linearized tree kernel
model using ?2 for feature selection.
7We do not consider adjuncts because we preferred the
number of classes to be similar across the three benchmarks.
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 1  10  100  1000  10000
Number of fragments (log)
ABBRDESCENTYHUMLOCNUM
1?
?
Figure 1: Percentage of gradient Norm, i.e. 1? ?,
according to the number of selected fragments, for
different QC classifiers.
STK Linearized
LIN OPT
F1 ||~w|| Frags F1 ||~win|| F1
A 80.00 11.77 566 66.67 7.13 90.91
D 86.26 41.33 5161 81.87 25.10 83.72
E 76.86 51.71 5,702 73.03 31.06 75.56
H 84.92 43.61 5,232 80.47 26.20 77.08
L 81.69 38.73 1,732 78.87 24.27 82.89
N 92.31 37.65 1,015 85.07 24.53 87.07
Table 1: Per-class comparison between STK and
the LIN/Grad and OPT/Grad models on the QC
task. Each class is identified by its initial (e.g.
A=ABBR). For each class, we considered a value
of the threshold factor parameter L so as to retain
at least 60% of the gradient norm after feature se-
lection.
6.2 Results
The plots in Figure 1 show, for each class, the per-
centage of the gradient norm (i.e. 1 ? ?, see Sec-
tion 3) retained when including a different num-
ber of fragments. This graph empirically validates
Lemma 2 since it clearly demonstrates that after
1,000-10,000 features the percentage of the norm
reaches a plateau (around 60-65%). This means
that after such threshold, which interestingly gen-
eralizes across all classifiers, a huge number of
features is needed for a small increase of the norm.
We recall that the maximum reachable norm is
around 70% since we apriori filter out fragments
of frequency lower than three.
Table 1 shows the F1 of the binary question clas-
sifiers learned with STK, LIN/Grad and OPT/Grad
models. It also shows the norm of the gradi-
ent before, ||~w||, and after, ||~win||, feature selec-
229
 50
 55
 60
 65
 70
 75
 80
 85
 90
 0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7
F1 
(LO
C)
LIN/GradOPT/GradLIN/ChiOPT/ChiSTK
1? ?
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0.1  0.2  0.3  0.4  0.5  0.6
F1 
(DE
SC)
LIN/GradOPT/GradLIN/ChiOPT/ChiSTK
1? ?
Figure 2: F1-measure of LOC and DESC wrt dif-
ferent 1? ? values.
tion along with the number of selected fragments,
Frags. Instead of selecting an optimal number of
fragments on a validation set, we investigated the
60% value suggested by the previous plot. Thus,
for each category we selected the feature set reach-
ing approximately 60% of ||~w||. The table shows
that the accuracy of the OPT/Grad model is in
line with STK. In some cases, e.g. ABBR, the
projected model is more accurate, i.e. 90.91 vs.
80.00, whereas in others, e.g. HUM, STK per-
forms better, i.e. 84.92 vs. 77.08. It is interesting
to see how the empirical results clearly comple-
ment the theoretical findings of the previous sec-
tions. For example, the LOC classifier uses only
1,732 of the ? 1012 features encoded by the cor-
responding STK model, but since only 40% of the
norm of ~w is lost, classification accuracy is af-
fected only marginally.
As mentioned above, the selected number of
features is not optimal for every class. Fig-
ure 2 plots the accuracy of the LIN/Grad and
OPT/Grad for different numbers of fragments on
two classes 8. These show that the former, with
8The other classes, which show similar behaviour, are
omitted due to lack of space.
 20
 30
 40
 50
 60
 70
 80
 90
 100  1000  10000  100000
Mu
ltic
lass
 ac
cur
acy
Number of fragments (log)
OPT/GradOPT/ChiSTK
Figure 3: Multiclass accuracy obtained by includ-
ing a growing number of fragments.
more than 60% of the norm, approaches STK
whereas the latter requires less fragments. The
plots also show the comparison against the same
fragment mining algorithm and learning frame-
work when using ?2-based selection. This also
provides similar good results, as far as the reduc-
tion of ||~w|| is kept under control, i.e. as far as we
select the components of the gradient that mostly
affect its norm.
To concretely assess the benefits of our models
for QC, Figure 3 plots the accuracy of OPT/Grad
and OPT/Chi on the multiclass QC problem wrt
the number of fragments employed. The results
for the multi-class classifier are less biased by the
binary Precision/Recall classifiers thus they are
more stable and clearly show how, after selecting
the optimal number of fragments (1,000-10,000
i.e. 60-65% of the norm), the accuracy of the OPT
and CHI classifiers stabilize around levels of accu-
racy which are in line with STK.
STK OPT/Grad
F1 F1 Frags
QC 83.70 84.12 ?2k
RE 67.53 66.31 ?10k
SRL 87.56 88.17 ?300k
Table 2: Multiclass classification accuracy on
three benchmarks.
Finally, Table 2 shows the best results that we
achieved on the three multi-class classification
tasks, i.e. QC, RE9 and SRL, and compares them
against the STK 10. For all the tasks OPT/Grad
9For RE, we show lower accuracy than in (Nguyen et al,
2009b) since, to have a closer comparison with STK, we do
not combine structural features with manual designed fea-
tures.
10We should point out that this models are only partially
230
produces the best results for all the tests, even
though the difference with OPT/Chi is generally
not statistically significant. Out of three tasks,
OPT/Grad manages to slightly improve two of
them, i.e. QC (84.12 vs. 83.7) and SRL (88.17
vs. 87.56), while STK is more accurate on RE, i.e.
67.53 vs. 66.31.
6.3 Comparison with P&M
The results on SRL can be compared against
those that we presented in (Pighin and Moschitti,
2009a), where we measured an accuracy of 87.13
exactly on the same benchmark. As we can see in
Table 2, our model improves the classification ac-
curacy of about 1 point, i.e. 88.17. On the other
hand, such comparison is not really fair since the
algorithms rely on different parameter sets, and it
is almost impossible to find matching configura-
tions for the different versions of the algorithms
that would result in exactly the same number of
fragments. In a projected space with approxi-
mately 103 or 104 fragments, including a few hun-
dred more features can produce noticeably differ-
ent accuracy readings.
Generally speaking, the current model can
achieve comparable accuracy with P&M while
considering a smaller number of fragments. For
example, in (Pighin and Moschitti, 2009b) the
best model for the A1 binary classifier of the
SRL benchmark was obtained by including 50,000
fragments, achieving an F1 score of 89.04. With
the new algorithm, using approximately half the
fragments the accuracy of the linearized A1 clas-
sifier is 90.09. In P&M, the algorithm would only
consider expansions of a fragment f where at most
m nodes are expanded. Consequently, the set of
mined fragments may include some small struc-
tures which can be less relevant than larger ones.
Conversely, the new algorithm (see Alg. 5.1) may
include larger but more relevant structures, thus
accounting for a larger fraction of the gradient
norm with a smaller number of fragments.
Concerning efficiency, the complexity of both
mining algorithms is proportional to the number
of fragments that they generate. Therefore, we can
conclude that the new implementation is more effi-
cient by considering that we can achieve the same
accuracy with less fragments. As for the complex-
optimized, as we evaluated them by using the same threshold
factor parameter L for all the classes. Better performances
could be achieved by selecting an optimal value of L for in-
dividual classes when building the multi-class classifier.
ity of decoding, i.e. providing explicit vector rep-
resentations of the input trees, in P&M, we used
a very naive approach, i.e. the generation of all
the fragments encoded in the tree and then look up
each fragment in the index. This solution has ex-
ponential complexity with the number of nodes in
the tree. Conversely, the new implementation has
approximately linear complexity. The approach is
based on the idea of an FST-like index, that we
can query with a tree node. Every time the tree
matches one of the fragments, the index increases
the count of that fragment for the tree. The reduc-
tion in time complexity is made possible by en-
coding in the index the sequence of expansion op-
erations that produced each indexed fragment, and
by considering only those expansions at decoding
time.
7 Conclusions
Available feature selection frameworks for very
high dimensional kernel families, such as tree ker-
nels, suffer from the lack of a theory that could
justify the very aggressive selection strategies nec-
essary to cope with the exceptionally high dimen-
sional feature space.
In this paper, we have provided a theoretical
foundation in the context of margin classifiers by
(i) linking the reduction of the gradient norm to the
theoretical error bound and (ii) by proving that the
norm is mostly concentrated in a relatively small
number of features. The two properties suggest
that we can apply an extremely aggressive fea-
ture selection by keeping the same accuracy. We
described a very efficient algorithm to carry out
such strategy in the fragment space. Our experi-
ments empirically support our theoretical findings
on three very different NLP tasks.
Acknowledgements
We would like to thank Truc-Vien T. Nguyen for
providing us with the SVM learning and test files
of the Relation Extraction dataset. Many thanks to
the anonymous reviewers for their valuable sug-
gestions.
This research has been partially supported by the
EC project, EternalS: ?Trustworthy Eternal Sys-
tems via Evolving Software, Data and Knowl-
edge?, project number FP7 247758.
231
References
P. Bartlett and J. Shawe-Taylor, 1998. Advances in Kernel
Methods ? Support Vector Learning, chapter Generaliza-
tion Performance of Support Vector Machines and other
Pattern Classifiers. MIT Press.
Stephan Bloehdorn and Alessandro Moschitti. 2007. Struc-
ture and semantics for expressive text kernels. In In Pro-
ceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and
Alessandro Moschitti. 2006. Semantic kernels for text
classification based on topological measures of feature
similarity. In Proceedings of ICDM 06, Hong Kong, 2006.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to
the CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of CoNLL?05.
Michael Collins and Nigel Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over Discrete
Structures, and the Voted Perceptron. In Proceedings of
ACL?02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceedings of
ACL?04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for Re-
lational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing by
maximum entropy tagging and SVM reranking. In Pro-
ceedings of EMNLP?04.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Auto-
matic Content Extraction (ACE) Program?Tasks, Data,
and Evaluation. Proceedings of LREC 2004, pages 837?
840.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovering using FrameNet, VerbNet and
PropBank. In In Proceedings of the Workshop on On-
tology and Knowledge Discovering at ECML 2004, Pisa,
Italy.
Isabelle Guyon and Andre? Elisseeff. 2003. An introduc-
tion to variable and feature selection. Journal of Machine
Learning Research, 3:1157?1182.
David Haussler. 1999. Convolution kernels on discrete struc-
tures. Technical report, Dept. of Computer Science, Uni-
versity of California at Santa Cruz.
T. Joachims. 2000. Estimating the generalization perfor-
mance of a SVM efficiently. In Proceedings of ICML?00.
Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for
semi-structured data. In Proceedings of ICML?02.
Rohit J. Kate and Raymond J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proceedings of
the 21st ICCL and 44th Annual Meeting of the ACL, pages
913?920, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2005. Speeding up
training with tree kernels for node relation labeling. In
Proceedings of HLT-EMNLP?05.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL?03, pages
423?430.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-
based parse reranking with subtree features. In Proceed-
ings of ACL?05.
Xin Li and Dan Roth. 2006. Learning question classifiers:
the role of semantic information. Natural Language En-
gineering, 12(3):229?249.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro
M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2009. Nonex-
tensive information theoretic kernels on measures. J.
Mach. Learn. Res., 10:935?975.
Alessandro Moschitti and Silvia Quarteroni. 2008. Kernels
on linguistic structures for answer extraction. In Proceed-
ings of ACL-08: HLT, Short Papers, Columbus, Ohio.
Alessandro Moschitti and Fabio Massimo Zanzotto. 2007.
Fast and effective kernels for relational learning from
texts. In Zoubin Ghahramani, editor, Proceedings of the
24th Annual International Conference on Machine Learn-
ing (ICML 2007).
Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2008. Tree kernels for semantic role labeling. Compu-
tational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Pro-
ceedings of ECML?06, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Proceeding
of CIKM ?08, NY, USA.
Julia Neumann, Christoph Schnorr, and Gabriele Steidl.
2005. Combined SVM-Based Feature Selection and Clas-
sification. Machine Learning, 61(1-3):129?150.
Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe
Riccardi. 2009a. Convolution kernels on constituent, de-
pendency and sequential structures for relation extraction.
In Proceedings of EMNLP.
Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe
Riccardi. 2009b. Convolution kernels on constituent,
dependency and sequential structures for relation extrac-
tion. In EMNLP ?09: Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing,
pages 1378?1387, Morristown, NJ, USA. Association for
Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of semantic
roles. Comput. Linguist., 31(1):71?106.
J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U. Dayal,
and M. C. Hsu. 2001. PrefixSpan Mining Sequential Pat-
terns Efficiently by Prefix Projected Pattern Growth. In
Proceedings of ICDE?01.
232
Daniele Pighin and Alessandro Moschitti. 2009a. Efficient
linearization of tree kernel functions. In Proceedings of
CoNLL?09.
Daniele Pighin and Alessandro Moschitti. 2009b. Reverse
engineering of tree kernel feature spaces. In Proceedings
of EMNLP, pages 111?120, Singapore, August. Associa-
tion for Computational Linguistics.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Us-
ing LTAG Based Features in Parse Reranking. In Proceed-
ings of EMNLP?06.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large online
QA collections. In Proceedings of ACL-08: HLT, Colum-
bus, Ohio.
Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree
Kernels with Statistical Feature Mining. In Proceedings
of NIPS?05.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings of
CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher Man-
ning. 2004. The Leaf Path Projection View of Parse
Trees: Exploring String Kernels for HPSG Parse Selec-
tion. In Proceedings of EMNLP 2004.
Ellen M. Voorhees. 2001. Overview of the trec 2001 ques-
tion answering track. In In Proceedings of the Tenth Text
REtrieval Conference (TREC, pages 42?51.
Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimil-
iano Pontil, Tomaso Poggio, and Vladimir Vapnik. 2001.
Feature Selection for SVMs. In Proceedings of NIPS?01.
Yiming Yang and Jan O. Pedersen. 1997. A comparative
study on feature selection in text categorization. In Dou-
glas H. Fisher, editor, Proceedings of ICML-97, 14th In-
ternational Conference on Machine Learning, pages 412?
420, Nashville, US. Morgan Kaufmann Publishers, San
Francisco, US.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of SI-
GIR?03, pages 26?32.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntac-
tic Features for Relation Extraction using a Convolution
tree kernel. In Proceedings of NAACL.
233
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?9,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
Automatic Projection of Semantic Structures:
an Application to Pairwise Translation Ranking
Daniele Pighin Llu??s Ma`rquez
TALP Research Center
Universitat Polite`cnica de Catalunya
{pighin,lluism}@lsi.upc.edu
Abstract
We present a model for the inclusion of se-
mantic role annotations in the framework of
confidence estimation for machine translation.
The model has several interesting properties,
most notably: 1) it only requires a linguis-
tic processor on the (generally well-formed)
source side of the translation; 2) it does
not directly rely on properties of the transla-
tion model (hence, it can be applied beyond
phrase-based systems). These features make
it potentially appealing for system ranking,
translation re-ranking and user feedback eval-
uation. Preliminary experiments in pairwise
hypothesis ranking on five confidence estima-
tion benchmarks show that the model has the
potential to capture salient aspects of transla-
tion quality.
1 Introduction
The ability to automatically assess the quality of
translation hypotheses is a key requirement to-
wards the development of accurate and depend-
able translation models. While it is largely agreed
that proper transfer of predicate-argument structures
from source to target is a very strong indicator of
translation quality, especially in relation to ade-
quacy (Lo and Wu, 2010a; 2010b), the incorpora-
tion of this kind of information in the Statistical Ma-
chine Translation (SMT) evaluation pipeline is still
limited to few and isolated cases, e.g., (Gime?nez and
Ma`rquez, 2010).
In this paper, we propose a general model for
the incorporation of predicate-level semantic anno-
tations in the framework of Confidence Estimation
(CE) for machine translation, with a specific focus
on the sub-problem of pairwise hypothesis ranking.
The model is based on the following underlying as-
sumption: by observing how automatic alignments
project semantic annotations from source to target
in a parallel corpus, it is possible to isolate features
that are characteristic of good translations, such as
movements of specific arguments for some classes
of predicates. The presence (or absence) of these
features in automatic translations can then be used as
an indicator of their quality. It is important to stress
that we are not claiming that the projections pre-
serve the meaning of the original annotation. Still,
it should be possible to observe regularities that can
be helpful to rank alternative translation hypotheses.
The general workflow (which can easily be ex-
tended to cope with different annotation layers,
such as sequences of meaningful phrase boundaries,
named entities or sequences of chunks or POS tags)
is exemplified in Figure 1. During training (on the
left), the system receives a parallel corpus of source
sentences and the corresponding reference transla-
tions. Source sentences are annotated with a lin-
guistic processor. The annotations are projected us-
ing training alignments, obtaining gold projections
that we can use to learn a model that captures cor-
rect annotation movements, i.e., observed in refer-
ence translations. At test time, we want to assess
the quality of a translation hypothesis given a source
sentence. As shown on the right side of Figure 1, the
first part of the process is the same as during train-
ing: the source sentence is annotated, and the an-
notation is projected onto the translation hypothesis
via automatic alignments. The model is then used
1
References Source Source Hypothesis
Align Annotate Annotate Align
Project Project
Learn Compare Score
Alignments
Parallel
Annotations
Model
AlignmentsAnnotations
Training Test
Figure 1: Architectural overview.
to compare the observed projection against the ex-
pected projection given the source annotation. The
distance between the two projections (observed and
expected) can then be used as a measure of the qual-
ity of the hypothesis.
As it only considers one-sided annotations, our
framework does not require the availability of com-
parable linguistic processors and linguistic annota-
tions, tagsets, etc., on both sides of the translation
process. In this way, it overcomes one of the main
obstacles to the adoption of linguistic analysis for
MT confidence estimation. Furthermore, the fact
that source data is generally well-formed lowers the
requirements on the linguistic processor in terms of
robustness to noisy data, making it possible to em-
ploy a wider range of linguistic processors.
Within this framework, in this paper we describe
our attempt to bridge Semantic Role Labeling (SRL)
and CE by modeling proposition-level semantics for
pairwise translation ranking. The extent to which
this kind of annotations are transferred from source
to target has indeed a very high correlation with re-
spect to human quality assessments (Lo and Wu,
2010a; 2010b). The measure that we propose is then
an ideal addition to already established CE mea-
sures, e.g., (Specia et al, 2009; Blatz et al, 2004),
as it attempts to explicitly model the adequacy of
translation hypotheses as a function of predicate-
argument structure coverage. While we are aware of
the fact that the current definition of the model can
be improved in many different ways, our preliminary
investigation, on five English to Spanish translation
benchmarks, shows promising accuracy on the dif-
ficult task of pairwise translation ranking, even for
translations with very few distinguishing features.
To capture different aspects of the projection of
SRL annotations we employ two instances of the
abstract architecture shown in Figure 1. The first
works at the proposition level, and models the cor-
rect movement of arguments from source to target.
The second works at the argument level, and models
the fluency and adequacy of individual arguments
within each predicate-argument structure. The mod-
els that we learn during training are simple phrase-
based translation models working on different kinds
of sequences, i.e., role labels in the former case and
words in the latter. To evaluate the adequacy of an
automatically projected proposition or argument, we
force the corresponding translation model to gener-
ate it (via constrained decoding). The reachability
and confidence of each translation are features that
we exploit to compare alternative translations, by
combining them in a simple voting scheme.
To score systems which are not under our direct
control (the typical scenario in CE benchmarks), we
introduce a component that generates source-target
alignments for any pair of aligned test sentences.
This addition has the nice property of allowing us
to handle the translation as a black-box, decoupling
the evaluation from a specific system and, in theory,
allowing the model to cope with phrase-based, rule-
based or hierarchical systems alike, as well as with
human-generated translations.
The rest of the paper is structured as follows: in
Section 2 we will review a selection of related work;
in Section 3 we will detail our approach; in Section 4
we will present the results of our evaluation; finally,
in Section 5 we will draw our conclusions.
2 Related work
Confidence estimation is the sub-problem within
MT evaluation concerned with the assessment of
translation quality in the absence of reference trans-
lations. A relevant initial work on this topic is
the survey by Blatz et al (2004), in which the au-
thors define a rich set of features based on source
data, translation hypotheses, n-best lists and model
characteristics to classify translations as ?good?
or ?bad?. In their observations, they conclude
2
that the most relevant features are those based on
source/target pairs and on characteristics of the
translation model.
Specia et al (2009) build on top these results by
designing a feature-selection framework for confi-
dence estimation. Translations are considered as
black-boxs (i.e., no system or model-dependent fea-
tures are employed), and novel features based on the
number of content words, a POS language model on
the target side, punctuation and number matchers in
source and target translations and the percentage of
uni-grams are introduced. Features are selected via
Partial Least Squares (PLS) regression (Wold et al,
1984). Inductive Confidence Machines (Papadopou-
los et al, 2002) are used to estimate an optimal
threshold to distinguish between ?good? and ?bad?
translations. Even though the authors show that a
small set of shallow features and some supervision
can produce good results on a specific benchmark,
we are convinced that more linguistic features are
needed for these methods to perform better across a
wider spectrum of domains and applications.
Concerning the usage of SRL for SMT, Wu and
Fung (2009) reported a first successful application of
semantic role labels to improve translation quality.
They note that improvements in translation quality
are not reflected by traditional MT evaluation met-
rics (Doddington, 2002; Papineni et al, 2002) based
on n-gram overlaps. To further investigate the topic,
Lo and Wu (2010a; 2010b) involved human annota-
tors to demonstrate that the quality of semantic role
projection on translated sentences is very highly cor-
related with human assessments.
Gime?nez and Ma`rquez (2010) describe a frame-
work for MT evaluation and meta-evaluation com-
bining a rich set of n-gram-based and linguistic met-
rics, including several variants of a metric based on
SRL. Automatic and reference translations are anno-
tated independently, and the lexical overlap between
corresponding arguments is employed as an indica-
tor of translation quality. The authors show that syn-
tactic and semantic information can achieve higher
reliability in system ranking than purely lexical mea-
sures.
Our original contribution lies in the attempt to ex-
ploit SRL for assessing translation quality in a CE
scenario, i.e., in the absence of reference transla-
tions. By accounting for whole predicate-argument
sequences as well as individual arguments, our
model has the potential to capture aspects which
relate both to the adequacy and to the fluency of
a translation. Furthermore, we outline a general
framework for the inclusion of linguistic processors
in CE that has the advantage of requiring resources
and software tools only on the source side of the
translation, where well-formed input can reasonably
be expected.
3 Model
The task of semantic role labeling (SRL) consists
in recognizing and automatically annotating seman-
tic relations between a predicate word (not nec-
essarily a verb) and its arguments in natural lan-
guage texts. The resulting predicate-argument struc-
tures are commonly referred to as propositions, even
though we will also use the more general term anno-
tations.
In PropBank (Palmer et al, 2005) style anno-
tations, which our model is based on, predicates
are generally verbs and roles are divided into two
classes: core roles (labeled A0, A1, . . . A5), whose
semantic value is defined by the predicate syntactic
frame, and adjunct roles (labeled AM-*, e.g., AM-
TMP or AM-LOC) 1 which are a closed set of verb-
independent semantic labels accounting for predi-
cate aspects such as temporal, locative, manner or
purpose. For instance, in the sentence ?The com-
mission met to discuss the problem? we can iden-
tify two predicates, met and discuss. The corre-
sponding annotations are ?[A0 The commission] [pred
met] [AM-PRP to discuss the problem]? and ?[A0 The
commission] met to [pred discuss] [A1 the problem]?.
Here, A0 and A1 play the role of prototypical sub-
ject and object, respectively, and AM-PRP is an ad-
junct modifier expressing a notion of purpose.
Sentence annotations are inherently non-
sequential, as shown by the previous example in
which the predicate and one of the arguments of
the second proposition (i.e., discuss and A1) are
completely embedded within an argument of the
first proposition (i.e., AM-PRP). Following a widely
adopted simplification, the annotations in a sentence
are modeled independently. Furthermore we de-
1The actual role labels are in the form Arg0, . . . Arg1 and
ArgM-*, but we prefer to adopt their shorter form.
3
scribe each annotation at two levels: a proposition
level, where we model the movement of arguments
from source to target; and an argument level, were
we model the adequacy and fluency of individual
argument translations. The comparison of two
alternative translations takes into account all these
factors but it models each of them independently,
i.e., we consider how properly each propositions is
rendered in each hypothesis, and how properly each
argument is translated within each proposition.
3.1 Annotation and argument projection
At the proposition level, we simply represent the se-
quence of role-label in each proposition, ignoring
their lexical content with the exception of the pred-
icate word. Considering the previous example, the
sentence would then be represented by the two se-
quences ?A0 met AM-PRP? and ?A0 * discuss A1?.
In the latter case, the special character ?*? marks
a ?gap? between A0 and the predicate word. The
annotation is projected onto the translation via di-
rect word alignments obtained through a constrained
machine translation process (i.e., we force the de-
coder to generate the desired translation). Eventual
discontinuities in the projection of an argument are
modeled as gaps. If two arguments insist on a shared
subset of words, then their labels are combined. If
the projection of an argument is a subset of the pro-
jection of the predicate word, then the argument is
discarded. If the overlap is partial, then the non-
overlapping part of the projection is represented.
If a word insertion occurs next to an argument
or the predicate, then we include it in the final se-
quence. This decision is motivated by the consider-
ation that insertions at the boundary of an argument
may be a clue of different syntactic realizations of
the same predicate across the two languages (Levin,
1993). For example, the English construct ?A0 give
A2 A1? could be rendered as ?doy A1 a A2? in Span-
ish. Here, the insertion of the preposition ?a? at de-
coding can be an important indicator of translation
quality.
This level of detail is insufficient to model some
important features of predicate-argument structures,
such as inter-argument semantic or syntactic depen-
dencies, but it is sufficient to capture a variety of
interesting linguistic phenomena. For instance, A0-
predicate inversion translating SVO into VSO lan-
guages, or the convergence of multiple source argu-
ments into a single target argument when translating
into a morphologically richer language. We should
also stress again that we are not claiming that the
structures that we observe on the target side are lin-
guistically motivated, but only that they contain rel-
evant clues to assess quality aspects of translation.
As for the representation of individual arguments,
we simply represent their surface form, i.e., the
sequence of words spanning each argument. So,
for example, the argument representations extracted
from ?[A0 The commission] [pred met] [AM-PRP to
discuss the problem]? would be ?The commission?,
?met?, ?to discuss the problem?. To project each ar-
gument we align all its words with the target side.
The leftmost and the rightmost aligned words de-
fine the boundaries of the argument in the target sen-
tence. All the words in between (including eventual
gaps) are considered as part of the projection of the
argument. This approach is consistent with Prop-
Bank style annotations, in which arguments are con-
tiguous word sequences, and it allows us to employ a
standard translation model to evaluate the fluency of
the argument projection. The rationale here is that
we rely on proposition level annotations to convey
the semantic structure of the sentence, while at the
argument level we are more interested in evaluating
the lexical appropriateness of their realization.
The projection of a proposition and its arguments
for an example sentence is shown in Figure 2. Here,
s is the original sentence and h1 and h2 are two
translation hypotheses. The figure shows how the
whole proposition (p) and the predicate word (pred)
along with its arguments (A0, A1 and A2) are repre-
sented after projection on the two hypotheses. As we
can observe, in both cases thank (the predicate word)
gets aligned with the word gracias. For h1, the de-
coder aligns I (A0) to doy, leaving a gap between A0
and the predicate word. The gap gets filled by gen-
erating the word las. Since the gap is adjacent to at
least one argument, las is included in the representa-
tion of p for h1. In h2, the projection of A0 exactly
overlaps the projection of the predicate (?Gracias?),
and therefore A0 is not included in n for h2.
3.2 Comparing hypotheses
At test time, we want to use our model to com-
pare translation pairs and recognize the most reli-
4
s I thank the commissioner for the detailed reply
h1 Doy las gracias al comisario por la detallada respuesta
h2 Gracias , al sen?or comisario por para el respuesta
p A0 thank A1 A2 pred thank
h1 A0 +las gracias A1 A2 h1 gracias
h2 Gracias A1 A2 h2 Gracias
A1 the commissioner A0 I
h1 al comisario h1 doy
h2 al sen?or comisario h2 Gracias
A2 for the detailed reply
h2 por la detallada respuesta
h2 para el respuesta
Figure 2: Comparison between two alternative transla-
tions h1 and h2 for the source sentence s.
able. Let s be the source sentence, and h1 and h2
be two translation hypotheses. For each proposition
p in s, we assign a confidence value to its represen-
tation in h1 and h2, i.e., p1 and p2, by forcing the
proposition-level translation system to generate the
projection observed in the corresponding hypothe-
sis. The reachability of p1 (respectively, p2) and the
decoder confidence in translating p as p1 are used as
features to estimate p1 (p2) accuracy. Similarly, for
each argument a in each proposition p we generate
its automatic projection on h1 and h2, i.e., a1 and
a2. We force the argument-level decoder to translate
a into a1 and a2, and use the respective reachability
and translation confidence as features accounting for
their appropriateness.
The best translation hypothesis (h1 or h2) is then
selected according to the following decision func-
tion:
h? = argmax
i?{0,1}
?
k
fk(hi, hj 6=i, s) (1)
where each feature function fk(?, ?, ?) defines a com-
parison measure between its first two arguments, and
returns 1 if the first argument is greater (better) than
the second, and 0 otherwise. In short, the decision
function selects the hypothesis that wins the highest
number of comparisons.
The feature functions that we defined account
for the following factors, the last three being eval-
uated once for each proposition in s: (1) Num-
ber of successfully translated propositions; (2) Av-
erage translation confidence for projected proposi-
tions; (3) Number of times that a proposition in hi
has higher confidence than the corresponding propo-
sition in hi 6=j ; (4) Number of successfully translated
arguments; (5) Average translation confidence for
projected arguments; (6) Number of times that an
argument in hi has higher confidence than the corre-
sponding argument in hi 6=j .
With reference to Figure 2, the two translation hy-
potheses have been scored 4 (very good) and 2 (bad)
by human annotators. The score assigned by the
proposition decoder to p1 is higher than p2, hence
comparisons (2) and (3) are won by h1. Accord-
ing to the arguments decoder, h1 does a better job
at representing A0 and A2; h2 is better at rendering
A1, and pred is a tie. Therefore, h1 also prevails
according to (6). Given the very high confidence as-
signed to the translation of A2 in h1, the hypothesis
also prevails in (5). In this case, (1) and (4) do not
contribute to the decision as the two projections have
the same coverage.
4 Evaluation
In this section, we present the results obtained by
applying the proposed method to the task of rank-
ing consistency, or pairwise ranking of alternative
translations: that is, given a source sentence s, and
two candidate translations h1 and h2, decide which
one is a better translation for s. Pairwise ranking
is a simplified setting for CE that is general enough
to model the selection of the best translation among
a finite set of alternatives. Even though it cannot
measure translation quality in isolation, a reliable
pairwise ranking model would be sufficient to solve
many common practical CE problems, such as sys-
tem ranking, user feedback filtering or hypotheses
re-ranking.
4.1 Datasets
We ran our experiments on the human assessments
released as part of the ACL Workshops on Machine
Translations in 2007 (Callison-Burch et al, 2007),
2008 (Callison-Burch et al, 2008), 2009 (Callison-
Burch et al, 2009) and 2010 (Callison-Burch et al,
2010). These datasets will be referred to as wm-
tYY(t) in the remainder, YY being the last two digits
of the year of the workshop and t = n for newswire
data or t = e for Europarl data. So, for example,
wmt08e is the Europarl test set of the 2008 edition
5
of the workshop. As our system is trained on Eu-
roparl data, newswire test sets are to be considered
out-of-domain. All the experiments are relative to
English to Spanish translations.
The wmt08, wmt09 and wmt10 datasets provide
a ranking among systems within the range [1,5] (1
being the worst system, and 5 the best). The dif-
ferent datasets contain assessments for a different
number of systems, namely: 11 for wmt08(e), 10 for
wmt08(n), 9 for wmt09 and 16 for wmt10n. Gener-
ally, multiple annotations are available for each an-
notated sentence. In all cases in which multiple as-
sessments are available, we used the average of the
assessments.
The wmt07 dataset would be the most interesting
of all, in that it provides separate assessments for
the two main dimensions of translation quality, ade-
quacy and fluency, as well as system rankings. Un-
luckily, the number of annotations in this dataset is
very small, and after eliminating the ties the num-
bers are even smaller. As results on such small num-
bers would not be very representative, we decided
not to include them in our evaluation.
We also evaluated on the dataset described
in (Specia et al, 2010), which we will refer to as
specia. As the system is based on Europarl data, it
is to be considered an in-domain benchmark. The
dataset includes results produced by four different
systems, each translation being annotated by only
one judge. Given the size of the corpus (the output
of each system has been annotated on the same set
of 4,000 sentences), this dataset is the most repre-
sentative among those that we considered. It is also
especially interesting for two other reasons: 1) sys-
tems are assigned a score ranging from 1 (bad) to 4
(good as it is) based on the number of edits required
to produce a publication-ready translation. There-
fore, here we have an absolute measure of transla-
tion accuracy, as opposed to relative rankings; 2)
each system involved in the evaluation has very pe-
culiar characteristics, hence they are very likely to
generate quite different translations for the same in-
put sentences.
4.2 Setup
Our model consists of four main components: an
automatic semantic role labeler (to annotate source
sentences); a lexical translation model (to gener-
ate the alignments required to map the annotations
onto a translation hypothesis); a translation model
for predicate-argument structures, to assign a score
to projected annotations; and a translation model for
role fillers, to assign a score to the projection of each
argument.
To automatically label our training data with se-
mantic roles we used the Swirl system2 (Surdeanu
and Turmo, 2005) with the bundled English mod-
els for syntactic and semantic parsing. On the
CoNLL-2005 benchmark (Carreras and Ma`rquez,
2005), Swirl sports an F1-measure of 76.46. This
figure drops to 75 for mixed data, and to 65.42 on
out-of-domain data, which we can regard as a con-
servative estimate of the accuracy of the labeler on
wmt benchmarks.
For all the translation tasks we employed the
Moses phrase-based decoder3 in a single-factor con-
figuration. The -constraint command line pa-
rameter is used to force Moses to output the desired
translation. For the English to Spanish lexical trans-
lation model, we used an already available model
learned using all available wmt10e data.
To build the proposition level translation system,
we first annotated all the English sentences from the
wmt10e (en?es) training set with Swirl; then, we
forced the lexical translation model to generate the
alignments for the reference translations and pro-
jected the annotations on the target side. The process
resulted in 2,493,476 parallel annotations. 5,000 an-
notations were held-out for model tuning. The train-
ing data was used to estimate a 5-gram language
model and the translation model, which we later op-
timized on held-out data.
As for the argument level translator, we trained
it on parallel word sequences spanning the same
role in an annotation and its projection. Each such
pair constitutes a training example for the argu-
ment translator, each argument representation being
modeled independently from the others. With the
same setup used for the proposition translator, we
collected 4,578,480 parallel argument fillers from
wmt10e en?es training data, holding out 20,000
pairs for model tuning.
2http://www.surdeanu.name/mihai/swirl/
3http://www.statmt.org/moses/
6
4.3 A note on recall
The main limitation of the model in its current im-
plementation is its low recall. The translation model
that we use to generate the alignments is mostly re-
sponsible for it. In fact, in approximately 35% of the
cases the constrained translation model is not able
to generate the required hypothesis. An obvious im-
provement would consist in using just an alignment
model for this task, instead of resorting to transla-
tion, for instance following the approach adopted in
(Espla` et al, 2011). It should also be noted that,
while this component adds the interesting property
of decoupling the measure from the system that pro-
duced the hypothesis, it is not strictly necessary in
all those cases in which translation alignments are
already available, e.g., for N-best re-ranking.
The second component that suffers from recall
problems is the semantic role labeler, which fails in
annotating sentences in approximately 6% of the re-
maining cases. These failures are by and large due
to the lack of proper verbal predicates in the target
sentence, and as such expose a limiting factor of the
underlying model. In another 3% of the cases, an
annotation is produced but it cannot be projected on
the hypothesis, since the predicate word on the target
side gets deleted during translation.
Another important consideration is that no mea-
sure for CE is conceived to be used in isolation, and
our measure is no exception. In combination with
others, the measure should only trigger when ap-
propriate, i.e., when it is able to capture interesting
patterns that are significant to discriminate transla-
tion quality. If it abstains, the other measures would
compensate for the missing values. In this respect,
we should also consider that not being able to pro-
duce a translation may be inherently considered an
indicator of translation quality.
4.4 Results
Table 1 lists, in each block of rows, pairwise classifi-
cation accuracy results obtained on a specific bench-
mark. The benchmarks are sorted in order of re-
verse relevance, the largest benchmark (specia) be-
ing listed first. In each row, we show results obtained
for different configurations in which the variable is
the distance d between two assessment scores. So,
for example, the row d = 1 accounts for all the
specia Corr Wrong Und(%) Acc(%)
d = 1 1076 656 14.26 62.12
d = 2 272 84 11.00 76.40
d = 3 30 8 13.64 78.95
d ? 1 1378 748 13.72 64.82
d ? 2 302 92 11.26 76.65
d ? 3 30 8 13.64 78.95
wmt10n Corr Wrong Und(%) Acc(%)
d = 1 428 374 15.04 53.37
d = 2 232 196 18.01 54.21
d = 3 98 74 16.50 56.98
d ? 1 784 664 16.20 54.14
d ? 2 356 290 17.60 55.11
d ? 3 124 94 16.79 56.88
wmt09n Corr Wrong Und(%) Acc(%)
d = 1 70 60 19.75 53.85
d = 2 30 40 20.45 42.86
d = 3 26 10 18.18 72.22
d ? 1 134 116 19.87 53.60
d ? 2 64 56 20.00 53.33
d ? 3 34 16 19.35 68.00
wmt08n Corr Wrong Und(%) Acc(%)
d = 1 64 36 12.28 64.00
d = 2 26 24 19.35 52.00
d = 3 12 6 18.18 66.67
d ? 1 104 70 14.71 59.77
d ? 2 40 34 17.78 54.05
d ? 3 14 10 14.29 58.33
wmt08e Corr Wrong Und(%) Acc(%)
d = 1 62 34 21.31 64.58
d = 2 40 30 10.26 57.14
d = 3 22 8 11.76 73.33
d ? 1 134 80 15.75 62.62
d ? 2 72 46 10.61 61.02
d ? 3 32 16 11.11 66.67
Table 1: Results on five confidence estimation bench-
marks. An n next to the task name (e.g. wmt08n) stands
for a news (i.e. out of domain) corpus, whereas an e (e.g.
wmt08e) stands for a Europarl (i.e. in domain) corpus.
The specia corpus is in-domain.
comparisons in which the distance between scores
is exactly one, while row d ? 2 considers all the
cases in which the distance is at least 2. For each
test, the columns show: the number of correct (Corr)
and wrong (Wrong) decisions, the percentage of un-
decidable cases (Und), i.e., the cases in which the
scoring function cannot decide between the two hy-
potheses, and the accuracy of classification (Acc)
measured without considering the unbreakable ties.
7
The accuracy for d ? 1, i.e., on all the available
annotations, is shown in bold.
First, we can observe that the results are above the
baseline (an accuracy of 50% for evenly distributed
binary classification) on all the benchmarks and for
all configurations. The only outlier is wmt09n for
d = 2, with an accuracy of 42.86%. Across the
different datasets, results vary from promising (spe-
cia and wmt08e, where accuracy is generally above
60%) to mildly good (wmt10n), but across all the
board the method seems to be able to provide useful
clues for confidence estimation.
As expected, the accuracy of classification tends
to increase as the difference between hypotheses be-
comes more manifest. In four cases out of six, the
accuracy for d = 3 is above 60%, with the notable
peaks on specia, wmt09n and wmt08e where it goes
over 70% (on the first, it arrives almost at 80%).
Unluckily, very few translations have very different
quality (a measure of the difficulty of the task). Nev-
ertheless, the general trend seems to support the re-
liability of the approach.
When we consider the results on the whole
datasets (i.e., d ? 1), pairwise classification accu-
racy ranges from 54% (for wmt09n and wmt10n,
both out-of-domain), to 63-64% (for specia and
wmt08e, both in-domain). Interestingly, the perfor-
mance on wmt08n, which is also out-of-domain, is
closer to in-domain benchmarks, i.e., 60%. These
figures suggest that the method is consistently reli-
able on in-domain data, but also out-of-domain eval-
uation can benefit from its application. The differ-
ence in performance between wmt08n and the other
out-of-domain benchmarks will be reason of further
investigation as future work, as well as the drop in
performance for d = 2 on three of the benchmarks.
5 Conclusions
We have presented a model to exploit the rich in-
formation encoded by predicate-argument structures
for confidence estimation in machine translation.
The model is based on a battery of translation sys-
tems, which we use to study the movement and
the internal representation of propositions and ar-
guments projected from source to target via auto-
matic alignments. Our preliminary results, obtained
on five different benchmarks, suggest that the ap-
proach is well grounded and that semantic annota-
tions have the potential to be successfully employed
for this task.
The model can be improved in many ways, its ma-
jor weakness being its low recall as discussed in Sec-
tion 4.3. Another area in which there is margin for
improvement is the representation of predicate ar-
gument structures. It is reasonable to assume that
different representations could yield very different
results. Introducing more clues about the seman-
tic content of the whole predicate argument struc-
ture, e.g., by including argument head words in the
representation of the proposition, or considering a
more fine-grained representation at the proposition
level, could make it possible to assess the quality of
a translation reducing the need to back-off to indi-
vidual arguments. As for the representation of ar-
guments, a first and straightforward improvement
would be to train a separate model for each argument
class, or to move to a factored model that would al-
low us to model explicitly the insertion of words or
the overlap of argument words due to the projection.
Another important research direction involves the
combination of this measure with already assessed
metric sets for CE, e.g., (Specia et al, 2010), to un-
derstand to what extent it can contribute to improve
the overall performance. In this respect, we would
also like to move from a heuristic scoring function
to a statistical model.
Finally, we would like to test the generality of the
approach by designing other features based on the
same ?annotate, project, measure? framework, as we
strongly believe that it is an effective yet simple way
to combine several linguistic features for machine
translation evaluation. For example, we would like
to apply a similar framework to model the movement
of chunks or POS sequences.
Acknowledgments
We would like to thank the anonymous reviewers for their
valuable comments. This research has been partially funded
by the Spanish Ministry of Education and Science (OpenMT-
2, TIN2009-14675-C03) and the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) under grant
agreement numbers 247762 (FAUST project, FP7-ICT-2009-
4-247762) and 247914 (MOLTO project, FP7-ICT-2009-4-
247914).
8
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. ACL.
Chris Callison-Burch, Philipp Koehn, Cameron Shaw
Fordyce, and Christof Monz, editors. 2007. Proceed-
ings of the Second Workshop on Statistical Machine
Translation. ACL, Prague, Czech Republic.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Josh Schroeder, and Cameron Shaw Fordyce, editors.
2008. Proceedings of the Third Workshop on Statisti-
cal Machine Translation. ACL, Columbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder, editors. 2009. Proceedings of the
Fourth Workshop on Statistical Machine Translation.
ACL, Athens, Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR. ACL, Uppsala,
Sweden.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 152?164, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02, pages 138?145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Miquel Espla`, Felipe Sa?nchez-Mart??nez, and Mikel L.
Forcada. 2011. Using word alignments to assist
computer-aided translation users by marking which
target-side words to change or keep unedited. In Pro-
ceedings of the 15th Annual Conference of the Euro-
pean Associtation for Machine Translation.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Lin-
guistic measures for automatic machine transla-
tion evaluation. Machine Translation, 24:209?240.
10.1007/s10590-011-9088-7.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press.
Chi-kiu Lo and Dekai Wu. 2010a. Evaluating machine
translation utility via semantic role labels. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC?10), Val-
letta, Malta. European Language Resources Associa-
tion (ELRA).
Chi-kiu Lo and Dekai Wu. 2010b. Semantic vs. syntac-
tic vs. n-gram structure for machine translation evalu-
ation. In Proceedings of the 4th Workshop on Syntax
and Structure in Statistical Translation, pages 52?60,
Beijing, China.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Comput. Linguist., 31(1):71?106.
Harris Papadopoulos, Kostas Proedrou, Volodya Vovk,
and Alexander Gammerman. 2002. Inductive confi-
dence machines for regression. In AMAI?02.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. ACL.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009. Improv-
ing the confidence of machine translation quality es-
timates. In Machine Translation Summit XII, Ottawa,
Canada.
Lucia Specia, Nicola Cancedda, and Marc Dymetman.
2010. A dataset for assessing machine translation
evaluation metrics. In Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta. European Lan-
guage Resources Association (ELRA).
Mihai Surdeanu and Jordi Turmo. 2005. Seman-
tic role labeling using complete syntactic analysis.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning (CoNLL-2005),
pages 221?224, Ann Arbor, Michigan, June. Associ-
ation for Computational Linguistics.
S. Wold, A. Ruhe, H Wold, and W.J. Dunn. 1984. The
collinearity problem in linear regression. the partial
least squares (pls) approach to generalized inverses.
5:735?743.
Dekai Wu and Pascale Fung. 2009. Semantic roles for
SMT: a hybrid two-pass model. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, NAACL-Short ?09, pages 13?16,
Stroudsburg, PA, USA. ACL.
9
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 127?132,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The UPC Submission to the WMT 2012 Shared Task on Quality Estimation
Daniele Pighin Meritxell Gonza?lez Llu??s Ma`rquez
Universitat Polite`cnica de Catalunya, Barcelona
{pighin,mgonzalez,lluism}@lsi.upc.edu
Abstract
In this paper, we describe the UPC system that
participated in the WMT 2012 shared task on
Quality Estimation for Machine Translation.
Based on the empirical evidence that fluency-
related features have a very high correlation
with post-editing effort, we present a set of
features for the assessment of quality estima-
tion for machine translation designed around
different kinds of n-gram language models,
plus another set of features that model the
quality of dependency parses automatically
projected from source sentences to transla-
tions. We document the results obtained on
the shared task dataset, obtained by combining
the features that we designed with the baseline
features provided by the task organizers.
1 Introduction
Quality Estimation (QE) for Machine Translations
(MT) is the task concerned with the prediction of the
quality of automatic translations in the absence of
reference translations. The WMT 2012 shared task
on QE for MT (Callison-Burch et al, 2012) required
participants to score and rank a set of automatic
English to Spanish translations output by a state-
of-the-art phrase based machine translation system.
Task organizers provided a training dataset of 1, 832
source sentences, together with reference, automatic
and post-edited translations, as well as human qual-
ity assessments for the automatic translations. Post-
editing effort, i.e., the amount of editing required to
produce an accurate translation, was selected as the
quality criterion, with assessments ranging from 1
(extremely bad) to 5 (good as it is). The organizers
also provided a set of linguistic resources and pro-
cessors to extract 17 global indicators of translation
quality (baseline features) that participants could de-
cide to employ for their models. For the evaluation,
these features are used to learn a baseline predictors
for participants to compare against. Systems partic-
ipating in the evaluation are scored based on their
ability to correctly rank the 422 test translations (us-
ing DeltaAvg and Spearman correlation) and/or to
predict the human quality assessment for each trans-
lation (using Mean Average Error - MAE and Root
Mean Squared Error - RMSE).
Our initial approach to the task consisted of sev-
eral experiments in which we tried to identify com-
mon translation errors and correlate them with qual-
ity assessments. However, we soon realized that
simple regression models estimated on the baseline
features resulted in more consistent predictors of
translation quality. For this reason, we eventually
decided to focus on the design of a set of global in-
dicators of translation quality to be combined with
the strong features already computed by the baseline
system.
An analysis of the Pearson correlation of the
baseline features (Callison-Burch et al, 2012)1
with human quality assessments shows that the two
strongest individual predictors of post-editing ef-
fort are the n-gram language model perplexities es-
timated on source and target sentences. This ev-
idence suggests that a reasonable approach to im-
1Baseline features are also described in http://www.
statmt.org/wmt12/quality-estimation-task.
html.
127
Feature Pearson |r| Feature Pearson |r|
BL/4 0.3618 DEP/C+/Q4/R 0.0749
BL/5 0.3544 BL/13 0.0741
BL/12 0.2823 DEP/C?/Q1/W 0.0726
BL/14 0.2675 DEP/C+/Q4/W 0.0718
BL/2 0.2667 DEP/C+/Q34/R 0.0687
BL/1 0.2620 BL/3 0.0623
BL/8 0.2575 DEP/C+/Q34/W 0.0573
BL/6 0.2143 SEQ/sys-ref/W 0.0495
DEP/C?/S 0.2072 SEQ/sys/W 0.0492
BL/10 0.2033 SEQ/ref-sys/W 0.0390
DEP/C?/Q12/S 0.1858 BL/7 0.0351
BL/17 0.1824 SEQ/sys/SStop 0.0312
BL/16 0.1725 SEQ/sys/RStop 0.0301
DEP/C?/W 0.1584 SEQ/sys-ref/SStop 0.0291
DEP/C?/R 0.1559 SEQ/sys-ref/RStop 0.0289
DEP/C?/Q12/R 0.1447 DEP/Coverage/S 0.0286
DEP/Coverage/W 0.1419 SEQ/ref-sys/S 0.0232
DEP/C?/Q1/S 0.1413 SEQ/ref-sys/R 0.0205
BL/15 0.1368 SEQ/ref-sys/RStop 0.0187
DEP/C+/Q4/S 0.1257 SEQ/sys-ref/R 0.0184
DEP/Coverage/R 0.1239 SEQ/sys/R 0.0177
SEQ/ref-sys/PStop 0.1181 SEQ/ref-sys/Chains 0.0125
SEQ/sys/PStop 0.1173 SEQ/ref-sys/SStop 0.0104
SEQ/sys-ref/PStop 0.1170 SEQ/sys/S 0.0053
DEP/C?/Q12/W 0.1159 SEQ/sys-ref/S 0.0051
DEP/C?/Q1/R 0.1113 SEQ/sys/Chains 0.0032
DEP/C+/Q34/S 0.0933 SEQ/sys-ref/Chains 0.0014
BL/9 0.0889 BL/11 0.0001
Table 1: Pearson correlation (in absolute value) of the
baseline (BL) features and the extended feature set (SEQ
and DEP) with the quality assessments.
prove the accuracy of the baseline would be to con-
centrate on the estimation of other n-gram language
models, possibly working at different levels of lin-
guistic analysis and combining information coming
from the source and the target sentence. On top of
that, we add another class of features that capture
the quality of grammatical dependencies projected
from source to target via automatic alignments, as
they could provide clues about translation quality
that may not be captured by sequential models.
The novel features that we incorporate are de-
scribed in full detail in the next section; in Sec-
tion 3 we describe the experimental setup and the
resources that we employ, while in Section 4 we
present the results of the evaluation; finally, in Sec-
tion 5 we draw our conclusions.
2 Extended features set
We extend the set of 17 baseline features with 35
new features:
SEQ: 21 features based on n-gram language mod-
els estimated on reference and automatic trans-
lations, combining lexical elements of the tar-
get sentence and linguistic annotations (POS)
automatically projected from the source;
DEP: 18 features that estimate a language model
on dependency parse trees automatically pro-
jected from source to target via unsupervised
alignments.
All the related models are estimated on a cor-
pus of 150K newswire sentences collected from the
training/development corpora of previous WMT edi-
tions (Callison-Burch et al, 2007; Callison-Burch et
al., 2011). We selected this resource because we pre-
fer to estimate the models only on in-domain data.
The models for SEQ features are computed based
on reference translations (ref ) and automatic trans-
lations generated by the same Moses (Koehn et al,
2007) configuration used by the organizers of this
QE task. As features, we encode the perplexity of
observed sequences with respect to the two models,
or the ratio of these values. For DEP features, we es-
timate a model that explicitly captures the difference
between reference and automatic translations for the
same sentence.
2.1 Sequential features (SEQ)
The simplest sequential models that we estimate
are 3-gram language models2 on the following se-
quences:
W: (Word), the sequence of words as they appear
in the target sentence;
R: (Root), the sequence of the roots of the words in
the target;
S: (Suffix) the sequence of the suffixes of the words
in the target;
As features, for each automatic translation we en-
code:
? The perplexity of the corresponding sequence
according to automatic (sys) translations: for
2We also considered using longer histories, i.e., 5-grams, but
since we could not observe any noticeable difference we finally
selected the least over-fitting alternative.
128
example, SEQ/sys/R and SEQ/sys/W are the
root-sequence and word-sequence perplexities
estimated on the corpus of automatic transla-
tions;
? The ratio between the perplexities according
the two sets of translations: for example,
SEQ/ref-sys/S is the ratio between the perplex-
ity of suffix-sequences on reference and auto-
matic translations, and SEQ/sys-ref/S is its in-
verse.3
We also estimate 3-gram language models on
three variants of a sequence in which non-stop words
(i.e., all words belonging to an open class) are re-
placed with either:
RStop: the root of the word;
SStop: the suffix of the word;
PStop: the POS of the aligned source word(s).
This last model (PStop) is the only one that requires
source/target pairs in order to be estimated. If the
target word is aligned to more than one word, we
use the ordered concatenation of the source words
POS tags; if the word cannot be aligned, we replace
it with the placeholder ?*?, e.g.: ?el NN de * VBZ
JJ en muchos NNS .?. Also in this case, different
features encode the perplexity with respect to au-
tomatic translations (e.g., SEQ/sys/PStop) or to the
ratio between automatic and reference translations
(e.g., SEQ/ref-sys/RStop).
Finally, a last class of sequences (Chains) col-
lapses adjacent stop words into a single token.
Content-words or isolated stop-words are not in-
cluded in the sequence, e.g: ?mediante la de los
de la y de las y la a los?. Again, we consider
the same set of variants, e.g. SEQ/sys/Chains or
SEQ/sys-ref/Chains.
Since there are 7 sequence types and 3 combinations
(sys, sys-ref, ref-sys) we end up with 21 new fea-
tures.
3Features extracted solely from reference translations have
been considered, but they were dropped during development
since we could not observe a noticeable effect on prediction
quality.
2.2 Dependency features (DEP)
These features are based on the assumption that
by observing how dependency parses are projected
from source to target we can gather clues concern-
ing translation quality that cannot be captured by se-
quential models. The features encode the extent to
which the edges of the projected dependency tree are
observed in reference-quality translations.
The model for DEP features is estimated on
the same set of 150K English sentences and the
corresponding reference and automatic translations,
based on the following algorithm:
1. Initialize two maps M+ and M? to store edge
counts;
2. Then, for each source sentence s: parse s with
a dependency parser;
3. Align the words of s with the reference and the
automatic translations r and a;
4. For each dependency relation ?d, sh, sm? ob-
served in the source, where d is the relation
type and sh and sm are the head and modifier
words, respectively:
(a) Identify the aligned head/modifier words
in r and a, i.e., ?rh, rm? and ?ah, am?;
(b) If rh = ah and rm = am, then incre-
ment M+?d,ah,am? by one, otherwise incre-
ment M??d,ah,am?.
In other terms, M+ keeps track of how many times
a projected dependency is the same in the automatic
and in the reference translation, while M? accounts
for the cases in which the two projections differ.
Let T be the set of dependency relations projected
on an automatic translation. In the feature space we
represent:
Coverage: The ratio of dependency edges found in
M? or M+ over the total number of projected
edges, i.e.
Coverage(T ) =
?
D?T M
+
D +M
?
D
|T |
;
C+: The quantity C+ = 1|T |
?
D?T
M+D
M+D?M
?
D
;
129
C?: The quantity C? = 1|T |
?
D?T
M?D
M+D?M
?
D
.
Intuitively, high values of C+ mean that most pro-
jected dependencies have been observed in reference
translations; conversely, high values of C? suggest
that most of the projected dependencies were only
observed in automatic translations.
Similarly to SEQ features, also in this case we ac-
tually employ three variants of these features: one in
which we use word forms (i.e., DEP/Coverage/W,
DEP/C+/W and DEP/C?/W), one in which we
look at roots (i.e., DEP/Coverage/R, DEP/C+/R
and DEP/C?/R) and one in which we only con-
sider suffixes (i.e., DEP/Coverage/S, DEP/C+/S and
DEP/C?/S).
Moreover, we also estimate C+ in the top (Q4)
and top two (Q34) fourths of edge scores, and C? in
the bottom (Q1) and bottom two (Q12) fourths. As
an example, the feature DEP/C+/Q4/R encodes the
value of C+ within the top fourth of the ranked list of
projected dependencies when only considering word
roots, while DEP/C?/W is the value of C? on the
whole edge set estimated using word forms.
3 Experiment setup
To extract the extended feature set we use an align-
ment model, a POS tagger and a dependency parser.
Concerning the former, we trained an unsupervised
model with the Berkeley aligner4, an implementa-
tion of the symmetric word-alignment model de-
scribed by Liang et al (2006). The model is trained
on Europarl and newswire data released as part of
WMT 2011 (Callison-Burch et al, 2011) training
data. For POS tagging and semantic role annota-
tion we use SVMTool5 (Jesu?s Gime?nez and Llu??s
Ma`rquez, 2004) and Swirl6 (Surdeanu and Turmo,
2005), respectively, with default configurations. To
estimate the SEQ and DEP features we use refer-
ence and automatic translations of the newswire sec-
tion of WMT 2011 training data. The automatic
translations are generated by the same configura-
tion generating the data for the quality estimation
task. The n-gram models are estimated with the
4http://code.google.com/p/berkeleyaligner
5http://www.lsi.upc.edu/?nlp/SVMTool/
6http://www.surdeanu.name/mihai/swirl/
Feature set DeltaAvg MAE
Baseline 0.4664 0.6346
Extended 0.4694 0.6248
Table 2: Comparison of the baseline and extended feature
set on development data.
SRILM toolkit 7, with order equal to 3 and Kneser-
Ney (Kneser and Ney, 1995) smoothing.
As a learning framework we resort to Support
Vector Regression (SVR) (Smola and Scho?lkopf,
2004) and learn a linear separator using the SVM-
Light optimizer by Joachims (1999)8. We represent
feature values by means of their z-scores, i.e., the
number of standard deviations that separate a value
from the average of the feature distribution. We
carry out the system development via 5-fold cross
evaluation on the 1,832 development sentences for
which we have quality assessments.
4 Evaluation
In Table 1 we show the absolute value of the Pear-
son correlation of the features used in our model,
i.e., the 17 baseline features (BL/*), the 21 sequence
(SEQ/*) and the 18 dependency (DEP/*) features,
with the human quality assessments. The more cor-
related features are in the top (left) part of the ta-
ble. At a first glance, we can see that 9 of the 10
features having highest correlation are already en-
coded by the baseline. We can also observe that
DEP features show a higher correlation than SEQ
features. This evidence seems to contradict our ini-
tial expectations, but it can be easily ascribed to the
limited size of the corpus used to estimate the n-
gram models (150K sentences). This point is also
confirmed by the fact that the three variants of the
*PStop model (based on sequences of target stop-
words interleaved by POS tags projected from the
source sentence and, hence, on a very small vocab-
ulary) are the three sequential models sporting the
highest correlation. Alas, the lack of lexical anchors
makes them less useful as predictors of translation
quality than BL/4 and BL/5. Another interesting as-
7http://www-speech.sri.com/projects/
srilm
8http://svmlight.joachims.org/
130
System DeltaAvg MAE
Baseline 0.55 0.69
Official Evaluation 0.22 0.84
Amended Evaluation 0.51 0.71
Table 3: Official and amended evaluation on test data of
the extended feature sets.
pect is that DEP/C? features show higher correlation
than DEP/C+. This is an expected behaviour, as be-
ing indicators of possible errors they are intended to
have discriminative power with respect to the human
assessments. Finally, we can see that more than 50%
of the included features, including five baseline fea-
tures, have negligible (less than 0.1) correlation with
the assessments. Even though these features may not
have predictive power per se, their combination may
be useful to learn more accurate models of quality.9
Table 2 shows a comparison of the baseline fea-
tures against the extended feature set as the average
DeltaAvg score and Mean Absolute Error (MAE) on
the 10 most accurate development configurations. In
both cases, the extended feature set results in slightly
more accurate models, even though the improve-
ment is hardly significant.
Table 3 shows the results of the official evaluation.
Our submission to the final evaluation (Official) was
plagued by a bug that affected the values of all the
baseline features on the test set. As a consequence,
the official performance of the model is extremely
poor. The row labeled Amended shows the results
that we obtained after correcting the problem. As we
can see, on both tasks the baseline outperforms our
model, even though the difference between the two
is only marginal. Ranking-wise, our official submis-
sion is last on the ranking task and last-but-one on
the quality prediction task. In contrast, the amended
model shows very similar accuracy to the baseline,
as the majority of the systems that took part in the
evaluation.
9Our experiments on development data were not signifi-
cantly affected by the presence or removal of low-correlation
features. Given the relatively small feature space, we adopted
a conservative strategy and included all the features in the final
models.
5 Discussion and conclusions
We have described the system with which we par-
ticipated in the WMT 2012 shared task on quality
estimation. The model incorporates all the base-
line features, plus two sets of novel features based
on: 1) n-gram language models estimated on mixed
sequences of target sentence words and linguistic
annotations projected from the source sentence by
means of automatic alignments; and 2) the likeli-
hood of the projection of dependency relations from
source to target.
On development data we found out that the ex-
tended feature set granted only a very marginal im-
provement with respect to the strong feature set of
the baseline. In the official evaluation, our submis-
sion was plagued by a bug affecting the generation
of baseline features for the test set, and as a result
we had an incredibly low performance. After fix-
ing the bug, re-evaluating on the test set confirmed
that the extended set of features, at least in the cur-
rent implementation, does not have the potential to
significantly improve over the baseline features. On
the contrary, the accuracy of the corrected model is
slightly lower than the baseline on both the ranking
and the quality estimation task.
During system development it was clear that im-
proving significantly over the results of the base-
line features would be very difficult. In our expe-
rience, this is especially due to the presence among
the baseline features of extremely strong predictors
of translation quality such as the perplexity of the
automatic translation. We could also observe that
the parametrization of the learning algorithm had
a much stronger impact on the final accuracy than
the inclusion/exclusion of specific features from the
model.
We believe that the information that we encode,
and in particular dependency parses and stop-word
sequences, has the potential to be quite relevant for
this task. On the other hand, it may be necessary to
estimate the models on much larger datasets in order
to compensate for their inherent sparsity. Further-
more, more refined methods may be required in or-
der to incorporate the relevant information in a more
determinant way.
131
Acknowledgments
This research has been partially funded by
the Spanish Ministry of Education and Science
(OpenMT-2, TIN2009-14675-C03) and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement numbers
247762 (FAUST project, FP7-ICT-2009-4-247762)
and 247914 (MOLTO project, FP7-ICT-2009-4-
247914).
References
[Callison-Burch et al2007] Chris Callison-Burch,
Philipp Koehn, Cameron Shaw Fordyce, and Christof
Monz, editors. 2007. Proceedings of the Second
Workshop on Statistical Machine Translation. ACL,
Prague, Czech Republic.
[Callison-Burch et al2011] Chris Callison-Burch,
Philipp Koehn, Christof Monz, and Omar F. Zaidan,
editors. 2011. Proceedings of the Sixth Workshop
on Statistical Machine Translation. Association for
Computational Linguistics, Edinburgh, Scotland, July.
[Callison-Burch et al2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
[Jesu?s Gime?nez and Llu??s Ma`rquez2004] Jesu?s Gime?nez
and Llu??s Ma`rquez. 2004. SVMTool: A general POS
tagger generator based on Support Vector Machines.
In Proceedings of the 4th LREC.
[Joachims1999] Thorsten Joachims. 1999. Making large-
Scale SVM Learning Practical. In B. Scho?lkopf,
C. Burges, and A. Smola, editors, Advances in Kernel
Methods - Support Vector Learning.
[Kneser and Ney1995] Reinhard Kneser and Hermann
Ney. 1995. Improved backing-off for m-gram lan-
guage modeling. In In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, volume I, pages 181?184, Detroit, Michi-
gan, May.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Liang et al2006] Percy Liang, Benjamin Taskar, and
Dan Klein. 2006. Alignment by agreement. In HLT-
NAACL.
[Smola and Scho?lkopf2004] Alex J. Smola and Bernhard
Scho?lkopf. 2004. A tutorial on support vector regres-
sion. Statistics and Computing, 14(3):199?222, Au-
gust.
[Surdeanu and Turmo2005] Mihai Surdeanu and Jordi
Turmo. 2005. Semantic Role Labeling Using Com-
plete Syntactic Analysis. In Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 221?224, Ann
Arbor, Michigan, June.
132
