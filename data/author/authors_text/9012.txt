  
A Unicode based Adaptive Segmentor 
Q. Lu, S. T. Chan, R. F. Xu, T. S. Chiu 
Dept. Of Computing, 
The Hong Kong Polytechnic University, 
Hung Hom, Hong Kong 
{csluqin,csrfxu}@comp.polyu.edu.hk 
B. L. Li, S. W. Yu 
The Institute of Computational Linguistics, 
Peking University, 
Beijing, China 
{libi,yusw}@pku.edu.cn 
 
Abstract 
This paper presents a Unicode based 
Chinese word segmentor. It can handle 
Chinese text in Simplified, Traditional, or 
mixed mode. The system uses the strategy 
of divide-and-conquer to handle the 
recognition of personal names, numbers, 
time and numerical values, etc in the pre-
processing stage. The segmentor further 
uses tagging information to work on 
disambiguation. Adopting a modular 
design approach, different functional parts 
are separately implemented using 
different modules and each module 
tackles one problem at a time providing 
more flexibility and extensibility. Results 
show that with added pre-processing 
modules and accessorial modules, the 
accuracy of the segmentor is increased 
and the system is easily adaptive to 
different applications. 
1 Introduction 
The most difficult problem in Chinese word 
segmentation is due to overlapping ambiguities [1-
2]. The recognition of names, foreign names, and 
organizations are quite unique for Chinese. Some 
systems can already achieve very high accuracy [3], 
but they heavily rely on manual work in getting the 
system to be trained to work certain language 
environment. However, for many applications, we 
need to look at the cost to achieve high accuracy. 
In a competitive environment, we also need to 
have systems that are quickly adaptive to new 
requirements with limited resources available. 
In this paper, we report a Unicode based Chinese 
word segmentor. The segmentor can handle 
Chinese text in Simplified, Traditional, or mixed 
mode where internally only one dictionary is 
needed. The system uses the strategy of divide-
and-conquer to handle the recognition of personal 
names, numbers, time and numerical values. The 
system has a built-in new word extractor that can 
extract new words from running text, thus save 
time on training and getting the system quickly 
adaptive to new language environment. The 
Bakeoff results in the open text for our system in 
all categories have shown that it works reasonably 
good for all different corpora. 
The rest of the paper is organized as follows. 
Section 2 presents our system design objectives 
and components. Section 3 discusses more 
implementation details. Section 4 gives some 
performance evaluations. Section 5 is the 
conclusion. 
2 Design Objectives and Components 
With the wide use of Unicode based operating 
systems such as Window 2000 and Window XP, 
we now see more and more text data written in 
both the Simplified form and the Traditional form 
to co-exist on the same system. It is also likely that 
text written in mixed mode. Because of this reality, 
the first design objective of this system is its ability 
to handle the segmentation of Chinese text written 
in either Simplified Chinese, Traditional Chinese, 
or mixed mode.  As an example, we should be able 
to segment the same sentence in different forms 
such as the example given below:   
 
The second design objective is to adopt the 
modular design approach where different 
functional parts are separately implemented using 
independent modules and each module tackles one 
problem at a time. Using this modular approach, 
we can isolate problems and fine tune each module 
with minimal effect on other modules in the system. 
  
Special features like adding new rules or new 
dictionary can be easily done without affecting 
other modules. Consequently, the system is more 
flexible and can be easily extended.  
The third design objective of the system is to make 
the segmentor adaptive to different application 
domains. We consider it having more practical 
value if the segmentor can be easily trained using 
some semi-automatic process to work in different 
domains and work well for text with different 
regional variations. We consider it essential that 
the segmentor has tools to help it to obtain regional 
related information quickly even if annotated 
corpora are not available. For instance, when it 
runs text from Hong Kong, it must be able to 
recognize the personal names such as  if 
such a name(quadra-gram) appears in the text often. 
 
Figure 1. System components 
Figure 1 shows the two major components, the 
segmentor and data manager. The segmentor is the 
core component of the system. It has a pre-
processor, the kernel, and a post-processor. As the 
system has to maintain a number of tables such as 
the dictionaries, family name list, etc., a separate 
component called data manager is responsible in 
handling the maintenance of these data.  The pre-
processor has separate modules to handle 
paragraphs, ASCII code, numbers, time, and 
proper names including personal names, place and 
organizational names, and foreign names. The 
kernel supports different segmentation algorithms. 
It is the application or user?s choice to invoke the 
preferred segmentation algorithms that at current 
time include the basic maximum matching and 
minimum matching in both forward and backward 
mode. These can also be used to build more 
complicated algorithms later on. In addition, the 
system provides segmentation using part-of-speech 
tagging information to help resolve ambiguity. The 
post-processor applies morphological rules which 
cannot be easily applied using a dictionary.   
The data manager helps to maintain the knowledge 
base in the system. It also has an accessory 
software called the new word extractor which can 
collect statistical information based on character 
bi-grams, tri-grams and quadra-grams to semi-
automatically extract words and names so that they 
can be used by the segmentor to improve 
performance especially when switching to a new 
domain. Another characteristic of this segmentor is 
that it provides tagging information for segmented 
text. The tagging information can be optionally 
omitted if not needed by an application. 
3 Implementation Details 
The basic dictionary of this system was provided 
by Peking University [4] and we also used the 
tagging data from [4]. The data structure for our 
dictionaries are very similar to that discussed in [5]. 
As our program needs to handle both Simplified 
and Traditional Chinese characters, Unicode is the 
only solution for dealing with more than one script 
at the same time. 
Even though it is our design objective to support 
both Simplified and Traditional Chinese, we do not 
want to keep two different sets of dictionaries for 
Simplified and Traditional Chinese. Even if two 
versions are kept, it would not serve well for text 
in mixed mode. For example, Traditional Chinese 
word of ?the day after tomorrow? should be , 
and for Simplified Chinese, it should be . 
However sometimes we can see the word  
appears in a Traditional Chinese text. We cannot 
say that it is wrong because the sentence is still 
semantically correct especially in Unicode 
environment. Therefore the segmentor should be 
able to segment those words correctly such as in 
the examples: ? ?, and in ?  
?. We must also deal with dictionary 
maintenance related to Chinese variants. For 
example, characters  are variants, so are 
. 
Data manager Segmentor 
Pre-
Processor 
Kernel 
Post 
Processor 
New Word
Extractor
Knowledge-
base 
  
In order to keep the dictionary maintenance simple, 
our system uses a single dictionary which only 
keeps the so called canonical form of a word. In 
our system, the canonical form of a word is its 
?simplified form?.  We quoted the word 
?simplified? because only certain characters have 
simplified forms such as  to , but for  
, there is no simplified form. In the case of 
variants, we simply choose one of them as the 
canonical character.  The canonical characters are 
maintained in the traditional-simplified character 
conversion table as well as in a variant table.  
Whenever a new word, item, is added into the 
dictionary, it must be added using a function 
CanonicalConversion(), which takes item as an 
input. During segmentation, the corresponding 
dictionary look up function will first convert the 
token to its canonical form before looking up in the 
dictionary.  
The personal name recognizers (separate for 
Chinese names and foreign names) use the 
maximum-likelihood algorithm with consideration 
of commonly used Chinese family names, given 
names, and foreign name characters. It works for 
Chinese names of length up to 5 characters. In the 
following examples you can see that our system 
successfully recognized the name . This 
is done using our algorithm, not by putting her 
name in our dictionary: 
 
Organization names and place names are 
recognized mainly using special purpose 
dictionaries. The segmentor uses tagging 
information to help resolve ambiguity. The 
disambiguation is mostly based on rules such as  
p + (n + f) -> p + n + f 
which would word to correct 
  
For efficiency reasons, our system uses only about 
20 rules. The system is flexible enough for new 
rules to be added to improve performance.  
The new word extractor is an accessory program to 
extract new words from running text based on 
statistical data which can either be grabbed from 
the internet or collected from other sources. The 
basic statistical data include bi-gram frequency, tri-
gram frequency, and quadra-gram frequencies. In 
order to further example whether a bi-gram, say  
, is indeed a word, we further collect forward 
conditional frequency of  , and 
the back-ward conditional frequency of , 
. For an i-gram token, we also 
use the (i+1)-gram statistics to eliminate those i-
grams that are only a part of (i+1) ? gram word.  
For instance, if the frequency of bi-gram  is 
very close to the frequency of tri-gram , it 
is less likely that  is a word. Of course, 
whether  is a word depends on quadra-gram 
results.  Using the statistical result, a set of rules 
was applied to these i-grams to eliminate entries 
that are not considered new words. Minimal 
manual work is required to identify whether the 
remaining candidates are new words. Before words 
are added into the dictionary, part-of-speech 
information are added manually (although not 
necessary) before using the canonical function. 
The following table shows examples of bi-grams 
which are found by the new word extractor using 
one year Hong Kong Commercial Daily News data. 
 
 
 
4 Performance Evaluation 
The valuation metrics used in [6] were adopted 
here. 
1
3
N
N
recall =     (1) 
 
2
3
N
Npresicion =     (2) 
  
precisionrecall
precisionrecallrecallprecisionF +
??= 2),(1   (3) 
where N  1  denotes the number of words in the 
annotated corpus, N 2 denotes the number of words 
identified by the segmentation algorithm , and N 3 is 
the number of words correctly identified. 
We participated in the open tests for all four 
corpora. The results are shown in the following 
table. 
The worst performance in the 4 tests were for the 
CTB(UPenn) data. From the observation from the 
testing data, we found that the main problem with 
have with CTB data is the difference in word 
granularity. To confirm our observation, we have 
done an analysis of combining errors and 
overlapping errors. The results show that the ratios 
of combining errors in all the error types are 
0.8425(AS), 0.87684(CTB), 0.82085(HK), and 
0.77102(PK). The biggest problem we have with 
AS data, on the other hand is due to out of 
vocabulary mistakes. Even though our new word 
extractor can help us to reduce this problem, but 
we have not trained our system using data from 
Taiwan.  Our best performance was on PK data 
because we used a very similar dictionary. The 
additional training of data for HK was done using 
one year Commercial Daily( ). 
The following table summarizes the execution 
speed of our program for the 4 different 
sources: 
Data No. of 
chars 
Processi
ng Time 
(sec.) 
Processin
g Rate 
(char/sec) 
Segmentat
ion Rate 
(char/sec) 
AS 18,743 4.703 3,985 7,641 
CTB 62,332 10.110 6,165 7,930 
HK 57,432 10.329 5,560 7,109 
PK 28,458 4.829 5,893 10,970 
 
The program initialization needs around 2.25 
seconds mainly to load the dictionaries and other 
data into the memory before the segmentation can 
start. If we only count the segmentation time, the 
rate of segmentation on the average is around 
7,500 characters for the first three corpora. It 
seems that the processing speed for Peking U. data 
is faster. This may be because the dictionaries we 
used are closer to the PK system, thus it would 
take less time to work on disambiguation.  
5 Conclusion 
In this paper, design and algorithms of a general-
purposed Unicode based segmentor is proposed. It 
is able to process Simplified and Traditional 
Chinese appear in the same text. Sophisticated pre-
processing and other auxiliary modules help 
segmenting text more accurately. User interactions 
and modules can be easily added with the help of 
its modular design. A built-in new word extractor 
is also implemented for extracting new words from 
running text. It saves much time on training and 
thus it can be quickly adapted to new environments. 
Acknowledgement 
We thank the PI of ITF Grant by ITC of 
HKSARG (ITS/024/01) entitled: Towards Cost-
Effective E-business in the News Media & 
Publishing Industry for the use of HK Commercial 
Daily. 
References 
[1] Automatic Segmentation and Tagging for Chinese 
Text ( ) , K.Y. Liu, 
Commercial Press, 2000 
[2] Segmentation Issues in Chinese Information 
Processing,  (C.N. Huang Issue No. 
1, 1997) 
[3] The design and Implementation of a Modern General 
Purpose Segmentation System (B. Lou,  R. Song, W.L. 
Li, and Z.Y. Luo, Journal of Chinese Information 
Processing, Issue No. 5, 2001) 
[4] (Institute of 
Computational Linguistics, Peking Univ., 2002) 
[5] 
  Journal of Chinese information 
processing vol. 14, no. 1, 2001) 
[6] Chinese Word Segmentation and Information 
Retrieval, Palmer D., and Burger J., In AAAI 
Symposium Cross-Language Text and Speech 
Retrieval 1997 
  
Using Synonym Relations In Chinese Collocation Extraction 
Wanyin Li 
Department of Computing,  
The Hong Kong Polytechnic University,  
Hung Hom, Kowloon, Hong Kong 
cswyli@comp.polyu.edu.hk 
Qin Lu 
Department of Computing,  
The Hong Kong Polytechnic University,  
Hung Hom, Kowloon, Hong Kong 
csluqin @comp.polyu.edu.hk 
Ruifeng Xu 
Department of Computing, The Hong Kong Polytechnic University,  
Hung Hom, Kowloon, Hong Kong 
csrfxu@comp.polyu.edu.hk 
 
 
Abstract 
A challenging task in Chinese collocation 
extraction is to improve both the precision and 
recall rate. Most lexical statistical methods 
including Xtract face the problem of unable to 
extract  collocations with lower frequencies than 
a given threshold. This paper presents a method 
where HowNet is used to find synonyms using a 
similarity function. Based on such synonym 
information, we have successfully extracted 
synonymous collocations which normally cannot 
be extracted using the lexical statistical 
approach. We applied synonyms mapping to 
each headword to extract more synonymous 
word bi-grams. Our evaluation over 60MB 
tagged corpus shows that we can extract 
synonymous collocations that occur with very 
low frequency, sometimes even for collocations 
that occur only once in the training set. 
Comparing to a collocation extraction system 
based on Xtract, we have reached the precision 
rate of 43% on word bi-grams for a set of 9 
headwords, almost 50% improvement from 
precision rate of 30% in the Xtract system. 
Furthermore, it  improves the recall rate of word 
bi-gram collocation extraction by 30%. 
1 Introduction 
A Chinese collocation is a recurrent and 
conventional expression of words which  holds 
syntactic and semantic relations.  A widely adopted 
definition given by Benson (Benson 1990) stated 
that ?a collocation is an arbitrary and recurrent 
word combination.?  For example, we say ?warm 
greetings? rather than ?hot greetings?, ?broad 
daylight? rather than ?bright daylight?.  Similarly, 
in Chinese ? ? ? ? ? ? are three nouns 
with similar meanings, however, we  say 
? ? rather than ? ?, 
? ?rather than ? ?.   
 
Study in collocation extraction using lexical 
statistics has gained some insights to the issues 
faced in collocation extraction (Church and Hanks 
1990, Smadja 1993, Choueka 1993, Lin 1998). As 
the lexical statistical approach is developed based 
on the ?recurrence? property of collocations, only 
collocations with reasonably good recurrence can 
be extracted. Collocations with low occurrence 
frequency cannot be extracted, thus affecting the 
recall rate. The precision rate using the lexical 
statistics approach can reach around 60% if both 
word bi-gram extraction and n-gram extractions 
are taking into account (Smadja 1993, Lin 1997 
and Lu et al 2003). The low precision rate is 
mainly due to the low precision rate of word bi-
gram extractions as only about 30% - 40% 
precision rate can be achieved for word bi-grams.  
In this paper, we propose a different approach to 
find collocations with low recurrences. The main 
idea is to make use of synonym relations to extract 
synonymous collocations. Lin (Lin 1997) 
described a distributional hypothesis that if two 
words have similar set of collocations, they are 
probably similar. In HowNet, Liu Qun (Liu et al 
2002) defined the word similarity as two words 
that can substitute each other in the context and 
keep the sentence consistent in syntax and 
semantic structure. That means, naturally, two 
similar words are very close to each other and they 
can be used in place of the other in certain context. 
For example, we may either say  ? ?or ? ? 
as  and are semantically close to each 
other. We apply this lexical phenomenal after the 
lexical statistics based extractor to find the low 
frequency synonymous collocations, thus 
increasing recall rate.  
 
  
  The rest of this paper is organized as follows. 
Section 2 describes related existing collocation 
extraction techniques based on both lexical 
statistics and synonymous collocation. Section 3 
describes our approach on collocation extraction. 
Section 4 evaluates the proposed method. Section 5 
draws our conclusion and presents possible future  
work. 
2 Related Work 
Methods have proposed to extract collocations 
based on lexical statistics. Choueka (Choueka 
1993) applied quantitative selection criteria based 
on frequency threshold to extract adjacent n-grams 
(including bi-grams). Church and Hanks (Church 
and Hanks 1990) employed mutual information to 
extract both adjacent and distant bi-grams that tend 
to co-occur within a fixed-size window. But the 
method did not extend to extract n-grams. Smadja 
(Smadja 1993) proposed a statistical model by 
measuring the spread of the distribution of co-
occurring pairs of words with higher strength. This 
method successfully extracted both adjacent and 
distant bi-grams and n-grams. However, the 
method failed to extract bi-grams with lower 
frequency. The precision rate on bi-grams 
collocation is very low, only around high 20% and 
low 30%. Even though, it is difficult to measure 
recall rate in collocation extraction (almost no 
report on recall estimation), It is understood that 
low occurrence collocations cannot be extracted. 
Our research group has further applied the Xtract 
system to Chinese (Lu et al 2003) by adjusting the 
parameters to optimize the algorithm for Chinese 
and a new weighted algorithm was developed 
based on mutual information to acquire word bi-
grams with one higher frequency word and one 
lower frequency word. The result has achieved an 
estimated 5% improvement in recall rate and a 
15% improvement in precision comparing to the 
Xtract system. 
All of the above techniques do not take 
advantage of the wide range of lexical resources 
available including synonym information. Pearce 
(Pearce 2001) presented a collocation extraction 
technique that relies on a mapping from a word to 
its synonyms for each of its senses. The underlying 
intuitions is that if the difference between the 
occurrence counts of one synonyms pair with 
respect to a particular word was at least two, then 
this was deemed sufficient to consider them as a 
collocation. To apply this approach, knowledge in 
word (concept) semantics and relations to other 
words must be available such as the use of 
WordNet. Dagan (Dagan 1997) applied similarity-
based smoothing method to solve the problem of 
data sparseness in statistical natural language 
processing. The experiments conducted in his later 
works showed that this method achieved much 
better results than back-off smoothing methods in 
word sense disambiguation. Similarly, Hua Wu 
(Wu and Zhou 2003) applied synonyms 
relationship between two different languages to 
automatically acquire English synonymous 
collocation. This is the first time that the concept 
synonymous collocation is proposed. A side 
intuition raised here is that nature language is full 
of synonymous collocations. As many of them 
have low occurrences, they are failed to be 
retrieved by lexical statistical methods. Even 
though there are Chinese synonym dictionaries, 
such as  ( Tong Yi Ci Lin), the 
dictionaries lack structured knowledge and 
synonyms are too loosely defined to be used for 
collocation extraction.  
HowNet developed by Dong et al(Dong and 
Dong 1999) is the best publicly available resource 
on Chinese semantics. By making use of semantic 
similarities of words, synonyms can be defined by 
the closeness of their related concepts and the 
closeness can be calculated. In Section 3, we 
present our method to extract synonyms from 
HowNet and using synonym relations to further 
extract collocations. 
Sun (Sun 1997) did a preliminary Quantitative 
analysis on Chinese collocations based on their 
arbitrariness, recurrence and the syntax structure. 
The purpose of this study is to help differentiate if 
a collocation is true or not according to the 
quantitative factors. By observing the existence of  
synonyms information in natural language use, we 
consider it possible to identify different types of 
collocations using more semantic and syntactic 
information available. We discuss the basic ideas 
in section 5.. 
3 Our Approach 
Our method of extracting Chinese collocations 
consists of three steps.  
Step 1: Take the output of any lexical statistical 
algorithm which extracts word bi-gram 
collocations. The data is then sorted 
according to each headword , Wh, with its co-
word, Wc, listed. 
Step 2: For each headword Wh used to extract bi-
grams, we acquire its synonyms based on a 
similarity function using HowNet. Any word 
in HowNet having similarity value over a 
threshold value is chosen as a synonym 
headword Ws for additional extractions. 
Step 3: For each synonym headword, Ws, and the 
co-word Wc of Wh, as its synonym, if the bi-
gram (Ws , Wc) is not in the output of the 
  
lexical statistical algorithm in Step one, take 
this bi-gram (Ws , Wc) as a collocation if the 
pair co-occurs in the corpus by additional 
search to the corpus. 
3.1 Structure of HowNet 
Different from WordNet or other synonyms 
dictionary, HowNet describes words as a set of 
concepts  and each concept is described by a 
set of primitives . The following lists for the 
word , one of its corresponding concepts 
 
 
In the above record, DEF is where the primitives 
are specified. DEF contains up to four types  of 
primitives: the basic independent primitive   
, the other independent 
primitive , the relation primitive  
, and the symbol primitive , 
where the basic independent primitive and the 
other independent primitive are used to indicate the 
semantics of a concept and the others are used to 
indicate syntactical relationships. The similarity 
model described in the next subsection will 
consider both of these relationships.  
The primitives are linked by a hierarchical tree 
to indicate the parent-child relationships of the 
primitives as shown in the following example:  
 
 
 
This hierarchical structure provides a way to link 
one concept with any other concept in HowNet, 
and the closeness of concepts can be simulated by 
the distance between two concepts. 
3.2 Similarity Model Based on HowNet 
Liu Qun (Liu 2002) defined word similarity as 
two words which can substitute each other in the 
same context and still maintain the sentence 
consistent syntactically and semantically. This is 
very close to our definition of synonyms. Thus we 
directly used their similarity function, which is 
stated as follows.  
A word in HowNet is defined as a set of 
concepts and each concept is represented by 
primitives.  Thus, HowNet can be described by W, 
a collection of n words, as: 
 W = { w1, w2, ? wn}Each word wi is, in 
turn, described by a set of concepts S as:  
 Wi = { Si1, Si2,?Six}, 
And, each concept Si  is, in turn, described by a 
set of primitives: 
 Si  = { pi1, pi2 ?piy } 
For each word pair, w1 and w2, the similarity 
function is defined by 
  )1(),(max),( 21...1,..121 jimjni SSSimwwSim ===     
where S1i is the list of concepts associated with W1 
and S2j is the list of concepts associated with W2.  
As any concept Si is presented by its primitives, 
the similarity of primitives for any p1, and  p2 of 
the same type, can be expressed by the following 
formula: 
 ?
?
+= ),(),( 2121 ppDis
ppSim     (2) 
where ?  is an adjustable parameter set to 1.6, 
and ),( 21 ppDis is the path length between p1 and 
p2 based on the semantic tree structure. The above 
formula where ? is a constant does not indicate 
explicitly the fact that the depth of a pair of nodes 
in the tree affects their similarity. For two pairs of 
nodes (p1 ,  p2) and  (p3 ,  p4) with the same distance,  
the deeper the depth is, the more commonly shared 
ancestros they would have which should be 
semantically closer to each other. In following two 
tree structures, the pair of nodes (p1, p2) in the left 
tree should be more similar than (p3 ,  p4)  in the 
right tree. 
 
                root 
 
 
 
 
 
                             p2 
 
             p1 
                      
                     root 
 
 
 
 
 
                          P3 
 
 
                    P4 
 
 
  
 
To indicate this observation,  ?  is modified as a 
function of tree depths of the nodes using the 
formula  ? =min(d(p1), d(p2)) . Consequently, the 
formula (2) is rewritten as formular (2?) during the 
experiment. 
 
))(),(min(),(
))(),(min(),(
2121
21
21 pdpdppDis
pdpdppSim +=
    (2?) 
 
where d(pi) is the depth of node pi  in the tree . The 
comparison of calculating the word similarity by 
applying the formula (2) and  (2?) is shown in 
Section 4.4. 
 
 Based on the DEF description in HowNet, 
different primitive types play different roles only 
some are directly related to semantics. To make 
use of both the semantic and syntactic information 
included in HowNet to describe a word, the 
similarity of two concepts should take into 
consideration of all primitive types with weighted 
considerations and thus the formula is defined as 
)3(),(),( 21
1
4
1
21 jj
i
j
j
i
i ppSimSSSim ??
==
= ?   
where ?i is a weighting factor given in (Liu 
2002) with the sum of ?1 + ?2 + ?3 + ?4 being 1? 
and ?1 ? ?2 ? ?3 ? ?4. The distribution of the 
weighting factors is given for each concept a priori 
in HowNet to indicate the importance of primitive 
pi in defining the corresponding concept S. 
 
3.3 Collocation Extraction 
In order to extract collocations from a corpus, 
and to obtain result for Step 1 of our algorithm, we 
used the collocation extraction algorithm 
developed by the research group at the Hong Kong 
Polytechnic University(Lu et al 2003). The 
extraction of bi-gram collocation is based on the 
English Xtract(Smaja 1993) with improvements. 
Based on the three Steps mentioned earlier, we will 
present the extractions in each step in the 
subsections. 
 
3.3.1 Bi-gram Extraction 
Based on the lexical statistical model proposed 
by Smadja in Xtract on extracting English 
collocations, an improved algorithm was 
developed for Chinese collocation by our research 
group and the system is called CXtract. For easy of 
understanding, we will explain the algorithm 
briefly here. According to Xtract, word 
cooccurence is denoted by a tripplet (wh, wi, d)  
where wh is a given headword, wi is a co-word 
appeared in the corpus in a distance d within the 
window of [-5, 5]. The frequency fi of the co-word   
wi   in the window of [-5, 5] is defined as: 
?
?=
=
5
5
,
j
jii ff    (4) 
where  fi, j   is the frequency of the co-word at distance 
j in the corpus within the window. The average 
frequency of  fi , denoted by if , is given by 
10/
5
5
,?
?=
=
j
jii ff    (5) 
Then, the average frequency f , and the standard 
deviation ? are defined by 
?
=
=
n
i
ifn
f
1
1
;  2
1
)(1?
=
?=
n
i
i ffn
?  (6) 
The Strength of the co-occurrence for the pair 
(wh, wi,), denoted by ki, is defined by 
?
ffk ii
?= ?   (7) 
Furthermore, the Spread of (wh, wi,),, denoted as 
Ui, which characterizes the distribution of  wi 
around  wh is define as: 
10
)( 2,? ?
=
iji
i
ff
U ;    (8) 
To eliminate the bi-grams with unlikely co-
occurrence, the following sets of threshold values 
is defined: 
0:1 K
ff
kC ii ??= ?    (9) 
0:2 UUC i ?     (10) 
)(:3 1, iiji UKffC ?+?   (11) 
However, the above statistical model given by 
Smadja fails to extract the bi-grams with a much 
higher frequency of wh but a relatively low 
frequency word of wi,,  For example,  in the bi-
gram , freq ( ) is much lower than the 
freq ( ). Therefore, we further defined a 
weighted mutual information to extract this kind of 
bi-grams: 
  ,
)(
),w(
0
h R
wf
wfR
i
i
i ?=      (12)  
As a result, the system should return a list of 
triplets (wh, wi, d), where  (wh, wi,) is considered 
collocations.  
  
3.3.2 Synonyms Set 
For each given headword wh, before taking it as 
an input to extract its bi-grams directly, we fist 
apply the similarity formula described in Equation 
(1) to generate a set of synonyms headwords Wsyn: 
 
   }),(:{ ?>= shssyn wwSimwW                             (13) 
Where 0 <? <1 is an algorithm parameter which 
is adjusted based on experience. We set it as 0.85 
from the experiment because we would like to 
balance the strength of the synonyms relationship 
and the coverage of the synonyms set. The setting 
of the parameter ? < 0.85 weaks the similarity 
strength of the extracted synonyms. For example, 
for a given collocation ? ?, that is unlikely 
to include the candidates ? ?, ? ?, 
? ?.  On the other hand, by setting the 
parameter ? > 0.85 will limit the coverage of the 
synonyms set and hence lose valuable synonyms. 
For example, for a given bi-gram ? ?, we 
hope to include the candidate synonymous 
collocations such as  ? ?, ? ?, 
? ?. We will show the test of ?  in the 
section 4.2. 
This synonyms headwords set provides the 
possibility to extract the synonymous collocation 
with the lower frequency that failed to be extracted 
by lexical statistic. 
3.3.3 Synonymous Collocations 
A phenomenal among the collocations in natural 
language is that there are many synonymous 
collocations exist. For example, ?switch on light? 
and ?turn on light?, ? ? and ? ?. 
Due to the domain specification of the corpus, 
some of the synonymous collocations may fail to 
be extracted by the lexical statistic model because 
of their lower frequency. Based on this 
observation, this paper takes a further step. The 
basic idea is for a bi-gram collocation (wh, wc, d ) 
we select the synonyms ws of wh with the 
maximum similarity respect to all the concepts 
contained by wh, we deem (ws, wc, d ) as a 
collocation if its occurrence is greater than 1 in the 
corpus. There are similar works discussed by 
Pearce (Pearce 2001). . 
For a given collocation (ws, wc,, d), if ws ? Wsyn, 
then we deem the triple (ws, wc,, d) as a 
synonymous collocation with respect to the 
collocation (wh, wc,, d) if the co-occurrence of (ws, 
wc, , d) in the corpus is greater than one. Therefore, 
we define the collection of synonymous 
collocations Csyn as: 
}1),,(:),,{( >= dwwFreqdwwC cscssyn           (14) 
where  ws ? Wsyn. 
4 Evaluation 
The performance of collocation is normally 
evaluated by precision and recall as defined below. 
nsCollocatioextractedofnumbertotal
nsCollocatioExtractedcorrectofnumberprecision
    
    = (15) 
nsCollocatioactualofnumbertotal
nsCollocatioExtractedcorrectofnumberrecall
    
    =  (16) 
To evaluate the performance of our approach, we 
conducted a set of experiments based on 9 selected 
headwords. A baseline system using only lexical 
statistics given in 3.3.1 is used to get a set of 
baseline data called Set A. The output using our 
algorithm is called Set B. Results are checked by 
hand for validation on what is true collocation and 
what is not a true collocation. 
 
Table 1. Sample table for the true collocation 
with headword ? ?  
 
Table 2. Sample table for the bi-grams that are 
not true collocations  
  
Table 1 shows samples of extracted word bi-grams 
using our algorithm that are considered 
synonymous collocations for the headword ? ?. 
Table 2 shows extracted bi-grams by our algorithm 
that are not considered true collocations. 
4.1 Test Set 
Our experiment is based on a corpus of six 
months tagged People Daily with 11 millions 
number of words. For word bi-gram extractions, 
we consider only content words, thus headwords 
are selected from noun, verb and adjective only. 
For evaluation purpose, we selected randomly 3 
nouns, 3 verbs and 3 adjectives with frequency of 
low, medium and high. Thus, in Step 1 of the 
algorithm, 9 headwords were  used to extract bi-
gram collocations from the corpus, and 253 pairs 
of collocations were extracted. Evaluation by hand 
has identified 77 true collocations in Set A test set. 
The overall precision rate is 30% (see Table 3).  
 
 Noun+Verb
+Adjective 
Headword 9 
Extracted Bi-grams 253 
True collocations using 
lexical statistics only 
77 
Precision rate 30% 
 Table 3: Statistics in test set for set A 
 
Using Step 2 of our algorithm, where ?=0.85 is 
used, we have obtained 55 synonym headwords 
(include the 9 headwords). Out of these 55 
synonyms, 614 bi-gram pairs were then extracted 
from the lexical statistics based algorithm, in 
which 179 are consider true collocations. Then, by 
applying Step 3 of our algorithm, we extracted an 
additional 201 bi-gram pairs, among them, 178 are 
considered true collocations. Therefore, using our 
algorithm, the overall precision rate has achieved 
43%, an improvement of almost 50%. The data is 
summarized in Table 4. 
 n., v, and adj. 
Synonyms headword 55 
Bi-grams (lexical statistics) 614 
Non-synonym collocations 
(lexical statistics only) 
179 
Extracted synonym 
collocations Step 2 
201 
True synonym collocations 
using Step 2 
178 
Overall precision rate 43% 
Table 4: Statistics in test set for mode B 
4.2 The choice of ? 
We also conducted a set of experiments to 
choose the best value for the similarity function?s 
threshold ?. We tested the best value of ? with both 
the precision rate and the estimated recall rate 
using the so called remainder bi-grams. The 
remainder bi-grams is the total number of bi-grams 
extracted by the algorithm. When precision goes 
up, the size of the result is smaller, which in a way 
is an indicator of less recalled collocations. Figure 
1 shows the precision rate and the estimated recall 
rate in testing the value of ?. 
 
Figure 1. Precision Rate vs. value of ? 
From Figure 1, it is obvious that at ?=0.85 the 
recall rate starts to drop more drastically without 
much incentive for precision. 
 
 Extracted Bi-
grams using 
lexical 
statistics 
Extracted 
Synonyms 
Collocations 
using Step 2 
(1.2,1.4,12) 465 328 
(1.4,1.4,12) 457 304 
(1.4,1.6,12) 394 288 
(1.2,1.2,12) 513 382 
(1.2,1.2,14) 503 407 
(1.2,1.2,16) 481 413 
      Table 5: Value of (K0, K1, U0). 
4.3 The test of (K0, K1, U0) 
The original threshold for CXtract is (1.2, 1.2, 12) 
for the parameters (K0, K1, U0). However, with 
synonyms collocations, we have also conducted 
some experiments to see whether the parameters 
should be adjusted. Table 5 shows the statistics to 
test the value of (K0, K1, U0). The similarity 
threshold ? was fixed at 0.85 throughout the 
experiments. 
  
The experimental shows that varying the value of 
(k0, k1) does not bring any benefit to our algorithm. 
However, increasing the value of u0 did improve 
the extraction of synonymous collocations. Figure 
2 shows that U0 =14 is a good trade-off for the 
precision rate and the remainder Bi-grams. The basic 
meaning behind the result is reasonable. According to 
Smadja, U0 defined in the formula (8) represents the 
co-occurrence distribution of the candidate 
collocation (wh, wc) in the position of d (-5 ? d ? 
5). For a true collocation (wh, wc,, d), its co-
occurrence  in the position d is much higher than in 
other positions which leads to a peak in the co-
occurrence distribution. Therefore, it is selected by 
the statistical algorithm based on the formula (10). 
Based on the physical meaning behind, one way to 
improve the precision rate is to increase the value of  
the threshold U0.  A side effect to an increased  value 
of U0  is that the recall is decreased because some  
true collocations do not meet the condition of co-
occurrence greater than U0. Step 2 of the new 
algorithm regains some  true collocations lost 
because of a higher U0. in Step 1.  
               Figure 2. Precision Rate vs. Value of U0 
 
4.4 The comparison of similarity calculation 
based on formula  (2) and (2?) 
Table 6 shows the similarity value given by 
formula (2) where ?  is a constant given the value 
1.6 and by formula (2?) where ? is replaced by a 
function of the depths of the nodes. Results show 
that (2?) is more fine tuned and reflects the nature 
of the data better. For example, and 
are more similar than and . 
 and are much similar but not the same. 
 
 
Table 6: comparison of similarity calculation 
5 Conclusion and Further Work 
In this paper, we have presented a method to 
extract bi-gram collocations using lexical statistics 
model with synonyms information. Our method 
reaches the precision rate of 43% for the tested data. 
Comparing to the precision of 30% using lexical 
statistics only, our improvement is close to 50%. In 
additional, the recall improved 30%. The contribution 
is that we have made use of synonym information 
which is plentiful in the natural language use and it 
works well to supplement the shortcomings of lexical 
statistical method.  
Manning claimed that the lack of valid 
substitution for a synonym is a characteristics of 
collocations in general (Manning and Schutze 
1999). To extend our work, we consider the use of 
synonym information can be further applied to 
help identify collocations of different types.  
Our preliminary study has suggested that 
collocation can be classified into 4 types:   
Type 0 Collocation:  Fully fixed collocation 
which include some idioms, proverbs and sayings 
such as ? ? ? ? and so on.  
Type 1 Collocation:  Fixed collocation in which 
the appearance of one word implies the co-
occurrence of another one such as ? ?.  
Type 2 Collocation: Strong collocation which 
allows very limited substitution of the components, 
for example, ? ?, ? ?, 
? ? and so on.  
Type 3 Collocation: Normal collocation which 
allows more substitution of the components, 
however a limitation is still required. For example, 
? ? ? ? ? ? 
? ? . 
  
By using synonym information and define 
substitutability, we can validate whether 
collocations are fixed collocations, strong 
collocations with very limited substitutions, or 
general collocations that can be substituted more 
freely. 
6 Acknowledgements 
Our great thanks to Dr. Liu Qun of the Chinese 
Language Research Center of Peking University for 
letting us share their data structure in the Synonyms 
Similarity Calculation. This work is partially 
supported by the Hong Kong Polytechnic 
University (Project Code A-P203) and CERG 
Grant (Project code 5087/01E) 
References  
M. Benson, 1990. Collocations and General 
Purpose Dictionaries. International Journal of 
Lexicography, 3(1): 23-35 
Y. Choueka, 1993. Looking for Needles in a 
Haystack or Locating Interesting Collocation 
Expressions in Large Textual Database. 
Proceedings of RIAO Conference on User-
oriented Content-based Text and Image 
Handling: 21-24, Cambridge. 
K. Church, and P. Hanks, 1990. Word Association 
Norms, Mutual Information,and Lexicography. 
Computational Linguistics, 6(1): 22-29. 
I. Dagan, L. Lee, and F. Pereira. 1997. Similarity-
based method for word sense disambiguation. 
Proceedings of the 35th Annual Meeting of 
ACL: 56-63, Madrid, Spain. 
Z. D. Dong and Q. Dong. 1999. Hownet, 
http://www.keenage.com 
D. K. Lin, 1997. Using Syntactic Dependency as 
Local Context to Resolve Word Sense Ambiguity. 
Proceedings of ACL/EACL-97: 64-71, Madrid, 
Spain 
Q. Liu, 2002. The Word Similarity Calculation on 
<<HowNet>>. Proceedings of 3rd Conference 
on Chinese lexicography, TaiBei 
Q. Lu, Y. Li, and R. F. Xu, 2003. Improving Xtract 
for Chinese Collocation Extraction.  Proceedings 
of IEEE International Conference on Natural 
Language Processing and Knowledge 
Engineering, Beijing 
C. D. Manning and H. Schutze, 1999. Foundations 
of Statistical Natural Language Processing. The 
MIT Press, Cambridge, Massachusetts  
D. Pearce, 2001. Synonymy in Collocation 
Extraction. Proceedings of NAACL'01 
Workshop on Wordnet and Other Lexical 
Resources: Applications, Extensions and 
Customizations 
F. Smadja, 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1): 143-
177 
H. Wu, and M. Zhou, 2003. Synonymous 
Collocation Extraction Using Translation 
Information. Proceeding of the 41st Annual 
Meeting of ACL 
D. K. Lin, 1998. Extracting collocations from text 
corpora. In Proc. First Workshop on 
Computational Terminology, Montreal, Canada. 
M. S. Sun, C. N. Huang and J. Fang, 1997. 
Preliminary Study on Quantitative Study on 
Chinese Collocations. ZhongGuoYuWen, No.1, 
29-38, (in Chinese). 
 
 
 The Construction of A Chinese Shallow Treebank 
Ruifeng Xu 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
csrfxu@comp.polyu.edu.hk 
Qin Lu 
Dept. Computing, 
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong  
csluqin@comp.polyu.edu.hk 
Yin Li 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
csyinli@comp.polyu.edu.hk 
Wanyin Li 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
cswyli@comp.polyu.edu.hk 
 
Abstract 
This paper presents the construction of a 
manually annotated Chinese shallow Treebank, 
named PolyU Treebank. Different from 
traditional Chinese Treebank based on full 
parsing, the PolyU Treebank is based on 
shallow parsing in which only partial syntactical 
structures are annotated. This Treebank can be 
used to support shallow parser training, testing 
and other natural language applications. 
Phrase-based Grammar, proposed by Peking 
University, is used to guide the design and 
implementation of the PolyU Treebank. The 
design principles include good resource sharing, 
low structural complexity, sufficient syntactic 
information and large data scale. The design 
issues, including corpus material preparation, 
standard for word segmentation and POS 
tagging, and the guideline for phrase bracketing 
and annotation, are presented in this paper. 
Well-designed workflow and effective 
semiautomatic and automatic annotation 
checking are used to ensure annotation accuracy 
and consistency. Currently, the PolyU Treebank 
has completed the annotation of a 
1-million-word corpus. The evaluation shows 
that the accuracy of annotation is higher than 
98%. 
1 Introduction 
A Treebank can be defined as a syntactically 
processed corpus. It is a language resource  
containing annotations of information at various 
linguistic levels such as words, phrases, clauses and 
sentences to form a ?bank of linguistic trees?. There 
are many Treebanks built for different languages 
such as the Penn Treebank (Marcus 1993), ICE-GB 
(Wallis 2003), and so on. The Penn Chinese 
Treebank is an important resource (Xia et al 2000; 
Xue et al 2002). Its annotation is based on 
Head-driven Phrase Structure Grammar (HPSG). 
The corpus of 100,000 Chinese words has been 
manually annotated with a strict quality assurance 
process. Another important work is the Sinica 
Treebank at the Academic Sinica, Taiwan ( Chen et 
al. 1999; Chen et al 2003). Information-based Case 
Grammar (ICG) was selected as the language 
framework. A head-driven chart parser was 
performed to do phrase bracketing and annotating. 
Then, manual post-editing was conducted. 
According to the report, The Sinica Treebank  
contains 38,725 parsed trees with 329,532 words.  
Most reported Chinese Treebanks, including the 
two above, are based on full parsing which requires 
complete syntactical analysis including determining 
syntactic categories of words, locating chunks that 
can be nested, finding relations between phrases and 
resolving the attachment ambiguities. The output of 
full parsing is a set of complete syntactic trees. 
Automatic full parsing, however, is difficult to 
achieve good performance. Shallow parsing (or 
partial parsing) is usually defined as a parsing 
process aiming to provide a limited amount of local 
syntactic information such as non-recursive noun 
phrases, V-O structures and S-V structures etc. Since 
shallow parsing can recognize the backbone of a 
sentence more effectively and accurately with lower 
cost, people has in recent years started to work using 
results from shallow parsing. A shallow parsed 
Treebank can be used to extract information for 
different applications especially for training shallow 
parsers. 
Different from full parsing, annotation to a 
shallow Treebank is only targeted at certain local 
structures in a sentence. The depth of ?shallowness?  
and the scope of annotation vary from different 
reported work. Thus, two issues in shallow Treebank 
annotation is (1) what information and (2) to what 
depths the syntactic information should be annotated. 
Generally speaking, the degree of ?shallowness? and 
the syntactical labeling are determined by the 
requirement of the serving applications. The choice 
of full parsing or shallow parsing is dependent on 
the need of the application including resources and 
 the capability of system to be developed (Xia et al 
2000; Chen et al 2000; Li et al 2003). Currently, 
there is no large-scale shallow annotated Treebank 
available as a publicly resource for training and 
testing.  
In this paper, we present a manually annotated 
shallow Treebank, called the PolyU Treebank. It is 
targeted to contain 1-million-word contemporary 
Chinese text. The whole work on the PolyU 
Treebank follows the Phrase-based Grammar 
proposed by Peking University (Yu et al 1998). In 
this language framework, a phrase, lead by a lexical 
word(or sometimes called a content word) as a head, 
is considered the basic syntactical unit in a Chinese 
sentence. The building of the PolyU Treebank was 
originally designed as training data for a shallow 
parser used for Chinese collocation extraction. From 
linguistics viewpoint, a collocation occurs only in 
words within a phrase, or between the headwords of 
related phrases (Zhang and Lin 1992). Therefore, the 
use of syntactic information is naturally considered 
an effective way to improve the performance of 
collocation extraction systems. The typical problems 
like doctor-nurse (Church and Hanks 1990) could be 
avoided by using such information. When 
employing syntactical information in collocation 
extraction, we restrict ourselves to identify the stable 
phrases in the sentences with certain levels of 
nesting. Thus it has motivated us to produce a 
shallow Treebank. 
A natural way to obtain a shallow Treebank is 
through extracting shallow structures from a fully 
parsed Treebank. Unfortunately, all the available 
fully parsed Treebank, such as the Penn Treebank 
and the Sinica Treebank, are annotated using 
different grammars than our chosen Phrase-based 
Grammar. Also, the sizes of these Treebank are 
much smaller in scale to be useful for training our 
shallow parser. 
This paper presents the most important design 
issues of the PolyU Treebank and the quality control 
mechanisms. The rest of this paper is organized as 
follows. Section 2 introduces the overview and 
design principles.  Section 3 to Section5, present 
the design issues on corpus material preparation, the 
standard for word segmentation and POS tagging, 
and the guideline for phrase bracketing and labeling, 
respectively. Section 6 discusses the quality 
assurance mechanisms including a carefully 
designed workflow, parallel annotation, and 
automatic and semi-automatic post-annotation 
checking. Section 7 gives the current progress and 
future work. 
2 Overview and Design Principles 
The objective of this project is to manually 
construct a large shallow Treebank with high 
accuracy and consistency.  
The design principles of The PolyU Treebank are: 
high resource sharing ability, low structural 
complexity, sufficient syntactic information and 
large data scale. First of all, the design and 
construction of The PolyU Treebank aims to provide 
as much a general purpose Treebank as possible so 
that different applications can make use of it as a 
NLP resource. With this objective, we chose to 
follow the well-known Phrase-based Grammar as 
the framework for annotation as this grammar is 
widely accepted by Chinese language researchers, 
and thus our work can be easily understood and 
accepted.  
Due to the lack of word delimitation in Chinese, 
word segmentation must be performed before any 
further syntactical annotation. High accuracy of 
word segmentation is very important for this project. 
In this project, we chose to use the segmented and 
tagged corpus of People Daily annotated by the 
Peking University. The annotated corpus contains 
articles appeared in the People Daily Newspaper in 
1998. The segmentation is based on the guidelines, 
given in the Chinese national standard GB13715, 
(Liu et al 1993) and the POS tagging specification 
was developed according to the ?Grammatical 
Knowledge-base of contemporary Chinese?. 
According to the report from Peking University, the 
accuracy of this annotated corpus in terms of 
segmentation and POS tagging are 99.9% and 99.5%, 
respectively (Yu et al 2001). The use of such mature 
and widely adopted resource can effectively reduce 
our cost, ensure syntactical annotation quality. With 
consistency in segmentation, POS, and syntactic 
annotation, the resulting Treebank can be readily 
shared by other researchers as a public resource. 
The second design principle is low structural 
complexity. That means, the annotation framework 
should be clear and simple, and the labeled syntactic 
and functional information should be commonly 
used and accepted. Considering the characteristics of 
shallow annotation, our project has focused on the 
annotation of phrases and headwords while the 
sentence level syntax are ignored.  
Following the framework of Phrase-based 
Grammar, a base-phrase is regarded as the smallest 
unit where a base-phrase is defined as a ?stable? and 
?simple? phrase without nesting components. Study 
on Chinese syntactical analysis suggests that phrases 
should be the fundamental unit instead of words in a 
sentence. This is because, firstly, the usage of 
Chinese words is very flexible. A word may have 
different POS tags serving for different functions in 
sentences. On the contrary, the use of Chinese 
phrases is much more stable. That is, a phrase has 
very limited functional use in a sentence. Secondly, 
the construction rules of Chinese phrases are nearly 
 the same as that of Chinese sentences. Therefore, the 
analysis of phrases can help identifying POS and 
grammatical functions of words. Naturally, it should 
be regarded as the basic syntactical unit. Usually, a 
base-phrase is driven by a lexical word as its 
headword. Examples of base-phrases include base 
NP, base VP and so on, such as the sample shown 
below. 
  
Using base-phrases as the start point, nested levels 
of phrases are then identified, until the maximum 
phrases (will be defined later) are identified. Since 
we do not intend to provide full parsing information, 
there has to be a limit on the level of nesting. For 
practical reasons, we choose to limit the nesting of 
brackets to 3 levels. That means, the depth of our 
shallow parsed Treebank will be limited to 3. This 
restriction can limit the structural complexity to a 
manageable level.  
Our nested bracketing is not strictly bottom up. 
That is we do not simply extend from base-phrase 
and move up until the 3rd level. Instead, we first 
identify the maximal-phrase which is used to 
identify the backbone of the sentence. The 
maximal-phrase provides the framework under 
which the base-phrases of up to 2 levels can be 
identified. The principles for the identification of 
scope and depth of phrase bracketing are briefly 
explained below and the operating procedure is 
indicated by the given order in which these 
principles are presented. More details is given in 
Section 5. 
Step 1: Annotation of maximal-phrase which is 
the shortest word sequence of maximally 
spanning non-overlapping edges which plays a 
distinct semantic role of a predicate. A 
maximal-phrase contains two or more lexical 
words. 
Step 2: Annotation of base-phrases within a 
maximal-phrase. In case a base-phrase and a 
maximal-phrase are identical and the 
maximal-phrase is already bracketed in Step 1, no 
bracketing is done in this step. For each identified 
base-phrase, its headword will be marked. 
Step 3: Annotation of next level of bracketing, 
called mid-phrase which is expended from a 
base-phrase. A mid-phrase is annotated only if it is 
deemed necessary. The process starts from the 
identified base-phrase. One more level of 
syntactical structure is then bracketed if it exists 
within the maximal-phrase.   
  
The third design principle is to provide sufficient 
syntactical information for natural language 
application even though shallow annotation does not 
necessarily contain complete syntactic information 
at sentence level. Some past research in Chinese 
shallow parsing were on single level base-phrases 
only (Sun 2001). However, for certain applications, 
such as for collocation extraction, identification of 
base-phrases only are not very useful. In this project, 
we have decided to annotate phrases within three 
levels of nesting within a sentence. For each phrase, 
a label is be given to indicate its syntactical 
information, and an optional semantic or structural 
label is given if applicable. Furthermore, the 
headword of a base-phrase is annotated. We believe 
these information are sufficient for many natural 
language processing research work and it is also 
manageable for this project within its working 
schedule. 
Fourthly, aiming to support practical language 
processing, a reasonably large annotated Treebank is 
expected. Studies on English have shown that 
Treebank of word size 500K to 1M is reasonable for 
syntactical structure analysis (Leech and Garside 
1996). In consideration of the resources available 
and the reference of studies on English, we have set 
out our Treebank size to be one million words. We 
hope such a reasonably large-scale data can 
effectively support some language research, such as  
collocation extraction.  
We chose to use the XML format to record the 
annotated data. Other information such as original 
article related information (author, date, etc.), 
annotator name, and other useful information are 
also given through the meta-tags provided by XML. 
All the meta-tags can be removed by a program to 
recover the original data. 
We have performed a small-scale experiment to 
compare the annotation cost of shallow annotation 
and full annotation (followed Penn Chinese 
Treebank specification) on 500 Chinese sentences 
by the same annotators. The time cost in shallow 
annotation is only 25% of that for full annotation. 
Meanwhile, due to the reduced structural complexity 
in shallow annotation, the accuracy of first pass 
shallow annotation is much higher than full 
annotation. 
3 Corpus Materials Preparation 
The People Daily corpus, developed by PKU, 
consists of more than 13k articles totaling 5M words. 
As we need one million words for our Treebank, we 
have selected articles covering different areas in 
different time span to avoid duplications due to 
short-lived events and news topics. Our selection 
takes each day?s news as one single unit, and then  
several distant dates are randomly selected among 
the whole 182 days in the entire collection.  We 
have also decided to keep the original articles? 
structures and topics indicators as they may be 
useful for some applications. 
 4 Word Segmentation and Part-of-Speech 
Tagging 
The articles selected from PKU corpus are already 
segmented into words following the guidelines 
given in GB13715. The annotated corpus has a basic 
lexicon of over 60,000 words. We simply use this 
segmentation without any change and the accuracy 
is claimed to be 99.9%.  
Each word in the PKU corpus is given a POS tag.  
In this tagging scheme, a total of 43 POS tags are 
listed (Yu et al 2001).  Our project takes the PKU 
POS tags with only notational changes explained as 
follows: 
The morphemes tags including Ag (Adjectives 
morphemes), Bg, Dg, Ng, Mg, Rg, Tg, Qg, and Ug 
are re-labeled as lowercase letters, ag, bg, dg, ng, mg, 
rg, tg, qg and ug, respectively. This modification is 
to ensure consistent labeling in our system where the 
lower cases are used to indicate word-level tags and 
upper cases are used to indicate phrase-level labels. 
5 Phrase Bracketing and Annotation 
Phrase bracketing and annotation is the core part 
of this project. Not only all the original annotated 
files are converted to XML files, results of our 
annotations are also given in XML form. The meta 
tags provided by XML are very helpful for further 
processing and searching to the annotated text. . 
Note that in our project, the basic phrasal analysis 
looks at the context of a clause, not a sentence. Here, 
the term clause refers the text string ended by some 
punctuations including comma (,), semicolon (;), 
colon (:), or period (.). Certain punctuation marks 
such as ? ?, ?<?, and ?>? are not considered clause 
separators. For example,  
  
is considered having two clauses and thus will be 
bracketed separately. It should be pointed out that he 
set of Chinese punctuation marks are different from 
that of English and their usage can also be different. 
Therefore, an English sentence and their Chinese 
translation may use different punctuation marks.  
For example, the sentence 
 
is the translation of the English ?Tom, John, and 
Jack go back to school together? , which uses ? ? 
rather than comma(,) to indicate parallel structures, 
and is thus considered one clause.   
Each clause will then be processed according to 
the principles discussed in Section 2. The symbols 
?[? and ?]? are used to indicate the left and right 
boundaries of a phrase. The right bracket is 
appended with syntactic labels as described in the 
general form of [Phrase]SS-FF, where SS is a 
mandatory syntactic label such as NP(noun phrase) 
and AP(adjective phrase), and FF is an optional label 
indicating internal structures and semantic functions 
such as BL(parallel), SB(a noun is the object of verb 
within a verb phrase). A total of  21 SS labels and 
20 FF labels are given in our phrase annotation 
specification. For example, the functional label BL 
identifies parallel components in a phrase as 
indicated in the example .  
As in another example shown below,  
 
the phrase  is a verb phrase, thus it is 
labeled as VP. Furthermore, the verb phrase can be 
further classified as a verb-complement type. Thus 
an additional SBU function label is marked. We 
should point out that since the FF labels are not 
syntactical information and are thus not expected to 
be used by any shallow parsers. The FF labels carry 
structural and/or semantic information which are of 
help in annotation. We consider it useful for other 
applications and thus decide to keep them in the 
Treebank. Appendix 1 lists all the FF labels used in 
the annotation. 
 
5.1  Identification of Maximal-phrase:  
The maximal-phrases are the main syntactical 
structures including subject, predicate, and objects in 
a clause. Again, maximal-phrase is defined as the 
phrase with the maximum spanning non-overlapping 
length, and it is a predicate playing a distinct 
semantic role and containing more than one lexical 
word. That means a maximal-phrase contains at least 
one base-phrase. As this is the first stage in the 
bracketing process, no nesting should occur. In the 
following annotated sentence, 
 (Eg.1) 
there are two separate maximal-phrases, 
, and 
. Note 
that  is considered a base-phrase, but not a 
maximal-phrase because it contains only one lexical 
word. Unlike many annotations where the object of 
a sentence is included as a part of the verb phrase, 
we treat them as separate maximal-phrases both due 
to our requirement and also for reducing nesting. 
If a clause is completely embedded in a larger 
clause, it is considered a special clause and given a 
special name called an internal clause .  We will 
bracket such an internal clause as a maximal phrase 
with the tag ?IC? as shown in the following example, 
 
 
5.2  Annotation of Base-phrases:  
A base-phrase is the phrase with stable, close and 
simple structure without nesting components. 
Normally a base-phrase contains a lexical word as 
 headword. Taking the  maximal-phrase 
in 
Eg.1 as an example,  and 
, are base-phrases in this 
maximal-phrase. Thus, the sentence is annotated as 
 
  
In fact, and are also 
base-phrases.  is not bracketed because it is a 
single lexical word as a base-phrase without any 
ambiguity and it is thus by default not being 
bracketed. is not further 
bracketed because it overlaps with a maximal-phrase. 
Our annotation principle here is that if a base-phrase 
overlaps with a maximal-phrase, it will not be 
bracketed twice.  
The identification of base-phrase is done only 
within an already identified maximal-phrase. In 
other words, if a base-phrase is identified, it must be 
nested inside a maximal-phrase or at most overlaps 
with it. It should be pointed out that the 
identification of a base-phrase is the most 
fundamental and most important goal of Treebank 
annotation. The identification of maximal-phrases 
can be considered as parsing a clause using a 
top-down approach. On the other hand, the 
identification of a base-phrase is a bottom up 
approach to find the most basic units within a 
maximal-phrase.  
 
5.3  Mid-Phrase Identification:  
Due to the fact that sometimes there may be more 
syntactic structures between the base-phrases and 
maximal-phrases, this step uses base-phrase as the 
starting point to further identify one more level of 
the syntactical structure in a maximal-phrase. Takes 
Eg.1 as an example, it is further annotated as 
 
where the underlined text shows the additional 
annotation. 
As we only limit our nesting to three levels, any 
further nested phrases will be ignored. The 
following sentence shows the result of our 
annotation with three levels of nesting:  
  
However, a full annotation should have 4 levels of 
nesting as shown below. The underlined text is the 
4th level annotation skipped by our system. 
 
 
5.4  Annotation of Headword 
In our system, a ?#? tag will be appended after a 
word to indicate that it is a headword of the 
base-phrase. Here, a headword must be a lexical 
word rather than a function word.  
In most cases, a headword stays in a fixed position 
of a base-phrase. For example, the headword of a 
noun phrase is normally the last noun in this phrase. 
Thus, we call this position the default position. If a 
headword is in the default position, annotation is not 
needed. Otherwise, a ?#? tag is used to indicate the 
headword. 
For example, in a clause, 
,  
 is a verb phrase, and the headword 
of the phrase is , which is not in the default 
position of a verb phrase. Thus, this phrase is further 
annotated as:  
  
Note that  is also a headword, but since it 
is in the default position, no explicit annotation is 
needed. 
6 Annotation and Quality Assurance 
Our research team is formed by four people at the 
Hong Kong Polytechnic University, two linguists 
from Beijing Language and Culture University and 
some research collaborators from Peking University. 
Furthermore, the annotation work has been 
conducted by four post-graduate students in 
language studies and computational linguistics from 
the Beijing Language and Culture University.  
The annotation work is conducted in 5 separate 
stages to ensure quality output of the annotation 
work. The preparation of annotation specification 
and corpus selection was done in the first stage. 
Researchers in Hong Kong invited two linguists 
from China to come to Hong Kong to prepare for the 
corpus collection and selection work. A thorough 
study on the reported work in this area was 
conducted. After the project scope was defined, the 
SS labels and the FF labels were then defined. A 
Treebank specification was then documented.  The 
Treebank was given the name PolyU Treebank to 
indicate that it is produced at the Hong Kong 
Polytechnic University. In order to validate the 
specifications drafted, all the six members first 
manually annotated 10k-word material, separately. 
The outputs were then compared, and the problems 
and ambiguities occurred were discussed and 
consolidated and named Version 1.0. Stage 1 took 
about 5 months to complete. Details of the 
specification can be downloaded from the project 
website www.comp.polyu.edu.hk/~cclab. 
In Stage 2, the annotators in Beijing were then 
involved. They had to first study the specification 
and understand the requirement of the annotation. 
Then, the annotators under the supervision of a team 
member in Stage 1 annotated 20k-word materials 
together and discussed the problems occurred. 
 During this two-month work, the annotators were 
trained to understand the specification. The 
emphasis at this stage was to train the annotators? 
good understanding of the specification as well as 
consistency by each annotator and consistency by 
different annotators. Further problems occurred in 
the actual annotation practice were then solved and 
the specification was also further refined or 
modified.  
In Stage 3, which took about 2 months, each 
annotator was  assigned 40k-word material each in 
which 5k-words material were duplicate annotated 
to all the annotators. Meanwhile, the team members 
in Hong Kong also developed a post-annotation 
checking tool to verify the annotation format, phrase 
bracketing, annotation tags, and phrase marks to 
remove ambiguities and mistakes. Furthermore, an 
evaluation tool was built to check the consistency of 
annotation output. The detected annotation errors 
were then sent back to the annotators for discussion 
and correction. Any further problems occurred were 
submitted for group discussion and minor 
modification on the specification was also done. 
In stage 4, each annotator was dispatched with one 
set of 50k-word material each time. For each 
distribution, 15k-word data in each set were 
distributed to more than two annotators in duplicates 
so that for any three annotators, there would be 5K 
duplicated materials. When the annotators finished 
the first pass annotation, we used the post-annotation 
checking tool to do format checking in order to 
remove the obvious annotation errors such as wrong 
tag annotation and cross bracketing. However, it was 
quite difficult to check the difference in annotation 
due to different interpretation of a sentence. What 
we did was to make use of the annotations done on 
the duplicate materials to compare for consistency. 
When ambiguity or differences were identified, 
discussions were conducted and a result used by the 
majority would be chosen as the accepted result. The 
re-annotated results were regarded as the Golden 
Standard to evaluate the accuracy of annotation and 
consistency between different annotators. The 
annotators were required to study this Golden 
Standard and go back to remove  similar mistakes. 
The annotated 50k data was accepted only after this. 
Then, a new 50k-word materials was distributed and 
repeated in the same way. During this stage, the 
ambiguous and out-of-tag-set phrase structures were 
marked as OT for further process. The annotation 
specification was not modified in order to avoid 
frequent revisit to already annotated data. About 4 
months were spent on this stage. 
In Stage 5, all the members and annotators were 
grouped and discuss the OT cases. Some typical new 
phrase structure and function types were appended 
in the specification and thus the final formal 
annotation specification was established. Using this 
final specification, the annotators had to go back to 
check their output, modify the mistakes and 
substitute the OT tags by the agreed tags. Currently, 
the project was already in Stage 5 with 2 months of 
work finished. A further 2 months was expected to 
complete this work. 
Since it is impossible to do all the checking and 
analysis manually, a series of checking and 
evaluating tools are established. One of the tools is 
to check the consistency between text corpus files 
and annotated XML files including checking the 
XML format, the filled XML header, and whether 
the original txt material is being altered by accident. 
This program ensures that the XML header 
information is correctly filled and during annotation 
process, no additional mistakes are introduced due to 
typing errors.  
Furthermore, we have developed and trained a 
shallow parser using the Golden Standard data. This 
shallow parser is performed on the original text data, 
and its output and manually annotated result are 
compared for verification to further remove errors 
Now, we are in the process of developing an 
effective analyzer to evaluate the accuracy and 
consistency for the whole annotated corpus. For the 
exactly matched bracketed phrases, we check 
whether the same phrase labels are given. Abnormal 
cases will be manually checked and confirmed. Our 
final goal is to ensure the bracketing can reach 99% 
accuracy and consistency. 
7 Current Progress and Future Work 
As mentioned earlier, we are now in Stage 5 of the 
annotation. The resulting annotation contains 2,639 
articles selected from PKU People Daily corpus. 
These articles contains 1, 035, 058 segmented 
Chinese words, with on average, around 394 words 
in each article. There are a total of 284, 665 
bracketed phrases including nested phrases. A 
summary of the different SS labels used are given in 
Table 1. 
 
Table 1. Statistics of annotated syntactical phrases 
 
For each bracketed phrase, if its FF label does not 
fit into the corresponding default pattern, (like for 
the noun phrase(NP), the default grammatical 
structure is that the last noun in the phrase is the 
headword and other components are the modifiers, 
using PZ tags), its FF labels should then be 
explicitly labeled. The statistics of annotated FF tags 
 are listed in Table 2.  
 
Table 2. Statistics of function and structure tags 
 
For the material annotated by multiple annotators 
as duplicates, the evaluation program has reported 
that the accuracy of phrase annotation is higher than 
99.5% and the consistency between different 
annotators is higher than 99.8%. As for other 
annotated materials, the quality evaluation program 
preliminarily reports the accuracy of phrase 
annotation is higher than 98%. Further checking and 
evaluation work are ongoing to ensure the final 
overall accuracy achieves 99%. 
Up to now, the FF labels of 5,255 phrases are 
annotated as OT. That means about 1.8% (5,255 out 
of a total of 284,665) of them do not fit into any 
patterns listed in Table 2. Most of them are proper 
noun phrase, syntactically labeled as PP. We are 
investigating these cases and trying to identify 
whether some of them can be in new function and 
structure patterns and give a new label. 
It is also our intention to further develop our tools 
to improve the automatic annotation analysis and 
evaluation program to find out the potential 
annotation error and inconsistency. Other 
visualization tools are also being developed to 
support keyword searching, context indexing, and 
annotation case searching. Once we complete Stage 
5, we intend to make the PolyU Treebank data 
available for public access.  Furthermore, we are 
developing a shallow parser and using The PolyU 
Treebank as training and testing data. 
 
Acknowledgement 
 
This project is partially supported by the Hong Kong 
Polytechnic University (Project Code A-P203) and 
CERG Grant (Project code 5087/01E) 
References  
Baoli Li, Qin Lu and Yin Li. 2003. Building a 
Chinese Shallow Parsed Treebank for Collocation 
Extraction, Proceedings of CICLing 2003: 
402-405 
Fei Xia, et al 2000. Developing Guidelines and 
Ensuring Consistency for Chinese Text Annotation 
Proceedings of LREC-2000, Greece 
Feng-yi Chen, et al 1999. Sinica Treebank, 
Computational Linguistics and Chinese Language 
Processing, 4(2):183-204 
G. N. Leech, R.Garside. 1996. Running a grammar 
factory: the production of syntactically analyzed 
corpora or ?treebanks?, Johansson and Stenstron. 
Honglin Sun, 2001. A Content Chunk Parser for 
Unrestricted Chinese Text, Ph.D Thesis, Peking 
University, 2001 
Keh-jiann Chen et al 2003. Building and Using 
Parsed Corpora (Anne Abeill? ed. s) KLUWER, 
Dordrecht 
Kenneth Church, and Patrick Hanks. 1990. Word 
association norms, mutual information, and 
lexicography, Computational Linguistics, 16(1): 
22-29 
Marcus, M. et al 1993. Building a Large Annotated 
Corpus of English: The Penn Treebank, 
Computational Linguistics, 19(1): 313-330. 
Nianwen Xue, et al 2002. Building a Large-Scale 
Annotated Chinese Corpus, Proceedings of 
COLING 2002, Taipei, Taiwan 
Sean Wallis, 2003. Building and Using Parsed 
Corpora (Anne Abeill? eds) KLUWER, Dordrecht 
Shiwen Yu, et al 1998. The Grammatical 
Knowledge- base of contemporary Chinese: a 
complete specification. Tsinghua University Press, 
Beijing, China 
Shiwen Yu, et al 2001. Guideline of People Daily 
Corpus Annotation, Technical report, Beijing 
University 
Shoukang Zhang and Xingguang Lin, 1992. 
Collocation Dictionary of Modern Chinese 
Lexical Words, Business Publisher, China 
Yuan Liu, et al 1993. Segmentation standard for 
Modern Chinese Information Processing and 
automatic segmentation methodology. Tsinghua 
University Press, Beijing, China 
  
Appendix 1 The structural and semantic FF labels  
 
 
Appendix 2 Example of an Annotated Article  
 
 
 
Proceedings of the Linguistic Annotation Workshop, pages 61?68,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating Chinese Collocations with Multi Information 
 
Ruifeng Xu1,     Qin Lu1,     Kam-Fai Wong2,    Wenjie Li1 
? 
?1 Department of Computing,                       2 Department of Systems Engineering and 
?                                                  Engineering Management 
 The Hong Kong Polytechnic University,        The Chinese University of Hong Kong,   
?Kowloon, Hong Kong                                      N.T., Hong Kong 
{csrfxu,csluqin,cswjli}@comp.polyu.edu.hk  kfwong@se.cuhk.edu.hk 
? 
 
Abstract 
This paper presents the design and construc-
tion of an annotated Chinese collocation bank 
as the resource to support systematic research 
on Chinese collocations. With the help of 
computational tools, the bi-gram and n-gram 
collocations corresponding to 3,643 head-
words are manually identified. Furthermore, 
annotations for bi-gram collocations include 
dependency relation, chunking relation and 
classification of collocation types. Currently, 
the collocation bank annotated 23,581 bi-
gram collocations and 2,752 n-gram colloca-
tions extracted from  a 5-million-word corpus. 
Through statistical analysis on the collocation 
bank, some characteristics of Chinese bi-
gram collocations are examined which is es-
sential to collocation research, especially for 
Chinese. 
1 Introduction 
Collocation is a lexical phenomenon in which two 
or more words are habitually combined and com-
monly used in a language to express certain seman-
tic meaning. For example, in Chinese, people will 
say ??-?? (historical baggage) rather than ?
? -?? (historical luggage) even though ??
(baggage) and ?? (luggage) are synonymous. 
However, no one can argue why ?? must collo-
cate with??. Briefly speaking, collocations are 
frequently used word combinations. The collocated 
words always have syntactic or semantic relations 
but they cannot be generated directly by syntactic 
or semantic rules. Collocation can bring out differ-
ent meanings a word can carry and it plays an in-
dispensable role in expressing the most appropriate 
meaning in a given context. Consequently, colloca-
tion knowledge is widely employed in natural lan-
guage processing tasks such as word sense disam-
biguation, machine translation, information re-
trieval and natural language generation (Manning 
et al 1999).  
Although the importance of collocation is well 
known, it is difficult to compile a complete collo-
cation dictionary. There are some existing corpus 
linguistic researches on automatic extraction of 
collocations from electronic text (Smadja 1993; 
Lin 1998; Xu and Lu 2006). These techniques are 
mainly based on statistical techniques and syntactic 
analysis. However, the performances of automatic 
collocation extraction systems are not satisfactory 
(Pecina 2005). A problem is that collocations are 
word combinations that co-occur within a short 
context, but not all such co-occurrences are true 
collocations. Further examinations is needed to 
filter out pseudo-collocations once co-occurred 
word pairs are identified.  A collocation bank with 
true collocations annotated is naturally an indis-
pensable resource for collocation research. (Kosho 
et al 2000) presented their works of collocation 
annotation on Japanese text. Also, the Turkish 
treebank, (Bedin 2003) included collocation anno-
tation as one step in its annotation. These two col-
location banks provided collocation identification 
and co-occurrence verification information. (Tutin 
2005) used shallow analysis based on finite state 
transducers and lexicon-grammar to identify and 
annotate collocations in a French corpus. This col-
location bank further provided the lexical functions 
of the collocations. However to this day, there is 
no reported Chinese collocation bank available. 
61
In this paper, we present the design and con-
struction of a Chinese collocation bank (acrony-
med CCB). This is the first attempt to build a 
large-scale Chinese collocation bank as a Chinese 
NLP resource with multiple linguistic information 
for each collocation including:  (1) annotating the 
collocated words for each given headword; (2) dis-
tinguishing n-gram and bi-gram collocations for 
the headword; (3) for bi-gram collocations, CCB 
provides their syntactic dependencies, chunking 
relation and classification of collocation types 
which is proposed by (Xu and Lu 2006). In addi-
tion, we introduce the quality assurance mecha-
nism used for CCB. CCB currently contains for 
3,643 common headwords taken from ?The Dic-
tionary of Modern Chinese Collocations? (Mei 
1999) with 23,581 unique bi-gram collocations and 
2,752 unique n-gram collocations extracted from a 
five-million-word segmented and chunked Chinese 
corpus (Xu and Lu, 2005). 
The rest of this paper is organized as follows. 
Section 2 presents some basic concepts. Section 3 
describes the annotation guideline. Section 4 de-
scribes the practical issues in the annotation proc-
ess including corpus preparation, headword prepa-
ration, annotation flow, and the quality assurance 
mechanism. Section 5 gives current status of CCB 
and characteristics analysis of the annotated collo-
cations. Section 6 concludes this paper. 
2 Basic Concepts 
Although collocations are habitual expressions in 
natural language use and they can be easily under-
stood by people, a precise definition of collocation 
is still far-reaching (Manning et al 1999). In this 
study, we define a collocation as a recurrent and 
conventional expression of two or more content 
words that holds syntactic and semantic relation. 
Content words in Chinese include noun, verb, ad-
jective, adverb, determiner, directional word, and 
gerund. Collocations with only two words are 
called bi-gram collocations and others are called n-
gram collocations. 
From a linguistic view point, collocations have a 
number of characteristics. Firstly, collocations are 
recurrent as they are of habitual use. Collocations 
occur frequently in similar contexts and they ap-
pear in certain fixed patterns. However, they can-
not be described by the same set of syntactic or 
semantic rules. Secondly, free word combinations 
which can be generated by linguistic rules are 
normally considered compositional.  In contrast, 
collocations should be limited compositional 
(Manning et al 1999) and they usually carry addi-
tional meanings when used as a collocation. 
Thirdly, collocations are also limited substitutable 
and limited modifiable. Limited substitutable here 
means that a word cannot be freely substituted by 
other words with similar linguistic functions in the 
same context such as synonyms. Also, many collo-
cations cannot be modified freely by adding modi-
fiers or through grammatical transformations. 
Lastly, collocations are domain-dependent (Smadja 
1993) and language-dependent.  
3 Annotation Guideline Design 
The guideline firstly determines the annotation 
strategy. 
(1) The annotation of CCB follows the head-
word-driven strategy. The annotation uses selected 
headwords as the starting point. In each circle, the 
collocations corresponding to one headword are 
annotated. Headword-driven strategy makes a 
more efficient annotation as it is helpful to estimate 
and compare the relevant collocations. 
(2) CCB is manually annotated with the help of 
automatic estimation of computational features, i.e. 
semi-automatic software tools are used to generate 
parsing and chunking candidates and to estimate 
the classification features. These data are present to 
the annotators for determination. The use of assis-
tive tools is helpful to produce accurate annota-
tions with efficiency. 
The guideline also specifies the information to 
be annotated and the labels used in the annotation. 
For a given headword, CCB annotates both bi-
gram collocations and n-gram collocations. Con-
sidering the fact that n-gram collocations consist-
ing of continuous significant bi-grams as a whole 
and, the n-gram annotation is based on the identifi-
cation and verification of bi-gram word combina-
tions and is prior to the annotation of bi-gram col-
locations.  
For bi-gram annotation, which is the major in-
terest  in collocation research, three kinds of in-
formation are annotated. The first one is the syn-
tactic dependency of the headword and its co-word 
in a bi-gram collocation . A syntactic dependency 
normally consists of one word as the governor (or 
head), a dependency type and another word serves 
62
as dependent (or modifier) (Lin 1998).Totally, 10 
types of dependencies are annotated in CCB. They 
are listed in Table 1 below. 
 
 Dependency Description Example 
ADA Adjective and its adverbial modifier ??/d ??/a  greatly painful 
ADV Predicate and its adverbial modifier in 
which the predicate serves as head 
??/ad ??/v heavily strike 
AN Noun and its adjective modifier ??/a ??/n lawful incoming 
CMP Predicate and its complement in which 
the predicate serves as head 
??/v??/v ineffectively treat
NJX Juxtaposition structure ??/a??/a fair and reasonable
NN Noun and its nominal modifier ??/n ??/n personal safety 
SBV Predicate and its subject ??/n ??/v property transfer 
VO Predicate and its object in which the 
predicate serves as head 
??/v ??/n change mechanism
VV Serial verb constructions which indi-
cates that there are serial actions  
??/v ??/v trace and report 
OT Others   
Table 1. The dependency categories 
The second one is the syntactic chunking informa-
tion (a chunk is defined as a minimum non-nesting 
or non-overlapping phrase) (Xu and Lu, 2005). 
Chunking information identifies all the words for a 
collocation within the context of an enclosed 
chunk. Thus, it is a way to identify its proper con-
text at the most immediate syntactic structure. 11 
types of syntactic chunking categories given in (Xu 
and 2006) are used as listed in Table 2.  
 
 Description Examples 
BNP Base noun phrase [??/n ??/n]NP     market economy 
BAP Base adjective phrase  [??/a??/a]BAP   fair and reasonable
BVP Base verb phrase [??/a??/v]BVP   successfully start 
BDP Base adverb phrase [?/d ??/d]BDP      no longer 
BQP Base quantifier phrase [?? /m ? /q]BQP ?? /n several thou-
sand soldiers 
BTP Base time phrase [??/t ??/t]BTP 8:00 in the morning 
BFP Base position phrase [??/ns ???/f]BFP Northeast of Mon-
golia 
BNT Name of an organization [??/ns ??/n]BNT Yantai University 
BNS Name of a place [??/ns ??/ns]BNS Tongshan, Jiangsu 
Province 
BNZ Other proper noun phrase [???/nr?/n]BNZ The Nobel Prize 
BSV S-V structure [??/n ??/a]BSV  territorial integrity 
Table 2. The chunking categories 
The third one is the classification of collocation 
types. Collocations cover a wide spectrum of ha-
bitual word combinations ranging from idioms to 
free word combinations. Some collocations are 
very rigid and some are more flexible. (Xu and Lu 
2006) proposed a scheme to classify collocations 
into four types according to the internal association 
of collocations including compositionality, non-
substitutability, non-modifiability, and statistical 
significance. They are,  
Type 0: Idiomatic Collocation 
Type 0 collocations are fully non-compositional 
as its meaning cannot be predicted from the mean-
ings of its components such as???? (climbing 
a tree to catch a fish, which is a metaphor for a 
fruitless endeavour). Some terminologies are also 
Type 0 collocations such as ?  ?(Blue-tooth ) 
which refers to a wireless communication protocol. 
Type 0 collocations must have fixed forms. Their 
components are non-substitutable and non-
modifiable allowing no syntactic transformation 
and no internal lexical variation. This type of col-
locations has very strong internal associations and 
co-occurrence statistics is not important.  
Type 1: Fixed Collocation 
Type 1 collocations are very limited composi-
tional with fixed forms which are non-substitutable 
and non-modifiable. However, this type can be 
compositional. None of the words in a Type 1 col-
location can be substituted by any other words to 
retain the same meaning such as in??/n ???
/n (diplomatic immunity). Finally, Type 1 colloca-
tions normally have strong co-occurrence statistics 
to support them. 
Type 2: Strong Collocation 
Type 2 collocations are limitedly compositional. 
They allow very limited substitutability. In other 
words, their components can only be substituted by 
few synonyms and the newly generated word com-
binations have similar meaning, e.g., ??/v ??
/n (alliance formation) and ??/v ??/n (alliance 
formation). Furthermore, Type 2 collocations al-
low limited modifier insertion and the order of 
components must be maintained. Type2 colloca-
tions normally have strong statistical support.  
Type 3: Loose Collocation 
Type 3 collocations have loose restrictions. 
They are nearly compositional. Their components 
may be substituted by some of their synonyms and 
the newly generated word combinations usually 
have very similar meanings. Type 3 collocations 
are modifiable meaning that they allow modifier 
insertions. Type 3 collocations have weak internal 
associations and they must have statistically sig-
nificant co-occurrence.  
The classification represents the strength of in-
ternal associations of collocated words. The anno-
tation of these three kinds of information is essen-
tial to all-rounded characteristic analysis of collo-
cations. 
63
4 Annotation of CCB 
4.1 Data Preparation 
CCB is based on the PolyU chunk bank (Xu and 
Lu, 2005) which contains chunking information on 
the People?s Daily corpus with both segmentation 
and part-of-speech tags. The accuracies of word 
segmentation and POS tagging are claimed to be 
higher than 99.9% and 99.5%, respectively (Yu et 
al. 2001). The use of this popular and accurate raw 
resource helped to reduce the cost of annotation 
significantly, and ensured maximal sharing of our 
output.  
The set of 3, 643 headwords are selected from 
?The Dictionary of Modern Chinese Collocation? 
(Mei 1999) among about 6,000 headwords in the 
dictionary. The selection  was based both on the 
judgment by linguistic experts as well as the statis-
tical information that they are commonly used. 
4.2 Corpus Preprocessing 
The CCB annotations are represented in XML. 
Since collocations are practical word combinations 
and word is the basic unit in collocation research, a 
preprocessing module is devised to transfer the 
chunked sentences in the PolyU chunk bank to 
word sequences with the appropriate labels to indi-
cate the corresponding chunking information. This 
preprocessing module indexes the words and 
chunks in the sentences and encodes the chunking 
information of each word in two steps. Consider 
the following sample sentence extracted from the 
PolyU chunk bank: 
??/v[??/n??/n]BNP?/u[??/n??/n??
/an ]BNP 
(ensure life and property safety of the people) 
The first step in preprocessing is to index each 
word and the chunk in the sentence by giving in-
cremental word ids and chunk ids from left to right. 
That is,, 
[W1]??/v [W2]??/n [W3]??/n [W4]?/u  
[W5]??/n [W6]??/n [W7]??/an [C1]BNP [C2]BNP 
where, [W1] to [W7] are the words and [C1] to [C2] 
are chunks although chunking positions are not 
included in this step. One Chinese word may occur 
in a sentence for more than one times, the unique 
word ids are helpful to avoid ambiguities in the 
collocation annotation on these words. 
The second step is to represent the chunking in-
formation of each word. Chunking boundary in-
formation is labeled by following initial/final rep-
resentation scheme. Four labels, O/B/I/E, are used 
to mark the isolated words outsides any chunks, 
chunk-initial words, words in the middle of chunks, 
and chunk-final words, respectively. Finally, a la-
bel H is used to mark the identified head of chunks 
and N to mark the non-head words. 
The above sample sentence is then transferred to 
a sequence of words with labels as shown below, 
<labeled> [W1][O_O_N][O]??/v [W2][B_BNP_N][C1]
??/n [W3][E_BNP_H][C1]??/n [W4][O_O_N][O]?/u 
[W5][B_BNP_N][C2]??/n [W6][I_BNP_N][C2]??/n 
[W7][E_BNP_N][C2]??/an </labeled> 
For each word, the first label is the word ID. The 
second one is a hybrid tag for describing its chunk-
ing status. The hybrid tags are ordinal with respect 
to the chunking status of boundary, syntactic cate-
gory and head, For example, B_BNP_N indicates 
that current word is the beginning or a BNP and 
this word is not the head of this chunk. The third 
one is the chunk ID if applicable. For the word out 
of any chunks, a fixed chunk ID O is given. 
4.3 Collocation Annotation 
Collocation annotation is conducted on one head-
word at a time. For a given headword, an annota-
tors examines its context to determine if its co-
occurred word(s) forms a collocation with it and if 
so, also annotate the collocation?s dependency, 
chunking and classification information. The anno-
tation procedure, requires three passes. We use a 
headword ??/an (safe), as an illustrative exam-
ple.  
Pass 1. Concordance and dependency identifica-
tion 
In the first pass, the concordance of the given 
headword is performed. Sentences containing the 
headwords are obtained, e.g.  
S1: ??/v [??/v  ??/an]BVP  ?/u  ??/n 
(follow the principles for ensuring the safety) 
S2: ??/v [??/n ??/n]BNP ?/u[??/n ??/n ?
?/an]BNP 
(ensure life and property safety of people) 
S3: ??/v  ??/ns  [??/an  ??/v]BVP 
(ensure the flood pass through Yangzi River safely) 
With the help of an automatic dependency pars-
er, the annotator determines all syntactically and 
semantically dependent words in the chunking con-
text of the observing headword. The annotation 
output of S1 is given below in which XML tags are 
used for the dependency annotation.  
S1:<sentence>??/v [??/v  ??/an]BVP  ?/u  ??/n 
64
<labeled> [W1][O_O_N][O]??/v [W2][B_BVP_H][C1]
??/v [W3][E_BNP_N][C1]??/an [W4][O_O_N][O]?
/u  [W5][O_O_N][O]??/n </labeled> 
<dependency no="1" observing="??/an" head="??
/v" head_wordid="W2" head_chunk ="B_BVP_H" 
head_chunkid="C1" modifier=" ? ? /an" modi-
fier_wordid="W3" modifier _chunk="E_BVP_N" 
modifer_chunkid="C1" relation="VO" > </dependency> 
</sentence> 
Dependency of word combination is annotated 
with the tag <dependency> which includes the fol-
lowing attributes: 
-<dependency> indicates an identified depend-
ency   
-no is the id of identified dependency within cur-
rent sentence according to ordinal sequence 
-observing indicates the current observing 
headword 
-head indicates the head of the identified word 
dependency 
-head_wordid is the word id of the head 
-head_chunk is the hybrid tags for labeling the 
chunking information of the head 
-head_chunkid is the chunk id of the head 
-modifier indicates the modifier of the identified 
dependency 
-modifier_wordid is the word id of the modifier 
-modifier_chunk is the hybrid tags for labeling 
chunking information of the modifier 
-modifier_chunkid is the chunk id of the modi-
fier 
-relation gives the syntactic dependency rela-
tions labeled according to the dependency labels 
listed in Table 1. 
In S1 and S2, the word combination ??/v??
/an has direct dependency, and in S3, such a de-
pendency does not exist as??/v only determines
??/v and ??/an depends on ??/v. The qual-
ity of CCB highly depends on the accuracy of de-
pendency annotation. This is very important for 
effective characteristics analysis of collocations 
and for the collocation extraction algorithms.  
Pass 2. N-gram collocations annotation 
It is relatively easy to identify n-gram colloca-
tions since an n-gram collocation is of habitual and 
recurrent use of a series of bi-grams. This means 
that n-gram collocations can be identified by find-
ing consecutive occurrence of significant bi-grams 
in certain position. In the second pass, the annota-
tors focus on the sentences where the headword 
has more than one dependency. The percentage of 
all appearances of each dependent word at each 
position around the headword is estimated with the 
help of a program (Xu and Lu, 2006). Finally, 
word dependencies frequently co-occurring in con-
secutive positions in a fixed order are extracted as 
n-gram collocations.   
For the headword, an n-gram collocation??/n 
? ? /n ? ? /an is identified since the co-
occurrence percentage of dependency??/-NN-?
?/an and dependency??/n-NN-??/an is 0.74 
is greater than a empirical threshold suggest in (Xu 
and Lu, 2006). This n-gram is annotated in S2 as 
follows: 
<ncolloc observing="??/an" w1="??/n" w2="??/n" 
w3="??/an" start_wordid="5"> </ncolloc> 
where, 
-<ncolloc> indicates an n-gram collocation  
-w1, w2,..wn give the components of the n-gram 
collocation according to the ordinal sequence.  
-start_wordid  indicates the word id of the first 
component of the n-gram collocation. 
Since n-gram collocation is regarded as a whole, 
its internal dependencies are ignored in the output 
file of pass 2. That is, if the dependencies of sev-
eral components are associated with an n-gram 
collocation in one sentence, the n-gram collocation 
is annotated and these dependencies are filtered out 
so as not to disturb the bi-gram dependencies.  
Pass 3. Bi-gram collocations annotation 
In this pass, all the word dependencies are ex-
amined to identify bi-gram collocations. Further-
more, if a dependent word combination is regarded 
as a collocation by the annotators, it will be further 
labeled based on the type determined. The identifi-
cation is based on expert knowledge combined 
with the use of several computational features as 
discussed in (Xu and Lu, 2006). 
An assistive tool is developed to estimate the 
computational features. We use the program to ob-
tain feature data based on two sets of data. The 
first data set is the annotated dependencies in the 
5-million-word corpus which is obtained through 
Pass 1 and Pass 2 annotations. Because the de-
pendent word combinations are manually identi-
fied and annotated in the first pass, the statistical 
significance is helpful to identify whether the word 
combination is a collocation and to determine its 
type. However, data sparseness problem must be 
considered since 5-million-word is not large 
enough. Thus, another set of statistical data are 
65
collected from a 100-million segmented and tagged 
corpus (Xu and Lu, 2006). With this large corpus, 
data sparseness is no longer a serious problem. But, 
the collected statistics are quite noisy since they 
are directly retrieved from text without any verifi-
cation. By analyzing the statistical features from 
both sets, the annotator can use his/her professional 
judgment to determine whether a bi-gram is a col-
location and its collocation type. 
In the example sentences, two collocations are 
identified. Firstly, ??/an ??/v is classified as a 
Type 1 collocation as they have only one peak co-
occurrence, very low substitution ratio and their 
co-occurrence order nearly never altered. Secondly, 
??/v ??/an is identified as a collocation. They 
have frequent co-occurrences and they are always 
co-occurred in fixed order among the verified de-
pendencies. However, their co-occurrences are dis-
tributed evenly and they have two peak co-
occurrences. Therefore, ??/v ??/an is classi-
fied as a Type 3 collocation. These bi-gram collo-
cations are annotated as illustrated below, 
<bcolloc observing="??/an" col="??/v" head="??
/v" type= "1" relation="ADV">  
<dependency no="1" observing="??/an" head="??/v" 
head_wordid="W4" head_chunk ="E_BVP_H" 
head_chunkid="C1" modifier=" ? ? /an" modi-
fier_wordid="W3" modifier _chunk="B_BVP_N" 
modifer_chunkid="C1" relation="ADV" 
></dependency></bcolloc> 
where, 
-<bcolloc> indicates a bi-gram collocation. 
-col is for  the collocated word. 
-head indicates the head of an identified colloca-
tion 
-type is the classified collocation type. 
-relation gives the syntactic dependency rela-
tions of this bi-gram collocation. 
Note that the dependency annotations within the 
bi-gram collocations are reserved. 
4.4 Quality Assurance 
The annotators of CCB are three post-graduate stu-
dents majoring in linguistics. In the first annotation 
stage, 20% headwords of the whole set was anno-
tated in duplicates by all three of them. Their out-
puts were checked by a program. Annotated collo-
cation including classified dependencies and types 
accepted by at least two annotators are reserved in 
the final data as the Golden Standard while the 
others are considered incorrect. The inconsisten-
cies between different annotators were discussed to 
clarify any misunderstanding in order to come up 
with the most appropriate annotations. In the sec-
ond annotation stage, 80% of the whole annota-
tions were then divided into three parts and sepa-
rately distributed to the annotators with 5% dupli-
cate headwords were distributed blindly. The du-
plicate annotation data were used to estimate the 
annotation consistency between annotators.  
5 Collocation Characteristic Analysis 
5.1 Progress and Quality of CCB 
Up to now, the first version of CCB is completed. 
We have obtained 23,581 unique bi-gram colloca-
tions and 2,752 unique n-gram collocations corre-
sponding to the 3,643 observing headwords. 
Meanwhile, their occurrences in the corpus are an-
notated and verified. With the help of a computer 
program, the annotators manually classified bi-
gram collocations into three types. The numbers of 
Type 0/1, Type 2 and Type 3 collocations are 152, 
3,982 and 19,447, respectively.  
For the 3,643 headwords in The Dictionary of 
Modern Chinese Collocations (Mei 1999) with 
35,742 bi-gram collocations,  20,035 collocations 
appear in the corpus. We call this collection as 
Mei?s Collocation Collection (MCC). There are 
19,967 common entries in MCC and CCB, which 
means 99.7% collocations in MCC appear in CCB 
indicating a good linguistic consistency. Further-
more, 3,614 additional collocations are found in 
CCB which enriches the static collocation diction-
ary.
 
5.2 Dependencies Numbers Statistics of Col-
locations 
Firstly, we study the statistics of how many types 
of dependencies a bi-gram collocation may have. 
The numbers of dependency types with respect to 
different collocation types are listed in Table 3.  
 
Collocations 1 type 2 types >2 types Total 
Type 0/1 152 0 0 152 
Type 2 3970 12 0 3982 
Type 3 17282 2130 35 19447 
Total 21404 2142 35 23581 
Table 3. Collocation classification versus number 
of dependency types 
66
It is observed that about 90% bi-gram collocations 
have only one dependency type. This indicates that 
a collocation normally has only one fixed syntactic 
dependency. It is also observed that about 10% bi-
gram collocations have more than one dependency 
type, especially Type 3 collocations. For example, 
two types of dependencies are identified in the bi-
gram collocation ??/an-??/n. They are ??
/an-AN-??/n (a safe nation) which indicates the 
dependency of a noun and its nominal modifier 
where ??/n serves as the head, and??/n-NN-
?? /an (national security) which indicates the 
dependency of a noun and its nominal modifier 
where ??/an serves as the head. It is attributed to 
the fact that the use of Chinese words is flexible. A 
Chinese word may support different part-of-speech. 
A collocation with different dependencies results 
in different distribution trends and most of these 
collocations are classified as Type 3. On the other 
hand, Type 0/1 and Type 2 collocations seldom 
have more than one dependency type. 
5.3 Syntactic Dependency Statistics of Collo-
cations 
The statistics of the 10 types of syntactic depend-
encies with respect to different types of bi-gram 
collocations are shown in Table 4. No. is the num-
ber of collocations with a given dependency type  
D and a given collocation type T. The percentage 
of No. among all collocations with the same collo-
cation type T is labeled as P_T, and the percentage 
of No. among all of the collocations with the same 
dependency D is labeled as P_D. 
 
 Type 0/1  Type 2  Type 3  Total 
 No. P_T P_D No. P_T P_D No. P_T P_D No. P_T
ADA 1 0.7 0.1 212 5.3 11.5 1637 7.6 88.5 1850 7.2
ADV 9 5.9 0.3 322 8.1 11.2 2555 11.8 88.5 2886 11.2
AN 20 13.2 0.4 871 21.8 15.4 4771 22.0 84.3 5662 22.0
CMP 12 7.9 2.2 144 3.6 26.9 379 1.8 70.8 535 2.1
NJX 8 5.3 3.2 42 1.1 16.9 198 0.9 79.8 248 1.0
NN 44 28.9 0.9 1036 25.9 21.6 3722 17.2 77.5 4802 18.6
SBV 4 2.6 0.2 285 7.1 11.1 2279 10.5 88.7 2568 10.0
VO 26 17.1 0.5 652 16.3 12.5 4545 21.0 87.0 5223 20.2
VV 3 2.0 0.2 227 5.7 13.4 1464 6.8 86.4 1694 6.6
OT 25 16.4 7.7 203 5.1 62.5 97 0.4 29.8 325 1.3
Total 152 100.0 0.6 3994 100.0 15.5 21647 100.0 83.9 25793 100.0
Table 4. The statistics of collocations with dif-
ferent collocation type and dependency 
 
Corresponding to 23,581 bi-gram collocations, 
25,793 types of dependencies are identified (some 
collocations have more than one types of depend-
ency). In which, about 82% belongs to five major 
dependency types. They are AN, VO, NN, ADV and 
SBV. It is note-worthy that the percentage of NN 
collocation is much higher than that in English. 
This is because nouns are more often used in paral-
lel to serve as one syntactic component in Chinese 
sentences than in English. 
The percentages of Type 0/1, Type 2 and Type 3 
collocations in CCB are 0.6%, 16.9% and 82.5%, 
respectively. However, the collocations with dif-
ferent types of dependencies have shown their own 
characteristics with respect to different collocation 
types. The collocations with CMP, NJX and NN 
dependencies on average have higher percentage to 
be classified into Type 0/1 and Type 2 collocations. 
This indicates that CMP, NJX and NN collocations 
in Chinese are always used in fixed patterns and 
these kinds of collocations are not freely modifi-
able and substitutable. In the contrary, many ADV 
and AN collocations are classified as Type 3. This 
is partially due to the special usage of auxiliary 
words in Chinese.  Many AN Chinese collocations 
can be inserted by a meaningless auxiliary word?
/u and many ADV Chinese collocations can be in-
serted by an auxiliary word?/u. This means that 
many AN and ADV collocations can be modified 
and thus, they always have two peak co-
occurrences. Therefore, they are classified as Type 
3 collocations. 7.7% and 62.5% of the collocations 
with dependency OT are classified as Type 0/1 and 
Type2 collocations, respectively. Such percentages 
are much higher than the average. This is attributed 
by the fact that some Type 0/1 and Type 2 colloca-
tions have strong semantic relations rather than 
syntactic relations and thus their dependencies are 
difficult to label. 
5.4 Chunking Statistics of Collocations 
The chunking characteristic for the collocations 
with different types and different dependencies are 
examined. In most cases, Type 0/1/2 collocations 
co-occur within one chunk or between neighboring 
chunks. Therefore, their chunking characteristics 
are not discussed in detail. The percentage of the 
occurrences of Type 3 collocations with different 
chunking distances are given in Table 5. If a collo-
cation co-occurs within one chunk, the chunking 
distance is 0. If a collocation co-occurs between 
neighboring chunks, or between neighboring words, 
or between a word and a neighboring chunk, the 
chunking distance is 1, and so on. 
67
 
 ADA ADV AN CMP NJX NN SBV VO VV OT
0 chunk 56.8 53.1 65.7 48.5 70.2 62.4 46.5 41.1 47.2 86.4
1 chunk 38.2 43.7 28.5 37.2 15.4 27.9 41.2 35.7 41.1 13.5
2 chunks 5.0 3.2 3.7 14.2 14.4 9.7 11.0 17.6 9.6 0.1
>2chunks 0.0 0.0 2.1 0.1 0.0 0.0 1.3 5.6 2.1 0.0
Table 5. Chunking distances of Type 3 collocations  
 
It is shown that the co-occurrence of collocations 
decreases with increased chunking distance. Yet, 
the behavior for decrease is different for colloca-
tions with different dependencies. Generally speak-
ing, the ADA, ADV, CMP, NJX, NN and OT collo-
cations seldom co-occur cross two words or two 
chunks. Furthermore, the occurrences of AN, NJX 
and OT collocations quickly drops when the 
chunking distance is greater than 0, i.e. these col-
locations tends to co-occur within the same chunk. 
In the contrary, the co-occurrences of ADA, ADV, 
CMP, SBV and VV collocations corresponding to 
chunking distance equals 0 and 1 decrease steadily. 
It means that these four kinds of collocations are 
more evenly distributed within the same chunk or 
between neighboring words or chunks. The occur-
rences of VO collocations corresponding to chunk-
ing distance from 0 to 3 with a much flatter reduc-
tion. This indicates that a verb may govern its ob-
ject in a long range. 
6 Conclusions 
This paper describes the design and construction of 
a manually annotated Chinese collocation bank. 
Following a set of well-designed annotation guide-
line, the collocations corresponding to 3,643 
headwords are identified from a chunked five-
million word corpus. 2,752 unique n-gram colloca-
tions and 23,581 unique bi-gram collocations are 
annotated. Furthermore, each bi-gram collocation 
is annotated with its syntactic dependency informa-
tion, classification information and chunking in-
formation. Based on CCB, characteristics of collo-
cations with different types and different depend-
encies are examined. The obtained result is essen-
tial for improving research related to Chinese col-
location. Also, CCB may be used as a standard an-
swer set for evaluating the performance of differ-
ent collocation extraction algorithms. In the future, 
collocations of all unvisited headwords will be an-
notated to produce a complete 5-million-word Chi-
nese collocation bank. 
Acknowledgement 
This research is supported by The Hong Kong 
Polytechnic University (A-P203), CERG Grant 
(5087/01E) and Chinese University of Hong Kong 
under the Direct Grant Scheme project (2050330) 
and Strategic Grant Scheme project (4410001). 
References 
Bedin N. et al 2003. The Annotation Process in the 
Turkish Treebank. In Proc. 11th Conference of the 
EACL-4th Linguistically Interpreted Corpora Work-
shop- LINC. 
Kosho S. et al 2000. Collocations as Word Co-
occurrence Restriction Data - An Application to 
Japanese Word Processor. In Proc. Second Interna-
tional Conference on Language Resources and 
Evaluation 
Lin D.K. 1998. Extracting collocations from text cor-
pora. In Proc. First Workshop on Computational 
Terminology, Montreal  
Manning, C.D., Sch?tze, H. 1999: Foundations of Sta-
tistical Natural Language Processing, MIT Press  
Mei J.J. 1999. Dictionary of Modern Chinese Colloca-
tions, Hanyu Dictionary Press  
Pecina P. 2005. An Extensive Empirical Study of Collo-
cation Extraction Methods. In Proc. 2005 ACL Stu-
dent Research Workshop. 13-18 
Smadja. F. 1993. Retrieving collocations from text: 
Xtract, Computational Linguistics. 19. 1.  143-177 
Tutin A. 2005. Annotating Lexical Functions in Corpora: 
Showing Collocations in Context. In Proc. 2nd Inter-
national Conference on the Meaning ? Text Theory 
Xu R. F. and Lu Q. 2005. Improving Collocation Ex-
traction by Using Syntactic Patterns, In Proc. IEEE 
International Conference on Natural Language Proc-
essing and Knowledge Engineering. 52-57 
Xu, R.F. and Lu, Q. 2006. A Multi-stage Chinese Col-
location Extraction System. Lecture Notes in Com-
puter Science, Vol. 3930, Springer-Verlag. 740-749 
Yu S.W. et al 2001. Guideline of People?s Daily Cor-
pus Annotation, Technical Report, Peking University  
 
 
68
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618?624,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Automatic Labelling of Topic Models Learned from Twitter by
Summarisation
Amparo Elizabeth Cano Basave
?
Yulan He
?
Ruifeng Xu
?
?
Knowledge Media Institute, Open University, UK
?
School of Engineering and Applied Science, Aston University, UK
?
Key Laboratory of Network Oriented Intelligent Computation
Shenzhen Graduate School, Harbin Institute of Technology, China
amparo.cano@open.ac.uk, y.he@cantab.net, xuruifeng@hitsz.edu.cn
Abstract
Latent topics derived by topic models such
as Latent Dirichlet Allocation (LDA) are
the result of hidden thematic structures
which provide further insights into the
data. The automatic labelling of such
topics derived from social media poses
however new challenges since topics may
characterise novel events happening in the
real world. Existing automatic topic la-
belling approaches which depend on exter-
nal knowledge sources become less appli-
cable here since relevant articles/concepts
of the extracted topics may not exist in ex-
ternal sources. In this paper we propose
to address the problem of automatic la-
belling of latent topics learned from Twit-
ter as a summarisation problem. We in-
troduce a framework which apply sum-
marisation algorithms to generate topic la-
bels. These algorithms are independent
of external sources and only rely on the
identification of dominant terms in doc-
uments related to the latent topic. We
compare the efficiency of existing state
of the art summarisation algorithms. Our
results suggest that summarisation algo-
rithms generate better topic labels which
capture event-related context compared to
the top-n terms returned by LDA.
1 Introduction
Topic model based algorithms applied to social
media data have become a mainstream technique
in performing various tasks including sentiment
analysis (He, 2012) and event detection (Zhao et
al., 2012; Diao et al, 2012). However, one of
the main challenges is the task of understanding
the semantics of a topic. This task has been ap-
proached by investigating methodologies for iden-
tifying meaningful topics through semantic coher-
ence (Aletras and Stevenson, 2013; Mimno et al,
2011; Newman et al, 2010) and for characterising
the semantic content of a topic through automatic
labelling techniques (Hulpus et al, 2013; Lau et
al., 2011; Mei et al, 2007). In this paper we focus
on the latter.
Our research task of automatic labelling a topic
consists on selecting a set of words that best de-
scribes the semantics of the terms involved in this
topic. The most generic approach to automatic la-
belling has been to use as primitive labels the top-
n words in a topic distribution learned by a topic
model such as LDA (Griffiths and Steyvers, 2004;
Blei et al, 2003). Such top words are usually
ranked using the marginal probabilities P (w
i
|t
j
)
associated with each word w
i
for a given topic t
j
.
This task can be illustrated by considering the fol-
lowing topic derived from social media related to
Education:
school protest student fee choic motherlod
tuition teacher anger polic
where the top 10 words ranked by P (w
i
|t
j
) for
this topic are listed. Therefore the task is to find
the top-n terms which are more representative of
the given topic. In this example, the topic certainly
relates to a student protest as revealed by the top
3 terms which can be used as a good label for this
topic.
However previous work has shown that top
terms are not enough for interpreting the coherent
meaning of a topic (Mei et al, 2007). More re-
cent approaches have explored the use of external
sources (e.g. Wikipedia, WordNet) for supporting
the automatic labelling of topics by deriving can-
didate labels by means of lexical (Lau et al, 2011;
Magatti et al, 2009; Mei et al, 2007) or graph-
based (Hulpus et al, 2013) algorithms applied on
these sources.
Mei et al (2007) proposed an unsupervised
probabilistic methodology to automatically assign
a label to a topic model. Their proposed approach
618
was defined as an optimisation problem involving
the minimisation of the KL divergence between a
given topic and the candidate labels while max-
imising the mutual information between these two
word distributions. Lau et al (2010) proposed to
label topics by selecting top-n terms to label the
overall topic based on different ranking mecha-
nisms including pointwise mutual information and
conditional probabilities.
Methods relying on external sources for auto-
matic labelling of topics include the work by Ma-
gatti et al (2009) which derived candidate topic
labels for topics induced by LDA using the hi-
erarchy obtained from the Google Directory ser-
vice and expanded through the use of the OpenOf-
fice English Thesaurus. Lau et al (2011) gen-
erated label candidates for a topic based on top-
ranking topic terms and titles of Wikipedia arti-
cles. They then built a Support Vector Regres-
sion (SVR) model for ranking the label candidates.
More recently, Hulpus et al (2013) proposed to
make use of a structured data source (DBpedia)
and employed graph centrality measures to gener-
ate semantic concept labels which can characterise
the content of a topic.
Most previous topic labelling approaches focus
on topics derived from well formatted and static
documents. However in contrast to this type of
content, the labelling of topics derived from tweets
presents different challenges. In nature microp-
ost content is sparse and present ill-formed words.
Moreover, the use of Twitter as the ?what?s-
happening-right now? tool, introduces new event-
dependent relations between words which might
not have a counter part in existing knowledge
sources (e.g. Wikipedia). Our original interest in
labelling topics stems from work in topic model
based event extraction from social media, in par-
ticular from tweets (Shen et al, 2013; Diao et
al., 2012). As opposed to previous approaches,
the research presented in this paper addresses the
labelling of topics exposing event-related content
that might not have a counter part on existing ex-
ternal sources. Based on the observation that a
short summary of a collection of documents can
serve as a label characterising the collection, we
propose to generate topic label candidates based
on the summarisation of a topic?s relevant docu-
ments. Our contributions are two-fold:
- We propose a novel approach for topics la-
belling that relies on term relevance of documents
relating to a topic; and
- We show that summarisation algorithms,
which are independent of extenal sources, can be
used with success to label topics, presenting a
higher perfomance than the top-n terms baseline.
2 Methodology
We propose to approach the topic labelling prob-
lem as a multi-document summarisation task. The
following describes our proposed framework to
characterise documents relevant to a topic.
2.1 Preliminaries
Given a set of documents the problem to be solved
by topic modelling is the posterior inference of the
variables, which determine the hidden thematic
structures that best explain an observed set of doc-
uments. Focusing on the Latent Dirichlet Alloca-
tion (LDA) model (Blei et al, 2003; Griffiths and
Steyvers, 2004), let D be a corpus of documents
denoted as D = {d
1
,d
2
, ..,d
D
}; where each doc-
ument consists of a sequence ofN
d
words denoted
by d = (w
1
, w
2
, .., w
N
d
); and each word in a
document is an item from a vocabulary index of
V different terms denoted by {1, 2, .., V }. Given
D documents containing K topics expressed over
V unique words, LDA generative process is de-
scribed as follows:
- For each topic k ? {1, ...K} draw ?
k
?
Dirichlet(?),
- For each document d ? {1..D}:
? draw ?
d
? Dirichlet(?);
? For each word n ? {1..N
d
} in document d:
? draw a topic z
d,n
? Multinomial(?
d
);
? draw a word w
d,n
? Multinomial(?
z
d,n
).
where ?
k
is the word distribution for topic k,
and ?
d
is the distribution of topics in document
d. Topics are interpreted using the top N terms
ranked based on the marginal probability p(w
i
|t
j
).
2.2 Automatic Labelling of Topic Models
Given K topics over the document collection D,
the topic labelling task consists on discovering a
sequence of words for each topic k ? K. We pro-
pose to generate topic label candidates by sum-
marising topic relevant documents. Such docu-
ments can be derived using both the observed data
from the corpus D and the inferred topic model
variables. In particular, the prominent topic of a
document d can be found by
k
d
= argmax
k?K
p(k|d) (1)
619
Therefore given a topic k, a set of C documents
related to this topic can be obtained via equation
1.
Given the set of documents C relevant to topic k,
we proposed to generate a label of a desired length
x from the summarisation of C.
2.3 Topic Labelling by Summarisation
We compare different summarisation algorithms
based on their ability to provide a good label to a
given topic. In particular we investigate the use of
lexical features by comparing three different well-
known multi-document summarisation algorithms
against the top-n topic terms baseline. These al-
gorithms include:
Sum Basic (SB) This is a frequency based sum-
marisation algorithm (Nenkova and Vanderwende,
2005), which computes initial word probabilities
for words in a text. It then weights each sen-
tence in the text (in our case a micropost) by
computing the average probability of the words in
the sentence. In each iteration it picks the high-
est weighted document and from it the highest
weighted word. It uses an update function which
penalises words which have already been picked.
Hybrid TFIDF (TFIDF) It is similar to SB,
however rather than computing the initial word
probabilities based on word frequencies it weights
terms based on TFIDF. In this case the document
frequency is computed as the number of times a
word appears in a micropost from the collection
C. Following the same procedure as SB it returns
the top x weighted terms.
Maximal Marginal Relevance (MMR) This is a
relevance based ranking algorithm (Carbonell and
Goldstein, 1998), which avoids redundancy in the
documents used for generating a summary. It mea-
sures the degree of dissimilarity between the docu-
ments considered and previously selected ones al-
ready in the ranked list.
Text Rank (TR) This is a graph-based sum-
mariser method (Mihalcea and Tarau, 2004) where
each word is a vertex. The relevance of a vertex
(term) to the graph is computed based on global
information recursively drawn from the whole
graph. It uses the PageRank algorithm (Brin and
Page, 1998) to recursively change the weight of
the vertices. The final score of a word is there-
fore not only dependent on the terms immediately
connected to it but also on how these terms con-
nect to others. To assign the weight of an edge
between two terms, TextRank computes word co-
occurrence in windows of N words (in our case
N = 10). Once a final score is calculated for each
vertex of the graph, TextRank sorts the terms in
a reverse order and provided the top T vertices in
the ranking. Each of these algorithms produces a
label of a desired length x for a given topic k.
3 Experimental Setup
3.1 Dataset
Our Twitter Corpus (TW) was collected between
November 2010 and January 2011. TW comprises
over 1 million tweets. We used the OpenCalais?
document categorisation service
1
to generate cate-
gorical sets. In particular, we considered four dif-
ferent categories which contain many real-world
events, namely: War and Conflict (War), Disaster
and Accident (DisAc), Education (Edu) and Law
and Crime (LawCri). The final TW dataset after
removing retweets and short microposts (less than
5 words after removing stopwords) contains 7000
tweets in each category.
We preprocessed TW by first removing: punc-
tuation, numbers, non-alphabet characters, stop
words, user mentions, and URL links. We then
performed Porter stemming (Porter, 1980) in order
to reduce the vocabulary size. Finally to address
the issue of data sparseness in the TW dataset, we
removed words with a frequency lower than 5.
3.2 Generating the Gold Standard
Evaluation of automatic topic labelling often re-
lied on human assessment which requires heavy
manual effort (Lau et al, 2011; Hulpus et al,
2013). However performing human evaluations of
Social Media test sets comprising thousands of in-
puts become a difficult task. This is due to both
the corpus size, the diversity of event-related top-
ics and the limited availability of domain experts.
To alleviate this issue here, we followed the distri-
bution similarity approach, which has been widely
applied in the automatic generation of gold stan-
dards (GSs) for summary evaluations (Donaway et
al., 2000; Lin et al, 2006; Louis and Nenkova,
2009; Louis and Nenkova, 2013). This approach
compares two corpora, one for which no GS labels
exist, against a reference corpus for which a GS
exists. In our case these corpora correspond to the
TW and a Newswire dataset (NW). Since previous
1
OpenCalais service, http://www.opencalais.com
620
research has shown that headlines are good indi-
cators of the main focus of a text, both in struc-
ture and content, and that they can act as a human
produced abstract (Nenkova, 2005), we used head-
lines as the GS labels of NW.
The News Corpus (NW) was collected during
the same period of time as the TW corpus. NW
consists of a collection of news articles crawled
from traditional news media (BBC, CNN, and
New York Times) comprising over 77,000 articles
which include supplemental metadata (e.g. head-
line, author, publishing date). We also used the
OpenCalais? document categorisation service to
automatically label news articles and considered
the same four topical categories, (War, DisAc,
Edu and LawCri). The same preprocessing steps
were performed on NW.
Therefore, following a similarity alignment ap-
proach we performed the steps oulined in Algo-
rithm 1 for generating the GS topic labels of a topic
in TW.
Algorithm 1 GS for Topic Labels
Input: LDA topics for TW, and the LDA topics for NW for
category c.
Output: Gold standard topic label for each of the LDA top-
ics for TW.
1: for each topic i ? {1, 2, ..., 100} from TW do
2: for each topic j ? {1, 2..., 100} from NW do
3: Compute the Cosine similarity between word dis-
tributions of topic t
i
and topic t
j
.
4: end for
5: Select topic j which has the highest similarity to i and
whose similarity measure is greater than a threshold
(in this case 0.7)
6: end for
7: for each of the extracted topic pairs (t
i
? t
j
) do
8: Collect relevant news articles C
j
NW
of topic t
j
from
the NW set.
9: Extract the headlines of news articles from C
j
NW
and
select the top x most frequent words as the gold stan-
dard label for topic t
i
in the TW set
10: end for
These steps can be outlined as follows:1) We
ran LDA on TW and NW separately for each cate-
gory with the number of topics set to 100; 2) We
then aligned the Twitter topics and Newswire top-
ics by the similarity measurement of word distri-
butions of these topics (Ercan and Cicekli, 2008;
Haghighi and Vanderwende, 2009; Wang et al,
2009; Delort and Alfonseca, 2012); 3) Finally to
generate the GS label for each aligned topic pair
(t
i
? t
j
), we extracted the headlines of the news
articles relevant to t
j
and selected the top x most
frequent words (after stop word removal and stem-
ming). The generated label was used as the gold
standard label for the corresponding Twitter topic
t
i
in the topic pair.
4 Experimental Results
We compared the results of the summarisation
techniques with the top terms (TT) of a topic as
our baseline. These TT set corresponds to the
top x terms ranked based on the probability of
the word given the topic (p(w|k)) from the topic
model. We evaluated these summarisation ap-
proaches with the ROUGE-1 method (Lin, 2004),
a widely used summarisation evaluation metric
that correlates well with human evaluation (Liu
and Liu, 2008). This method measures the over-
lap of words between the generated summary and
a reference, in our case the GS generated from the
NW dataset.
The evaluation was performed at x =
{1, .., 10}. Figure 1 presents the ROUGE-1 per-
formance of the summarisation approaches as the
lengthx of the generated topic label increases. We
can see in all four categories that the SB and
TFIDF approaches provide a better summarisa-
tion coverage as the length of the topic label in-
creases. In particular, in both the Education
and Law & Crime categories, both SB and
TFIDF outperforms TT and TR by a large margin.
The obtained ROUGE-1 performance is within the
same range of performance previously reported on
Social Media summarisation (Inouye and Kalita,
2011; Nichols et al, 2012; Ren et al, 2013).
Table 1 presents average results for ROUGE-
1 in the four categories. Particularly the SB
and TFIDF summarisation techniques consis-
tently outperform the TT baseline across all four
categories. SB gives the best results in three cate-
gories except War.
ROUGE-1
TT SB TFIDF MMR TR
War 0.162 0.184 0.192 0.154 0.141
DisAc 0.134 0.194 0.160 0.132 0.124
Edu 0.106 0.240 0.187 0.104 0.023
LawCri 0.035 0.159 0.149 0.034 0.115
Table 1: Average ROUGE-1 for topic labels at x =
{1..10}, generated from the TW dataset.
The generated labels with summarisation at x =
5 are presented in Table 2, where GS represents the
label generated from the Newswire headlines.
Different summarisation techniques reveal
words which do not appear in the top terms but
621
0.05
0.10
0.15
0.20
0.25
2.5 5.0 7.5 10.0
x
Ro
uge
War_Conflict
0.10
0.15
0.20
0.25
2.5 5.0 7.5 10.0
x
Ro
uge
Disaster_Accident
0.1
0.2
2.5 5.0 7.5 10.0
x
Ro
uge
Education
0.00
0.05
0.10
0.15
0.20
2.5 5.0 7.5 10.0
x
Ro
uge
Law_Crime
Tw
itte
r To
pic
s
variable
TT
SB
TFIDF
TR
MMR
Figure 1: Performance in ROUGE for Twitter-derived topic labels, where x is the number of terms in the
generated label
which are relevant to the information clustered
by the topic. In this way, the labels generated for
topics belonging to different categories generally
extend the information provided by the top terms.
For example in Table 2, the DisAc headline is
characteristic of the New Zealand?s Pike River?s
coal mine blast accident, which is an event
occurred in November 2010.
Although the top 5 terms set from the LDA topic
extracted from TW (listed under TT) does capture
relevant information related to the event, it does
not provide information regarding the blast. In this
sense the topic label generated by SB more accu-
rately describes this event.
We can also notice that the GS labels generated
from Newswire media presented in Table 2 appear
on their own, to be good labels for the TW topics.
However as we described in the introduction we
want to avoid relaying on external sources for the
derivation of topic labels.
This experiment shows that frequency based
summarisation techniques outperform graph-
based and relevance based summarisation
techniques for generating topic labels that im-
prove upon the top-terms baseline, without relying
on external sources. This is an attractive property
for automatically generating topic labels for
tweets where their event-related content might not
have a counter part on existing external sources.
5 Conclusions and Future Work
In this paper we proposed a novel alternative to
topic labelling which do not rely on external data
sources. To the best of out knowledge no existing
work has been formally studied for automatic la-
belling through summarisation. This experiment
shows that existing summarisation techniques can
be exploited to provide a better label of a topic,
extending in this way a topic?s information by pro-
War DisAc
GS protest brief polic
afghanistan attack world
leader bomb obama
pakistan
mine zealand rescu miner
coal fire blast kill man dis-
ast
TT polic offic milit recent
mosqu
mine coal pike river
zealand
SB terror war polic arrest offic mine coal explos river pike
TFIDF polic war arrest offic terror mine coal pike safeti
zealand
MMR recent milit arrest attack
target
trap zealand coal mine ex-
plos
TR war world peac terror hope mine zealand plan fire fda
Edu LawCri
GS school protest student fee
choic motherlod tuition
teacher anger polic
man charg murder arrest
polic brief woman attack
inquiri found
TT student univers protest oc-
cupi plan
man law child deal jail
SB student univers school
protest educ
man arrest law kill judg
TFIDF student univers protest
plan colleg
man arrest law judg kill
MMR nation colleg protest stu-
dent occupi
found kid wife student jail
TR student tuition fee group
hit
man law child deal jail
Table 2: Labelling examples for topics generated
from the TW Dataset. GS represents the gold-
standard generated from the relevant Newswire
dataset. All terms are Porter stemmed as described
in subsection 3.1
viding a richer context than top-terms. These re-
sults show that there is room to further improve
upon existing summarisation techniques to cater
for generating candidate labels.
Acknowledgments
This work was supported by the EPRSC grant
EP/J020427/1, the EU-FP7 project SENSE4US
(grant no. 611242), and the Shenzhen Interna-
tional Cooperation Research Funding (grant num-
ber GJHZ20120613110641217).
622
References
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-
ing topic coherence using distributional semantics.
In Proceedings of the 10th International Conference
on Computational Semantics (IWCS 2013) ? Long
Papers, pages 13?22, Potsdam, Germany, March.
Association for Computational Linguistics.
David Meir Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. In J. Mach. Learn.
Res. 3, pages 993?1022.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine*
1. In Computer networks and ISDN systems, vol-
ume 30, pages 107?117.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?98, pages 335?336, New York,
NY, USA. ACM.
Jean-Yves Delort and Enrique Alfonseca. 2012. Dual-
sum: A topic-model based approach for update sum-
marization. In Proceedings of the 13th Conference
of the European Chapter of the Association for Com-
putational Linguistics, EACL ?12, pages 214?223,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.
2012. Finding bursty topics from microblogs. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 536?544, Jeju Island, Korea,
July. Association for Computational Linguistics.
Robert L. Donaway, Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced
by summarization evaluation measures. In Proceed-
ings of the 2000 NAACL-ANLP Workshop on Au-
tomatic Summarization, NAACL-ANLP-AutoSum
?00, pages 69?78, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Gonenc Ercan and Ilyas Cicekli. 2008. Lexical co-
hesion based topic modeling for summarization. In
Proceedings of the 9th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, CICLing?08, pages 582?592, Berlin, Hei-
delberg. Springer-Verlag.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL ?09, pages 362?370,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Yulan He. 2012. Incorporating sentiment prior
knowledge for weakly supervised sentiment analy-
sis. ACM Transactions on Asian Language Infor-
mation Processing, 11(2):4:1?4:19, June.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. 2013. Unsupervised graph-based
topic labelling using dbpedia. In Proceedings of the
sixth ACM international conference on Web search
and data mining, WSDM ?13, pages 465?474, New
York, NY, USA. ACM.
David Inouye and Jugal K. Kalita. 2011. Comparing
twitter summarization algorithms for multiple post
summaries. In SocialCom/PASSAT, pages 298?306.
IEEE.
Jey Han Lau, David Newman, Karimi Sarvnaz, and
Timothy Baldwin. 2010. Best Topic Word Selec-
tion for Topic Labelling. CoLing.
Jey Han Lau, Karl Grieser, David Newman, and Tim-
othy Baldwin. 2011. Automatic labelling of topic
models. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 1536?1545, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and
Jian-Yun Nie. 2006. An information-theoretic
approach to automatic evaluation of summaries.
In Proceedings of the Main Conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL ?06, pages 463?
470, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Feifan Liu and Yang Liu. 2008. Correlation between
rouge and human evaluation of extractive meeting
summaries. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics on Human Language Technologies: Short
Papers, HLT-Short ?08, pages 201?204, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization with-
out human models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP
?09, pages 306?314, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically
assessing machine summary content without a gold
623
standard. Computational Linguistics, 39(2):267?
300.
Davide Magatti, Silvia Calegari, Davide Ciucci, and
Fabio Stella. 2009. Automatic labeling of top-
ics. In Proceedings of the 2009 Ninth International
Conference on Intelligent Systems Design and Appli-
cations, ISDA ?09, pages 1227?1232, Washington,
DC, USA. IEEE Computer Society.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In Proceedings of the 13th ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ?07, pages 490?499, New
York, NY, USA. ACM.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?04, pages 404?411, Barcelona, Spain. As-
sociation for Computational Linguistics.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 262?272, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Microsoft Re-
search, Redmond, Washington, Tech. Rep. MSR-TR-
2005-101.
Ani Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document un-
derstanding conference. In Proceedings of the 20th
National Conference on Artificial Intelligence - Vol-
ume 3, AAAI?05, pages 1436?1441. AAAI Press.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 100?108, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.
2012. Summarizing sporting events using twitter. In
Proceedings of the 2012 ACM International Confer-
ence on Intelligent User Interfaces, IUI ?12, pages
189?198, New York, NY, USA. ACM.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Zhaochun Ren, Shangsong Liang, Edgar Meij, and
Maarten de Rijke. 2013. Personalized time-aware
tweets summarization. In Proceedings of the 36th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
?13, pages 513?522, New York, NY, USA. ACM.
Chao Shen, Fei Liu, Fuliang Weng, and Tao Li. 2013.
A participant-based approach for event summariza-
tion using twitter streams. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?13, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 297?300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Xin Zhao, Baihan Shu, Jing Jiang, Yang Song, Hongfei
Yan, and Xiaoming Li. 2012. Identifying event-
related bursts via social media activities. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1466?1477, Jeju Island, Korea, July. Association for
Computational Linguistics.
624
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860?865,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
___________________  
*Corresponding author 
Cross-lingual Opinion Analysis via Negative Transfer Detection  
Lin Gui1,2, Ruifeng Xu1*, Qin Lu2, Jun Xu1, Jian Xu2, Bin Liu1, Xiaolong Wang1 
1Key Laboratory of Network Oriented Intelligent Computation, Shenzhen Graduate School, 
Harbin Institute of Technology, Shenzhen 518055 
2Department Of Computing, the Hong Kong Polytechnic University 
guilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn, 
csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn 
 
Abstract 
Transfer learning has been used in opin-
ion analysis to make use of available lan-
guage resources for other resource scarce 
languages. However, the cumulative 
class noise in transfer learning adversely 
affects performance when more training 
data is used. In this paper, we propose a 
novel method in transductive transfer 
learning to identify noises through the 
detection of negative transfers. Evalua-
tion on NLP&CC 2013 cross-lingual 
opinion analysis dataset shows that our 
approach outperforms the state-of-the-art 
systems. More significantly, our system 
shows a monotonic increase trend in per-
formance improvement when more train-
ing data are used.  
1 Introduction 
Mining opinions from text by identifying their 
positive and negative polarities is an important 
task and supervised learning methods have been 
quite successful. However, supervised methods 
require labeled samples for modeling and the 
lack of sufficient training data is the performance 
bottle-neck in opinion analysis especially for re-
source scarce languages. To solve this problem, 
the transfer leaning method (Arnold et al, 2007) 
have been used to make use of samples from a 
resource rich source language to a resource 
scarce target language, also known as cross lan-
guage opinion analysis (CLOA). 
In transductive transfer learning (TTL) where 
the source language has labeled data and the tar-
get language has only unlabeled data, an algo-
rithm needs to select samples from the unlabeled 
target language as the training data and assign 
them with class labels using some estimated con-
fidence. These labeled samples in the target lan-
guage, referred to as the transferred samples, also 
have a probability of being misclassified. During 
training iterations, the misclassification introduc-
es class noise which accumulates, resulting in a 
so called negative transfer that affects the classi-
fication performance.  
In this paper, we propose a novel method 
aimed at reducing class noise for TTL in CLOA. 
The basic idea is to utilize transferred samples 
with high quality to identify those negative trans-
fers and remove them as class noise to reduce 
noise accumulation in future training iterations. 
Evaluations on NLP&CC 2013 CLOA evalua-
tion data set show that our algorithm achieves the 
best result, outperforming the current state-of-
the-art systems. More significantly, our system 
shows a monotonic increasing trend in perfor-
mance when more training data are used beating 
the performance degradation curse of most trans-
fer learning methods when training data reaches 
certain size. 
The rest of the paper is organized as follows. 
Section 2 introduces related works in transfer 
learning, cross lingual opinion analysis, and class 
noise detection technology. Section 3 presents 
our algorithm. Section 4 gives performance eval-
uation. Section 5 concludes this paper. 
2 Related works 
TTL has been widely used before the formal 
concept and definition of TTL was given in (Ar-
nold, 2007). Wan introduced the co-training 
method into cross-lingual opinion analysis (Wan, 
2009; Zhou et al, 2011), and Aue et al intro-
duced transfer learning into cross domain analy-
sis (Aue, 2005) which solves similar problems. 
In this paper, we will use the terms source lan-
guage and target language to refer to all cross 
lingual/domain analysis. 
Traditionally, transfer learning methods focus 
on how to estimate the confidence score of trans-
ferred samples in the target language or domain 
(Blitzer et al 2006, Huang et al, 2007; Sugiya-
ma et al, 2008, Chen et al 2011, Lu et al, 2011). 
In some tasks, researchers utilize NLP tools such 
as alignment to reduce the bias towards that of 
860
 the source language in transfer learning (Meng et 
al., 2012). However, detecting misclassification 
in transferred samples (referred to as class noise) 
and reducing negative transfers are still an unre-
solved problem. 
There are two basic methods for class noise 
detection in machine learning. The first is the 
classification based method (Brodley and Friedl, 
1999; Zhu et al 2003; Zhu 2004; Sluban et al, 
2010) and the second is the graph based method 
(Zighed et al 2002; Muhlenbach et al 2004; 
Jiang and Zhou, 2004). Class noise detection can 
also be applied to semi-supervised learning be-
cause noise can accumulate in iterations too. Li 
employed Zighed?s cut edge weight statistic 
method in self-training (Li and Zhou, 2005) and 
co-training (Li and Zhou, 2011). Chao used Li?s 
method in tri-training (Chao et al 2008). (Fuku-
moto et al 2013) used the support vectors to de-
tect class noise in semi-supervised learning.  
In TTL, however, training and testing samples 
cannot be assumed to have the same distributions. 
Thus, noise detection methods used in semi-
supervised learning are not directly suited in 
TTL. Y. Cheng has tried to use semi-supervised 
method (Jiang and Zhou, 2004) in transfer learn-
ing (Cheng and Li, 2009). His experiment 
showed that their approach would work when the 
source domain and the target domain share simi-
lar distributions. How to reduce negative trans-
fers is still a problem in transfer learning. 
3 Our Approach 
In order to reduce negative transfers, we pro-
pose to incorporate class noise detection into 
TTL. The basic idea is to first select high quality 
labeled samples after certain iterations as indica-
tor to detect class noise in transferred samples. 
We then remove noisy samples that cause nega-
tive transfers from the current accumulated train-
ing set to retain an improved set of training data 
for the remainder of the training phase. This neg-
ative sample reduction process can be repeated 
several times during transfer learning. Two ques-
tions must be answered in this approach: (1) how 
to measure the quality of transferred samples, 
and (2) how to utilize high quality labeled sam-
ples to detect class noise in training data. 
3.1 Estimating Testing Error 
To determine the quality of the transferred 
samples that are added iteratively in the learning 
process, we cannot use training error to estimate 
true error because the training data and the test-
ing data have different distributions. In this work, 
we employ the Probably Approximately Correct 
(PAC) learning theory to estimate the error 
boundary. According to the PAC learning theory, 
the least error boundary ? is determined by the 
size of the training set m and the class noise rate 
?, bound by the following relation: 
  ?   (   )                      ( ) 
In TTL, m increases linearly, yet ? is multi-
plied in each iteration. This means the signifi-
cance of m to performance is higher at the begin-
ning of transfer learning and gradually slows 
down in later iterations. On the contrary, the in-
fluence of class noise increases. That is why per-
formance improves initially and gradually falls to 
negative transfer when noise accumulation out-
performs the learned information as shown in 
Fig.1. In TTL, transferred samples in both the 
training data and test data have the same distribu-
tion. This implies that we can apply the PAC 
theory to analyze the error boundary of the ma-
chine learning model using transferred data. 
 
Figure 1 Negative transfer in the learning process 
According to PAC theorem with an assumed 
fixed probability ? (Angluin and Laird, 1988), 
the least error boundary ? is given by:   
  ?   (   ? )  ( (   ) )       ( ) 
where N is a constant decided by the hypothesis 
space.  In any iteration during TTL, the hypothe-
sis space is the same and the probability ? is 
fixed. Thus the least error boundary is deter-
mined by the size of the transferred sample m 
and the class noise of transferred samples ?. Ac-
cording to (2), we apply a manifold assumption 
based method to estimate ?. Let T be the number 
of iterations to serve as one period. We then es-
timate the least error boundary before and after 
each T to measure the quality of transferred sam-
ples during each T. If the least error boundary is 
reduced, it means that transferred samples used 
in this period are of high quality and can improve 
the performance. Otherwise, the transfer learning 
algorithm should stop.  
861
 3.2 Estimating Class Noise 
For formula (2) to work, we need to know the 
class noise rate ? to calculate the error boundary. 
Obviously, we cannot use conditional probabili-
ties from the training data in the source language 
to estimate the noise rate ? of the transferred 
samples because the distribution of source lan-
guage is different from that of target language. 
Consider a KNN graph on the transferred 
samples using any similarity metric, for example, 
cosine similarity, for any two connected vertex 
(     )and (     ) in the graph from samples to 
classes, the edge weight is given by: 
       (     )                         ( ) 
Furthermore, a sign function for the two vertices 
(     )and (     ), is defined as: 
    {
          
          
                   ( ) 
According to the manifold assumption, the 
conditional probability  (  |  ) can be approxi-
mated by the frequency of  (     ) which is 
equal to  (     ). In opinion annotations, the 
agreement of two annotators is often no larger 
than 0.8. This means that for the best cases 
 (     )=0.2. Hence     follows a Bernoulli 
distribution with p=0.2 for the best cases in 
manual annotations.  
Let      (     )  be the vertices that are 
connected to the     vertex, the statistical magni-
tude of the     vertex can be defined as: 
   ?                                 ( )  
where j refers to the     vertex that is connected 
to the     vertex.  
From the theory of cut edge statics, we know 
that the expectation of    is: 
    (     )  ?                  ( )  
And the variance of    is: 
  
   (     ) (     )  ?    
 
 ( )  
By the Center Limit Theorem (CLT),    fol-
lows the normal distribution: 
(     )
  
  (   )                    ( )  
To detect the noise rate of a sample (     ) , 
we can use (8) as the null hypothesis to test the 
significant level. Let    denotes probability of 
the correct classification for a transferred sample. 
   should follow a normal distribution,  
   
 
?    
?  
 
(    )
 
   
   
  
           ( )  
Note that experiments (Li and Zhou, 2011; 
Cheng and Li, 2009; Brodley and Friedl, 1999) 
have shown that     is related to the error rate of 
the example (     ), but it does not reflect the 
ground-truth probability in statistics. Hence we 
assume the class noise rate of example (     ) is: 
                              (  ) 
 We take the general significant level of 0.05 
to reject the null hypothesis. It means that if    of 
(     ) is larger than 0.95, the sample will be 
considered as a class noisy sample. Furthermore, 
   can be used to estimate the average class noise 
rate of a transferred samples in (2). 
In our proposed approach, we establish the 
quality estimate period T to conduct class noise 
detection to estimate the class noise rate of trans-
ferred samples. Based on the average class noise 
we can get the least error boundary so as to tell if 
an added sample is of high quality. If the newly 
added samples are of high quality, they can be 
used to detect class noise in transferred training 
data. Otherwise, transfer learning should stop. 
The flow chart for negative transfer is in Fig.2. 
SLS(labeled)
TLS
(unlabeled)
Classifier
Top k
TS
 period 1
TS
period 2
TS
 period n
KNN 
graph
Estimate ?i and ?n 
?n ? ?n-1?
Output SLS and TS 
(period 1 to n-1)
No
Yes
Del te TS
 ?i? 0.95 
period 1 to n-1
Input 
Input 
T iterations per period
Transfer
process
Negative
transfer
detection
Figure 2 Flow charts of negative transfer detection 
In the above flow chart, SLS and TLS refer to 
the source and target language samples, respec-
tively. TS refers to the transferred samples. Let T 
denote quality estimate period T in terms of itera-
tion numbers. The transfer process select k sam-
ples in each iteration. When one period of trans-
fer process finishes, the negative transfer detec-
tion will estimate the quality by comparing and 
either select the new transferred samples or re-
move class noise accumulated up to this iteration. 
4 Experiment 
4.1 Experiment Setting 
The proposed approach is evaluated on the 
NLP&CC 2013 cross-lingual opinion analysis (in 
862
 short, NLP&CC) dataset 1 . In the training set, 
there are 12,000 labeled English Amazon.com 
products reviews, denoted by Train_ENG, and 
120 labeled Chinese product reviews, denoted as 
Train_CHN, from three categories, DVD, BOOK, 
MUSIC. 94,651 unlabeled Chinese products re-
views from corresponding categories are used as 
the development set, denoted as Dev_CHN. In 
the testing set, there are 12,000 Chinese product 
reviews (shown in Table.1). This dataset is de-
signed to evaluate the CLOA algorithm which 
uses Train_CHN, Train_ENG and Dev_CHN to 
train a classifier for Test_CHN. The performance 
is evaluated by the correct classification accuracy 
for each category in Test_CHN2:  
          
                                  
    
 
where c is either DVD, BOOK or MUSIC. 
Team DVD Book Music 
Train_CHN 40 40 40 
Train_ENG 4000 4000 4000 
Dev_CHN 17814 47071 29677 
Test_CHN 4000 4000 4000 
Table.1 The NLP&CC 2013 CLOA dataset 
In the experiment, the basic transfer learning 
algorithm is co-training. The Chinese word seg-
mentation tool is ICTCLAS (Zhang et al 2003) 
and Google Translator3 is the MT for the source 
language. The monolingual opinion classifier is 
SVMlight4, word unigram/bigram features are em-
ployed. 
4.2 CLOA Experiment Results 
Firstly, we evaluate the baseline systems 
which use the same monolingual opinion classi-
fier with three training dataset including 
Train_CHN, translated Train_ENG and their un-
ion, respectively.  
 DVD Book Music Accuracy 
Train_CHN 0.552 0.513 0.500 0.522 
Train_ENG 0.729 0.733 0.722 0.728 
Train_CHN 
+Train_ENG 
0.737 0.722 0.742 0.734 
Table.2 Baseline performances  
It can be seen that using the same method, the 
classifier trained by Train_CHN are on avergage 
20% worse than the English counter parts.The 
combined use of Train_CHN and translated 
Train_ENG, however, obtained similar 
                                                 
1http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip 
2http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf 
3https://translate.google.com 
4http://svmlight.joachims.org/ 
performance to the English counter parts. This 
means the predominant training comes from the 
English training data. 
In the second set of experiment, we compare  
our proposed approach to the official results in 
NLP&CC 2013 CLOA evaluation and the result 
is given in Table 3. Note that in Table 3, the top 
performer of NLP&CC 2013 CLOA evaluation 
is the HLT-HITSZ system(underscored in the 
table), which used the co-training method in 
transfer learning (Gui et al 2013), proving that 
co-training is quite effective for cross-lingual 
analysis. With the additional negative transfer 
detection, our proposed approach achieves the 
best performance on this dataset outperformed 
the top system (by HLT-HITSZ) by a 2.97% 
which translate to 13.1% error reduction im-
provement to this state-of-the-art system as 
shown in the last row of Table 3.     
Team DVD Book Music Accuracy 
BUAA 0.481 0.498 0.503 0.494 
BISTU 0.647 0.598 0.661 0.635 
HLT-HITSZ 0.777 0.785 0.751 0.771 
THUIR 0.739 0.742 0.733 0.738 
SJTU 0.772 0.724 0.745 0.747 
WHU 0.783 0.770 0.760 0.771 
Our approach 0.816 0.801 0.786 0.801 
Error 
Reduction 
0.152 0.072 0.110 0.131 
Table.3 Performance compares with NLP&CC 
2013 CLOA evaluation results 
To further investigate the effectiveness of our 
method, the third set of experiments evaluate the 
negative transfer detection (NTD) compared to 
co-training (CO) without negative transfer 
detection as shown in Table.4 and Fig.3 Here, we 
use the union of Train_CHN and Train_ENG as 
labeled data and Dev_CHN as unlabeled data to 
be transferred in the learning algorithms. 
 DVD Book Music Mean 
NTD 
Best case 0.816 0.801 0.786 0.801 
Best period 0.809 0.798 0.782 0.796 
Mean 0.805 0.795 0.781 0.794 
CO 
Best case 0.804 0.796 0.783 0.794 
Best period 0.803 0.794 0.781 0.792 
Mean 0.797 0.790 0.775 0.787 
Table.4 CLOA performances 
Taking all categories of data, our proposed 
method improves the overall average precision 
(the best cases) from 79.4% to 80.1% when 
compared to the state of the art system which 
translates to error reduction of 3.40% (p-
value?0.01 in Wilcoxon signed rank test). Alt-
hough the improvement does not seem large, our 
863
  
   
Figure 3 Performance of negative transfer detection vs. co-training 
algorithm shows a different behavior in that it 
can continue to make use of available training 
data to improve the system performance. In other 
words, we do not need to identify the tipping 
point where the performance degradation can 
occur when more training samples are used. Our 
approach has also shown the advantage of stable 
improvement.  
In the most practical tasks, co-training based 
approach has the difficulty to determine when to 
stop the training process because of the negative 
transfer. And thus, there is no sure way to obtain 
the above best average precision. On the contrary, 
the performance of our proposed approach keeps 
stable improvement with more iterations, i.e. our 
approach has a much better chance to ensure the 
best performance. Another experiment is con-
ducted to compare the performance of our pro-
posed transfer learning based approach with su-
pervised learning. Here, the achieved perfor-
mance of 3-folder cross validation are given in 
Table 5. 
 DVD Book Music Average 
Supervised 0.833 0.800 0.801 0.811 
Our approach 0.816 0.801 0.786 0.801 
Table.5 Comparison with supervised learning  
The accuracy of our approach is only 1.0% 
lower than the supervised learning using 2/3 of 
Test_CHN. In the BOOK subset, our approach 
achieves match result. Note that the performance 
gap in different subsets shows positive correla-
tion to the size of Dev_CHN. The more samples 
are given in Dev_CHN, a higher precision is 
achieved even though these samples are unla-
beled. According to the theorem of PAC, we 
know that the accuracy of a classifier training 
from a large enough training set with confined 
class noise rate will approximate the accuracy of 
classifier training from a non-class noise training 
set. This experiment shows that our proposed 
negative transfer detection controls the class 
noise rate in a very limited boundary. Theoreti-
cally speaking, it can catch up with the perfor-
mance of supervised learning if enough unla-
beled samples are available. In fact, such an ad-
vantage is the essence of our proposed approach.  
5 Conclusion 
In this paper, we propose a negative transfer 
detection approach for transfer learning method 
in order to handle cumulative class noise and 
reduce negative transfer in the process of transfer 
learning. The basic idea is to utilize high quality 
samples after transfer learning to detect class 
noise in transferred samples. We take cross lin-
gual opinion analysis as the data set to evaluate 
our method. Experiments show that our proposed 
approach obtains a more stable performance im-
provement by reducing negative transfers. Our 
approach reduced 13.1% errors than the top sys-
tem on the NLP&CC 2013 CLOA evaluation 
dataset. In BOOK category it even achieves bet-
ter result than the supervised learning. Experi-
mental results also show that our approach can 
obtain better performance when the transferred 
samples are added incrementally, which in pre-
vious works would decrease the system perfor-
mance. In future work, we plan to extend this 
method into other language/domain resources to 
identify more transferred samples.  
Acknowledgement 
This research is supported by NSFC 61203378, 
61300112, 61370165, Natural Science Founda-
tion of GuangDong S2013010014475, MOE 
Specialized Research Fund for the Doctoral Pro-
gram of Higher Education 20122302120070,  
Open Projects Program of National Laboratory 
of Pattern Recognition?Shenzhen Foundational 
Research Funding JCYJ20120613152557576, 
JC201005260118A, Shenzhen International Co-
operation Research Funding 
GJHZ20120613110641217 and Hong Kong Pol-
ytechnic University Project code Z0EP. 
DVD Book Music 
864
 Reference 
Angluin, D., Laird, P. 1988. Learning from Noisy 
Examples. Machine Learning, 2(4): 343-370. 
Arnold, A., Nallapati, R., Cohen, W. W. 2007. A 
Comparative Study of Methods for Transductive 
Transfer Learning. In Proc. 7th IEEE ICDM Work-
shops, pages 77-82. 
Aue, A., Gamon, M. 2005. Customizing Sentiment 
Classifiers to New Domains: a Case Study, In Proc. 
of t RANLP. 
Blitzer, J., McDonald, R., Pereira, F. 2006. Domain 
Adaptation with Structural Correspondence Learn-
ing. In Proc. EMNLP, 120-128. 
Brodley, C. E., Friedl, M. A. 1999. Identifying and 
Eliminating Mislabeled Training Instances. Journal 
of Artificial Intelligence Research, 11:131-167. 
Chao, D., Guo, M. Z., Liu, Y.,  Li, H. F. 2008.  Partic-
ipatory Learning based Semi-supervised Classifica-
tion. In Proc. of 4th ICNC, pages 207-216. 
Cheng, Y., Li, Q. Y. 2009. Transfer Learning with 
Data Edit. LNAI, pages 427?434. 
Chen, M., Weinberger, K. Q.,  Blitzer, J. C. 2011.  
Co-Training for Domain Adaptation. In Proc. of 
23th NIPS. 
Fukumoto, F., Suzuki, Y., Matsuyoshi, S. 2013. Text 
Classification from Positive and Unlabeled Data 
using Misclassified Data Correction. In Proc. of 
51st ACL, pages 474-478. 
Gui, L., Xu, R.,  Xu, J., et al 2013. A Mixed Model 
for Cross Lingual Opinion Analysis. In CCIS, 400, 
pages 93-104. 
Huang, J., Smola, A., Gretton, A., Borgwardt, K.M., 
Scholkopf, B. 2007. Correcting Sample Selection 
Bias by Unlabeled Data. In Proc. of 19th NIPS,  
pages 601-608. 
Jiang, Y., Zhou, Z. H. 2004. Editing Training Data for 
kNN Classifiers with Neural Network Ensemble. In 
LNCS, 3173,  pages 356-361. 
Li, M., Zhou, Z. H. 2005. SETRED: Self-Training 
with Editing. In Proc. of PAKDD, pages 611-621. 
Li, M., Zhou, Z. H. 2011.  COTRADE: Confident Co-
Training With Data Editing. IEEE Transactions on 
Systems, Man, and Cybernetics?Part B: Cyber-
netics, 41(6):1612-1627. 
Lu, B., Tang, C. H., Cardie, C., Tsou, B. K. 2011. 
Joint Bilingual Sentiment Classification with Un-
labeled Parallel Corpora. In Proc. of 49th ACL, 
pages 320-330. 
Meng, X. F., Wei, F. R., Liu, X. H., et al 2012. 
Cross-Lingual Mixture Model for Sentiment Clas-
sification. In Proc. of 50th ACL, pages 572-581. 
Muhlenbach, F., Lallich, S., Zighed, D. A. 2004. 
Identifying and Handling Mislabeled Instances.  
Journal of Intelligent Information System, 22(1): 
89-109. 
Pan, S. J., Yang, Q. 2010. A Survey on Transfer 
Learning, IEEE Transactions on Knowledge and 
Data Engineering, 22(10):1345-1360. 
Sindhwani, V., Rosenberg, D. S. 2008. An RKHS for 
Multi-view Learning and Manifold Co-
Regularization. In Proc. of 25th  ICML, pages 976?
983. 
Sluban, B., Gamberger, D., Lavra, N. 2010.  Advanc-
es in Class Noise Detection. In Proc.19th ECAI,  
pages 1105-1106. 
Sugiyama, M.,  Nakajima, S., Kashima, H., Buenau, 
P.V., Kawanabe, M. 2008. Direct Importance Es-
timation with Model Selection and its Application 
to Covariate Shift Adaptation. In Proc. 20th NIPS. 
Wan, X. 2009. Co-Training for Cross-Lingual Senti-
ment Classification, In Proc. of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP of the 
AFNLP,  235?243. 
Zhang, H. P., Yu, H. K., Xiong, D. Y., and Liu., Q. 
2003. HHMM-based Chinese Lexical Analyzer 
ICTCLAS. In 2nd SIGHAN workshop affiliated 
with 41th ACL, pages 184-187. 
 Zhou, X., Wan X., Xiao, J. 2011. Cross-Language 
Opinion Target Extraction in Review Texts. In 
Proc. of IEEE 12th ICDM, pages 1200-1205. 
Zhu, X. Q., Wu, X. D., Chen, Q. J. 2003.  Eliminating 
Class Noise in Large Datasets. In Proc. of 12th 
ICML, pages 920-927. 
Zhu, X. Q. 2004. Cost-guided Class Noise Handling 
for Effective Cost-sensitive Learning In Proc. of 4th  
IEEE ICDM,  pages 297-304. 
Zighed, D. A., Lallich, S., Muhlenbach, F. 2002.  
Separability Index in Supervised Learning. In Proc. 
of PKDD, pages 475-487. 
865
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 97?102,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
Web Information Mining and Decision Support Platform for the  
Modern Service Industry 
 
Binyang Li1,2, Lanjun Zhou2,3, Zhongyu Wei2,3, Kam-fai Wong2,3,4,  
Ruifeng Xu5, Yunqing Xia6 
 
1 Dept. of Information Science & Technology, University of International Relations, China 
2Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong 
Kong, Shatin, N.T., Hong Kong  
3MoE Key Laboratory of High Confidence Software Technologies, China 
4 Shenzhen Research Institute, The Chinese University of Hong Kong 
5Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China 
6Department of Computer Science & Technology, TNList, Tsinghua University, China 
{byli,ljzhou,zywei,kfwong}@se.cuhk.edu.hk  
 
 
Abstract 
This demonstration presents an intelligent infor-
mation platform MODEST. MODEST will pro-
vide enterprises with the services of retrieving 
news from websites, extracting commercial in-
formation, exploring customers? opinions, and 
analyzing collaborative/competitive social net-
works. In this way, enterprises can improve the 
competitive abilities and facilitate potential col-
laboration activities. At the meanwhile, MOD-
EST can also help governments to acquire in-
formation about one single company or the entire 
board timely, and make prompt strategies for 
better support. Currently, MODEST is applied to 
the pillar industries of Hong Kong, including 
innovative finance, modem logistics, information 
technology, etc. 
1 Introduction 
With the rapid development of Web 2.0, the 
amount of information is exploding. There are 
millions of events towards companies and bil-
lions of opinions on products generated every 
day (Liu, 2012). Such enormous information 
cannot only facilitate companies to improve their 
competitive abilities, but also help government to 
make prompt decisions for better support or 
timely monitor, e.g. effective risk management. 
For this reason, there is a growing demand of 
Web information mining and intelligent decision 
support services for the industries. Such services 
are collectively referred as modern service, 
which includes the following requirements: 
(1) To efficiently retrieve relevant information 
from the websites; 
(2) To accurately determine the latest business 
news and trends of the company; 
(3) To identify and analyze customers? opinions 
towards the company; 
(4) To explore the collaborative and competitive 
relationship with other companies; 
(5) To leverage the knowledge mined from the 
business news and company social network 
for decision support. 
In this demonstration, we will present a Web 
information mining and decision support plat-
form, MODEST1. The objective of MODEST is 
to provide modern services for both enterprises 
and government, including collecting Web in-
formation, making deep analysis, and providing 
supporting decision. The innovation of MOD-
EST is focusing on deep analysis which incor-
porates the following functions: 
? Topic detection and tracking function is to 
cluster the hot events and capture the rela-
tionship between the relevant events based on 
the collected data from websites (event also 
referred as topic in this paper). In order to re-
alize this function, Web mining techniques 
are adopted, e.g. topic clustering, heuristics 
algorithms, etc. 
? The second function is to identify and analyze 
customers? opinions about the company. 
Opinion mining technology (Zhou et al., 2010) 
is adopted to determine the polarity of those 
news, which can help the company timely and 
appropriately adjust the policy to strengthen 
the dominant position or avoid risks. 
                                                          
1 This work is supported by the Innovation and Technology 
Fund of Hong Kong SAR. 
97
? The third function is to explore and analyze 
social network based on the company centric. 
We utilize social network analysis (SNA) 
technology (Xia et al., 2010) to discover the 
relationships, and we further analyze the con-
tent in fine-grained granularity to identify its 
potential partners or competitors.  
With the help of MODEST, the companies can 
acquire modern service-related information, and 
timely adjust corporate policies and marketing 
plan ahead. Hence, the ability of information ac-
quisition and the competitiveness of the enter-
prises can be improved accordingly. 
In this paper, we will use a practical example 
to illustrate our platform and evaluate the per-
formance of main functions.  
The rest of this paper is organized as follows. 
Section 2 will introduce the system description 
as well as the main functions implementation. 
The practical case study will be illustrated in 
Section 3. The performance of MODEST will be 
evaluated in Section 4.Finally, this paper will be 
concluded in Section 5. 
2 System Description 
In this section, we first outline the system archi-
tecture of MODEST, and then describe the im-
plementation of the main functionality in detail.  
2.1 Architecture and Workflow 
The MODEST system consists of three modules: 
data acquisition, data analysis, and result display. 
The system architecture is shown in Figure 1. 
 
Figure 1: System architecture. (The module in 
blue is data acquisition, the module in orange is 
data analysis, and the module in light green is 
result display) 
(1) The core technique in the data acquisition 
module is the crawler, which is developed to 
collect raw data from websites, e.g. news portals, 
blogosphere. Then the system parse the raw web 
pages and extract information to store in the local 
database for further processing. 
(2) The data analysis module can be divided into 
two parts:  
? NLP pre-processor: utilizes NLP (natural 
language processing) techniques and some 
toolkits to perform the pre-processing on the 
raw data in (1), including word segmenta-
tion, part-of-speech (POS) tagging1, stop-
word removal, and named entity recognition 
(NER)2. We then create knowledgebase for 
individual industry, such as domain-specific 
sentiment word lexicon, name entity collec-
tion, and so on. 
? Miner?makes use of data mining techniques 
to realize four functions, topic detection and 
tracking (TDT), multi-document summari-
zation 3  (MDS), social network analysis 
(SNA), and opinion mining (OM). The re-
sults of data analysis are also stored in the 
database.  
(3) The result display module read out the analy-
sis results from the database and display them to 
users in the form of plain text, charts, figures, as 
well as video. 
2.2 Function Implementation 
Since the innovation of MODEST is focusing on 
the module of data analysis, we will describe its 
main functions in detail, including topic detec-
tion and tracking, opinion mining, and social 
networks analysis. 
2.2.1 Topic Detection and Tracking 
The TDT function targets on detecting and 
tracking the hot topics for each individual com-
pany. Given a period of data collected from web-
sites, there are various discussions about the 
company. In order to extract these topics, clus-
tering methods (Viermetz et al., 2007 and Yoon 
et al., 2009) are implemented to explore the top-
ics. Note that during the period of data collection, 
different topics with respect to the same compa-
ny may have relations. We, therefore, utilize hi-
erarchical clustering methods4to capture the po-
tential relations.  
Due to the large amount of data, it is impossi-
ble to view all the topics at a snapshot. MODEST 
utilizes topic tracking technique (Wang et al., 
2008) to identify related stories with a stream of 
                                                          
1 www.ictclas.org 
2http://ir.hit.edu.cn/demo/ltp 
3http://libots.sourceforge.net/ 
4http://dragon.ischool.drexel.edu/ 
Raw
files
Pre-processed
files
Database
Crawler UI
Word
segmentation
Web
NER
Stopword
removal
POS
tagging
NLP pre-p ocessor
TDT OM
MDS SNS
Miner
Data Layer
98
media. It is convenient for the users to see the 
latest information about the company.  
In summary, TDT function provides the ser-
vices of detecting and tracking the latest and 
emergent topics, analyzing the relationships of 
topics on the dynamics of the company. It meets 
the aforementioned demand, ?to accurately grasp 
the latest business news and trends of the com-
pany?. 
2.2.2 Opinion Mining 
The objective of OM function is to discover 
opinions towards a company and classify the 
opinions into positive, negative, or neutral. 
The opinion mining function is redesigned 
based on our own opinion mining engine (Zhou 
et al., 2010). It separates opinion identification 
and polarity classification into two stages.  
Given a set of documents that are relevant to 
the company, we first split the documents into 
sentences, and then identify whether the sentence 
is opinionated or not. We extract the features 
shown in Table 1 for opinion identification. 
(Zhou et al., 2010) 
Table 1: Features adopted in the opinionated 
sentence classifier 
Punctuation level features 
The presence of direct quote punctuation "?" and "?"  
The presence of other punctuations: "?" and "!" 
Word-Level and entity-level features 
The presence of known opinion operators 
The percentage of known opinion word in sentence 
Presence of a named entity 
Presence of pronoun 
Presence of known opinion indicators 
Presence of known degree adverbs 
Presence of known conjunctions 
Bi-gram features 
Named entities + opinion operators 
Pronouns + opinion operators 
Nouns or named entities + opinion words 
Pronouns + opinion words 
Opinion words (adjective) + opinion words(noun) 
Degree adverbs + opinion words 
Degree adverbs + opinion operators 
These features are then combined using a ra-
dial basis function (RBF) kernel and a support 
vector machine (SVM) classifier (Drucker et al., 
1997) is trained based on the NTCIR 8training 
data for opinion identification (Kando, 2010). 
For those opinionated sentences, we then clas-
sify them into positive, negative, or neutral. In 
addition to the features shown in Table 1, we 
incorporate features of s-VSM (Sentiment Vector 
Space Model) (Xia et al., 2008) to enhance the 
performance. The principles of the s-VSM are 
listed as follows: (1) Only sentiment-related 
words are used to produce sentiment features for 
the s-VSM. (2) The sentiment words are appro-
priately disambiguated with the neighboring ne-
gations and modifiers. (3) Negations and modifi-
ers are included in the s-VSM to reflect the func-
tions of inversing, strengthening and weakening. 
Sentiment unit is the appropriate element com-
plying with the above principles. (Zhou et al., 
2010) 
In addition to polarity classification, opinion 
holder and target are also recognized in OM 
function for further identifying the relationship 
that two companies have, e.g. collaborative or 
competitive. Both of the dependency parser and 
the semantic role labeling1 (SRL) tool are in-
corporated to identify the semantic roles of each 
chunk based on verbs in the sentence. 
The OM function provides the company with 
services of analyzing the social sentimental 
feedback on the dynamics of the company. It 
meets the aforementioned demand, ?to identify 
and analyze customers? opinions towards the 
company?. 
2.2.3 Social Network Analysis  
SNA function aims at producing the commercial 
network of companies that are hidden within the 
articles.  
To achieve this goal, we maintain two lexicons, 
the commercial named entity lexicon and com-
mercial relation lexicon. Commercial named en-
tity are firstly located within the text and then 
recorded in the commercial entity lexicon in the 
pre-processor NER. Commercial relation lexicon 
record the articles/documents that involve the 
commercial relations. Note that the commercial 
relation lexicon (Table 2) is manually compiled. 
In this work, we consider only two general 
commercial relations, namely cooperation and 
competition.  
 
Table 2: Statistics on relation lexicon. 
Type Amount Examples 
Competition 20 ??(challenge), ??
(compete), ? ?
(opponent) 
Collaboration 18 ??(collaborate),??
(coordinate), ? ?
(cooperate) 
SNA function produces the social network of a 
centric company, which can provide the compa-
                                                          
1http://ir.hit.edu.cn/demo/ltp 
99
ny with the impact analysis and decision-making 
chain tracking. It meets the aforementioned de-
mand, ?to explore the collaborative and competi-
tive relationship between companies?. 
3 Practical Example 
In this section, we use a case study to illustrate 
our system and further evaluate the performance 
of the main functions with respect to those com-
panies. Due to the limited space, we just illus-
trate the main functions of topic detection, opin-
ion mining and social network analysis. 
3.1 Topic Detection and Opinion Mining 
Figure 2(a) showed the results of topic detection 
and opinion mining functions for a Hong Kong 
local financial company Sun Hung Kai Proper-
ties (?????). On top of the figure are the 
results of topic detection and tracking function. 
Multi-document summary of the latest news is 
provided for the company and more news with 
the similar topics can be found by pressing the 
button ???? (more). Since there are a lot of 
duplicates of a piece of news on the websites, the 
summary is a direct way to acquire the recent 
news, which can improve the effectiveness of the 
company.  
The results of opinion mining function are 
shown at the bottom of Figure 2(a), where the 
green line indicates negative while the red line 
indicates positive. In order to give a dynamic 
insight of public opinions, we provide the 
amount changes of positive and negative articles 
with time variant. This is very helpful for the 
company to capture the feedback of their mar-
keting policies. As shown in Figure 2(a), there 
were 14 negative articles (????) on Oct. 29, 
2012, which achieved negative peak within the 6 
months. The users would probably read those 14 
articles and adjust the company strategy accord-
ingly.  
3.2 Social Network Analysis 
Figure 2(b) shows the social network based on 
the centric company in yellow, Sun Hung Kai 
Properties (?????). We only list the half 
of the connected companies with collaborative 
relationship from Sun Hung Kai Properties, and 
remove the competitive ones due to limited space. 
The thickness of the line indicates the strength of 
the collaboration between the two companies. 
The social network can explore the potential 
partners/competitors of a company. Furthermore, 
users are allowed to adjust the depth and set the 
nodes count of the network. The above analysis 
can provide a richer insight in to a company.  
In the following section, we will make exper-
iments to investigate the performance of the 
above functions.
 
(a) Topic detection and opinion mining of Sun Hung Kai Properties (?????). (For convenience, 
we translate the texts on the button in English) 
Opinion Mining 
Topic Detection 
100
 (b)Social network of Sun Hung Kai Properties (?????). (The rectangle in yellow is the centric) 
 
Figure 2: Screenshot of the MODEST system. 
4 Experiment and Result 
In our evaluation, the experiments were made 
based on 17692 articles collected from 52 Hong 
Kong websites during 6 months (1/7/2012~ 
31/12/2012). We investigate the performance of 
MODEST based on the standard metrics pro-
posed by NIST1, including precision, recall, and 
F-score. 
Precision (P) is the fraction of detected articles 
(U) that are relevantto the topic (N). 
  
 
 
      
Recall (R) is the fraction of the articles (T) that 
are relevant to the topic that are successfully de-
tected (N). 
  
 
 
      
Usually, there is an inverse relationship be-
tween precision and recall, where it is possible to 
increase one at the cost of reducing the other. 
Therefore, precision and recall scores are not 
discussed in isolation. Instead, F-Score (F) is 
proposed to combine precision and recall, which 
is the harmonic meanof precision and recall. 
  
 
 
 
 
 
 
      
     
   
      
4.1 Topic Detection and Tracking 
We first assess the performance of the topic de-
tection function. The data is divided into 6 parts 
                                                          
1http://trec.nist.gov/ 
according to the time. For different companies, 
the amount of articles vary a lot. Therefore, we 
calculate the metrics for each individual dataset, 
and then compute the weighted mean value. The 
experimental results are shown in Table 3.  
Table 3: Experimental results on topic detection. 
Dataset Recall Precision F-Score 
1/7/12-31/7/12 85.71% 89.52% 85.38% 
1/8/12-31/8/12 93.10% 93.68% 92.49% 
1/9/12-30/9/12 76.50% 83.13% 76.56% 
1/10/12-31/10/12 83.32% 88.53% 85.84% 
1/11/12-30/11/12 86.11% 89.94% 87.98% 
1/12/12-31/12/12 84.26% 87.65% 85.92% 
Average 85.13% 88.78% 85.69% 
From the experimental results, we can find 
that the average F-Score is about 85.69%.The 
dataset in the second row achieves the best per-
formance while the dataset in the third only get 
76.56% in F-Score. It is because that the amount 
of articles is smaller than the others and the re-
call value is very low. As far as we know, the 
best run of topic detection in (Allan et al., 2007) 
achieved 84%. The performance of topic detec-
tion in MODEST is comparable. 
4.2 Opinion Mining 
We then evaluate the performance of opinion 
mining function. We manually annotated 1568 
articles, which is further divided into 8 datasets 
randomly. Precision, recall, and F-score are also 
used as the metrics for the evaluation. The ex-
perimental results are shown in Table 4. 
101
  From Table 4, we can find that the average 
F-Score can reach 74.09%. Note that the opinion 
mining engine of MODEST is the implementa-
tion of (Zhou et al., 2010), which achieved the 
best run in NTCIR. However, the engine is 
trained on NTCIR corpus, which consists of arti-
cles of general domain, while the test set focuses 
on the financial domain. We further train our 
engine on the data from the financial domain and 
the average F-Score improves to over 80%. 
5 Conclusions 
This demonstration presents an intelligent infor-
mation platform designed to mine Web infor-
mation and provide decisions for modern service, 
MODEST. MODEST can provide the services of 
retrieving news from websites, extracting com-
mercial information, exploring customers? opin-
ions about a given company, and analyzing its 
collaborative/competitive social networks. Both 
enterprises and government are the target cus-
tomers. For enterprise, MODEST can improve 
the competitive abilities and facilitate potential 
collaboration. For government, MODEST can 
collect information about the entire industry, and 
make prompt strategies for better support. 
In this paper, we first introduce the system ar-
chitecture design and the main functions imple-
mentation, including topic detection and tracking, 
opinion mining, and social network analysis. 
Then a case study is given to illustrate the func-
tions of MODEST. In order to evaluate the per-
formance of MODEST, we also conduct the ex-
periments based on the data from 52 Hong Kong 
websites, and the results show the effectiveness 
of the above functions. 
In the future, MODEST will be improved in 
two directions: 
? Extend to other languages, e.g. English, 
Simplified Chinese, etc. 
? Enhance the compatibility to implement 
on mobile device.  
The demo of MODEST and the related 
toolkits can be found on the homepage: 
http://sepc111.se.cuhk.edu.hk:8080/adcom_hk/ 
Acknowledgements 
This research is partially supported by General Re-
search Fund of Hong Kong (417112), Shenzhen Fun-
damental Research Program (JCYJ201304011720464 
50, JCYJ20120613152557576), KTO(TBF1ENG007), 
National Natural Science Foundation of China 
(61203378, 61370165), and Shenzhen International 
Cooperation Funding (GJHZ20120613110641217). 
References: 
James Allan, Jaime Carbonell, George Doddington, 
Jonathan Yamron, and Yiming Yang. 1998. Topic 
Detection and Tracking Pilot Study: Final Report. 
Proceedings of the DARPA Broadcast News Tran-
scription and Understanding Workshop. 
Harris Drucker, Chris J.C. Burges, Linda Kaufman, 
Alex Smola, and Vladimir Vpnik. 1997. Support 
Vector Regression Machines. Proceedings of Ad-
vances in Neural Information Processing Systems, 
pp. 155-161. 
Noriko Kando.2010. Overview of the Eighth NTCIR 
Workshop. Proceedings of NTCIR-8 Workshop. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Proceedings of Synthesis Lectures on Hu-
man Language Technologies, pp. 1-167. 
Maximilian Viermetz, and Michal Skubacz. 2007. 
Using Topic Discovery to Segment Large Commu-
nication Graphs for Social Network Analysis. Pro-
ceedings of the IEEE/WIC/ACM International 
Conference on Web Intelligence, pp. 95-99. 
Canhui Wang, Min Zhang, Liyun Ru, and Shaoping 
Ma. 2008. Automatic Online News Topic Ranking 
Using Media Focus and User Attention based on 
Aging Theory. Proceedings of the Conference on 
Information and Knowledge Management. 
Yunqing Xia, Nianxing Ji, Weifeng Su, and Yi Liu. 
2010. Mining Commercial Networks from Online 
Financial News. Proceedings of the IEEE Interna-
tional Conference on E-Business Engineering, pp. 
17-23. 
Ruifeng Xu, Kam-fai Wong, and Yunqing Xia. 2008. 
Coarse-Fine Opinion Mining-WIA in NTCIR-7 
MOAT Task. In NTCIR-7 Workshop, pp. 307-313. 
Seok-Ho Yoon, Jung-Hwan Shin, Sang-Wook Kim, 
and Sunju Park. 2009. Extraction of a Latent Blog 
Community based on Subject. Proceeding of the 18th 
ACM Conference on Information and Knowledge 
Management, pp. 1529-1532. 
Lanjun Zhou, Yunqing Xia, Binyang Li, and Kam-fai 
Wong. 2010. WIA-Opinmine System in NTCIR-8 
MOAT Evaluation. Proceedings of NTCIR-8 
Workshop Meeting, pp. 286-292. 
Table 4: Experimental results on opinion mining. 
Dataset Size Precision Recall F-Score 
dataset-1 200 76.57% 78.26% 76.57% 
dataset-2 200 83.55% 89.64% 86.07% 
dataset-3 200 69.12% 69.80% 69.44% 
dataset-4 200 77.13% 75.40% 75.67% 
dataset-5 200 76.21% 77.65% 76.74% 
dataset-6 200 63.76% 66.22% 64.49% 
dataset-7 200 78.56% 78.41% 78.43% 
dataset-8 168 65.72% 65.15% 65.32% 
Average 196 73.83% 75.07% 74.09% 
102
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 448?451,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
HITSZ_CITYU: Combine Collocation, Context Words and Neighbor-
ing Sentence Sentiment in Sentiment Adjectives Disambiguation 
 
 
Ruifeng Xu1,2, Jun Xu1 
1Harbin Institute of Technology, 
Shenzhen Campus, China 
xuruifeng@hitsz.edu.cn 
hit.xujun@gmail.com 
Chunyu Kit2 
2City University of Hong Kong, 
Hong Kong 
ctckit@cityu.edu.hk  
  
 
Abstract 
This paper presents the HIT_CITYU systems 
in Semeval-2 Task 18, namely, disambiguat-
ing sentiment ambiguous adjectives. The base-
line system (HITSZ_CITYU_3) incorporates 
bi-gram and n-gram collocations of sentiment 
adjectives, and other context words as features 
in a one-class Support Vector Machine (SVM) 
classifier. To enhance the baseline system, col-
location set expansion and characteristics 
learning based on word similarity and semi-
supervised learning are investigated, respec-
tively. The final system (HITSZ_CITYU_1/2) 
combines collocations, context words and 
neighboring sentence sentiment in a two-class 
SVM classifier to determine the polarity of 
sentiment adjectives. The final systems 
achieved 0.957 and 0.953 (ranked 1st and 2nd) 
macro accuracy, and 0.936 and 0.933 (ranked 
2nd and 3rd) micro accuracy, respectively.  
 
1 Introduction 
Sentiment analysis is always puzzled by the con-
text-dependent sentiment words that one word 
brings positive, neutral or negative meanings in 
different contexts. Hatzivassiloglou and 
Mckeown (1997) predicated the polarity of ad-
jectives by using the pairs of adjectives linked by 
consecutive or negation conjunctions. Turney 
and Littman (2003) determined the polarity of 
sentiment words by estimating the point-wise 
mutual information between sentiment words 
and a set of seed words with strong polarity. An-
dreevskaia and Bergler (2006) used a Sentiment 
Tag Extraction Program to extract sentiment-
bearing adjectives from WordNet. Esuli and Se-
basian (2006) studied the context-dependent sen-
timent words in WordNet but ignored the in-
stances in real context. Wu et al (2008) applied 
collocation plus a SVM classifier in Chinese sen-
timent adjectives disambiguation. Xu et al (2008) 
proposed a semi-supervised learning algorithm to 
learn new sentiment word and their context-
dependent characteristics.  
Semeval-2 Task 18 is designed to provide a 
common framework and dataset for evaluating 
the disambiguation techniques for Chinese sen-
timent adjectives. The HITSZ_CITYU group 
submitted three runs corresponding to one base-
line system and one improved systems (two runs). 
The baseline system (HITSZ_CITYU_3) is 
based on collocations between sentiment words 
and their targets as well as their context words. 
For the ambiguous adjectives, 412 positive and 
191 negative collocations are built from a 100-
million-word corpus as the seed collocation set. 
Using the context words of seed collocations as 
features, a one-class SVM classifier is trained in 
the baseline system. Using HowNet-based word 
similarity as clue, the seed collocations are ex-
panded to improve the coverage of collocation-
based technique. Furthermore, a semi-supervised 
learning algorithm is developed to learn new col-
locations between sentiment words and their tar-
gets from raw corpus. Finally, the inner sentence 
features, such as collocations and context words, 
and the inter sentence features, i.e. neighboring 
sentence sentiments, are incorporated to deter-
mine the polarity of ambiguous adjectives. The 
improved systems (HITSZ_CITYU_1/2) 
achieved 0.957 and 0.953 macro accuracy 
(ranked 1st and 2nd) and 0.936 and 0.933 micro 
accuracy (ranked 2nd and 3rd), respectively. This 
result shows that collocation, context-words and 
neighboring sentence sentiment are effective in 
sentiment adjectives disambiguation. 
The rest of this paper is organized as follows. 
Section 2 presents the collocation extraction sub-
system based on lexical statistics. Section 3 
448
presents the baseline system and Section 4 
presents the improved systems. The experiment 
results are given in Section 5 and finally, Section 
6 concludes. 
2 Collocation Extraction 
A lexical statistics-based collocation extraction 
subsystem is developed to identify both the bi-
gram and n-gram collocations of sentiment ad-
jectives. This subsystem is based on our previous 
research on Chinese collocation extraction. It 
recognizes the co-occurring words of a headword 
as collocations which have co-occurrence fre-
quency significance among all co-occurring 
words and co-occurrence position significance 
among all co-occurring positions.  
For a sentiment adjective, noted as whead, any 
word within the [-5,+5] context window is a co-
word, denoted as wco-i for 1? i ? k, where k is the 
total number of different co-words of whead.  
BI-Strength(whead,wco-i) between a head word 
whead and a co-word w co-i (i=1, to k) is designed 
to measure the co-occurrence frequency signifi-
cance as follows:  
)()(
)()(
5.0
)()(
)()(
5.0
),(
minmaxminmax icoico
icoicohead
headhead
headicohead
icohead
wfwf
wfwwf
wfwf
wfwwf
wwStrengthBI
??
???
?
?
??+?
??
=?
(1) 
where, fmax(whead) , fmin(whead) and )( headwf are the 
highest, lowest and average co-occurrence fre-
quencies among all the co-words of whead,, re-
spectively; fmax(wco-i), fmin(wcoi) and )( icowf ?  are 
respectively the highest, lowest and average co-
occurrence frequencies of the co-words for wco-i. 
The value of BI-Strength(whead wco-j) ranges from 
-1 to 1, and a larger value means a stronger asso-
ciation. Suppose f(whead,wco-i, m) is the frequency 
that wco-i co-occurs with whead at position m(?
5<=m<=5). The BI-Spread(whead,wco-i) is de-
signed to characterizes the significance that wco-i 
around whead at neighbouring places as follows: 
 
),,(
|),(),,(|
),(
5
5
5
5
?
?
?=
?
?=
??
?
?
=?
m
icohead
m
icoheadicohead
icohead
mwwf
wwfmwwf
wwSpreadBI
(2) 
where, ),( icohead wwf ? , fmax(whead,,wco-i), and fmin 
(whea,,dwco-i) are the average, highest, and lowest 
co-occurrence frequencies among all 10 posi-
tions, respectively. The value of BI-Spread(whead, 
wco-i) ranges from 0 to 1. A larger value means 
that whead and wco-i tend to co-occur in one or two 
positions.  
The word pairs satisfying, (1) BI-
Strength(whead wco-j)>K0 and (2) BI-Spread(whead, 
wco-i)>U0, are extracted as bi-gram collocations, 
where K0 and U0 are empirical threshold.  
Based on the extracted bi-gram collocations, 
the appearance of each co-word in each position 
around whead is analyzed. For each of the possible 
relative distances from whead, only words occupy-
ing the position with a probability greater than a 
given threshold T are kept. Finally, the adjacent 
words satisfying the threshold requirement are 
combined as n-gram collocations. 
3 The Baseline System 
The baseline system incorporates collocation and 
context words as features in a one-class SVM 
classifier. It consists of two steps: 
 STEP 1: To match a test instance containing 
seed collocation set. If the instance cannot be 
matched by any collocations, go to STEP 2. 
STEP 2: Use a trained classifier to indentify 
the sentiment of the word.  
The collocations of 14 testing sentiment adjec-
tives are extracted from a 100-million-word cor-
pus. Collocations with obvious and consistent 
sentiment are manually identified. 412 positive 
and 191 negative collocations are established as 
the seed collocation set.  
We think that the polarity of a word can be de-
termined by exploiting the association of its co-
occurring words in sentence. We assume that, the 
two instances of an ambiguous sentiment adjec-
tives that have similar neighboring nouns may 
have the same polarity. Gamon and Aue (2005) 
made an assumption to label sentiment terms. 
We extract 13,859 sentences containing collo-
cations between negative adjective and targets 
in seed collocation set or collocations between 
ambiguous adjective and negative modifier 
(such as ?? too) as the training data. These 
sentences are assume negative. A single-class 
classifier is then trained to recognize negative 
sentences. Three types of features are used:  
(1) Context features include bag of words 
within context in window of [-5, +5] 
(2) Collocation features contain bi-grams in 
window [-5,+5] 
(3) Collocation features contain n-grams in 
window [-5,+5] 
In our research, SVM with linear kernel is 
employed and the open source SVM package ? 
LIBSVM is selected for the implementation.  
4 The Improved System 
The preliminary experiment shows that the base-
line system is not satisfactory, especially the 
449
coverage is low. It is observed that the seed col-
location set covers 17.54% of sentences contain-
ing the ambiguous adjectives while the colloca-
tions between adjective and negative modifier 
covers only 11.28%. Therefore, we expand the 
sentiment adjective-target collocation set based 
on word similarity and a semi-supervised learn-
ing algorithm orderly. We then incorporate both 
inner-sentence features (collocations, context 
words, etc.) and inter-sentence features in the 
improved systems for sentiment adjectives dis-
ambiguation.  
4.1 Collocation Set Expansion based on 
Word Similarity 
First, we expand the seed collocation set on the 
target side. The words strongly similar to known 
targets are identified by using a word similarity 
calculation package, provided by HowNet (a 
Chinese thesaurus). Once these words co-occur 
with adjective within a context window more 
often than a threshold, they are appended to seed 
collocation set. For example, ??-??(low ca-
pacity)?is expanded from a seed collocation ??
-?? (low capacity)?. 
Second, we manually identify the words hav-
ing the same ?trend? as the testing adjectives. 
For example, ??? increase? is selected as a 
same-trend word of ?? high?. The collocations 
of ???? are extracted from corpus. Its collo-
cated targets with confident and consistent sen-
timent are appended to the sentiment collocation 
set of ??? if they co-occurred with ??? more 
than a threshold. In this way, some low-
frequency sentiment collocation can be obtained. 
4.2 Semi-supervised Learning of Sentiment 
Collocations 
A semi-supervised learning algorithm is devel-
oped to further expand the collocation seed set, 
which is described as follows. (It is revised based 
on our previous research (Xu et al 2008). The 
basic assumption here is that, the sentiment of a 
sentence having ambiguous adjectives can be 
estimated based on the sentiment of its neighbor-
ing sentences.  
 
Input: Raw training corpus, labeled as Su,  
Step 1. The sentences holding strong polarities 
are recognized from Su which satisfies any two of 
following requirements, (1) contains known con-
text-free sentiment word (CFSW); (2) contains 
more than three known context-dependent senti-
ment words (CDSW); (3) contains collocations 
between degree adverbs and known CDSWs; (4) 
contains collocations between degree adverbs 
and opinion operators (the verbs indicate a opi-
nion operation, such as?? praise); (5) contains 
known opinion indicator and known CDSWs. 
Step 2. Identify the strong non-opinionated sen-
tences in Su. The sentences satisfying all of fol-
lowing four conditions are recognized as non-
opinionated ones, (1) have no known sentiment 
words; (2) have no known opinion operators; (3) 
have no known degree adverbs and (4) have no 
known opinion indicators.  
Step 3. Identify the opinion indicators in the rest 
sentences. Determine their polarities if possible 
and mark the conjunction (e.g.? and) or nega-
tion relationship (e.g.? but) in the sentences. 
Step 4. Match the CFSWs and known CDSWs in 
Su. The polarities of CFSWs are assigned based 
on sentiment lexicon.  
Step 5. If a CDSW occurs in a sentence with cer-
tain orientations which is determined by the opi-
nion indicators, its polarity is assigned as the 
value suggested. If a CDSW co-occur with a 
seed collocated target, it polarity is assigned ac-
cording to the seed sentiment collocation set. 
Otherwise, if a CDSW co-occur with a CFSW in 
the same sentence, or the neighboring continual 
or compound sentence, the polarity of CDSW is 
assigned as the same as CFSW, or the reversed 
polarity if a negation indicator is detected. 
Step 6. Update the polarity scores of CDSWs in 
the target set by using the cases where the polari-
ty is determined in Step 5. 
Step 7. Determine the polarities of CDSWs in 
the undetermined sentences. Suppose Si is a sen-
tence and the polarity scores of all its CFSWs 
and CDSWs are known, its polarity, labeled as 
Plo(Si), is estimated by using the polarity scores 
of all of the opinion words in this sentence, viz.: 
  
? ?+ ?= )(_)(_ )(_)(pos_)( CDSWnegPCDSWposP CFSWnegPCFSWPSiPlo (3) 
A large value (>0) of Plo(si) implies that si tends 
to be positive, and vice versa.  
Step 8. If the sentence polarity cannot be deter-
mined by its components, we use the polarity of 
its neighboring sentences sj-1 and sj+1, labeled as 
Plo(sj-1) and Plo(sj+1), respectively, to help de-
termine Plo(sj), viz.:  
)(5.0)(*)(5.0)( 11 +? ?++?= jjjj sPlosPlosPlosPlo (4) 
where, Plo*(sj) is the polarity score of Sj (Fol-
lowing Equation 3) but ignore the contribution of 
testing adjectives while 0.5 are empirical weights.  
450
Step 9. After all of the polarities of known 
CDSWs in the training data are determined, up-
date the collocation set by identifying co-
occurred pairs with consistent sentiment. 
Step 10. Repeat Step 5 to Step 9 to re-estimate 
the sentiment of CDSWs and expand the colloca-
tion set, until the collocation set converge. 
 
In this way, the seed collocation set is further 
expanded and their sentiment characteristics are 
obtained.  
4.3 Sentiment Adjectives Classifier 
We incorporate the following 8 groups of fea-
tures in a linear-kernel two-class SVM classifier 
to classify the sentences with sentiment adjec-
tives into positive or negative: 
(1) The presence of known positive/negative 
opinion indicator and opinion operator 
(2) The presence of known positive/negative 
CFSW 
(3) The presence of known positive/negative 
CDSW(exclude the testing adjectives) 
(4) The presence of known positive/negative 
adjective-target bi-gram collocations 
(5) The presence of known positive/negative 
adjective-target n-gram collocations 
(6) The coverage of context words surround-
ing the adjectives in the context words in 
training positive/negative sentences 
(7) The sentiment of -1 sentence 
(8) The sentiment of +1 sentence 
The classifier is trained by using the sentences 
with determined sentiment which is obtained in 
the semi-supervised learning stage. 
5 Evaluations and Conclusion 
The ACL-SEMEVAL task 18 testing dataset 
contains 14 ambiguous adjectives and 2,917 in-
stances. HITSZ_CITYU group submitted three 
runs. Run-1 and Run-2 are two runs correspond-
ing to the improved system and Run-3 is the 
baseline system. The achieved performances are 
listed in Table 1.  
 
Run ID Marco Accuracy Micro Accuracy
1 0.953 0.936 
2 0.957 0.933 
3(baseline) 0.629 0.665 
Table 1: Performance of HITSZ_CITYU Runs 
 
It is observed that the improved systems 
achieve promising results which is obviously 
higher than the baseline. They are ranked 1st and 
2nd in Macro Accuracy evaluation and 2nd and 3rd 
in Micro Accuracy evaluation among 16 submit-
ted runs, respectively. 
6 Conclusion 
In this paper, we proposed similarity-based and 
semi-supervised based methods to expand the 
adjective-target seed collocation set. Meanwhile, 
we incorporate both inner-sentence (collocations 
and context words) and inter-sentence features in 
a two-class SVM classifier for the disambigua-
tion of sentiment adjectives. The achieved prom-
ising results show the effectiveness of colloca-
tion features, context words features and senti-
ment of neighboring sentences. Furthermore, we 
found that the neighboring sentence sentiments 
are important features for the disambiguation of 
sentiment ambiguous adjectives, which is ob-
viously different from the traditional word sense 
disambiguation that emphasize the inner-
sentence features. 
References  
Andreevskaia, A. and Bergler, S. 2006. Mining 
WordNet for fuzzy sentiment: Sentiment tag ex-
traction from WordNet glosses. In Proceedings of 
EACL 2006, pp. 209-216 
Esuli, A. and Sebastian, F. 2006. SENTIWORDNET: 
A publicly available lexical resource for opinion 
mining. In Proceeding of LREC 2006, pp. 417-422. 
Hatzivassiloglou, V. and McKeown, K. R. 1997. Pre-
dicting the semantic orientation of adjectives. In 
Proceeding of ACL 1997, pp.174-181 
Michael Gamon and Anthony Aue. 2005. Automatic 
identification of sentiment vocabulary: Exploiting 
low association with known sentiment terms. In 
Proceedings of the ACL05 Workshop on Feature 
Engineering for Machine Learning in Natural 
Language Processing, pp.57-64 
Ruifeng Xu, Kam-Fai Wong et al 2008. Learning 
Knowledge from Relevant Webpage for Opinion 
Analysis, in Proceedings of 2008 IEEE / WIC / 
ACM Int. Conf. Web Intelligence, pp. 307-313  
Turney, P. D. and Littman, M. L. 2003. Measuring 
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on In-
formation Systems, vol. 21, no. 4, pp.315-346 
Yunfang Wu, Miao Wang and Peng Jin. 2008. Dis-
ambiguating sentiment ambiguous adjectives, In 
Proceedings of Int. Conf. on Natural Language 
Processing and Knowledge Engineering 2008, pp. 
1-8 
451
 Combine Person Name and Person Identity Recognition and Docu-
ment Clustering for Chinese Person Name Disambiguation 
Ruifeng Xu1,2,Jun Xu1,Xiangying Dai1
Harbin Institute of Technology,  
Shenzhen Postgraduate School, China 
{xuruifeng.hitsz;hit.xujun; 
mi-chealdai}@gmail.com 
     Chunyu Kit2 
         2City University of Hong Kong,    
        Hong Kong, China 
    ctckit@cityu.edu.hk 
 
Abstract 
This paper presents the HITSZ_CITYU 
system in the CIPS-SIGHAN bakeoff 
2010 Task 3, Chinese person name dis-
ambiguation. This system incorporates 
person name string recognition, person 
identity string recognition and an agglo-
merative hierarchical clustering for 
grouping the documents to each identical 
person. Firstly, for the given name index 
string, three segmentors are applied to 
segment the sentences having the index 
string into Chinese words, respectively. 
Their outputs are compared and analyzed. 
An unsupervised clustering is applied 
here to help the personal name recogni-
tion. The document set is then divided 
into subsets according to each recog-
nized person name string. Next, the sys-
tem identifies/extracts the person identity 
string from the sentences based on lex-
icon and heuristic rules. By incorporat-
ing the recognized person identity string, 
person name, organization name and 
contextual content words as features, an 
agglomerative hierarchical clustering is 
applied to group the similar documents 
in the document subsets to obtain the fi-
nal person name disambiguation results. 
Evaluations show that the proposed sys-
tem, which incorporates extraction and 
clustering technique, achieves encourag-
ing recall and good overall performance. 
1 Introduction 
Many people may have the same name which 
leads to lots of ambiguities in text, especially for 
some common person names. This problem puz-
zles many information retrieval and natural lan-
guage processing tasks. The person name ambi-
guity problem becomes more serious in Chinese 
text. Firstly, Chinese names normally consist of 
two to four characters. It means that for a two-
character person name, it has only one character 
as surname to distinguish from other person 
names with the same family name. It leads to 
thousands of people have the same common 
name, such ??  and ?? . Secondly, some 
three-character or four-character person name 
may have one two-character person name as its 
substring such as ?? and ???, which leads 
to more ambiguities. Thirdly, some Chinese per-
son name string has the sense beyond the person 
name. For example, a common Chinese name, 
?? has a sense of ?Peak?. Thus, the role of a 
string as person name or normal word must be 
determined. Finally, Chinese text is written in 
continuous character strings without word gap. It 
leads to the problem that some person names 
may be segmented into wrong forms.  
In the recent years, there have been many re-
searches on person name disambiguation 
(Fleischman and Hovy 2004; Li et al 2004; Niu 
et al 2004; Bekkerman and McCallum 2005; 
Chen and Martin 2007; Song et al 2009). To 
promote the research in this area, Web People 
Search (WePS and WePS2) provides a standard 
evaluation, which focuses on information extrac-
tion of personal named-entities in Web data (Ar-
tiles et al, 2007; Artiles et al, 2009; Sekine and 
Artiles, 2009). Generally speaking, both cluster-
based techniques which cluster documents cor-
responding to one person with similar contexts, 
global features and document features (Han et al 
2004; Pedersen et al 2005; Elmacioglu et al 
2007; Pedersen and Anagha 2007; Rao et al 
2007) and information extraction based tech-
niques which recognizes/extracts the description 
features of one person name (Heyl and Neumann 
2007; Chen et al 2009) are adopted. Consider-
ing that these evaluations are only applied to 
English text, CIPS-SIGHAN 2010 bakeoff pro-
posed the first evaluation campaign on Chinese 
person name disambiguation. In this evaluation, 
corresponding to given index person name string, 
the systems are required to recognize each iden-
tical person having the index string as substring 
and classify the document corresponding to each 
identical person into a group. 
This paper presents the design and implemen-
tation of HITSZ_CITYU system in this bakeoff. 
This system incorporates both recogni-
tion/extract technique and clustering technique 
for person name disambiguation. It consists of 
two major components. Firstly, by incorporating 
word segmentation, named entity recognition, 
and unsupervised clustering, the system recog-
nize the person name string in the document and 
then classify the documents into subsets corres-
ponding to the person name. Secondly, for the 
documents having the same person name string, 
the system identifies the person identify string, 
other person name, organization name and con-
textual context words as features. An agglomera-
tive hierarchical clustering algorithm is applied 
to cluster the documents to each identical person. 
In this way, the documents corresponding to 
each identical person are grouped, i.e. the person 
name ambiguities are removed. The evaluation 
results show that the HITSZ_CITYU system 
achieved 0.8399(B-Cubed)/0.8853(P-IP) preci-
sions and 0.9329(B-Cubed)/0.9578(P-IP) recall, 
respectively. The overall F1 performance 
0.8742(B-Cubed)/0.915(P-IP) is ranked 2nd in 
ten participate teams. These results indicate that 
the proposed system incorporating both extrac-
tion and clustering techniques achieves satisfac-
tory recall and overall performance. 
The rest of this report is organized as follows. 
Section 2 describes and analyzes the task. Sec-
tion 3 presents the word segmentation and per-
son name recognition and Section 4 presents the 
person description extraction and document 
clustering. Section 5 gives and discusses the 
evaluation results. Finally, Section 6 concludes.  
2 Task Description 
CIPS-SIGHAN bakeoff on person name disam-
biguation is a clustering task. Corresponding to 
26 person name query string, the systems are 
required to cluster the documents having the in-
dex string into multiple groups, which each 
group representing a separate entity.   
HITSZ_CITYU system divided the whole 
task into two subtasks: 
1. Person name recognition. It includes:  
1.1  Distinguish person name/ non person 
name in the document. For a given index 
string ??, in Example 1, ?? is a person 
name while in Example 2, ?? is a noun 
meaning ?peak? rather than a person name. 
Example 1. ????????????
???????(Gaofeng, the Negotiator 
and professor of Beijing People's Police 
College, said). 
Example 2. ??????? 11.83%??
?? (This value raise to the peak value of 
11.83%). 
1.2  Recognize the exact person name, espe-
cially for three-character to four-character 
names. For a given index string, ??, a 
person name ?? should be identified in 
Example 3 while??? should be identi-
fied from Example 4. 
Example 3. ????????????
??????? (Li Yan from Chinese 
team one is the highest one in the female 
athletes participating this game). 
Example 4. ?????????  (The 
soldier Li YanQing is an orphan) 
2. Cluster the documents for each identical 
person. That is for each person recognized 
person name, cluster documents into groups 
while each group representing an individual 
person.  For the non person names instances 
(such as Example 2), they are clustered into 
a discarded group. Meanwhile, the different 
person with the same name should be sepa-
rated. For example, ?? in the Example 3 
and Example 5 is a athlete and a painter, re-
spectively. These two sentences should be 
cluster into different groups. 
Example 5. ????????????
????(The famous painter Li Yan , who 
involved in hosting this exhibition, said that) 
3 Person Name Recognition 
As discussed in Section 2, HITSZ_CITYU sys-
tem firstly recognizes the person names from the 
text including distinguish the person name/ non-
person name word and recognize the different 
person name having the name index string. In 
our study, we adopted three developed word 
segmentation and named entity recognition tools 
to generate the person name candidates. The 
three tools are: 
1. Language Processing Toolkit from Intel-
ligent Technology & Natural Language 
Processing Lab (ITNLP) of Harbin Insti-
tute of Technology (HIT).  
http://www.insun.hit.edu.cn/ 
2. ICTCLAS from Chinese Academy of 
Sciences. http://ictclas.org/ 
3. The Language Technology Platform from 
Information Retrieval Lab of Harbin Insti-
tute of Technology. http://ir.hit.edu.cn 
We apply the three tools to segment and tag 
the documents into Chinese words. The recog-
nized person name having the name index string 
will be labeled as /nr while the index string is 
labeled as discard if it is no recognized as a per-
son name even not a word.  For the sentences 
having no name index string, we simply vote the 
word segmentation results by as the output. As 
for the sentences having name index string, we 
conduct further analysis on the word segmenta-
tion results.  
1. For the cases that the matched string is 
recognized as person name and non-
person name by different systems, respec-
tively, we selected the recognized person 
name as the output. For example, in  
Example 6. ???????????
????????????? (Secre-
tary for Health, Welfare and Food, Yang 
Yongqiang commended the excellent work 
of Tse Wanwen). 
the segmentation results by three segmen-
tors are ???/nr |discarded|???/nr, 
respectively. We select ???/nr as the 
output. 
2. For the cases that three systems generate 
different person names, we further incor-
porating unsupervised clustering results 
for determination. Here, an agglomerative 
hierarchical clustering with high threshold 
is applied (the details of clustering will be 
presented in Section 4).  
Example 7. ?????? (Zhufang 
overcome three barriers) 
In this example, the word segmentation 
results are ??/nr, ???/nr, ???
/nr, respectively. It is shown that there is 
a segmentation ambiguity here because 
both ?? and ??? are legal Chinese 
person names. Such kinds of ambiguity 
cannot be solved by segmentors indivi-
dually. We further consider the clustering 
results. Since the Example 7 is clustered 
with the documents having the segmenta-
tion results of ??, two votes (emphasize 
the clustering confidence) for ?? are as-
signed. Thus, ?? and ??? obtained 3 
votes and 2 votes in this case, respectively, 
and thus ?? is selected as the output. 
3. For cases that the different person name 
forms having the same votes, the longer 
person name is selected. In the following 
example, 
Example 8. ???????????
??????????? (Prof. Zhang 
Mingxuan, the deputy director of Shang-
hai Municipal Education Commission, 
said at the forum) 
The segmentation form of ?? and ??
?  received the same votes, thus, the 
longer one??? is selected as the out-
put. 
In this component, we applied three segmen-
tors (normally using the local features only) with 
the help of clustering to (using both the local and 
global features) recognize person name in the 
text with high accuracy. It is important to ensure 
the recall performance of the final output. Noted, 
in order to ensure the high precision of cluster-
ing, we set a high similarity threshold here.  
4 Person Name Disambiguation 
4.1 Person Identity Recognition/Extraction 
A person is distinguished by its associated 
attributes in which its identity description is es-
sential. For example, a person name has the 
identity of ?? president and ?? farmer, re-
spectively, tends to be two different persons. 
Therefore, in HITSZ_CITYU system, the person 
identity is extracted based on lexicon and heuris-
tic rules before person name disambiguation. 
We have an entity lexicon consisting of 85 
suffixes and 248 prefix descriptor for persons as 
the initial lexicon. We further expand this lex-
icon through extracting frequently used entity 
words from Gigaword. Here, we segmented 
documents in Gigaword into word sequences. 
For each identified person name, we collect its 
neighboring nouns. The associations between the 
nouns and person name can be estimated by their 
?2 test value. For a candidate entity wa and per-
son name wb, (here, wb is corresponding to per-
son name class with the label /nr), the following 
2-by-2 table shown the dependence of their oc-
currence.  
Table 1 The co-occurrence of two words 
 
awx = awx ?  
bwy =  C11 C12 
bwy ?  C21 C22 
For wa and wb, ?2 test (chi-square test) esti-
mates the differences between observed and ex-
pected values as follows: 
)()()()(
)(
2221221221112211
2
211222112
CCCCCCCC
CCCCN
+++++++
??=?       (1) 
where, N is the total number of words in the 
corpus. The nouns having the ?2 value greater 
than a threshold are extracted as entity descrip-
tors. 
In person entity extraction subtask, for each 
sentence has the recognized person name, the 
system matches its neighboring nouns (-2 to +2 
words surrounding the person name) with the 
entries in entity descriptor lexicon. The matched 
entity descriptors are extracted.  
In this part, several heuristic rules are applied 
to handle some non-neighboring cases. Two ex-
ample rules with cases are given below. 
Example Rule 1. The prefix entity descriptor 
will be assigned to parallel person names with 
the split mark of ?/? , ???and ???,???(and). 
??????? /??  (Chinese players 
Gong Yuechun/Wang Hui)?>  
?? player-??? Gong Yuechun 
?? player-?? Wang Hui 
Example Rule 2. The entity descriptor will be 
assigned to each person in the structure of paral-
lel person name following ??(etc.)? and then a 
entity word. 
???????????????????
??? (The painter, Liu Bingsen, Chen Daz-
hang, Li Yan, Jin Hongjun, etc., paint a.. ) -> 
??? Liu Bingsen - ??? painter 
??? Chen Dazhang - ??? painter 
?? Li Yan - ??? painter 
??? Jin Hongjun - ??? painter 
Furthermore, the HITSZ_CITYU system ap-
plies several rules to identify a special kind of 
person entity, i.e. the reporter or author using 
structure information. For example, in the be-
ginning or the end of a document, there is a per-
son name in a bracket means this person and this 
name appear in the document for only once; 
such person name is regarded as the reporter or 
author. (????????) ?>??? Jin Lin-
peng - ?? reporter 
(??? ??) ?>??? Jin Linpeng - ?? 
reporter 
4.2 Clustering-based Person Name Disam-
biguation 
For the document set corresponding to each giv-
en index person name, we firstly split the docu-
ment set into: (1) Discarded subset, (2) Subset 
with different recognized person name. The sub-
sets are further split into (2-1) the person is the 
author/reporter and (2-2) the person is not the 
author/reporter. The clustering techniques are 
then applied to group documents in each (2-2) 
subset into several clusters which each cluster is 
corresponding to each identical person.  
In the Chinese Person Name Disambiguation 
task, the number of clusters contained in a subset 
is not pre-available. Thus, the clustering method 
which fixes the number of clusters, such as k-
nearest neighbor (k-NN) is not applicable. Con-
sidering that Agglomerative Hierarchical Clus-
tering (AHC) algorithm doesn?t require the fixed 
number of cluster and it performs well in docu-
ment categorization (Jain and Dubes 1988), it is 
adopted in HITSZ_CITYU system. 
Preprocessing and Document Representation 
Before representing documents, a series of pro-
cedures are adopted to preprocess these docu-
ments including stop word removal. Next, we 
select feature words for document clustering. 
Generally, paragraphs containing the target per-
son name usually contain more person-related 
information, such as descriptor, occupation, af-
filiation, and partners. Therefore, larger weights 
should be assigned to these words. Furthermore, 
we further consider the appearance position of 
the features. Intuitively, local feature words with 
small distance are more important than the glob-
al features words with longer distance. 
We implemented some experiments on the 
training data to verify our point. Table 2 and Ta-
ble 3 show the clustering performance achieved 
using different combination of global features 
and local features as well as different similarity 
thresholds.  
Table 2. Performance achieved on training set 
with different weights (similarity threshold 0.1) 
Feature words Precision Recall F-1 
Paragraph 0.820 0.889 0.849 
All 0.791 0.880 0.826 
All+ Paragraph?1 0.791 0.904 0.839 
All+ Paragraph?2 0.802 0.908 0.848 
All+ Paragraph?3 0.824 0.909 0.860 
All+ Paragraph?4 0.831 0.911 0.865 
All+ Paragraph?5 0.839 0.910 0.869 
All+ Paragraph?6 0.833 0.905 0.864 
All+ Paragraph?7 0.838 0.904 0.867 
 
Table 3. Performance achieved on training set 
with different weights (similarity threshold 0.15) 
Feature words Precision Recall F-1 
Paragraph 0. 901       0.873        0.883 
All 0.859        0.867 0.859 
All+ Paragraph?1 0.875 0.887 0.877 
All+ Paragraph?2 0.885 0.890 0.884 
All+ Paragraph?3 0.889 0.887 0.885 
All+ Paragraph?4 0.896 0.887 0.880 
All+ Paragraph?5 0.906 0.882 0.891 
All+ Paragraph?6 0.905 0.884 0.891 
All+ Paragraph?7 0.910 0.882 0.893 
In this two tables, ?Paragraph? means that we 
only select words containing in paragraph which 
contains the person index name as feature words 
(which are the local features), and ?All? means 
that we select all words but stop words in a doc-
ument as feature words. ?All+ Paragraph?k? 
means feature words consist of two parts, one 
part is obtained from ?All?, the other is gained 
from ?Paragraph?, at the same time, we assign 
the feature weights to the two parts, respectively. 
The feature weight coefficient of ?All? is 
)1(1 +k , while the feature weight coefficient of 
?All+ Paragraph?k? is )1( +kk . 
It is shown that, the system perform best using 
appropriate feature weight coefficient distribu-
tion. Therefore, we select all words in the docu-
ment (besides stop words) as global feature 
words and the words in paragraph having the 
index person name as local feature words. We 
then assign the corresponding empirical feature 
weight coefficient to the global/local features, 
respectively. A document is now represented as 
a vector of feature words as follows: 
)))(,());(,());(,(()( 2211 dwtdwtdwtdV nnL?   (2) 
where, d is a document, it  is a feature word, 
)(dwi  is the feature weight of it  in the document 
d . In this paper, we adopt a widely used weight-
ing scheme, named Term Frequency with In-
verse Document Frequency (TF-IDF). In addi-
tion, for each document, we need to normalize 
weights of features because documents have dif-
ferent lengths. The weight of word it in docu-
ment d  is shown as: 
 
? +
+?
=
=
n
i i
i
i
i
i
df
N
dtf
df
N
dtf
dw
1
2))05.0log(*)((
)05.0log()(
)(
        (3)
 
where )(dtf i means how many times word it oc-
curs in the document d , idf  means how many 
documents contains word it , and N  is the num-
ber of documents in the corpus. 
Similarity Estimation 
We use the cosine distance as similarity calcula-
tion function. After the normalization of weights 
of each document, the similarity between docu-
ment 1d  and document 2d  is computed as: 
? ?=
?? 21
2121 )()(),(
ddit
ii dwdwddsim   (4) 
where it  is the term which appears in document 
1d  and document 2d  simultaneously, )( 1dwi  and 
)( 1dwi  are the weights of it  in document 1d  and  
document 2d  respectively. If it  does not appear 
in a document, the corresponding weight in the 
document is zero. 
Agglomerative Hierarchical Clustering (AHC) 
AHC is a bottom-up hierarchical clustering 
method. The framework of AHC is described as 
follows: 
Assign each document to a single cluster. 
Calculate all pair-wise similarities between 
clusters. 
Construct a distance matrix using the similari-
ty values.  
Look for the pair of clusters with the largest 
similarity.  
Remove the pair from the matrix and merge 
them. 
Evaluate all similarities from this new cluster 
to all other clusters, and update the matrix. 
Repeat until the largest similarity in the matrix 
is smaller than some similarity criteria. 
There are three methods to estimate the simi-
larity between two different clusters during the 
cluster mergence: single link method, average 
link method and complete link method (Nallapati 
et al 2004). The three methods define the similar-
ity between two clusters 1c  and 2c  as follows: 
Single link method: The similarity is the 
largest of all similarities of all pairs of docu-
ments between clusters 1c  and 2c  and defined as: 
),(max),(
2,1
21 ji
cjdcid
ddsimccsim
??
=      (5) 
Average link method: The similarity is the 
average of the similarities of all pairs of docu-
ments between clusters 1c  and 2c  and defined as: 
21
1 2
21
),(
),(
cc
ddsim
ccsim
cid cjd
ji
?
? ?
= ? ?         (6) 
Complete link method: The similarity is the 
smallest of all similarities of all pairs of docu-
ments between clusters 1c  and 2c  and defined as: 
),(min),(
2,1
21 ji
cjdcid
ddsimccsim
??
=       (7) 
where, id   and jd   are the documents belongs 
to clusters 1c  and 2c , respectively.  
We evaluated the AHC algorithm with the 
above three link methods. The achieved perfor-
mance are given in Table 4. It is shown that the 
system performs best with the complete link me-
thod. Therefore, the complete link method is 
selected for the bakeoff testing. 
Table 4. Performance achieved on training set 
with different link method 
Similarity 
threshold 
Link method Precision Recall F1 
0.1 Single link 0.048 1.000 0.089 
0.1 Average link 0.839 0.910 0.869 
0.1 Complete link 0.867 0.888 0.874 
0.15 Single link 0.048 1.000 0.089 
0.15 Average link 0.906 0.882 0.891 
0.15 Complete link 0.923 0.868 0.891 
5 Evaluations 
The task organizer provides two set of evalua-
tion criteria. They are purity-based score (usual-
ly used in IR), B-cubed score (used in WePS-2), 
respectively. The details of the evaluation crite-
ria are given in the task overview.  
The performance achieved by the top-3 sys-
tems are shown in Table 5. 
Table 5. Performance of Top-3 Systems 
 B-Cubed P-IP 
System Precision Recall F1 Precision Recall F1 
NEU 0.957 0.883 0.914 0.969 0.925 0.945
HITSZ 0.839 0.932 0.874 0.885 0.958 0.915
DLUT 0.826 0.913 0.863 0.879 0.942 0.907
 
The evaluation results show that the 
HITSZ_CITYU system achieved overall F1 per-
formance of 0.8742(B-Cubed)/ 0.915(P-IP), re-
spectively.   
It is also shown that HITSZ_CITYU achieves 
the highest the recall performance. It shows that 
the proposed system is good at split the docu-
ment to different identical persons. Meanwhile, 
this system should improve the capacity on 
merge small clusters to enhance the precision 
and overall performance. 
6 Conclusions 
The presented HITSZ_CITYU system applies 
multi-segmentor and unsupervised clustering to 
achieve good accuracy on person name string 
recognition. The system then incorporates entity 
descriptor extraction, feature word extraction 
and agglomerative hierarchical clustering me-
thod for person name disambiguation. The 
achieved encouraging performance shown the 
high performance word segmentation/name rec-
ognition and extraction-based technique are 
helpful to improve the cluster-based person 
name disambiguation. 
References 
Andrea Heyl and G?nter Neumann. DFKI2: An In-
formation Extraction based Approach to People 
Disambiguation. Proceedings of ACL SEMEVAL 
2007, 137-140, 2007. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine, The 
SemEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task, Pro-
ceedings of Semeval 2007, Association for Com-
putational Linguistics, 2007. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 
?WePS 2 Evaluation Campaign: Overview of the 
Web People Search Clustering Task, In 2nd Web 
People Search Evaluation Workshop (WePS 2009), 
18th WWW Conference, 2009 
Bekkerman, Ron and McCallum, Andrew, Disambi-
guating Web Appearances of People in a Social 
Network, Proceedings of WWW2005, pp.463-470, 
2005 
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen 
Kan, and Dongwon Lee. PSNUS: Web People 
Name Disambiguation by Simple Clustering with 
Rich Features. Proceedings of ACL SEMEVAL 
2007, 268-271, 2007. 
Fei Song, Robin Cohen, Song Lin, Web People 
Search Based on Locality and Relative Similarity 
Measures, Proceedings of WWW 2009 
Fleischman M. B. and Hovy E., Multi-document Per-
son Name Resolution, Proceedings of ACL-42, 
Reference Resolution Workshop, 2004 
Hui Han , Lee Giles , Hongyuan Zha , Cheng Li , 
Kostas Tsioutsiouliklis, Two Supervised Learning 
Approaches for Name Disambiguation in Author 
Citations, Proceedings of the 4th ACM/IEEE-CS 
joint conference on Digital libraries, 2004 
Jain, A. K. and Dubes, R.C. Algorithms for Cluster-
ing Data, Prentice Hall, Upper Saddle River, N.J., 
1988  
Nallapati, R., Feng, A., Peng, F., Allan, J., Event 
Threading within News Topics, Proceedings of-
CIKM 2004, pp. 446?453, 2004 
Niu, Cheng, Wei Li, and Rohini K. Srihari,Weakly 
Supervised Learning for Cross-document Person 
Name Disambiguation Supported by Information 
Extraction, Proceedings of ACL 2004 
Pedersen, Ted, Amruta Purandare, and Anagha Kul-
karni, Name Discrimination by Clustering Similar 
Contexts, Proceedings of the Sixth International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Mexico, 
2005 
Pedersen, Ted and Anagha Kulkarni, Unsupervised 
Discrimination of Person Names in Web Contexts, 
Proceedings of the Eighth International Confe-
rence on Intelligent Text Processing and Computa-
tional Linguistics, Mexico City, 2007. 
Rao, Delip, Nikesh Garera and David Yarowsky, 
JHU1: An Unsupervised Approach to Person 
Name Disambiguation using Web Snippets, In 
Proceedings of ACL Semeval 2007 
Sekine, Satoshi and Javier Artiles. WePS 2 Evalua-
tion Campaign: overview of the Web People 
Search Attribute Extraction Task, Proceedings of 
2nd Web People Search Evaluation Workshop 
(WePS 2009), 18th WWW Conference, 2009  
Xin Li, Paul Morie, and Dan Roth, Robust Reading: 
Identification and Tracing of Ambiguous Names, 
Proceedings of NAACL,pp. 17-24, 2004. 
Ying Chen, Sophia Yat Mei Lee, Chu-Ren Huang, 
PolyUHK: A Robust Information Extraction Sys-
tem for Web Personal Names, Proceedings of 
WWW 2009 
Ying Chen and Martin J.H. CU-COMSEM: Explor-
ing Rich Features for Unsupervised Web Personal 
Name Disambiguation, Proceedings of ACL Se-
meval 2007 
 
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 182?188,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Instance Level Transfer Learning for Cross Lingual Opinion Analysis
Ruifeng Xu, Jun Xu and Xiaolong Wang
Key Laboratory of Network Oriented Intelligent Computation
Department of Computer Science and Technology
Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China
{xuruifeng,xujun}@hitsz.edu.cn, wangxl@insun.hit.edu.cn
Abstract
This paper presents two instance-level transfer
learning based algorithms for cross lingual
opinion analysis by transferring useful
translated opinion examples from other
languages as the supplementary training
data for improving the opinion classifier in
target language. Starting from the union of
small training data in target language and
large translated examples in other languages,
the Transfer AdaBoost algorithm is applied
to iteratively reduce the influence of low
quality translated examples. Alternatively,
starting only from the training data in target
language, the Transfer Self-training algorithm
is designed to iteratively select high quality
translated examples to enrich the training
data set. These two algorithms are applied to
sentence- and document-level cross lingual
opinion analysis tasks, respectively. The
evaluations show that these algorithms
effectively improve the opinion analysis by
exploiting small target language training data
and large cross lingual training data.
1 Introduction
In recent years, with the popularity of Web 2.0,
massive amount of personal opinions including
comments, reviews and recommendations in dif-
ferent languages have been shared on the Internet.
Accordingly, automated opinion analysis has
attracted growing attentions. Opinion analysis, also
known as sentiment analysis, sentiment classifica-
tion, and opinion mining, aims to identify opinions
in text and classify their sentiment polarity (Pang
and Lee, 2008).
Many sentiment resources such as sentiment
lexicons (e.g., SentiWordNet (Esuli and Sebastiani,
2006))and opinion corpora (e.g., MPQA (Blitzer
et al, 2007)) have been developed on different
languages in which most of them are for English.
The lack of reliably sentiment resources is one
of the core issues in opinion analysis for other
languages. Meanwhile, the manually annotation
is costly, thus the amount of available annotated
opinion corpora are still insufficient for supporting
supervised learning, even for English. These facts
motivate to ?borrow? the opinion resources in one
language (source language, SL) to another language
(target language, TL) for improving the opinion
analysis on the target language.
Cross lingual opinion analysis (CLOA) tech-
niques are investigated to improve opinion analysis
in TL through leveraging the opinion-related
resources, such as dictionaries and annotated
corpus in SL. Some CLOA works used bilingual
dictionaries (Mihalcea et al, 2007), or aligned
corpus (Kim and Hovy, 2006) to align the expres-
sions between source and target languages. These
works are puzzled by the limited aligned opinion
resources. Alternatively, some works used machine
translation system to do the opinion expression
alignment. Banea et al (2008) proposed several
approaches for cross lingual subjectivity analysis by
directly applying the translations of opinion corpus
in source language to train the opinion classifier
on target language. Wan (2009) combined the
annotated English reviews, unannotated Chinese
reviews and their translations to co-train two
separate classifiers for each language, respectively.
182
These works directly used all of the translation of
annotated corpus in source language as the training
data for target language without considering the
following two problems: (1) the machine translation
errors propagate to following CLOA procedure; (2)
The annotated corpora from different languages are
collected from different domains and different writ-
ing styles which lead the training and testing data
having different feature spaces and distributions.
Therefore, the performances of these supervised
learning algorithms are affected.
To address these problems, we propose two
instance level transfer learning based algorithms
to estimate the confidence of translated SL ex-
amples and to transfer the promising ones as
the supplementary TL training data. We firstly
apply Transfer AdaBoost (TrAdaBoost) (Dai et
al., 2007) to improve the overall performance with
the union of target and translated source language
training corpus. A boosting-like strategy is used
to down-weight the wrongly classified translated
examples during iterative training procedure. This
method aims to reduce the negative affection of low
quality translated examples. Secondly, we propose
a new Transfer Self-training algorithm (TrStr). This
algorithm trains the classifier by using only the
target language training data at the beginning. By
automatically labeling and selecting the translated
examples which is correct classified with higher
confidence, the classifier is iteratively trained by
appending new selected training examples. The
training procedure is terminated until no new
promising examples can be selected. Differen-
t from TrAdaBoost, TrStr aims to select high
quality translated examples for classifier training.
These algorithms are evaluated on sentence- and
document-level CLOA tasks, respectively. The
evaluations on simplified Chinese (SC) opinion
analysis by using small SC training data and large
traditional Chinese (TC) and English (EN) training
data, respectively, show that the proposed transfer
learning based algorithms effectively improve the
CLOA. Noted that, these algorithms are applicable
to different language pairs.
The rest of this paper is organized as follows.
Section 2 describes the transfer learning based
approaches for opinion analysis. Evaluations and
discussions are presented in Section 3. Finally,
Section 4 gives the conclusions and future work.
2 CLOA via Transfer Learning
Given a large translated SL opinion training data,
the objective of this study is to transfer more high
quality training examples for improving the TL
opinion analysis rather than use the whole translated
training data. Here, we propose to investigate the
instance level transfer learning based approaches.
In the case of transfer learning, the set of trans-
lated training SL examples is denoted by Ts =
{(xi, yi)}ni=1, and the TL training data is denoted
by Tt={(xi, yi)}n+mi=n+1, while the size of Tt is much
smaller than that of Ts, i.e., |m| ? |n|. The idea
of transfer learning is to use Tt as the indicator to
estimate the quality of translated examples. By
appending selected high quality translated examples
as supplement training data, the performance of
opinion analysis on TL is expected to be enhanced.
2.1 The TrAdaBoost Approach
TrAdaBoost is an extension of the AdaBoost
algorithm (Freund and Schapir, 1996). It uses
boosting technique to adjust the sample weight
automatically (Dai et al, 2007). TrAdaBoost joins
both the source and target language training data
during learning phase with different re-weighting
strategy. The base classifier is trained on the
union of the weighted source and target examples,
while the training error rate is measured on the
TL training data only. In each iteration, for a SL
training example, if it is wrongly classified by prior
base classifier, it tends to be a useless examples
or conflict with the TL training data. Thus, the
corresponding weight will be reduced to decrease
its negative impact. On the contrary, if a TL training
example is wrongly classified, the corresponding
weight will be increased to boost it. The ensemble
classifier is obtained after several iterations.
In this study, we apply TrAdaBoost algorithm
with small revision to fit the CLOA task, as de-
scribed in Algorithm 1. Noted that, our revised
algorithm can handle multi-category problem which
is different with original TrAdaBoost algorithm for
binary classification problem only. More details and
theoretical analysis of TrAdaBoost are given in Dai
et al?s work (Dai et al, 2007).
183
Algorithm 1 CLOA with TrAdaBoost.
Input: Ts, translated opinion training data in SL,
n= |Ts|; Tt, training data in TL , m= |Tt|; L,
base classifier; K, iteration number.
1: Initialize the distribution of training samples:
D1(i) = 1/(n+m).
2: for each k ? [1,K] do
3: Get a hypothesis hk by training L with the
combined training set Ts ? Tt using distribu-
tion Dk: hk = L(Ts ? Tt, Dk).
4: Calculate the training error of hk on Tt:
?t =
?n+m
i=n+1
Dk(i)?I[hk(xi) ?=yi]
?n+m
i=n+1 Dk(i)
.
5: if ?t = 0 or ?k ? 1/2 then
6: K = k ? 1, break.
7: end if
8: Set ?k = ?k/(1? ?k), ? = 1/(1 +
?
2 lnn
K ).
9: if hk(xi) ?= yi then
10: Update the distribution:
Dk+1(i) =
{ Dk(i)?
Zk
1 ? i ? n
Dk(i)/?k
Zk
n + 1 ? i ? n + m
, where
Zk is a normalization constant and
?n+m
i=1 Dk+1(i) = 1.
11: end if
12: end for
Output: argmaxy
?K
?K/2?I[hk(x) = y]log(1/?k)
/* I[?] is an indicator function, which equals 1 if the
inner expression is true and 0 otherwise.*/
2.2 The Transfer Self-training Approach
Different from TrAdaBoost which focuses on the
filtering of low quality translated examples, we
propose a new Transfer Self-training algorithm
(TrStr) to iteratively train the classifier through
transferring high quality translated SL training data
to enrich the TL training data. The flow of this
algorithm is given in Algorithm 2.
The algorithm starts from training a classifier
on Tt. This classifier is then applied to Ts, the
translated SL training data. For each category in
Ts (subjective/objective or positive/negative in our
different experiments), top p correctly classified
translated examples are selected. These translated
examples are regarded as high quality ones and thus
they are appended in the TL training data. Next, the
classifier is re-trained on the enriched training data.
The updated classifier is applied to SL examples
again to select more high quality examples. Such
Algorithm 2 CLOA with Transfer Self-training.
Input: Ts, translated opinion training data in SL,
n= |Ts|; Tt, training data in TL , m= |Tt|; L,
base classifier; K, iteration number.
1: T0 = Tt, k = 1.
2: Get a hypothesis hk by training a base classifier
L with the training set Tk?1.
3: for each instance (xi, yi) ? Ts do
4: Use hk to label (xi, yi) .
5: if ht(xi) = yi then
6: Add (xi, yi)to T ?
7: end if
8: end for
9: Choose p instances per class with top confi-
dence from T ? and denote the set as Tp.
10: Tk = Tk?1
?
Tp, Ts = Ts ? Tp.
11: k = k + 1.
12: Iterate K times over steps 2 to 11 or repeat until
Tp = ?.
Output: Final classifier by using the enriched train-
ing set Tk.
procedure terminates until the increments are less
than a specified threshold or the maximum number
of iterations is exceeded. The final classifier is
obtained by training on the union of target data and
selected high quality translated SL training data.
3 Evaluation and Discussion
The proposed approaches are evaluated on sentence-
and document-level opinion analysis tasks in the
bi-lingual case, respectively. In our experiments,
the TL is simplified Chinese (SC) and the SL for
the two experiments are English (EN)/traditional
Chinese (TC) and EN, respectively.
3.1 Experimental Setup
3.1.1 Datasets
In the sentence-level opinionated sentence recog-
nition experiment , the dataset is from the NTCIR-7
Multilingual Opinion Analysis Tasks (MOAT) (Se-
ki et al, 2008) corpora. The information of
this dataset is given in Table 1. Two experi-
ments are performed. The first one is denoted by
SenOR : TC ? SC, which uses TCs as source
language training dataset, while the second one
184
is SenOR : EN ? SC, which uses ENs1. SCs
is shrunk to different scale as the target language
training corpus by random. The opinion analysis
results are evaluated with simplified Chinese testing
dataset SCt under lenient and strict evaluation
standard 2, respectively, as described in (Seki et al,
2008).
Note Lang. Data Total subjective/objectiveLenient Strict
SCs SC Training 424 130/294 \SCt Test 4877 1869/3008 898/2022
TCs TC Training 1365 740/625 \
ENs EN Training 1694 648/1046 \
Table 1: The NTCIR-7 MOAT Corpora(unit:sentence).
In the document-level review polarity classifi-
cation experiment,, we used the dataset adopted
in (Wan, 2009). Its English subset is collected by
Blitzer et al (2007), which contains a collection of
8,000 product reviews about four types of products:
books, DVDs, electronics and kitchen appliances.
For each type of products, there are 1,000 positive
reviews and 1,000 negative ones, respectively. The
Chinese subset has 451 positive reviews and 435
negative reviews of electronics products such as
mp3 players, mobile phones etc. In our experiments,
the Chinese subset is further split into two parts
randomly: TL training dataset and test set. The
cross lingual review polarity classification task is
then denoted by DocSC: EN?SC.
In this study, Google Translate3 is choose for pro-
viding machine translation results.
3.1.2 Base Classifier and Baseline Methods
This study focus on the approaches improving the
opinion analysis by using cross lingual examples,
while the classifier improving on target language is
not our major target. Therefore, in the experiments,
a Support Vector Machines (SVM) with linear
kernel is used as the base classifier. We use the
1There are only 248 sentences in NTCIR-7 MOAT English
training data set. It is too small to use for CLOA. We s-
plit some samples from the test set to build a new English
dataset for training, which contains all sentences from topics:
N01,N02,T01,N02,N03,N04,N05,N06 and N07.
2All sentences are annotated by 3 assessors, strict standard
means all 3 assessors have the same annotation and lenient
means any 2 of them have the same annotation.
3http://translate.google.com/
open source SVM package ?LIBSVM(Chang and
Lin, 2001) with all default parameters. In the
opinionated sentence recognition experiment, we
use the presences of following linguistic features
to represent each sentence example including
opinion word, opinion operator, opinion indicator,
the unigram and bigram of Chinese words. It is
developed with the reference of (Xu et al, 2008).
In the review polarity classification experiment, we
use unigram, bigram of Chinese words as features
which is suggested by (Wan, 2009). Here, document
frequency is used for feature selection. Meanwhile,
term frequency weighting is chosen for document
representation.
In order to investigate the effectiveness of the
two proposed transfer learning approaches, they
are compared with following baseline methods: (1)
NoTr(T), which applies SVM with only TL training
data; (2) NoTr(S),which applies SVM classifier with
only the translated SL training data; (3) NoTr(S&T),
which applies SVM with the union of TL and SL
training data.
3.1.3 Evaluation Criteria
Accuracy (Acc), precision (P), recall (R) and F-
measure (F1) are used as evaluation metrics. All the
performances are the average of 10 experiments.
3.2 Experimental Results and Discussion
Here, the number of iterations in TrAdaBoost is set
to 10 in order to avoid over-discarding SL examples.
3.2.1 Sentence Level CLOA Results
The achieved performance of the opinionated
sentence recognition task under lenient and strict
evaluation are given in Table 2 respectively, in
which only 1/16 target train data is used as Tt.
It is shown that NoTr(T) achieves a acceptable
accuracy, but the recall and F1 for ?subjective?
category are obviously low. For the two sub-tasks,
i.e. SenOR : TC ?SC and SenOR :EN ?SC
tasks, the accuracies achieved by NoTr(S&T) are
always between that of NoTr(T) and NoTr(S).
The reason is that some translated examples from
source language may likely conflict with the target
language training data. It is shown that the direct
use of all of the translated training data is infeasible.
It is also shown that our approaches achieve better
185
Method Sub-task
Lenient Evaluation Strict Evaluation
Acc subjective objective Acc subjective objectiveP R F1 P R F1 P R F1 P R F1
NoTr(T) .6254 .534 .3468 .355 .6824 .7985 .7115 .6922 .5259 .3900 .3791 .7725 .8264 .7776
NoTr(S)
TC
?
SC
.6059 .4911 .7828 .6035 .7861 .4960 .6082 .6448 .4576 .8352 .5912 .8845 .5603 .6860
NoTr(S&T) .6101 .4943 .7495 .5957 .7711 .5236 .6235 .6531 .4632 .8051 .588 .8714 .5856 .7004
TrAdaBoost .6533 .5335 .7751 .6314 .8063 .5777 .6720 .7184 .5273 .8473 .6494 .9077 .6611 .7643
TrStr .6625 .5448 .7309 .6238 .7884 .6199 .6934 .7304 .5414 .8182 .6511 .896 .6914 .7801
NoTr(S)
EN
?
SC
.6590 .5707 .4446 .4998 .6966 .7922 .7413 .7390 .5872 .5100 .5459 .7944 .8408 .8169
NoTr(S&T) .6411 .5292 .5759 .5515 .7212 .6817 .7009 .7105 .5254 .608 .5637 .8129 .7560 .7834
TrAdaBoost .6723 .5988 .4371 .5018 .7019 .8184 .7549 .7630 .6485 .5019 .5614 .8002 .8789 .8371
TrStr .6686 .5691 .5746 .5678 .7360 .7271 .7292 .7484 .589 .6276 .6026 .8315 .8021 .8147
Table 2: The Performance of Opinionated Sentence Recognition Task.
performance on both tasks while few TL training
data is used. In which, TrStr performances the
best on SenOR:TC?SC task while TrAdaBoost
outperforms other methods on SenOR :EN?SC
task. The proposed transfer learning approaches
enhanced the accuracies achieved by NoTr(S&T)
for 4.2-8.6% under lenient evaluation and 5.3-11.8%
under strict evaluation, respectively.
3.2.2 Document Level CLOA Results
Method Acc positive negativeP R F1 P R F1
NoTr(T) .7542 .7447 .8272 .7747 .8001 .6799 .7235
NoTr(S) .7122 .6788 .8248 .7447 .7663 .5954 .6701
NoTr(S&T) .7531 .714 .8613 .7801 .8187 .6415 .7179
TrAdaBoost .7704 .8423 .6594 .7376 .7285 .8781 .7954
TrStr .7998 .8411 .7338 .7818 .7727 .8638 .8144
Table 3: The Results of Chinese Review Polarity Classi-
fication Task (Features:Unigrams; m=20).
Method Acc positive negativeP R F1 P R F1
NoTr(T) .7518 .7399 .8294 .7741 .7983 .6726 .7185
NoTr(S) .7415 .7143 .8204 .7637 .7799 .6598 .7148
NoTr(S&T) .7840 .7507 .8674 .8035 .8385 .6982 .7592
TrAdaBoost .7984 .8416 .7297 .7792 .7707 .8652 .8138
TrStr .8022 .8423 .7393 .7843 .7778 .8634 .8164
Table 4: The Results of Chinese Review Polarity Classi-
fication Task (Features:Unigrams+Bigrams; m=20).
Table 3 and Table 4 give the achieved results of
different methods on the task DocSC : EN?SC
by using 20 Chinese annotated reviews as Tt. It is
shown that transfer learning approaches outperform
other methods, in which TrStr performs better than
TrAdaBoost when unigram+bigram features are
used. Compared to NoTr(T&S), the accuracies
are increased about 1.8-6.2% relatively. Overall,
the transfer learning approaches are shown are
beneficial to TL polarity classification.
3.2.3 Influences of Target Training Corpus Size
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(a) SenOR : TC ? SC
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(b) SenOR : EN ? SC
Figure 1: Performances with Different Size of SCs on
Opinionated Sentence Recognition Task under Lenient E-
valuation
In order to estimate the influence of different size
of TL training data, we conduct a set of experiments
on both tasks. Fig 1 and Fig 2 show the influence
186
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  20  30  40  50  60  70  80  90  100
Ac
cu
ra
cy
Number of Target Training Instances
Transfer Self-training
TrAdaBoost
NoTr(S&T)
NoTr(T)
(a) Unigrams
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  20  30  40  50  60  70  80  90  100
Ac
cu
ra
cy
Number of Target Training Instances
Transfer Self-training
TrAdaBoost
NoTr(S&T)
NoTr(T)
(b) Unigrams+Bigrams
Figure 3: Performances with Different Number of TL Training Instances on Task of DocSC: EN?SC
 0.6
 0.65
 0.7
 0.75
 0.8
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(a) SenOR : TC ? SC
 0.6
 0.65
 0.7
 0.75
 0.8
1/32 1/16 1/8 1/4 1/2 1
Acc
ura
cy
Size of  Target Languae Training Data (SCs)
NoTr(T)NoTr(T&S) TrAdaBoostTransfer Self-training
(b) SenOR : EN ? SC
Figure 2: Performances with Different Size of SCs on
Opinionated Sentence Recognition Task under Strict E-
valuation
on the opinionated sentence recognition task under
lenient and strict evaluation respectively. Fig 3
shows the influence on task DocSC : EN ?SC.
Fig 3(a) shows the results use unigram features
and Fig 3(b) uses both unigrams and bigrams. It is
observed that TrAdaBoost and TrStr achieve better
performances than the baseline NoTr(S&T) in most
cases. More specifically, TrStr performs the best
when few TL training data is used. When more TL
training data is used, the performance improvements
by transfer learning approaches become small. The
reason is that less target training data is helpful to
transfer useful knowledge in translated examples.
If too much TL training data is used, the weights
of SL instances may decrease exponentially after
several iterations, and thus more source training
data is not obviously helpful.
4 Conclusions and Future Work
To address the problems in CLOA caused by inac-
curate translations and different domain/category
distributions between training data in different
languages, two transfer learning based algorithms
are investigated to transfer promising translated SL
training data for improving the TL opinion analysis.
In this study, Transfer AdaBoost and Transfer
Self-Training algorithms are investigated to reduce
the influences of low quality translated examples
and to select high quality translated examples,
respectively. The evaluations on sentence- and
document-level opinion analysis tasks show that the
proposed algorithms improve opinion analysis by
using the union of few TL training data and selected
cross lingual training data.
One of our future directions is to develop other
transfer leaning algorithms for CLOA task. Another
future direction is to employ other moderate weight-
ing scheme on source training dataset to reduce the
over-discarding of training examples from source
language.
187
References
Bo Pang and Lillian Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trends
in Information Retrieval, 2(1?2):1?135.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource
for opinion mining. Proceedings of the 5th Inter-
national Conference on Language Resources and
Evaluation, 417?422.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and e-
motions in language. Language Resources and E-
valuation, 39(2?3):165?210.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. Proceedings of the
45th Annual Meeting of the Association of Com-
putational Linguistics, Prague, Czech Republic.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. Proceedings of
HLT/NAACL-2006, 200?207.
Carmen Banea, Rada Mihalcea, Janyce Wiebe and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, Honolulu, Hawaii,
127?135.
Xiaojun Wan. 2009. Co-training for cross-lingual
sentiment classification. Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP
of the AFNLP, Suntec, Singapore, 235?243.
Wenyuan Dai ,Qiang Yang, GuiRong Xue and Yong
Yu. 2007. Boosting for transfer learning. Pro-
ceedings of the 24th International Conference on
Machine Learning, 193?200.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: domain adaptation for sentiment classi-
fication. Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics,
440?447.
Xiaojun Wan. 2008. Using bilingual knowledge
and ensemble techniques for unsupervised chi-
nese sentiment analysis. Proceedings of EMNLP
2008,553?561.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. Proceedings
of the 13th International Conference on Machine
Learning, 148?156.
Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, and Noriko Kand. 2008. Overview
of multilingual opinion analysis task at NTCIR-7.
Proceeding of NTCIR-7, NII, Tokyo, 185?203.
Ruifeng Xu, Kam-Fai Wong, Qin Lu, and Yunqing
Xia 2008. Learning Multilinguistic Knowledge
for Opinion Analysis. D. S. Huang et al, edi-
tors:Proceedings of ICIC 2008, volume 5226 of L-
NCS, 993?1000, Springer-Verlag.
Chih-Chung Chang and Chih-Jen Lin.
2001. LIBSVM: a library for support
vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
188
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 107?112,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Incorporating Rule-based and Statistic-based Techniques for 
Coreference Resolution 
 
 
Ruifeng Xu, Jun Xu, Jie Liu, Chengxiang Liu, Chengtian Zou, Lin Gui, Yanzhen 
Zheng, Peng Qu
Human Language Technology Group, Key Laboratory of Network Oriented Intelligent 
Computation, Shenzhen Graduate School, Harbin Institute of Technology, China 
{xuruifeng.hitsz;hit.xujun;lyjxcz;matitalk;chsky.zou;monta3pt;
zhyz.zheng;viphitqp@gmail.com} 
 
 
Abstract 
This paper describes a coreference resolution 
system for CONLL 2012 shared task 
developed by HLT_HITSZ group, which 
incorporates rule-based and statistic-based 
techniques. The system performs coreference 
resolution through the mention pair 
classification and linking. For each detected 
mention pairs in the text, a Decision Tree (DT) 
based binary classifier is applied to determine 
whether they form a coreference. This 
classifier incorporates 51 and 61 selected 
features for English and Chinese, respectively. 
Meanwhile, a rule-based classifier is applied to 
recognize some specific types of coreference, 
especially the ones with long distances. The 
outputs of these two classifiers are merged. 
Next, the recognized coreferences are linked to 
generate the final coreference chain. This 
system is evaluated on English and Chinese 
sides (Closed Track), respectively. It achieves 
0.5861 and 0.6003 F1 score on the 
development data of English and Chinese, 
respectively. As for the test dataset, the 
achieved F1 scores are 0.5749 and 0.6508, 
respectively. This encouraging performance 
shows the effectiveness of our proposed 
coreference resolution system. 
1 Introduction 
Coreference resolution aims to find out the 
different mentions in a document which refer to the 
same entity in reality (Sundheim and Beth, 1995; 
Lang et al 1997; Chinchor and Nancy, 1998;). It is 
a core component in natural language processing 
and information extraction.  Both rule-based 
approach (Lee et al 2011) and statistic-based 
approach (Soon et al, 2001; Ng and Cardie, 2002; 
Bengtson and Roth, 2008; Stoyanov et al, 2009; 
Chen et al 2011) are proposed in coreference 
resolution study. Besides the frequently used 
syntactic and semantic features, the more linguistic 
features are exploited in recent works (Versley, 
2007; Kong et al 2010). 
CoNLL-2012 proposes a shared task, ?Modeling 
multilingual unrestricted coreference in the 
OntoNotes? (Pradhan et al 2012). This is an 
extension of the CoNLL-2011 shared task. The 
task involves automatic anaphoric mention 
detection and coreference resolution across three 
languages including English, Chinese and Arabic. 
HLT_HITSZ group participated in the Closed 
Track evaluation on English and Chinese side. This 
paper presents the framework and techniques of 
HLT_HITSZ system which incorporates both rule-
based and statistic-based techniques. In this system, 
the mentions are firstly identified based on the 
provided syntactic information. The mention pairs 
in the document are fed to a Decision Tree based 
classifier to determine whether they form a 
coreference or not. The rule-based classifiers are 
then applied to recognize some specific types of 
coreference, in particular, the long distance ones. 
Finally, the recognized coreference are linked to 
obtain the final coreference resolution results. This 
system incorporates lexical, syntactical and 
semantic features. Especially for English, WordNet 
is used to provide semantic information of the 
mentions, such as semantic distance and the 
107
category of the mentions and so on. Other than the 
officially provided number and gender data, we 
generated some lexicons from the training dataset 
to obtain the values of some features. This system 
achieves 0.5861 and 0.6003 F1 scores on English 
and Chinese development data, respectively, and 
0.5749 and 0.6508 F1 scores on English and 
Chinese testing data, respectively. The achieved 
encouraging performances show that the proposed 
incorporation of rule-based and statistic-based 
techniques is effective. 
The rest of this report is organized as below. 
Section 2 presents the mention detection. Section 3 
presents the coreference determination and Section 
4 presents the coreference linking. The 
experimental results are given in Section 5 in detail. 
Finally, Section 6 concludes this report. 
2 Mention Detection 
In this stage, the system detects the mentions from 
the text. The pairs of these mentions in one 
document are regarded as the coreference 
candidates. Thus, the high recall is a more 
important target than higher precision for this stage. 
Corresponding to English and Chinese, we adopted 
different detection methods, respectively. 
2.1 Mention Detection - English 
HLT_HITSZ system chooses the marked noun 
phrase (NP), pronouns (PRP) and PRP$ in English 
data as the mentions. The system selects most 
named entities (NE) as the mentions but filter out 
some specific types. Firstly, the NEs which cannot 
be labeled either as NP or NML are filter out 
because there are too cases that the pairs of these 
NEs does not corefer even they are in the same 
form as shown in the training dataset. Second, the 
NEs of ORDINAL, PERCENT and MONEY types 
are filtered because they have very low coreference 
ratio (less than 2%). Furthermore, for the cases that 
NPs overlapping a shorter NP, normally, only the 
longer one are choose. An exception is that if the 
shorter NPs are in parallel structures with the same 
level to construct a longer NP. For example, for a 
NP ?A and B?, ?A?, ?B? and ?A and B? as 
regarded ed as three different mentions.   
2.2 Mention Detection ? Chinese 
HLT_HITSZ system extracts all NPs and PNs as 
the mention candidates. For the NPs have the 
overlaps, we handle them in three ways: 1. For the 
cases that two NPs share the same tail, the longer 
NP is kept and the rest discarded; 2. For cases that 
the longer NP has a NR as its tail, the NPs which 
share the same tail are discarded; 3. In MZ and 
NW folders, they are many mentions nested 
marked as the nested co-referent mentions. The 
system selects the longest NP as mention in this 
stage while the other mention candidates in the 
longest NP will be recalled in the post processing 
stage. 
3 Coreference Determination 
Any pair of two detected mentions in one 
document becomes one coreference candidate. In 
this stage, the classifiers are developed to 
determine whether this pair be a coreference or not. 
During the generation of mention pairs, it is 
observed that linking any two mentions in one 
document as candidates leads to much noises.  The 
statistical observation on the Chinese training 
dataset show that 90% corefered mention pairs are 
in the distance of 10 sentences. Similar results are 
found in the English training dataset while the 
context window is set to 5 sentences. Therefore, in 
this stage, the context windows for generating 
mention pairs as coreference candidates for 
English and Chinese are limited to 5 and 10 
sentences, respectively. 
3.1 The Statistic-based Coreference 
Determination 
The same framework is adopted in the statistical-
based coreference determination for English and 
Chinese, respectively, which is based on a machine 
learning-based statistical classifier and selected 
language-dependent features. Through transfer the 
examples in the training test into feature-valued 
space, the classifier is trained. This binary 
classifier will be applied to determine whether the 
input mention pair be a coreference or not. Here, 
we evaluated three machine learning based 
classifiers including Decision Tree, Support Vector 
Machines and Maximum Entropy on the training 
data while Decision Tree perform the best. Thus, 
DT classifier is selected. Since the annotations on 
the training data from different directory show 
some inconsistence, multiple classifiers 
corresponding to each directory are trained 
individually.  
108
3.1.1 Features - English 
51 features are selected for English coreference 
determination. The features are camped to six 
categories. Some typical features are listed below: 
1. Basic features: 
(1) Syntactic type of the two mentions, 
includes NP, NE, PRP, PRP$. Here, only 
the NPs which do not contain any named 
entities or its head word isn?t a named 
entity are considered as an NP while the 
others are discarded. 
(2) If one mention is a PRP or PRP$, use an 
ID to specify which one it is.  
(3) The sentence distance between two 
mentions. 
(4) Whether one mention is contained by 
another one. 
2. Parsing features: 
(1) Whether two mentions belong to one NP. 
(2) The phrase distance between the two 
mentions.  
(3) The predicted arguments which the two 
mentions belong to. 
3. Named entity related features: 
(1) If both of the two mentions may be 
considered as named entities, whether 
they have the same type. 
(2) If one mention is a common NP or PRP 
and another one can be considered as 
named entity, whether the words of the 
common NP or PRP can be used to refer 
this type of named entity. This knowledge 
is extracted from the training dataset. 
(3) Whether the core words of the two named 
entity type NP match each other.    
4. Features for PRP: 
(1) If both mentions are PRP or PRP$, use an 
ID to show what they are. The PRP$ with 
the same type will be assigned the same 
ID, for example, he, him and his. 
(2) Whether the two mentions has the same 
PRP ID. 
5. Semantic Features: 
(1) Whether the two mentions have the same 
headword. 
(2) Whether the two mentions belong to the 
same type. Here, we use WordNet to get 
three most common sense of each NP and 
compare the type they belong to.  
(3) The semantic distance between two 
mentions. WordNet is used here.  
(4) The natures of the two mentions, including 
number, gender, is human or not, and 
match each other or not. We use WordNet 
and a lexicon extracted from the gender 
and number file here. 
6. Document features: 
(1) How many speakers in this document. 
(2) Whether the mention is the first or the last 
sentence of the document. 
(3) Whether the two mentions are from the 
same speaker. 
3.1.2 Features - Chinese 
There are 61 features adopted in Chinese side. 
Because of the restriction of closed crack, most of 
features use the position and POS information. It is 
mentionable that the ways for calculating the 
features values. For instance, the sentence distance 
is not the real sentence distance in the document. 
For instead, the value is the number of sentences in 
which there are at least one mention between the 
mention pair. This ignores the sentences of only 
modal particles.  
The 61 features are camped into five groups. 
Some example features are listed below. 
1. Basic information: 
(1) The matching degree of two mentions 
(2) The word distance of two mentions 
(3) The sentence distance of two mentions 
2. Parsing information: 
(1) Predicted arguments which the two 
mentions belong to and corresponding 
layers. 
3. POS features 
(1) Whether the mention is NR 
(2) Whether the two mentions are both NR 
and are matched 
4. Semantic features: 
(1) Whether the two mention is related 
(2) Whether the two mentions corefer in the 
history. Since the restriction of closed 
track, we did not use any additional 
semantic resources. Here, we extract the 
co-reference history from the training set 
to obtain some semantic information, such 
as ?NN ??? and ?NN ??? corefered in 
the training data, and they are regarded as 
coreference in the testing data.  
5. Document Features: 
109
(1) Whether the two mentions have the same 
speaker. 
(2) Whether the mention is a human. 
(3) Whether the mention is the first mention in 
the sentence. 
(4) Whether the sentence to which the mention 
belongs to is the first sentence. 
(5) Whether the sentence to which the mention 
belongs to is the second sentence 
(6) Whether the sentence to which the mention 
belongs to is the last sentence 
(7) The number of the speakers in the 
document. 
3.2 The Rule-based Coreference 
Determination 
The rule-based classifier is developed to recognize some 
specific types of coreference and especially, the long 
distance ones.  
3.2.1 Rule-based Classifier - English 
To achieve a high precision, only the mention pairs 
of NE-NE (include NPs those can be considered as 
NE) or NP-NP types with the same string are 
classified here.  
For the NE-NE pair, the classifier identifies their 
NE part from the whole NP, if their strings are the 
same, they are considered as coreference. 
For the NP-NP pair, the pairs satisfy the 
following rules are regarded as coreference. 
(1) The POS of the first word isn?t ?JJR? or ?JJ?. 
(2) If NP has only one word, its POS isn?t ?NNS? 
or ?NNPS?. 
(3) The NP have no word like ?every?, ?every-?, 
?none?, ?no?,  ?any?,  ?some?,  ?each?. 
(4) If the two NP has article, they can?t be both 
?a? or ?an?. 
Additionally, for the PRP mention pairs, only 
?I?, ?me?, ?my? with the same speaker can be 
regarded as coreference. 
3.2.2 Rule-based Classifier - Chinese 
A rule-based classifier is developed to determine 
whether the mention pairs between PNs and 
mentions not PN corefer or not. For instance, the 
mention pairs between the PN ??? which is after a 
comma and the mention which is marked as ARG0 
in the same sentence. In the sentence ?????? 
??  ?  ?  ??  ??  ??  ??  ?  ???, 
because the mention pair between ??????? 
and the first ??? match the mentioned above rule, 
it  is classified as a positive one. The result on the 
development set shows that the rule-based 
classifier brings good improvement. 
4 Coreference Chain Construction  
4.1 Coreference Chain Construction-English 
The evaluation on development data shows that the 
achieved precision of our system is better than 
recall.  Thus, in this stage, we simply link every 
pair of mentions together if there is any links can 
link them together to generate the initial 
coreference chain. After that, the mentions have 
the distance longer than 5 sentences are observed. 
The NE-NE or NP-NP mention pairs between one 
known coreference and an observing mention with 
long distance are classified to determine they are 
corefered or not by using a set of rules. The new 
detected conference will be linked to the initial 
coreference chain.  
4.2 Coreference Chain Construction-Chinese 
The coreference chain construction for Chinese is 
similar to English. Furthermore, as mentioned 
above, in MZ and NW folders, there are many 
mentions nested marked as the nested co-
referenced mentions. In this stage, HLT_HITSZ 
system generates the nested co-reference mentions 
for improving the analysis for these two folders. 
Additionally, the system uses some rules to 
improve the coreference chain construction. We 
find that the trained classifier performs poor in co-
reference resolution related to Pronoun. So, most  
rules adopted here are related to these Pronouns: 
????, ???, ???, ???, ???, ????, ????, 
???. We use these rules to bridge the chain of 
pronouns and the chain of other type. 
Although high precision for NT co-reference 
cases are achieved through string matching, the 
recall is not satisfactory. It partially attributes to 
the fact that the flexible use of Chinese. For 
example, to express the year of 1980, we found 
???????, ??????, ? ?????, ???
? ?, ?1980 ? ?. Similar situation happens for 
month (?, ??) and day (?,?), we conclude 
most situations to several templates to improve the 
rule-based conference resolution. 
110
5 Evaluation Results 
5.1 Dataset 
The status of training dataset, development dataset 
and testing dataset in CoNLL 2012 for English and 
Chinese are given in Table 1 and Table 2, 
respectively. 
 Files Sentence Cluster Coreference
Train 1,940 74,852 35,101 155,292 
Development 222 9,603 4,546 19,156 
Test 222 9,479 n/a n/a 
Table 1. Status of CoNLL 2012 dataset - English 
 
 Files Sentence Cluster Coreference
Train 1,391 36,487 28,257 102,854 
Develop 172 6,083 3,875 14,383 
Test 166 4,472 n/a n/a 
Table 2. Status of CoNLL 2012 dataset - Chinese 
5.2 Evaluation on Mention Detection 
Firstly, the mention detection performance is evaluated. 
The performance achieved on the development dataset 
(Gold/Auto) and test data on English and Chinese are 
given in Table 3 and Table 4, respectively. In which, 
Gold means the development dataset with gold 
manually annotation and Auto means the automatically 
generated annotations.  
 Precision Recall F1 
Develop-Gold 0.8499 0.6716 0.7503 
Develop-Auto 0.8456 0.6256 0.7192 
Test 0.8455 0.6264 0.7196 
Table 3. Performance on Mention Detection - English 
 
 Precision Recall F1 
Develop-Gold 0.7402 0.7360 0.7381 
Develop-Auto 0.6987 0.6429 0.6697 
Test 0.7307 0.7502 0.7403 
Table 4. Performance on Mention Detection - Chinese 
 
Generally speaking, our system achieves acceptable 
mention detection performance, but further 
improvements are desired.  
5.3 Evaluation on Coreference Resolution 
The performance on coreference resolution is next 
evaluated. The achieved performances on the 
development data (Gold/Auto) and test dataset on 
English and Chinese are given in Table 5 and Table 6, 
respectively. It is shown that the OF performance drops 
0.0309(Gold) and 0.0112(Auto) from development 
dataset to test dataset on English, respectively. On the 
contrary, the OF performance increases 0.0096(Gold) 
and 0.0505(Auto) from development dataset to test 
dataset on Chinese, respectively. Compared with the 
performance reported in CoNLL2012 shared task, our 
system achieves a good result, ranked 3rd, on Chinese. 
The results show the effectiveness of our proposed 
system. 
 Precision Recall F1 
MUC 0.7632 0.6455 0.6994 
BCUB 0.7272 0.6797 0.7027 
CEAFE 0.3637 0.4840 0.4154 
OF-Develop-Gold   0.6058 
MUC 0.7571 0.5993 0.6691 
BCUB 0.7483 0.6441 0.6923 
CEAFE 0.3350 0.4865 0.3968 
OF-Develop-Auto   0.5861 
MUC 0.7518 0.5911 0.6618 
BCUB 0.7329 0.6228 0.6734 
CEAFE 0.3264 0.4829 0.3895 
OF-Test   0.5749 
Table 5. Performance on Coreference Resolution ? 
English  
 Precision Recall F1 
MUC 0.6892 0.6655 0.6771
BCUB 0.7547 0.7410 0.7478
CEAFE 0.4876 0.5105 0.4988
OF-Develop-Gold   0.6412
MUC 0.6535 0.5643 0.6056
BCUB 0.7812 0.6809 0.7276
CEAFE 0.4322 0.5101 0.4679
OF-Develop-Auto   0.6003
MUC 0.6928 0.6595 0.6758
BCUB 0.7765 0.7328 0.7540
CEAFE 0.5072 0.5390 0.6253
OF-Test(Gold parses)   0.6508
MUC 
BCUB 
CEAFE 
OF-Test-Predicted-mentions 
(Auto parses) 
0.5502 
0.6839 
0.5040 
0.6147
0.7638
0.4481
0.5807
0.7216
0.4744
0.5922
MUC 
BCUB 
CEAFE 
OF-Test-Gold-mention-
boundaries(Auto parses) 
0.6354 
0.7136 
0.5390 
0.6873
0.7870
0.4907
0.6603
0.7485
0.5137
0.6408
MUC 
BCUB 
CEAFE 
OF-Test-Gold-mentions 
(Auto parses) 
0.6563 
0.6505 
0.7813 
0.9407
0.9123
0.4377
0.7732
0.7595
0.5611
0.6979
Table 6. Performance on Coreference Resolution ? 
Chinese 
111
6 Conclusions 
This paper presents the HLT_HITSZ system for 
CoNLL2012 shared task. Generally speaking, this 
system uses a statistic-based classifier to handle 
short distance coreference resolution and uses a 
rule-based classifier to handle long distance cases. 
The incorporation of rule-based and statistic-based 
techniques is shown effective to improve the 
performance of coreference resolution. In our 
future work, more semantic and knowledge bases 
will be incorporated to improve coreference 
resolution in open track. 
 
Acknowledgement 
This research is supported by HIT.NSFIR.201012 
from Harbin Institute of Technology, China and 
China Postdoctoral Science Foundation No. 
2011M500670. 
References  
B. Baldwin. 1997. CogNIAC: High Precision 
Coreference with Limited Knowledge and Linguistic 
Resources. Proceedings of Workshop on Operational 
Factors in Practical, Robust Anaphora Resolution for 
Unrestricted Texts. 
E. Bengtson, D. Roth. 2008. Understanding the Value of 
Features for Coreference Resolution. Proceedings of 
EMNLP 2008, 294-303. 
M. S. Beth M. 1995. Overview of Results of the MUC-6 
Evaluation. Proceedings of the Sixth Message 
Understanding Conference (MUC-6) 
W. P. Chen, M. Y. Zhang, B. Qin,  2011. Coreference 
Resolution System using Maximum Entropy 
Classifier. Proceedings of CoNLL-2011. 
N. A. Chinchor. 1998. Overview of MUC-7/MET-2. 
Proceedings of the Seventh Message Understanding 
Conference (MUC-7). 
F. Kong, G. D. Zhou, L. H. Qian, Q. M. Zhu. 2010. 
Dependency-driven Anaphoricity Determination for 
Coreference Resolution. Proceedings of COLING 
2010, 599-607  
J. Lang, B. Qin, T. Liu. 2007. Intra-document 
Coreference Resolution: The State of the Art. Journal 
of Chinese Language and Computing , 2007, 17( 4) : 
227-253. 
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. 
Surdeanu, D. Jurafsky. 2011. Stanford?s Multi-Pass 
Sieve Coreference Resolution System at the CoNLL-
2011 Shared Task. Proceedings of CoNLL-2011. 
V. Ng and C. Cardie. 2002. Improving Machine 
Learning Approaches to Coreference Resolution. 
Proceedings of ACL 2002. 
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A 
Machine Learning Approach to Coreference 
Resolution of Noun Phrases. Computational 
Linguistics, 27(4):521-544 
S. Pradhan and A. Moschitti et al 2012. CoNLL-2012 
Shared Task: Modeling Multilingual Unrestricted 
Coreference in OntoNotes. Proceedings of CoNLL 
2012 
V. Stoyanov, N. Gilbert, C. Cardie, E. Riloff. 2009. 
Conundrums in Noun Phrase Coreference Resolution: 
Making Sense of the State-of-the-Art. Proceeding 
ACL 2009  
Y. Versley. 2007. Antecedent Selection Techniques for 
High-recall Coreference Resolution. Proceedings of 
EMNLP/CoNLL 2007. 
Y. Yang, N. W. Xue, P. Anick. 2011. A Machine 
Learning-Based Coreference Detection System For 
OntoNotes.  Proceedings of CoNLL-2011. 
 
 
112
