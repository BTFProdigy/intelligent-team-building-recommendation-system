Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Scalable Global Model for Summarization
Dan Gillick1,2, Benoit Favre2
1 Computer Science Division, University of California Berkeley, USA
2 International Computer Science Institute, Berkeley, USA
{dgillick,favre}@icsi.berkeley.edu
Abstract
We present an Integer Linear Program for
exact inference under a maximum coverage
model for automatic summarization. We com-
pare our model, which operates at the sub-
sentence or ?concept?-level, to a sentence-
level model, previously solved with an ILP.
Our model scales more efficiently to larger
problems because it does not require a
quadratic number of variables to address re-
dundancy in pairs of selected sentences. We
also show how to include sentence compres-
sion in the ILP formulation, which has the
desirable property of performing compression
and sentence selection simultaneously. The
resulting system performs at least as well as
the best systems participating in the recent
Text Analysis Conference, as judged by a va-
riety of automatic and manual content-based
metrics.
1 Introduction
Automatic summarization systems are typically ex-
tractive or abstractive. Since abstraction is quite
hard, the most successful systems tested at the Text
Analysis Conference (TAC) and Document Under-
standing Conference (DUC)1, for example, are ex-
tractive. In particular, sentence selection represents
a reasonable trade-off between linguistic quality,
guaranteed by longer textual units, and summary
content, often improved with shorter units.
Whereas the majority of approaches employ a
greedy search to find a set of sentences that is
1TAC is a continuation of DUC, which ran from 2001-2007.
both relevant and non-redundant (Goldstein et al,
2000; Nenkova and Vanderwende, 2005), some re-
cent work focuses on improved search (McDonald,
2007; Yih et al, 2007). Among them, McDonald is
the first to consider a non-approximated maximiza-
tion of an objective function through Integer Linear
Programming (ILP), which improves on a greedy
search by 4-12%. His formulation assumes that the
quality of a summary is proportional to the sum of
the relevance scores of the selected sentences, penal-
ized by the sum of the redundancy scores of all pairs
of selected sentences. Under a maximum summary
length constraint, this problem can be expressed as
a quadratic knapsack (Gallo et al, 1980) and many
methods are available to solve it (Pisinger et al,
2005). However, McDonald reports that the method
is not scalable above 100 input sentences and dis-
cusses more practical approximations. Still, an ILP
formulation is appealing because it gives exact so-
lutions and lends itself well to extensions through
additional constraints.
Methods like McDonald?s, including the well-
known Maximal Marginal Relevance (MMR) algo-
rithm (Goldstein et al, 2000), are subject to an-
other problem: Summary-level redundancy is not
always well modeled by pairwise sentence-level re-
dundancy. Figure 1 shows an example where the
combination of sentences (1) and (2) overlaps com-
pletely with sentence (3), a fact not captured by pair-
wise redundancy measures. Redundancy, like con-
tent selection, is a global problem.
Here, we discuss a model for sentence selection
with a globally optimal solution that also addresses
redundancy globally. We choose to represent infor-
10
(1) The cat is in the kitchen.
(2) The cat drinks the milk.
(3) The cat drinks the milk in the kitchen.
Figure 1: Example of sentences redundant as a group.
Their redundancy is only partially captured by sentence-
level pairwise measurement.
mation at a finer granularity than sentences, with
concepts, and assume that the value of a summary is
the sum of the values of the unique concepts it con-
tains. While the concepts we use in experiments are
word n-grams, we use the generic term to emphasize
that this is just one possible definition. Only credit-
ing each concept once serves as an implicit global
constraint on redundancy. We show how the result-
ing optimization problem can be mapped to an ILP
that can be solved efficiently with standard software.
We begin by comparing our model to McDonald?s
(section 2) and detail the differences between the re-
sulting ILP formulations (section 3), showing that
ours can give competitive results (section 4) and of-
fer better scalability2 (section 5). Next we demon-
strate how our ILP formulation can be extended to
include efficient parse-tree-based sentence compres-
sion (section 6). We review related work (section 7)
and conclude with a discussion of potential improve-
ments to the model (section 8).
2 Models
The model proposed by McDonald (2007) considers
information and redundancy at the sentence level.
The score of a summary is defined as the sum of
the relevance scores of the sentences it contains mi-
nus the sum of the redundancy scores of each pair of
these sentences. If si is an indicator for the presence
of sentence i in the summary, Reli is its relevance,
and Redij is its redundancy with sentence j, then a
summary is scored according to:
?
i
Relisi ?
?
ij
Redijsisj
Generating a summary under this model involves
maximizing this objective function, subject to a
2Strictly speaking, exact inference for the models discussed
in this paper is NP-hard. Thus we use the term ?scalable? in a
purely practical sense.
length constraint. A variety of choices for Reli and
Redij are possible, from simple word overlap met-
rics to the output of feature-based classifiers trained
to perform information retrieval and textual entail-
ment.
As an alternative, we consider information and re-
dundancy at a sub-sentence, ?concept? level, model-
ing the value of a summary as a function of the con-
cepts it covers. While McDonald uses an explicit
redundancy term, we model redundancy implicitly:
a summary only benefits from including each con-
cept once. With ci an indicator for the presence of
concept i in the summary, and its weight wi, the ob-
jective function is:
?
i
wici
We generate a summary by choosing a set of sen-
tences that maximizes this objective function, sub-
ject to the usual length constraint.
In summing over concept weights, we assume that
the value of including a concept is not effected by the
presence of any other concept in the summary. That
is, concepts are assumed to be independent. Choos-
ing a suitable definition for concepts, and a map-
ping from the input documents to concept weights,
is both important and difficult. Concepts could be
words, named entities, syntactic subtrees or seman-
tic relations, for example. While deeper semantics
make more appealing concepts, their extraction and
weighting are much more error-prone. Any error in
concept extraction can result in a biased objective
function, leading to poor sentence selection.
3 Inference by ILP
Each model presented above can be formalized as an
Integer Linear Program, with a solution represent-
ing an optimal selection of sentences under the ob-
jective function, subject to a length constraint. Mc-
Donald observes that the redundancy term makes for
a quadratic objective function, which he coerces to
a linear function by introducing additional variables
sij that represent the presence of both sentence i and
sentence j in the summary. Additional constraints
ensure the consistency between the sentence vari-
ables (si, sj) and the quadratic term (sij). With li
the length of sentence i and L the length limit for
11
the whole summary, the resulting ILP is:
Maximize:
?
i
Relisi ?
?
ij
Redijsij
Subject to:
?
j
ljsj ? L
sij ? si sij ? sj ?i, j
si + sj ? sij ? 1 ?i, j
si ? {0, 1} ?i
sij ? {0, 1} ?i, j
To express our concept-based model as an ILP, we
maintain our notation from section 2, with ci an in-
dicator for the presence of concept i in the summary
and sj an indicator for the presence of sentence j in
the summary. We add Occij to indicate the occur-
rence of concept i in sentence j, resulting in a new
ILP:
Maximize:
?
i
wici
Subject to:
?
j
ljsj ? L
sjOccij ? ci, ?i, j (1)?
j
sjOccij ? ci ?i (2)
ci ? {0, 1} ?i
sj ? {0, 1} ?j
Note that Occ, like Rel and Red, is a constant pa-
rameter. The constraints formalized in equations (1)
and (2) ensure the logical consistency of the solu-
tion: selecting a sentence necessitates selecting all
the concepts it contains and selecting a concept is
only possible if it is present in at least one selected
sentence. Constraint (1) also prevents the inclusion
of concept-less sentences.
4 Performance
Here we compare both models on a common sum-
marization task. The data is part of the Text Analy-
sis Conference (TAC) multi-document summariza-
tion evaluation and involves generating 100-word
summaries from 10 newswire documents, each on
a given topic. While the 2008 edition of TAC also
includes an update task?additional summaries as-
suming some prior knowledge?we focus only on
the standard task. This includes 48 topics, averag-
ing 235 input sentences (ranging from 47 to 652).
Since the mean sentence length is around 25 words,
a typical summary consists of 4 sentences.
In order to facilitate comparison, we generate
summaries from both models using a common
pipeline:
1. Clean input documents. A simple set of rules
removes headers and formatting markup.
2. Split text into sentences. We use the unsuper-
vised Punkt system (Kiss and Strunk, 2006).
3. Prune sentences shorter than 5 words.
4. Compute parameters needed by the models.
5. Map to ILP format and solve. We use an open
source solver3.
6. Order sentences picked by the ILP for inclusion
in the summary.
The specifics of step 4 are described in detail in
(McDonald, 2007) and (Gillick et al, 2008). Mc-
Donald?s sentence relevance combines word-level
cosine similarity with the source document and the
inverse of its position (early sentences tend to be
more important). Redundancy between a pair of sen-
tences is their cosine similarity. For sentence i in
document D,
Reli = cosine(i,D) + 1/pos(i,D)
Redij = cosine(i, j)
In our concept-based model, we use word bi-
grams, weighted by the number of input documents
in which they appear. While word bigrams stretch
the notion of a concept a bit thin, they are eas-
ily extracted and matched (we use stemming to al-
low slightly more robust matching). Table 1 pro-
vides some justification for document frequency as a
weighting function. Note that bigrams gave consis-
tently better performance than unigrams or trigrams
for a variety of ROUGE measures. Normalizing
by document frequency measured over a generic set
(TFIDF weighting) degraded ROUGE performance.
3gnu.org/software/glpk
12
Bigrams consisting of two stopwords are pruned, as
are those appearing in fewer than three documents.
We largely ignore the sentence ordering problem,
sorting the resulting sentences first by source docu-
ment date, and then by position, so that the order of
two originally adjacent sentences is preserved, for
example.
Doc. Freq. (D) 1 2 3 4 5 6
In Gold Set 156 48 25 15 10 7
Not in Gold Set 5270 448 114 42 21 11
Relevant (P ) 0.03 0.10 0.18 0.26 0.33 0.39
Table 1: There is a strong relationship between the docu-
ment frequency of input bigrams and the fraction of those
bigrams that appear in the human generated ?gold? set:
Let di be document frequency i and pi be the percent of
input bigrams with di that are actually in the gold set.
Then the correlation ?(D,P ) = 0.95 for DUC 2007 and
0.97 for DUC 2006. Data here averaged over all prob-
lems in DUC 2007.
The summaries produced by the two systems
have been evaluated automatically with ROUGE and
manually with the Pyramid metric. In particular,
ROUGE-2 is the recall in bigrams with a set of
human-written abstractive summaries (Lin, 2004).
The Pyramid score arises from a manual alignment
of basic facts from the reference summaries, called
Summary Content Units (SCUs), in a hypothesis
summary (Nenkova and Passonneau, 2004). We
used the SCUs provided by the TAC evaluation.
Table 2 compares these results, alongside a base-
line that uses the first 100 words of the most re-
cent document. All the scores are significantly
different, showing that according to both human
and automatic content evaluation, the concept-
based model outperforms McDonald?s sentence-
based model, which in turn outperforms the base-
line. Of course, the relevance and redundancy func-
tions used for McDonald?s formulation in this exper-
iment are rather primitive, and results would likely
improve with better relevance features as used in
many TAC systems. Nonetheless, our system based
on word bigram concepts, similarly primitive, per-
formed at least as well as any in the TAC evaluation,
according to two-tailed t-tests comparing ROUGE,
Pyramid, and manually evaluated ?content respon-
siveness? (Dang and Owczarzak, 2008) of our sys-
tem and the highest scoring system in each category.
System ROUGE-2 Pyramid
Baseline 0.058 0.186
McDonald 0.072 0.295
Concepts 0.110 0.345
Table 2: Scores for both systems and a baseline on TAC
2008 data (Set A) for ROUGE-2 and Pyramid evalua-
tions.
5 Scalability
McDonald?s sentence-level formulation corresponds
to a quadratic knapsack, and he shows his particu-
lar variant is NP-hard by reduction to 3-D matching.
The concept-level formulation is similar in spirit to
the classical maximum coverage problem: Given a
set of items X , a set of subsets S of X , and an in-
teger k, the goal is to pick at most k subsets from
S that maximizes the size of their union. Maximum
coverage is known to be NP-hard by reduction to the
set cover problem (Hochbaum, 1996).
Perhaps the simplest way to show that our formu-
lation is NP-hard is by reduction to the knapsack
problem (Karp, 1972). Consider the special case
where sentences do not share any overlapping con-
cepts. Then, the value of each sentence to the sum-
mary is independent of every other sentence. This is
a knapsack problem: trying to maximize the value
in a container of limited size. Given a solver for our
problem, we could solve all knapsack problem in-
stances, so our problem must also be NP-hard.
With n input sentences and m concepts, both
formulations generate a quadratic number of con-
straints. However, McDonald?s has O(n2) variables
while ours has O(n + m). In practice, scalability
is largely determined by the sparsity of the redun-
dancy matrix Red and the sentence-concept matrix
Occ. Efficient solutions thus depend heavily on the
choice of redundancy measure in McDonald?s for-
mulation and the choice of concepts in ours. Prun-
ing to reduce complexity involves removing low-
relevance sentences or ignoring low redundancy val-
ues in the former, and corresponds to removing low-
weight concepts in the latter. Note that pruning con-
cepts may be more desirable: Pruned sentences are
irretrievable, but pruned concepts may well appear
in the selected sentences through co-occurrence.
Figure 2 compares ILP run-times for the two
13
formulations, using a set of 25 topics from DUC
2007, each of which have at least 500 input sen-
tences. These are very similar to the TAC 2008
topics, but more input documents are provided for
each topic, which allowed us to extend the analysis
to larger problems. While the ILP solver finds opti-
mal solutions efficiently for our concept-based for-
mulation, run-time for McDonald?s approach grows
very rapidly. The plot includes timing results for
250-word summaries as well, showing that our ap-
proach is fast even for much more complex prob-
lems: A rough estimate for the number of possible
summaries has
(500
4
) = 2.6?109 for 100-word sum-
maries and
(500
10
) = 2.5 ? 1020 for 250 words sum-
maries.
While exact solutions are theoretically appealing,
they are only useful in practice if fast approxima-
tions are inferior. A greedy approximation of our
objective function gives 10% lower ROUGE scores
than the exact solution, a gap that separates the high-
est scoring systems from the middle of the pack in
the TAC evaluation. The greedy solution (linear in
the number of sentences, assuming a constant sum-
mary length) marks an upper bound on speed and
a lower bound on performance; The ILP solution
marks an upper bound on performance but is subject
to the perils of exponential scaling. While we have
not experimented with much larger documents, ap-
proximate methods will likely be valuable in bridg-
ing the performance gap for complex problems. Pre-
liminary experiments with local search methods are
promising in this regard.
6 Extensions
Here we describe how our ILP formulation can
be extended with additional constraints to incor-
porate sentence compression. In particular, we
are interested in creating compressed alternatives
for the original sentence by manipulating its parse
tree (Knight and Marcu, 2000). This idea has been
applied with some success to summarization (Turner
and Charniak, 2005; Hovy et al, 2005; Nenkova,
2008) with the goal of removing irrelevant or redun-
dant details, thus freeing space for more relevant in-
formation. One way to achieve this end is to gen-
erate compressed candidates for each sentence, cre-
ating an expanded pool of input sentences, and em-
50 100 150 200 250 300 350 400 450 5000
1
2
3
4
5
6
7
8
Number of Sentences
Av
era
ge 
Tim
e p
er P
rob
lem
 (se
con
ds)
 
 
100 word summaries
250 word summaries
100 word summaries (McDonald)
Figure 2: A comparison of ILP run-times (on an AMD
1.8Ghz desktop machine) of McDonald?s sentence-based
formulation and our concept-based formulation with an
increasing number of input sentences.
ploy some redundancy removal on the final selec-
tion (Madnani et al, 2007).
We adapt this approach to fit the ILP formulations
so that the optimization procedure decides which
compressed alternatives to pick. Formally, each
compression candidate belongs to a group gk corre-
sponding to its original sentence. We can then craft
a constraint to ensure that at most one sentence can
be selected from group gk, which also includes the
original:
?
i?gk
si ? 1,?gk
Assuming that all the compressed candidates are
themselves well-formed, meaningful sentences, we
would expect this approach to generate higher qual-
ity summaries. In general, however, compression
algorithms can generate an exponential number of
candidates. Within McDonald?s framework, this
can increase the number of variables and constraints
tremendously. Thus, we seek a compact representa-
tion for compression in our concept framework.
Specifically, we assume that compression in-
volves some combination of three basic operations
on sentences: extraction, removal, and substitution.
In extraction, a sub-sentence (perhaps the content of
a quotation) may be used independently, and the rest
of the sentence is dropped. In removal, a substring
14
is dropped (a temporal clause, for example) that pre-
serves the grammaticality of the sentence. In sub-
stitution, one substring is replaced by another (US
replaces United States, for example).
Arbitrary combinations of these operations are
too general to be represented efficiently in an ILP.
In particular, we need to compute the length of a
sentence and the concepts it covers for all compres-
sion candidates. Thus, we insist that the operations
can only affect non-overlapping spans of text, and
end up with a tree representation of each sentence:
Nodes correspond to compression operations and
leaves map to the words. Each node holds the length
it contributes to the sentence recursively, as the sum
of the lengths of its children. Similarly, the concepts
covered by a node are the union of the concepts cov-
ered by its children. When a node is activated in the
ILP, we consider that the text attached to it is present
in the summary and update the length constraint and
concept selection accordingly. Figure 3 gives an ex-
ample of this tree representation for a sentence from
the TAC data, showing the derivations of some com-
pressed candidates.
For a given sentence j, let Nj be the set of nodes
in its compression tree, Ej ? Nj be the set of
nodes that can be extracted (used as independent
sentences), Rj ? Nj be the set of nodes that can
be removed, and Sj ? Nj be the set of substitu-
tion group nodes. Let x and y be nodes from Nj ; we
create binary variables nx and ny to represent the in-
clusion of x or y in the summary. Let x ? y denote
the fact that x ? Nj is a direct parent of y ? Nj .
The constraints corresponding to the compression
tree are:
?
x?Ej
nx ? 1 ?j (3)
?
x?y
ny = nx ?x ? Sj ?j (4)
nx ? ny ?(y ? x ? x /? {Rj ? Sj}) ?j (5)
nx ? ny ?(y ? x ? x /? {Ej ? Sj}) ?j (6)
Eq. (3) enforces that only one sub-sentence is ex-
tracted from the original sentence; eq. (4) enforces
that one child of a substitution group is selected if
and only if the substitution node is selected; eq. (5)
ensures that a child node is selected when its parent
is selected unless the child is removable (or a substi-
tution group); eq. (6) ensures that if a child node is
selected, its parent is also selected unless the child is
an extraction node (that can be used as a root).
Each node is associated with the words and the
concepts it contains directly (which are not con-
tained by a child node) in order to compute the new
length constraints and activate concepts in the ob-
jective function. We set Occix to represent the oc-
currence of concept i in node x as a direct child.
Let lx be the length contributed to node x as direct
children. The resulting ILP for performing sentence
compression jointly with sentence selection is:
Maximize:
?
i
wici
Subject to:
?
j
lxnx ? L
nxOccix ? ci, ?i, x?
x
nxOccix ? ci ?i
idem constraints (3) to (6)
ci ? {0, 1} ?i
nx ? {0, 1} ?x
While this framework can be used to imple-
ment a wide range of compression techniques, we
choose to derive the compression tree from the
sentence?s parse tree, extracted with the Berkeley
parser (Petrov and Klein, 2007), and use a set of
rules to label parse tree nodes with compression op-
erations. For example, declarative clauses contain-
ing a subject and a verb are labeled with the extract
(E) operation; adverbial clauses and non-mandatory
prepositional clauses are labeled with the remove
(R) operation; Acronyms can be replaced by their
full form by using substitution (S) operations and a
primitive form of co-reference resolution is used to
allow the substitution of noun phrases by their refer-
ent.
System R-2 Pyr. LQ
No comp. 0.110 0.345 2.479
Comp. 0.111 0.323 2.021
Table 3: Scores of the system with and without sentence
compression included in the ILP (TAC?08 Set A data).
When implemented in the system presented in
section 4, this approach gives a slight improvement
15
countries are    planning to hold the euroas part of their    reserves
the magazine quoted    chief Wilm Disenbergas sayingalready (ECB | European Central Bank)foreign currency
(1):E(2):E
(6):R (7):R(8):R
(4):R(3):S
A number of(5):R
Node Len. Concepts
(1):E 6 {the magazine, magazine quoted, chief Wilm, Wilm Disenberg}
(2):E 7 {countries are, planning to, to hold, hold the, the euro}
(3):S 0 {}
(3a) 1 {ECB}
(3b) 3 {European Central, Central Bank}
(4):R 2 {as saying}
(5):R 3 {a number, number of}
(6):R 1 {}
(7):R 5 {as part, part of, reserves}
(8):R 2 {foreign currency}
? Original: A number of Countries
are already planning to hold the
euro as part of their foreign cur-
rency reserves, the magazine quoted
European Central Bank chief Wim
Duisenberg as saying.
? [1,2,5,3a]: A number of countries are
planning to hold the euro, the maga-
zine quoted ECB chief Wim Duisen-
berg.
? [2,5,6,7,8]: A number of countries
are already planning to hold the euro
as part of their foreign currency re-
serves.
? [2,7,8]: Countries are planning to
hold the euro as part of their foreign
currency reserves.
? [2]: Countries are planning to hold
the euro.
Figure 3: A compression tree for an example sentence. E-nodes (diamonds) can be extracted and used as an indepen-
dent sentences, R-nodes (circles) can be removed, and S-nodes (squares) contain substitution alternatives. The table
shows the word bigram concepts covered by each node and the length it contributes to the summary. Examples of
resulting compression candidates are given on the right side, with the list of nodes activated in their derivations.
in ROUGE-2 score (see Table 3), but a reduction in
Pyramid score. An analysis of the resulting sum-
maries showed that the rules used for implementing
sentence compression fail to ensure that all com-
pression candidates are valid sentences, and about
60% of the summaries contain ungrammatical sen-
tences. This is confirmed by the linguistic qual-
ity4 score drop for this system. The poor quality
of the compressed sentences explains the reduction
in Pyramid scores: Human judges tend to not give
credit to ungrammatical sentences because they ob-
scure the SCUs.
We have shown in this section how sentence com-
pression can be implemented in a more scalable way
under the concept-based model, but it remains to be
shown that such a technique can improve summary
quality.
7 Related work
In addition to proposing an ILP for the sentence-
level model, McDonald (2007) discusses a kind of
summary-level model: The score of a summary is
4As measured according to the TAC?08 guidelines.
determined by its cosine similarity to the collection
of input documents. Though this idea is only imple-
mented with approximate methods, it is similar in
spirit to our concept-based model since it relies on
weights for individual summary words rather than
sentences.
Using a maximum coverage model for summa-
rization is not new. Filatova (2004) formalizes the
idea, discussing its similarity to the classical NP-
hard problem, but in the end uses a greedy approxi-
mation to generate summaries. More recently, Yih et
al. (2007) employ a similar model and uses a stack
decoder to improve on a greedy search. Globally
optimal summaries are also discussed by Liu (2006)
and Jaoua Kallel (2004) who apply genetic algo-
rithms for finding selections of sentences that maxi-
mize summary-level metrics. Hassel (2006) uses hill
climbing to build summaries that maximize a global
information criterion based on random indexing.
The general idea of concept-level scoring for
summarization is employed in the SumBasic sys-
tem (Nenkova and Vanderwende, 2005), which
chooses sentences greedily according to the sum
of their word values (values are derived from fre-
16
quency). Conroy (2006) describes a bag-of-words
model, with the goal of approximating the distribu-
tion of words from the input documents in the sum-
mary. Others, like (Yih et al, 2007) train a model to
learn the value of each word from a set of features
including frequency and position. Filatova?s model
is most theoretically similar to ours, though the con-
cepts she chooses are ?events?.
8 Conclusion and Future Work
We have synthesized a number of ideas from
the field of automatic summarization, including
concept-level weighting, a maximum coverage
model to minimize redundancy globally, and sen-
tence compression derived from parse trees. While
an ILP formulation for summarization is not novel,
ours provides reasonably scalable, efficient solutions
for practical problems, including those in recent
TAC and DUC evaluations. We have also shown
how it can be extended to perform sentence com-
pression and sentence selection jointly.
In ROUGE and Pyramid evaluation, our system
significantly outperformed McDonald?s ILP sys-
tem. However, we would note that better design of
sentence-level scoring would likely yield better re-
sults as suggested by the success of greedy sentence-
based methods at the DUC and TAC conferences
(see for instance (Toutanova et al, 2007)). Still, the
performance of our system, on par with the current
state-of-the-art, is encouraging.
There are three principal directions for future
work. First, word bigram concepts are convenient,
but semantically unappealing. We plan to explore
concepts derived from parse trees, where weights
may be a function of frequency as well as hierar-
chical relationships.
Second, our current approach relies entirely on
word frequency, a reasonable proxy for relevance,
but likely inferior to learning weights from train-
ing data. A number of systems have shown im-
provements by learning word values, though prelim-
inary attempts to improve on our frequency heuristic
by learning bigram values have not produced sig-
nificant gains. Better features may be necessary.
However, since the ILP gives optimal solutions so
quickly, we are more interested in discriminative
training where we learn weights for features that
push the resulting summaries in the right direction,
as opposed to the individual concept values.
Third, our rule-based sentence compression is
more of a proof of concept, showing that joint com-
pression and optimal selection is feasible. Better
statistical methods have been developed for produc-
ing high quality compression candidates (McDon-
ald, 2006), that maintain linguistic quality, some re-
cent work even uses ILPs for exact inference (Clarke
and Lapata, 2008). The addition of compressed sen-
tences tends to yield less coherent summaries, mak-
ing sentence ordering more important. We would
like to add constraints on sentence ordering to the
ILP formulation to address this issue.
Acknowledgments
This work is supported by the Defense Advanced
Research Projects Agency (DARPA) GALE project,
under Contract No. HR0011-06-C-0023. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
References
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2006. Topic-focused multi-document sum-
marization using an approximate oracle score. In Pro-
ceedings of COLING/ACL.
Hoa Trang Dang and Karolina Owczarzak. 2008.
Overview of the TAC 2008 Update Summarization
Task. In Proceedings of Text Analysis Conference.
E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In Proceedings of ACL
Workshop on Summarization, volume 111.
G. Gallo, PL Hammer, and B. Simeone. 1980. Quadratic
knapsack problems. Mathematical Programming
Study, 12:132?149.
D. Gillick, B. Favre, and D. Hakkani-Tur. 2008. The
ICSI Summarization System at TAC 2008. In Pro-
ceedings of the Text Understanding Conference.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by
sentence extraction. Proceedings of the ANLP/NAACL
Workshop on Automatic Summarization, pages 40?48.
17
Martin Hassel and Jonas Sjo?bergh. 2006. Towards
holistic summarization: Selecting summaries, not sen-
tences. In Proceedings of Language Resources and
Evaluation.
D.S. Hochbaum. 1996. Approximating covering and
packing problems: set cover, vertex cover, indepen-
dent set, and related problems. PWS Publishing Co.
Boston, MA, USA, pages 94?143.
E. Hovy, C.Y. Lin, and L. Zhou. 2005. A BE-based
multi-document summarizer with sentence compres-
sion. In Proceedings of Multilingual Summarization
Evaluation.
Fatma Jaoua Kallel, Maher Jaoua, Lamia Bel-
guith Hadrich, and Abdelmajid Ben Hamadou. 2004.
Summarization at LARIS Laboratory. In Proceedings
of the Document Understanding Conference.
Richard Manning Karp. 1972. Reducibility among com-
binatorial problems. Complexity of Computer Compu-
tations, 43:85?103.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multi-
lingual sentence boundary detection. Computational
Linguistics, 32.
K. Knight and D. Marcu. 2000. Statistics-Based
Summarization-Step One: Sentence Compression. In
Proceedings of the National Conference on Artificial
Intelligence, pages 703?710. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of the
Workshop on Text Summarization Branches Out (WAS
2004), pages 25?26.
D. Liu, Y. Wang, C. Liu, and Z. Wang. 2006. Multiple
Documents Summarization Based on Genetic Algo-
rithm. Lecture Notes in Computer Science, 4223:355.
N. Madnani, D. Zajic, B. Dorr, N.F. Ayan, and J. Lin.
2007. Multiple Alternative Sentence Compressions
for Automatic Text Summarization. In Proceed-
ings of the Document Understanding Conference at
NLT/NAACL.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
the 11th EACL, pages 297?304.
R. McDonald. 2007. A Study of Global Inference Al-
gorithms in Multi-document Summarization. Lecture
Notes in Computer Science, 4425:557.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of HLT-NAACL.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical Report MSR-
TR-2005-101, Microsoft Research, Redmond, Wash-
ington.
A. Nenkova. 2008. Entity-driven rewrite for multidocu-
ment summarization. Proceedings of IJCNLP.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
D. Pisinger, A.B. Rasmussen, and R. Sandvik. 2005.
Solution of large-sized quadratic knapsack problems
through aggressive reduction. INFORMS Journal on
Computing.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The Pythy
Summarization System: Microsoft Research at DUC
2007. In Proceedings of the Document Understanding
Conference.
J. Turner and E. Charniak. 2005. Supervised and Unsu-
pervised Learning for Sentence Compression. In Pro-
ceedings of ACL.
W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximiz-
ing informative content-words. In International Joint
Conference on Artificial Intelligence.
18
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 99?100,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
ICSI-CRF: The Generation of References to the Main Subject and Named
Entities using Conditional Random Fields
Benoit Favre and Bernd Bohnet
International Computer Science Institute
1947 Center Street. Suite 600
Berkeley, CA 94704, USA
{favre|bohnet}@icsi.berkeley.edu
Abstract
In this paper, we describe our contribution to
the Generation Challenge 2009 for the tasks of
generating Referring Expressions to the Main
Subject References (MSR) and Named Enti-
ties Generation (NEG). To generate the refer-
ring expressions, we employ the Conditional
Random Fields (CRF) learning technique due
to the fact that the selection of an expres-
sion depends on the selection of the previ-
ous references. CRFs fit very well to this
task since they are designed for the labeling
of sequences. For the MSR task, our system
has a String Accuracy of 0.68 and a REG08-
Type Accuracy of 0.76 and for the NEG task a
String Accuracy of 0.79 and REG08-Type Ac-
curacy of 0.83.
1 Introduction
The GREC Generation Challenge 2009 consists of
two tasks. The first task is to generate appropriate
references to an entity due to a given context which
is longer than a sentence. In the GREC-MSR task,
data sets are provided of possible referring expres-
sions which have to be selected. In the first shared
task on same topic (Belz and Varges, 2007), the main
task was to select the referring expression type cor-
rectly. In the GREC-MSR 2009 task, the main task
is to select the actual word string correctly, and the
main evaluation criterion is String Accuracy.
The GREC-NEG task is about the generation of
references to all person entities in a context longer
than a sentence. The NEG data also provides sets of
possible referring expressions to each entity (?he?),
groups of multiple entities (?they?) and nested refer-
ences (?his father?).
2 System Description
Our approach relies in mapping each input expres-
sion for a given reference to a class label. We use
the attributes of the REFEX tags as basic labels so
that, for instance, a REFEX with attributes REG08-
TYPE=?pronoun? CASE=?nominative? is mapped
to the label ?nominative pronoun?. In order to de-
crease the number of potential textual units for a pre-
dicted label, we derive extra label information from
the text itself. For instance a qualifier ?first name?
or ?family name? is added to the expressions rela-
tive to a person. Similarly, types of pronouns (he,
him, his, who, whose, whom, emphasis) are speci-
fied in the class label, which is very useful for the
NEG task. Only the person labels have been refined
this way. While we experimented with a few ap-
proaches to remove the remaining ambiguity (same
label for different text), they generally did not per-
form better than a random selection. We opted for a
deterministic generation with the last element in the
list of possibilities given a class label.
For prediction of attributes, our system uses Con-
ditional Random Fields, as proposed by (Lafferty et
al., 2001). We use chain CRFs to estimate the prob-
ability of a sequence of labels (Y = Y
1
. . . Y
n
) given
a sequence of observations (X = X
1
. . . X
m
).
P (Y |X) ? exp
?
?
n
?
j=1
m
?
i=1
?
i
f
i
(Y
j?1
, Y
j
, X)
?
?
(1)
Here, f
i
(?) are decision functions that depend on
99
MSR NEG
Evaluation Metric R1 R2 S1 S2 S2R S2O R1 R2 S1 S2 S2R S2O
REG08 Type Accuracy 0.36 1.00 0.74 0.75 0.75 0.75 0.40 1.00 0.83 0.83 0.83 0.83
String Accuracy 0.12 0.82 0.62 0.67 0.66 0.75 0.12 0.70 0.52 0.79 0.79 0.80
Mean Edit Distance 2.52 0.31 0.95 0.85 0.87 0.72 2.38 0.61 1.07 0.53 0.52 0.49
Mean Norm. Edit Dist. 0.79 0.09 0.31 0.28 0.28 0.24 0.84 0.22 0.43 0.19 0.20 0.19
BLEU 1 0.19 0.88 0.65 0.69 0.68 0.74 0.17 0.79 0.64 0.81 0.81 0.83
BLEU 2 0.14 0.76 0.55 0.60 0.59 0.71 0.18 0.75 0.69 0.83 0.83 0.85
BLEU 3 0.10 0.69 0.51 0.56 0.55 0.70 0.18 0.73 0.71 0.83 0.84 0.86
Table 1: Results for the GREC MSR and NEG tasks. Are displayed: a random
2
output (R1), a random output when
the attributes are guessed correctly (R2), the CRF system predicting basic attributes (S1), the CRF system predicting
refined attributes (S2), CRF-predicted attributes with random selection of text (S2R) and CRF-predicted attributes with
oracle selection of text (S2O).
the examples and a clique of boundaries close to Y
j
,
and ?
i
is the weight of f
i
estimated on training data.
For our experiments, we use the CRF++ toolkit,
1
which allows binary decision functions dependent
on the current label and the previous label.
All features are used for both MSR and NEG
tasks, where applicable:
? word unigram and bigram before and after the
reference
? morphology of the previous and next words (-
ed, -ing, -s)
? punctuation type, before and after (comma,
parenthesis, period, nothing)
? SYNFUNC, SYNCAT and SEMCAT
? whether or not the previous reference is about
the same entity as the current one
? number of occurrence of the entity since the be-
ginning of the text (quantized 1,2,3,4+)
? number of occurrence of the entity since the
last change of entity (quantized)
? beginning of paragraph indicator
In the MSR case, this list is augmented with the fea-
tures of the two previous references. In the NEG
case, we use the features of the previous reference
and those of the previous occurrence of the same en-
tity.
1
http://crfpp.sourceforge.net/
3 Results and Conclusion
Table 1 shows the results for the GREC MSR and
NEG tasks.
2
We observe that for both tasks, our sys-
tem exceeds the performance of a random
3
selection
(columns R1 vs. S2). In the MSR task, guessing
correctly the attributes seems more important than
in the NEG task, as suggested by the difference in
string accuracy when randomly selecting the refer-
ences with the right attributes (columns R2). Gener-
ating more specific attributes from the text is espe-
cially important for the NEG task (columns S1 vs.
S2). This was expected because we only refined
the attributes for person entities. We also observe
that a deterministic disambiguation of the references
with the same attributes is not distinguishable from
a random selection (columns S2 vs. S2R). However
it seems that selecting the right text, as in the ora-
cle experiment, would hardly help in the NEG task
while the gap is larger for the MSR task. This shows
that refined classes work well for person entities but
more refinements are needed for other types (city,
mountain, river...).
References
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML, pages 282-289
A. Belz and S. Varges. 2007 Generation of Repeated
References to Discourse Entities. In Proceedings of
the 11th European Workshop on Natural Language
Generation (ENLG07), pages 9-16.
2
Our system is available http://www.icsi.berkeley.edu/
?favre/grec/
3
All random experiments are averaged over 100 runs.
100
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 491?498, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Robust Named Entity extraction from large spoken archives
Beno??t Favre
Thales, MMP Laboratory
Colombes, France
benoit.favre@
fr.thalesgroup.com
Fre?de?ric Be?chet
LIA, University of Avignon
Avignon, France
frederic.bechet@
univ-avignon.fr
Pascal Noce?ra
LIA, University of Avignon
Avignon, France
pascal.nocera@
univ-avignon.fr
Abstract
Traditional approaches to Information Ex-
traction (IE) from speech input simply
consist in applying text based methods to
the output of an Automatic Speech Recog-
nition (ASR) system. If it gives satis-
faction with low Word Error Rate (WER)
transcripts, we believe that a tighter inte-
gration of the IE and ASR modules can
increase the IE performance in more dif-
ficult conditions. More specifically this
paper focuses on the robust extraction of
Named Entities from speech input where
a temporal mismatch between training and
test corpora occurs. We describe a Named
Entity Recognition (NER) system, de-
veloped within the French Rich Broad-
cast News Transcription program ESTER,
which is specifically optimized to pro-
cess ASR transcripts and can be integrated
into the search process of the ASR mod-
ules. Finally we show how some meta-
data information can be collected in or-
der to adapt NER and ASR models to new
conditions and how they can be used in a
task of Named Entity indexation of spoken
archives.
1 Introduction
Named Entity Recognition (NER) is a crucial step
in many Information Extraction (IE) tasks. It
has been a specific task in several evaluation pro-
grams such as the Message Understanding Confer-
ences (MUC), the Conferences on Natural Language
Learning (CoNLL), the DARPA HUB-5 program or
more recently the French ESTER Rich Transcription
program on Broadcast News data. Most of these
conferences have studied the impact of using tran-
scripts generated by an Automatic Speech Recogni-
tion (ASR) system rather than written texts. It ap-
pears from these studies that unlike other IE tasks,
NER performance is greatly affected by the Word
Error Rate (WER) of the transcripts processed. To
tackle this problem, different ideas have been pro-
posed: modeling explicitly the ASR errors (Palmer
and Ostendorf, 2001) or using the ASR system
alternate hypotheses found in word lattices (Sar-
aclar and Sproat, 2004). However performance in
NER decreases dramatically when processing high
WER transcripts like the ones that are obtained
with unmatched conditions between the ASR train-
ing model and the data to process. This paper in-
vestigates this phenomenon in the framework of the
NER task of the French Rich Transcription program
of Broadcast News ESTER (Gravier et al, 2004).
Several issues are addressed:
? how to jointly optimize the ASR and the NER
models ?
? what is the impact in term of ASR and NER
performance of a temporal mismatch between
the corpora used to train and test the models
and how can it be recovered by means of meta-
data information ?
? Can metadata information be used for indexing
large spoken archives ?
491
After a quick overview of related works in IE
from speech input, we present the ESTER evaluation
program ; then we introduce a NER system tightly
integrated to the ASR process and show how it
can successfully index high WER spoken databases
thanks to metadata.
2 Information extraction from large
spoken archives
The NIST Topic Detection and Tracking (Fiscus and
Doddington, 2002) and TREC document retrieval
evaluation programs has studied the impact of recog-
nition errors in the overall performance of Informa-
tion Extraction systems for tasks like story segmen-
tation or topic detection and retrieval. This impact
has been shown to be very limited compared to clean
text corpora. The main explanation for this phe-
nomenon is the redundancy effect: themes, topics
are very likely to be represented in texts by many
occurrences of salient words characterizing them.
Therefore, even if some of these words are missing,
numerical Information Extraction methods can use
the remaining salient words and discard the noise
generated by ASR errors.
However, this phenomenon is not true for tasks
related to the extraction of fine grained entities, like
Named Entities. Indeed, several studies have shown
that F-measure and WER are strongly correlated :
0.7 points of F-measure lost for each additional 1%
of WER according to (Miller et al, 2000) on the ex-
periments of 1998 NIST Hub-4 evaluations (Przy-
bocki et al, 1999).
Despite the continuous improvement of ASR
techniques, high WER transcriptions are inevitable
in difficult conditions like those found in large spo-
ken archives like in the MALACH project (Ramab-
hadran et al, 2003). Moreover, Named Entities ex-
traction performance is greatly affected by a mis-
match between training and testing data. This is
due mainly because proper names, which represent
most of the Named Entity items, are a very dynamic
category of words, strongly related to the period of
time representing the documents to process. There-
fore this mismatch is inevitable when dealing with
archives spreading over a long period of time and
containing multiple domain information.
One way of tackling this problem is to gather
metadata information related to the documents to
process. This information can be newspaper corpora
related to the same period of time, abstract describ-
ing the document content, or simply lists of terms or
entities likely to occur. Although such collected data
can be used to update the ASR and NER models,
the potential gain is rather small unless the metadata
corpus gathered fits perfectly the document to pro-
cess and is of a reasonable size. But another way
of exploiting this metadata information is to use it
as set of index terms that are going to be explicitly
looked for in the processed documents. We present
in section 7 an implementation of this idea that uses
word lattices as input.
3 The ESTER Named Entity evaluation
program
This work has been done within the framework of
the French Rich Transcription program of Broadcast
News ESTER. ESTER is organized by l?Association
Francophone de la Communication Parle?e (AFCP),
la De?le?gation Ge?ne?rale pour l?Armement (DGA)
and the Evaluation language Resources Distribution
Agency (ELDA). The ESTER corpus is made of 100
hours of Broadcast News data (from 6 French speak-
ing radio channels), manually transcribed, and la-
beled with a tagset of about 30 Named Entity cate-
gories folded in 8 main types:
? persons (pers): human beings, fiction charac-
ters, animals;
? locations (loc): geographical, traffic lines, elec-
tronic and real addresses, dial numbers;
? organizations (org): political, business, non
profit;
? geo-socio-political groups (gsp): clans, fami-
lies, nations, administrative regions;
? amounts (amount): durations, money, lengths,
temperature, age, weight and speed;
? time (time): relative and absolute time expres-
sions, hours;
? products (prod): art, printings, awards and ve-
hicles;
492
? facilities (fac): buildings, monuments.
This data is divided in 3 sets: a training set (84%),
a development set(8%) and a test set (8%). There
is a 6 month gap difference between the training
corpus and the test corpus while the development
corpus matches the training data from a temporal
point of view: the training corpus contains Broad-
cast News spreading from 2002 to December 2003;
the development corpus contains news from 2003;
the test corpus has been recorded in October 2004.
There are also 2 new radio channels in the test cor-
pus which were not in the training data.
For these reasons the development data is called
the matched corpus as the recording conditions
match those of the training corpus and the test data
is called the unmatched corpus. As a consequence,
we can study the effect of unmatched conditions on
ASR as well as IE performance and propose solu-
tions for dealing with this problem.
One of the main characteristics of the ESTER cor-
pus is the size of the NE tagset and the high ambi-
guity rate among the NE categories (eg. administra-
tive regions and geographical locations): 83% of the
matched corpus entities occur in the training corpus
and 40% of them are ambiguous whereas only 61%
of the unmatched corpus entities occur in the train-
ing corpus and 32% of them are ambiguous.
The most commonly used measures for evaluat-
ing NE extraction performance are Slot Error Rate
(SER) and F-measure. SER is very similar to WER
because it takes into account fine grained errors like
insertions, deletions and substitutions (entity type
and extent). The scoring process is based on the
same alignment between reference and hypothesis
data than the one obtained for measuring WER and
SER is known for being more accurate and penal-
izing than F-measure. Both measures weights can
be adjusted to favor recall or precision and therefore
adapted to a specific task.
SER = 100 ?
?
e?E ?e|e|
|Ref slots| F? =
(1+?2)RP
R+?2P
R = |Correct slots||Ref slots| P =
|Correct slots|
|Hyp slots|
with e ? E being an error type (insertion, dele-
tion, type, extent, type+extent, multiple) and ?e its
weight (resp. 1, 1, .5, .5, .8, 1.5) ; P is the precision
and R the recall; F1 is used in this paper.
4 Extracting NE from written text vs. ASR
output
As previously mentioned in section 2, WER and
SER performance are strongly correlated. Besides
the intrinsic difficulties of ASR (robustness to noise,
speaker variation, lack of coverage of the Language
Models used, . . . ), there is a source of errors which
is particularly important in IE from speech input:
the Out-Of-Vocabulary (OOV) word phenomenon.
Indeed, ASR models are built on huge textual cor-
pus and only contain the most frequent words to
limit computation and memory usage. If this is the
right approach to WER reduction, it is certainly not
valuable to information extraction where unlikely
events are considered as important. For instance,
many document retrieval models use inverse docu-
ment frequency (rareness) as a word weighting pa-
rameter. So, unlikely proper names are not in reach
of the ASR transcription system and hence cannot
be spotted by a Named Entity extraction module.
In addition to Out-of-Vocabulary words, two other
phenomenons have also a strong impact on NER
performance: the insertion of erroneous proper
names that automatically trigger the insertion of an
entity and spontaneous speech phenomenons. These
speech dysfluencies (hesitations, filled pauses, false
starts...) reduce the quality of the transcript because
they are usually not covered by language models
(built from textual data) or artificially introduced.
One should remove these from the transcript to im-
prove the quality of the labeling.
In order to deal with ASR errors two approaches
have been proposed:
? modeling explicitly the ASR errors, thanks to
a development corpus and a set of confidence
measures, in order to detect the possible er-
rors of the 1-best word string hypothesis (with
the type of errors) before extracting the NEs
(Palmer and Ostendorf, 2001);
? exploiting a search space bigger than the 1-best
hypothesis alone, either by taking into account
an n-best list (Zhai et al, 2004) or the whole
word lattice (Saraclar and Sproat, 2004).
493
The method proposed in this paper is close to this
second approach where the whole word lattice out-
put by the ASR system is used in order to increase
NER performance from noisy input.
We will present also in the next section a new
strategy for adapting NER models to ASR tran-
scripts, based on one of the main characteristics of
such transcripts: a closed vocabulary is used by
the ASR system. To our knowledge this has never
been fully exploited by NER systems. Indeed while
the key point of NER systems on written text is
their generalization capabilities when processing un-
known words, this feature is not relevant for ASR
transcripts as the system cannot generate words out
of the lexicon (there are no unknown words). There-
fore we propose here to fully exploit this constraint
(close vocabulary): since the OOV words cannot ap-
pear in the ASR transcripts, the NER models can
by over-trained on the words belonging to the ASR
lexicon. This is going to be developed in the next
section.
5 Robust Named Entity extraction
We have developed in this study two NER systems:
one is based on the freely available NLP tool Ling-
pipe1, adapted and trained on the French ESTER
corpus and dedicated to process text input. This sys-
tem is going to be called NERtext in the experiment
section. The second NER system has been devel-
oped for this study and is specifically built for being
tightly integrated with the ASR processes. The two
main features of this system, called NERasr in the
following, are its ability to process word lattices and
the fact that the NER models are trained for a spe-
cific ASR lexicon. These two systems are going to
be presented in the next sections.
5.1 Text-based NER system: NERtext
Among all the different methods that have been pro-
posed for NER, one can find rule based models
(Cunningham et al, 2002), Maximum Entropy mod-
els (Brothwick et al, 1998), Conditionnal Random
Fields or probabilistic HMM-based models (Bikel et
al., 1999).
Lingpipe implements an HMM-based model. It
maximizes the probability of a tag sequence Ti over
1Lingpipe: http://alias-i.com/lingpipe/
a word sequence Wi. A context of two preceding
words and one preceding tag is used to approximate
this probability. Generalization is done through a
simple process: words occurring with low frequency
are replaced by feature based categories (capitalized,
contains digits, . . . ). In this approach, there must be
one tag per word. Words starting and ending entities
are labeled with special tags. Because some features
are lacking in ASR transcripts (e.g. capitalization,
digits, sentence boundaries, . . . ) some word lists for
each kind of features are added as presented in (Ap-
pelt and Martin, 1999).
5.2 ASR-based NER system: NERasr
Errors occurring in ASR output lead NER systems
to overgenerate NE detections. This is due to both
erroneous words insertions in the ASR transcripts
as well as some abusive generalization made by the
NER systems. If these generalization capabilities
are very important for processing unknown words
in written texts, they can be an handicap in a closed-
vocabulary situation like the one observed when pro-
cessing ASR output. In order to reduce and con-
trol the insertion rate of our NER system, we im-
plemented a two level approach: the first level is
made of NE grammars coded as Finite State Ma-
chine (FSM) transducers and the second level is a
statistical HMM-based tagger.
5.2.1 NE transducers
To each NE category is attached a set of regular
grammars, extracted from the ESTER training cor-
pus and generalized thanks to the annotation guide-
lines and web-gathered word lists. Theses grammars
are represented by Finite State Machines (FSMs)
(thanks to the AT&T GRM/FSM toolkit (Allauzen et
al., 2003)). These FSMs are transducers that accept
word sequences on the input symbols and output NE
labels on the output symbols. They are all grouped
together in a single transducer, called Tgram, with
a filler model that accepts any string of words. Be-
cause these FSMs are lexicalized with the words of
the ASR lexicon, one can control the generalization
capabilities of the grammars thanks to the occur-
rence contexts of these words in the training corpus.
During the NER process, the first step is to compose
the FSM representing the NE transducer and the out-
put of the ASR module (either a 1-best word string
494
or a word lattice, both encoded as an FSM called G).
5.2.2 NE tagger
The result of the composition of the NE trans-
ducer with the ASR output is an FSM (G ? Tgram)
containing all the possible parsing made by the NE
grammars. In order to find the best analysis a sta-
tistical model is used to decide between entity types
and entities boundaries. This model is a 2nd order
n-gram model (trigram) represented by a weighted
FSM (called Ttagger) with the same framework as
the grammars. The most likely NE label sequence
is obtained by finding the best path in the FSM:
G?Tgram ?Ttagger. This corresponds to maximize
the following probability:
PW =
n
?
i=1
P (Wi, Ti|Wi?1, Ti?1,Wi?2, Ti?2)
This model is similar to the one implemented in
Lingpipe but it uses different smoothing methods.
Similarly, first and last words of entities are repre-
sented by special tags (this helps getting more accu-
rate boundaries) and low frequency words (appear-
ing less than a fixed number of times in the training
corpus) are generalized using their Part-Of-Speech
tags. The key points of this approach are that it has a
better control of the generalization capabilities than
a feature based NER system, thanks to the NE gram-
mars; it integrates the closed vocabulary constraint
of the ASR systems; and it is not limited to the 1-
best word hypothesis but can use the full ASR search
space (through word lattices) in order to detect en-
tities. Processing word lattices allows us to output,
at the end of the extraction process, an n-best list of
NE hypotheses. To each hypothesis are attached two
scores:
? the likelihood score given by the ASR model to
the best word string supporting this NE hypoth-
esis in the word lattice;
? the probability P (Wn, Tn, . . . ,W0, T0) given
by the NE tagger to the sequence of NE la-
bels T0, . . . , Tn and the sequence of words
W0, . . . ,Wn.
From this n-best list we can estimate the Oracle
performance of the NER system. This measure is
the recall measure upper bound than can be obtained
by extracting all the possible entities from a word
lattice, thanks to the NE transducers, and simulating
a perfect strategy that always take the right decision
in choosing among all the possible entities.
Decision strategies on such an n-best of NE hy-
pothesis can also involve other levels of informa-
tion on the document to process like the date or
the theme, for example. In the evaluation presented
in the next section we compare this Oracle perfor-
mance measure to the results of the simplest deci-
sion strategy which consists in choosing the NE hy-
pothesis with the highest likelihood.
5.3 Evaluation
The evaluation presented in Tables 1 and 2 is per-
formed using the Slot Error Rate and the F-measure
on the matched and unmatched corpora presented in
section 3.
corpus matched unmatched
tagger SER F-m SER F-m
NERtext 21 84 27 79
NERasr 23 84 37 74
WER 0 0
Figure 1: F-measure and Slot Error Rate measures
on the ESTER reference corpora (matched and un-
matched) for both NER systems
corpus matched unmatched
tagger SER F-m SER F-m Oracle
NERtext 42 72 55 63 61.9
NERasr 41 73 54 63 76.9
WER 21.2 26.4
Figure 2: F-measure, Slot Error Rate and Oracle re-
call measures on the ASR output of the matched and
unmatched corpora for both NER systems
Figure 1 presents SER and F-measure on the two
test sets (matched and unmatched) for the text ori-
ented (NERtext) and the speech oriented (NERasr)
NER systems, on clean text (manual transcripts).
Figure 2 shows the results obtained on the ASR tran-
scripts.
As expected on manually transcribed data,
NERtext obtains better results than NERasr (which
495
has poorer generalization capabilities). On the ASR
outputs the results obtained by both systems are
comparable however NERasr has the advantage of
processing word lattices, leading to an interesting
Oracle performance. We are studying now more
elaborate decision strategies in order to take fully
advantage of this feature.
The decrease in F-measure observed between the
reference and the ASR transcripts is similar to the
one obtained in other studies (Miller et al, 2000).
One observation that can be made on these results is
the impact of the time mismatch between the train-
ing and the test corpora. A 6 month difference in the
unmatched corpus leads to a very big drop in both
SER and F-measure. This can be explained by the
fact that NEs are very time-dependent. We are going
now to present some methods developed to tackle
this problem.
6 Updating Language and NE models with
metadata information
The only mismatch between the training and the un-
matched corpus of our experiments is a 6 months
temporal mismatch, therefore we collected a cor-
pus of newsletters made on a daily basis by the
French newspaper Le Monde corresponding to these
6 months. These newsletters contain an abstract of
the news of each day. We make the following two
hypotheses:
? firstly these newsletters are related to the same
time period as the unmatched corpus, there-
fore integrating them into the ASR models (lex-
icon+Language Model) should help reducing
the OOV word effect;
? secondly because they represent an abstract of
the news of each day, the Named Entities oc-
curring in a particular newsletter should contain
all the major events of the corresponding day
and therefore constitutes a useful list of terms
that can be used for indexing a Broadcast News
document related to the same period of time.
6.1 NE distribution analysis
This newsletter corpus contains 1M words and after
being tagged by the NERtext system, 140k entities
were extracted. To check the relevance of this cor-
pus for adapting the models to the unmatched test
corpus, we studied the distribution of the words and
the entities for each day, from January to December
2004. The unmatched test corpus is made of Broad-
cast News ranging from October 10th to October
16th 2004. The following observations were made:
72% of the NEs and 60% of the words contained in
them occur only one day in this corpus; the inter-
section of the NEs occurring in both the newsletter
of a particular day and the entities belonging to the
unmatched test corpus shows a peak, illustrated by
figure 3, for the days of the test corpus; at this peak,
25% of the NEs are used the same day in the two
corpora.
 0
 5
 10
 15
 20
 25
-20 -15 -10 -5  0  5  10  15  20
%
 e
nt
itie
s 
m
at
ch
in
g
 time window in days
Figure 3: Percentage of entities of the unmatched
corpus occuring at least n days earlier or later in the
newsletter corpus (at a window of 0 days, entities
appear on the same day in both corpus).
The first observation matches those presented in
(Whittaker, 2001) and validates our approach which
consists in carefully adapting the ASR and NER
models with data corresponding to the exact period
of time as the one of the documents to process: by
taking into account a larger period of time for the
adaptation corpus, the necessity of restraining the
models to the most frequent entities would lead to
discard low frequency terms that can be crucial for
characterizing the news of a given day.
If the second observation clearly highlights the
correlation between the NE distribution in both cor-
pora, it also points out that only 25% of the enti-
ties of the unmatched corpus occur in the newslet-
ters corresponding to the same days. Therefore the
potential improvement in the overall NER perfor-
mance is clearly limited. This will be confirmed in
the next section, however one can think that if these
496
entities are shared, for a given day, by both corpora,
it is because they represent the key topics of this day
and therefore they can be considered as very rele-
vant indexing terms for applications like document
retrieval. This last point is developed in section 7.
6.2 Model adaptation
Several studies (Whittaker, 2001; Federico and
Bertoldi, 2001; Chen et al, 2004) propose adap-
tation methods of a general language model to the
possibly small corpora extracted from these kinds of
metadata information (an overview of these meth-
ods can be found in (Bellegarda, 2004)). Depending
on the adaptation method and the kind of metadata
information used, some gains in performance have
been reported. But it appears that the choice of the
metadata and the size of the adaptation corpus col-
lected are critical in this adaptation process: if the
adaptation corpus is not exactly related to the topics
of the document to process, no real gains are ob-
tained (e.g. (Chen et al, 2004) reports that the best
gains were obtained with a story-based adaptation
method).
From all these previous works, our system imple-
ments the following adaptation process:
? the text corpus corresponding to the newsletters
is added to the ASR language model by means
of a linear interpolation;
? proper names occuring twice or more in the
newsletter corpus are added to the ASR lexi-
con;
? for the same days as those of the unmatched
corpus, this cutoff is suppressed and all the
proper names are added;
? the Named Entity wordlists and grammars are
also enriched with these proper names and en-
tities extracted from the collected corpus.
1K new proper names were added to the 65K
word ASR lexicon. The general OOV reduction ob-
tained was 0.14% leading to an absolute WER re-
duction of 0.3%. Similarly the SER decreased of
about 0.3% thanks to this adaptation and the Ora-
cle recall measure in the word lattices was improved
by an absolute 3%. These improvements are not
significant enough to justify the use of this kind of
metadata information for improving the general per-
formance of both ASR and NER processes. How-
ever, if we focus now on the entities occurring in the
newsletters corresponding to the exact days of the
unmatched corpus, the improvement is much more
significant, as presented in the next section.
7 Named Entity Indexation
As previously mentioned, 25% of the unmatched
corpus entities occur in the newsletters correspond-
ing to the same day as those of the unmatched test.
In order to measure the improvement obtained with
our adaptation technique on these particular entities,
we did the following experiment:
? a set of 352 entities was selected from the
newsletters related to same period of time as the
test, these entities represent the indexing terms
that are going to be looked for in the word lat-
tices of the unmatched corpus;
? the NERasr system was then applied to these
word lattices with two conditions: the word
lattices and the NER models before adaptation
and those obtained after adaptation with the
newsletter corpus;
? precision, recall, F-measure and Oracle error
rate were estimated for both conditions.
Condition Prec. Recall F-m Oracle
no adaptation 87.0 75.7 80.9 83.6
with adaptation 87.5 83.9 85.7 92
Figure 4: Extraction results on the selected NEs on
the unmatched corpus with and without adaptation
of the ASR and NER models on the newsletter cor-
pus
As we can see in table 4, the adaptation process
increases very significantly the recall measure of the
NE extraction. This is particularly relevant in some
IE tasks like the document retrieval task.
8 Conclusion
We have presented in this paper a robust Named En-
tity Recognition system dedicated to process ASR
transcripts. The FSM-based approach allows us to
497
control the generalization capabilities of the system
while the statistical tagger provides good labeling
decisions. The main feature of this system is its
ability to extract n-best lists of NE hypothesis from
word lattices leaving the decision strategy choosing
to either emphasize the recall or the precision of the
extraction, according to the task targeted. A compar-
ison between this approach and a standard approach
based on the NLP tools Lingpipe validates our hy-
potheses. This integration of the ASR and the NER
processes is particularly important in difficult con-
ditions like those that can be found in large spoken
archives where the training corpus does not match
all the documents to process. A study of the use of
metadata information in order to adapt the ASR and
NER models to a specific situation showed that if the
overall improvement is small, some salient informa-
tion related to the metadata added can be better ex-
tracted by means of this adaptation.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In ACL?03, Sapporo, Japan.
D. Appelt and D. Martin. 1999. Named entity extraction
from speech: Approach and results using the TextPro
system. In Proceedings Darpa Broadcast News Work-
shop.
Jerome R. Bellegarda. 2004. Statistical language model
adaptation: review and perspectives. Speech Commu-
nication, 42 Issue 1:93?108.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. volume 24, pages 211?231.
Andrew Brothwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Exploiting diverse knowl-
edge sources via maximum entropy in named entity
recognition.
Langzhou Chen, Jean-Luc Gauvain, Lori Lamel, and
Gilles Adda. 2004. Dynamic language modeling for
broadcast news. In In International Conference on
Speech and Language Processing, pages 1281?1284.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics.
M. Federico and N. Bertoldi. 2001. Broadcast news LM
adaptation using contemporary texts. In Proceedings
of European Conference on Speech Communication
and Technology (Eurospeech), pages 239?242, Aal-
borg, Denmark.
Jonathan G. Fiscus and George R. Doddington. 2002.
Topic detection and tracking evaluation overview.
Topic detection and tracking: event-based information
organization, pages 17?31.
G. Gravier, J.F. Bonastre, E. Geoffrois, S. Galliano,
K. McTait, and K. Choukri. 2004. ESTER, une cam-
pagne d?e?valuation des syste`mes d?indexation automa-
tique d?e?missions radiophoniques en franc?ais. In Proc.
Journe?es d?Etude sur la Parole (JEP).
David Miller, Sean Boisen, Richard Schwartz, Rebecca
Stone, and Ralph Weischedel. 2000. Named entity
extraction from noisy input: Speech and OCR. In Pro-
ceedings of ANLP-NAACL 2000, pages 316?324.
D. D. Palmer and M. Ostendorf. 2001. Improving in-
formation extraction by modeling errors in speech rec-
ognizer output. In Proceedings of the First Interna-
tional Conference on Human Language Technology
Research.
M. A. Przybocki, J. G. Fiscus, J. S. Garofolo, and D. S.
Pallett. 1999. 1998 Hub-4 Information Extraction
Evaluation. In Proceedings Of The DARPA Broad-
cast News Workshop, pages 13?18. Morgan Kaufmann
Publishers.
Bhuvana Ramabhadran, Jing Huang, and Michael
Picheny. 2003. Towards automatic transcription of
large spoken archives - english ASR for the MALACH
project. In Proc. IEEE International Conference on
Acoustics, Speech and Signal Processing, ICASSP,
pages 216?219.
Murat Saraclar and Richard Sproat. 2004. Lattice-based
search for spoken utterance retrieval. In HLT-NAACL
2004: Main Proceedings, pages 129?136, Boston,
Massachusetts, USA. Association for Computational
Linguistics.
E. W. D. Whittaker. 2001. Temporal adaptation of lan-
guage models. In Adaptation Methods for Speech
Recognition, ISCA Tutorial and Research Workshop
(ITRW), August. LM Adaptation for information re-
trieval of spoken news/radio programs (i.e. Speech-
Bot).
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine
Carpuat, and Dekai Wu. 2004. Using n-best lists
for named entity recognition from chinese speech.
In HLT-NAACL 2004: Short Papers, pages 37?40,
Boston, Massachusetts, USA. Association for Compu-
tational Linguistics.
498
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 86?91,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
MACAON
An NLP Tool Suite for Processing Word Lattices
Alexis Nasr Fre?de?ric Be?chet Jean-Franc?ois Rey Beno??t Favre Joseph Le Roux?
Laboratoire d?Informatique Fondamentale de Marseille- CNRS - UMR 6166
Universite? Aix-Marseille
(alexis.nasr,frederic.bechet,jean-francois.rey,benoit.favre,joseph.le.roux)
@lif.univ-mrs.fr
Abstract
MACAON is a tool suite for standard NLP tasks
developed for French. MACAON has been de-
signed to process both human-produced text
and highly ambiguous word-lattices produced
by NLP tools. MACAON is made of several na-
tive modules for common tasks such as a tok-
enization, a part-of-speech tagging or syntac-
tic parsing, all communicating with each other
through XML files . In addition, exchange pro-
tocols with external tools are easily definable.
MACAON is a fast, modular and open tool, dis-
tributed under GNU Public License.
1 Introduction
The automatic processing of textual data generated
by NLP software, resulting from Machine Transla-
tion, Automatic Speech Recognition or Automatic
Text Summarization, raises new challenges for lan-
guage processing tools. Unlike native texts (texts
produced by humans), this new kind of texts is the
result of imperfect processors and they are made
of several hypotheses, usually weighted with con-
fidence measures. Automatic text production sys-
tems can produce these weighted hypotheses as n-
best lists, word lattices, or confusion networks. It is
crucial for this space of ambiguous solutions to be
kept for later processing since the ambiguities of the
lower levels can sometimes be resolved during high-
level processing stages. It is therefore important to
be able to represent this ambiguity.
?This work has been funded by the French Agence Nationale
pour la Recherche, through the projects SEQUOIA (ANR-08-
EMER-013) and DECODA (2009-CORD-005-01)
MACAON is a suite of tools developped to pro-
cess ambiguous input and extend inference of in-
put modules within a global scope. It con-
sists in several modules that perform classical
NLP tasks (tokenization, word recognition, part-of-
speech tagging, lemmatization, morphological anal-
ysis, partial or full parsing) on either native text
or word lattices. MACAON is distributed under
GNU public licence and can be downloaded from
http://www.macaon.lif.univ-mrs.fr/.
From a general point of view, a MACAON module
can be seen as an annotation device1 which adds a
new level of annotation to its input that generally de-
pends on annotations from preceding modules. The
modules communicate through XML files that allow
the representation different layers of annotation as
well as ambiguities at each layer. Moreover, the ini-
tial XML structuring of the processed files (logical
structuring of a document, information from the Au-
tomatic Speech Recognition module . . . ) remains
untouched by the processing stages.
As already mentioned, one of the main charac-
teristics of MACAON is the ability for each module
to accept ambiguous inputs and produce ambiguous
outputs, in such a way that ambiguities can be re-
solved at a later stage of processing. The compact
representation of ambiguous structures is at the heart
of the MACAON exchange format, described in sec-
tion 2. Furthermore every module can weight the
solutions it produces. such weights can be used to
rank solutions or limit their number for later stages
1Annotation must be taken here in a general sense which in-
cludes tagging, segmentation or the construction of more com-
plex objets as syntagmatic or dependencies trees.
86
of processing.
Several processing tools suites alread exist for
French among which SXPIPE (Sagot and Boullier,
2008), OUTILEX (Blanc et al, 2006), NOOJ2 or UNI-
TEX3. A general comparison of MACAON with these
tools is beyond the scope of this paper. Let us just
mention that MACAON shares with most of them the
use of finite state machines as core data represen-
tation. Some modules are implemented as standard
operations on finite state machines.
MACAON can also be compared to the numerous
development frameworks for developping process-
ing tools, such as GATE4, FREELING5, ELLOGON6
or LINGPIPE7 that are usually limited to the process-
ing of native texts.
The MACAON exchange format shares a cer-
tain number of features with linguistic annotation
scheme standards such as the Text Encoding Initia-
tive8, XCES9, or EAGLES10. They all aim at defining
standards for various types of corpus annotations.
The main difference between MACAON and these
approaches is that MACAON defines an exchange for-
mat between NLP modules and not an annotation
format. More precisely, this format is dedicated to
the compact representation of ambiguity: some in-
formation represented in the exchange format are
to be interpreted by MACAON modules and would
not be part of an annotation format. Moreover,
the MACAON exchange format was defined from the
bottom up, originating from the authors? need to use
several existing tools and adapt their input/output
formats in order for them to be compatible. This is in
contrast with a top down approach which is usually
chosen when specifying a standard. Still, MACAON
shares several characteristics with the LAF (Ide and
Romary, 2004) which aims at defining high level
standards for exchanging linguistic data.
2www.nooj4nlp.net/pages/nooj.html
3www-igm.univ-mlv.fr/?unitex
4gate.ac.uk
5garraf.epsevg.upc.es/freeling
6www.ellogon.org
7alias-i.com/lingpipe
8www.tei-c.org/P5
9www.xml-ces.org
10www.ilc.cnr.it/eagles/home.html
2 The MACAON exchange format
The MACAON exchange format is based on four con-
cepts: segment, attribute, annotation level and seg-
mentation.
A segment refers to a segment of the text or
speech signal that is to be processed, as a sentence,
a clause, a syntactic constituent, a lexical unit, a
named entity . . . A segment can be equipped with at-
tributes that describe some of its aspects. A syntac-
tic constituent, for example, will define the attribute
type which specifies its syntactic type (Noun Phrase,
Verb Phrase . . . ). A segment is made of one or more
smaller segments.
A sequence of segments covering a whole sen-
tence for written text, or a spoken utterance for oral
data, is called a segmentation. Such a sequence can
be weighted.
An annotation level groups together segments of
a same type, as well as segmentations defined on
these segments. Four levels are currently defined:
pre-lexical, lexical, morpho-syntactic and syntactic.
Two relations are defined on segments: the prece-
dence relation that organises linearly segments of a
given level into segmentations and the dominance
relation that describes how a segment is decomposed
in smaller segments either of the same level or of a
lower level.
We have represented in figure 2, a schematic rep-
resentation of the analysis of the reconstructed out-
put a speech recognizer would produce on the in-
put time flies like an arrow11. Three annotation lev-
els have been represented, lexical, morpho-syntactic
and syntactic. Each level is represented by a finite-
state automaton which models the precedence rela-
tion defined over the segments of this level. Seg-
ment time, for example, precedes segment flies. The
segments are implicitly represented by the labels of
the automaton?s arcs. This label should be seen as
a reference to a more complex objet, the actual seg-
ment. The dominance relations are represented with
dashed lines that link segments of different levels.
Segment time, for example, is dominated by seg-
ment NN of the morpho-syntactic level.
This example illustrates the different ambiguity
cases and the way they are represented.
11For readability reasons, we have used an English example,
MACAON, as mentioned above, currently exists for French.
87
thyme
time
flies like
liken
an arrow
a row
JJ IN
VB
DT NN
DT NN
VB
NN
NN
VBZ
VB VB
VP
VP
NP
NP
VP
NP
VP
VP
PP
NP
NP
Figure 1: Three annotation levels for a sample sentence.
Plain lines represent annotation hypotheses within a level
while dashed lines represent links between levels. Trian-
gles with the tip up are ?and? nodes and triangles with
the tip down are ?or? nodes. For instance, in the part-of-
speech layer, The first NN can either refer to ?time? or
?thyme?. In the chunking layer, segments that span mul-
tiple part-of-speech tags are linked to them through ?and?
nodes.
The most immediate ambiguity phenomenon is
the segmentation ambiguity: several segmentations
are possible at every level. This ambiguity is rep-
resented in a compact way through the factoring of
segments that participate in different segmentations,
by way of a finite state automaton.
The second ambiguity phenomenon is the dom-
inance ambiguity, where a segment can be decom-
posed in several ways into lower level segments.
Such a case appears in the preceding example, where
the NN segment appearing in one of the outgoing
transition of the initial state of the morpho-syntactic
level dominates both thyme and time segments of the
lexical level. The triangle with the tip down is an
?or? node, modeling the fact that NN corresponds to
time or thyme.
Triangles with the tip up are ?and? nodes. They
model the fact that the PP segment of the syntac-
tic level dominates segments IN, DT and NN of the
morpho-syntactic level.
2.1 XML representation
The MACAON exchange format is implemented in
XML. A segment is represented with the XML tag
<segment> which has four mandatory attributes:
? type indicates the type of the segment, four dif-
ferent types are currently defined: atome (pre-
lexical unit usually referred to as token in en-
glish), ulex (lexical unit), cat (part of speech)
and chunk (a non recursive syntactic unit).
? id associates to a segment a unique identifier in
the document, in order to be able to reference
it.
? start and end define the span of the segment.
These two attributes are numerical and repre-
sent either the index of the first and last char-
acter of the segment in the text string or the
beginning and ending time of the segment in
a speech signal.
A segment can define other attributes that can be
useful for a given description level. We often find
the stype attribute that defines subtypes of a given
type.
The dominance relation is represented through the
use of the <sequence> tag. The domination of the
three segments IN, DT and NN by a PP segment,
mentionned above is represented below, where p1,
p2 and p3 are respectively the ids of segments IN,
DT and NN.
<segment type="chunk" stype="PP" id="c1">
<sequence>
<elt segref="p1"/>
<elt segref="p2"/>
<elt segref="p3"/>
</sequence>
</segment>
The ambiguous case, described above where seg-
ment NN dominates segments time or thyme is rep-
resented below as a disjunction of sequences inside
a segment. The disjunction itself is not represented
as an XML tag. l1 and l2 are respectively the ids
of segments time and thyme.
<segment type="cat" stype="NN" id="c1">
<sequence>
<elt segref="l1" w="-3.37"/>
</sequence>
<sequence>
<elt segref="l2" w="-4.53"/>
</sequence>
</segment>
88
The dominance relation can be weighted, by way
of the attribute w. Such a weight represents in the
preceding example the conditional log-probability
of a lexical unit given a part of speech, as in a hidden
Markov model.
The precedence relation (i.e. the organization
of segments in segmentations), is represented as a
weighted finite state automaton. Automata are rep-
resented as a start state, accept states and a list of
transitions between states, as in the following exam-
ple that corresponds to the lexical level of our exam-
ple.
<fsm n="9">
<start n="0"/>
<accept n="6"/>
<ltrans>
<trans o="0" d="1" i="l1" w="-7.23"/>
<trans o="0" d="1" i="l2" w="-9.00"/>
<trans o="1" d="2" i="l3" w="-3.78"/>
<trans o="2" d="3" i="l4" w="-7.37"/>
<trans o="3" d="4" i="l5" w="-3.73"/>
<trans o="2" d="4" i="l6" w="-6.67"/>
<trans o="4" d="5" i="l7" w="-4.56"/>
<trans o="5" d="6" i="l8" w="-2.63"/>
<trans o="4" d="6" i="l9" w="-7.63"/>
</ltrans>
</fsm>
The <trans/> tag represents a transition, its
o,d,i and w features are respectively the origin, and
destination states, its label (the id of a segment) and
a weight.
An annotation level is represented by the
<section> tag which regroups two tags, the
<segments> tag that contains the different segment
tags defined at this annotation level and the <fsm>
tag that represents all the segmentations of this level.
3 The MACAON architecture
Three aspects have guided the architecture of
MACAON: openness, modularity, and speed. Open-
ness has been achieved by the definition of an ex-
change format which has been made as general as
possible, in such a way that mapping can be de-
fined from and to third party modules as ASR, MT
systems or parsers. Modularity has been achieved
by the definition of independent modules that com-
municate with each other through XML files using
standard UNIX pipes. A module can therefore be re-
placed easily. Speed has been obtained using effi-
cient algorithms and a representation especially de-
signed to load linguistic data and models in a fast
way.
MACAON is composed of libraries and compo-
nents. Libraries contain either linguistic data, mod-
els or API functions. Two kinds of components are
presented, the MACAON core components and third
party components for which mappings to and from
the MACAON exchange format have been defined.
3.1 Libraries
The main MACAON library is macaon common.
It defines a simple interface to the MACAON ex-
change format and functions to load XML MACAON
files into memory using efficient data structures.
Other libraries macaon lex, macaon code and
macaon tagger lib represent the lexicon, the
morphological data base and the tagger models in
memory.
MACAON only relies on two third-party libraries,
which are gfsm12, a finite state machine library and
libxml, an XML library13.
3.2 The MACAON core components
A brief description of several standard components
developed in the MACAON framework is given be-
low. They all comply with the exchange format de-
scribed above and add a <macaon stamp> to the
XML file that indicates the name of the component,
the date and the component version number, and rec-
ognizes a set of standard options.
maca select is a pre-processing component: it adds
a macaon tag under the target tags specified by
the user to the input XML file. The follow-
ing components will only process the document
parts enclosed in macaon tags.
maca segmenter segments a text into sentences by
examining the context of punctuation with a
regular grammar given as a finite state automa-
ton. It is disabled for automatic speech tran-
scriptions which do not typically include punc-
tuation signs and come with their own segmen-
tation.
12ling.uni-potsdam.de/?moocow/projects/
gfsm/
13xmlsoft.org
89
maca tokenizer tokenizes a sentence into pre-
lexical units. It is also based on regular gram-
mars that recognize simple tokens as well as a
predefined set of special tokens, such as time
expressions, numerical expressions, urls. . . .
maca lexer allows to regroup pre-lexical units into
lexical units. It is based on the lefff French lex-
icon (Sagot et al, 2006) which contains around
500,000 forms. It implements a dynamic pro-
gramming algorithm that builds all the possible
grouping of pre-lexical units into lexical units.
maca tagger associates to every lexical unit one or
more part-of-speech labels. It is based on a
trigram Hidden Markov Model trained on the
French Treebank (Abeille? et al, 2003). The es-
timation of the HMM parameters has been re-
alized by the SRILM toolkit (Stolcke, 2002).
maca anamorph produces the morphological anal-
ysis of lexical units associated to a part of
speech. The morphological information come
from the lefff lexicon.
maca chunker gathers sequences of part-of-speech
tags in non recursive syntactic units. This com-
ponent implements a cascade of finite state
transducers, as proposed by Abney (1996). It
adds some features to the initial Abney pro-
posal, like the possibility to define the head of
a chunk.
maca conv is a set of converters from and to the
MACAON exchange format. htk2macaon
and fsm2macaon convert word lattices from
the HTK format (Young, 1994) and ATT
FSM format (Mohri et al, 2000) to the
MACAON exchange format. macaon2txt and
txt2macaon convert from and to plain text
files. macaon2lorg and lorg2macaon
convert to and from the format of the LORG
parser (see section 3.3).
maca view is a graphical interface that allows to in-
spect MACAON XML files and run the compo-
nents.
3.3 Third party components
MACAON is an open architecture and provides a rich
exchange format which makes possible the repre-
sentation of many NLP tools input and output in the
MACAON format. MACAON has been interfaced with
the SPEERAL Automatic Speech Recognition Sys-
tem (Nocera et al, 2006). The word lattices pro-
duced by SPEERAL can be converted to pre-lexical
MACAON automata.
MACAON does not provide any native module for
parsing yet but it can be interfaced with any already
existing parser. For the purpose of this demonstra-
tion we have chosen the LORG parser developed at
NCLT, Dublin14. This parser is based on PCFGs
with latent annotations (Petrov et al, 2006), a for-
malism that showed state-of-the-art parsing accu-
racy for a wide range of languages. In addition it of-
fers a sophisticated handling of unknown words re-
lying on automatically learned morphological clues,
especially for French (Attia et al, 2010). Moreover,
this parser accepts input that can be tokenized, pos-
tagged or pre-bracketed. This possibility allows for
different settings when interfacing it with MACAON.
4 Applications
MACAON has been used in several projects, two of
which are briefly described here, the DEFINIENS
project and the LUNA project.
DEFINIENS (Barque et al, 2010) is a project that
aims at structuring the definitions of a large coverage
French lexicon, the Tre?sor de la langue franc?aise.
The lexicographic definitions have been processed
by MACAON in order to decompose the definitions
into complex semantico-syntactic units. The data
processed is therefore native text that possesses a
rich XML structure that has to be preserved during
processing.
LUNA15 is a European project that aims at extract-
ing information from oral data about hotel booking.
The word lattices produced by an ASR system have
been processed by MACAON up to a partial syntactic
level from which frames are built. More details can
be found in (Be?chet and Nasr, 2009). The key aspect
of the use of MACAON for the LUNA project is the
ability to perform the linguistic analyses on the mul-
tiple hypotheses produced by the ASR system. It is
therefore possible, for a given syntactic analysis, to
14www.computing.dcu.ie/?lorg. This software
should be freely available for academic research by the time
of the conference.
15www.ist-luna.eu
90
Figure 2: Screenshot of the MACAON visualization inter-
face (for French models). It allows to input a text and see
the n-best results of the annotation.
find all the word sequences that are compatible with
this analysis.
Figure 2 shows the interface that can be used to
see the output of the pipeline.
5 Conclusion
In this paper we have presented MACAON, an NLP
tool suite which allows to process native text as well
as several hypotheses automatically produced by an
ASR or an MT system. Several evolutions are cur-
rently under development, such as a named entity
recognizer component and an interface with a de-
pendency parser.
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. In Workshop on Robust Parsing, 8th European
Summer School in Logic, Language and Information,
Prague, Czech Republic, pages 8?15.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL.
Lucie Barque, Alexis Nasr, and Alain Polgue`re. 2010.
From the definitions of the tre?sor de la langue franc?aise
to a semantic database of the french language. In EU-
RALEX 2010, Leeuwarden, Pays Bas.
Fre?de?ric Be?chet and Alexis Nasr. 2009. Robust depen-
dency parsing for spoken language understanding of
spontaneous speech. In Interspeech, Brighton, United
Kingdom.
Olivier Blanc, Matthieu Constant, and Eric Laporte.
2006. Outilex, plate-forme logicielle de traitement de
textes e?crits. In TALN 2006, Leuven.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural language engineering, 10(3-4):211?225.
M. Mohri, F. Pereira, and M. Riley. 2000. The design
principles of a weighted finite-state transducer library.
Theoretical Computer Science, 231(1):17?32.
P. Nocera, G. Linares, D. Massonie?, and L. Lefort. 2006.
Phoneme lattice based A* search algorithm for speech
recognition. In Text, Speech and Dialogue, pages 83?
111. Springer.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Beno??t Sagot and Pierre Boullier. 2008. Sxpipe 2:
architecture pour le traitement pre?syntaxique de cor-
pus bruts. Traitement Automatique des Langues,
49(2):155?188.
Beno??t Sagot, Lionel Cle?ment, Eric Villemonte de la
Clergerie, and Pierre Boullier. 2006. The lefff 2 Syn-
tactic Lexicon for French: Architecture, Acquisition,
Use. In International Conference on Language Re-
sources and Evaluation, Genoa.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, Denver, Colorado.
S.J. Young. 1994. The HTK Hidden Markov Model
Toolkit: Design and Philosophy. Entropic Cambridge
Research Laboratory, Ltd, 2:2?44.
91
The UMUS system for named entity generation at GREC 2010
Benoit Favre
LIUM, Universite? du Maine
72000 Le Mans, France
benoit.favre@gmail.com
Bernd Bohnet
Universita?t Stuttgart
Stuttgart, Germany
bohnet@informatik.uni-stuttgart.de
Abstract
We present the UMUS (Universite? du
Maine/Universita?t Stuttgart) submission
for the NEG task at GREC?10. We re-
fined and tuned our 2009 system but we
still rely on predicting generic labels and
then choosing from the list of expressions
that match those labels. We handled recur-
sive expressions with care by generating
specific labels for all the possible embed-
dings. The resulting system performs at a
type accuracy of 0.84 an a string accuracy
of 0.81 on the development set.
1 Introduction
The Named Entity Generation (NEG) task con-
sists in choosing a referential expression (com-
plete name, last name, pronoun, possessive pro-
noun, elision...) for all person entities in a text.
Texts are biographies of chefs, composers and in-
ventors from Wikipedia. For each reference, a list
of expressions is given from which the system has
to choose. This task is challenging because of the
following aspects:
1. The data is imperfect as it is a patchwork of
multiple authors? writing.
2. The problem is hard to handle with a classi-
fier because text is predicted, not classes.
3. The problem has a complex graph structure.
4. Some decisions are recursive for embedded
references, i.e. ?his father?.
5. Syntactic/semantic features cannot be ex-
tracted with a classical parser because the
word sequence is latent.
We do not deal with all of these challenges but
we try to mitigate their impact. Our system ex-
tends our approach for GREC?09 (Favre and Bon-
het, 2009). We use a sequence classifier to predict
generic labels for the possible expressions.
2 Labels for classification
Each referential expression (REFEX) is given a la-
bel consisting of sub-elements:
? The REG08 TYPE as given in the REFEX
(name, common, pronoun, empty...)
? The CASE as given in the REFEX (plain,
genitive, accusative...)
? If the expression is a pronoun, then one of
?he, him, his, who, whom, whose, that?, after
gender and number normalization.
? ?self? if the expression contains ?self?.
? ?short? if the expression is a one-word long
name or common name.
? ?nesting? if the expression is recursive.
For recursive expressions, a special handling is ap-
plied: All possible assignments of the embedded
entities are generated with labels corresponding
to the concatenation of the involved entities? la-
bels. If the embedding is on the right (left) side
of the expression, ?right? (?left?) is added to the
label. Non-sensical labels (i.e. ?he father?) are not
seen in the training data, and therefore not hypoth-
esized.
3 Features
Each reference is characterized with the following
features:
? SYNFUNC, SEMCAT, SYNCAT: syntactic
function, semantic category, syntactic cate-
gory, as given in REF node.
? CHANGE, CHANGE+SYNFUNC: previous
reference is for a different entity, possibly
with syntactic function.
? PREV GENDER NUMBER: if the refer-
ence is from a different entity, can be ?same?
or ?different?. The attribute is being com-
pared is ?male?, ?female? or ?plural?, deter-
mined by looking at the possible expressions.
? FIRST TIME: denotes if it?s the first time
that the entity is seen. For plural entities, the
entity is considered new if at least one of the
involved entities is new.
? BEG PARAGRAPH: the first entity of a
paragraph.
? {PREV,NEXT} PUNCT: the punctuation
immediately before (after) the entity. Can be
?sentence? if the punctuation is one of ?.?!?,
?comma? for ?,;?, ?parenthesis? for ?()[]?
and ?quote?.
? {PREV,NEXT} SENT: whether or not a sen-
tence boundary occurs after (before) the pre-
vious (next) reference.
? {PREV,NEXT} WORD {1,2}GRAM: cor-
responding word n-gram. Words are ex-
tracted up to the previous/next reference or
the start/end of a sentence, with parenthe-
sized content removed. Words are lower-
cased tokens made of letters and numbers.
? {PREV,NEXT} TAG: most likely part-of-
speech tag for the previous/next word, skip-
ping adverbs.
? {PREV,NEXT} BE: any form of the verb ?to
be? is used after (before) the previous (next)
reference.
? EMBEDS PREV: the entity being embedded
was referred to just before.
? EMBEDS ALL KNOWN: all the entities be-
ing embedded have been seen before.
4 Sequence classifier
We rely on Conditional Random Fields1 (Lafferty
et al, 2001) for predicting one label (as defined
previously) per reference. We lay the problem as
one sequence of decisions per entity to prevent, for
instance, the use of the same name twice in a row.
Last year, we generated one sequence per docu-
ment with all entities, but it was less intuitive. To
the features extracted for each reference, we add
the features of the previous and next reference, ac-
cording to label unigrams and label bigrams. The
c hyperparameter and the frequency cutoff of the
classifier are optimized on the dev set. Note that
1CRF++, http://crfpp.sourceforge.net
for processing the test set, we added the develop-
ment data to the training set.
5 Text generation
For each reference, the given expressions are
ranked by classifier-estimated posterior probabil-
ity and the best one is used for output. In case
multiple expressions have the same labeling (and
the same score), we use the longest one and iter-
ate through the list for each subsequent use (useful
for repeated common names). If an expression is
more than 4 words, it?s flagged for not being used
a second time (only ad-hoc rule in the system).
6 Results
Evaluation scores for the output are presented in
Table 1. The source code of our systems is made
available to the community at http://code.google
.com/p/icsicrf-grecneg.
Sys. T.acc Prec. Rec. S.acc Bleu Nist
Old 0.826 0.830 0.830 0.786 0.811 5.758
New 0.844 0.829 0.816 0.813 0.817 6.021
Table 1: Results on the dev set comparing our sys-
tem from last year (old) to the refined one (new),
according to REG08 TYPE accuracy (T.acc), pre-
cision and recall, String accuracy (S.acc), BLEU1
an NIST.
About 50% of the errors are caused by the se-
lection of pronouns instead of a name. The selec-
tion of the pronoun or name seems to depend on
the writing style since a few authors prefer nearly
always the name. The misuse of names instead
of pronouns is second most error with about 15%.
The complex structured named entities are respon-
sible for about 9% of the errors. The selection of
the right name such as given name, family name or
both seems to be more difficult. The next frequent
errors are confusions between pronouns, elisions,
common names, and names.
References
Benoit Favre and Bernd Bonhet. 2009. ICSI-CRF: The
Generation of References to the Main Subject and
Named Entities Using Conditional Random Fields.
In ACL-IJCNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for
segmenting and labeling sequence data. Machine
Learning, pages 282?289.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 89?99,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Generative Constituent Parsing and Discriminative Dependency Reranking:
Experiments on English and French
Joseph Le Roux Beno?t Favre? Alexis Nasr? Seyed Abolghasem Mirroshandel?,?
LIPN, Universit? Paris Nord ? CNRS UMR 7030, Villetaneuse, France
?LIF, Universit? Aix-Marseille ? CNRS UMR 7279, Marseille, France
?Computer Engineering Department, Sharif university of Technology, Tehran, Iran
leroux@univ-paris13.fr, benoit.favre@lif.univ-mrs.fr,
alexis.nasr@lif.univ-mrs.fr, ghasem.mirroshandel@lif.univ-mrs.fr
Abstract
We present an architecture for parsing in two
steps. A phrase-structure parser builds for
each sentence an n-best list of analyses which
are converted to dependency trees. These de-
pendency structures are then rescored by a dis-
criminative reranker. Our method is language
agnostic and enables the incorporation of ad-
ditional information which are useful for the
choice of the best parse candidate. We test
our approach on the the Penn Treebank and
the French Treebank. Evaluation shows a sig-
nificative improvement on different parse met-
rics.
1 Introduction
Two competing approaches exist for parsing natural
language. The first one, called generative, is based
on the theory of formal languages and rewriting sys-
tems. Parsing is defined here as a process that trans-
forms a string into a tree or a tree forest. It is of-
ten grounded on phrase-based grammars ? although
there are generative dependency parsers ? in partic-
ular context-free grammars or one of their numer-
ous variants, that can be parsed in polynomial time.
However, the independence hypothesis that under-
lies this kind of formal system does not allow for
precise analyses of some linguistic phenomena, such
as long distance and lexical dependencies.
In the second approach, known as discriminative,
the grammar is viewed as a system of constraints
over the correct syntactic structures, the words of the
sentence themselves being seen as constraints over
the position they occupy in the sentence. Parsing
boils down to finding a solution that is compatible
with the different constraints. The major problem of
this approach lies in its complexity. The constraints
can, theoretically, range over any aspect of the final
structures, which prevents from using efficient dy-
namic programming techniques when searching for
a global solution. In the worst case, final structures
must be enumerated in order to be evaluated. There-
fore, only a subset of constraints is used in imple-
mentations for complexity reasons. This approach
can itself be divided into formalisms relying on logic
to describe constraints, as the model-theoretic syn-
tax (Pullum and Scholz, 2001), or numerical for-
malisms that associate weights to lexico-syntactic
substructures. The latter has been the object of some
recent work thanks to progresses achieved in the
field of Machine Learning. A parse tree is repre-
sented as a vector of features and its accuracy is
measured as the distance between this vector and the
reference.
One way to take advantage of both approaches
is to combine them sequentially, as initially pro-
posed by Collins (2000). A generative parser pro-
duces a set of candidates structures that constitute
the input of a second, discriminative module, whose
search space is limited to this set of candidates.
Such an approach, parsing followed by reranking,
is used in the Brown parser (Charniak and Johnson,
2005). The approach can be extended in order to
feed the reranker with the output of different parsers,
as shown by (Johnson and Ural, 2010; Zhang et al,
2009).
In this paper we are interested in applying rerank-
ing to dependency structures. The main reason is
that many linguistic constraints are straightforward
to implement on dependency structures, as, for ex-
ample, subcategorization frames or selectional con-
straints that are closely linked to the notion of de-
89
pendents of a predicate. On the other hand, depen-
dencies extracted from constituent parses are known
to be more accurate than dependencies obtained
from dependency parsers. Therefore the solution we
choose is an indirect one: we use a phrase-based
parser to generate n-best lists and convert them to
lists of dependency structures that are reranked. This
approach can be seen as trade-off between phrase-
based reranking experiments (Collins, 2000) and the
approach of Carreras et al (2008) where a discrimi-
native model is used to score lexical features repre-
senting unlabelled dependencies in the Tree Adjoin-
ing Grammar formalism.
Our architecture, illustrated in Figure 1, is based
on two steps. During the first step, a syntagmatic
parser processes the input sentence and produces n-
best parses as well as their probabilities. They are
annotated with a functional tagger which tags syn-
tagms with standard syntactic functions subject, ob-
ject, indirect object . . . and converted to dependency
structures by application of percolation rules. In the
second step, we extract a set of features from the
dependency parses and the associated probabilities.
These features are used to reorder the n-best list
and select a potentially more accurate parse. Syn-
tagmatic parses are produced by the implementation
of a PCFG-LA parser of (Attia et al, 2010), simi-
lar to (Petrov et al, 2006), a functional tagger and
dependency converter for the target language. The
reranking model is a linear model trained with an
implementation of the MIRA algorithm (Crammer et
al., 2006)1.
Charniak and Johnson (2005) and Collins (2000)
rerank phrase-structure parses and they also include
head-dependent information, in other words unla-
belled dependencies. In our approach we take into
account grammatical functions or labelled depen-
dencies.
It should be noted that the features we use are very
generic and do not depend on the linguistic knowl-
edge of the authors. We applied our method to En-
glish, the de facto standard for testing parsing tech-
nologies, and French which exhibits many aspects of
a morphologically rich language. But our approach
could be applied to other languages, provided that
1This implementation is available at https://github.
com/jihelhere/adMIRAble.
the resources ? treebanks and conversion tools ? ex-
ist.
(1) PCFG-LA n-best constituency parses
(2) Function annotation
(3) Conversion to dependency parses
(4) Feature extraction
(5) MIRA reranking
w
Final constituency & dependency parse
Input text
Figure 1: The parsing architecture: production of the n-
best syntagmatic trees (1) tagged with functional labels
(2), conversion to a dependency structure (3) and feature
extraction (4), scoring with a linear model (5). The parse
with the best score is considered as final.
The structure of the paper is the following: in
Section 2 we describe the details of our generative
parser and in Section 3 our reranking model together
with the features templates. Section 4 reports the re-
sults of the experiments conducted on the Penn Tree-
bank (Marcus et al, 1994) as well as on the Paris 7
Treebank (Abeill? et al, 2003) and Section 5 con-
cludes the paper.
2 Generative Model
The first part of our system, the syntactic analysis
itself, generates surface dependency structures in a
sequential fashion (Candito et al, 2010b; Candito
et al, 2010a). A phrase structure parser based on
Latent Variable PCFGs (PCFG-LAs) produces tree
structures that are enriched with functions and then
converted to labelled dependency structures, which
will be processed by the parse reranker.
90
2.1 PCFG-LAs
Probabilistic Context Free Grammars with Latent
Annotations, introduced in (Matsuzaki et al, 2005)
can be seen as automatically specialised PCFGs
learnt from treebanks. Each symbol of the gram-
mar is enriched with annotation symbols behaving
as subclasses of this symbol. More formally, the
probability of an unannotated tree is the sum of the
probabilities of its annotated counterparts. For a
PCFG-LA G, R is the set of annotated rules, D(t)
is the set of (annotated) derivations of an unanno-
tated tree t, and R(d) is the set of rules used in a
derivation d. Then the probability assigned by G to
t is:
PG(t) =
?
d?D(t)
PG(d) =
?
d?D(t)
?
r?R(d)
PG(r) (1)
Because of this alternation of sums and products
that cannot be optimally factorised, there is no ex-
act polynomial dynamic programming algorithm for
parsing. Matsuzaki et al (2005) and Petrov and
Klein (2007) discuss approximations of the decod-
ing step based on a Bayesian variational approach.
This enables cubic time decoding that can be fur-
ther enhanced with coarse-to-fine methods (Char-
niak and Johnson, 2005).
This type of grammars has already been tested
on a variety of languages, in particular English
and French, giving state-of-the-art results. Let us
stress that this phrase-structure formalism is not lex-
icalised as opposed to grammars previously used in
reranking experiments (Collins, 2000; Charniak and
Johnson, 2005). The notion of lexical head is there-
fore absent at parsing time and will become avail-
able only at the reranking step.
2.2 Dependency Structures
A syntactic theory can either be expressed with
phrase structures or dependencies, as advocated for
in (Rambow, 2010). However, some information
may be simpler to describe in one of the representa-
tions. This equivalence between the modes of repre-
sentations only stands if the informational contents
are the same. Unfortunately, this is not the case
here because the phrase structures that we use do
not contain functional annotations and lexical heads,
whereas labelled dependencies do.
This implies that, in order to be converted
into labelled dependency structures, phrase struc-
ture parses must first be annotated with functions.
Previous experiments for English and French as
well (Candito et al, 2010b) showed that a sequential
approach is better than an integrated one for context-
free grammars, because the strong independence hy-
pothesis of this formalism implies a restricted do-
main of locality which cannot express the context
needed to properly assign functions. Most func-
tional taggers, such as the ones used in the following
experiments, rely on classifiers whose feature sets
can describe the whole context of a node in order to
make a decision.
3 Discriminative model
Our discriminative model is a linear model
trained with the Margin-Infused Relaxed Algorithm
(MIRA) (Crammer et al, 2006). This model com-
putes the score of a parse tree as the inner product
of a feature vector and a weight vector represent-
ing model parameters. The training procedure of
MIRA is very close to that of a perceptron (Rosen-
blatt, 1958), benefiting from its speed and relatively
low requirements while achieving better accuracy.
Recall that parsing under this model consists in
(1) generating a n-best list of constituency parses
using the generative model, (2) annotating each of
them with function tags, (3) converting them to de-
pendency parses, (4) extracting features, (5) scoring
each feature vector against the model, (6) selecting
the highest scoring parse as output.
For training, we collect the output of feature ex-
traction (4) for a large set of training sentences and
associate each parse tree with a loss function that de-
notes the number of erroneous dependencies com-
pared to the reference parse tree. Then, model
weights are adjusted using MIRA training so that the
parse with the lowest loss gets the highest score. Ex-
amples are processed in sequence, and for each of
them, we compute the score of each parse according
to the current model and find an updated weight vec-
tor that assigns the first rank to the best parse (called
oracle). Details of the algorithm are given in the fol-
lowing sections.
91
3.1 Definitions
Let us consider a vector space of dimensionmwhere
each component corresponds to a feature: a parse
tree p is represented as a sparse vector ?(p). The
model is a weight vector w in the same space where
each weight corresponds to the importance of the
features for characterizing good (or bad) parse trees.
The score s(p) of a parse tree p is the scalar product
of its feature vector ?(p) and the weight vector w.
s(p) =
m?
i=1
wi?i(p) (2)
Let L be the n-best list of parses produced by the
generative parser for a given sentence. The highest
scoring parse p? is selected as output of the reranker:
p? = argmax
p?L
s(p) (3)
MIRA learning consists in using training sen-
tences and their reference parses to determine the
weight vector w. It starts with w = 0 and modifies
it incrementally so that parses closest to the refer-
ence get higher scores. Let l(p), loss of parse p,
be the number of erroneous dependencies (governor,
dependent, label) compared to the reference parse.
We define o, the oracle parse, as the parse with the
lowest loss in L.
Training examples are processed in sequence as
an instance of online learning. For each sentence,
we compute the score of each parse in the n-best
list. If the highest scoring parse differs from the or-
acle (p? 6= o), the weight vector can be improved.
In this case, we seek a modification of w ensuring
that o gets a better score than p? with a difference
at least proportional to the difference between their
loss. This way, very bad parses get pushed deeper
than average parses. Finding such weight vector
can be formulated as the following constrained opti-
mization problem:
minimize: ||w||2 (4)
subject to: s(o)? s(p?) ? l(o)? l(p?) (5)
Since there is an infinity of weight vectors that
satisfy constraint 5, we settle on the one with the
smallest magnitude. Classical constrained quadratic
optimization methods can be applied to solve this
problem: first, Lagrange multipliers are used to in-
troduce the constraint in the objective function, then
the Hildreth algorithm yields the following analytic
solution to the non-constrained problem:
w? = w + ? (?(o)? ?(p?)) (6)
? = max
[
0,
l(o)? l(p?)? [s(o)? s(p?)]
||?(o)? ?(p?)||2
]
(7)
Here, w? is the new weight vector, ? is an up-
date magnitude and [?(o)? ?(p?)] is the difference
between the feature vector of the oracle and that of
the highest scoring parse. This update, similar to
the perceptron update, draws the weight vector to-
wards o while pushing it away from p?. Usual tricks
that apply to the perceptron also apply here: (a) per-
forming multiple passes on the training data, and (b)
averaging the weight vector over each update2. Al-
gorithm 1 details the instructions for MIRA training.
Algorithm 1 MIRA training
for i = 1 to t do
for all sentences in training set do
Generate n-best list L from generative parser
for all p ? L do
Extract feature vector ?(p)
Compute score s(p) (eq. 2)
end for
Get oracle o = argminp l(p)
Get best parse p? = argmaxp s(p)
if p? 6= o then
Compute ? (eq. 7)
Update weight vector (eq. 6)
end if
end for
end for
Return average weight vector over updates.
3.2 Features
The quality of the reranker depends on the learning
algorithm as much as on the feature set. These fea-
tures can span over any subset of a parse tree, up to
the whole tree. Therefore, there are a very large set
of possible features to choose from. Relevant fea-
tures must be general enough to appear in as many
2This can be implemented efficiently using two weight vec-
tors as for the averaged perceptron.
92
parses as possible, but specific enough to character-
ize good and bad configurations in the parse tree.
We extended the feature set from (McDonald,
2006) which showed to be effective for a range of
languages. Our feature templates can be categorized
in 5 classes according to their domain of locality.
In the following, we describe and exemplify these
templates on the following sentence from the Penn
treebank, in which we target the PMOD dependency
between ?at? and ?watch.?
Probability Three features are derived from the
PCFG-LA parser, namely the posterior proba-
bility of the parse (eq. 1), its normalized prob-
ability relative to the 1-best, and its rank in the
n-best list.
Unigram Unigram features are the most simple as
they only involve one word. Given a depen-
dency between position i and position j of type
l, governed by xi, denoted xi
l
? xj , two fea-
tures are created: one for the governor xi and
one for the dependent xj . They are described
as 6-tuples (word, lemma, pos-tag, is-governor,
direction, type of dependency). Variants with
wildcards at each subset of tuple slots are also
generated in order to handle sparsity.
In our example, the dependency between
?looked? and ?at? generates two features:
[at, at, IN, G, R, PMOD] and
[looked, look, NN, D, L, PMOD]
And also wildcard features such as:
[-, at, IN, G, R, PMOD], [at,
-, IN, G, R, PMOD] ...
[at, -, -, -, -, PMOD]
This wildcard feature generation is applied to
all types of features. We will omit it in the re-
mainder of the description.
Bigram Unlike the previous template, bigram fea-
tures model the conjunction of the governor
and the dependent of a dependency relation,
like bilexical dependencies in (Collins, 1997).
Given dependency xi
l
? xj , the feature cre-
ated is (word xi, lemma xi, pos-tag xi, word
xj , lemma xj , pos-tag xj , distance3 from i to
j, direction, type).
The previous example generates the following
feature:
[at, at, IN, watch, watch, NN,
2, R, PMOD]
Where 2 is the distance between ?at? and
?watch?.
Linear context This feature models the linear con-
text between the governor and the dependent
of a relation by looking at the words between
them. Given dependency xi
l
? xj , for each
word from i + 1 to j ? 1, a feature is created
with the pos-tags of xi and xj , and the pos tag
of the word between them (no feature is create
if j = i + 1). An additional feature is created
with pos-tags at positions i? 1, i, i+ 1, j ? 1,
j, j +1. Our example yields the following fea-
tures:
[IN, PRP$, NN], and [VBD, IN,
PRP$, PRP$, NN, .].
Syntactic context: siblings This template and the
next one look at two dependencies in two con-
figurations. Given two dependencies xi
l
? xj
and xi
m
? xk, we create the feature (word,
lemma, pos-tag for xi, xj and xk, distance from
i to j, distance from i to k, direction and type of
each of the two dependencies). In our example:
[looked, look, VBD, I, I, PRP,
at, at, IN, 1, 1, L, SBJ, R,
ADV]
Syntactic context: chains Given two dependencies
xi
l
? xj
m
? xk, we create the feature (word,
lemma, pos-tag of xi, xj and xk, distance from
i to j, distance from i to k, direction and type of
each of the two dependencies). In our example:
[looked, look, VBD, at, at, IN,
watch, watch, NN, 1, 2, R, ADV,
3In every template, distance features are quantified in 7
classes: 1, 2, 3, 4, 5, 5 to 10, more.
93
R, PMOD]
It is worth noting that our feature templates only
rely on information available in the training set, and
do not use any external linguistic knowledge.
4 Experiments
In this section, we evaluate our architecture on
two corpora, namely the Penn Treebank (Marcus et
al., 1994) and the French Treebank (Abeill? et al,
2003). We first present the corpora and the tools
used for annotating and converting structures, then
the performances of the phrase structure parser alone
and with the discriminative reranker.
4.1 Treebanks and Tools
For English, we use the Wall Street Journal sections
of the Penn Treebank. We learn the PCFG-LA from
sections 02-214. We then use FUNTAG (Chrupa?a
et al, 2007) to add functions back to the PCFG-LA
analyses. For the conversion to dependency struc-
tures we use the LTH tool (Johansson and Nugues,
2007). In order to get the gold dependencies, we run
LTH directly on the gold parse trees. We use sec-
tion 22 for development and section 23 for the final
evaluation.
For French, we use the Paris 7 Treebank (or
French Treebank, FTB). As in several previous ex-
periments we decided to divide the 12,350 phrase
structure trees in three sets: train (80%), develop-
ment (10%) and test (10%). The syntactic tag set for
French is not fixed and we decided to use the one
described in (Candito and Crabb?, 2009) to be able
to compare this system with recent parsing results
on French. As for English, we learn the PCFG-LA
without functional annotations which are added af-
terwards. We use the dependency structures devel-
oped in (Candito et al, 2010b) and the conversion
toolkit BONSA?. Furthermore, to test our approach
against state of the art parsing results for French
we use word clusters in the phrase-based parser as
in (Candito and Crabb?, 2009).
For both languages we constructed 10-fold train-
ing data from train sets in order to avoid overfitting
the training data. The trees from training sets were
divided into 10 subsets and the parses for each sub-
set were generated by a parser trained on the other
4Functions are omitted.
9 subsets. Development and test parses are given by
a parser using the whole training set. Development
sets were used to choose the best reranking model.
For lemmatisation, we use the MATE lemmatiser
for English and a home-made lemmatiser for French
based on the lefff lexicon (Sagot, 2010).
4.2 Generative Model
The performances of our parser are summarised in
Figure 2, (a) and (b), where F-score denotes the Par-
seval F-score5, and LAS and UAS are respectively
the Labelled and Unlabelled Attachment Score of
the converted dependency structures6. We give or-
acle scores (the score that our system would get if
it selected the best parse from the n-best lists) when
the parser generates n-best lists of depth 10, 20, 50
and 100 in order to get an idea of the effectiveness
of the reranking process.
One of the issues we face with this approach is
the use of an imperfect functional annotator. For
French we evaluate the loss of accuracy on the re-
sulting dependency structure from the gold develop-
ment set where functions have been omitted. The
UAS is 100% but the LAS is 96.04%. For English
the LAS from section 22 where functions are omit-
ted is 95.35%.
From the results presented in this section we can
make two observations. First, the results of our
parser are at the state of the art on English (90.7%
F-score) and on French (85.7% F-score). So the
reranker will be confronted with the difficult task of
improving on these scores. Second, the progression
margin is sensible with a potential LAS error reduc-
tion of 41% for English and 40.2% for French.
4.3 Adding the Reranker
4.3.1 Learning Feature Weights
The discriminative model, i.e. template instances
and their weights, is learnt on the training set parses
obtained via 10-fold cross-validation. The genera-
tive parser generates 100-best lists that are used as
learning example for the MIRA algorithm. Feature
extraction produces an enormous number of fea-
tures: about 571 millions for English and 179 mil-
5We use a modified version of evalb that gives the ora-
cle score when the parser outputs a list of candidates for each
sentence.
6All scores are measured without punctuation.
94
(a) Oracle Scores on PTB dev set (b) Oracle Scores on FTB dev set
8990
9192
9394
9596
97
1 10 20 50 100Size of n-best list
UASLASF-score
Oracle s
core
86
88
90
92
94
86
88
90
92
94
1 10 20 50 100Size of n?best list
Oracle s
core
UASLASF-score
(c) Reranker scores on PTB dev set (d) Reranker scores on FTB dev set
899
091
929
394
1 10 20 50 100Size of n-best list
UASLASF-score
Reranke
d score
868
788
899
091
1 10 20 50 100Size of n?best list
UASLASF-score
Rerank
er score
Figure 2: Oracle and reranker scores on PTB and FTB data on the dev. set, according to the depth of the n-best.
lions for French. Let us remark that this large set of
features is not an issue because our discriminative
learning algorithm is online, that is to say it consid-
ers only one example at a time, and it only gives
non-null weights to useful features.
4.3.2 Evaluation
In order to test our system we first tried to eval-
uate the impact of the length of the n-best list over
the reranking predictions7. The results are shown in
Figure 2, parts (c) and (d).
For French, we can see that even though the LAS
and UAS are consistently improving with the num-
ber of candidates, the F-score is maximal with 50
candidates. However the difference between 50 can-
didates and 100 candidates is not statistically signifi-
cant. For English, the situation is simpler and scores
improve continuously on the three metrics.
Finally we run our system on the test sets for both
treebanks. Results are shown8 in Table 1 for En-
glish, and Table 2 for French. For English the im-
provement is 0.9% LAS, 0.7% Parseval F-score and
7The model is always trained with 100 candidates.
8F < 40 is the parseval F-score for sentences with less than
40 words.
0.8% UAS.
Baseline Reranker
F 90.4 91.1
F < 40 91.0 91.7
LAS 88.9 89.8
UAS 93.1 93.9
Table 1: System results on PTB Test set
For French we have improvements of 0.3/0.7/0.9.
If we add a template feature indicating the agree-
ment between part-of-speech provided by the PCFG-
LA parser and a part-of-speech tagger (Denis
and Sagot, 2009), we obtain better improvements:
0.5/0.8/1.1.
Baseline Reranker Rerank + MElt
F 86.6 87.3 87.4
F < 40 88.7 89.0 89.2
LAS 87.9 89.0 89.2
UAS 91.0 91.9 92.1
Table 2: System results on FTB Test set
95
4.3.3 Comparison with Related Work
We compare our results with related parsing re-
sults on English and French.
For English, the main results are shown in Ta-
ble 3. From the presented data, we can see that
indirect reranking on LAS may not seem as good
as direct reranking on phrase-structures compared to
F-scores obtained in (Charniak and Johnson, 2005)
and (Huang, 2008) with one parser or (Zhang et
al., 2009) with several parsers. However, our sys-
tem does not rely on any language specific feature
and can be applied to other languages/treebanks. It
is difficult to compare our system for LAS because
most systems evaluate on gold data (part-of-speech,
lemmas and morphological information) like Bohnet
(2010).
Our system also compares favourably with the
system of Carreras et al (2008) that relies on a more
complex generative model, namely Tree Adjoining
Grammars, and the system of Suzuki et al (2009)
that makes use of external data (unannotated text).
F LAS UAS
Huang, 2008 91.7 ? ?
Bohnet, 2010 ? 90.3 ?
Zhang et al 2008 91.4 ? 93.2
Huang and Sagae, 2010 ? ? 92.1
Charniak et al 2005 91.5 90.0 94.0
Carreras et al 2008 ? ? 93.5
Suzuki et al 2009 ? ? 93.8
This work 91.1 89.8 93.9
Table 3: Comparison on PTB Test set
For French, see Table 4, we compare our system
with the MATE parser (Bohnet, 2010), an improve-
ment over the MST parser (McDonald et al, 2005)
with hash kernels, using the MELT part-of-speech
tagger (Denis and Sagot, 2009) and our own lemma-
tiser.
We also compare the French system with results
drawn from the benchmark performed by Candito et
al. (2010a). The first system (BKY-FR) is close to
ours without the reranking module, using the Berke-
ley parser adapted to French. The second (MST-
FR) is based on MSTParser (McDonald et al, 2005).
These two system use word clusters as well.
The next section takes a close look at the models
of the reranker and its impact on performance.
F < 40 LAS UAS
This work 89.2 89.2 92.1
MATE + MELT ? 89.2 91.8
BKY-FR 88.2 86.8 91.0
MST-FR ? 88.2 90.9
Table 4: Comparison on FTB Test set
4.3.4 Model Analysis
It is interesting to note that in the test sets, the
one-best of the syntagmatic parser is selected 52.0%
of the time by the reranker for English and 34.3% of
the time for French. This can be explained by the
difference in the quantity of training data in the two
treebanks (four times more parses are available for
English) resulting in an improvement of the quality
of the probabilistic grammar.
We also looked at the reranking models, specifi-
cally at the weight given to each of the features. It
shows that 19.8% of the 571 million features have
a non-zero weight for English as well as 25.7% of
the 179 million features for French. This can be ex-
plained by the fact that for a given sentence, features
that are common to all the candidates in the n-best
list are not discriminative to select one of these can-
didates (they add the same constant weight to the
score of all candidates), and therefore ignored by the
model. It also shows the importance of feature engi-
neering: designing relevant features is an art (Char-
niak and Johnson, 2005).
We took a closer look at the 1,000 features of
highest weight and the 1,000 features of lowest
weight (negative) for both languages that represent
the most important features for discriminating be-
tween correct and incorrect parses. For English,
62.0% of the positive features are backoff features
which involve at least one wildcard while they are
85.9% for French. Interestingly, similar results hold
for negative features. The difference between the
two languages is hard to interpret and might be due
in part to lexical properties and to the fact that these
features may play a balancing role against towards
non-backoff features that promote overfitting.
Expectedly, posterior probability features have
the highest weight and the n-best rank feature has the
highest negative weight. As evidenced by Table 5,
96
en (+) en (-) fr (+) fr (-)
Linear 30.4 36.1 44.8 44.0
Unigram 20.7 16.3 9.7 8.2
Bigram 27.4 29.1 20.8 24.4
Chain 15.4 15.3 13.7 19.4
Siblings 5.8 3.0 10.8 3.6
Table 5: Repartition of weight (in percentage) in the
1,000 highest (+) and lowest (-) weighted features for En-
glish and French.
among the other feature templates, linear context oc-
cupies most of the weight mass of the 1,000 highest
weighted features. It is interesting to note that the
unigram and bigram templates are less present for
French than for English while the converse seems to
be true for the linear template. Sibling features are
consistently less relevant.
In terms of LAS performance, on the PTB test
set the reranked output is better than the baseline
on 22.4% of the sentences while the opposite is true
for 10.4% of the sentences. In 67.0% of the sen-
tences, they have the same LAS (but not necessar-
ily the same errors). This emphasises the difficulty
of reranking an already good system and also ex-
plains why oracle performance is not reached. Both
the baseline and reranker output are completely cor-
rect on 21.3% of the sentences, while PCFG-LA cor-
rectly parses 23% of the sentences and the MIRA
brings that number to 26%.
Figures 3 and 4 show hand-picked sentences for
which the reranker selected the correct parse. The
French sentence is a typical difficult example for
PCFGs because it involves a complex rewriting rule
which might not be well covered in the training
data (SENT ? NP VP PP PONCT PP PONCT PP
PONCT). The English example is tied to a wrong
attachment of the prepositional phrase to the verb
instead of the date, which lexicalized features of the
reranker handle easily.
5 Conclusion
We showed that using a discriminative reranker, on
top of a phrase structure parser, based on converted
dependency structures could lead to significant im-
provements over dependency and phrase structure
parse results. We experimented on two treebanks
for two languages, English and French and we mea-
sured the improvement of parse quality on three dif-
ferent metrics: Parseval F-score, LAS and UAS,
with the biggest error reduction on the latter. How-
ever the gain is not as high as expected by looking
at oracle scores, and we can suggest several possible
improvements on this method.
First, the sequential approach is vulnerable to cas-
cading errors. Whereas the generative parser pro-
duces several candidates, this is not the case of the
functional annotators: these errors are not amend-
able. It should be possible to have a functional tag-
ger with ambiguous output upon which the reranker
could discriminate. It remains an open question as
how to integrate ambiguous output from the parser
and from the functional tagger. The combination
of n-best lists would not scale up and working on
the ambiguous structure itself, the packed forest as
in (Huang, 2008), might be necessary. Another pos-
sibility for future work is to let the phrase-based
parser itself perform function annotation, but some
preliminary tests on French showed disappointing
results.
Second, designing good features, sufficiently gen-
eral but precise enough, is, as already coined
by Charniak and Johnson (2005), an art. More for-
mally, we can see several alternatives. Dependency
structures could be exploited more thoroughly using,
for example, tree kernels. The restricted number of
candidates enables the use of more global features.
Also, we haven?t used any language-specific syntac-
tic features. This could be another way to improve
this system, relying on external linguistic knowledge
(lexical preferences, subcategorisation frames, cop-
ula verbs, coordination symmetry . . . ). Integrating
features from the phrase-structure trees is also an op-
tion that needs to be explored.
Third this architecture enables the integration of
several systems. We experimented on French using a
part-of-speech tagger but we could also use another
parser and either use the methodology of (Johnson
and Ural, 2010) or (Zhang et al, 2009) which fu-
sion n-best lists form different parsers, or use stack-
ing methods where an additional parser is used as
a guide for the main parser (Nivre and McDonald,
2008).
Finally it should be noted that this system does not
rely on any language specific feature, and thus can
be applied to languages other that French or English
97
NNSStocks
NP
VBDwere CD698 CDmillion
QP
NNSbushels
NP
INon NNPMay CD31
NP
INof DTthis NNyear
NP
PP
NP
PP
VP
..
S
NNSStocks
NP
VBDwere CD698 CDmillion
QP
NNSbushels
NP
INon NNPMay CD31
NP
PP
INof DTthis NNyear
NP
PP
VP
..
S
depen
dency parse
s
synta
gmati
c
parse
s
Before reranking After reranking
Figure 3: English sentence from the WSJ test set for which the reranker selected the correct tree while the first
candidate of the n-best list suffered from an incorrect attachment.
SENT
NP VN PP
PONCT
NP
PONCTNPP NPP V VPP P AP DET ADJ PONCT P ADJADJ
SENT
NP VN PP
PONCT NP PONCTNPP NPP V VPP P AP P NC PONCT P ADJADJ NP
PP PP
NPP NPP V VPP P ADJ PONCT DET ADJ PONCT P ADJ PONCT NPP NPP V VPP P ADJ PONCT P NC PONCT P ADJ PONCTdepen
dency parses
syntag
matic parses
Before reranking After reranking
Figure 4: Sentence from the FTB for which the best parse according to baseline was incorrect, probably due to the
tendency of the PCFG-LA model to prefer rules with more support. The reranker selected the correct parse.
without re-engineering new reranking features. This
makes this architecture suitable for morphologically
rich languages.
Acknowledgments
This work has been funded by the French Agence
Nationale pour la Recherche, through the project
SEQUOIA (ANR-08-EMER-013).
References
Anne Abeill?, Lionel Cl?ment, and Toussenel Fran?ois,
2003. Treebanks, chapter Building a treebank for
French. Kluwer, Dordrecht.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceedings
of COLING.
M.-H. Candito and B. Crabb?. 2009. Improving Gen-
erative Statistical Parsing with Semi-Supervised Word
Clustering. In Proceedings of IWPT 2009.
M.-H. Candito, J. Nivre, P. Denis, and E. Henestroza An-
guiano. 2010a. Benchmarking of Statistical Depen-
dency Parsers for French. In Proceedings of COL-
ING?2010.
Marie Candito, Beno?t Crabb?, and Pascal Denis. 2010b.
Statistical French Dependency Parsing : Treebank
Conversion and First Results. In Proceedings of
LREC2010.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, Dynamic Programming and the Perceptron for
Efficient, Feature-rich Parsing. In CONLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of ACL.
98
Grzegorz Chrupa?a, Nicolas Stroppa, Josef van Genabith,
and Georgiana Dinu. 2007. Better training for func-
tion labeling. In Proceedings of RANLP, Borovets,
Bulgaria.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL.
Michael Collins. 2000. Discriminative Reranking for
Natural Language Parsing. In Proceedings of ICML.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
ShalevShwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithm. Journal of Machine
Learning Research.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Pro-
ceedings PACLIC 23, Hong Kong, China.
Liang Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-Local Features. In Proceedings of
ACL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia, May 25-26.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 665?668, Los An-
geles, California, June. Association for Computational
Linguistics.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
Proceedings of the ARPA Speech and Natural Lan-
guage Workshop.
Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsujii.
2005. Probabilistic CFG with Latent Annotations. In
Proceedings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. In Association for Computational Linguistics
(ACL).
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL, pages 950?958.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In HLT-NAACL, pages
404?411.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Geoffrey K. Pullum and Barbara C. Scholz. 2001. On the
distinction between model-theoretic and generative-
enumerative syntactic frameworks. In Logical Aspects
of Computational Linguistics.
Owen Rambow. 2010. The Simple Truth about Depen-
dency and Phrase Structure Representations: An Opin-
ion Piece. In NAACL HLT.
Frank Rosenblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psychological Review.
Beno?t Sagot. 2010. The lefff, a freely available and
large-coverage lexicon for french. In Proceedings of
LREC 2010, La Valette, Malta.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
An empirical study of semi-supervised structured con-
ditional models for dependency parsing. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2-Volume 2,
pages 551?560. Association for Computational Lin-
guistics.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of EMNLP.
99
