Proceedings of the NAACL HLT 2010: Demonstration Session, pages 5?8,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Summarizing Textual Information about Locations  
In a Geo-Spatial Information Display System

Congxing Cai Eduard Hovy
Information Sciences Institute Information Sciences Institute 
University of Southern California University of Southern California 
Marina del Rey, California, USA 90292 Marina del Rey, California, USA 90292 
ccai@isi.edu hovy@isi.edu 
Abstract 
This demo describes the summarization of 
textual material about locations in the context 
of a geo-spatial information display system. 
When the amount of associated textual data is 
large, it is organized and summarized before 
display. A hierarchical summarization frame-
work, conditioned on the small space availa-
ble for display, has been fully implemented. 
Snapshots of the system, with narrative de-
scriptions, demonstrate our results.  
1 Introduction 
Geospatial display systems are increasingly gain-
ing attention, given the large amounts of geospatial 
data and services available online. Although geos-
patial imagery and maps show geometric relations 
among entities, they cannot be used to present oth-
er kinds of knowledge about the temporal, topic, 
and other conceptual relations and entities. Given 
an entity on a map, a description of what happened 
there, in what order in time, when, and why, re-
quires additional types of information, typically 
contained in text, in order to support varied search 
and decision tasks.  
In this demo, we apply text summarization to a 
geo-spatial information display system with poten-
tially large amounts of textual data. By summariz-
ing the textual material linked to each location, we 
demonstrate the ways one can organize this ma-
terial for optimal display and search. 
Of the many different types of text-oriented re-
sources available, some are structured and others 
unstructured. This textual data can be linked to 
locations based on different reasons (containing 
place names, addresses, real objects with geo-
graphical features, etc.). Appropriately grouping 
and presenting the different aspects of the textual 
information in summarization is a challenging task. 
A second challenge stems from the huge amounts 
of web material related to some geographical ob-
jects. For example, one may find millions of pages 
for a famous place or event at a specific map loca-
tion. Given the common limitations of display 
space in most geospatial display systems, one must 
also design the interface to support dynamic 
browsing and search. 
All these challenges bring new problems to exist-
ing summarization techniques. In the following 
sections, we demonstrate a hierarchical summari-
zation framework that reduces displayed text and 
fully utilizes the small display space available for 
textual information.  
2 Related Work 
Associating each news page individually to its lo-
cation(s) may overwhelm the amount of informa-
tion displayable at any point and thereby limit the 
scalability of the system. Existing systems pre-
sented in (Teitler et al, 2008) and GeoTracker 
(Chen et al 2007) organize material (at the area 
level) by time instead of somehow aggregating 
over larger numbers of related content. Since fre-
quently the associated news contents overlap at 
least in part, a natural solution is to aggregate the 
content somehow to remove duplication. Moreo-
ver, the aggregation of news provides a global 
view of the textual information about the specific 
5
location. Our system is the first available geo-
spatial text aggregation system to our knowledge.  
Within geospatial display systems, the space avail-
able to display textual information is often quite 
limited. We therefore need to summarize the most 
important and relevant information about each lo-
cation, drawing from all the web pages linked to it. 
However, directly applying a multi-document 
summarization (Lin and Hovy, 2001) to the web 
pages will generate poor results, due to unrelated 
titles, duplicate articles, and noisy contents con-
tained in web pages. When several different events 
have occurred at a location, more than one distinct 
summary may be needed. It is therefore important 
to deploy topic recognition (Lin and Hovy, 2000) 
and/or topic clustering (Osinski and Weiss, 2005) 
to identify and group relevant pieces of each text 
into single-topic ?chunks?. We develop a novel 
hierarchical summarization system to improve the 
interactivity and browsability.   
3 Text Summarization 
3.1 Content Extraction and Summarization 
Multi-webpage summarization is different from 
traditional multi-doc summarization. First, most 
web pages are much more complex than pure text 
documents. Since the web contains a combination 
of types of information?static text, image, videos, 
dynamic layout, etc.?even a single page can be 
treated as multiple documents. Current linking 
functions are based on keywords, making the rele-
vant content of each relevant web page only a li-
mited block within the page. Second, our task is 
oriented to locations, and hence differs from gen-
eral content summarization. Hence, we need to 
identify and extract the essential part(s) of the 
webpage linked to the geospatial imagery for 
summarization and display. In our work, we utilize 
two important features, layout and semantics, to 
identify and extract the relevant content. 
By rendering each web page into a DOM tree, we 
segment the page into large blocks based on its 
layout, including header, footer, left bar, right bar, 
main block, etc. We implemented a rule-based ex-
tractor to extract the most relevant block from the 
web page based on the relevance to the location.  
3.2 Clustering 
Given a list of text blocks relevant to a local point 
of interest, one can employ traditional text summa-
rization techniques to produce a short summary for 
each one. This solution may not be helpful, how-
ever, since a long list of pages associated with each 
point of interest would be very hard for users to 
browse. Especially when the space allocated to text 
display by the geospatial system is also limited, a 
high compression ratio is typically required for the 
summarization system. 
The solution we adopt is to deploy cluster-based 
multi-document summarization. Clustering must 
observe two criteria: first, the location of interest, 
and second, the text topic. Different clustering me-
thods can be employed. To delimit topics, a simple 
heuristic is to introduce as additional criterion the 
event/article date: when the difference in document 
dates within a topical cluster is (far) larger than the 
actual duration of the topic event, we are probably 
dealing with multiple separate events at the same 
location. Better performance is obtained by using a 
topic detection module first, and then clustering 
documents based on the topics identified.  
Unfortunately, documents usually contain multiple 
locations and multiple topics. The problem of ?top-
ic drift? can cause confusion in a short summary. 
As in (Hearst, 1997), we segment each document 
into several ?mini-documents?, each one devoted to 
a single topic, and then to perform location- and 
topic-based clustering over the (now larger) set of 
mini-documents.  
3.3 Hierarchical Summary Generation  
Whatever the clustering approach, the result is a 
potentially rather large set of individual topics as-
sociated with each location. Since screen space for 
the summaries may be very limited next to the 
maps / imagery, they have to be formatted and pre-
sented for maximal interpretability. To address this 
problem, we adopt a hierarchical structure to dis-
play incrementally longer summaries for each loca-
tion of interest. At present we have found three 
levels of incrementally longer summaries to be 
most useful. 
Thumbnail: a very short ?topic? that characte-
rizes the (clusters of) documents or segments asso-
ciated with each location. We present essentially 
one or two single keywords -- the most informative 
6
words for each cluster. We implemented a new 
version of our topic signature technology, one that 
uses tf.idf instead of the entropy ratio, as scoring 
measure to rank each cluster?s words. 
Title: a headline-length phrase or short sen-
tence (or two). The original titles of the web pages 
are often noisy or even unrelated to the current top-
ic cluster. Sometimes, the title may be meaningless 
(it might for example contain the website?s name 
?Pr Newswire?), or two different web pages may 
share the same title. We implemented a topic-
related headline generator based on our previous 
work (Lin and Hovy, 2000) by incorporating a top-
ic-based selector. 
Snippet: a paragraph-length excerpt characteriz-
ing the cluster. To produce paragraph-length sum-
maries, we implemented an extraction-based text 
summarizer. We built a new version of previously 
investigated technology (Lin and Hovy, 2001), 
implementing several sentence scoring techniques 
and a score combination function. 
4 Demonstration 
4.1 Geospatial Interaction  
The hierarchical summarization service is built 
upon the geo-spatial information display system, 
GeoXRAY1, a commercial product developed by 
Geosemble Technologies2. Figure 1 shows the sys-
tem?s display to support search and browsing of 
text content based on location of interest.  
Figure 1. Geospatial Information Display System 
                                                          
1GeoXRAY: http://www.geosemble.com/products_geoxray.html
2Geosemble Technologies: http://www.geosemble.com/
The user can enter an address in the top search 
box, or search by business name. The system then 
centers the imagery at that address or business. 
Clicking on ?Get Features? invokes the web ser-
vices to get al features about the displayed image 
and displays the features in the ?AREA: Features 
Found? list, and also draws them as points on the 
maps.  
The user can explore the map using the navigation 
controller. On clicking the marker of an identified 
building, an information window pops up contain-
ing the associated structured web information 
(building name, business type, website, online im-
ages, and so on), as shown in Figure 2. 
Figure 2. Navigating the Integrated Map 
Clicking on ?Get News? retrieves all news related 
to the displayed features; features with associated 
news show a small newspaper icon (see next to 
?Sony Pictures Entertainment? in Figure 4). Click-
ing on the icon displays the news that was linked 
with the feature, sorted by date. 
The hierarchical summarization system, described 
in this paper extends the GeoXRAY system to 
show a summarized view of the news. The user can 
click on the ?Cluster News? link. The results are 
displayed in a tree, showing the title of the cluster 
(thumbnail and title), under which appears a small 
summary of the cluster, under which appear links 
to all the news articles belonging to that cluster. 
4.2 Summarization Example  
We provide an example of our text summariza-
tion system performance in Figure 3. In this exam-
ple, we have selected the location of Sony Film 
Studios in Culver City by clicking on the map. 
Figure 3(a) shows the titles and dates of some of 
7
the 126 news articles that contain the words ?Sony 
Pictures Entertainment?. As described above, these 
documents are clustered based on topics. Using our 
current parameter settings, 20 multi-result clusters 
are formed, leaving 34 results unclustered. (The 
size of clusters, or the number of clusters desired, 
can be varied by the user.) As mentioned above, 
each cluster is presented to the users by a minimal 
length thumbnail summary consisting of a few cha-
racteristic keywords; a partial list of these is shown 
in Figure 3(b). Figure 3(c) shows the result of se-
lecting the cluster labeled ?solar electrical system? 
(second from the bottom in Figure 3(b)), which 
contains two results. The summary contains the 5 
top-ranked sentences from the two documents, pre-
sented in document order. In addition, the sum-
mary includes two hyperlinks to the two full texts 
for further inspection. 
(a) Partial list of the news articles linked to Sony Pictures 
Entertainment 
(b) Clustering results relevant to Sony Pictures Entertainment 
(c) Summarization from the news articles in cluster Solar 
electricity system 
Figure 3. Document clustering and summarization for news 
relevant to Sony Picture Entertainment 
The summary illustrates some of the strengths but 
also the shortcomings of the current system. It is 
clearly about a solar energy system installed in 
2007 on top of the Jimmy Stewart Building by EI 
Solutions. This is enough detail for a user to de-
termine whether or not to read the texts any fur-
ther. However, two of the extracted sentences are 
not satisfactory: sentence 2 is broken off and sen-
tence 3 should not be part of the news text at all. 
Premature sentence breaks result from inadequate 
punctuation and line break processing, which is 
still a research problem exacerbated by the com-
plexity of web pages. 
By showing the summary results, we merely dem-
onstrate the improvement on browsability of the 
search system. We are relatively satisfied with the 
results. While the summaries are not always very 
good, they are uniformly understandable and com-
pletely adequate to prove that one can combine 
geospatial information access and text summariza-
tion in a usable and coherent manner.  
Acknowledgments 
Thanks to Geosemble Technologies for providing 
support of the geospatial information system.   
References  
Yih-Farn Robin Chen, Giuseppe Di Fabbrizio, David 
Gibbon, Serban Jora,  Bernard Renger and Bin Wei. 
Geotracker: Geospatial and temporal rss navigation. 
In WWW ?07: Proceedings of the 16th International 
Conference on World Wide Web, 2007. 
Marti A. Hearst. TexTiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguis-
tics, 23(1):33?64, 1997. 
Chin-Yew Lin and Eduard Hovy. The automated acqui-
sition of topic signatures for text summarization. In
Proceedings of the 18th Conference on Computation-
al Linguistics, 2000. 
Chin-Yew Lin and Eduard Hovy. From single to multi-
document summarization: A prototype system and its 
evaluation. In ACL ?02: Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, 2001. 
Stanislaw Osinski and Dawid Weiss. Carrot2: Design of 
a flexible and efficient web information retrieval 
framework. In AWIC, 2005. 
Benjamin E. Teitler, Michael D. Lieberman, Daniele 
Panozzo, Jagan Sankaranarayanan, Hanan Samet and 
Jon Sperling. Newsstand: a new view on news. In
GIS ?08: Proceedings of the 16th ACM SIGSPATIAL 
international conference on Advances in geographic 
information systems, 2008. 
8
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 646?655,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Structured Event Retrieval over Microblog Archives
Donald Metzler, Congxing Cai, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
Abstract
Microblog streams often contain a consider-
able amount of information about local, re-
gional, national, and global events. Most ex-
isting microblog search capabilities are fo-
cused on recent happenings and do not provide
the ability to search and explore past events.
This paper proposes the problem of structured
retrieval of historical event information over
microblog archives. Rather than retrieving in-
dividual microblog messages in response to an
event query, we propose retrieving a ranked
list of historical event summaries by distill-
ing high quality event representations using
a novel temporal query expansion technique.
The results of an exploratory study carried
out over a large archive of Twitter messages
demonstrates both the value of the microblog
event retrieval task and the effectiveness of our
proposed search methodologies.
1 Introduction
Real-time user generated content is one of the key
driving forces behind the growing popularity of so-
cial media-centric communication. The ability to in-
stantly share, often from your mobile phone, your
thoughts (via Twitter), your photos (via Facebook),
your location (via Foursquare), and a variety of other
information is changing the way that information is
created, communicated, and consumed.
There has been a substantial amount of research
effort devoted to user generated content-related
search tasks, including blog search, forum search,
and community-based question answering. How-
ever, there has been relatively little research on mi-
croblog search. Microblog services, such as Tumblr
and Twitter, provide users with the ability to broad-
cast short messages in real-time. This is in contrast
to traditional blogs that typically have considerably
more content that is updated less frequently. By
their very nature, microblog streams often contain
a considerable amount of information about local,
regional, national, and global news and events. A
recent study found that over 85% of trending topics
on Twitter are news-related (Kwak et al, 2010). An-
other recent study by Teevan et al that investigated
the differences between microblog and Web search
reported similar findings (Teevan et al, 2011). The
study also found that microblog search queries are
used to find information related to news and events,
while Web search queries are more navigational in
nature and used to find a variety of information on a
specific topic.
It is likely that microblogs have not received much
attention because, unlike blog search, there is no
well-defined microblog search task. Existing mi-
croblog search services, such as those offered by
Twitter and Google, only provide the ability to re-
trieve individual microblog posts in response to a
query. Unfortunately, this task has limited utility
since very few real information needs can be satis-
fied by a single short piece of text (e.g., the max-
imum length of a message on Twitter is 140 char-
acters). Hence, novel search tasks defined over mi-
croblog streams that go beyond ?message retrieval?
have the potential to add substantial value to users.
Given the somewhat limited utility of microblog
message search and the preponderance of news and
event-related material posted on microblogs, this pa-
646
July 16 2010 at 17 UTC, for 11 hours
Summary tweets:
i. Ok a 3.6 ?rocks? nothing. But boarding a plane
there now, Woodward ho! RT @todayshow: 3.6 mag-
nitude #earthquake rocks Washington DC area.
ii. RT @fredthompson: 3.6-magnitude earthquake hit
DC. President Obama said it was due to 8 years of
Bush failing to regulate plate tectonic ...
iii. 3.6-magnitude earthquake wakes Md. residents:
Temblor centered in Gaithersburg felt by as many as
3 million people... http://bit.ly/9iMLEk
Figure 1: Example structured event representation re-
trieved for the query ?earthquake?.
per proposes a novel search task that we call mi-
croblog event retrieval. Given a query that describes
an event, such as earthquake, terrorist bombing, or
bieber concert, the goal of the task is to retrieve a
ranked list of structured event representations, such
as the one shown in Figure 1, from a large archive of
historical microblog posts.
In this work, structured representations come in
the form of a list of timespans during which an in-
stance of the event occurred and was actively dis-
cussed within the microblog stream. Additionally,
for each timespan, a small set of relevant messages
are retrieved for the purpose of providing a high-
level summary of the event that occurred during the
timespan. This task leverages the large amount of
real-time, often first-hand information found in mi-
croblog archives to deliver a novel form of user gen-
erated content-based search results to users. Unlike
news search, which finds professionally written ar-
ticles on a news-related topic, and general-purpose
Web search, which is likely to find a large amount
of unrelated information, this task is designed to re-
trieve highly relevant news and event-related infor-
mation viewed through the lens of users who ex-
perienced or discussed the event while it happened
(or during its aftermath). Such search functional-
ity would not only be useful for everyday end-users,
but also social scientists, historians, journalists, and
emergency planners.
This paper has three primary contributions. First,
we introduce the microblog event retrieval task,
which retrieves a ranked list of structured event rep-
resentations in response to an event query. By going
beyond individual microblog message retrieval, the
task adds value to microblog archives and provides
users with the ability to find information that was
disseminated in real-time about past events, which
is not possible with news and Web search engines.
Second, we propose an unsupervised methodology
for distilling high quality event representations using
a novel temporal query expansion technique. The
technique synthesizes ideas from pseudo-relevance
feedback, term burstiness, and temporal aspects
of microblog streams. Third, we perform an ex-
ploratory evaluation of 50 event queries over a cor-
pus of 46 million Twitter messages. The results of
our evaluation demonstrate both the value of the mi-
croblog event retrieval task itself and the effective-
ness of our proposed search methodologies, which
show improvements of up to 42% compared to a
baseline approach.
2 Related Work
There are several directions of microblog research
that are related to our proposed work. First, there is
a growing body of literature that has focused on the
topical content of microblog posts. This research
has focused on microblog topic models (Hong and
Davison, 2010), event and topic detection and track-
ing (Sankaranarayanan et al, 2009; Cataldi et al,
2010; Petrovic? et al, 2010; Lin et al, 2010), predict-
ing flu outbreaks using keyword tracking (Culotta,
2010), and using microblog streams as a source
of features for improving recency ranking in Web
search (Dong et al, 2010). Most of these approaches
analyze content as it arrives in the system. While
tracking a small number of topics or keywords is fea-
sible using online algorithms, the general problem
of topic detection and tracking (Allan et al, 1998) is
considerably more challenging given the large num-
ber of topics being discussed at any one point. Our
work differs in that it does not attempt to track or
model topics as they arrive in the system. Instead,
given an event query, our system retrospectively an-
alyzes the corpus of microblog messages for the pur-
pose of retrieving structured event representations.
There is no shortage of previous work on using
pseudo-relevance feedback approaches for query ex-
pansion. Relevant research includes classical vector-
space approaches (Rocchio, 1971), language mod-
647
eling approaches (Lavrenko and Croft, 2001; Zhai
and Lafferty, 2001; Li and Croft, 2003), among oth-
ers (Metzler and Croft, 2007; Cao et al, 2008; Lv
and Zhai, 2010). The novel aspect of our proposed
temporal query expansion approach is the fact that
expansion is done over a temporal stream of very
short, noisy messages.
There has also been recent work on summarizing
sets of microblog posts (Sharifi et al, 2010). We
chose to make use of a simple approach in favor of
a more sophisticated one because summarization is
only a minor aspect of our proposed framework.
Finally, there are two previous studies that are
the most relevant to our work. First, Massoudi et
al. propose a retrieval model that uses query ex-
pansion and microblog quality indicators to retrieve
individual microblog messages (Massoudi et al,
2011). Their proposed query expansion approach
differs from ours in the sense that we utilize times-
pans from the (possibly distant) past when generat-
ing expanded queries and focus on event retrieval,
rather than individual message retrieval. The other
research that is closely related to ours is the work
done by Chieu and Lee (Chieu and Lee, 2004). The
authors propose an approach for automatically con-
structing timelines from news articles in response
to a query. The novelty of our proposed work de-
rives from our novel temporal query expansion ap-
proach, and the fact that our work focuses on mi-
croblog streams which are fundamentally different
in nature from news articles.
3 Microblog Event Retrieval
The primary goal of this paper is to introduce a new
microblog search paradigm that goes beyond retriev-
ing messages individually. We propose a novel task
called microblog event retrieval, which is defined as
follows. Given a query that specifies an event, re-
trieve a set of relevant structured event representa-
tions from a large archive of microblog messages.
This definition is purposefully general to allow for a
broad interpretation of the task.
There is nothing in our proposed retrieval frame-
work that precludes it from producing reasonable re-
sults for any type of query, not just those related to
events. However, we chose to primarily focus on
events in this paper because previous studies have
shown that a majority of trending topics within mi-
croblog streams are about news and events (Kwak et
al., 2010). The information found in microblogs is
difficult to find anywhere else, including news and
Web archives, thereby making it a valuable resource
for a wide variety of users.
3.1 Overview of Framework
Our microblog event retrieval framework takes a
query as input and returns a ranked list of struc-
tured event representations. To accomplish this, the
framework breaks the work into two steps ? times-
pan retrieval and summarization. The timespan re-
trieval step identifies the timespans when the event
happened, while the summarization step retrieves
a small set of microblog messages for each times-
pan that are meant to act as a summary. Figure 1
shows an example result that is returned in response
to the query ?earthquake?. The result consists of a
start time that indicates when the event began be-
ing discussed, a duration that specifies how long the
event was discussed, and a small number of mes-
sages posted during the time interval that are meant
to summarize what happened. This example corre-
sponds to an earthquake that struck the metropoli-
tan District of Colombia area in the United States.
The earthquake was heavily discussed for nearly 11
hours, because it hit a densely populated area that
does not typically experience earthquakes.
3.2 Temporal Query Expansion
We assume that queries issued to our retrieval frame-
work are simple keyword queries that consist of a
small number of terms. This sparse representation
of the user?s information need makes finding rel-
evant messages challenging, since microblog mes-
sages that are highly related to the query might not
contain any of the query keywords. It is common for
microblog messages about a given topic to express
the topic in a different, possibly shortened or slang,
manner. For example, rather than writing ?earth-
quake?, users may instead use the word ?quake? or
simply include a hashtag such as ?#eq? in their mes-
sage. It is impractical to manually identify the full
set of related keywords and folksonomy tags (i.e.,
hashtags) for each query. In information retrieval,
this is known as the vocabulary mismatch problem.
To address this problem, we propose a novel unsu-
648
pervised temporal query expansion technique. The
approach is unsupervised in the sense that it makes
use of a pseudo-relevance feedback-like mechanism
when extracting expansion terms. Traditional query
expansion approaches typically find terms that com-
monly co-occur with the query terms in documents
(or passages). However, such approaches are not
suitable for expanding queries in the microblog set-
ting since microblog messages are very short, yield-
ing unreliable co-occurrence information. Further-
more, microblog messages have an important tem-
poral dimension that should be considered when
they are being used to generate expansion terms.
Our proposed approach generates expansion
terms based on the temporal co-occurrence of terms.
Given keyword query q, we first automatically re-
trieve a set of N timespans for which the query key-
words were most heavily discussed. To do so, we
rank timespans according to the proportion of mes-
sages posted during the timespan that contain one
or more of the query keywords. This is a simple,
but highly reliable way of identifying timespans dur-
ing which a specific topic is being heavily discussed.
These timespans are then considered to be pseudo-
relevant. In our experiments, the microblog stream
is divided into hours, with each hour corresponding
to an atomic timespan. Although it is possible to
define timespans in many different ways, we found
that this was a suitable level of granularity for most
events that was neither overly broad nor overly spe-
cific.
For each pseudo-relevant timespan, a burstiness
score is computed for all of the terms that occur in
messages posted during the timespan. The bursti-
ness score is meant to quantify how trending a term
is during the timespan. Thus, if the query is be-
ing heavily discussed during the timespan and some
term is also trending during the timespan, then the
term may be related to the query. For each of the top
N time intervals, the burstiness score of each term
is computed as follows:
burstiness(w, TSi) =
P (w|TSi)
P (w)
(1)
which is the ratio of the term?s likelihood of occur-
ring within timespan TSi versus the likelihood of
the term occurring during any timespan. Hence, if
a term that generally infrequently occurs within the
message stream suddenly occurs many times within
a single time interval, then the term will be assigned
a high burstiness score. This weighting is similar in
nature to that proposed by Ponte for query expansion
within the language modeling framework for infor-
mation retrieval (Ponte, 1998). The following prob-
ability estimates are used for the expressions within
the burstiness score:
P (w|TSi) =
tfw,TSi + ?
tfw
N
|TSi|+ ?
, P (w) =
tfw +K
N +K|V |
where tfw,TSi is the number of occurrences of w in
timespan TSi, tfw is the number of occurrences of
w in the entire microblog archive, |TSi| is the num-
ber of terms in timespan TSi, N is the total number
of terms in the microblog archive, V is the vocabu-
lary size, and ? and K are smoothing parameters.
While it is common practice to smooth P (w|TSi)
using Dirichlet (or Bayesian) smoothing (Zhai and
Lafferty, 2004), it is less common to smooth the gen-
eral English language model P (w). However, we
found that this was necessary since term distribu-
tions in microblog services exhibit unique character-
istics. By smoothing P (w), we dampen the effect of
overweighting very rare terms. In our experiments,
we set the value of ? to 500 and K to 10 after some
preliminary exploration. We found that the overall
system effectiveness is generally insensitive to the
choice of smoothing parameters.
The final step of the query expansion process
involves aggregating the burstiness scores across
all pseudo-relevant timespans to generate an over-
all score for each term. To do so, we compute
the geometric mean of the burstiness scores across
the pseudo-relevant timespans. Preliminary experi-
ments showed that the arithmetic mean was suscep-
tible to overweighting terms that had a very large
burstiness score in a single timespan. By utiliz-
ing the geometric average instead, we ensure that
the highest weighted terms are those that have large
weights in a large number of the timespans, thereby
eliminating spurious terms. Seo and Croft (2010)
observed similar results with traditional pseudo-
relevance feedback techniques.
The k highest weighted terms are then used as
expansion terms. Using this approach, terms that
commonly trend during the same timespans that
649
the query terms commonly occur (i.e., the pseudo-
relevant timespans) are assigned high weights.
Hence, the approach is capable of capturing sim-
ple temporal dependencies between terms and query
keywords, which is not possible with traditional ap-
proaches.
3.3 Timespan Ranking
The end result of the query expansion process just
described is an expanded query q? that consists of a
set of k terms and their respective weights (denoted
as ?w). Our framework uses the expanded query q?
to retrieve relevant timespans. We hypothesize that
using the expanded version of the query for timespan
retrieval will yield significantly better results than
using the keyword version.
To retrieve timespans, we first identify the 1000
highest scoring timespans (with respect to q?). We
then merge contiguous timespans into a single,
longer timespan, where the score of the merged
timespan is the maximum score of its component
timespans. The final ranked list consists of the
merged timespans. Therefore, although our times-
pans are defined as hour intervals, it is possible for
our system to return longer (merged) timespans.
We now describe two scoring functions that can
be used to compute the relevance of a timespan with
respect to an expanded query representation.
3.3.1 Coverage Scoring Function
The coverage scoring function measures rele-
vance as the (weighted) number of expansion terms
that are covered within the timespan. This measure
assumes that the expanded query is a faithful repre-
sentation of the information need and that the more
times the highly weighted expansion terms occur,
the more relevant the timespan is. Using this defi-
nition, the coverage score of a time interval is com-
puted as:
s(q?, TS) =
?
w?q?
?w ? tfw,TS
where tfwi,TS is the term frequency of wi in times-
pan TS and ?w is the expansion weight of term w.
3.3.2 Burstiness Scoring Function
Since multiple events may occur at the same time,
microblog streams can easily be dominated by the
larger of two events. However, less popular events
may also exhibit burstiness at the same time. There-
fore, another measure of relevance is the burstiness
of the event signature during the timespan. If all
of the expansion terms exhibit burstiness during the
time interval, it strongly suggests the timespan may
be relevant to the query.
Therefore, to measure the relevance of the times-
pan, we first compute the burstiness scores for all of
the terms within the time interval. This yields a vec-
tor ?TS of burstiness scores. The cosine similarity
measure is used to compute the similarity between
the query burstiness scores and the timespan bursti-
ness scores. Hence, the burstiness scoring function
is computed as:
s(q?, TS) = cos(?q? , ?TS)
3.4 Timespan Summarization
The final step of the retrieval process is to produce
a short query-biased summary for each retrieved
time interval. The primary purpose for generating
this type of summary is to provide the user with
a quick overview of what happened during the re-
trieved timespans.
We utilize a simple, straightforward approach that
generates unexpectedly useful summaries. Given a
timespan, we use a relatively simple information re-
trieval model to retrieve a small set of microblog
messages posted during the timespan that are the
most relevant to the expanded representation of the
original query. These messages are then used as a
short summary of the timespan.
This is accomplished by scoring a microblog mes-
sageM with respect to an expanded query represen-
tation q? using a weighted variant of the query like-
lihood scoring function (Ponte and Croft, 1998):
s(q?,M) =
?
w?q?
?w ? logP (w|M)
where ?w is the burstiness score for expansion term
w and P (w|M) is a Dirichlet smoothed language
modeling estimate for term w in message M . This
scoring function is also equivalent to the cross en-
tropy and KL-divergence scoring functions (Lafferty
and Zhai, 2001).
650
Category Events
Business layoffs, bankruptcy, acquisition,
merger, hostile takeover
Celebrity wedding, divorce
Crime shooting, robbery, assassination,
court decision, school shooting
Death death, suicide, drowned
Energy blackout, brownout
Entertainment awards, championship game,
world record
Health recall, pandemic, disease, flu,
poisoning
Natural Disaster hurricane, tornado, earthquake,
flood, tsunami, wildfire, fire
Politics election, riots, protests
Terrorism hostage, explosion, terrorism,
bombing, terrorist attack, suicide
bombing, hijacked
Transportation plane crash, traffic jam, sinks,
pileup, road rage, train crash, de-
railed, capsizes
Table 1: The 50 event types used as queries during our
evaluation, divided into categories.
4 Experiments
This section describes our empirical evaluation of
the proposed microblog event retrieval task.
4.1 Microblog Corpus
Our microblog message archive consists of data
that we collected from Twitter using their Stream-
ing API. The API delivers a continuous 1% ran-
dom sample of public Twitter messages (also called
?tweets?). Our evaluation makes use of data col-
lected between July 16, 2010 and Jan 1st, 2011. Af-
ter eliminating all non-English tweets, our corpus
consists of 46,611,766 English tweets, which corre-
sponds to roughly 10,000 tweets per hour. Although
this only represents a 1% sample of all tweets, we
believe that the corpus is sizable enough to demon-
strate the utility of our proposed approach.
4.2 Event Queries
To evaluate our system, we prepared a list of 50
event types that fall into 11 different categories.
The event types and their corresponding categories
are listed in Table 1. The different event types
can have substantially different characteristics, such
as the frequency of occurrence, geographic or de-
mographic interest, popularity, etc. For example,
there are more weddings than earthquakes. Pub-
lic events, such as federal elections involve people
across the country. However, a car pileup typically
only attracts local attention. Moreover, microblog-
gers show different amounts of interest to each type
of event. For example, Twitter users are more likely
to tweet about politics than a business acquisition.
4.3 Methodology
To evaluate the quality of a particular configuration
of our framework, we run the microblog event re-
trieval task for the 50 different event type queries de-
scribed in the previous section. For each query, the
top 10 timespans retrieved are manually judged to
be relevant or non-relevant. If the summary returned
clearly indicated a real event instance occurred, then
the timespan was marked as relevant. The primary
metric of interest is precision at 10.
In addition to the temporal query expansion
approach (denoted TQE), we also ran exper-
iments using relevance-based language models,
which is a state-of-the-art query expansion ap-
proach (Lavrenko and Croft, 2001). We ran two
variants of relevance-based language models. In
the first, query expansion was done using the Twit-
ter corpus itself (denoted TwitterRM). This allows
us to compare the effectiveness of the TQE ap-
proach against a more traditional query expansion
approach. In the other variant, query expansion was
done using the English Gigaword corpus (denoted
NewsRM), which is a rich source of event informa-
tion created by traditional news media.
For all three query expansion approaches (TQE,
TwitterRM, and NewsRM), the two scoring func-
tions, burstiness and coverage, are used to rank
timespans. Hence, we evaluate six specific instances
of our framework. As a baseline, we use a sim-
ple (unexpanded) keyword retrieval approach that
scores timespans according to the relative frequency
of event keywords that occur during the timespan.
4.4 Timespan Retrieval Results
Before delving into the details of our quantitative
evaluation of effectiveness, we provide an illustra-
tive example of the type of results our system is ca-
pable of producing. Table 2 shows the top four re-
651
July 16 2010 at 17 UTC, for 11 hours
Ok a 3.6 ?rocks? nothing. But boarding a plane there
now, Woodward ho! RT @todayshow: 3.6 magnitude
#earthquake rocks Washington DC area.
September 28 2010 at 11 UTC, for 6 hours
RT @Quakeprediction: 2.6 earthquake
(possible foreshock) hits E of Los Ange-
les; http://earthquake.usgs.gov/
earthquakes/recenteqscanv/Fau ...
September 04 2010 at 01 UTC, for 3 hours
7.0 quake strikes New Zealand - A 7.0-magnitude
earthquake has struck near New Zealand?s second
largest city. Reside... http://ht.ly/18R2rw
October 27 2010 at 01 UTC, for 5 hours
RT @SURFER Magazine: Tsunami Strikes
Mentawais: Wave Spawned By A 7.5-Magnitude
Earthquake Off West Coast Of Indonesia
http://bit.ly/8Z9Lbv
Table 2: Top four timespans (with a single summary
tweet) retrieved for the query ?earthquake?.
sults retrieved using temporal query expansion with
the burstiness scoring function for the query ?earth-
quake?. Only a single summary tweet is displayed
for each timespan due to space restrictions. As we
can see from the tweets, all of the results are rele-
vant to the query, in that they all correspond to times
when an earthquake happened and was actively dis-
cussed on Twitter. Different from Web and news
search results, these types of ranked lists provide a
clear temporal picture of relevant events that were
actively discussed on Twitter.
The results of our microblog retrieval task are
shown in Table 3. The table reports the per-category
and overall precision at 10 for the baseline, and
the six configurations of our proposed framework.
Bolded values represent the best result per category.
As the results show, using temporal query expan-
sion with burstiness ranking yields a mean preci-
sion at 10 of 61%, making it the best overall sys-
tem configuration. The approach is 41.9% better
than the baseline, which is statistically significant
according to a one-sided paired t-test at the p < 0.01
level. Interestingly, the relevance model-based ex-
pansion techniques exhibit even worse performance,
on average, than our simple keyword baseline. For
example, the news-based expansion approach was
11.6% worse using the coverage scoring function
and 18.6% worse using the burstiness scoring func-
tion compared to the baseline. All of the traditional
query expansion results are statistically significantly
worse than the temporal query expansion-based ap-
proaches. Hence, the results suggest that capturing
temporal dependencies between terms yields bet-
ter expanded representations than simply capturing
term co-occurrences, as is done in traditional query
expansion approaches.
The results also indicate the burstiness scoring
function outperforms the coverage scoring function
for temporal query expansion. An analysis of the
results revealed that in many cases the timespans
returned using the coverage scoring function had a
small number of frequent terms that matched the ex-
panded query. This happened less often with the
burstiness scoring function, which is based on the
cosine similarity between the query and timespan?s
burstiness scores. The combination of burstiness
weighting and l2 normalization (when computing
the cosine similarity) appears to yield a more robust
scoring function.
4.5 Event Popularity Effects
It is also interesting to note that the retrieval perfor-
mance varies substantially across the different event
type categories. For example, the performance on
queries about ?natural disasters? and ?politics? is
consistently strong. Similar performance can also
be achieved for popular events related to celebri-
ties. However, energy-related event queries, such as
?blackout?, achieves very poor effectiveness. This
observation seems to suggest that the more popu-
lar an event is, the better the retrieval performance
that can be achieved. This is a reasonable hypothe-
sis since the more people tweet about the event, the
easier it is to identify the trend from the background.
To better understand this phenomenon, we com-
pute the correlation between timespan retrieval pre-
cision and event (query) popularity, where popular-
ity is measured according to:
Popularity(q) =
1
N
N?
i=1
burstiness(q, TSi),
where q is the event query, burstiness(q, TSi) is
the burstiness score of the event during timespan
652
Event Category Baseline
NewsRM TwitterRM TQE
burst cover burst cover burst cover
Business 0.50 0.46 0.30 0.70 0.18 0.74 0.64
Celebrity 0.75 0.30 0.40 0.50 0.60 0.80 0.45
Crime 0.44 0.28 0.54 0.22 0.32 0.46 0.28
Death 0.43 0.20 0.33 0.30 0.30 0.47 0.47
Energy 0.05 0.10 0.05 0.20 0.05 0.15 0.00
Entertainment 0.47 0.53 0.67 0.30 0.53 0.70 0.70
Health 0.48 0.28 0.36 0.44 0.16 0.60 0.60
Nat. Disaster 0.50 0.53 0.59 0.66 0.46 0.87 0.66
Politics 0.67 0.70 0.53 0.63 0.30 0.87 0.60
Terrorism 0.41 0.44 0.39 0.39 0.17 0.69 0.51
Transportation 0.21 0.08 0.08 0.08 0.10 0.31 0.19
All 0.43 0.35 0.38 0.40 0.26 0.61 0.47
Table 3: Per-category and overall (All) precision at 10 for the keyword only approach (Baseline), traditional newswire
expansion (NewsRM), traditional pseudo relevance feedback using the Twitter corpus (TwitterRM), and tempo-
ral query expansion (TQE). For the expansion-based approaches, results for the burstiness scoring (burst) and the
coverage-based scoring (cover) are given. Bold values indicate the best result per category.
Correlation
Baseline 0.63 (p < 0.01)
NewsRM 0.53 (p < 0.01)
TwitterRM 0.61 (p < 0.01)
TQE 0.50 (p < 0.01)
Table 4: Spearman rank correlation between event re-
trieval precisions and event popularity. All methods use
the burstiness scoring function.
TSi, as defined in Equation 1, and the sum goes over
the topN timespans retrieved for the event using our
proposed retrieval approach.
Using this measure, we find that Twitter users are
more interested in events related to entertainment
and politics, and less interested in events related to
energy or transportation. Also, we notice that Twit-
ter users actively discuss dramatic crisis-related top-
ics, including natural disasters (e.g., earthquakes,
hurricanes, tornado, etc.) and terrorist attacks.
Table 4 shows the correlations between effec-
tiveness and event popularity across different ap-
proaches. The correlations indicate a strong cor-
relation with event popularity for the keyword ap-
proach. This is expected, since the approach is based
on the number of times the keywords are mentioned
within the timespan. The correlations are signif-
icantly reduced by incorporating query expansion
terms. The configurations that use temporal query
expansion tend to have lower correlation than the
other approaches. Although the correlation is still
significant, the lower correlation suggests that tem-
poral query expansion approaches are more robust to
popularity effects than simple keywords approaches.
Additional work is necessary to better understand
the role of popularity in retrieval tasks like this.
5 Conclusions
In this paper, we proposed a novel microblog search
task called microblog event retrieval. Unlike previ-
ous microblog search tasks that retrieve individual
microblog messages, our task involves the retrieval
of structured event representations during which an
event occurs and is discussed within the microblog
community. In this way, users are presented with a
ranked list or timeline of event instances in response
to a query.
To tackle the microblog search task, we proposed
a novel timespan retrieval framework that first con-
structs an expanded representation of the incoming
query, performs timespan retrieval, and then pro-
duces a short summary of the timespan. Our experi-
mental evaluation, carried out over a corpus of over
46 million microblog messages collected from Twit-
ter, showed that microblog event retrieval is a feasi-
ble, challenging task, and that our proposed times-
pan retrieval framework is both robust and effective.
653
References
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic De-
tection and Tracking Pilot Study. In In Proceedings of
the DARPA Broadcast News Transcription and Under-
standing Workshop, pages 194?218.
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen
Robertson. 2008. Selecting good expansion terms for
pseudo-relevance feedback. In Proc. 31st Ann. Intl.
ACM SIGIR Conf. on Research and Development in In-
formation Retrieval, SIGIR ?08, pages 243?250, New
York, NY, USA. ACM.
Mario Cataldi, Luigi Di Caro, and Claudio Schifanella.
2010. Emerging topic detection on twitter based on
temporal and social terms evaluation. In Proceedings
of the Tenth International Workshop on Multimedia
Data Mining, MDMKDD ?10, pages 4:1?4:10, New
York, NY, USA. ACM.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?04, pages 425?432, New York, NY,
USA. ACM.
Aron Culotta. 2010. Towards detecting influenza epi-
demics by analyzing twitter messages. In 1st Work-
shop on Social Media Analytics (SOMA?10), July.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: im-
proving recency ranking using twitter data. In Pro-
ceedings of the 19th international conference on World
wide web, WWW ?10, pages 331?340, New York, NY,
USA. ACM.
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in twitter. In 1st Workshop on
Social Media Analytics (SOMA?10), July.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is twitter, a social network or
a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ?10,
pages 591?600, New York, NY, USA. ACM.
J. Lafferty and C. Zhai. 2001. Document language mod-
els, query models, and risk minimization for informa-
tion retrieval. In Proc. 24th Ann. Intl. ACM SIGIR
Conf. on Research and Development in Information
Retrieval, pages 111?119.
V. Lavrenko and W. B. Croft. 2001. Relevance-based
language models. In Proc. 24th Ann. Intl. ACM SI-
GIR Conf. on Research and Development in Informa-
tion Retrieval, pages 120?127.
Xiaoyan Li and W. Bruce Croft. 2003. Time-based lan-
guage models. In Proc. 12th Intl. Conf. on Information
and Knowledge Management, CIKM ?03, pages 469?
475, New York, NY, USA. ACM.
Cindy Xide Lin, Bo Zhao, Qiaozhu Mei, and Jiawei Han.
2010. Pet: a statistical model for popular events track-
ing in social communities. In Proc. 16th Ann. Intl.
ACM SIGKDD Conf. on Knowledge Discovery and
Data Mining, KDD ?10, pages 929?938, New York,
NY, USA. ACM.
Yuanhua Lv and ChengXiang Zhai. 2010. Positional rel-
evance model for pseudo-relevance feedback. In Proc.
33rd Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, SIGIR ?10, pages
579?586, New York, NY, USA. ACM.
Kamran Massoudi, Manos Tsagkias, Maarten de Rijke,
and Wouter Weerkamp. 2011. Incorporating query ex-
pansion and quality indicators in searching microblog
posts. In Proc. 33rd European Conf. on Information
Retrieval, page To appear.
Donald Metzler and W. Bruce Croft. 2007. Latent con-
cept expansion using markov random fields. In Proc.
30th Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, SIGIR ?07, pages
311?318, New York, NY, USA. ACM.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to twitter. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 181?189, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
J. Ponte and W. Bruce Croft. 1998. A language modeling
approach to information retrieval. In Proc. 21st Ann.
Intl. ACM SIGIR Conf. on Research and Development
in Information Retrieval, pages 275?281.
Jay Ponte. 1998. A Language Modeling Approach to In-
formation Retrieval. Ph.D. thesis, University of Mas-
sachusetts, Amherst, MA.
J. J. Rocchio, 1971. Relevance Feedback in Information
Retrieval, pages 313?323. Prentice-Hall.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings of
the 17th ACM SIGSPATIAL International Conference
on Advances in Geographic Information Systems, GIS
?09, pages 42?51, New York, NY, USA. ACM.
Jangwon Seo and W. Bruce Croft. 2010. Geometric rep-
resentations for multiple documents. In Proceeding of
the 33rd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?10, pages 251?258, New York, NY, USA. ACM.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010. Experiments in microblog summariza-
tion. Social Computing / IEEE International Confer-
654
ence on Privacy, Security, Risk and Trust, 2010 IEEE
International Conference on, 0:49?56.
Jaime Teevan, Daniel Ramage, and Meredith Ringel Mor-
ris. 2011. #twittersearch: A comparison of microblog
search and web search. In WSDM 2011: Fourth Inter-
national Conference on Web Search and Data Mining,
Feb.
ChengXiang Zhai and John Lafferty. 2001. Model-based
feedback in the language modeling approach to infor-
mation retrieval. In Proc. 10th Intl. Conf. on Informa-
tion and Knowledge Management, pages 403?410.
C. Zhai and J. Lafferty. 2004. A study of smoothing
methods for language models applied to information
retrieval. ACM Trans. Inf. Syst., 22(2):179?214.
655
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 19?27,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Learning Phenotype Mapping for Integrating Large Genetic Data
Chun-Nan Hsu1,2,?, Cheng-Ju Kuo2, Congxing Cai1
Sarah A. Pendergrass3, Marylyn D. Ritchie3,4 and Jose Luis Ambite1
1USC Information Sciences Institute, Marina del Rey, CA, USA
2Institute of Information Sciences, Academia Sinica, Taipei, Taiwan
3Center for Human Genetics Research, 4Dept. of Molecular Physiology and
Biophysics, Vanderbilt University, Nashville, TN, USA
?chunnan@isi.edu
Abstract
Accurate phenotype mapping will play an im-
portant role in facilitating Phenome-Wide As-
sociation Studies (PheWAS), and potentially
in other phenomics based studies. The Phe-
WAS approach investigates the association be-
tween genetic variation and an extensive range
of phenotypes in a high-throughput manner to
better understand the impact of genetic varia-
tions on multiple phenotypes. Herein we de-
fine the phenotype mapping problem posed
by PheWAS analyses, discuss the challenges,
and present a machine-learning solution. Our
key ideas include the use of weighted Jaccard
features and term augmentation by dictionary
lookup. When compared to string similarity
metric-based features, our approach improves
the F-score from 0.59 to 0.73. With augmenta-
tion we show further improvement in F-score
to 0.89. For terms not covered by the dictio-
nary, we use transitive closure inference and
reach an F-score of 0.91, close to a level suffi-
cient for practical use. We also show that our
model generalizes well to phenotypes not used
in our training dataset.
1 Introduction
There is a wealth of biomedical data available in
public and private repositories (e.g. the database
issue of Nucleic Acids Research (?).) Along with
this explosion of information comes the need to inte-
grate data from multiple sources to achieve sufficient
statistical power for analyses and/or to characterize
phenomena more precisely. This trend manifests it-
self in two primary ways: the formation of large
multi-institution multi-study consortia and public
repositories. Although this situation occurs across
many areas of biomedicine and our techniques are
general, in this paper we will illustrate the ideas with
examples from genetic studies in which we are par-
ticipating.
Consider the National Center for Biotechnol-
ogy Information (NCBI) database of Genotypes
and Phenotypes (dbGaP) (www.ncbi.nlm.nih.
gov/gap), that was developed to archive and dis-
tribute the results of studies that have investigated
the interaction of genotype and phenotype. This is a
large repository that includes genome-wide associa-
tion studies (GWAS), medical sequencing, molecu-
lar diagnostic assays, as well as association between
genotype and non-clinical traits. Genetic studies
funded by the National Institutes of Health (NIH)
over a certain size are required to submit the ge-
netic and phenotypic data to dbGaP. There are over
130 top-level studies, 1900 datasets, 5600 analyses,
comprising about 125000 phenotypic variables. Un-
fortunately, each study uses its own set of variables,
thus far dbGaP does not attempt to reconcile, match
or harmonize any of these variables. For example,
a variable called ?BMI? in one study and ?Body
Mass Index? in another study are recorded as
different variables. The task of matching or harmo-
nizing these variables falls on each researcher that
obtains dbGaP data from multiple studies.
Similarly, consider a large consortium, such
as the Population Architecture Using Genomics
and Epidemiology (PAGE) network. PAGE
(www.pagestudy.org) is a consortium of four
major studies with the goal of understanding the
19
association of genetic variants with complex dis-
eases and traits across a variety of populations. The
studies that comprise PAGE include: the Women?s
Health Initiative (WHI, www.whiscience.
org/); the Multiethnic Cohort (MEC,
www.crch.org/multiethniccohort/,
www.uscnorris.com/mecgenetics/); the
CALiCo Consortium, comprised in turn of the
Atherosclerosis Risk In Communities (ARIC) study
(www.cscc.unc.edu/aric/), the Coronary
Artery Risk In Young Adults (CARDIA) study
(www.cardia.dopm.uab.edu), the Cardio-
vascular Heart Study (www.chs-nhlbi.org/),
the Hispanic Community Health Study
(www.cscc.unc.edu/hchs/), the Strong
Heart Cohort Study, and the Strong Heart Family
Study (strongheart.ouhsc.edu/); and the
Epidemiologic Architecture of Genes Linked to
Environment (chgr.mc.vanderbilt.edu/
eagle/) study, which utilizes genotypic and phe-
notypic data from the National Health and Nutrition
Examination Surveys (NHANES) from the Centers
for Disease Control and Prevention (CDC). The
studies of PAGE represent a pool of over 200,000
individuals with genotypic data collected across
multiple race/ethnicities, and an extremely diverse
collection of phenotypic data. Within PAGE there
are numerous analyses and writing groups that
focus on specific diseases. Each group selects
variables relevant to their disease and harmonizes
the variables across studies.
A group within PAGE is investigating a novel
approach to genetic association analysis called a
Phenome Wide Association Studies (PheWAS) (?).
This is a different approach compared to the cur-
rent paradigm of Genome Wide Association Stud-
ies (GWAS) (?; ?). GWAS focus on calculating
the association between the variation of hundreds
of thousands of genotyped single nucleotide poly-
morphisms (SNPs) and a single or small number
of phenotypes. This approach has provided valu-
able information about the contribution of genetic
variation to a wide range of diseases and pheno-
types. A common limitation of GWAS is the in-
vestigation of a limited phenotypic domain. In con-
trast, PheWAS utilizes an extensive range of de-
tailed phenotypic measurements including interme-
diary biomarkers, in addition to prevalent and in-
cident status for multiple common clinical condi-
tions, risk factors, and quantitative traits for compre-
hensively exploring the association between genetic
variations and all PheWAS phenotypes. The inves-
tigation of a broad range of phenotypes has the po-
tential to identify pleiotropy, novel mechanistic in-
sights fostering hypothesis generation, and to define
a more complete picture of genetic variations and
their impact on human diseases.
In order to compare PheWAS results across stud-
ies within PAGE to seek replication for significant
genotype/phenotype associations, an important step
is matching and mapping phenotypes across stud-
ies. As the number and range of phenotypes is
large across studies, manually matching phenotypes
is less than ideal. Therefore, an important step in im-
proving the feasibility of PheWAS studies is to use
computational approaches to map phenotypes across
studies, effectively matching related phenotypes.
Definition Phenotype Mapping is the task of assign-
ing every variable from each participating study to
one out of a set of categories. The categories can be
defined for a given integrated study or consortium,
or can be taken from pre-existing ontologies, such
as PhenX (www.phenx.org).
For one example, consider the variable hypt
from WHI which is described by the text
?Hypertension ever? and the variable
HAE5A from the EAGLE study described by the
text ?Now taking prescribed medicine
for HBP?. To manually match these phenotypes,
a human expert declares these two variables to
be relevant to class ?hypertension?. Table 1
shows additional examples.
The phenotype mapping problem is quite chal-
lenging. First, the variable descriptions are quite
short (around 10 words, often less). Second, map-
ping the variables to a category, such as hyperten-
sion, may require significant background knowledge
(HBP stands for High Blood Pressure, also known
as hypertension). Third, there are large numbers of
variables, so the solution needs to scale gracefully.
In summary, in order to integrate data from public
repositories, such as dbGaP, or from large consortia,
such as the PAGE network, a critical task is to un-
derstand how the available phenotypes relate to each
other. In this paper, we present machine-learning
techniques for phenotype mapping that significantly
20
reduce the burden on researchers when integrating
data from multiple studies.
2 Related Work
From the perspective of biomedical sciences, phe-
notype mapping is a pre-requisite and a generaliza-
tion for the task of phenotype harmonization (?). In
harmonization, a single variable is identified or cal-
culated for each phenotype within each study. This
can only be accomplished for a very limited set of
variables. There is a need, however, to provide
enough information on a much larger set of pheno-
type variables so that researchers can determine the
common denominator version of a measure across
studies. For example, if a researcher is interested
in hypertension status as an outcome, there needs
to be an assessment of how hypertension status was
ascertained in each study. Different approaches in-
clude self-report, clinic-based blood pressure mea-
surement and/or anti-hypertensive medication use.
Only after this information is obtained, along with
other information, such as at what visit was status
assessed and whether the variable is available for
the entire cohort or only a portion of it will the re-
searcher be able to determine what to use in analysis
and how to interpret the findings. The phenotype
mapping task that we address in this paper enables
a researcher to rapidly find all the phenotype vari-
ables that are related to a given category, which then
constitutes the input to the harmonization process.
From the computer science perspective, the task
of phenotype mapping can be seen as an instance of
the problem of entity linkage, which appears in a va-
riety of forms across many contexts, namely record
linkage (?), object identification (?), duplicate de-
tection (?), and coreference (?; ?). That is, the prob-
lem of recognizing when multiple objects (in multi-
ple sources) actually correspond to the same entity.
Record linkage generally consists of three phases:
(1) blocking, where the number of pairs of objects
is reduced, which is critical for large datasets (e.g.,
(?; ?; ?)), (2) field similarity, where the attributes
of an object are compared (e.g., (?; ?; ?; ?; ?), and
(3) record similarity, which weights how different
attributes contribute to the similarity of records as a
whole (e.g., (?; ?)). Machine learning techniques are
used for many of these tasks.
The task of phenotype mapping is related, but dif-
fers from previous incarnations of record linkage. In
our case, the variables are the objects to be mapped.
However, the only attribute of an object is a terse
textual description (cf. Table 1). This makes the
problem harder since, as we will see, string simi-
larity measures are not enough, and term expansion
with additional background knowledge is necessary.
We do not consider blocking techniques in this pa-
per, since the number of phenotypes is in the thou-
sands and an exhaustive O(n2) comparison is still
feasible.
In this paper, we define and present an approach to
phenotype mapping with good experimental perfor-
mance, but there are many opportunities for refine-
ment by incorporating additional techniques from
the record linkage literature.
3 Phenotype Mapping
For the PAGE PheWAS study, phenotypes were first
manually matched, through the creation of 106 phe-
notype classes, in order to bring together related
phenotypes across studies. The following steps were
then used: First, the data from different studies were
filtered independently for any significant associa-
tion results with p < 0.01. Closely related phe-
notypes were then matched up between studies and
assigned to phenotype classes. Finally, phenotypes
from all studies, regardless of association results,
were matched up to the already defined phenotype
classes. In this way, a phenotype that might not
have shown a significant association result for a sin-
gle study, but that matched a phenotype class, would
still be added to the phenotype-class list. To scale up
the process it is important to develop a semi or fully
automatic approach for the task.
Table 1 shows some example phenotypes and
their classification. Class labels were assigned when
we manually matched the phenotypes. The real ID
of a phenotype in a study is given in column ID.
Description will be the main clue for automatic
matching. These examples were chosen to illustrate
unique characteristics that we observed in the manu-
ally matched data set and the challenges of the task.
? The descriptions are in a wide variety of forms.
They may be a compound term, a phrase, a sen-
tence, or even a question, and usually contain
21
Class Study ID Description
Allergy ARIC MHQA2A EVER TOLD HAD HAY FEVER
Allergy ARIC MHQA2B STILL HAVE HAY FEVER
Allergy EAGLEIII ALPBERFL Cat - flare length (mm)
Allergy EAGLEIII ALPCATWL Cat - wheal length (mm)
Allergy EAGLEIII ALPBERFL Cat - flare width (mm)
Allergy EAGLEIII ALPCATWL Cat - wheal width (mm)
Allergy MEC asthma History of Asthma, Hayfever, Skin Allergy,
Food Allergy or Any Other Allergy from
Baseline Questionnaire
CigaretteSmokedPerDay ARIC HOM32 NUMBER OF CIGARETTES PER DAY
CigaretteSmokedPerDay ARIC HOM35 OVERALL NUM OF CIGARETTES PER DAY
CigaretteSmokedPerDay CHS AMOUNT CIGS SMOKED/DAY
CigaretteSmokedPerDay WHI cigsday Smoke or smoked, cigarettes/day
Hematocrit ARIC HMTA01 HEMATOCRIT
Hematocrit EAGLEIII HTP Hematocrit (%)
Hematocrit WHI hematocr Hematocrit (%)
Hypertension ARIC HYPERT04 HYPERTENTION, DEFINITION 4
Hypertension ARIC HOM10A HIGH BP EVER DIAGNOSED
Hypertension CHS HYPER 1 CALCULATED HTN STATUS
Hypertension CHS HYPER 2 CALCULATED HTN STATUS
Hypertension CHS HYPER 3 CALCULATED HTN STATUS
Hypertension CHS HTNMED06 ANY HYPERTENTION MEDICATION
Hypertension EAGLEIII HAE2 Doctor ever told had hypertension/HBP
Hypertension EAGLEIII HAE5A Now taking prescribed medicine for HBP
Hypertension MEC q2hibp History of High Blood Pressure from QX2
Hypertension MEC hibp History of High Blood Pressure from
Baseline Questionnaire
Hypertension WHI hypt f30 Hypertension ever
Hypertension WHI htntrt f30 Hypertension
Smoker ARIC CURSMK01 CURRENT CIGARETTE SMOKER
Smoker CHS PRESSM PRESENT SMOKER
Smoker WHI smoknow Smoke cigarettes now
Table 1: Example phenotypes and their classification
less than 10 words, so it is difficult to apply so-
phisticated Natural Language Processing tech-
niques.
? Phenotypes may be related in different ways:
subsumption, overlapping, at the same layer of
semantic hierarchy, etc.
? The granularity of the classes varies. For exam-
ple, we have classes as specifically defined as
Hematocrit, the ratio of the volume of red
blood cells to the total volume of blood. But the
class Allergy covers a wide range of allergy
sources and symptoms. In Table 1, we show
four phenotype variables for allergies against
cats with flare and wheal sizes measured. Sim-
ilar variables include those for allergies of a
wide range of sources: alternaria, bermuda
grass, german cockroach, mite, peanut, rag-
weed, rye grass, Russian thistle, and white oak.
While in the same class, MEC uses a single phe-
notype asthma to cover just about all types of
allergies. On the other hand, phenotypes about
cigarette smoking are distinctively divided into
two categories: cigarettes smoked per day and
currently smoking. As we explained earlier, the
main criterion here is to maximize the chance
to detect unexpected associations, not necessar-
ily to match the most semantically similar phe-
notypes. As a result, directly applying conven-
tional clustering or topic modeling techniques
in Information Retrieval may not be appropri-
ate here.
? Some phenotypes in the same class appear
nearly identical. For example, the three hemat-
22
ocrit phenotypes have almost identical descrip-
tions. HYPER 1, 2 and 3 of the study CHS
in the class Hypertension have exactly the
same descriptions. For those cases, apply-
ing string similarity metrics can easily match
them together. However, some phenotypes
in the same class appear completely different
due to the use of synonyms and abbreviations.
Again in class Hypertension, ?hyperten-
sion,? ?HTN,? ?high blood pressure,? ?HBP,? and
?high BP? are keywords appearing in the de-
scriptions of phenotypes. It is possible for
an effective string similarity metric to recog-
nize abbreviations like ?HTN? for ?hyperten-
sion,? but without additional information there
is no way for a string similarity metric to match
?hypertension? and ?high blood pressure.?
4 Methods
We formulate the task as a problem of learning to
score the degree of match of a pair of phenotypes
based on their descriptions. By setting a threshold
of the score for match or not, the problem reduces to
a standard binary classification problem in Machine
Learning.
We started by performing a pre-processing step of
data cleaning to remove redundant phenotypes with
no description, then pairing the resulting pheno-
types for training and testing in a supervised learn-
ing framework. The data is skewed as most pairs are
negative.
Studies 5 Phenotypes 733
Classes 106 Total pairs 298378
Positives 10906 Negatives 287472
Table 2: Statistics of Data
Another pre-processing step is tokenization,
which was applied to the description of each phe-
notype before we extracted a set of features from
each pairs. The tokenization step includes convert-
ing all uppercase letters to lowercase letters, re-
moving punctuations, segmenting the text into to-
kens, and using Porter?s stemmer (?) to stem to-
kens, removing stop words and digits. For exam-
ple, ?TRANSIENT ISCHEMIC ATTACK? will
become (transient, ischem, attack). Note
that ?ic? was removed from ?ischemic? by the
stemming process.
The next step is feature extraction. The goal here
is to represent each pair of phenotype variables by
a set of feature values as the input to a machine-
learning model. We considered two types of fea-
tures. The first type is based on string similarity
metrics. The idea is to combine the strength of a va-
riety of string similarity metrics to measure the edit
distance between the descriptions of a pair of pheno-
types and use the result to determine if they match
each other. We chose 16 metrics as shown in Ta-
ble 3. Some of them are sophisticated and designed
for challenging record linkage tasks, such as match-
ing personal records in census data.
Levenshtein Distance
Needleman-Wunch Distance
Smith-Waterman Distance
Smith-Waterman-Gotoh Distance
Monge Elkan Distance Q-grams Distance
Jaro Distance Jaro Winkler
Block Distance Soundex Distance
Matching Coefficient Dice?s Coefficient
Jaccard Similarity Overlap Coefficient
Euclidean Distance Cosine Similarity
Table 3: String similarity metrics
We used the Java implementation provided by
SimMetrics1 to obtain the values of these metrics
given a pair of phenotype descriptions. SimMetrics
also provides descriptions and references of these
string similarity metrics. Each metric is treated as
one feature and normalized into a real value between
0 and 1, where 1 indicates that the two strings are
identical.
These string similarity metrics, however, treat all
words equally but apparently some words are more
important than others when we match phenotypes.
To assign different weights to different words, we
designed a feature set that can be considered as
weighted Jaccard as follows. Let t be a token or
a bi-gram (i.e., pair of consecutive tokens). For each
t there are two features in the feature set of the fol-
lowing forms:
? share-t: if t appears in the pre-processed de-
scriptions of both variables, then its value is 1
1staffwww.dcs.shef.ac.uk/people/S.
Chapman/simmetrics.html
23
and 0 otherwise;
? miss-t: if t appears in the pre-processed de-
scription of one variable only, then its value is
1 and 0 otherwise;
For example, suppose we have tokenized variables
V1 = (age, menopause, start), and V2 =
(menopause, start, when), then the features for
this pair will be
(miss-?age? : 1,
share-?menopause? : 1,
share-?start? : 1,
miss-?when? : 1,
miss-?age menopause? : 1,
share-?menopause start? : 1,
miss-?start when? : 1).
All other features will have value 0. In this way,
each example pair of variables will be represented as
a very high-dimensional feature vector of binary val-
ues. The dimensionality is proportional to the square
of the number of all distinct tokens appearing in the
training set.
Now we are ready to train a model by a machine-
learning algorithm using the examples represented
as feature vectors. The model of our choice is the
maximum entropy model (MaxEnt), also known as
logistic regression (?). An advantage of this model
is that efficient learning algorithms are available for
training this model with high-dimensional data and
the model not only classifies an example into posi-
tive or negative but also gives an estimated probabil-
ity as its confidence. The basic idea of logistic re-
gression is to search for a weight vector of the same
dimension as the feature vector such that this weight
vector when applied in the logit function of the prob-
ability estimation of the training examples will max-
imize the likelihood of the positive-negative assign-
ment of the training examples (?). The same model
can also be derived from the principle of maximum
entropy. We randomly selected half of the pairs as
the training examples and the rest as the holdout set
for evaluation.
We used the Merriam-Webster Medical Dictio-
nary (?)2 to augment the descriptions of phenotypes.
If there is an entry for a token in the dictionary,
2www.m-w.com/browse/medical/a.htm
then its definition will be included in the description
and then the same pre-processing and feature extrac-
tion steps will be applied. Pre-processing is also re-
quired to remove useless words from the definitions
in the dictionary. We chose this dictionary instead
of some ontology or phenotype knowledge base for
its quality of contents and comprehensive coverage
of biomedical terms. The Merriam-Webster Med-
ical Dictionary is also chosen as the only medical
dictionary included in the MedlinePlus3, a Web ser-
vice produced by the National Library of Medicine
for the National Institute of Health to provide reli-
able and up-to-date information about diseases, con-
ditions and wellness issues to the patients and their
families and friends.
5 Results
Table 4 shows the results in terms of precision, re-
call, and F-score. The first two rows show the use of
string similarity metrics as features to train a Naive
Bayes model and a MaxEnt model. The F-scores of
both models are similar, but Naive Bayes has higher
false positives while MaxEnt made more false neg-
ative errors. MaxEnt with weighted Jaccard out-
performs one with string-similarity features. Aug-
mentation by dictionary lookup (?w/ dictionary?) is
proved effective by improving recall from 0.59 to
0.82, as more positive mappings were identified for
those phenotype pairs described in different terms.
One may suspect that the augmentation may in-
crease false positives due to incorrectly associating
common words in the descriptions. But remarkably,
the false positives also decreased, resulting in the
improvement in precision as well.
Table 5 shows a set of selected examples to il-
lustrate the effectiveness of augmentation by dictio-
nary lookup. The first column shows the original de-
scriptions of the phenotype variable pairs. The sec-
ond and third columns show the classification results
(0 for negative, 1 for positive) and the confidence
scores by the MaxEnt model without augmentation.
The next two columns are their counterparts for the
model with augmentation.
For example, the definition of ?Goiter? is
?an enlargement of the thyroid gland.? There-
fore, after augmented by dictionary lookup, goi-
3www.nlm.nih.gov/medlineplus
24
Method / Model Precision Recall F-score
String similarity metrics feature
NaiveBayes 0.5236 0.6492 0.5797
MaxEnt 0.8092 0.4760 0.5994
Weighted Jaccard
MaxEnt 0.9655 0.5931 0.7348
w/ dictionary 0.9776 0.8208 0.8924
w/ transitive closure (depth= 1) 0.9138 0.8064 0.8568
w/ both 0.8961 0.9177 0.9068
Table 4: Performance results
Phenotypes w/o dic Score w/ dic Score
Goiter ever
Overactive thyroid ever 0 0.014562 1 0.996656
History of High Blood Pressure from
Baseline Questionnaire
Hypertension ever 0 0.014562 1 0.641408
DIABETES W/ FASTING GLUCOSE CUTPT.<126
Insulin shots now 0 0.014562 1 0.523262
TIA STATUS AT BASELINE
Stroke 0 0.014562 1 0.517444
NUMBER OF CIGARETTES PER DAY
CIGS SMOKED/DAY 0 0.014562 0 0.002509
Table 5: Examples of Mapping Results
ter can be matched with overactive thyroid. Sim-
ilarly, it is now possible to match ?High Blood
Pressure? with ?hypertension? and ?TIA?
with ?stroke.? ?DIABETES?, ?GLUCOSE?
and ?Insulin? can also be associated together.
However, terms must be covered in the medical
dictionary for this method to work. For example,
since ?CIGARETTES? is not a medical term and
even the most sophisticated string similarity met-
rics cannot match the local abbreviation ?CIGS?
to ?CIGARETTES?, both models failed to match
?SMOKE? and ?CIGARETTES? together.
A solution to this issue is to compute transitive
closure of the mapping. For example, if
V1 = (SMOKE) and
V2 = (SMOKE CIGARETTES)
are matched together by the model because of a
shared term ?smoke? and so are V2 and
V3 = (cigarettes),
but not V1 and V3, then transitive closure will infer
a match of V1 and V3. That will improve recall and
F-score further.
Figure 1 shows the performance of applying in-
creasing depths of transitive closure to the results
(a) without and (b) with augmentation by dictio-
nary lookup. Transitive closure improves the per-
formance for both models in the beginning but de-
grades quickly afterward because a phenotype may
be assigned to multiple classes. As false positives in-
crease, they will ripple when we infer new positives
from false positives. Improvement for the model (a)
is more obvious and degradation is not as grave. Ap-
plying transitive closure with depth = 1 yields the
best performance. The exact scores are shown in
Table 4 (See ?w/ transitive closure? and ?w/ both?).
The results above were obtained by splitting the
set of all pairs by half into training and test sets.
It is possible that the model remembers phenotype
descriptions because they distribute evenly in both
training and test sets. To apply the system in prac-
tice, the model must generalize to unseen pheno-
types. To evaluate the generalization power, instead
of splitting the set of pairs, we split the set of vari-
25
0 1 2 3 4 5 6 7 80.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Depth of transitive closure
Per
form
anc
e
(a) MaxEnt
 
 
F?scorePrecisionRecall
(a) MaxEnt model
0 1 2 3 4 5 6 7 80.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Depth of transitive closure
Per
form
anc
e
(b) With Dictionary
 
 
F?scorePrecisionRecall
(b) MaxEnt model with augmentation by dictionary lookup
Figure 1: Performance with increasing depths of transitive closure
ables by 2 to 1, and used 2/3 of phenotype variables
to generate pairs as the training set and 1/3 to pair
with those in the 2/3 set as well as with each other
for testing. That resulted in 129286 pairs for training
and 169092 pairs for testing. In this test set, 6356
pairs are positive.
We used this training set to train MaxEnt mod-
els using the weighted Jaccard feature set with and
without dictionary augmentation. Table 6 shows
the results. Again, dictionary augmentation signif-
icantly improves the performance in this case, too,
with the F-score reaching 0.81. Though the results
degrade slightly from the ones obtained by splitting
by pairs, this is expected as the training set is smaller
(129286 pairs vs. 149189 = 298378/2, see Ta-
ble 2). Consequently, the proposed models can gen-
eralize well to unseen phenotypes to some extent.
Method/Model Precision Recall F-score
w/o dictionary 0.9398 0.5817 0.7186
w/ dictionary 0.8213 0.7977 0.8093
Table 6: Performance results of splitting by variables
6 Conclusions and Future Work
In this paper, we define the problem of phenotype
mapping and present a solution by learning to score
and classify pairs of phenotypes. We evaluate our
solution using a data set of manually matched phe-
notypes from the PAGE PheWAS study. We show
that weighted Jaccard features are more effective for
this problem than combining string similarity met-
rics for a MaxEnt model and that dictionary aug-
mentation improves the performance by allowing
matching of phenotypes with semantically related
but syntactically different descriptions. We show
that inferring more positives by depth-one transitive
closure fixes those false negatives due to the lack of
dictionary definitions. Finally, the evaluation results
of splitting-by-variables show that the models gen-
eralize well to unseen variables, which is important
for the solution to be practical.
Our future work includes to apply blocking as a
pre-processing step to keep the number of pairs man-
ageable and to apply active or unsupervised learning
to alleviate the burden of generating training corpora
by manual matching.
Acknowledgments
This work was supported by NHGRI grant
HG004801 to C.-N.H. and J.L.A. and HG004798 to
S.A.P. and M.D.R. C.-J.K. was supported by NSC
99-3112-B-001-028, Taiwan. The data were made
available by the participating components of the
NHGRI PAGE program. The complete list of PAGE
members can be found at www.pagestudy.org.
The contents of this paper are solely the responsi-
bility of the authors and do not necessarily represent
the official views of the NIH.
26
References
Siiri N. Bennett, Neil Caporaso, Annette L. Fitzpatrick,
Arpana Agrawal, Kathleen Barnes, Heather A. Boyd,
Marilyn C. Cornelis, Nadia N. Hansel, Gerardo Heiss,
John A. Heit, Jae Hee Kang, Steven J. Kittner, Pe-
ter Kraft, William Lowe, Mary L. Marazita, Kris-
tine R. Monroe, Louis R. Pasquale, Erin M. Ramos,
Rob M. van Dam, Jenna Udren, Kayleen Williams, and
for the GENEVA Consortium. 2011. Phenotype har-
monization and cross-study collaboration in gwas con-
sortia: the GENEVA experience. Genetic Epidemiol-
ogy, 35(3):159?173.
Mikhail Bilenko and Raymond J. Mooney. 2003.
Adaptive duplicate detection using learnable string
similarity measures. In Proceedings of the Ninth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 39?48, Wash-
ington, DC, USA.
William W. Cohen, Pradeep Ravikumar, and Stephen
Fienberg. 2003. A comparison of string distance
metrics for name-matching tasks. In Proceedings of
IJCAI-03 Workshop on Information Integration on the
Web (IIWeb-03).
The Wellcome Trust Case Control Consortium. 2007.
Genome-wide association study of 14,000 cases of
seven common diseases and 3,000 shared controls.
Nature, 447(7145):661?678, June.
Joshua C. Denny, Marylyn D. Ritchie, Melissa A. Bas-
ford, Jill M. Pulley, Lisa Bastarache, Kristin Brown-
Gentry, Deede Wang, Dan R. Masys, Dan M. Ro-
den, and Dana C. Crawford. 2010. Phewas: demon-
strating the feasibility of a phenome-wide scan to
discover gene-disease associations. Bioinformatics,
26(9):1205?1210.
Ivan P. Felligi and Alan B. Sunter. 1969. A theory for
record linkage. Journal of the American Statistical As-
sociation, 64(328):1183?1210.
Michael Y. Galperin and Guy R. Cochrane. 2011. The
2011 nucleic acids research database issue and the on-
line molecular biology database collection. Nucleic
Acids Research, 39(suppl 1):D1?D6.
John Hardy and Andrew Singleton. 2009. Genomewide
association studies and human disease. New England
Journal of Medicine, 360(17):1759?1768.
T. Hastie, R. Tibshirani, and J. Friedmann. 2009. The El-
ements of Statistical Learning (2nd Edition). Springer-
Verlag, New York, NY, USA.
Mauricio A. Herna?ndez and Salvatore J. Stolfo. 1998.
Real-world data is dirty: Data cleansing and the
merge/purge problem. Data Mining and Knowledge
Discovery, 2:9?37.
Jerry R. Hobbs. 1979. Coherence and coreference. Cog-
nitive Science, 3(1):67?90.
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In Pro-
ceedings of the Sixth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 169?178.
Merriam-Webster. 2006. Medical Dictionary. Merriam-
Webster, Springfield, MA, USA.
Matthew Michelson and Craig A. Knoblock. 2006.
Learning blocking schemes for record linkage. In Pro-
ceedings of the 21st National Conference on Artificial
Intelligence (AAAI-06), Boston, MA.
Steven Minton, Claude Nanjo, Craig A. Knoblock, mar-
tin Michalowski, and Matthew Michelson. 2005. A
heterogeneous field matching method for record link-
age. In Proceedings of the Fifth IEEE International
Conference on Data Mining, Novemeber.
Alvaro Monge and Charles Elkan. 1996. The field
matching problem: Algorithms and applications. In
In Proceedings of the Second International Confer-
ence on Knowledge Discovery and Data Mining, pages
267?270.
Felix Naumann and Melanie Herschel. 2010. An Intro-
duction to Duplicate Detection. Synthesis Lectures on
Data Management. Morgan & Claypool Publishers.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 104?111,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Sheila Tejada, Craig A. Knoblock, and Steven Minton.
2001. Learning object identification rules for informa-
tion integration. Information Systems, 26(8).
27
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 20?29,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Contextual Bearing on Linguistic Variation in Social Media
Stephan Gouws?, Donald Metzler, Congxing Cai and Eduard Hovy
{gouws, metzler, ccai, hovy}@isi.edu
USC Information Sciences Institute
Marina del Rey, CA
90292, USA
Abstract
Microtexts, like SMS messages, Twitter posts,
and Facebook status updates, are a popular
medium for real-time communication. In this
paper, we investigate the writing conventions
that different groups of users use to express
themselves in microtexts. Our empirical study
investigates properties of lexical transforma-
tions as observed within Twitter microtexts.
The study reveals that different populations of
users exhibit different amounts of shortened
English terms and different shortening styles.
The results reveal valuable insights into how
human language technologies can be effec-
tively applied to microtexts.
1 Introduction
Microtexts, like SMS messages, Twitter posts, and
Facebook status updates, are becoming a popular
medium for real-time communication in the modern
digital age. The ubiquitous nature of mobile phones,
tablets, and other Internet-enabled consumer devices
provide users with the ability to express what is
on their mind nearly anywhere and at just about
any time. Since such texts have the potential to
provide unique perspectives on human experiences,
they have recently become the focus of many studies
within the natural language processing and informa-
tion retrieval research communities.
The informal nature of microtexts allows users
to invent ad hoc writing conventions that suit their
?This work was done while the first author was a visiting stu-
dent at ISI from the MIH Media Lab at Stellenbosch University,
South Africa. Correspondence may alternatively be directed to
stephan@ml.sun.ac.za.
particular needs. These needs strongly depend on
various user contexts, such as their age, geographic
location, how they want to be outwardly perceived,
and so on. Hence, social factors influence the way
that users express themselves in microtexts and other
forms of media.
In addition to social influences, there are also us-
ability and interface issues that may affect the way a
user communicates using microtexts. For example,
the Twitter microblog service imposes an explicit
message length limit of 140 characters. Users of
such services also often send messages using mobile
devices. There may be high input costs associated
with using mobile phone keypads, thus directly im-
pacting the nature of how users express themselves.
In this paper, we look specifically at understand-
ing the writing conventions that different groups
of users use to express themselves. This is ac-
complished by carrying out a novel empirical in-
vestigation of the lexical transformation character-
istics observed within Twitter microtexts. Our em-
pirical evaluation includes: (i) an analysis of how
frequently different user populations apply lexical
transformations, and (ii) a study of the types of
transformations commonly employed by different
populations of users. We investigate several ways of
defining user populations (e.g., based on the Twitter
client, time zone, etc.). Our results suggest that not
all microtexts are created equal, and that certain pop-
ulations of users are much more likely to use certain
types of lexical transformations than others.
This paper has two primary contributions. First,
we present a novel methodology for contextualized
analysis of lexical transformations found within mi-
20
crotexts. The methodology leverages recent ad-
vances in automated techniques for cleaning noisy
text. This approach enables us to study the fre-
quency and types of transformations that are com-
mon within different user populations and user con-
texts. Second, we present results from an empirical
evaluation over microtexts collected from the Twit-
ter microblog service. Our empirical analysis re-
veals that within Twitter microtexts, different user
populations and user contexts give rise to different
forms of expression, by way of different styles of
lexical transformations.
The remainder of this paper is laid out as follows.
Section 2 describes related work, while Section 3
motivates our investigation. Our multi-pronged
methodology for analyzing lexical transformations
is described in Section 4. Section 5 describes our
experimental results. Finally, Section 6 concludes
the paper and describes possible directions for fu-
ture work.
2 Related Work
Although our work is primarily focused on analyz-
ing the lexical variation in language found in on-
line social media, our analysis methodology makes
strong use of techniques for normalizing ?noisy text?
such as SMS-messages and Twitter messages into
standard English.
Normalizing text can traditionally be approached
using three well-known NLP metaphors, namely
that of spell-checking, machine translation (MT) and
automatic speech recognition (ASR) (Kobus et al,
2008).
In the spell-checking approach, corrections from
?noisy? words to ?clean? words proceed on a word-
by-word basis. Choudhury (2007) implements
the noisy channel model (Shannon and Weaver,
1948) using a hidden Markov model to handle both
graphemic and phonemic variations, and Cook and
Stevenson (2009) improve on this model by adapt-
ing the channel noise according to several predefined
word formations such as stylistic variation, word
clipping, etc. However, spelling correction is tra-
ditionally conducted in media with relatively high
percentages of well-formed text where one can per-
form word boundary detection and thus tokenization
to a high degree of accuracy. The main drawback is
the strong confidence this approach places on word
boundaries (Beaufort et al, 2010), since detecting
word boundaries in noisy text is not a trivial prob-
lem.
In the machine translation approach (Bangalore
et al, 2002; Aw et al, 2006), normalizing noisy
text is considered as a translation task from a source
language (the noisy text) to a target language (the
cleansed text). Since noisy- and clean text typically
vary wildly, it satisfies the notion of translating be-
tween two languages. However, since these trans-
formations can be highly creative, they usually need
a wide context (more than one word) to be resolved
adequately. Kobus (2008) also points out that de-
spite the fairly good results achieved with this sys-
tem, such a purely phrase-based translation model
cannot adequately handle the wide level of lexical
creativity found in these media.
Finally, the ASR approach is based on the ob-
servation that many noisy word forms in SMSes
or other noisy text are based on phonetic plays of
the clean word. This approach starts by convert-
ing the input message into a phone lattice, which
is converted to a word lattice using a phoneme-
grapheme dictionary. Finally the word lattice is de-
coded by applying a language model to the word lat-
tice and using a best-path algorithm to recover the
most likely original word sequence. This approach
has the advantage of being able to handle badly seg-
mented word boundaries efficiently, however it pre-
vents the next normalization steps from knowing
what graphemes were in the initial sequence (Kobus
et al, 2008).
What fundamentally separates the noisy text
cleansing task from the spell-checking problem is
that most often lexical ill-formedness in these me-
dia is intentional. Han (2011) proposes that this
might be in an attempt to save characters in length-
constrained media (such as Twitter or SMS), for
social identity (conversing in the dialect of a spe-
cific group), or due to convention of the medium.
Emotional context is typically expressed with re-
peat characters such as ?I am sooooooo tired? or
excessive punctuation. At times, however, out-
of-vocabulary tokens (spelling errors) might result
purely as the result of cognitive oversight.
Cook and Stevenson (2009) are one of the first to
explicitly analyze the types of transformations found
21
in short message domains. They identify: 1) stylis-
tic variation (better?betta), 2) subsequence abbre-
viation (doing?dng), 3) clipping of the letter ?g?
(talking?talkin), 4) clipping of ?h? (hello?ello),
and 5) general syllable clipping (anyway?neway),
to be the most frequent transformations. Cook and
Stevenson then incorporate these transformations
into their model. The idea is that such an unsuper-
vised approach based on the linguistic properties of
creative word forms has the potential to be adapted
for normalization in other similar genres without the
cost of developing a large training corpus. Most im-
portantly, they find that many creative texting forms
are the result of a small number of specific word for-
mation processes.
Han (2011) performs a simple analysis on the out-
of-vocabulary words found in Twitter, and find that
the majority of ill-formed words in Twitter can be
attributed to instances where letters are missing or
where there are extraneous letters, but the lexical
correspondence to the target word is trivially acces-
sible. They find that most ill-formed words are based
on morphophonemic variations.
3 Motivation
All of the previous work described in Section 2 ei-
ther
i) only focus on recovering the most likely ?stan-
dard English? form of a message, disregarding
the stylistic structure of the original noisy text,
or
ii) considers the structure of the noisy text found
in a medium as a whole, only as a first step
(the means) to identify common types of noisy
transformations which can subsequently be ac-
counted for (or ?corrected?) to produce normal-
ized messages (the desired end result).
However, based on the fact that language is highly
contextual, we ask the question: What influence
does the context in which a message is produced
have on the resulting observed surface structure and
style of the message?
In general, since some topics are for instance
more formal or informal than others, vocabulary and
linguistic style often changes based on the topic that
is being discussed. Moreover, in social media one
can identify several other types of context. Specif-
ically in Twitter, one might consider a user?s geo-
graphical location, the client from which a user is
broadcasting her message, how long she has been
using the Twitter service, and so forth.
The intuition is that the unconstrained nature of
these media afford users the ability to invent writing
conventions to suit their needs. Since users? needs
depend on their circumstances, and hence their con-
text, we hypothesize that the observed writing sys-
tems might be influenced by some elements of their
context. For instance, phonemic writing systems
might be related to a user?s dialect which is re-
lated to a user?s geographical location. Furthermore,
highly compressed writing conventions (throwing
away vowels, using prefixes of words, etc.) might
result from the relatively high input cost associ-
ated with using unwieldy keypads on some mobile
clients, etc.
The present work is focused on looking at these
stylistic elements of messages found in social media,
by analyzing the types of stylistic variation at the
lexical level, across these contextual dimensions.
4 Method
In the following discussion we make a distinc-
tion between within-tweet context and the general
message-context in which a message is created.
Within-tweet context is the linguistic context (the
other terms) that envelopes a term in a Twitter mes-
sage. The general context of a Twitter message is the
observable elements of the environment in which it
was conceived. For the current study, we record
1. the user?s location, and
2. the client from which the message was sent,
We follow a two-pronged analytic approach:
Firstly, we conduct a na??ve, context-free analysis
(at the linguistic level) of all words not commonly
found in standard, everyday English. This analy-
sis purely looks at the terminology that are found
on Twitter, and does not attempt to normalize these
messages in any way. Therefore, different surface
forms of the same word, such as ?today?, ?2day?,
?2d4y?, are all considered distinct terms. We then
analyse the terminology over different contextual di-
mensions such as client and location.
22
Secondly, we perform a more in-depth and con-
textual analysis (at the word level) by first normaliz-
ing the potentially noisy message to recover the most
likely surface form of the message and recording the
types of changes that were made, and then analyz-
ing these types of changes across different general
contextual dimensions (client and location).
As noted in Section 2, text message normalization
is not a trivial process. As shown by Han (2011),
most transformations from in-vocabulary words to
out-of-vocabulary words can be attributed to a single
letter that is changed, removed, or added. Further-
more, they note that most ill-formed words are re-
lated to some morphophonemic variation. We there-
fore implemented a text cleanser based on the de-
sign of Contractor (2010) using pre-processing tech-
niques discussed in (Kaufmann and Kalita, 2010).
It works as follows: For each input message, we
replace @-usernames with ?*USR*? and urls with
?*URL*?. Hash tags can either be part of the sen-
tence (?just got a #droid today?) or be peripheral to
the sentence (?what a loooong day! #wasted?). Fol-
lowing Kaufmann (2010) we remove hashtags at the
end of messages when they are preceded by typical
end-of-sentence punctuation marks. Hash tags in the
middle of messages are retained, and the hash sign
removed.
Next we tokenize this preprocessed message us-
ing the NLTK tokenizer (Loper and Bird, 2002). As
noted earlier, standard NLP tools do not perform
well on noisy text out-of-the-box. Based on inspec-
tion of incorrectly tokenized output, we therefore in-
clude a post-tokenization phase where we split all
tokens that include a punctuation symbol into the in-
dividual one or two alphanumeric tokens (on either
side of the punctuation symbol), and the punctuation
symbol1. This heuristic catches most cases of run-on
sentences.
Given a set of input tokens, we process these one
by one, by comparing each token to the words in
the lexicon L and constructing a confusion network
CN. Each in-vocabulary term, punctuation token or
other valid-but-not-in-vocabulary term is added to
CN with probability 1.0 as shown in Algorithm 1.
1This is easily accomplished using a regular expression
group-substitution of the form (\w*)([P])(\w*)?[\1,
\2, \3], where \w represents the set of alphanumeric char-
acters, and P is the set of all punctuation marks [.,;?". . .]
Character Transliteration candidates
1 ?1?, ?l?, ?one?
2 ?2?, ?to?, ?too?, ?two?
3 ?3?, ?e?, ?three?
4 ?4?, ?a?, ?for?, ?four?
5 ?5?, ?s?, ?five?
6 ?6?, ?b?, ?six?
7 ?7?, ?t?, ?seven?
8 ?8?, ?ate?, ?eight?
9 ?9?, ?g?, ?nine?
0 ?0?, ?o?, ?zero?
?@? ?@?, ?at?
?&? ?&?, ?and?
Table 1: Transliteration lookup table.
valid tok(wi) checks for ?*USR*?, ?*URL*?, or
any token longer than 1 character with no alphabet-
ical characters. This heuristic retains tokens such as
?9-11?, ?12:44?, etc.
At this stage, all out-of-vocabulary (OOV) terms
represent the terms that we are uncertain about, and
hence candidate terms for cleansing. First, for each
OOV term, we enumerate each possibly ambiguous
character into all its possible interpretations with the
transliteration table shown in Table 1. This expands,
for example, ?t0day?? [?t0day?, ?today?], and also
?2day?? [?2day?, ?twoday?, ?today?], etc.
Each transliterated candidate word in each con-
fusion set produced this way is then scored with
the original word and ranked using the heuristic
function (sim()) described in (Contractor et al,
2010)2. We also evaluated a purely phonetic edit-
distance similarity function, based on the Double
Metaphone algorithm (Philips, 2000), but found the
string-similarity-based function to give more reli-
able results.
Each confusion set produced this way (see Al-
gorithm 2) is joined to its previous set to form a
growing confusion lattice. Finally this lattice is de-
coded by converting it into the probabilistic finite-
state grammar format, and by using the SRI-LM
toolkit?s (Stolcke, 2002) lattice-tool com-
mand to find the best path through the lattice by
2The longest common subsequence between the two words,
normalized by the edit distances between their consonant skele-
tons.
23
Transformation Type Rel %
single char (?see?? ?c?) 29.1%
suffix (?why?? ?y?) 18.8%
drop vowels (?be?? ?b?) 16.4%
prefix (?tomorrow?? ?tom?) 9.0%
you to u (?you?? ?u?) 8.3%
drop last char (?running?? ?runnin?) 7.0%
repeat letter (?so?? ?soooo?) 5.5%
contraction (?you will?? ?you?ll?) 5.0%
th to d (?this?? ?dis?) 1.0%
Table 2: Most frequently observed types of transforma-
tions with an example in parentheses. Rel % shows the
relative percentage of the top-10 transformations which
were identified (excluding unidentified transformations)
to belong to a specific class.
making use of a language model to promote fluid-
ity in the text, and trained as follows:
We generated a corpus containing roughly 10M
tokens of clean English tweets. We used a simple
heuristic for selecting clean tweets: For each tweet
we computed if #(OOV )#(IV )+1 < ?, where ? = 0.5
was found to give good results. On this corpus
we trained a trigram language model, using Good-
Turing smoothing. Next, a subset of the LA Times
containing 30M words was used to train a ?general
English? language model in the same way. These
two models were combined3 in the ratio 0.7 to 0.3.
The result of the decoding process is the hypoth-
esized clean tokens of the original sentence. When-
ever the cleanser makes a substitution, it is recorded
for further analysis. Upon closer inspection, it was
found that most transformation types can be recog-
nized by using a fairly simple post-processing step.
Table 2 lists the most frequent types of transforma-
tions. While these transformations do not have per-
fect coverage, they account for over 90% of the (cor-
rect) transformations produced by the cleanser. The
rules fail to cover relatively infrequent edge cases,
such as ?l8r ? later?, ?cuz ? because?, ?dha ?
the?, and ?yep? yes? 4.
3Using the -mix-lm and -lambda and -mix-lambda2
options to the SRI-LM toolkit?s ngram module.
4To our surprise these ?typical texting forms? disappeared
into the long tail in our data set.
Original Cleansed
Swet baby jeebus, some-
one PLEASE WINE ME!
sweet baby jesus , some-
one please wine me !
2 years with Katie today! two years with katie to-
day!
k,hope nobody was
hurt.gud mornin jare
okay , hope nobody was
hurt . good morning jamie
When u a bum but think u
da best person on da court
you doodooforthebooboo
when you a bum but think
you the best person on the
court you dorothy
NYC premiere 2morrow. nice premiere tomorrow .
Table 3: Examples of original and automatically cleansed
versions of Twitter messages.
Algorithm 1 Main cleanser algorithm pseudo code.
The decode() command converts the confusion
network (CN) into PFSG format and decodes it us-
ing the lattice-tool of the SRI-LM toolkit.
Require: Lexicon L, Punctuation set P
function CLEANSE MAIN(Min)
for wi ?Min do
if wi ? L ? P or valid tok(wi) then
Add (1.0, wi) to CNout . Probability 1.0
else
Add conf set(wi) to CNout
end if
end for
return decode(CNout)
end function
Table 3 illustrates some example corrections
made by the cleanser. As the results show, the
cleanser is able to correct many of the more com-
mon types of transformations, but can fail when it
encounters infrequent or out-of-vocabulary terms.
5 Evaluation
This section describes our empirical evaluation and
analysis of how users in different contexts express
themselves differently using microtexts. We focus
specifically on the types of lexical transformations
that are commonly applied globally, within popula-
tions of users, and in a contextualized manner.
24
Algorithm 2 Algorithm pseudo code for generating
confusion set CS. L[wi] is the lexicon partitioning
function for word wi.
Require: Lexicon L, confusion set CS, implemented as
top-K heap containing (si, wi), indexed on si
function CONF SET(wi)
W? translits(wi)
for wj ?W do
for wk ? L[wj ] do
sk ? sim(wj , wk)
if sk > min(CS) then
Add (sk, wk) to CS
end if
end for
end for
return CS
end function
5.1 Out-of-Vocabulary Analysis
We begin by analyzing the types of terms that are
common in microtexts but not typically used in
proper, everyday English texts (such as newspapers).
We refer to such terms as being out-of-vocabulary,
since they are not part of the common written En-
glish lexicon. The goal of this analysis is to un-
derstand how different contexts affect the number
of out-of-vocabulary terms found in microtexts. We
hypothesize that certain contextual factors may in-
fluence a user?s ability (or interest) to formulate
clean microtexts that only contain common English
terms.
We ran our analysis over a collection of one mil-
lion Twitter messages collected using the Twitter
streaming API during 2010. Tweets gathered from
the Twitter API are tagged with a language identifier
that indicates the language a user has chosen for his
or her account. However, we found that many tweets
purported to be English were in fact not. Hence,
we ran all of the tweets gathered through a simple
English language classifier that was trained using a
small set of manually labeled tweets, uses character
trigrams and average word length as features, and
achieves an accuracy of around 93%. The every-
day written English lexicon, which we treat as the
?gold standard? lexicon, was distilled from the same
collection of LA Times news articles described in
Section 4. This yielded a comprehensive lexicon of
approximately half a million terms.
Timezone % In-Vocabulary
Australia 86%
UK 85%
US (Atlantic) 84%
Hong Kong 83%
US (Pacific) 81%
Hawaii 81%
Overall 81%
Table 4: Percentage of in-vocabulary found in large En-
glish lexicon for different geographic locations.
For each tweet, the tokenized terms were looked
up in the LA Times lexicon to determine if the
term was out-of-vocabulary or not. Not surprisingly,
the most frequent out-of-vocabulary terms identi-
fied are Twitter usernames, URLs, hasthags, and RT
(the terminology for a re-broadcast, or re-tweeted,
message). These tokens alone account for approx-
imately half of all out-of-vocabulary tokens. The
most frequent out-of-vocabulary terms include ?lol?,
?haha?, ?gonna?, ?lmao?, ?wanna?, ?omg?, ?gotta?.
Numerous expletives also appear amongst the most
common out-of-vocabulary terms, since such terms
never appear in the LA Times. Out of vocabulary
terms make up 19% of all terms in our data set.
In the remainder of this section, we examine
the out-of-vocabulary properties of different popu-
lations of users based on their geographic location
and their client (e.g., Web-based or mobile phone-
based).
5.1.1 Geographic Locations
To analyze the out-of-vocabulary properties of
users in different geographic locations, we extracted
the time zone information from each Tweet in our
data set. Although Twitter allows users to specify
their location, many users leave this field blank, use
informal terminology (?lower east side?), or fabri-
cate non-existent locations (e.g., ?wherever i want
to be?). Therefore, we use the user?s time zone as
a proxy for their actual location, in hopes that users
have less incentive to provide incorrect information.
For the Twitter messages associated with a given
time zone, we computed the percentage of tokens
found within our LA Times-based lexicon. The re-
sults from this analysis are provided in Table 4. It is
25
Client % In-Vocabulary
Facebook 88%
Twitter for iPhone 84%
Twitter for Blackberry 83%
Web 82%
UberTwitter 78%
Snaptu 73%
Overall 81%
Table 5: Percentage of in-vocabulary found in large En-
glish lexicon for different Twitter clients.
important to note that these results were computed
over hundreds of thousands of tokens, and hence
the variance of our estimates is very small. This
means that the differences observed here are statis-
tically meaningful, even though the absolute differ-
ences tend to be somewhat small.
These results indicate that microtexts composed
by users in different geographic locations exhibit
different amounts of out-of-vocabulary terms. Users
in Australia, the United Kingdom, Hong Kong, and
the East Coast of the United States (e.g., New York
City) include fewer out-of-vocabulary terms in their
Tweets than average. However, users from the West
Coast of the United States (e.g., Los Angeles, CA)
and Hawaii are on-par with the overall average, but
include 5% more out-of-vocabulary terms than the
Australian users.
As expected, the locations with fewer-than-
average in-vocabulary tokens are associated with
non-English speaking countries, despite the output
from the classifier.
5.1.2 Twitter Clients
In a similar experiment, we also investigated the
frequency of out-of-vocabulary terms conditioned
on the Twitter client (or ?source?) used to compose
the message. Example Twitter clients include the
Web-based client at www.twitter.com, official
Twitter clients for specific mobile platforms (e.g.,
iPhone, Android, etc.), and third-party clients. Each
client has its own characteristics, target user base,
and features.
In Table 5, we show the percentage of in-
vocabulary terms for a sample of the most widely
used Twitter clients. Unlike the geographic location-
based analysis, which showed only minor differ-
ences amongst the user populations, we see much
more dramatic differences here. Some clients, such
as Facebook, which provides a way of cross-posting
status updates between the two services, has the
largest percentage of in-vocabulary terms of the ma-
jor clients in our data.
One interesting, but unexpected, finding is that the
mobile phone (i.e., iPhone and Blackberry) clients
have fewer out-of-vocabulary terms, on average,
than the Web-based client. This suggests that ei-
ther the users of the clients are less likely to misspell
words or use slang terminology or that the clients
may have better or more intuitive spell checking ca-
pabilities. A more thorough analysis is necessary to
better understand the root cause of this phenomenon.
At the other end of the spectrum are the UberTwit-
ter and Snaptu clients, which exhibit a substantially
larger number of out-of-vocabulary terms. These
clients are also typically used on mobile devices. As
with our previous analysis, it is difficult to pinpoint
the exact cause of such behavior, but we hypothe-
size that it is a function of user demographics and
difficulties associated with inputting text on mobile
devices.
5.2 Contextual Analysis
In this section, we test the hypothesis that different
user populations make use of different types of lex-
ical transformations. To achieve this goal, we make
use of our noisy text cleanser. For each Twitter mes-
sage run through the cleanser, we record the origi-
nal and cleaned version of each term. For all of the
terms that the cleanser corrects, we automatically
identify which (if any) of the transformation rules
listed in Table 2 explain the transformation between
the original and clean version of the term. We use
this output to analyze the distribution of transforma-
tions observed across different user populations.
We begin by analyzing the types of transforma-
tions observed across Twitter clients. Figure 1 plots
the (normalized) distribution of lexical transforma-
tions observed for the Web, Twitter for Blackberry,
Twitter for iPhone, and UberTwitter clients, grouped
by the transformations. We also group the trans-
formations by the individual clients in Figure 2 for
more direct comparison.
The results show that Web users tend to use more
26
Figure 1: Proportion of transformations observed across
Twitter clients, grouped by transformation type.
contractions than Blackberry and UberTwitter users.
We relate this result to the differences in typing on
a virtual compared to a multi-touch keypad. It was
surprising to see that iPhone users tended to use con-
siderably more contractions than the other mobile
device clients, which we relate to its word-prediction
functionality. Another interesting result is the fact
that Web users often drop vowels to shorten terms
more than their mobile client counterparts. Instead,
mobile users often use suffix-style transformations
more, which is often more aggressive than the drop-
ping vowels transformation, and possibly a result of
the pervasiveness of mobile phones: Large popu-
lations of people?s first interaction with technology
these days are through a mobile phone, a device
where strict length limits are imposed on texting,
and which hence enforce habits of aggressive lex-
ical compression, which might transfer directly to
their use of PCs. Finally, we observe that mobile de-
vice users replace ?you? with ?u? substantially more
than users of the Web client.
We also performed the same analysis across time
zones/locations. The results are presented in Fig-
ure 3 by transformation-type, and again grouped by
location for direct comparison in Figure 4. We ob-
serve, perhaps not surprisingly, that the East Coast
US, West Coast US, and Hawaii are the most similar
with respect to the types of transformations that they
Figure 2: Proportion of transformations observed across
Twitter clients, grouped by client.
commonly use. However, the most interesting find-
ing here is that British users tend to utilize a notice-
ably different set of transformations than American
users in the Pacific time zones. For example, British
users are much more likely to use contractions and
suffixes, but far less likely to drop the last letter of
a word, drop all of the vowels in a word, use prefix-
style transformations, or to repeat a given letter mul-
tiple times. In a certain sense, this suggests that
British users tend to write more proper, less informal
English and make use of strikingly different styles
for shortening words compared to American users.
This might be related to the differences in dialects
between the two regions manifesting itself during a
process of phonetic transliteration when composing
the messages: Inhabitants of the south-west regions
in the US are known for pronouncing for instance
running as runnin?, which manifests as dropping the
last letter, and so forth.
Therefore, when taken with our out-of-vocabulary
analysis, our experimental evaluation shows clear
evidence that different populations of users express
themselves differently online and use different types
of lexical transformations depending on their con-
text. It is our hope that the outcome of this study
will spark further investigation into these types of
issues and ultimately lead to effective contextually-
aware natural language processing and information
retrieval approaches that can adapt to a wide range
of user contexts.
27
Figure 3: Proportion of transformations observed across
geographic locations, grouped by transformation type.
6 Conclusions and Future Work
This paper investigated the writing conventions that
different groups of users use to express themselves
in microtexts. We analyzed characteristics of terms
that are commonly found in English Twitter mes-
sages but are never seen within a large collection
of LA Times news articles. The results showed
that a very small number of terms account for a
large proportion of the out-of-vocabulary terms. The
same analysis revealed that different populations of
users exhibit different propensities to use out-of-
vocabulary terms. For example, it was found that
British users tend to use fewer out-of-vocabulary
terms compared to users within the United States.
We also carried out a contextualized analysis that
leveraged a state-of-the-art noisy text cleanser. By
analyzing the most common types of lexical trans-
formations, it was observed that the types of trans-
formations used varied across Twitter clients (e.g.,
Web-based clients vs. mobile phone-based clients)
and geographic location. This evidence supported
our hypothesis that the measurable contextual indi-
cators surrounding messages in social media play an
important role in determining how messages in these
media vary at the surface (lexical) level from what
might be considered standard English.
The outcome of our empirical evaluation and
subsequent analysis suggests that human language
Figure 4: Proportion of transformations observed across
geographic locations, grouped by location.
technologies (especially natural language process-
ing techniques that rely on well-formed inputs) are
likely to be highly susceptible to failure as the result
of lexical transformations across nearly all popula-
tions and contexts. However, certain simple rules
can be used to clean up a large number of out-of-
vocabulary tokens. Unfortunately, such rules would
not be able to properly correct the long tail of
the out-of-vocabulary distribution. In such cases,
more sophisticated approaches, such as the noisy
text cleanser used in this work, are necessary to
combat the noise. Interestingly, most of the lexical
transformations observed affect non-content words,
which means that most information retrieval tech-
niques will be unaffected by such transformations.
As part of future work, we are generally interested
in developing population and/or context-aware lan-
guage processing and understanding techniques on
top of microtexts. We are also interested in ana-
lyzing different user contexts, such as those based
on age and gender and to empirically quantify the
effect of noise on actual natural language process-
ing and information retrieval tasks, such as part of
speech tagging, parsing, summarization, etc.
Acknowledgments
We would like to thank the anonymous reviewers for
their insightful comments. Stephan Gouws would
like to thank MIH Holdings Ltd. for financial sup-
port during the course of this work.
28
References
A.T. Aw, M. Zhang, J. Xiao, and J. Su. 2006. A Phrase-
based Statistical Model for SMS Text Normalization.
In Proceedings of the COLING/ACL Main Conference
Poster Sessions, pages 33?40. Association for Compu-
tational Linguistics.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Boot-
strapping Bilingual Data Using Consensus Transla-
tion for a Multilingual Instant Messaging System. In
Proceedings of the 19th International Conference on
Computational Linguistics-Volume 1, pages 1?7. As-
sociation for Computational Linguistics.
R. Beaufort, S. Roekhaut, L.A. Cougnon, and C. Fa-
iron. 2010. A Hybrid Rule/Model-based Finite-State
Framework for Normalizing SMS Messages. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 770?779. Asso-
ciation for Computational Linguistics.
M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,
and A. Basu. 2007. Investigation and Modeling of the
Structure of Texting Language. International Journal
on Document Analysis and Recognition, 10(3):157?
174.
D. Contractor, T.A. Faruquie, and L.V. Subramaniam.
2010. Unsupervised Cleansing of Noisy Text. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 189?196.
Association for Computational Linguistics.
P. Cook and S. Stevenson. 2009. An Unsupervised
Model for Text Message Normalization. In Proceed-
ings of the Workshop on Computational Approaches
to Linguistic Creativity, pages 71?78. Association for
Computational Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics.
M. Kaufmann and J. Kalita. 2010. Syntactic Normaliza-
tion of Twitter Messages.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normaliz-
ing SMS: Are Two Metaphors Better Than One? In
Proceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 441?448.
Association for Computational Linguistics.
E. Loper and S. Bird. 2002. NLTK: The Natural Lan-
guage Toolkit. In Proceedings of the ACL-02 Work-
shop on Effective tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics-Volume 1, pages 63?70. Association for
Computational Linguistics.
L. Philips. 2000. The Double Metaphone Search Algo-
rithm. CC Plus Plus Users Journal, 18(6):38?43.
C.E. Shannon and W. Weaver. 1948. The Mathemati-
cal Theory of Communication. Bell System Technical
Journal, 27:623?656.
A. Stolcke. 2002. SRILM - An Extensible Language
Modeling Toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901?904.
29
