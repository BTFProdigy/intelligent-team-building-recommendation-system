St ructura l  d i sambiguat ion  of  morpho-syntact i c  categor ia l  pars ing 
for Korean  * 
Jeongwon Cha and Geunbae Lee 
Department of Computer Science & Engineering 
Pohang University of Science & Technology 
Pohang, Korea 
{himen, gblee}@postech.ac.kr 
Abstract 
The Korean Combinatory Categorial Grammar 
(KCCG) tbrmalism can unitbrmly handle word 
order variation among arguments and adjuncts 
within a clause as well as in complex clauses 
and across clause boundaries, i.e., long distance 
scrambling. Ill this paper, incremental pars- 
ing technique of a morpheme graph is devel- 
oped using the KCCG. We present echniques 
for choosing the most plausible parse tree us- 
ing lexical information such as category merge 
probability, head-head co-occurrence heuristic, 
and the heuristic based on the coverage of sub- 
trees. The performance r sults for various mod- 
els for choosing the most plausible parse tree are 
compared. 
1 I n t roduct ion  
Korean is a non-configurational, t)ostpositional, 
agglutinative language. Postpositions, uch as 
noun-endings, verb-endings, and prefinal verb- 
endings, are morphemes that determine the 
fnnctional role of NPs (noun phrases) and VPs 
(verb phrases) in sentences and also transform 
VPs into NPs or APs (adjective phrases). Since 
a sequence of prefinal verb-endings, auxiliary 
verbs and verb-endings can generate hundreds 
of different usages of the same verb, morpheme- 
based grammar modeling is considered as a nat- 
ural consequence for Korean. 
There have been various researches to dis- 
ambiguate the structural ambiguities in pars- 
ing. Lexical and contextual information has 
been shown to be most crucial for many pars- 
ing decisions, such as prepositional-phrase at- 
tachment (Hindle and Rooth, 1993). (Charniak, 
1995; Collins, 1996) use the lexical intbrmation 
* This  research was part ial ly supported by KOSEF  spe- 
cial basic resem'ch 1)rogram (1997.9 ~ 2000.8). 
and (Magerman and Marcus, 1991; Magerman 
and Weir, 1992) use the contextual information 
for struct;nral disambiguation. But, there have 
been few researches that used probability intbr- 
marion for reducing the spurious ambiguities in 
choosing the most plausible parse tree of CCG 
formalism, especially for morpho-syntactic pars- 
ing of agglutinative language. 
In this paper, we describe the probabilistic 
nmthod (e.g., category merge probability, head- 
head co-occurrence, coverage heuristics) to re- 
duce the spurious atnbiguities and choose the 
most plausible parse tree for agglutinative lan- 
guages uch as Korean. 
2 Overv iew of  KCCG 
This section briefly reviews the basic KCCG for- 
malism. 
Following (Steedman, 1985), order-preserving 
type-raising rules are used to convert nouns in 
grammar into the functors over a verb. The 
following rules are obligatorily activated uring 
parsing when case-marking morphemes attach 
to  nora1 s tems.  
? Type Raising Rules: 
np + case-marker 
feature\]) 
v/(v\np\[case- 
This rule indicates that a noun in the pres- 
ence of a case morpheme becomes a functor 
looking for a verb on its right; this verb is also 
a flmctor looking for the original noun with the 
appropriate case on its left. Alter tile noun 
functor combines with the appropriate verb, the 
result is a flmctor, which is looking for the re- 
maining arguments of the verb. 'v' is a w~ri- 
able tbr a verb phrase at ally level, e.g., the 
verb of a matrix clause or the verb of an em- 
bedded clause. And 'v' is matched to all of 
1002 
the "v\[X\]\Args" patterns of the verl, categories. 
Since all case-marked ilouns in Korean occur in 
front of the verb, we don't need to e, mploy the 
directional rules introduced by (Hoffman, 1995). 
We extend the combinatory rules ibr uncm'- 
ried flmctions as follows. The sets indicated by 
braces in these rules are order-free. 
? Forward Application (A>): 
x/(args u {Y}) Y X/Args 
? Backward Application (A<): 
Y X\(Args U {Y}) ==4- X\Args 
Using these rules, a verb can apply to its 
arguments in any order, or as in most cases, 
the casednarked noun phrases, which are type- 
raised flmctors, can apply to the, al)t)roi)riate 
verbs. 
Coordination constructions are moditied to 
allow two type-raised noml 1)hrases that are 
looking tbr the saxne verb to combine together. 
Since noun phrases, or a noun phrase and ad- 
verb phrase, are fimctors, the following compo- 
sition rules combine two flmctions with a set 
vahle al'gulnents. 
? Forward Composition (B>): 
X/(X\Ar.q.sx) Y/(Y\Arg.sy) ==~ 
x /  (X\ ( A,.:j<,: u ) ), 
Y = X\Arqsx 
? Backward Comi)osition (B<): 
Y\Arg.sy X\(Ar.q.sx U {Y}) ===> 
X\(A'rgsx U Arosy) 
? Coordination (~): 
X CONJX  ~ X 
3 Bas ic  morph-syntact i c  char t  
pars ing  
Korean chart parser has been developed based 
on our KCCG modeling with a 10(},0()0 mor- 
pheme dictionary. Each morpheme entry in 
the dictionary has morphological category, mor- 
photactics connectivity and KCCG syntax (:at- 
egories tbr the morpheme. 
In the morphological analysis stage, a un- 
known word treatment nmthod based on a mor- 
pheme pattern dictionary and syllable bigrams 
is used after (Cha et al, 1998). POS(part -of  
speech) tagger which is tightly coupled with 
the morphological analyzer removes the irrele- 
wmt morpheme candidates from the lnorpheme 
graph. The morpheme graph is a compact 
representation method of Korean morphologi- 
cal structure. KCCG parser analyzes the mor- 
pheme graph at once through the morpheme 
graph embedding technique (Lee et al, 1996). 
The KCCG parser incrementally analyzes the 
sentence, eojeol by eojeol :1 Whenever an eo- 
jeol is newly processed by the morphological n- 
alyzer, the morphenms resulted in a new mor- 
pheme graph are embedded in a chart and an- 
alyzed and combined with the previous parsing 
results. 
4 Statistical structured 
disambiguation for  KCCG parsing 
Th(' statistics which have been used in the ex- 
perinlents have been collected fronl the KCCG 
parsed corpora. The data required for train- 
ing have been collected by parsing the stan- 
dard Korean sentence types 2, example sentences 
of grammar book, and colloquial sentences in 
trade interview domain 3 and hotel reservation 
domain 4. We use about; 1500 sentences for 
training and 591 indq)endent sentences for eval- 
uation. 
The evaluation is based on parsewfl 
method (Black el, a\]., 1991). In the evalu- 
ation, "No-crossing" is 1;11o number of selltellces 
which have no crossing brackets between the 
result and |;tie corresponding correct trees of 
the sentences. "Ave. crossing" is the average 
number of crossings per sentence. 
4.1 Bas ic  s ta t i s t i ca l  model  
A basic method of choosing the nlost plausible 
parse tree is to order the prot)abilities by the lex- 
ical preib, rences 5 and the syntactic merge prob- 
ability. In general, a statistical parsing model 
defines the conditional probability, 1"(71S), for 
each candidate tree r tbr a sentence S. A gener- 
ative model uses the observation that maximis- 
ing P(% S) is equivalent to maximising P(r IS)  6. 
1Eojeol is a spacing unit in Korean and is similar to 
an English word. 
2Sentences of length < 11. 
aSentences of length < 25. 
4Sentences ofhmgth _< 13. 
5The frequency with which a certain category is as- 
sociated with a morpheme tagged for part-of-speech. 
c'P(S) is constmlt. 
1003 
Thus, when S is a sentence consisted of a se- 
quence of morphemes tagged for part-of-speech, 
(w~, t~), (w2, t2), ..., (w,,, tu), where wi is a i th 
morpheme, ti is the part-of-speech tag of the 
morpheme wi, and cij is a category with rela- 
tive position i, j, the basic statistical model will 
be given by: 
r* = arg ,~x P(rl,S' ) (1) 
(2) = argn~x P(S) 
,~ argmaxP(T,S ). (3) 
T 
The r* is the probabilities of the optimM parse 
tree. 
P(r, S) is then estimated by attaching proba- 
bilities to a bottom-up composition of the tree. 
P(r,S) = I I  P(cij) (4) 
c i j  ~T  
= H (P(eiilcik'ck+'J) 
c i j  ET  
xP(cik)P(cl~+lj)), (5) 
i<k<j ,  
i f  cij is a terminal, 
the,  P(c j) = 
and 
frcquency(cij, ti, wi) 
frequency(ti, wi) ' (6) 
frequency(eli, cik, Ch+lj) (7) 
P(eijleik, C~+lj) ~ frequency(cik, ck+lj) 
The basic statistical model has been applied 
to morpheme/part-of-speech/category 3-tuple. 
Due to the sparseness of the data, we have 
used part-of-speech/category pairs 7 together, 
i.e., collected the frequencies of the categories 
associated with the part-of-speeches assigned to 
the morpheme. Table 1 illustrates the sample 
entries of the category probability database. In 
table, 'nal (fly)' has two categories with 0.6375 
mid 0.3625 probability respectively. Table 2 il- 
lustrates the sample entries of the merge prob- 
ability database using equation 7. 
f requency  (old ,tl ) 7We def ine  th i s  as  P(c l j l t l )  ~ fvcq . . . . . .  y ( tD  " 
Table 3: 
Model 
Results fl'om the Basic Statistical 
Total sentences 
No-crossing 
Ave. crossing 
Labeled Recall 
Labeled Precision 
591 
74.62% 
1.00 
77.02 
79.15 
Figure 1: Sub-constituents for head-head co- 
occurrence heuristics 
Table 3 summarizes the results on an open 
test set of 591 sentences. 
4.2 Head-head co -occur rence  heur i s t i cs  
In the basic statistical model, lexicM depen- 
dencies between morphemes that take part in 
merging process cannot be incorporated into the 
model. When there is a different morpheme 
with the same syntactic category, it can be a 
miss match on merging process. This linfita- 
tion can be overcome through the co-occurrence 
between the head morphemes of left and right 
sub-constituent. 
When B h is a head morphenm of left sub- 
constituent, r is a case relation, C h is a head 
morpheme of right sub-constituent as shown in 
figure 1, head-head co-occurrence heuristics are 
defined by: 
p(B,LI,. ,Ch ) ~ f requency(B h,r, C h) 
frequency(r, C h) " (8) 
Tile head-head co-occurrence heuristics have 
been augmented to equation 5 to model the lex- 
ical co-occurrence preference in category merg- 
ing process. Table 4 illustrates the sample en- 
tries of the co-occurrence probability database. 
In Table 4, a morpheme 'sac (means 'bird')', 
which has a "MCK (common noun)" ms POS 
tag, has been used a nominative of verb 'hal 
(means 'fly')' with 0.8925 probability. 
1004 
Table 1: Sample entries of the category probal)ility database ('DII' Ineans an '1' irregular verb.) 
P()S, morpheme category probability 
DII, nal v\[D\]\ {np\[noln\]} 0.6375 
DI1, hal v\[D\]\{np\[noln\],nl)\[acc\]} 0.362,5 
DI1 v \[D\]  {rip \[nora\] } 0.3079 
DI1 v\[D\]\ {np\[llOm\],np\[acc\] } 0.2020 
Table 2: Sample entries of' syntactic merge probability database 
left; category 
~, / ( ~ \,u,\[,,o,,l\]) 
~,/(~, \ ,p lace\]) 
right category 
v\[D\]\ {np\[noml,np\[acc\]} 
v\[D\]\ {,,p \[,lo,,,\],,u,\[acd } 
inerged category 
v\[D\]\{,,p\[acd} 
v\[D\]\ {ni)\[nonl\] } 
probability 
0.0473 
0.6250 
nl, (v / (v \nont ) ) \n  t , v/(v\np\[nom\]) I).2197 
The modified model has been tested Oil the 
same set of the open sentences as in the 1)asic 
model ext)eriment. 'l~fl)le 5 smnmarizes the re- 
sult of these expcwiments. 
? Ezperimcnt: (linear combination af th, c ba- 
sic model and the head-h, cad co-occurrence 
heuristics). 
P(% s) 
eij { r 
+/~p( \ ] / '  I,,., c*')) 
? P(~,ik)~'(~,k+,;)), (9) 
i < k < j, 
i f  cij is a terminal, 
~J,.,;',~ p(c#i) = P(c.~:i I~g, td. 
Ta,bh; 5: Results from the Basic: Statistical 
Model t)lns head-head co-occurrence heuristics 
Total sentences 591 
No-crossing 81.05% 
Ave. crossing 0.70 
Labeled Recall 84.02 
Labeled Precision 85.30 
4.3 The  coverage heur is t ics  
If" there is a case relation or a modification re- 
lation in two constituents, coverage heuristics 
designate it is easier to add the smaller tree to 
the larger one ttlan to merge the two medium 
sized trees. On the contrary, in the coordination 
relation, it is easier to nmrge two medium sized 
trees. We implemented these heuristics using 
/;tie tbllowing coverage score: 
Case relation, modification relation: 
COV_scorc = 
le f t  subtrec coverage + riqh, t sub/roe coverage. (j_()~ 
4 ? ~7~ ,~,,bt,.~,.,~ o,,,,',,e ;< ','i:jl,,i ~,,b>'.e eo,,~',',,.~,'. " 
Coo~d'iuatio'n: 
COV_sco'rc = 
'e x x / le f t  .~,a,l.,'~c. ~o.,,,.~,.,,.:,.. x ,'#lht .~,O,l,,.,, ,:o.,,~,.,,,~ 1~ 
leJ't subtree cove,.aqe + R~ ~b~r('.e ~; .~t  . 
A coverage heuristics are added to the basic: 
model to model the structural preferences. Ta- 
ble 6 shows the results of the experinlents on 
the same set of the open sentences. 
? Ezpcriment: (the basic model to th, c 
COV_scorc heuristics). We have used (;tie 
COV_.sco're as the exponent weight feature 
for this experiment since the two nmnl)ers 
arc; in the different nature of statistics. 
P(7-, S) = H (P(ciJ\] cik, Ok+l J) l-COV-'sc?rc 
eij CT 
?p(~k)p(c~+,j)), (1~,) 
i<k<j ,  
i f  Cij iS  a terminal, 
o,: , ,  P(~.j) = 1)(c~.jl~,~, ~d. 
1005 
Table 4: Sample entries of co-occurrence probability database. 
head-head co-occurrence probability 
(MCC <ganeungseong>,np\[nom\],HIl.< nob>) 0.8932 
(MCK<sae>,np\[nom\],DIl<nal>) 0.8925 
(MCK<galeuchim>,np\[acc\],DIeu<ddaleu>) 0.8743 
Table 6: Results from the Basic Statistical 
model plus Coverage heuristics 
Total sentences 591 
No-crossing 80.13% 
Ave. crossing 0.81 
Labeled Recall 82.59 
Labeled Precision 83.75 
5 Summary 
We developed a morpho-syntactic categorial 
parser of Korean and devised a morpheme- 
based statistical structural disambiguation 
s(;henles. 
Through the KCCG model, we successthlly 
handled ifficult Korean modeling problems, in- 
chtding relative free-word ordering, coordina- 
tion, and case-marking, during the parsing. 
To extract he most plausible parse trees ti'om 
the parse forest, we have presented basic statis- 
tical techniques using the lexical and contextual 
information such as morpheme-category p oba- 
bility and category merge probability. 
Two different nature of heuristics, head-head 
co-occurrence and coverage scores, are also de- 
veloped and tested to augment the basic statis- 
tical model. Each of them demonstrates reason- 
able t)ertbrmance increase. 
The next step will be to devise more heuristics 
and good combination strategies tbr the differ- 
ent nature of heuristics. 
References 
E. Black, S. Abney, D. Flickenger, C. Gdaniec, 
R. Grishman, P. Harrison, D. Hindle, R. In- 
gria, F. Jelinek, J. Klavans, M. Liberman, 
M. Marcus, S. Roukos, B. Santorini, and 
T. Strzalkowski. 1991. A Procedm'e for 
Quantitatively Comparing the Syntactic Coy- 
erage of English Grammars. In Prec. of 
Fourth DARPA Speech and Natural Lan- 
guage Workshop. 
Jeongwon Cha, Gcunbae Lee, and Jong-Hyeok 
Lee. 1998. Generalized unknown morpheme 
guessing for hybrid pos tagging of korean. 
In Pwceedings of Sixth Workshop on Very 
Large Corpora in Coling-ACL 98, Montreal, 
Canada.  
E. Charniak. 1995. Prsing with Context-Free 
Grammars and Word Statistics. Technical 
Report CS-95-28, Brown University. 
M. Collins. 1996. A New Statistical Parser 
Based on Bigram Lexical Dependencies. In 
Proceedings of th, e 3/tth Annual Meeting of the 
A CL, Santa Cruz. 
D. Hindle and M. Rooth. 1993. Structural am- 
biguity and lexical relations. Computational 
Linguistics, 19(1):103-120. 
B. Hoffman. 1995. ~7~,c Computational Analy- 
sis of the Syntax and Interpretation of 'if;roe" 
Word Order in Turkish. Ph.D. thesis, Univer- 
sity of Pennsylwmia. IRCS Report 95-17. 
Wonil Lee, Gennb:m Lee, and Jong-Hyeok Lee. 
1996. Chart-driven connectionist categorial 
t)arsing of spoken korean. Computer process- 
ing of oriental languages, Vol 10, No 2:147-- 
159. 
D. M. Magerman and M. P. Marcus. 1991. 
Parsing the voyager domain using t)earl. In 
In Prec. Of the DARPA Speech and Natural 
Language Workshop~ pages 231-236. 
D. M. Magerman and C. Weir. 1992. E f  
ficiency, robustness and accuracy in picky 
chart parsing. In In Prec. Of the 30th An- 
nual Meeting of the Assoc. For Computa- 
tional Linfluisties(ACL-92), pages 40 47. 
Mark Steedman. 1985. Dependency and Coor- 
dination in the Grammar of Dutch and En- 
glish. Language, 61:523 568. 
1006 
Dec is ion -Tree  based Error  Cor rec t ion  for Stat is t ica l  Phrase  Break  
Pred ic t ion  in Korean  * 
Byeongchang K im and Geunbae Lee 
Del)artment of Computer  Science & Engineering 
Pohang University of Science & Technology 
Pohang, 790-784, South Korea 
{bckim, gblee}((~postech.ac.kr 
Abst ract  
tn this paper, we present a new 1)hrase break 
prediction architecture that integrates proba- 
bilistic apt)roach with decision-tree based error 
correction. The probabilistic method alone usu- 
ally sufl'crs fronl performance degradation due 
to inherent data sparseness l)rolflems and it only 
covers a limited range of contextual informa- 
tion. Moreover, the module can not utilize the 
selective morpheme tag and relative distance 
to the other phrase breaks. The decision-tree 
based error correction was tightly integrated to 
overt:ohm these limitations. 
The initially phrase break tagged morphcnm se- 
quence is corrected with the error correcting de- 
cision tree which was induced by C4.5 fl'om the 
correctly tagged corpus with the outtmt of the 
15mbabilistic predictor. The decision tree-based 
post error correction l)rovided improved results 
even with the phrase break predictor that has 
l)oor initial performance. Moreover, tim system 
can be flexibly tamed to new corI)uS without 
massive retraining. 
1 I n t roduct ion  
During 1;15(; past thw years, there has l)een a 
great deal of interest in high quality text-to- 
speech (TTS) systelns (van Santen et al, 1997). 
One of the essential prolflenlS ill developing high 
quality TTS systems is to predict phrase breaks 
flora texts. Phrase breaks are especially es- 
sential fbr subsequent processing in the TTS 
systems uch as grapheme-to-iflloneme conver- 
sion and prosodic feature generation. More- 
over, gral)helnes in the phrase,-break bound- 
aries are not phonologically changed and should 
be i)ronommed as their original corresponding 
p honenles. 
There have been two apln'oaches to predict 
phrase breaks (Taylor and Black, 1998). The 
* This paper was supported by the University Research 
Program of the Ministry of Intbrmation & Communica- 
tion in South Korea through the IITA(1998.7-2000.6). 
first: uses some sort of syntactic information to 
In:edict prosodic boundaries based on the fact 
that syntactic structure and prosodic structure 
are co-related. This method needs a reliable 
parser and syntax-to-prosody 1nodule. These 
modules are usnally implemented in rule-driven 
methods, consequently, they are difficult to 
write, modi(y, maintain and adapt to new do- 
mains and languages. Ill addition, a greater use 
of syntactic information will require, more con> 
lmtation for finding n more detailed syntactic 
parse. Considering these shortcomings, the sec- 
ond approach uses some probabilistic methods 
on the crude POS sequence of the text:, and this 
lnethod will be fln:ther developed in this paper. 
However, t:he. probabilistic method alone usu- 
ally sufl'ers front pertbrmance degradation due 
to inherent data sparseness problems. 
So we adopted decision tree-based error COl'- 
re, ction to overconm these training data limi- 
tations. Decision tree induction iv 1;t5(; most 
widely used \]calming reel;hod. Espcci~flly in lla~l;- 
m:al language and speech processing, decision 
tree learning has been apt)lied to many prob- 
h,.nls including stress acquisition fl'om texts, 
gralflmme to phonenm conversion and prosodic 
phrase, modeling (Daelemans et al, 1994) (van 
Santen et al, 1997) (Lee and Oh, 1999). 
In the next section, linguistic fb, atures of Ko- 
rean relevant o phrase break prediction are de- 
scribed. Section 3 presents the probabilistic 
phrase break prediction method and the tree- 
based error correction method. Section 4 shows 
experimental results to demonstrate he t>erfor - 
mam:e of the method and section 5 draws st)me 
conclusions. 
2 Features  o f  Korean  
This section brMly explains the linguistic char- 
acterists of spoken Korean before describing the 
phrase break prediction. 
1) A Korean word consists of more than 
one morpheme with clear-cut morphenm bound- 
aries (Korean is all agglutinative language). 
1051 
2) Korean is a postpositional language with 
many kinds of noun-endings, verb-endings, and 
prefinal verb-endings. These functional mor- 
phemes determine a noun's case roles, a verb's 
tenses, modals, and modification relations be- 
twcen words. 3) Korean is basically an SOV 
language but has relatively free word order 
compared to other rigid word-order languages 
such as English ,except br the constraints that 
the verb must appear in a sentence-final posi- 
tion. However, in Korean, some word-order con- 
straints actually do exist such that the auxiliary 
verbs representing modalities must follow the 
main verb, and modifiers must be placed betbre 
the word (called head) they modify. 4) Phono- 
logical changes can occur in a morpheme, be- 
tween morphemes in a word, and even between 
words in a phrase, but not between phrases. 
3 Hybr id  Phrase  Break  Detect ion  
Part-of speech (POS) tagging is a basic step 
to phrase break prediction. POS tagging sys- 
tems have to handle out-of vocabulary (OOV) 
words for an unlimited vocabulary TTS sys- 
tem. Figure 1 shows the architecture of our 
phrase break predictor integrated with the POS 
tagging system. The POS tagging system era- 
ploys generalized OOV word handling mecha- 
nisms in the morphological analysis and cas- 
cades statistical and rule-based approaches in 
the two-phase training architecture tbr POS dis- 
ambiguation. 
Morphological !;i 
I analyzc r~ 
{M~pl me ..... ~se~i~ ': 
~, :  . / Protmbilistlc I rtgram ~z'~*t  $ 
I 
Figure 1: Architecture of the hybrid phrase 
break prediction. 
Tire probabilistic phrase break predictor seg- 
ments the POS sequences into several phrases 
according to word trigram probabilities. Tire 
irdtial phrase break tagged morpheme sequence 
is corrected with the error correcting tree 
learned by the C4.5 (Quinlan, 1983). 
Tire next two subsections will give detailed 
descriptions of the probabilistic phrase predic- 
tion and error correcting tree learning. The hy- 
brid POS tagging system will not l)e explained 
in this paper, and the interested readers can see 
(Cha et al, 1998) tbr further reference. 
3.1 Probabi l i s t ie  Phrase Break 
Detect ion  
3.1.1 Probabi l l s t ie  Models  
For phrase break prediction, we develop tire 
word POS tag trigrmn model. Some experi- 
ments are performed on all the possible trigram 
sequences and 'word-tag word-tag break word- 
tag' sequence turns out to be the most fl'uitful 
of any others, which are the same results as the 
previous tudies in English (Sanders, 1995). 
The probability of a phrase break bi appear- 
ing after the second word POS tag is given by 
P(bilt?,2ta) = C(tlt2bit3) 
Ej=o,~,2 C(ht2b j t3)  '
where C is a frequency count flmction and b0, 
bl and b2 mean no break, minor break and ma- 
jor break, respectively. Even with a large num- 
ber of training patterns it is very clear that 
there will be a number of word POS tag se- 
quences that never occur or occur only once in 
the training corpus. One solution to this data 
sparseness problem is to smooth the probabili- 
ties by using the bigram and unigram probabil- 
ities, which adjusts the fl'equency counts of rare 
or non-occurring POS tag sequences. We use 
the smoothed probabilities: 
P(biltlt,2ta) = )q C(trt2bit3) 
~j=o,l,2 C(t~ t2bjt3) 
C(t2bita) 
q- A2 ~j=0,1,2 C(t2bjt3) 
+ C(t2bi) 
)t3 j=_0,1,2 C( 2bj) ' 
where )~1, A2 and ),a are three nommgative con- 
stants such that h I q- ~2 Jr- ~3 = 1. In some 
experiments, we can get the weights ~1, ~2 and 
A3 as 0.2, 0.7 and 0.1, respectively. 
3.1.2 Adjust ing the POS Tag 
Sequences of  Words 
Previous researchers of phrase break predic- 
tion used mainly content-flnmtion word rule, 
wherel)y a phrase break is placed before every 
flmction word that follows a content word (Allen 
and Hmmicut, 1987) (Taylor et al, 1991). The 
1052 
researchers used tag set size of only 3, including 
function, content ~md t)lmctuation i  the rule. 
However, Korean is a post-positional aggln- 
tinative language. If the eontent-t'unction word 
rule is to be adapted in Korean, the rule nmst 
be changed so that a phrase break is placed 
before every content mort/henm that R)llows a 
fimction morl)heme. Unfortunately this rule 
is very inet\[icient in Korean since it tends to 
create too many pauses. In our works, only 
the POS tags of Nnction mort)heroes are used 
be, cause the function morphelnes constrain the 
classes of precedent n:orpheanes and t)b\y impor- 
tant roles in syntactic relation. So, each word 
is represented by the I?OS tag of its fimction 
morpheme. In the case of the word which has 
no function mort)heine , simplified POS tags of 
content mort)henms are used. The nmnber of 
POS tags use, d in this rese, m'ch is a2. 
3.2 Dec is ion -Tree  Based  Error  
Correct ion  
The t)robabilistic phrase break prediction only 
covers a limited range of contextual infornm- 
tion, i.e. two preceding words and one. follow- 
ing word. Moreove, r the module can not utilize 
the morl)heme tag se.lectively and relative dis- 
tance to the other phrase breaks. For this rea- 
son we designed error correcting tree to con> 
pensate for |;tie limitations of the. probal)ilistic 
phrase break prediction. However, designing er- 
ror corre, cting rules with knowledge ngineering 
is te, dious and error-prone,, lTnstead, we, adopte, d 
decision tree learning ai)proa(:h to auton~atically 
learn the error correcting rules froln a correctly 
t:,hrase break tagged eorlms. 
Most algorithms th:~t have t)een develope, d 
for lmilding decision trees employ a top-down, 
greedy search through the space of possible deci- 
sion trees (Mitchell, 1997). The 04.5 (Quinlan, 
1983) is adequate to buiht a decision tree easily 
for successively dividing the regions of feature 
vector to minimize the prediction error. It also 
uses intbrmation gain which lneasures how well 
a given attril)ute separates the training vectors 
according to their target classification in order 
to select he most critical attrilmtes at each step 
while growing |;tit tree (hence the nmne is IG- 
~l'ree). Now, we utilize it for correcting the ini- 
tially phrase break tagged POS tag sequences 
generated by probabilistic predictor. 
However, wc invented novel way of using the 
decision tree as trmlsibrmation-t)ased rule in- 
duction (Brill, 1992). l?igure, 2 shows the tree 
learning architecture tbr phrase break error cor- 
rection. The initial phrase break tagged POS 
tag sequences upport the ti;ature vectors tbr 
attributes which are used tbr decision mak- 
ing. Because the ii;atm'e vectors include phrase 
break sequences as well as POS tag sequences, 
a learned decision tree can check |;lie morphenm 
tag selectively and utilize |;lie relative distanee 
to the other phrase breaks. The correctly phrase 
break tagged POS tag sequences upport the 
classes into which the feature vectors are classi- 
fied. C4.5 lmilds a decision tree fl'om the t)airs 
which consist of the feature vectors and their 
classes. 
. . . .  I Prol,al,ilislic 11 
l'I,rase l,reak ta~ged 
POS tag seque nee 
. . . . .  Correctly plmlse break tagged 
~-~\ ] : :  ........ ...... POS tag sequence  
ErrOr Correcting decision tree 
Figure 2: Architecture of the error correcting 
decision tree learner. 
4 Exper imenta l  Resu l ts  
4.1 Corpus  
The, experiments are t)ertbrmed on a Korean 
news story database,, called MBCNF, WS\])I~, of 
spoken Korean directly recorded f rom broad- 
casting news. The size of th(; database is now 
6,111 sentences (75,647 words) and it is eontin- 
nously growing. '12) lm used in the phrase break 
prediction experiments, |;tie database has been 
POS tagged and break-b~beled with major and 
minor phrase breaks. 
4.2 Phrase  Break  Detect ion  and Error  
Correct ion  
We, I)eribrmed three experiments o show syner- 
gistic results of probabilistic method and tree- 
based error correction method. First, only prob- 
abilistic method was used to predict phrase 
breaks. %'igrams, bigrams and unigrams for 
phrase break prediction were trained fl:om the 
break-labeled an(1 POS tagged 5,492 sentences 
of the MBCNEWSDB by adjusting the POS 
sequences of words as described in sut)section 
3.1.2. The other 619 sentences are used to 
test the t)ertbrnum(:e of the probabilistic I)hrase 
break predictor. In the second experiment, we 
made a decision tree, which can be used only 
to predict phrase breaks and cannot be used to 
1053 
correct phrase breaks, from the 5,429 sentences. 
Also the 619 sentences were used to test the 
performance of the decision tree-based phrase 
break predictor. The size of feature vector (the 
size of the window) is w~ried fi'om 7 (the POS 
tag of current word, preceding 3 words and fol- 
lowing 3 words) to 15 (the POS tag of current 
word, preceding 7 words and following 7 words). 
The third experiment utilized a decision tree as 
post error corrector as presented in this paper. 
We trained trigrams, bigrams and unigrams us- 
ing 60% of totM sentences, and learned the deci- 
sion tree using 3(1% of total sentences. For the 
other experiment, 50% aim 40% of total sen- 
fences are used tbr probability training and tbr 
decision tree learning, respectively. Tim other 
10% of total sentences were used to test as in 
the prevkms ext)eriments(Figure 3). For the de- 
cision tree in the tlfird experiment, hough the 
size of the window is also varied from 7 words 
to 15 words, the size of feature vector is varied 
from 14 to 30 because phrase breaks tagged by 
probabilistic predictor are include in the feature 
vector. 
El I :oI  prababililies trai,ling D For decision Iree induction \[3 I:(!r lest 
(}9; . . . . .  
PI ol~:lbilislic Iil~,lhod 
only 
\[ 
\]GITtee mlly Prolmbilisfic ii~01\]lod Pmballilistic iii0thod 
lind post error ~tlltl IX)st ?IlOl 
cDrl ocliOll(6:3 ) t'olteclJoll(4:5 ) 
Fignre 3: The number of sentences for the prob- 
ability training, the decision tree learning and 
the test in the experiments. 
Tit(; performance is assessed with reference to 
N, the total number of junctures (spaces in text 
including any type of phrase breaks), and B, the 
total number of phrase breaks (only minor(b1) 
and major(b,)) breaks) in the test set. The er- 
rors can be divided into insertions, deletions and 
substitutions. An insertion (I) is a break in- 
serted in the test sentence, where there is not a 
break in the reference sentence. A deletion (D) 
occurs when a break is marked in the rethrence 
sentence but not in the test sentence. A substi- 
tution (S) is an error between major break and 
minor break or vice versa. Since there is no sin- 
gle way to measure the performance of phrase 
break prediction, we use the following peribr- 
mance measures (Taylor and Black, 1998). 
Break_Cor rect  - 
B -D-S  
B 
x 100%, 
N - D - S - I  
Juncture_Cor rect  = x 100% 
N 
We use another pertbrmance nmasure, cMled 
adjusted score, which refer to the prediction ac- 
curacy in proportion to the total nmnber of 
phrase breaks as following performance measure 
proposed by Sanders (Sanders, 1995). 
Adjus ted_Score  -
, IC  - NB 
1-N I3  ' 
where NB 1 means the proportion of no 
breaks to the number of interword spaces and 
, lC  means the Juncture_Correct / lO0.  
Table 1. shows the experimental results of our 
phrase break prediction and error con:ection 
method on the 619 open test sentences (10% 
of the total corpus). In the table, W means the 
thature vector size tbr the decision tree, and 6:3 
and 4:5 mean ratio of the number of sentences 
used in the probabilistic train and the decision 
tree induction. 
The performance of probabilistic method is 
better than that of IG-tree method with any 
window size in U'reak_Cor'rect.  However, as the 
ti;ature vector size is growing in IO-tree method, 
.lv, nctv, re_Co'rrect  and Ad j , ( s led_Score  become 
better than those of the l)robM)ilitic method. 
From the fact that the attribute located in the 
first level of the decision trees is the POS tag of 
preceding word, we can see that the POS tag of 
preceding word iv the most useful attribute for 
predicting phrase breaks. 
The pertbrmance before the error correction 
in hyl)rid experiments iv worse ttlan that of the 
original 1)robabilistic method because the size of 
training corlms for probabilistic method is only 
66.6% and 44.4:% of that of the original one, re- 
spectively. However, the performance sets im- 
proved by the post error correction tree, and be- 
corns finally higher than that of both the prob- 
abilistic nmthod and the IG-tree method. The 
attribute located in the first level of the decision 
tree is the phrase break that was predicted in 
the probatfilistic method phase. Although the 
initial pertbrmmme (beibre error correction) of 
the exImriment using 4:5 corpus ratio is worse 
than that of the experiment using 6:3 corlms 
ratio, the final perfbrmance gets impressively 
improved as the decision tree induction corpus 
1NB _ N- I~ 
N 
1054 
'li~l)l( ~, 1: 1)hrase break t)\]e{ti(:tion ~md error eorr{',ction results. 
\]3r(;~k_Correet 
1}rol)al)ilis|;ic method only 
IG-Tree only 
Prol)al)ilisl;ie 
mel;ho(t 6:3 
~I~15(I 
l)ost error 
eorr(~,e|; iol l  
4:5 
W=7 
W- 11 
W= 15 
l)efore error eorre{;t;iol~ 
W=7 
W=l l  
W= 15 
1)~;~, ore, error con'e(;tion 
W=7 
W- - l l  
W- -15  
52.17% 
5O.58% 
51.66% 
51.77% 
52.03% 
57.34{~) 
59.8O% 
60.75% 
51.30% 
59.O4% 
61.83% 
62.7/1% 
J un{:l;{~re_CorreeI; 
81.39% 
81.39% 
81.65% 
81.71.% 
81.29~/~0 
83.67% 
84.69% 
85.O6% 
80.85'~ 
84.42% 
85.16% 
85.57% 
Adj us te ( l _ -~ 
0.48O 
0.480 
0,487 
0.488 
0.477 
0.543 
0.572 
0.582 
0.465 
0.564: 
0.585 
0.597 
incre,~ses from 30% 1;o 50% of the to|;al (:()rims. 
~l)his result; shows t;h~t |;he prol)osed ~rehilx~e- 
ture c~m 1)rovi(te, improved results evell with the 
phrase 1)re~k \])re(tie|:or |:h~l; h~s I)oor |nit|a,1 per- 
f()z'51 I~L51(;(~,. 
5 Conc lus ion 
This t)~l)er l)r{;s(:nts ;~ new 1)hr;tse t)rea.k predic- 
|;ion ~rt:hil;eel;ure l;h~l; inl;egr~tes the t}rob~bilis- 
tie ;~t)t)ro~eh wii;h the (le(:ision-tree 1)~s{'(t ~tl)- 
t)ro~teh in ;~ synergistic w~y. Our m~in contl'it)u- 
|,ions include presenl;ing (leeision |;ree-1);~se(1 r-
ror correction for 1)hr~se t)re~k prediel;ion. Also, 
i)rol)~fl)ilistic t)hrase break prediction w~ts im- 
t)leme, nt;e(l a.s ;I,51 inil:i~tl am~ot~l;or f the (tet:i- 
si()n tree-t)~tse(t e, rror (:orre('tiolL The m:ehite,('- 
ture ('~m t)rovi(te imt)l"ove(t results even with l;he 
1)\]u:;~se t)re;~k l)redi{:tor |;h;t|; ha,s l)o{)r inil;i~d t}er- 
t'orm~mee. Moreover, I;he, syslxun (:~m 1)e ttexit)ly 
t;u, ned t;o new eort)us wil;houl; nmssive rel;r;~ining 
which is necess~ry in the t)rol}~bilisti(: metho(t. 
As shown in the result, t)erli)rmamce, of the hy- 
t)rid t)hr~se 1)re~k t)redietion is dctermilmd t)y 
how well the error eorre('|;or ( ; t in  (;Ollll)ellS;~l;e 
i;he defi(:iencies of the 1)rol)~d)ilistie t)hrase 1)re~k 
predict;ion. 
The next sl;el) will 1)e to :m~lyze the le~rned 
de(:ision trees eareflflly I;() exi;r;~(:l; more desir~t)le 
ti',~tl;ur(', veel;ors. W(', ~re now working 055 in(:or- 
l)or~|;ing this i)hl'~se, break 1)redi('tion 5he|hod 
into the ext)erimenl;~l Korean 'I?'\]?S sysl;em. 
References  
J. Alle, n ~md S. Humli{'ut. 1987. l'"ro~tt !l'ext to 
St)eech: the: MITalk  Systcnt. Cmnl)ridge Uni- 
versity Press. 
IB. \]\]rill. 1992. A simple rule-based p~rl;-ot/- 
speech t~gg{'r. In \]}roccediT~,.qs of the co~:fer- 
c'~tcc o~, applied ~,at'u,r'rd la~zg'aage processi~zg. 
.\]eos\]gwon Ch~, Oeunbae \]~(;e, ~md Jong-Hyeok 
Lee. :1998. Ge, nc, r~dized mlknown morphelne 
guessing for hybrid P()S l;~tgging of Kor(';m. 
In P'lw('ecdi'l~,gs of th.(: Sizth, Wo'r~:.sh, op o'l~, l&-"cq 
ha'rqc Co'rpora, t}~g('s 85 93. 
Dvr~llx'r l)a(;lem~ms, St;ev{'n Gills, ~md Gerl; 
\])urieux. 1994. 'l'he ~cquisil;ion of sl;ress: A 
d~l;a-orienl;ed ~l)l)ro~{:h. Co~tI)~tl, al, io'l~,al Liw,- 
g'ltil, ics, 20(3):421 451. 
Sangho Lee ;rod Yung-Itw~m O15. 1999. ~.lS:ee- 
t)~rse(t mo(l(;ling of 1)roso(tie 1)hr,(sing ~1~(t 
segmei~l;~tl ( m:a.t;ion for kore~m |.t.q ,qy,qJ;elll,q. 
,~peech, Co'll~,~n,'w~,icatio~,, 28(4):283 300. 
q'om M. Mil;ehell. 1!)97. Mach, i~,c Lea'~"H,i~,g. 
MeGr~w-Ilill. 
J. l/,. {~uinDm. 1.983. C~.5: l'ro!tr(~ms fo'~" M(z- 
ch, i'~,e Lea'r'~,i'~ 9. Morgan K~mflnlmn. 
\]Brie S~mtlers. 1.9!t5. Using prot}al)ilistic mel:h- 
()<Is |;o t)re(lict phr~se t)oundaries for ~t text- 
to-sl)eeeh system. Master's thesis, University 
of Nijmegen. 
P~ml T~,ylor ;md Alum W. BD~t:k. 1!)98. As- 
signing phrase l)rc,~ks kom 1)ar|;-of-st)e, eeh sc,- 
qllelIces. Cotlt\])'lttcr Speech (t~td La~tg'~t(,,gc, 
12(2):9!} 1~7. 
lhml A. 'l?aylor, I. A. N~irn, A. M. fiutherl;md, 
~md M. A..l~ck. 1991. A re~l time speech 
synthesis ystem. 1151 l~rot:ce, dirt.q.s of th, c \]~'~t- 
rospecch, '9.l. 
,l~m P.II. wm Santen, l/.iehard W. Sl)roat, 
3osel)h P. Olive, ~md .luli~ Hirsehl)erg. 
1997. l}rog'r(;ss in Speech Sy~,th, esis. Springer- 
VerLzg. 
1055 
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1250?1259,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Semi-supervised Speech Act Recognition in Emails and Forums
Minwoo Jeong
??
Chin-Yew Lin
?
Gary Geunbae Lee
?
?
Pohang University of Science & Technology, Pohang, Korea
?
Microsoft Research Asia, Beijing, China
?
{stardust,gblee}@postech.ac.kr
?
cyl@microsoft.com
Abstract
In this paper, we present a semi-supervised
method for automatic speech act recogni-
tion in email and forums. The major chal-
lenge of this task is due to lack of labeled
data in these two genres. Our method
leverages labeled data in the Switchboard-
DAMSL and the Meeting Recorder Dia-
log Act database and applies simple do-
main adaptation techniques over a large
amount of unlabeled email and forum data
to address this problem. Our method uses
automatically extracted features such as
phrases and dependency trees, called sub-
tree features, for semi-supervised learn-
ing. Empirical results demonstrate that
our model is effective in email and forum
speech act recognition.
1 Introduction
Email and online forums are important social me-
dia. For example, thousands of emails and posts
are created daily in online communities, e.g.,
Usenet newsgroups or the TripAdvisor travel fo-
rum
1
, in which users interact with each other us-
ing emails/posts in complicated ways in discus-
sion threads. To uncover the rich interactions in
these email exchanges and forum discussions, we
propose to apply speech act recognition to email
and forum threads.
Despite extensive studies of speech act recogni-
tion in many areas, developing speech act recogni-
tion for online forms of conversation is very chal-
lenging. A major challenge is that emails and
forums usually have no labeled data for training
statistical speech act recognizers. Fortunately, la-
beled speech act data are available in other do-
mains (i.e., telephone and meeting conversations
?
This work was conducted during the author?s internship
at Microsoft Research Asia.
1
http://tripadvisor.com/
in this paper) and large unlabeled data sets can be
collected from the Web. Thus, we focus on the
problem of how to accurately recognize speech
acts in emails and forums by making maximum
use of data from existing resources.
Recently, there are increasing interests in
speech act recognition of online text-based con-
versations. Analysis of speech acts for online
chat and instant messages and have been studied
in computer-mediated communication (CMC) and
distance learning (Twitchell et al, 2004; Nastri et
al., 2006; Ros?e et al, 2008). In natural language
processing, Cohen et al (2004) and Feng et al
(2006) used speech acts to capture the intentional
focus of emails and discussion boards. However,
they assume that enough labeled data are available
for developing speech act recognition models.
A main contribution of this paper is that we ad-
dress the problem of learning speech act recog-
nition in a semi-supervised way. To our knowl-
edge, this is the first use of semi-supervised speech
act recognition in emails and online forums. To
do this, we make use of labeled data from spo-
ken conversations (Jurafsky et al, 1997; Dhillon
et al, 2004). A second contribution is that our
model learns subtree features that constitute dis-
criminative patterns: for example, variable length
n-grams and partial dependency structures. There-
fore, our model can capture both local features
such as n-grams and non-local dependencies. In
this paper, we extend subtree pattern mining to the
semi-supervised learning problem.
This paper is structured as follows. Section 2
reviews prior work on speech act recognition and
Section 3 presents the problem statement and our
data sets. Section 4 describes a supervised method
of learning subtree features that shows the effec-
tiveness of subtree features on labeled data sets.
Section 5 proposes semi-supervised learning tech-
niques for speech act recognition and Section 6
demonstrates our method applied to email and on-
1250
line forum thread data. Section 7 concludes this
paper with future work.
2 Related Work
Speech act theory is fundamental to many stud-
ies in discourse analysis and pragmatics (Austin,
1962; Searle, 1969). A speech act is an illo-
cutionary act of conversation and reflects shal-
low discourse structures of language. Recent re-
search on spoken dialog processing has investi-
gated computational speech act models of human-
human and human-computer conversations (Stol-
cke et al, 2000) and applications of these mod-
els to CMC and distance learning (Twitchell et al,
2004; Nastri et al, 2006; Ros?e et al, 2008).
Our work in this paper is closely related to prior
work on email and forum speech act recognition.
Cohen et al (2004) proposed the notion of ?email
speech act? for classifying the intent of an email
sender. They defined verb and noun categories
for email speech acts and used supervised learn-
ing to recognize them. Feng et al (2006) pre-
sented a method of detecting conversation focus
based on the speech acts of messages in discus-
sion boards. Extending Feng et al (2006)?s work,
Ravi and Kim (2007) applied speech act classifi-
cation to detect unanswered questions. However,
none of these studies have focused on the semi-
supervised speech act recognition problem and ex-
amined their methods across different genres.
The speech processing community frequently
employs two large-scale corpora for speech act
annotation: Switchboard-DAMSL (SWBD) and
Meeting Recorder Dialog Act (MRDA). SWBD is
an annotation scheme and collection of labeled di-
alog act
2
data for telephone conversations (Juraf-
sky et al, 1997). The main purpose of SWBD is
to acquire stochastic discourse grammars for train-
ing better language models for automatic speech
recognition. More recently, an MRDA corpus has
been adapted from SWBD but its tag set for la-
beling meetings has been modified to better reflect
the types of interaction in multi-party face-to-face
meetings (Dhillon et al, 2004). These two corpora
have been extensively studied, e.g., (Stolcke et al,
2000; Ang et al, 2005; Galley et al, 2004). We
also use these for our experiments.
2
A dialog act is the meaning of an utterance at the level
of illocutionary force (Austin, 1962), and broadly covers the
speech act and adjacency pair (Stolcke et al, 2000). In this
paper, we use only the term ?speech act? for clarity.
This paper focuses on the problem of semi-
supervised speech act recognition. The goal of
semi-supervised learning techniques is to use aux-
iliary data to improve a model?s capability to rec-
ognize speech acts. The approach in Tur et al
(2005) presented semi-supervised learning to em-
ploy auxiliary unlabeled data in call classification,
and is closely related to our work. However, our
approach uses the most discriminative subtree fea-
tures, which is particularly attractive for reducing
the model?s size. Our problem setting is closely re-
lated to the domain adaptation problem (Ando and
Zhang, 2005), i.e., we seek to obtain a model that
analyzes target domains (emails and forums) by
adapting a method that analyzes source domains
(SWBD and MRDA). Recently, this type of do-
main adaptation has become an important topic in
natural language processing.
3 Problem Definition
3.1 Problem Statement
We define speech act recognition to be the task
that, given a sentence, maps it to one of the speech
act types. Figure 1 shows two examples of our
email and forum speech act recognition. E1?6 are
all sentences in an email message. F1?3, F4?5,
and F6 are three posts in a forum thread. A sen-
tence interacts alone or with others, for example,
F6 agrees with the previous post (F4?5). To gain
insight into our work, it is useful to consider that
E2, 3 and F1, 4, 6 are summaries of two dis-
courses. In particular, F1 denotes a question and
F4 and F6 are corresponding answers. More re-
cently, using speech acts has become an appealing
approach in summarizing the discussions (Galley
et al, 2004; McKeown et al, 2007).
Next, we define speech act category based on
MRDA. Dhillon et al (2004) included definitions
of speech acts for colloquial style interactions
(e.g., backchannel, disruption, and floorgrabber),
but these are not applicable in emails and forums.
After removing these categories, we define 12 tags
(Table 1). Dhillon et al (2004) provides detailed
descriptions of each tag. We note that our tag set
definition is different from (Cohen et al, 2004;
Feng et al, 2006; Ravi and Kim, 2007) for two
reasons. First, prior work primarily interested in
the domain-specific speech acts, but our work use
domain-independent speech act tags. Second, we
focus on speech act recognition on the sentence-
level.
1251
E1: I am planning my schedule at CHI 2003 (http://www.chi2003.org/) S
E2: - will there be anything happening at the conference related to this W3C User interest group? QY
E3: I do not see anything on the program yet, but I suspect we could at least have an informal SIG S
E4: - a chance to meet others and bring someone like me up to speed on what is happening. S
E5: There will be many competing activities, so the sooner we can set this up the more likely I can attend. S
E6: Keith S
F1: If given a choice, should I choose Huangpu area, or should I choose Pudong area? QR
F2: Both location are separated by a Huangpu river, not sure which area is more convenient for sight seeing? QW
F3: Thanks in advance for reply! P
F4: Stay on the Puxi side of the Huangpu river and visit the Pudong side by the incredible tourist tunnel. AC
F5: If you stay on the Pudong side add half an hour to visit the majority of the tourist attractions. S
F6: I definitely agree with previous post. AA
Figure 1: Examples of speech act recognition in emails and online forums. Tags are defined in Table 1.
Table 1: Tags used to describe components of
speech acts
Tag Description
A Accept response
AA Acknowledge and appreciate
AC Action motivator
P Polite mechanism
QH Rhetorical question
QO Open-ended question
QR Or/or-clause question
QW Wh-question
QY Yes-no question
R Reject response
S Statement
U Uncertain response
The goal of semi-supervised speech act recogni-
tion is to learn a classifier using both labeled and
unlabeled data. We formally define our problem
as follows. Let x = {x
j
} be a forest, i.e., a set of
trees that represents a natural language structure,
for example, a sequence of words and a depen-
dency parse tree. We will describe this in more
detail in Section 4. Let y be a speech act. Then,
we define D
L
= {x
i
, y
i
}
n
i=1
as the set of labeled
training data, and D
U
= {x
i
}
l
i=n+1
as the set of
unlabeled training data where l = n+m and m is
the number of unlabeled data instances. Our goal
is to find a learning method to minimize the clas-
sification errors in D
L
and D
U
.
3.2 Data Preparation
In this paper, we separate labeled (D
L
) and un-
labeled data (D
U
). First we use SWBD
3
and
MRDA
4
as our labeled data. We automatically
3
LDC Catalog No. LDC97S62
4
http://www.icsi.berkeley.edu/?ees/dadb/
map original annotations in SWBD and MRDA to
one of the 12 speech acts.
5
Inter-annotator agree-
ment ? in both data sets is ? 0.8 (Jurafsky et al,
1997; Dhillon et al, 2004). For evaluation pur-
poses, we divide labeled data into three sets: train-
ing, development, and evaluation sets (Table 2).
Of the 1,155 available conversations in the SWBD
corpus, we use 855 for training, 100 for devel-
opment, and 200 for evaluation. Among the 75
available meetings in the MRDA corpus, we ex-
clude two meetings of different natures (btr001
and btr002). Of the remaining meetings, we use
59 for training, 6 for development, and 8 for eval-
uation. Then we merge multi-segments utterances
that belong to the same speaker and then divide all
data sets into sentences.
As stated earlier, our unlabeled data consists
of email (EMAIL) and online forum (FORUM)
data. For the EMAIL set, we selected 22,391
emails from Enron data
6
(discussion threads,
all documents, and calendar folders). For the FO-
RUM set, we crawled 11,602 threads and 55,743
posts from the TripAdvisor travel forum site (Bei-
jing, Shanghai, and Hongkong forums). As our
evaluation sets, we used 40 email threads of the
BC3 corpus
7
for EMAIL and 100 threads selected
from the same travel forum site for FORUM. Ev-
ery sentences was automatically segmented by the
MSRA sentence boundary detector (Table 2). An-
notation was performed by two human annotators,
and inter-annotator agreements were ? = 0.79 for
EMAIL and ? = 0.73 for FORUM.
Overall performance of automatic evaluation
measures usually depends on the distribution of
tags. In both labeled and unlabeled sets, the most
5
Our mapping tables are available at
http://home.postech.ac.kr/?stardust/acl09/.
6
http://www.cs.cmu.edu/?enron/
7
http://www.cs.ubc.ca/nest/lci/bc3.html
1252
Table 2: Number of sentences in labeled and unlabeled data
Set SWBD MRDA
Training 96,553 50,865
Development 12,299 8,366
Evaluation 24,264 10,492
Set EMAIL FORUM
Unlabeled 122,125 297,017
Evaluation 2,267 3,711
Figure 2: Distribution of speech acts in the evaluation sets. Tags are defined in Table 1.
frequent tag is the statement (S) tag (Figure 2).
Distributions of tags are similar in training and de-
velopment sets of SWBD and MRDA.
4 Speech Act Recognition
Previous work in speech act recognition used a
large set of lexical features, e.g., bag-of-words,
bigrams and trigrams (Stolcke et al, 2000; Co-
hen et al, 2004; Ang et al, 2005; Ravi and Kim,
2007). However, these methods create a large
number of lexical features that might not be nec-
essary for speech act identification. For example,
a Wh-question ?What site should we use to book a
Beijing-Chonqing flight?? can be predicted by two
discriminative features, ?(<s>, WRB) ? QW?
and ?(?, </s>) ? QW? where <s> and </s>
are sentence start and end symbols, and WRB is
a part-of-speech tag that denotes a Wh-adverb.
In addition, useful features could be of various
lengths, i.e. not fixed length n-grams, and non-
adjacent. One key idea of this paper is a novel use
of subtree features to model these for speech act
recognition.
4.1 Exploiting Subtree Features
To exploit subtree features in our model, we use
a subtree pattern mining method proposed by
Kudo and Matsumoto (2004). We briefly intro-
duce this algorithm here. In Section 3.1, we de-
fined x = {x
j
} as the forest that is a set of trees.
More precisely, x
j
is a labeled ordered tree where
each node has its own label and is ordered left-
to-right. Several types of labeled ordered trees
Figure 3: Representations of tree: (a) bag-of-
words, (b) n-gram, (c) word pair, and (d) depen-
dency tree. A node denotes a word and a directed
edge indicates a parent-and-child relationship.
are possible (Figure 3). Note that S-expression
can be used instead for computation, for example
(a(b(c(d)))) for the n-gram (Figure 3(b)).
Moreover, we employ a combination of multiple
trees as the input of the subtree pattern mining al-
gorithm.
We extract subtree features from the forest set
{x
i
}. A subtree t is a tree if t ? x. For exam-
ple, (a), (a(b)), and (b(c(d))) are subtrees
of Figure 3(b). We define the subtree feature as a
weak learner:
f(y, t,x) ,
{
+y t ? x,
?y otherwise,
(1)
where we assume a binary case y ? Y =
{+1,?1} for simplicity. Even though the ap-
proach in Kudo and Matsumoto (2004) and ours
are simiar, there are two clear distinctions. First,
our method employs multiple tree structures, and
uses different constraints to generate subtree can-
didates. In this paper, we only restrict generating
1253
the dependency subtrees which should have 3 or
more nodes. Second, our method is of interest
for semi-supervised learning problems. To learn
subtree features, Kudo and Matsumoto (2004) as-
sumed supervised data {(x
i
, y
i
)}. Here, we de-
scribe the supervised learning method and will de-
scribe our semi-supervised method in Section 5.
4.2 Supervised Boosting Learning
Given training examples, we construct a ensem-
ble learner F (x) =
?
k
?
k
f(y
k
, t
k
,x), where ?
k
is a coefficient for linear combination. A final
classifier h(x) can be derived from the ensemble
learner, i.e., h(x) , sgn (F (x)). As an optimiza-
tion framework (Mason et al, 2000), the objective
of boosting learning is to find F such that the cost
of functional
C(F ) =
?
i?D
?
i
C[y
i
F (x
i
)] (2)
is minimized for some non-negative and monoton-
ically decreasing cost function C : R ? R and
the weight ?
i
? R
+
. In this paper, we use the
AdaBoost algorithm (Schapire and Singer, 1999);
thus the cost function is defined as C(z) = e
?z
.
Constructing an ensemble learner requires that
the user choose a base learner, f(y, t,x), to
maximize the inner product ???C(F ), f? (Ma-
son et al, 2000). Finding f(y, t,x) to maxi-
mize ???C(F ), f? is equivalent to searching for
f(y, t,x) to minimize 2
?
i:f(y,t,x
i
)6=y
i
w
i
? 1,
where w
i
for i ? D
L
, is the empirical data dis-
tribution w
(k)
i
at step k. It is defined as:
w
(k)
i
= ?
i
? e
?y
i
F (x
i
)
. (3)
From Eq. 3, a proper base learner (i.e., subtree)
can be found by maximizing weighted gain, where
gain(t, y) =
?
i?D
L
y
i
w
i
f(y, t,x
i
). (4)
Thus, subtree mining is formulated as the prob-
lem of finding (
?
t, y?) = argmax
(t,y)?X?Y
gain(t, y). We
need to search with respect to a non-monotonic
score function (Eq. 4), thus we use the monotonic
bound, gain(t, y) ? ?(t), where
?(t) =max
?
?
2
?
w
i
{i|y
i
=+1,t?x
i
}
?
n
?
i=1
y
i
f(y, t,x
i
),
2
?
w
i
{i|y
i
=?1,t?x
i
}
+
n
?
i=1
y
i
f(y, t,x
i
)
?
?
. (5)
Table 3: Result of supervised learning experiment;
columns are micro-averaged F
1
score with macro-
averaged F
1
score in parentheses. MAXENT:
maximum entropy model; BOW: bag-of-words
model; NGRAM: n-gram model; +POSTAG,
+DEPTREE, +SPEAKER indicate that the com-
ponents were added individually onto NGRAM.
?
?
? indicates results significantly better than the
NGRAM model (p < 0.001).
Model SWBD MRDA
MAXENT 92.76 (63.54) 82.48 (57.19)
BOW 91.32 (54.47) 82.17 (55.42)
NGRAM 92.60 (58.43) 83.30 (57.53)
+POSTAG 92.69 (60.07) 83.60 (58.46)
+DEPTREE 92.67 (61.75)
?
83.57 (57.45)
+SPEAKER
?
92.86 (63.13) 83.40 (58.20)
ALL
?
92.87 (63.77) 83.49 (59.04)
The subtree set is efficiently enumerated using a
branch-and-bound procedure based on ?(t) (Kudo
and Matsumoto, 2004).
After finding an optimal base leaner, f(y?,
?
t,x),
we need to set the coefficient ?
k
to form a new en-
semble, F (x
i
) ? F (x
i
) + ?
k
f(
?
t, y?,x
i
). In Ad-
aBoost, we choose
?
k
=
1
2
log
(
1 + gain(
?
t, y?)
1? gain(
?
t, y?)
)
. (6)
After K iterations, the boosting algorithm returns
the ensemble learner F (x) which consists of a set
of appropriate base learners f(y, t,x).
4.3 Evaluation on Labeled Data
We verified the effectiveness of using subtree fea-
tures on the SWBD and MRDA data sets. For
boosting learning, one typically assumes ?
i
= 1.
In addition, the number of iterations, which relates
to the number of patterns, was determined by a
development set. We also used a one-vs.-all strat-
egy for the multi-class problem. Precision and re-
call were computed and combined into micro- and
macro-averaged F
1
scores. The significance of our
results was evaluated using the McNemar paired
test (Gillick and Cox, 1989), which is based on in-
dividual labeling decisions to compare the correct-
ness of two models. All experiments were imple-
mented in C++ and executed in Windows XP on a
PC with a Dual 2.1 GHz Intel Core2 processor and
2.0 Gbyte of main memory.
1254
Figure 4: Comparison of different trees (SWBD)
We show that use of subtree features is ef-
fective to solve the supervised speech act recog-
nition problem. We also compared our model
with the state-of-the-art maximum entropy classi-
fier (MAXENT). We used bag-of-words, bigram
and trigram features for MAXENT, which mod-
eled 702k (SWBD) and 460k (MRDA) parameters
(i.e., patterns), and produced micro-averaged F
1
scores of 92.76 (macro-averaged F
1
= 63.54) for
SWBD and 82.48 (macro-averaged F
1
= 57.19)
for MRDA. In contrast, our method generated ap-
proximately 4k to 5k patterns on average with sim-
ilar or greater F
1
scores (Table 3); hence, com-
pared to MAXENT, our model requires fewer cal-
culations and is just as accurate.
The n-gram model (NGRAM) performed signif-
icantly better than the bag-of-words model (Mc-
Nemar test; p < 0.001) (Table 3). Unlike MAX-
ENT, NGRAM automatically selects a relevant set
of variable length n-gram features (i.e., phrase
features). To this set, we separately added two
syntax type features, part-of-speech tag n-gram
(POSTAG) and dependency parse tree (DEPTREE)
automatically parsed by Minipar
8
, and one dis-
course type feature, speaker n-gram (SPEAKER).
Although some micro-averaged F
1
are not statisti-
cally significant between the original NGRAM and
the models that include POSTAG, DEPTREE or
SPEAKER, macro-averaged F
1
values indicate that
minor classes can take advantage of other struc-
tures. For example, in the result of SWBD (Fig-
ure 4), DEPTREE and SPEAKER models help to
predict uncertain responses (U), whereas NGRAM
and POSTAG cannot do this.
5 Semi-supervised Learning
Our goal is to eventually make maximum use
of existing resources in SWBD and MRDA for
8
http://www.cs.ualberta.ca/?lindek/minipar.htm
email/forum speech act recognition. We call the
model trained on the mixed data of these two cor-
pora BASELINE. We use ALL features in con-
structing the BASELINE for the semi-supervised
experiments. While this model gave promising re-
sults using SWBD and MRDA, language used in
emails and forums differs from that used in spo-
ken conversation. For example, ?thanx? is an ex-
pression commonly used as a polite mechanism
in online communications. To adapt our model to
understand this type of difference between spoken
and online text-based conversations, we should in-
duce new patterns from unlabeled email and fo-
rum data. We describe here two methods of semi-
supervised learning.
5.1 Method 1: Bootstrapping
First, we bootstrap the BASELINE model using au-
tomatically predicted unlabeled examples. How-
ever, using all of the unlabeled data results in noisy
models; therefore filtering or selecting data is very
important in practice. To this end, we only select
similar examples by criterion, d(x
i
,x
j
) < r or k
nearest neighbors where x
i
? D
L
and x
j
? D
U
.
In practice, r or k are fixed. In our method, exam-
ples are represented by trees; hence we use a ?tree
edit distance? for calculating d(x
i
,x
j
) (Shasha
and Zhang, 1990). Selected examples are evalu-
ated using BASELINE, and using subtree pattern
mining runs on the augmented data (i.e. unla-
beled). We call this method BOOTSTRAP.
5.2 Method 2: Semi-supervised Boosting
Our second method is based on a principle of
semi-supervised boosting learning (Bennett et al,
2002). Because we have no supervised guidance
for D
U
, our objective functional to find F is de-
fined as:
C(F ) =
?
i?D
L
?
i
C[y
i
F (x
i
)] +
?
i?D
U
?
i
C[|F (x
i
)|]
(7)
This cost functional is non-differentiable. To
solve it, we introduce pseudo-labels y? where y? =
sgn(F (x)) and |F (x)| = y?F (x). Using the same
derivation in Section 4.2, we obtain the following
1255
gain function and update rules:
gain(t, y) =
?
i?D
L
y
i
w
i
f(y, t,x
i
)
+
?
i?D
U
y?
i
w
i
f(y, t,x
i
), (8)
w
i
=
{
?
i
? e
?y
i
F (x
i
)
i ? D
L
,
?
i
? e
?y?
i
F (x
i
)
i ? D
U
.
(9)
Intuitively, an unlabeled example that has a
high-confidence |F (x)| at the current step, will
probably receive more weight at the next step.
That is, similar instances become more impor-
tant when learning and mining subtrees. This
semi-supervised boosting learning iteratively gen-
erates pseudo-labels for unlabeled data and finds
the value of F that minimizes training errors (Ben-
nett et al, 2002). Also, the algorithm infers new
features from unlabeled data, and these features
are iteratively re-evaluated by the current ensem-
ble learner. We call this method SEMIBOOST.
6 Experiment
6.1 Setting
We describe specific settings used in our exper-
iment. Because we have no development set,
we set the maximum number of iterations K at
10,000. At most K patterns can be extracted, but
this seldom happens because duplicated patterns
are merged. Typical settings for semi-supervised
boosting are ?
i
= 1 and ?
i
= 0.5, that is, we
penalize the weights for unlabeled data.
For efficiency, BASELINE model used 10% of
the SWBD and MRDA data, selected at random.
We observed that this data set does not degrade the
results of semi-supervised speech act recognition.
For BOOTSTRAP and SEMIBOOST, we selected
k = 100 nearest neighbors of unlabeled exam-
ples for each labeled example using tree edit dis-
tance, and then used 24,625 (SWBD) and 54,961
(MRDA) sentences for the semi-supervised set-
ting.
All trees were combined as described in Section
4.3 (ALL model). In EMAIL and FORUM data we
added different types of discourse features: mes-
sage type (e.g., initial or reply posts), authorship
(e.g., an identification of 2nd or 3rd posts written
by the same author), and relative position of a sen-
tence. In Figure 1, for example, F1?3 is an initial
post, and F4?5 and F6 are reply posts. Moreover,
F1, F4, and F6 are the first sentence in each post.
Table 4: Results of speech act recognition on on-
line conversations; columns are micro-averaged
F
1
score with macro-averaged scores in parenthe-
ses. ?
?
? indicates that the result is significantly bet-
ter than BASELINE (p < 0.001).
Model EMAIL FORUM
BASELINE 78.87 (37.44) 78.93 (35.57)
BOOTSTRAP
?
83.11 (44.90) 79.09 (44.38)
SEMIBOOST
?
82.80 (44.64)
?
81.76 (44.21)
SUPERVISED 90.95 (75.71) 83.67 (40.68)
These features do not occur in SWBD or MRDA
because these are utterance-by-utterance conver-
sations.
6.2 Result and Discussion
First, we show that our method of semi-supervised
learning can improve modeling of the speech
act of emails and forums. As our baseline,
BASELINE achieved a micro-averaged F
1
score
of ? 79 for both data sets. This implies that
SWBD and MRDA data are useful for our prob-
lem. Using unlabeled data, semi-supervised meth-
ods BOOTSTRAP and SEMIBOOST perform bet-
ter than BASELINE (Table 4; Figure 5). To verify
our claim, we evaluated the supervised speech act
recognition on EMAIL and FORUM evaluation
sets with 5-fold cross validation (SUPERVISED in
Table 4). In particular, our semi-supervised speech
act recognition is competitive with the supervised
model in FORUM data.
The difference in performance between super-
vised results in EMAIL and FORUM seems to
indicate that the latter is a more difficult data
set. However, our SEMIBOOST method were able
to come close to the supervised FORUM results
(81.76 vs. 83.67). This is also close to the range of
supervised MRDA data set (F
1
= 83.49 for ALL,
Table 3). Moreover, we analyzed a main reason of
why transfer results were competitive in the FO-
RUM but not in the EMAIL. This might be due
to the mismatch in the unlabeled data, that is, we
used different email collections, the BC3 corpus
(email communication of W3C on w3.org sites),
for evaluation while used Enron data for adaption.
We also conjecture that the discrepancy between
EMAIL and FORUM is probably due to the more
heterogeneous nature of the FORUM data where
anyone can post and reply while EMAIL (Enron or
1256
(a) EMAIL (b) FORUM
Figure 5: Result of the semi-supervised learning method
BC3) might have a more fix set of participants.
The improvement of less frequent tags is promi-
nent, for example 25% for action motivator (AC),
40% for polite mechanism (P), and 15% for rhetor-
ical question (QR) error rate reductions were
achieved in FORUM data (Figure 5(b)). There-
fore, the semi-supervised learning method is more
effective with small amounts of labeled data (i.e.,
less frequent annotations). We believe that despite
their relative rarity, these speech acts are more im-
portant than the statement (S) in some applica-
tions, e.g., summarization.
Next, we give a qualitative analysis for better
interpretation of our problem and results. Due to
limited space, we focus on FORUM data, which
can potentially be applied to many applications.
Of the top ranked patterns extracted by SEMI-
BOOST (Figure 6(a)), subtree patterns of n-gram,
part-of-speech, dependency parse trees are most
discriminative. The patterns from unlabeled data
have relatively lower ranks, but this is not surpris-
ing. This indicates that BASELINE model provides
the base knowledge for semi-supervised speech
act recognition. Also, unlabeled data for EMAIL
and FORUM help to induce new patterns or ad-
just the model?s parameters. As a result, the semi-
supervised method is better than the BASELINE
when an identical number of patterns is modeled
(Figure 6(b)). For this result, we conclude that our
method successfully transfers knowledge from a
source domain (i.e., SWBD and MRDA) to a tar-
get domain (i.e., EMAIL and FORUM); hence it
can be a solution to the domain adaption problem.
Finally, we determine the main reasons for error
(in SEMIBOOST), to gain insights that may allow
development of better models in future work (Fig-
ure 6(c)). We sorted speech act tags by their se-
mantics and partitioned the confusion matrix into
question type (Q*) and statement, which are two
high-level speech acts. Most errors occur in the
similar categories, that is, language usage in ques-
tion discourse is definitely distinct from that in
statement discourse. From this analysis, we be-
lieve that more advanced techniques (e.g. two-
stage classification and learning with hierarchy-
augmented loss) can improve our model.
7 Conclusion
Despite the increasing interest in online text-based
conversations, no study to date has investigated
semi-supervised speech act recognition in email
and forum threads. This paper has addressed the
problem of learning to recognize speech acts us-
ing labeled and unlabeled data. We have also con-
tributed to the development of a novel applica-
tion of boosting subtree mining. Empirical results
have demonstrated that semi-supervised learning
of speech act recognition with subtree features im-
proves the performance in email and forum data
sets. An attractive future direction is to exploit
prior knowledge for semi-supervised speech act
recognition. Druck et al (2008) described gen-
eralized expectation criteria in which a discrimi-
native model can employ the labeled features and
unlabeled instances. Using prior knowledge, we
expect that our model will effectively learn useful
patterns from unlabeled data.
As work progresses on analyzing online text-
based conversations such as emails, forums, and
online chats, the importance of developing models
for discourse without annotating much new data
will become more important. In the future, we
plan to explore other related problems such as ad-
jacency pairs (Levinson, 1983) and discourse pars-
ing (Soricut and Marcu, 2003) for large-scale on-
line forum data.
1257
(a) Example patterns
0 2000 4000 6000
20
25
30
35
40
45
Number of base leaners
Erro
r Ra
te(%)
BASELINEBOOTSTRAPSEMIBOOST
(b) Learning behavior (c) Confusion matrix
Figure 6: Analysis on FORUM data
Acknowledgement
We would like to thank to anonymous reviewers
for their valuable comments, and Yunbo Cao, Wei
Lai, Xinying Song, Jingtian Jing, and Wei Wu for
their help in preparing our data.
References
R. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and
unlabeled data. Journal of Machine Learning Re-
search, 6:1817?1853.
J. Ang, Y. Liu, and E. Shriberg. 2005. Automatic dia-
log act segmentation and classification in multiparty
meetings. In Proceedings of ICASSP, pages 1061?
106.
J. Austin. 1962. How to Do Things With Words. Har-
vard Univ. Press, Cambridge, MA.
K.P. Bennett, A. Demiriz, and R. Maclin. 2002. Ex-
ploiting unlabeled data in ensemble methods. In
Proceedings of ACM SIGKDD, pages 289?296.
W.W. Cohen, V.R. Carvalho, and T. Mitchell. 2004.
Learning to classify email into ?speech acts?. In
Proceedings of EMNLP, pages 309?316.
R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg.
2004. Meeting recorder project: Dialog act label-
ing guide. Technical report, International Computer
Science Institute.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of ACM SIGIR, pages
595?602.
D. Feng, E. Shaw, J. Kim, and E. H. Hovy. 2006.
Learning to detect conversation focus of threaded
discussions. In Proceedings of HLT-NAACL, pages
208?215.
M. Galley, K. McKeown, J. Hirschberg, and
E. Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of bayesian
networks to model pragmatic dependencies. In Pro-
ceedings of ACL.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of ICASSP, pages 532?535.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL labeling project coder?s
manual, draft 13. Technical report, Univ. of Col-
orado Institute of Cognitive Science.
T. Kudo and Y. Matsumoto. 2004. A boosting algo-
rithm for classification of semi-structured text. In
Proceedings of EMNLP, pages 301?308.
S. Levinson. 1983. Pragmatics. Cambridge Univ.
Press, Cambridge.
L. Mason, P. Bartlett, J. Baxter, and M. Frean. 2000.
Functional gradient techniques for combining hy-
potheses. In A.J. Smola, P.L. Bartlett, B. Sch?olkopf,
and D. Schuurmans, editors, Advances in Large
Margin Classifiers, pages 221?246. MIT Press,
Cambridge, MA.
K. McKeown, L. Shrestha, and O. Rambow. 2007. Us-
ing question-answer pairs in extractive summariza-
tion of email conversations. In Proceedings of CI-
CLing, volume 4394 of Lecture Notes in Computer
Science, pages 542?550.
J. Nastri, J. Pe na, and J. T. Hancock. 2006. The
construction of away messages: A speech act anal-
ysis. Journal of Computer-Mediated Communica-
tion, 11(4):article 7.
S. Ravi and J. Kim. 2007. Profiling student interac-
tions in threaded discussions with speech act classi-
fiers. In Proceedings of the AI in Education Confer-
ence.
1258
C. Ros?e, Y. Wang, Y. Cui, J. Arguello, K. Stegmann,
A. Weinberger, and F. Fischer. 2008. Analyzing
collaborative learning processes automatically: Ex-
ploiting the advances of computational linguistics in
computer-supported collaborative learning. Interna-
tional Journal of Computer-Supported Collabora-
tive Learning, 3(3):237?271.
R.E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3):297?336.
J. Searle. 1969. Speech Acts. Cambridge Univ. Press,
Cambridge.
D. Shasha and K. Zhang. 1990. Fast algorithms for the
unit cost editing distance between trees. Journal of
Algorithms, 11(4):581?621.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of NAACL-HLT, pages 149?
156.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-
Dykema, and M. Meteer. 2000. Dialogue act
modeling for automatic tagging and recognition of
conversational speech. Computational Linguistics,
26(3):339?373.
G. Tur, D. Hakkani-T?ur, and R. E. Schapire. 2005.
Combining active and semi-supervised learning for
spoken language understanding. Speech Communi-
cation, 45(2):171?186.
D. P. Twitchell, J. F. Nunamaker, and J. K. Burgoon.
2004. Using speech act profiling for deception de-
tection. In Second Symposium on Intelligence and
Security Informatics, volume 3073 of Lecture Notes
in Computer Science, pages 403?410.
1259
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 36?37,
Vancouver, October 2005.
POSBIOTM/W: A Development Workbench For Machine Learning
Oriented Biomedical Text Mining System ?
Kyungduk Kim, Yu Song, Gary Geunbae Lee
Department of Computer Science and Engineering
Pohang University of Science & Technology (POSTECH)
San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea
{getta, songyu, gblee}@postech.ac.kr
Abstract
The POSBIOTM/W1 is a workbench for
machine-learning oriented biomedical text
mining system. The POSTBIOTM/W is
intended to assist biologist in mining use-
ful information efficiently from biomed-
ical text resources. To do so, it pro-
vides a suit of tools for gathering, manag-
ing, analyzing and annotating texts. The
workbench is implemented in Java, which
means that it is platform-independent.
1 Introduction
Large amounts of biomedical literature exist and the
volume continues to grow exponentially. Following
the increase of literature, there is growing need for
appropriate tools in support of collecting, managing,
creating, annotating and exploiting rich biomedical
text resources.
Especially, information on interactions among bi-
ological entities is very important for understanding
the biological process in a living cell (Blascheke et.
al., 1999). In our POSBIOTM/W workbench, we
use a supervised machine learning method to gen-
erate rules automatically to extract biological events
from free texts with minimum human effort. And we
adopt the Conditional Random Fields (CRF) model
(Lafferty et. al.,2001) for the biomedical named-
entity recognition (NER) task. Finally, to reduce the
? The research was supported by Brain Neuro Informatics
Research program by MOCIE.
1POSBIOTM/W stands for POSTECH Bio-Text Mining
System Workbench
labeling effort in a larger extent we incorporate an
active learning idea into the workbench.
2 System Description
The POSBIOTM/W comprises a set of appropriate
tools to provide users a convenient environment for
gathering, managing and analyzing biomedical text
and for named-entity annotation. The workbench
consists of four components: Managing tool, NER
tool, Event Extraction Tool and Annotation Tool.
And we adopt an active learning idea into the work-
bench to improve the NER and the Event Extraction
module?s performance. The overall design is shown
in Figure 1.
POSBIOTM W client/ ( ) POSBIOTM Syst erverem s( )
NER Module
Event ExtractionModule
NER Tool
Event Extraction Tool
ManagingTool AnnotatingTool TrainingData ActiveearningL
Figure 1: Overview of POSBIOTM/W
2.1 Managing tool
Main objective of the Managing tool is to help biolo-
gists search, collect and manage literatures relevant
to their interest. Users can access to the PubMed
database of bibliographic information using quick
searching bar and incremental PubMed search en-
gine.
36
2.2 NER tool
The NER tool is a client tool of POSBIOTM-
NER module and able to automatically annotate
biomedical-related texts. The NER tool provides
access to three target-specific named entity mod-
els - GENIA-NER model, GENE-NER model and
GPCR-NER model. Each of these model is trained
based on GENIA-Corpus (Kim et. al., 2003),
BioCreative data (Blaschke et. al., 2004) and POS-
BIOTM/NE corpus2 respectively. In POSBIOTM-
NER system, we adopt the Conditional Random
Fields (CRF) model (Lafferty et. al., 2001) for the
biomedical NER task.
2.3 Event Extraction tool
The Event Extraction tool extracts several biologi-
cal events from texts using automatically generated
rules. We use a supervised machine learning method
to overcome a knowledge-engineering bottleneck by
learning event extraction rules automatically. We
modify the WHISK (Soderland, 1999) algorithm to
provide a two-level rule learning method as a divide-
and-conquer strategy. In two-level rule learning, the
system learns event extraction rules which are inside
of the noun chunk at first level, and then it learns the
rules for whole sentence.
Since the system extracts biological events using
automatically generated rules, we can not guarantee
that every extracted event is always correct because
many different rules can be applied to the same sen-
tence. Therefore we try to verify the result with a
Maximum Entropy (ME) classifier to remove incor-
rectly extracted events. For each extracted event,
we verify each component of the event with the ME
classifier model. If one component is contradicted
to the class assigned by the classification model, we
will remove the event. For detail event extraction
process, please consult our previous paper (Kim et.
al., 2004).
2.4 Annotation tool
Our workbench provides a Graphical User Interface
based Annotation tool which enables the users to
annotate and correct the result of the named-entity
recognition and the event extraction. And users can
2POSBIOTM/NE corpus, our own corpus, is used to identify
four target named entities: protein, gene, small molecule and
cellular process.
upload the revised data to the POSBIOTM system,
which would contribute to the incremental build-up
of named-entity and relation annotation corpus.
2.5 Active learning
To minimize the human labeling effort, we employ
the active learning method to select the most infor-
mative samples. We proposed a new active learning
paradigm which considers not only the uncertainty
of the classifier but also the diversity of the corpus,
which will soon be published.
References
Christian Blaschke, Andrade, M.A., Ouzouis, C., Valen-
cia, A.. 1999. Automatic extraction of biological in-
formation from scientific text : protein-protein interac-
tions. Intelligent Systems for Molecular Biology 60-
67.
Christian Blaschke, L. Hirschman, and A.
Yeh, editors. 2004. Proceedings of the
BioCreative Workshop, Granda, March.
http://www.pdg.cnb.uam.es/BioLINK/
workshop BioCreative 04/handout/
Eunju Kim, Yu Song, Gary Geunbae Lee, Byoung-Kee
Yi. 2004. Learning for interaction extraction and ver-
ification from biological full articles. Proceedings of
the ACM SIGIR 2004 workshop on search and discov-
ery in bioinformatics, July 2004, Sheffield, UK
J.-D. Kim, T. Ohta, Y. Tateisi and J. Tsujii 2003. GE-
NIA corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics, Vol 19 Suppl. 1 2003,
pages i180-i182
J. Lafferty, A. McCallum and F. Pereira 2001. Con-
ditional random fields: probabilistic models for seg-
menting and labelling sequence data. International
Conference on Machine Learning.
Soderland S. 1999. Learning information extraction
rules for semi-structured and free text. Machine
Learning, volume 34, 233-272.
37
Heuristic Methods for Reducing Errors of
Geographic Named Entities Learned
by Bootstrapping
Seungwoo Lee and Gary Geunbae Lee
Department of Computer Science and Engineering,
Pohang University of Science and Technology,
San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Republic of Korea
Abstract. One of issues in the bootstrapping for named entity recogni-
tion is how to control annotation errors introduced at every iteration. In
this paper, we present several heuristics for reducing such errors using
external resources such as WordNet, encyclopedia and Web documents.
The bootstrapping is applied for identifying and classifying fine-grained
geographic named entities, which are useful for applications such as in-
formation extraction and question answering, as well as standard named
entities such as PERSON and ORGANIZATION. The experiments show
the usefulness of the suggested heuristics and the learning curve evalu-
ated at each bootstrapping loop. When our approach was applied to a
newspaper corpus, it could achieve 87 F1 value, which is quite promising
for the fine-grained named entity recognition task.
1 Introduction
A bootstrapping process for named entity recognition is usually as follows. In
the initial stage, it selects seeds and annotates a raw corpus using the seeds.
From the annotation, internal and contextual patterns are learned and applied
to the corpus again to obtain new candidates of each type. Several methods
are adopted to reduce over-generation and incorrect annotation and accept only
correct ones. One sense per discourse heuristic may also be adopted to expand
the annotated instances. It repeats until no more new patterns and entities
are learned.
There are several issues in bootstrapping approaches for named entity recog-
nition task to achieve successful performance. One of them is how to control
annotation errors introduced in the bootstrapping process, on which we are fo-
cusing in this paper. As iteration continues, the bootstrapping expands previous
annotation to increase recall. But this expansion may also introduce annotation
errors and, as a result, decrease the precision. Ambiguous entities may be mis-
classified since learning speed per class depends on seeds. For example, ?New
York? may be misclassified to a city name before the patterns that correctly
classify it to a state name are learned. Especially such errors in the early stage
of the bootstrapping are quite harmful because the errors are accumulated. The
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 658?669, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Heuristic Methods for Reducing Errors of Geographic Named Entities 659
annotation errors are classified into following four cases: inclusion, crossing, type
conflict and spurious. The first three errors occur when a learned entity overlaps
a true entity whereas the last one occurs when a learned entity does not overlap
any true entity. Most previous works depend only on the statistics (e.g., scores
of patterns) obtained from the previous annotation to control such errors. How-
ever, this strategy is not always the best because some trivial errors can also
be corrected by simple heuristics. We suggest several heuristics that control the
annotation errors in Section 4. The heuristics are embedded in a bootstrapping
algorithm, which is modified and improved from [4] and shortly described in
Section 3.
Unlike the traditional named entity task, we deal with sub-categorized ge-
ographic named entities (i.e., locations) in addition to PERSON and ORGA-
NIZATION. Geographic named entities can be classified into many sub-types
that are critical for applications such as information extraction and question
answering. As a first step, we define their ten sub-classes: COUNTRY, STATE,
COUNTY, CITY, MOUNTAIN, RIVER, ISLAND, LAKE, CONTINENT and
OCEAN. We attempt to identify and classify all instances of the eleven classes as
well as PERSON and ORGANIZATION in plain text. Annotation of geographic
named entities is a formidable task. Geographic named entities are frequently
shared between their sub-classes as well as with person names. For example,
?Washington? may indicate a person in one context but may also mean a city or
state in another context. Even country names cannot be exceptions. For some
Americans, ?China? and ?Canada? may be cities where they live. Geographic
named entities such as ?Turkey? and ?Chile? can also be shared with common
nouns. Contextual similarity among geographic named entities is much higher
than the one between PLO (Person-Location-Organization) entities since they
are much closer semantically. These make geographic named entity annotation
task more difficult than that of the traditional named entity task.
The remainder of this paper is as follows. Section 2 presents and compares
related works to our approach. The bootstrapping algorithm is shortly described
in Section 3 and several heuristics for controlling the annotation errors are ex-
plained in Section 4. Section 5 gives some experimental results verifying our
approach, which is followed by conclusions and future works in Section 6.
2 Related Works
Most bootstrapping approaches start with incomplete annotations and patterns
obtained from selected seeds and learn to obtain more complete annotations and
patterns. However, the incompleteness is apt to cause annotation errors to be
introduced in each bootstrapping iteration. Most previous works have designed
their own statistical measures to control such errors. Phillips and Riloff [9] de-
veloped evidence and exclusivity measures to filter out ambiguous terms and
Yangarber et al [12] calculated accuracy, confidence and score of their pat-
terns to select better patterns. However, those statistical measures are calcu-
lated only using data obtained from their training corpus which cannot often
660 S. Lee and G.G. Lee
give enough information. Instead, other resources like World Wide Web as well
as a gazetteer can be incorporated to compensate the lack of information from the
training corpus.
Research on analysis of geographic references recently started to appear and
has two directions. One is to focus on building gazetteer databases [6,11] and
the other is to focus on classifying geographic entity instances in text [5].
Manov et al [6] presented KIM (Knowledge and Information Management)
that consists of an ontology and a knowledge base. They used it for information
extraction but did not show notable results. Uryupina [11] presented a boot-
strapping method to obtain gazetteers from the internet. By searching for seed
names on the internet, she obtained lexical patterns and learned each classi-
fier for six location sub-types, such as COUNTRY, CITY, ISLAND, RIVER,
MOUNTAIN and REGION. Then she obtained and classified candidate names
by searching the patterns in the internet. Li et al [5] suggested a hybrid approach
to classify geographic entities already identified as location by an existing named
entity tagger. They first matched local context patterns and then used a max-
imum spanning tree search for discourse analysis. They also applied a default
sense heuristic as well as one sense per discourse principle. According to their
experiments, the default sense heuristic showed the highest contribution.
3 Bootstrapping
Our bootstrapping algorithm was modified and improved from [4] and the boot-
strapping flow has one initial step and four iterative steps, as shown in Figure 1.
In the initial step, we annotate a raw corpus with seeds automatically obtained
from various gazetteers. Starting and ending boundary patterns are learned from
the annotation and applied to the corpus again to obtain new candidates of
each type. Then we eliminate annotation errors in the candidates using several
Initial Annotation
Corpus
(raw  tagged
(partial  complete))
Learn
Boundary Patterns
Extract
Entity Candidates
Control 
Annotation Errors
Expand/Accept 
Annotation
Boundary
Patterns
Entity
Candidates
Correct
Entities
Seeds
Accepted
Entities
Fig. 1. The bootstrapping overview
Heuristic Methods for Reducing Errors of Geographic Named Entities 661
linguistic heuristics, which is described in detail in Section 4. Finally, the re-
maining entity candidates propagate their annotations into other occurrences
within the same document by one sense per discourse principle [2]. This loop
continues until there are no new patterns learned. The algorithm is summarized
as follows:
Step 0: Seed Preparation and Initial Annotation
We prepare seeds from the gazetteer and obtain initial entity candidate set, C1,
by marking occurrences of the seeds in the training raw corpus.
C1 = {ei|ei is an entity candidate obtained from seeds but not accepted
yet};
And we initialize the number of iteration (k), the set of accepted boundary
patterns (P0) and the set of accepted entities (E0) as follows:
k = 1; P0 = ?; E0 = ?;
Step 1: Controlling the Annotation Errors
We filter out annotation errors among the entity candidates (Ck) using several
heuristics with external resources and construct Ek, a set of entities checked as
correct (see Section 4).
Ek = {ei|ei ? Ck and ei is checked as correct by heuristics};
Step 2: Expanding and Accepting the Annotation
After removing erroneous candidates, we expand the correct entities by applying
one sense per document heuristic and then accept M1 top-ranked entities to
construct a new Ek, the set of currently accepted entities.
Ek = {ei|ei ? Ek or is an instance expanded from ej ? Ek, and
Rank(ei) ? Rank(ei+1), 1 ? i ? M};
Ek = Ek?1 ? Ek;
The rank of an entity candidate, Rank(ei), is computed as follows:
Rank(ei) = 1 ? {1 ? Score(BPs(ei))} ? {1 ? Score(BPe(ei))} (1)
BPs(e) and BPe(e) indicate starting and ending boundary patterns of an entity
e, respectively.
1 M was set to 300 in our experiment.
662 S. Lee and G.G. Lee
Step 3: Learning Boundary Patterns
From the currently accepted entity set, Ek, we learn a new boundary pattern can-
didate set, P?k. We generate starting and ending boundary patterns and compute
the accuracy (Acc(pi)) of each pattern pi which is used to filter out inaccurate
patterns below ?a2 and construct P?k. Then we compute the score (Score(pi))
of each pattern pi and add new N3 top-scored patterns among P?k to the ac-
cepted boundary pattern set, Pk, if there exist new patterns in P?k. Otherwise,
the bootstrapping process stops.
P?k = {pi|pi = BP (e), e ? Ek and pi /? Pk?1 and Acc(pi) ? ?a};
If P?k = ? then stop;
Otherwise, Pk = Pk?1 ? {pi|pi ? P?k and Score(pi) ? Score(pi+1), 1 ?
i ? N};
The accuracy, Acc(p) and the score, Score(p), of a boundary pattern, p, are
computed as follows:
Acc(p) =
pos(p)
pos(p) + neg(p)
?
1 ? 1pos(p)2+1
1 ? 1Np2+1
, (2)
Score(p) =
pos(p)
pos(p) + 2 ? neg(p) + unk(p) ?
1 ? 1ln(pos(p)+3)
1 ? 1ln(Np+3)
, (3)
where pos(p) is the number of instances that are matched to p and already
annotated with the same entity type; neg(p) is the number of instances that are
matched to p but already annotated with a different type or previously filtered
out; unk(p) is the number of instances that are matched to p but not annotated
yet; Np is the maximum value of pos(p).
Step 4: Applying Boundary Patterns and Extracting Candidates
We extract new entity candidates, Ck+1, for the next iteration by applying the
accepted boundary patterns, Pk, to the training corpus and then go to Step 1.
Ck+1 = {ei|BPs(ei) ? Pk and BPe(ei) ? Pk and ei /? Ek};
k := k + 1;
Go to Step 1.
Since each pattern determines only one ? i.e., starting or ending ? boundary, a
candidate is identified and classified by a pair of starting and ending boundary
patterns with the same type.
2 ?a was set to 0.1 in our experiment.
3 N was set to 700 in our experiment.
Heuristic Methods for Reducing Errors of Geographic Named Entities 663
4 Error Controls
The annotation errors introduced in the bootstrapping process are classified into
following four cases, based on the inconsistency between an erroneous entity can-
didate and a true entity: inclusion, crossing, type conflict and spurious. Inclusion
occurs when a candidate is a sub-phrase of a true entity ? e.g., ?U.S.? in ?U.S.
Army?. Crossing occurs when a candidate partially overlaps with a true entity
? e.g., ?Columbia River? in ?British Columbia River?, which means a river in
?British Columbia?. Type conflict occurs when a candidate has the same text
span but different type from a true entity ? e.g., ?New York? may be misclassi-
fied into STATE but it is CITY. Spurious indicates that a candidate is spurious
and does not interfere with any true entities.
To resolve these inconsistencies, we basically use statistical measures such as
the score of a boundary pattern, Score(p), and the rank of an entity candidate,
Rank(e), as in most previous works. However, this strategy is not always the
best because some trivial errors can also be removed by simple heuristics and
linguistic knowledge. Especially, the strategy cannot be applied to erroneous
entities whose inconsistencies cannot be detected since their true entities are not
identified yet. We call it potential inconsistency. We examine potential inclusion
and potential type conflict for each entity candidate using the gazetteer and Web
resources. To overcome this limitation of statistical measures obtained from the
training corpus, we design several methods that incorporate linguistic knowledge
and external resources, which are described in the following subsections.
4.1 Co-occurrence Information
Co-occurrence information (CI) has been widely used to resolve word sense am-
biguity [3,8,10] and also can be employed to resolve crossing and type conflict
inconsistencies, which can be regarded as word sense ambiguity problem. We
assume that two instances of an ambiguous entity that occur in different texts
can be classified into the same class if they share their CI. CI can be collected
from definition statements of an entity of an encyclopedia. For example, the
underlined phrases are collected as CI of an entity ?Clinton? with class CITY
from a statement ?Clinton is a city in Big Stone County, Minnesota, USA?. In
this way, we could construct initial CI for 18000 entities from the Probert En-
cyclopedia (http://www.probertencyclopaedia.com/places.htm), most of which
are geographic entities. We also augment CI from the accepted entity instances
during the bootstrapping process. We consider capitalized nouns or noun phrases
in the window of up to left/right 60 words, within sentence boundary, from an
entity as its CI. Then, the score of an entity e with class t, Coinfo(e, t), is
calculated as the similarity of CI:
Coinfo(e, t) =
?N
i=1 freq(cwi, e, t) ? count(cwi, e)
N
, (4)
where N is the number of co-occurrence information cwi, freq(cwi, e, t) means
the frequency of cwi co-occurring with an entity e of class t in the learned
664 S. Lee and G.G. Lee
co-occurrence information and count(cwi, e) means the frequency of cwi co-
occurring with the entity in the current pending context. When two candidates
cause crossing or type conflict, the candidate having smaller Coinfo is consid-
ered to be incorrect and removed.
4.2 Gazetteer with Locator
Most entities are often mentioned with geographic entities where they are lo-
cated, especially when they are not familiar to general readers. For example,
?Dayton? in ?the Dayton Daily News, Dayton, Ohio? is restricted to an entity
in ?Ohio?. This means that we can classify ?Dayton? into CITY if we know a
fact that there is a city named ?Dayton? and located at ?Ohio?. We can say that
the locator information is a special case of the co-occurrence information. The
locator information was also collected from the Probert Encyclopedia. If one of
two entity candidates causing crossing or type conflict has a verified locator, the
other can be regarded as an error and removed.
4.3 Prior Probability
Ambiguous entities often have different prior probability according to each class.
For example, ?China? appears frequently in general text as a country name but
rarely as a city name. ?Canada? is another example. This means that when
two entity candidates cause type conflict we can remove one having lower prior
probability. It is hard to acquire such probabilities if we do not have a large
annotated corpus. However, WordNet [7] can give us the information that is
needed to infer the relative prior probability since the sense order in WordNet
reflects the frequency that the sense appears in text. According to WordNet, for
example, ?New York? is more frequently mentioned as a city name than as a state
name and, therefore, is classified into CITY if its context does not give strong
information that it is a state name. We could construct relative prior probabilities
for 961 ambiguous gazetteer entries from WordNet. The prior probability of
entity e with type t based on WordNet, PriorWN (e, t), is calculated as follows:
PriorWN (e, t)=
{
1
N+1 + ?WN ?
(m+1)?Sense#W N (e,t)
? m
i=1 i
if there exist in WordNet
1
N+1 ? ?WN otherwise,
(5)
where N is the number of possible types of entity candidate e, m is the number of
types of entity candidate e registered in WordNet, and Sense#WN (e, t) means
the WordNet sense no. of entity candidate e with type t. ?WN and ?WN are
calculated as follows:
?WN = ?WN ? (N ? m) +
1
N + 1
?WN =
m
(N + 1)2
Heuristic Methods for Reducing Errors of Geographic Named Entities 665
Based on these formulas, the prior probabilities of an entity ?New York ? are
given as follows according to its type: (CITY, 0.44), (STATE, 0.32), (TOWN,
0.12), and (COUNTY, 0.12).4
Although this prior probability is quite accurate, it does not have sufficient
applicability. Therefore, we need to develop another method that can acquire
prior probabilities of much more entities and Web can be one alternative. For
each ambiguous entity X, we query ?X is a/an? to at least two Web search
engines5 and extract and collect a noun phrase Y matching to ?X is a/an Y?.
Then, we determine a type, which Y belongs to, using WordNet and count its
frequency. This frequency for each possible type of the entity X is regarded
as sense order information. That is, we can assign to each possible type a sense
number in the descending order of the frequency. Now, the prior probability of an
entity e with type t based on the Web, PriorWeb(e, t), can be similarly calculated.
Then, the final prior probability, Prior(e, t), is computed by arithmetic mean
of PriorWN (e, t) and PriorWeb(e, t). Combined with the Web search, the prior
probabilities of the above example are changed as follows: (CITY, 0.36), (STATE,
0.29), (TOWN, 0.18), and (COUNTY, 0.17).
4.4 Default Type
When an ambiguous candidate causing type conflict is not registered in WordNet
and cannot be detected by the Web search, we can apply default type heuristic.
Unlike the prior probability, default type indicates a priority between any two
target classes regardless of each individual entity. In general, we can say that, for
an ambiguous entity between COUNTRY and CITY, COUNTRY is more dom-
inant than CITY since a country name is more familiar to common people. We
built up default types between all pairs of target classes using human linguistic
knowledge and prior probability described in the previous subsection.
4.5 Part of Other Entity
Potential inclusion is often not exposed at a bootstrapping iteration since bound-
ary patterns for each class are generated at different speeds and, in addition, all
required boundary patterns cannot be generated from seeds. For this, we design
two methods in addition to gazetteer consulting.
First, we check if there exists an acronym for a super-phrase. [1] says that
we can consult a commonly-used acronym to determine extent of a named en-
tity. In other words, ?University of California, Los Angeles?, for example, must
4 WordNet does not have COUNTY and TOWN senses of ?New York ?.
5 We used eight well-known Web search engines such as Google
(http://www.google.com/), Ask Jeeves (http://web.ask.com/), AltaVista
(http://www.altavista.com/), LookSmart (http://search.looksmart.com/),
Teoma (http://s.teoma.com/), AlltheWeb (http://www.alltheweb.com/), Ly-
cos (http://search.lycos.com/), and Yahoo! (http://search.yahoo.com/). We
specially thank to the service providers.
666 S. Lee and G.G. Lee
be annotated as a unique organization name since the university is commonly
referred to as ?UCLA?. As an another example, ?U.S.? in ?U.S. Navy? should
not be annotated as a country name but ?U.S.? in ?U.S. President? should be
since ?U.S. Navy? is represented as the acronym ?USN? but ?U.S. President? is
not represented as ?USP?. To check the existence of their acronyms, we can con-
sult Web search engines by querying the suspected phrases with their possible
acronyms, such as ?U.S. Navy (USN)? and ?U.S. President (USP)?, respectively,
with exact match option.
Another solution is to check if a super-phrase beginning with a candidate
whose class is one of geographic classes can be modified by a prepositional phrase
which is derived by in or comma (,) plus the candidate (denoted as in-loc). For
example, we can decide that ?Beijing? in ?Beijing University? is a part of the
university name, since the phrase ?Beijing University in Beijing? is found by
Web search engines. If the ?Beijing? denotes CITY, ?Beijing University? means
a university in Beijing and is not modified by the prepositional phrase ?in
Beijing? duplicately.
5 Experiments
The bootstrapping algorithm was developed and trained on part of New York
Times articles (the first half of June, 1998; 28MB; 5,330 articles) from the
AQUAINT corpus. We manually annotated 107 articles for test and the counts
of annotated instances were listed in Table 1. A gazetteer composed of 80,000
entries was compiled from several Web sites6. This includes non-target entities
as well as various aliases of entity names.
Table 1. The counts of instances annotated in the test corpus
C
O
U
N
T
R
Y
S
T
A
T
E
C
O
U
N
T
Y
C
IT
Y
R
IV
E
R
M
O
U
N
T
.
IS
L
A
N
D
C
O
N
T
I.
O
C
E
A
N
L
A
K
E
P
E
R
S
O
N
O
R
G
A
N
.
T
ot
al
596 422 61 868 26 15 29 74 19 9 2,660 1,436 6,215
We first examined the usefulness of the heuristics, based on the instances
(i.e., key instances) annotated in the test corpus. Applicability (app.) is defined
as the number of key instances (denoted as #app), to which the heuristic can be
applied, divided by the number of ambiguous ones (denoted as #ambi). Accuracy
(acc.) is defined as the number of instances correctly resolved (denoted as #corr)
divided by #app. There were 2250 ambiguous key instances in the test corpus.
6 http://www.census.gov/, http://crl.nmsu.edu/Resources/resource.htm,
http://www.timeanddate.com/, http://www.probertencyclopaedia.com/places.htm,
http://www.world-of-islands.com/, and http://islands.unep.ch/isldir.htm
Heuristic Methods for Reducing Errors of Geographic Named Entities 667
Applicability and accuracy of the first four heuristics for resolving type conflict
are summarized in Table 2. As shown in the table, the first two heuristics ? co-
occurrence information and gazetteer with locator ? have very low applicability
but very high accuracy. On the contrary, the last two heuristics ? prior probability
and default type ? show moderate accuracy with relatively high applicability.
Based on this result, we combine the four heuristics in sequence such as high
accurate one first and high applicable one last.
We also examined how well the heuristics such as acronym and in-loc can
detect potential inclusion of an entity. In case of acronym, there were 2,555 key
instances (denoted as #app) composed of more than one word and we searched
the Web to check the existence of any possible acronym of each instance. As
a result, we found out the correct acronyms for 1,143 instances (denoted as
#corr). On the contrary, just 47 instances were incorrectly matched when we
tried to search any acronyms of super-phrases of each key instance. In other
words, acronym can detect potential inclusion at 46.58 applicability and 96.05
accuracy. In case of in-loc, 1,282 key instances beginning with a geographic word
are tried to be checked if they appear with in-loc pattern in Web documents and
313 instances of them were confirmed. On the contrary, only 1 super-phrase of
a key instance was incorrectly detected. Therefore, in-loc can detect potential
inclusion at 24.49 applicability and 99.68 accuracy. These are summarized in
Table 3. It says that the heuristics can detect quite accurately the extent of
named entities although they do not have high applicability.
Finally, we evaluated the bootstrapping with the heuristics by investigating
the performance change at every iteration. 50,349 seeds were selected from the
gazetteer after removing ambiguous ones and only 3,364 seeds among them,
which could be applied to the training corpus, were used for training. The recall
and precision were measured using the standard MUC named entity scoring
scheme and plotted in Figure 2. Starting at low recall and high precision, it
gradually increases recall but slightly degrades precision, and it arrived at 87 F1
Table 2. Applicability and accuracy of the heuristics for resolving the inconsistency
of 2,250 ambiguous instances (#ambi=2,250)
#app #corr app. acc.
co. info. 44 42 1.96 95.45
gaz. loc. 148 141 6.58 95.27
prior prob. 2,072 1,741 92.09 84.03
def. type 2,225 1,367 98.89 61.44
Table 3. Applicability and accuracy of the heuristics for detecting potential inclusion
#ambi #app #corr app. acc.
acronym 2,555 1,190 1,143 46.58 96.05
in-loc 1,282 314 313 24.49 99.68
668 S. Lee and G.G. Lee
85
90
95
100
0 10 20 30 40 50 60 70 80 90
Recall
Pr
e c
is i
on
without heuristics
with heuristics
Fig. 2. The learning curve of the bootstrapping with the heuristics
(81 recall and 93 precision) after 1,100 iterations. We think that this performance
is quite notable considering our fine-grained target classes, and the suggested
heuristics work well to prevent incorrect entity candidates from being accepted
during bootstrapping process.
6 Conclusions
In this paper, we observed four kinds of inconsistencies that degrade the per-
formance of bootstrapping for named entity recognition with fine-grained geo-
graphic classes. To resolve such inconsistencies, we suggested several heuristics
incorporating human linguistic knowledge and external resources like encyclope-
dia and Web documents. By analyzing the capability of each heuristic, we com-
bined them in sequence. The bootstrapping with the heuristics was evaluated.
Starting at low recall and high precision, the bootstrapping largely increased
recall at a small cost of precision, and finally it achieved 87 F1. This means
that the suggested approach is quite promising for the fine-grained named entity
recognition task and the suggested heuristics can effectively reduce incorrect can-
didates introduced at the intermediate bootstrapping steps. In future, we plan
to design a uniform statistical method that can augment the suggested heuris-
tics especially using Web resources and also incorporate our heuristic knowledge
used for filtering into the statistical model.
Acknowledgements
This work was supported by 21C Frontier Project on Human-Robot Interface
(MOCIE) and by BK21 Project (Ministry of Education).
Heuristic Methods for Reducing Errors of Geographic Named Entities 669
References
1. Chinchor, N., Brown, E., Ferro, L., Robinson, P.: 1999 Named Entity
Recognition Task Definition (version 1.4). http://www.nist.gov/speech/tests/ie-
er/er 99/doc/ne99 taskdef v1 4.pdf (1999)
2. Gale, W.A., Church, K.W., Yarowsky, D.: One Sense Per Discourse. In: Proceedings
of the 4th DARPA Speech and Natural Language Workshop. (1992) 233?237
3. Guthrie, J.A., Guthrie, L., Wilks, Y., Aidinejad, H.: Subject-dependent Co-
occurrence and Word Sense Disambiguation. In: Proceedings of the 29th Annual
Meeting of the Association for Computational Linguistics (ACL), Berkeley, CA
(1991) 146?152
4. Lee, S., Lee, G.G.: A Bootstrapping Approach for Geographic Named Entity
Annotation. In: Proceedings of the 2004 Conference on Asia Information Retrieval
Symposium (AIRS2004), Beijing, China (2004) 128?133
5. Li, H., Srihari, R.K., Niu, C., Li, W.: InfoXtract location normalization: a hybrid
approach to geographic references in information extraction. In: Proceedings of
the HLT-NAACL 2003 Workshop on Analysis of Geographic References, Alberta,
Canada (2003) 39?44
6. Manov, D., Kirjakov, A., Popov, B., Bontcheva, K., Maynard, D., Cunningham, H.:
Experiments with geographic knowledge for information extraction. In: Proceed-
ings of the HLT-NAACL 2003 Workshop on Analysis of Geographic References,
Alberta, Canada (2003) 1?9
7. Miller, G.A.: WordNet: A lexical database for English. Communications of the
ACM 38 (1995) 39?41
8. Niwa, Y., Nitta, Y.: Co-occurrence Vectors from Corpora vs Distance Vectors from
Dictionaries. In: Proceedings of the 15th International Conference on Computa-
tional Linguistics (COLING?94), Kyoto, Japan (1994) 304?309
9. Phillips, W., Riloff, E.: Exploiting Strong Syntactic Heuristics and Co-Training
to Learn Semantic Lexicons. In: Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP2002), Philadelphia, PA (2002)
125?132
10. Shin, S., soek Choi, Y., Choi, K.S.: Word Sense Disambiguation Using Vectors
of Co-occurrence Information. In: Proceedings of the Sixth Natural Language
Processing Pacific Rim Symposium (NLPRS2001), Tokyo, Japan (2001) 49?55
11. Uryupina, O.: Semi-supervised learning of geographical gazetteers from the inter-
net. In: Proceedings of the HLT-NAACL 2003 Workshop on Analysis of Geographic
References, Alberta, Canada (2003) 18?25
12. Yangarber, R., Lin, W., Grishman, R.: Unsupervised Learning of Generalized
Names. In: Proceedings of the 19th International Conference on Computational
Linguistics (COLING 2002), Taipei, Taiwan (2002) 1135?1141
A Transformation-based Sentence Splitting Method for Statistical Ma-
chine Translation 
Jonghoon Lee, Donghyeon Lee and Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science & Technology (POSTECH) 
{jh21983, semko, gblee}@postech.ac.kr 
 
 
 
Abstract 
We propose a transformation based sen-
tence splitting method for statistical ma-
chine translation. Transformations are ex-
panded to improve machine translation 
quality after automatically obtained from 
manually split corpus. Through a series of 
experiments we show that the transforma-
tion based sentence splitting is effective 
pre-processing to long sentence translation. 
1 Introduction 
Statistical approaches to machine translation have 
been studied actively, after the formalism of statis-
tical machine translation (SMT) is proposed by 
Brown et al (1993). Although many approaches of 
them were effective, there are still lots of problems 
to solve. Among others, we have an interest in the 
problems occurring with long sentence decoding. 
Various problems occur when we try to translate 
long input sentences because a longer sentence 
contains more possibilities of selecting translation 
options and reordering phrases. However, reorder-
ing models in traditional phrase-based systems are 
not sufficient to treat such complex cases when we 
translate long sentences (Koehn et al 2003). 
Some methods which can offer powerful reor-
dering policies have been proposed like syntax 
based machine translation (Yamada and Knight, 
2001) and Inversion Transduction Grammar (Wu, 
1997). Although these approaches are effective, 
decoding long sentences is still difficult due to 
their computational complexity. As the length of 
an input sentence becomes longer, the analysis and 
decoding become more complex. The complexity 
causes approximations and errors inevitable during 
the decoding search. 
In order to reduce this kind of difficulty caused 
by the complexity, a long sentence can be paraph-
rased by several shorter sentences with the same 
meaning. Generally, however, decomposing a 
complex sentence into sub-sentences requires in-
formation of the sentence structures which can be 
obtained by syntactic or semantic analysis. Unfor-
tunately, the high level syntactic and semantic 
analysis can be erroneous and costs as expensive as 
SMT itself. So, we don?t want to fully analyze the 
sentences to get a series of sub-sentences, and our 
approach to this problem considers splitting only 
compound sentences. 
In the past years, many research works were 
concerned with sentence splitting methods to im-
prove machine translation quality. This idea had 
been used in speech translation (Furuse et al 1998) 
and example based machine translation (Doi and 
Sumita, 2004). These research works achieved 
meaningful results in terms of machine translation 
quality. Unfortunately, however, the method of 
Doi and Sumita using n-gram is not available if the 
source language is Korean. In Korean language, 
most of sentences have special form of ending 
morphemes at the end. For that reason, we should 
determine not only the splitting position but also 
the ending morphemes that we should replace in-
stead of connecting morphemes. And the Furuse et 
al?s method involves parsing which requires heavy 
cost. 
In this paper we propose a transformation based 
splitting method to improve machine translation 
quality which can be applied to the translation 
tasks with Korean as a source language. 
2 Methods 
Our task is splitting a long compound sentence into 
short sub-sentences to improve the performance of 
phrase-based statistical machine translation system. 
We use a transformation based approach to 
accomplish our goal. 
2.1 A Concept of Transformation 
The transformation based learning (TBL) is a kind 
of rule learning methods. The formalism of TBL is 
introduced by Brill (1995). In past years, the TBL 
approach was used to solve various problems in 
natural language processing such as part of speech 
(POS) tagging and parsing (Brill, 1993). 
A transformation consists of two parts: a trigger-
ing environment and a rewriting rule. And the re-
writing rule consists of a source pattern and a tar-
get pattern. Our consideration is how to get the 
right transformations and apply them to split the 
long sentences. 
A transformation works in the following man-
ner; some portion of the input is changed by the 
rewriting rule if the input meets a condition speci-
fied in the triggering environment. The rewriting 
rule finds the source pattern in the input and rep-
laces it with the target pattern. For example, sup-
pose that a transformation which have a triggering 
environment A, source pattern B and target pattern 
C. We can describe this transformation as a sen-
tence: if a condition A is satisfied by an input sen-
tence, then replace pattern B in the input sentence 
with pattern C. 
2.2 A Transformation Based Sentence Split-
ting Method 
Normally, we have two choices when there are 
two or more transformations available for an input 
pattern at the same time. The first choice is apply-
ing the transformation one by one, and the second 
choice is applying them simultaneously. The 
choice is up to the characteristics of the problem 
that we want to solve. In our problem, we choose 
the former strategy which is applying the transfor-
mations one by one, because it gives direct intui-
tion about the process of splitting sentences. By 
choosing this strategy, we can design splitting 
process as a recursive algorithm. 
At first, we try to split an input sentence into 
two sub-sentences. If the sentence has been split by 
some transformation, the result involves exactly 
two sub-sentences. And then we try to split each 
sub-sentence again. We repeat this process in re-
cursive manner until no sub-sentences are split. 
In the above process, a sentence is split into at 
most two sub-sentences through a single trial. In a 
single trial, a transformation works in the follow-
ing manner:  If an input sentence satisfies the envi-
ronment, we substitute the source pattern into the 
target pattern. That is, replace the connecting mor-
phemes with the proper ending morphemes. And 
then we split the sentence with pre-defined posi-
tion in the transformation. And finally, we insert 
the junction word that is also pre-defined in the 
transformation between the split sentences after the 
sub sentences are translated independently. 
From the above process, we can notice easily 
that a transformation for sentence splitting consists 
of the four components: a triggering environment, 
a rewriting rule, a splitting position and a junction 
type. The contents of each component are as fol-
lows. (1) A triggering environment contains a se-
quence of morphemes with their POS tags. (2) A 
rewriting consists of a pair of sequences of POS 
tagged morphemes. (3) A junction type can have 
one of four types: ?and?, ?or?, ?but? and ?NULL?. 
(4) A splitting position is a non-negative integer 
that means the position of starting word of second 
sub-sentence. 
2.3 Learning the Transformation for Sen-
tence Splitting 
At the training phase, TBL process determines 
the order of application (or rank) of the transforma-
tions to minimize the error-rate defined by a spe-
cific measure. The order is determined by choosing 
the best rule for a given situation and applying the 
best rule for each situation iteratively. In the sen-
tence splitting task, we maximize the machine 
translation quality with BLEU score (Papineni et 
al., 2001) instead of minimizing the error of sen-
tence splitting. 
During the training phase, we determine the or-
der of applying transformation after we build a set 
of transformations. To build the set of transforma-
tions, we need manually split examples to learn the 
transformations. 
Building a transformation starts from extracting 
a rewriting rule by calculating edit-distance matrix 
between an original sentence and its split form 
from the corpus. We can easily extract the different 
parts from the matrix. 
BaseBLEU :=  BLEU score of the baseline system 
S := Split example sentence 
T := Extracted initial transformation  
for each t? T  
    for each s?S 
        while true 
             try to split s with t 
             if mis-splitting is occurred 
                  Expand environment 
             else exit while loop 
             if environment cannot be expanded 
                  exit while loop 
S? := apply t to S 
    Decode S? 
    BLEU := measure BLEU 
    Discard t if BLEU < BaseBLEU 
sort  T w.r.t. BLEU 
From the difference pattern, we can make the 
source pattern of a rewriting rule by taking the dif-
ferent parts of the original sentence side. Similarly, 
the target pattern can be obtained from the differ-
ent parts of split form. And the junction type and 
splitting position are directly obtained from the 
difference pattern. Finally, the transformation is 
completed by setting the triggering environment as 
same to the source pattern. The set of initial trans-
formations is obtained by repeating this process on 
all the examples. 
The Transformations for sentence splitting are 
built from the initial transformations through ex-
panding process. In the expanding process, each 
rule is applied to the split examples. We expand 
the triggering environment with some heuristics (in 
section 2.4), if a sentence is a mis-split. 
And finally, in order to determine the rank of 
each transformation, we sorted the extracted trans-
formations by decreasing order of resulted BLEU 
scores after applying the transformation to each 
training sentence. And some transformations are 
discarded if they decrease the BLEU score. This 
process is different from original TBL. The mod-
ified TBL learning process is described in figure 1. 
2.4 Expanding Triggering Environments 
Expanding environment should be treated very 
carefully. If the environment is too specific, the 
transformation cannot be used in real situation. On 
the other hand, if it is too general, then the trans-
formation becomes erroneous. 
Our main strategy for expanding the environ-
ment is to increase context window size of the 
triggering environment one by one until it causes 
no error on the training sentences. In this manner, 
we can get minimal error-free transformations on 
the sentence splitting corpus. 
We use two different windows to define a trig-
gering environment: one for morpheme and anoth-
er for its part of speech (POS) tag. Figure 2 shows 
this concept of two windows. The circles corres-
pond to sequences of morphemes and POS tags in 
a splitting example. Window 1 represents a mor-
pheme context and window 2 represents a POS tag 
context. The windows are independently expanded 
from the initial environment which consists of a 
morpheme ?A? and its POS tag. In the figure, win-
dow 1 is expanded to one forward morpheme and 
one backward morpheme while window 2 is ex-
panded to two backward POS tags. 
In order to control these windows, we defined 
some heuristics by specifying the following three 
policies of expanding windows: no expansion, 
forward only and forward and backward. From 
those three polices, we have 9 combinations of 
heuristics because we have two windows. By ob-
serving the behavior of these heuristics, we can 
estimate what kind of information is most impor-
tant to determine the triggering environment. 
Figure 1. Modified TBL for sentence splitting 
 
 
 
Figure 2. Window-based heuristics for triggering 
environments 
 
 
 
 
  
 
Test No. Window1 policy Window2 policy 
Test 1 
No expansion 
No expansion 
Test 2 Forward only 
Test 3 Free expansion 
Test 4 
Forward only 
No expansion 
Test 5 Forward only 
Test 6 Free expansion 
Test 7 
Free expansion 
No expansion 
Test 8 Forward only 
Test 9 Free expansion 
 
Table 2.Experimental setup 
 
 
 
 
We have at most 4 choices for a single step of 
the expanding procedure: forward morpheme, 
backward morpheme, forward POS tag, and back-
ward POS tag. We choose one of them in a fixed 
order: forward POS tag, forward morpheme, 
backward POS tag and backward morpheme. 
These choices can be limited by 9 heuristics. For 
example, suppose that we use a heuristic with for-
ward policy on morpheme context window and no 
expansion policy for POS tag context window. In 
this case we have only one choice: forward mor-
pheme. 
3  Experiments 
We performed a series of experiments on Korean 
to English translation task to see how the sentence 
splitting affects machine translation quality and 
which heuristics are the best. Our baseline system 
built with Pharaoh (Koehn, 2004) which is most 
popular phrase-based decoder. And trigram lan-
guage model with KN-discounting (Kneser and 
Ney, 1995) built by SRILM toolkit (Stolcke, 2002) 
is used. 
Test 
No. 
# of  af-
fected sen-
tences 
BLEU score 
Before 
splitting 
After 
splitting 
Test 1 209 0.1778 0.1838 
Test 2 142 0.1564 0.1846 
Test 3 110 0.1634 0.1863 
Test 4 9 0.1871 0.2150 
Test 5 96 0.1398 0.1682 
Test 6 100 0.1452 0.1699 
Test 7 8 0.2122 0.2433 
Test 8 157 0.1515 0.1727 
Test 9 98 0.1409 0.1664 
Table 1 shows the corpus statistics used in the 
experiments. The training corpus for MT system 
has been built by manually translating Korean sen-
tences which are collected from various sources. 
We built 123,425 sentence pairs for training SMT, 
1,577 pairs for splitting and another 1,577 pairs for 
testing. The domain of the text is daily conversa-
tions and travel expressions. The sentence splitting 
corpus has been built by extracting long sentences 
from the source-side mono-lingual corpus. The 
sentences in the splitting corpus have been manual-
ly split. 
The experimental settings for comparing 9 heu-
ristics described in the section 2.4 are listed in ta-
ble 2. Each experiment corresponds to a heuristic. 
To see the effect of sentence splitting on transla-
tion quality, we evaluated BLEU score for affected 
sentenced by the splitting.  The results are shown 
in table 3. Each test number shows the effect of 
transformation-based sentence splitting with dif-
ferent window selection heuristics listed in table 2. 
The scores are consistently increased with signifi-
cant differences. After analyzing the results of ta-
ble 3, we notice that we can expect some perfor-
 
SMT Splitting 
Korean English Before Split After Split 
Train # of Sentences 123,425 1,577 1,906 
# of Words 1,083,912 916,950 19,918 20,243 
Vocabulary 15,002 14,242 1,956 1,952 
Test #of Sentences 1,577 - - 
 
Table 1. Corpus statistics 
Table 3. BLEU scores of affected sentences 
 
mance gain when the average sentence length is 
long. 
The human evaluation shows more promising 
results in table 4. In the table, the superior change 
means that the splitting results in better translation 
and inferior means the opposite case. Two ratios 
are calculated to see the effects of sentence split-
ting. The ratio ?sup/inf? shows the ratio of superior 
over inferior splitting. And ratio trans/change 
shows how many sentences are affected by a trans-
formation in an average. In most of the experi-
ments, the number of superior splitting is over 
three times larger than that of inferior ones. This 
result means that the sentence splitting is a helpful 
pre-processing for machine translation. 
We listed some example translations affected by 
sentence splitting in the table 5. In the three cases, 
junction words don?t appear in the results of trans-
lation after split because their junction types are 
NULL that involves no junction word. Although 
several kinds of improvements are observed in su-
perior cases, the most interesting case occurs in 
out-of-vocabulary (OOV) cases. A translation re-
sult has a tendency to be a word salad when 
OOV?s are included in the input sentence. In this 
case, the whole sentence may lose its original 
meaning in the result of translation. But after split-
ting the input sentence, the OOV?s have a high 
chance to be located in one of the split sub-
sentences. Then the translation result can save at 
least a part of its original meaning. This case oc-
curs easily if an input sentence includes only one 
OOV. The Superior change of table 5 is the case. 
Although both baseline and split are far from the 
reference, split catches some portion of the mean-
ing. 
Test 
No. 
# of trans-
formations 
(rules) 
# of 
changes 
(sentences) 
# of supe-
rior 
changes 
# of infe-
rior 
changes 
# of insig-
nificant 
changes 
Ratio 
Sup/Inf 
Ratio 
trans/chang
e 
1 34 209 60 30 119 2.00 6.15 
2 177 142 43 9 90 4.78 0.802 
3 213 110 29 9 72 3.22 0.516 
4 287 9 4 1 4 4.00 0.031 
5 206 96 25 4 67 6.25 0.466 
6 209 100 23 8 69 2.88 0.478 
7 256 8 3 1 4 3.00 0.031 
8 177 157 42 10 102 4.20 0.887 
9 210 98 21 4 73 5.25 0.467 
Table 4. Human evaluation results 
Superior change 
Reference I saw that some items are on sale on window . what are they ? 
Baseline 
What kind of items do you have this item in OOV some discount, I get a 
discount ? 
Split 
You have this item in OOV some discount . what kind of items do I get 
a discount ? 
Insignificant 
change 
Reference What is necessary to be issued a new credit card? 
Baseline I ?d like to make a credit card . What do I need? 
Split I ?d like to make a credit card . What is necessary? 
Inferior change 
 
Reference 
I ?d like to make a reservation by phone and tell me the phone number 
please . 
Baseline 
I ?d like to make a reservation but can you tell me the phone number , 
please . 
Split I  ?d like to make a reservation . can you tell me the , please . 
Table 5. Example translations (The sentences are manually re-cased for readability) 
Most of the Inferior cases are caused by mis-
splitting. Mis-splitting includes a case of splitting a 
sentence that should not be split or splitting a sen-
tence on the wrong position. This case can be re-
duced by controlling the heuristics described in 
section 2.4. But the problem is that the effort to 
reducing inferior cases also reduces the superior 
cases. To compare the heuristics each other in this 
condition, we calculated the ratio of superior and 
inferior cases. The best heuristic is test no. 5 in 
terms of the ratio of sup/inf. 
The test no. 4 and 7 show that a trans-formation 
becomes very specific when lexical information is 
used alone. Hence the ratio trans/change becomes 
below 0.01 in this case.  And test no. 1 shows that 
the transformations with no environment expan-
sion are erroneous since it has the lowest ratio of 
sup/inf. 
4 Conclusion 
We introduced a transformation based sentence 
splitting method for machine translation as a effec-
tive and efficient pre-processing. A transformation 
consists of a triggering environment and a rewrit-
ing rule with position and junction type informa-
tion. The triggering environment of a transforma-
tion is extended to be error-free with respect to 
training corpus after a rewriting rule is extracted 
from manually split examples. The expanding 
process for the transformation can be generalized 
by adding POS tag information into the triggering 
environment. 
The experimental results show that the effect of 
splitting is clear in terms of both automatic evalua-
tion metric and human evaluation. The results con-
sistently state that the statistical machine transla-
tion quality can be improved by transformation 
based sentence splitting method. 
Acknowledgments 
This research was supported by the MIC (Ministry 
of Information and Communication), Korea, under 
the ITRC (Information Technology Research Cen-
ter) support program supervised by the IITA (Insti-
tute of Information Technology Assessment) (II-
TA-2006-C1090-0603-0045). The parallel corpus 
was courteously provided by Infinity Telecom, Inc. 
References 
Eric Brill. 1993. Transformation-based error-driven 
parsing. In Proc. of third International Workshop on 
Parsing.  
Eric Brill. 1995. Transformation-based error-driven 
learning and natural language processing: A Case 
Study in Part-of-Speech Tagging. Computational 
Linguistics 21(4):543-565. 
Peter F. Brown, Stephen A. Della Pietra, Vincent 
J.Della Pietra and Robert L. Mercer. 1993. The Ma-
thematics of Statistical Machine Translation: Parame-
ter estimation. Computational Linguistics, 19(2):263-
312. 
Takao Doi and Eiichiro Sumita. 2004. Splitting input 
sentence for machine translation using language 
model with sentence similarity. In Proc. of the 20th 
international conference on Computational Linguis-
tics. 
Osamu Furuse, Setsuo Yamada and Kazuhide Yamamo-
to. 1998. Splitting Long or Ill-formed Input for Ro-
bust Spoken-language Translation. In Proc of the 36th 
annual meeting on Association for Computational 
Linguistics. 
Reinhard Kneser and Hermann Ney. 1995. Improved 
backing-off for m-gram lnguage modeling. In Proc. 
of the International Conference on Acoustics, Speech, 
and Signal Processing (ICASSP). 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation mod-
els. In Proc. of the 6th Conference of the Association 
for Machine translation in the Americas. 
Philipp Koehn, Franz Josef Och and Kevin Knight. 
2003. Statistical Phrase-Based Translation. In Proc of 
the of the 2003 Conference of the North American 
Chapter of the Association for Computational Lin-
guistics on Human Language Technology. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic 
evaluation of Machine Translation. Technical Report 
RC22176, IBM. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. In Proc. of the 7th International 
Conference on Spoken Language Processing (ICSLP). 
 Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics 23(3):377-404. 
Kenji Yamada and Kevin Knight. 2001. A syntax-based 
statistical translation Model. In Proc. of the confe-
rence of the Association for Computational Linguis-
tics (ACL). 
 
Syllable-Pattern-Based Unknown-
Morpheme Segmentation and Estimation
for Hybrid Part-of-Speech Tagging of
Korean
Gary Geunbae Lee Jeongwon Chay
Pohang University of Science and
Technology
Pohang University of Science and
Technology
Jong-Hyeok Leez
Pohang University of Science and
Technology
Most errors in Korean morphological analysis and part-of-speech (POS) tagging are caused
by unknown morphemes. This paper presents a syllable-pattern-based generalized unknown-
morpheme-estimation method with POSTAG (POStech TAGger),1 which is a statistical and
rule-based hybrid POS tagging system. This method of guessing unknown morphemes is based
on a combination of a morpheme pattern dictionary that encodes general lexical patterns of Korean
morphemes with a posteriori syllable trigram estimation. The syllable trigrams help to calculate
lexical probabilities of the unknown morphemes and are utilized to search for the best tagging
result. This method can guess the POS tags of unknown morphemes regardless of their numbers
and/or positions in an eojeol (a Korean spacing unit similar to an English word), which is not
possible with other systems for tagging Korean. In a series of experiments using three different
domain corpora, the system achieved a 97% tagging accuracy even though 10% of the morphemes
in the test corpora were unknown. It also achieved very high coverage and accuracy of estimation
for all classes of unknown morphemes.
1. Introduction
Part-of-speech (POS) tagging involves many difficult problems, such as insufficient
amounts of training data, inherent POS ambiguities, and (most seriously) many types
of unknown words. Unknown words are ubiquitous in any application and cause
major tagging failures in many cases. Since Korean is an agglutinative language, it
presents more serious problems with unknown morphemes than with unknown words
because more than one morpheme can be unknown in a single word and morpheme
segmentation is usually very difficult.
 NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and
Technology (POSTECH), Pohang, 790-784, Korea. E-mail: gblee@postech.ac.kr.
y NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and
Technology (POSTECH), Pohang, 790-784, Korea. E-mail: himen@postech.ac.kr.
z NLP Laboratory, Electrical and Computer Engineering Division, Pohang University of Science and
Technology (POSTECH), Pohang, 790-784, Korea. E-mail: jhlee@postech.ac.kr.
1 The binary code of POSTAG is open to the public for research and evaluation purposes at
http://nlp.postech.ac.kr/. Follow the link OpenResources!DownLoad.
c? 2002 Association for Computational Linguistics
Computational Linguistics Volume 28, Number 1
Previous techniques for guessing unknown words mostly utilize the guessing rules
to analyze the word features by looking at leading and trailing characters. Most of them
employ the analysis of trailing characters and other features such as capitalization and
hyphenation (Kupiec 1992; Weischedel et al 1993). Some of them use more morpho-
logically oriented word features such as suffixes, prefixes, and character lengths (Brill
1995; Voutilainen 1995). The guessing rules are usually handcrafted using knowledge
of morphology but sometimes are acquired automatically using lexicons and corpora
(Brill 1995; Mikheev 1996; Oflazer and Tu?r 1996). Previously developed methods for
guessing unknown morphemes in Korean are not much different from the methods
used for English. Basically, they rely on the rules that reflect knowledge of Korean
morphology and word formation. The usual way of handling unknown morphemes is
to guess all the possible POS tags for an unknown morpheme by checking connectable
functional morphemes in the same eojeol (Kang 1993).2 However, in this way, it is only
possible to guess probable POS tags for a single unknown morpheme when it occurs
at the beginning of an eojeol. Unlike in English, in Korean, more than one unknown
morpheme can appear in a single eojeol because an eojeol can include complex compo-
nents such as Chinese characters, Japanese words, and other foreign words. If an eojeol
contains more than one unknown morpheme or if the unknown morphemes appear
in other than first position in the eojeol, all previous methods fail to efficiently estimate
them. This is the reason why we try to avoid conventional guessing rules using word
morphology features such as those proposed in Mikheev (1996) and Oflazer and Tu?r
(1996).3
In this paper, we propose a syllable-pattern-based generalized unknown-morph-
eme estimation method using a morpheme pattern dictionary that enables us to treat
unknown morphemes in the same way as registered known morphemes, and thereby
to guess them regardless of their numbers or positions in an eojeol. The method for
estimating unknown morphemes using the morpheme pattern dictionary in Korean
needs to be tightly integrated into morphological analysis and POS disambiguation
systems.
POS disambiguation has usually been performed by statistical approaches, mainly
using the hidden Markov model (HMM) in English research communities (Cutting
et al 1992; Kupiec 1992; Weischedel et al 1993). These approaches are also domi-
nant for Korean, with slight improvements to accommodate the agglutinative nature
of Korean. For Korean, early HMM tagging was based on eojeols. The eojeol-based
tagging model calculates lexical and transition probabilities with eojeols as a unit; it
suffers from severe data sparseness problems since a single eojeol consists of many
different morphemes (Lee, Choi, and Kim 1993). Later, morpheme-based HMM tag-
ging was tried; such models assign a single tag to a morpheme regardless of the
space in a sentence. Morpheme-based tagging can reduce data sparseness problems
but incurs multiple observation sequences in Viterbi decoding since an eojeol can be
segmented in many different ways. Researchers then tried many ways of reducing
computation due to multiple observation sequences, such as shared word sequences
and virtual words (Kim, Lim, and Seo 1995) and two-ply HMM for morpheme unit
computation but restricted within an eojeol (Kim, Im, and Im 1996). However, since
statistical approaches take neighboring tags into account only within a limited win-
2 An eojeol is a Korean spacing unit (similar to an English word), which usually consists of one or more
stem morphemes and a series of functional morphemes.
3 Even though Turkish and Finnish are in the same class of agglutinative languages and German also
has very complex morphological structures, in our view word formation is more diverse and complex
in Korean than in these Western languages because of its mix of Oriental and Western culture.
54
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
dow (usually two or three), sometimes the decision fails to cover important linguistic
contexts necessary for POS disambiguation. Also, approaches using only statistical
methods are inappropriate for idiomatic expressions, for which lexical terms need
to be directly referenced. And especially, statistical approaches alone do not suffice
for agglutinative languages, which usually have complex morphological structures.
In agglutinative languages, a word usually consists of one or more stem morphemes
plus a series of functional morphemes; therefore, each morpheme should receive a
POS tag appropriate to its functional role to cope with the complex morphological
phenomena in such languages. Recently, rule-based approaches, which learn symbolic
tagging rules automatically from a corpus, have been reconsidered, to overcome the
limitations of statistical approaches (Brill 1995). Some systems even perform POS tag-
ging as part of a syntactic analysis process (Voutilainen 1995). Following the success
of transformation-based approaches, attempts have been made to use transformation
rules in systems for tagging Korean (Im, Kim, and Im 1996). However, in general,
rule-based approaches alone are not very robust and are not portable enough to be
adjusted to new tagsets or new languages. Also, they usually perform no better than
their statistical counterparts (Brill 1995). To gain portability and robustness and also
to overcome the limited coverage of statistical approaches, we need to somehow com-
bine the two approaches to gain the advantages of each. In this paper, we propose
a hybrid method that combines statistical and rule-based approaches to POS disam-
biguation and can be tightly coupled with generalized unknown-morpheme-guessing
techniques.
2. Linguistic Characteristics of Korean
Korean is classified as an agglutinative language. In Korean, an eojeol consists of sev-
eral morphemes that have clear-cut morpheme boundaries. For example, na-neun gam-
gi-e geol-lyeoss-dda ?I caught a cold? consists of 3 eojeols and 7 morphemes:4 na(?I?)/T
+ neun(?auxiliary particle?)/jS, gam-gi(?cold?)/MC + e(?adverb and conjunctive parti-
cle?)/jO, geol-li(?catch?)/DR + eoss(?past tense?)/eGS + dda(?final ending?)/eGE. Below
are the characteristics of Korean that must be considered for morphological-level nat-
ural language processing and POS tagging.
 POS tagging of Korean is usually performed on a morpheme basis rather
than on an eojeol basis. Accordingly, morphological analysis is essential
to POS tagging because morpheme segmentation is much more
important and difficult than POS assignment. Moreover, morphological
analysis should segment eojeols that contain unknown morphemes as
well as known morphemes. Hence, unknown-morpheme handling
should be integrated into the morphological analysis process. Because a
single eojeol can have many possible analyses (e.g., na-neun: na(?I?)/T +
neun(?topic marker?)/jS, na(?sprout?)/DR + neun(?adnominal?)/eCNMG,
nal(?fly?)/DI + neun(?adnominal?)/eCNMG, morpheme segmentation is
inherently ambiguous.
 Korean is a postpositional language with many kinds of noun endings
(particles), verb endings, and prefinal verb endings. It is these functional
morphemes, rather than the order of eojeols, that determine grammatical
4 Here, ?+? represents a morpheme boundary in an eojeol and ?/? introduces the POS tag symbols (see
Table 2).
55
Computational Linguistics Volume 28, Number 1
Table 1
Sample distribution of unknown morphemes in
Korean.
Tag # morphemes Tag # morphemes
MC 2,888 (29.7%) S 1,358 (14.0%)
MPN 650 (6.7%) B 603 (6.2%)
MPP 235 (2.4%) T 50 (0.5%)
MPC 56 (0.6%) Symbol 10 (0.1%)
MPO 728 (7.5%) Foreign word 3,140 (32.3%)
relations such as a noun?s syntactic function, a verb?s tense, aspect,
modals, and even modifying relations between eojeols. For example,
ga/jC is a case particle, so the eojeol uri(we)-ga has a subject role due to
the particle ga/jC. Korean has a clear syllable structure within the
morpheme; most nominal content morphemes keep their surface form
when they are combined with functional morphemes.
 Korean is basically an SOV language but has relatively free word order
compared with English. The weight , in Equation (1) (Section 4.1)
reflects the fact that transition probability is less important in Korean
than in English. However, Korean does have some word order
constraints: verbs must appear in sentence-final position, and modifiers
must be placed before the element they modify. So some order
constraints must be selectively utilized as contextual information in the
POS tagging process, which is taken well into account in the design of
error correction rules (Section 4.3).
 Complex spelling changes (irregular conjugations) frequently occur
between morphemes when two morphemes combine to form an eojeol.
These spelling changes make it difficult to segment the original
morphemes before the POS tag symbols are assigned.
 The unknown-morpheme problem in Korean differs in some ways from
the unknown-word problem in English. In English, it is easy to identify
unknown words because they occur between spaces. However, in
Korean, since unknown morphemes are hidden in an eojeol, we only
know that morphological analysis failed in that eojeol; pinpointing the
exact unknown morphemes is usually difficult. This is why, unlike in
English, it is not possible to fully guess an unknown morpheme using
only affixes. The distribution of POS tags for unknown morphemes
extracted from a 130,000-morpheme training corpus (9,718 unknown
morphemes) is shown in Table 1. The distribution from even a small
corpus shows that we need to estimate various parts of speech for
unknown morphemes rather than simply guess them as nouns.
Table 2 shows the tagset that was used in the experiments reported in Section 5.
The tagset was selected from hierarchically organized POS tags for Korean. We defined
about 100 different POS tags, which can be used in morphological analysis as well as
in POS tagging. We also designed over 300 morphotactic adjacency symbols to be
used in morpheme connectivity checks for correct morpheme segmentation (to be
explained in the next section). The POS tags are hierarchically organized symbols
56
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 2
A tagset with 41 tags.
Major category Tag Description
Nominal MC common noun
MPN person name
MPC country name
MPP place name
MPO other proper noun
MD bound noun
T pronoun
S numeral
Predicate DR regular verb
DI irregular verb
HR regular adjective
HI irregular adjective
I i-predicative particle
E existential predicate
b auxiliary verb
Modifier G adnoun
B adverb
Particle y predicative particle
jC case particle
jS auxiliary particle
jO adverb and conjunctive particle
Ending eGE final ending
eGS prefinal ending
eCNDI aux conj ending
eCNDC quote conj ending
eCNMM nominal ending
eCNMG adnominal ending
eCNB adverbial ending
eCC conjunctive ending
Affix + prefix
? suffix
Special symbol su unit symbol
s? left parenthesis
s? right parenthesis
s. sentence closer
s- sentence connection
s, sentence comma
sf foreign word
sh Chinese character
so other symbol
Interjection K interjection
57
Computational Linguistics Volume 28, Number 1
that were iteratively refined from the eight major grammatical categories of Korean:
nominal, predicate, modifier, particle, ending, affix, special symbol, and interjection.
For a given morpheme, the acronym of a path name in the symbol hierarchy up to a
certain level is assigned as a POS tag.5 The rest of the detailed hierarchies, which are
related only to morpheme connectivity, are independently assigned as morphotactic
adjacency symbols. Therefore, we can use either full or partial path names as POS tags
in order to adjust the total number of tags. The size of the tagset can thus be adapted
by refining grammatical categories that are more pertinent to a given application. For
example, for text-indexing applications, we refine nominals more than predicates since
index terms are usually nominals in these applications.
3. Unknown-Morpheme Segmentation during Morphological Analysis
The agglutinative nature of Korean inevitably requires doing morphological analysis
before POS tagging. Morphological analysis, which segments input texts into morpho-
tactically connectable morphemes and assigns all possible POS tags to each morpheme
by looking them up in a morpheme dictionary, is a basic step in natural language pro-
cessing.
Our morphological analysis follows three general steps (Sproat 1992): morpheme
segmentation, recovering original morphemes from spelling changes, and morpho-
tactic modeling. Input texts are scanned from left to right, character by character,6
to be matched with morphemes in a morpheme dictionary. The morpheme dictio-
nary has a trie structured index for fast matching. It also has an independent entry
for each variant surface form (called allomorph) of the original morpheme so the
original morphemes can easily be reconstructed from spelling changes (see Table 3).
For morphotactic modeling, we used the POS tags and the morphotactic adjacency
symbols in the dictionary. The POS tags provide information about morpheme class,
while the morphotactic adjacency symbols provide information about grammatical
connectivity between morphemes needed to form an eojeol. The full hierarchy of POS
tags and morphotactic adjacency symbols is encoded in the morpheme dictionary
for each morpheme. Besides the morpheme dictionary, to model morphemes? con-
nectability to one another the system uses an independent morpheme connectivity ta-
ble that encodes all the connectable pairs of morpheme groups using the morphemes?
tags and morphotactic adjacency symbol patterns. After an input eojeol is segmented
by trie indexed dictionary searches, the morphological analysis checks whether each
segmentation is grammatically connectable by looking in the morpheme connectivity
table.
For unknown-morpheme segmentation, we developed a generalized method for
estimating unknown morphemes regardless of their position and number. Using a
morpheme pattern dictionary, our system can look up unknown morphemes exactly
the same way it looks up known registered morphemes. The morpheme pattern dic-
tionary covers all the necessary syllable patterns for unknown morphemes, including
common nouns, proper nouns, adverbs, regular and irregular verbs, regular and ir-
regular adjectives, and special symbols for foreign words. The lexical patterns for
morphemes are collected from previous studies (Kang 1993) where the constraints on
Korean syllable patterns regarding morpheme connectivity are well described. Table 4
shows some sample entries in the morpheme pattern dictionary, where Z, V, ?*? are
5 For example, nominal(M):proper-noun(P):person-name(N) is a three-level path name.
6 The character sequence in na-neun is n, a, n, eu, n.
58
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 3
Examples of morpheme dictionary entries. MCC is a full POS tag that
identifies a common noun consisting of Chinese characters. MCK identifies a
common noun consisting only of Korean characters. DIgeo-la represents a
geo-la irregular verb, and HIl represents an l irregular adjective. Yu represents
that ga-gong has a final consonant (ng). D-ha, H-ha, and D-doe are morphotactic
adjacency symbols for predicate particles. Nominals that have a D-ha as a
morphotactic adjacency symbol can be connected with predicate particles, and
they play the role of a verb or adjective. In verb or adjective, gyu represents a
regular form of an irregular conjugation, bul represents an irregular form of
an irregular conjugation. Eo is a morphotactic adjacency symbol for vowel
harmony when connecting with endings. Chug-yag represents that a particular
verb (or adjective) contains the special contracted ending. ?>? is a special
symbol for adjacent direction (?>?= right connection; ?<?= left connection).
POS-tag<original form> (Allomorph) [Morphotactic adjacency symbols]
MCC<ga-gong> (ga-gong) [yu>D-ha>H-ha>D-doe>]
MCK<geo-leum> (geo-leum) [yu>D-ha>]
DIgeo-la<geon-neo-ga> (geon-neo-ga) [gyu>chug-yag>]
DId<al-a-deud> (al-a-deud) [gyu>]
DId<al-a-deud> (al-a-deul) [bul>eo>]
DIs<heu-li-jeos> (heu-li-jeo) [bul>eo>]
DIs<heu-li-jeos> (heu-li-jeos) [gyu>]
HIl<ga-neul> (ga-neu) [bul>]
HIl<ga-neul> (ga-neul) [gyu>eo>]
Table 4
Sample entries in the morpheme pattern dictionary. Symbol meanings are
explained in Table 3.
POS-tag<original form> (Allomorph) [Morphotactic adjacency symbols]
HIl<ZV*gal> (ZV*gal) [gyu>eo>]
HIl<ZV*gal> (ZV*ga) [bul>]
HIb<ZV*ZVb> (ZV*u) [bul>]
HIb<ZV*ZVb> (ZV*weo) [chug-yag>]
HIb<ZV*ZVb> (ZV*wa) [chug-yag>]
DIs<ZV*jeos> (ZV*jeos) [gyu>]
DIs<ZV*jeos> (ZV*jeo) [bul>eo>]
DId<ZV*deud> (ZV*deud) [gyu>]
DId<ZV*deud> (ZV*deul) [bul>eo>]
metacharacters that indicate a consonant, a vowel, and any number of Korean charac-
ters, respectively. For example, go-ma-weo ?thanks?, which is a morpheme and an eojeol
at the same time, is matched to (ZV*weo) (shown in Table 4, where it is b, irregular ad-
jective (HIb)) in the morpheme pattern dictionary, which allows the system to recover
its original morpheme form go-mab.
Once the unknown morphemes are identified and recovered using the pattern
dictionary, when checking the unknown morphemes to see if they are connectable,
the system can use the same information about adjacent morphemes in the unknown
morphemes? eojeol that it would use if they were known morphemes. This is the reason
why our method can be called ?generalized? and can identify unknown morphemes
regardless of their position and number in an eojeol. The actual POS estimation is inte-
grated into the POS tagging process that will be described in Section 4.2. The essential
59
Computational Linguistics Volume 28, Number 1
morphological
analyzer
statistical
POS tagger
post error-
corrector
morpheme
dictionary
morpheme
pattern
dictionary
morpheme
connectivity
table
Input
sentence
lexical /
transition
probabilities
syllable
trigram
morpheme
graph
morpheme
graph
tagged
sentence
error-
correcting
rules
Figure 1
Statistical and rule-based hybrid architecture for POS tagging of Korean.
idea of the morpheme pattern dictionary is to pre-collect all the possible general lexical
patterns of Korean morphemes and encode each lexical syllable pattern with all the
candidate POS tags. Therefore, the system can assign initial POS tags to each unknown
morpheme simply by matching the syllable patterns in the pattern dictionary. In this
way, unlike previous approaches, ours does not need to incorporate a special rule-
based unknown-morpheme-handling module into its morphological analyzer, and all
the possible POS tags can be assigned to unknown morphemes just as they are to
known morphemes.
4. A Statistical and Rule-Based Hybrid Tagging Model
Figure 1 shows a proposed hybrid architecture for POS tagging of Korean with syllable-
pattern-based generalized unknown-morpheme guessing. It has three major compo-
nents: the morphological analyzer with unknown-morpheme handler, the statistical
POS tagger, and the rule-based error corrector. The morphological analyzer segments
the morphemes from input eojeols and reconstructs the original morphemes from
spelling changes by recovering the irregular conjugations. It also assigns all possi-
ble POS tags to each morpheme by consulting a morpheme dictionary. The unknown-
morpheme handler, which is tightly integrated into the morphological analyzer, assigns
initial POS tags to morphemes that are not registered in the dictionary, as explained
in the previous section. The statistical POS tagger runs the Viterbi algorithm (Forney
1973) on the morpheme graph to search for the optimal tag sequence for POS dis-
ambiguation. To remedy the defects of a statistical POS tagger, we developed an a
posteriori error correction mechanism. The error corrector is a rule-based transformer
60
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
(Brill 1995), and it corrects mistagged morphemes by consulting lexical patterns and
necessary contextual information.
4.1 The Statistical POS Tagger
The statistical POS tagging model takes the morpheme graph (output of the morpho-
logical analyzer) and selects the best morpheme and POS tag sequence7 for sentences
represented in the morpheme graph. The morpheme graph is a compact way of repre-
senting multiple morpheme sequences for a sentence. Each morpheme?s tag is a node
in the graph and its morpheme connectivity is a link. Our statistical tagging model is
modified from the standard bigrams (Cutting et al 1992) using Viterbi search plus on-
the-fly extra computing of lexical probabilities for unknown morphemes. The equation
used for the statistical tagging model is a modified bigram model with left-to-right
search,
T = argmax T
n
Y
i=1
Pr(ti j ti?1)

Pr(ti j mi)
Pr(ti)

(1)
where T is an optimal tag sequence that maximizes the forward Viterbi scores.
Pr(ti j ti?1) is a bigram tag transition probability and Pr(tijmi)Pr(ti) is a modified morpheme
lexical probability.  and  are weights and are set at 0.4 and 0.6, respectively, which
means that lexical probability is more important than transition probability given the
relatively free word order of Korean. This equation was finally selected after extensive
experiments using the following six equations:
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(mi j ti) (2)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(mi j ti) (3)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(ti j mi) (4)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)Pr(ti j mi) (5)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)
Pr(ti j mi)
Pr(ti)
(6)
T = argmax T
n
Y
i=1
Pr(ti j ti?1)

Pr(ti j mi)
Pr(ti)

(7)
In the experiments, we used the 10,204-morpheme training corpus from the Kemong
Encyclopedia.8 Table 5 shows the tagging performance of each equation.
Training of the statistical tagging model requires a parameter estimation process
for two different parameters, that is, morpheme lexical probabilities and bigram tag
transition probabilities. Several studies show that using as much tagged material as
7 Because a Korean eojeol can be segmented in many different ways, selecting the best morpheme
segmentation sequence is as important as selecting the best POS sequence in POS tagging.
8 Provided by the Electronics and Telecommunications Research Institute (ETRI).
61
Computational Linguistics Volume 28, Number 1
Table 5
Tagging performance (all in %) of each equation. The ?eojeol? row shows
eojeol-unit tagging performance, and the ?morpheme? row shows
morpheme-unit performance.
Eq. (2) Eq. (3) Eq. (4) Eq. (5) Eq. (6) Eq. (7) (Eq. (1))
Eojeol 86.80 90.48 89.40 89.62 91.73 92.48
Morpheme 91.32 94.93 94.40 94.48 95.77 96.12
possible for training gives much better performance than unsupervised training using
the Baum-Welch reestimation algorithm (Merialdo 1994). We therefore decided to use
supervised training using tagged corpora with relative frequency counts. The three
necessary probabilities can be estimated as in Equations (8)?(10),
Pr(ti j mi)  f (ti j mi) =
N(mi, ti)
N(mi)
(8)
Pr(ti)  f (ti) =
N(ti)
PNts
n=1 N(tn)
(9)
Pr(ti j ti?1)  f (ti j ti?1) =
N(ti?1, ti)
N(ti?1)
(10)
where N(mi, ti) indicates the total number of occurrences of the morpheme mi together
with the specific tag ti, while N(mi) indicates the total number of occurrences of the
morpheme mi in the tagged training corpus. Nts indicates the total number of POS tags
in the tagset. N(ti?1, ti) and N(ti?1) can be interpreted similarly for two consecutive
tags ti?1 and ti.
A beam search strategy is utilized for high-speed tagging. For each morpheme in
the sentence, the highest probability, Ph, of the tag is recorded. All other tags associated
with the same morpheme must have probabilities greater than Ph? for some constant
beam size ?; otherwise, they are discarded. The beam may introduce search errors,
but, in practice, search efficiency can be greatly improved with virtually no loss of
accuracy.
4.2 Lexical Probability Estimation for Unknown-Morpheme Guessing
The lexical probabilities for unknown morphemes cannot be precalculated using Equa-
tion (8) since we assume the unknown morphemes do not appear in the training cor-
pus, so a special on-the-fly estimation method must be applied. We suggest using
syllable trigrams since Korean syllables can play an important role in restricting units
for guessing the POS of a morpheme. The lexical probability Pr(tijmi)Pr(ti) for unknown mor-
phemes can be estimated using the frequency of syllable trigram products according
to the formula in (11)?(13) (Nagata 1994),
m = e1e2 : : : en (11)
Pr(t j m)
Pr(t)
 Pr t(e1 j #, #)Pr t(e2 j #, e1)

n
Y
i=3
Pr t(ei j ei?2, ei?1)
62
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
 Pr(# j en?1, en) (12)
Pr t(ei j ei?2, ei?1)  ft(ei j ei?2, ei?1)
+ ft(ei j ei?1)
+ ft(ei) (13)
where m is a morpheme, e is a syllable, t is a POS tag, ?#? is a morpheme boundary
symbol, and ft(ei j ei?2, ei?1) is a frequency datum for tag t with co-occurrence syllables
ei?2, ei?1, and ei. Trigram probabilities are smoothed by Equation (13) to cope with the
data sparseness problem. For example, Park-jong-man is the name of a person, so it is
an unknown morpheme. The lexical probability that Park-jong-man should be assigned
the tag MPN (person name) is estimated using the following formula:
Pr(MPN j Park ? jong ?man)
Pr(MPN )
 PrMPN (Park j #, #)
 PrMPN (jong j #, Park)
 PrMPN (man j Park , jong)
 PrMPN (# j jong , man) (14)
In Park-jong-man, Park is usually a family name. If the first position of this mor-
pheme is a family name, the probability that MPN is the correct tag becomes higher
than the probability that the other tags are correct. Table 6 shows the distribution of
Pr(Park j #, #) for each possible tag. In Equation (14), PrMPN (Park j #, #) represents
the popularity of the tag MPN for the morpheme Park-jong-man.
All the trigrams for Korean syllables were precalculated and stored in the database
and are applied with the candidate tags during the unknown-morpheme POS guessing
and smoothing portion of the statistical tagging process.
4.3 A Posteriori Error Correction Rules
Statistical morpheme tagging is widely known to cover only a limited range of con-
textual information. Moreover, it cannot refer to lexical patterns as a context for POS
disambiguation. As mentioned earlier, because Korean eojeols have very complex mor-
phological structures, it is necessary to look at the functional morphemes selectively to
determine the grammatical relations between eojeols. For these reasons, we designed
error correction rules for eojeols to compensate for the estimation and modeling errors
Table 6
The distribution of Pr(Park j #, #) for each tag.
MC MPN MPC MPP MPO T
No. of ?##Park? 125 2081 0 0 8 0
No. of ?##? 115,841 25,915 589 1,209 50,671 4,255
Pr(Park j #, #) 0.001 0.080 0.000 0.000 0.000 0.000
B DR DI HR HI
No. of ?##Park? 5 17 2 0 9
No. of ?##? 9,169 21,119 13,555 2,243 5,217
Pr(Park j #, #) 0.000 0.000 0.000 0.000 0.001
63
Computational Linguistics Volume 28, Number 1
Table 7
Examples of rule schemata used to extract the error correction rules automatically from the
tagged corpus. The POSTAG system has about 24 rule schemata of this form.
Rule schema Acronym description
N1FT the tag of the first morpheme (FT) of the next eojeol (N1)
P1LT the tag of the last morpheme (LT) of the previous eojeol (P1)
N2FT the tag of the first morpheme (FT) of the eojeol after the next one (N2)
N3FT the tag of the first morpheme (FT) of the second eojeol after the next one (N3)
P1LM the lexical form of the last morpheme (LM) of the previous eojeol (P1)
P1FM the lexical form of the first morpheme (FM) of the previous eojeol (P1)
N1FM the lexical form of the first morpheme (FM) of the next eojeol (N1)
[current eojeol or morpheme] [rule schemata, referenced morpheme or tag]
! [corrected eojeol or morpheme]
Figure 2
Error correction rule format.
of the statistical morpheme tagger. However, designing the error correction rules with
knowledge engineering is tedious and error prone. Instead, we adopted Brill?s ap-
proach (Brill 1995) whereby the error correction rules are learned automatically from a
small amount of tagged corpus. Fortunately, Brill showed that one does not normally
need a large amount of tagged corpus to extract the symbolic tagging rules compared
with statistical tagging. Table 7 shows examples of carefully designed rule schemata
used to extract the error correction rules for Korean, where a rule schema designates
the context of rule applications (i.e., the morpheme position and the lexical/tag deci-
sion in a context eojeol).
The form of the rules that can be automatically learned using the schemata in
Table 7 is shown in Figure 2, where [current eojeol or morpheme] consists of the mor-
pheme (with current tag) sequence in an eojeol, and [corrected eojeol or morpheme]
consists of the morpheme (with corrected tag) sequence in the same eojeol. For exam-
ple, the rule [meog(?Chinese ink0)=MC + eun=jS ][N1FT , MC ] ! [meog(?to eat0)=DR +
eun=eCNMG ] says that the current eojeol was statistically tagged as a common noun
(MC) plus auxiliary particle (jS), but if the next eojeol?s (N1) first-position morpheme
tag (FT) is also MC, the eojeol should be tagged as a regular verb (DR) plus ad-
nominal ending (eCNMG). This statistical error is caused by the ambiguity of the
morpheme meog, which has two meanings: ?Chinese ink? (noun) and ?to eat? (verb).
Since morpheme segmentation is very difficult in Korean, many tagging errors also
arise from the morpheme segmentation errors. Our error correction rules can also cope
with these morpheme segmentation errors by correcting the errors in the whole eojeol
at once. For example, the following rule can correct morpheme segmentation errors:
[jul=MC + i ? go=jO ][P1LM , ] ! [jul ? i=DR + go=eCC ]. This rule says that the eojeol
jul-i-go is usually segmented as a common noun, jul ?string, rope?, plus the adverb
and conjunctive particle i-go, but when the morpheme eul appears before the eojeol,
it should be segmented as a regular verb, jul-i ?shrink?, plus the conjunctive ending
go. This kind of segmentation error correction can greatly enhance the tagging perfor-
mance. The rules are automatically learned by comparing the correctly tagged corpus
with the output of the statistical tagger. The training is leveraged so the error correc-
64
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 8
Performance of the statistical tagger (all in %) on
three document sets, using three progressively
degraded versions of the tagger.
Document set Version 1 Version 2 Version 3
Set 1 96.4 89.5 87.1
Set 2 96.0 92.8 89.0
Set 3 96.7 88.7 84.8
Total 96.4 90.3 87.0
tion rules are gradually learned as the statistically tagged texts are corrected by the
rules learned so far.
5. Experimental Results
5.1 Embedded Performance with Hybrid POS Tagging
For morphological analysis and POS tagging experiments, we used a 130,000-morph-
eme balanced training corpus for statistical parameter estimation and a 50,000-morph-
eme corpus for learning the a posteriori error correction rules. The training corpus was
collected from various sources such as Internet documents, encyclopedias, newspapers,
and school textbooks.
For test sets, we carefully selected three different document sets, aiming for broad
coverage. The first document set (Set 1: 25,299 morphemes, 1,338 sentences), which
was collected from the Kemong Encyclopedia,9 a hotel reservation dialogue corpus,10
and assorted Internet documents, contains about 10% unknown morphemes. The sec-
ond document set (Set 2: 15,250 morphemes, 574 sentences), which consists solely of
Internet documents from assorted domains, such as broadcasting scripts and news-
papers, contains about 8.5% unknown morphemes. The third document set (Set 3:
20,919 morphemes, 555 sentences), which comes from a standard Korean document
set called KTSET 2.011 including academic articles and electronic newspapers, con-
tains about 14% unknown morphemes (mainly technical jargon). Table 8 shows our
system?s statistical tagging performance for these three document sets, using three
progressively degraded versions of the tagging mechanism. Version 1 is a full version
using the statistical method. Version 2 is a somewhat degraded version that does not
use the system?s unknown-morpheme guessing capability but treats all the segmented
unknown morphemes as nouns (the typical method of estimation). Version 3 is an even
more degraded version that rejects all unknown morphemes as tagging failures; this
version does not even perform unknown-morpheme segmentation during morpho-
logical analysis. This experiment verifies the effectiveness of our unknown-morpheme
segmentation and guessing techniques, as shown by the sharp performance drops
between Versions 1, 2, and 3. As another experiment showed, the automatically ac-
quired a posteriori error correction rules also proved to be useful. In this experiment,
two versions of the hybrid tagger were tested on the three document sets. Version 1
was the full POSTAG system with unknown-morpheme segmentation, guessing, and
9 From the Electronics and Telecommunications Research Institute (ETRI).
10 From Sogang University, Seoul, Korea.
11 From KT (Korea Telecom).
65
Computational Linguistics Volume 28, Number 1
Table 9
Performance of the hybrid tagger (all
in %) on three document sets, using
two versions of the tagger.
Document set Version 1 Version 2
Set 1 97.2 96.4
Set 2 96.9 96.0
Set 3 97.4 96.7
Total 97.2 96.4
Table 10
Unknown-morpheme estimation performance
(all in %). Experiments were performed on
three different document sets as before. #UKM
designates the number of unknown morphemes
in each document set and their percentage.
Recall (Rec.) measures the coverage of the
estimation and precision (Pre.) demonstrates its
accuracy.
Document set #UKM Rec. Pre.
Set 1 2,531 (10.0%) 93.9 94.8
Set 2 1,303 (8.5%) 92.9 88.9
Set 3 2,889 (13.8%) 98.0 85.5
Total 6,723 (10.8%) 94.9 89.7
rule-based error correction. Version 2 did not employ a posteriori error correction rules
(the same system as Version 1 in the first experiment). Performance dropped between
Version 1 and Version 2 (see Table 9); however, the drop rates were mild due to the
performance saturation at Version 1, which means that our statistical tagger alone
already achieves state-of-the-art performance for tagging of Korean morphemes.
5.2 Unknown-Morpheme Segmentation and Guessing Performance
To see the independent performance of unknown-morpheme handling more precisely
(explained in Sections 3 and 4.2), we separated the unknown-morpheme performance
from hybrid tagging experiments. Using the same test corpus, we measured the cover-
age and correctness of our unknown-morpheme estimation techniques. Table 10 shows
the results, which were evaluated by the metrics defined as follows:
Recall =
#unknown morphemes detected
#unknown morphemes
(segmentation performance)
Precision =
#unknown morphemes correctly estimated
#unknown morphemes detected
(guessing performance)
When the morphological analyzer meets an unknown morpheme, it is important
to detect first whether it is unknown or not, because sometimes, due to incorrect
segmentation, an unknown morpheme can be incorrectly processed as a known one.
Our system reached an average recall level of 94.9%. Once the unknown morphemes
are detected, the correct POS needs to be estimated. Our system tries to guess the POS
66
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
Table 11
Unknown-morpheme estimation performance (all in %) for each POS tag. N/A
means the morpheme with the corresponding tag does not appear in the
corpus. Recall (Rec.) measures the coverage of the estimation, and precision
(Pre.) demonstrates its accuracy.
Set 1 Set 2 Set 3 Total
POS tag Rec. Pre. Rec. Pre. Rec. Pre. Rec. Pre.
MC 96.9 95.4 94.5 91.7 93.9 72.5 95.1 86.5
MPN 80.0 86.7 87.4 95.0 100.0 100.0 89.1 93.9
MPC 54.3 73.7 72.7 37.5 N/A N/A 42.3 37.1
MPP 75.2 63.3 86.9 75.0 100.0 100.0 87.4 79.4
MPO 79.4 79.4 94.8 68.3 100.0 93.8 91.4 79.7
B 87.9 100.0 42.9 66.7 100.0 100.0 76.9 88.9
T N/A N/A 100.0 100.0 N/A N/A 100.0 100.0
S 99.8 100.0 99.0 100.0 100.0 100.0 99.6 100.0
Foreign word 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
Special symbol 100.0 100.0 N/A N/A 100.0 100.0 100.0 100.0
of every open class item including common nouns, proper nouns, pronouns, numbers,
adverbs, and others.12 The average precision of 89.7% reflects very accurate guessing
considering the range of POSs that need to be estimated. Table 11 shows the system?s
unknown-morpheme guessing performance for each POS tag.
To show the pattern dictionary?s utility, we conducted another experiment in which
we gradually reduced the morpheme dictionary size to see the smooth hybrid tagging
performance (same as in Table 9) drops. As the morpheme dictionary gets smaller,
POSTAG becomes more dependent on the morpheme pattern dictionary and also on
the unknown-morpheme estimation process. From the full dictionary (with 65,000
nouns), we randomly deleted 5,000 nouns step by step for this series of experiments.
(We deleted only nouns because noun estimation is the best arena for showing the sys-
tem?s unknown-morpheme estimation power.) Figure 3 shows the results. Even if the
POSTAG system relies heavily on unknown-morpheme estimation instead of on more
accurate dictionary lookups, the performance drop is very slow. This result explains
why POSTAG can be used on open domain materials such as Internet documents even
when only a small morpheme dictionary is available.
6. Conclusion
This paper presents a pattern-dictionary-based unknown-morpheme estimation method
for generalized and powerful unknown-morpheme segmentation and guessing for a
hybrid POS tagging system. Generalized unknown-morpheme handling is a new and
powerful idea that adopts a morpheme pattern dictionary and syllable-based lexical
probability estimation. The morpheme pattern dictionary enables the system to seg-
ment unknown morphemes in the same way as registered morphemes without any
separate rules for Korean, and thereby to handle them regardless of their numbers
or positions in an eojeol. The paper also presents an error-corrective statistical and
12 Pronouns, numbers, and adverbs may be considered as closed classes. However, in real-world corpora,
we frequently find unexpectedly coined terms in these classes since Korean word formation is affected
by very diverse sources such as foreign words, old Chinese words, archaic pure-Korean words, and so
on.
67
Computational Linguistics Volume 28, Number 1
90
92
94
96
98
100
0 2 4 6 8 10 12
?set1?
?set2?
?set3?
?total?
Figure 3
Hybrid tagging performance change (all in %), showing the utility of the pattern dictionary.
Experiments were performed on three different document sets as before. The x-axis designates
the number of deletion steps whereby the morpheme dictionary was decreased (by 5,000s)
from its full size of 65,000 nouns (Step 0) to 5,000 nouns (Step 12).
rule-based hybrid POS tagging method that exhibits many novel features such as an
experiment-based statistical model for Korean, rule-based error correction, and hier-
archically expandable tagsets. The POSTAG system was developed to test these novel
ideas, especially for agglutinative languages such as Korean. (Japanese, being similar
to Korean in linguistic characteristics, will be a good target for testing these ideas.)
Unlike previous systems, POSTAG is a hybrid tagging system; such a system has never
been tried before, but it turns out to be most suitable for agglutinative languages such
as Korean. POSTAG mainly applies a state-of-the-art HMM tagger for morphemes
but considers multiple observations in the Viterbi score calculation. Because of the
complexity of the morpheme sequence in a Korean eojeol, a morpheme-based HMM?s
tagging accuracy is relatively low for Korean, compared with its accuracy for English.
POSTAG compensates extremely well for the limitations of HMMs by rule-based error
correction. The error correction rules are automatically learned to selectively correct
HMM tagging errors. Similar hybrid methods have been tried for English, but they
integrate HMM tagging and rule-based tagging at the same level (Tapanainen and
Voutilainen 1994). POSTAG integrates morphological analysis with the generalized
68
Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation
unknown-morpheme segmentation so that unknown morphemes can be processed in
the same manner as registered morphemes during tagging. POSTAG also employs
hierarchical tagsets that are flexible enough to expand/shrink according to the given
application. The hierarchical tagset is a novel idea. Most tagging systems for Korean
have applied flat, fixed tagsets and have suffered from using varying tagsets in various
applications. However, POSTAG?s tagsets, based on the over 100 finely differentiated
POS symbols for Korean are hierarchically organized and are flexibly reorganizable ac-
cording to the application at hand. The hierarchical tagsets can be mapped to any other
existing tagset as long as they are fairly well classified and therefore can encourage cor-
pus sharing in the Korean-tagging community. POSTAG is constantly being improved
through expansion of its morpheme dictionary, pattern dictionary, and tagged cor-
pus for statistical and rule-based learning. Since the generalized unknown-morpheme
handling is integrated into the system, POSTAG proves to be a good tagger for open
domain applications such as Internet indexing, filtering, and summarization, and we
are now developing a Web indexer using POSTAG technology.
Acknowledgments
This project was partly supported by
KOSEF (teukjeongkicho #970-1020-301-3,
1997.9-2000.8) and a Ministry of Education
BK21 program awarded to the Electrical
and Computer Engineering Division of
POSTECH. We would like to thank
JunHyeok Shim for coding the
unknown-morpheme estimation
experiments. An earlier version of this
paper was presented at the 6th Workshop
on Very Large Corpora in Montreal, 15?16
August 1998.
References
Brill, E. 1995. Transformation-based
error-driven learning and natural
language processing: A case study in
part-of-speech tagging. Computational
Linguistics, 21:543?565.
Cutting, D., J. Kupiec, J. Pedersen, and P.
Sibun. 1992. A practical part-of-speech
tagger. In Proceedings of the 3rd Conference
on Applied Natural Language Processing,
pages 133?140.
Forney, G. 1973. The Viterbi algorithm.
Proceedings of the IEEE, 61:268?278.
Im, H. S., J. D. Kim, and H. C. Im. 1996.
Transformation rule-based tagging
considering Korean characteristics. In
Proceedings of the Spring Conference of the AI
SIG Meeting of the Korean Information Science
Society, pages 3?10. (Written in Korean.)
Kang, S. S. 1993. Korean Morphological
Analysis Using Syllable Information and
Multiple-Word Units. Ph.D. thesis,
Department of Computer Engineering,
Seoul National University. (Written in
Korean.)
Kim, J. D., H. S. Im, and H. C. Im. 1996.
Morpheme-based Korean part-of-speech
tagging model considering eojeol-unit
contexts. In Proceedings of the Spring
Conference of the Korean Cognitive Science
Society, pages 97?106. (Written in Korean.)
Kim, J. H., C. S. Lim, and J. Seo. 1995. An
efficient Korean part-of-speech tagging
using a hidden Markov model. Journal of
the Korean Information Science Society,
22:136?146. (Written in Korean.)
Kupiec, J. 1992. Robust part-of-speech
tagging using a hidden Markov model.
Computer Speech and Language, 6:225?242.
Lee, U. J., K. S. Choi, and G. C. Kim. 1993.
Korean text-tagging system. In Proceedings
of the Spring Conference of the Korean
Information Science Society, pages 805?808.
(Written in Korean.)
Merialdo, B. 1994. Tagging English text with
a probabilistic model. Computational
Linguistics, 20:155?171.
Mikheev, A. 1996. Unsupervised learning of
word-category guessing rules. In
Proceedings of the 34th Annual Meeting of the
Association for the Computational Linguistics,
pages 327?334.
Nagata, M. 1994. A stochastic Japanese
morphological analyzer using a
forward-DP backward-A N-best search
algorithm. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 201?207.
Oflazer, K. and G. Tu?r. 1996. Combining
hand-crafted rules and unsupervised
learning in constraint-based
morphological disambiguation. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
pages 69?81.
69
Computational Linguistics Volume 28, Number 1
Sproat, R. 1992. Morphology and Computation.
MIT Press, Cambridge, MA.
Tapanainen, P. and A. Voutilainen. 1994.
Tagging accurately?don?t guess if you
know. In Proceedings of the Conference on
Applied Natural Language Processing,
pages 149?156.
Voutilainen, A. 1995. A syntax-based
part-of-speech analyzer. In Proceedings of
the 7th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 157?164.
Weischedel, R., M. Meteer, R. Schwartz, L.
Rawshaw, and J. Ralmucci. 1993. Coping
with ambiguity and unknown words
through probabilistic models.
Computational Linguistics, 19:359?382.
70
MMR-based feature selection for text categorization 
 
Changki Lee 
Dept. of Computer Science and Engineering
Pohang University of Science & Technology
San 31, Hyoja-Dong, Pohang,  
790-784, South Korea 
phone: +82-54-279-5581   
leeck@postech.ac.kr
Gary Geunbae Lee 
Dept. of Computer Science and Engineering
Pohang University of Science & Technology
San 31, Hyoja-Dong, Pohang,  
790-784, South Korea 
phone: +82-54-279-5581   
gblee@postech.ac.kr
 
Abstract 
We introduce a new method of feature selec-
tion for text categorization. Our MMR-based 
feature selection method strives to reduce re-
dundancy between features while maintaining 
information gain in selecting appropriate fea-
tures for text categorization. Empirical results 
show that MMR-based feature selection is 
more effective than Koller & Sahami?s 
method, which is one of greedy feature selec-
tion methods, and conventional information 
gain which is commonly used in feature selec-
tion for text categorization. Moreover, MMR-
based feature selection sometimes produces 
some improvements of conventional machine 
learning algorithms over SVM which is 
known to give the best classification accuracy. 
1  Introduction 
Text categorization is the problem of automatically as-
signing predefined categories to free text documents. A 
growing number of statistical classification methods and 
machine learning techniques have been applied to text 
categorization in recent years [9]. 
A major characteristic, or difficulty, of text catego-
rization problems is the high dimensionality of the fea-
ture space [10]. The native feature space consists of the 
unique terms that occur in documents, which can be tens 
or hundreds of thousands of terms for even a moderate-
sized text collection. This is prohibitively high for many 
machine learning algorithms. If we reduce the set of 
features considered by the algorithm, we can serve two 
purposes. We can considerably decrease the running 
time of the learning algorithm, and we can increase the 
accuracy of the resulting model. In this line, a number 
of researches have recently addressed the issue of fea-
ture subset selection [2][4][8]. Yang and Pederson 
found information gain (IG) and chi-square test (CHI) 
most effective in aggressive term removal without los-
ing categorization accuracy in their experiments [8]. 
Another major characteristic of text categorization 
problems is the high level of feature redundancy [11]. 
While there are generally many different features rele-
vant to classification task, often several such cues occur 
in one document. These cues are partly redundant. Na-
?ve Bayes, which is a popular learning algorithm, is 
commonly justified using assumptions of conditional 
independence or linked dependence [12]. However, the-
ses assumptions are generally accepted to be false for 
text. To remove these violations, more complex de-
pendence models have been developed [13]. 
Most previous works of feature selection empha-
sized only the reduction of high dimensionality of the 
feature space [2][4][8]. The most popular feature selec-
tion method is IG. IG works well with texts and has 
often been used. IG looks at each feature in isolation 
and measures how important it is for the prediction of 
the correct class label. In cases where all features are 
not redundant with each other, IG is very appropriate. 
But in cases where many features are highly redundant 
with each other, we must utilize other means, for exam-
ple, more complex dependence models. 
In this paper, for the high dimensionality of the fea-
ture space and the high level of feature redundancy, we 
propose a new feature selection method which selects 
each feature according to a combined criterion of infor-
mation gain and novelty of information. The latter 
measures the degree of dissimilarity between the feature 
being considered and previously selected features. 
Maximal Marginal Relevance (MMR) provides pre-
cisely such functionality [5]. So we propose MMR-
based feature selection method which strives to reduce 
redundancy between features while maintaining infor-
mation gain in selecting appropriate features for text 
categorization. 
In machine learning field, some greedy methods that 
add or subtract a single feature at a time have been de-
veloped for feature selection [3][14]. S. Della Pietra et 
al. proposed a method for incrementally constructing 
random field [14]. Their method builds increasingly 
complex fields to approximate the empirical distribution 
of a set of training examples by allowing features. Fea-
tures are incrementally added to the field using a top-
down greedy algorithm, with the intent of capturing the 
salient properties of the empirical sample while allow-
ing generalization to new configurations. However the 
method is not simple, and this is problematic both com-
putationally and statistically in large-scale problems. 
Koller and Sahami proposed another greedy feature 
selection method which provides a mechanism for 
eliminating features whose predictive information with 
respect to the class is subsumed by the other features [3]. 
This method is also based on the Kullback-Leibler di-
vergence to minimize the amount of predictive informa-
tion lost during feature elimination. 
In order to compare the performances of our method 
and greedy feature selection methods, we implemented 
Koller and Sahami?s method, and empirically tested it in 
section 4. 
We also compared the performance of conventional 
machine learning algorithms using our feature selection 
method with Support Vector Machine (SVM) using all 
features in section 4. Previous works show that SVM 
consistently achieves good performance on text catego-
rization tasks, outperforming existing methods substan-
tially and significantly [10][11]. With its ability to 
generalize well in high dimensional feature spaces and 
high level of feature redundancy, SVM is known that it 
does not need any feature selection [11]. 
The remainder of this paper is organized as follows. 
In section 2, we describe the Maximal Marginal Rele-
vance, and in section 3, we describe the MMR-based 
feature selection. Section 4 presents the in-depth ex-
periments and the results. Section 5 concludes the re-
search. 
2 Maximal Marginal Relevance 
Most modern IR search engines produce a ranked list of 
retrieved documents ordered by declining relevance to 
the user's query. In contrast, the need for ?relevant nov-
elty? was motivated as a potentially superior criterion. A 
first approximation to relevant novelty is to measure the 
relevance and the novelty independently and provide a 
linear combination as the metric. 
The linear combination is called ?marginal rele-
vance? - i.e. a document has high marginal relevance if 
it is both relevant to the query and contains minimal 
similarity to previously selected documents. In docu-
ment retrieval and summarization, marginal relevance is 
strived to maximize, hence the method is labeled 
?Maximal Marginal Relevance? (MMR) [5]. 
?
?
?
?
?
?
???
=
??
),(max)1(),(max 21\ jiSDiSRD DDSimQDSimArg
MMR
ji
??
 
where C={D1,?,Di,?} is a document collection (or 
document stream); Q is a query or user profile; R = 
IR(C, Q, ? ), i.e., the ranked list of documents retrieved 
by an IR system, given C and Q and a relevance thresh-
old ? , below which it will not retrieve documents (?   
can be degree of match or number of documents); S is 
the subset of documents in R which is already selected; 
R\S is the set difference, i.e. the set of as yet unselected 
documents in R; Sim1 is the similarity metric used in 
document retrieval and relevance ranking between 
documents (passages) and a query; and Sim2 can be the 
same as Sim1 or a different metric. 
3  MMR-based Feature Selection 
We propose a MMR-based feature selection which 
selects each feature according to a combined criterion of 
information gain and novelty of information. We define 
MMR-based feature selection as follows: 
?
?
?
?
?
?
???
=
??
)|;(max)1();(max
_
\
CwwIGpairCwIGArg
FSMMR
jiSwiSRw ji
??
  
where C is the set of class labels, R is the set of candi-
date features, S is the subset of features in R which was 
already selected, R\S is the set difference, i.e. the set of 
as yet unselected features in R, IG is the information 
gain scores, and IGpair is the information gain scores of 
co-occurrence of the word (feature) pairs. IG and IGpair 
are defined as follows: 
?
?
?
+
+
?=
k
ikiki
k
ikiki
k
kki
wCpwCpwp
wCpwCpwp
CpCpCwIG
)|(log)|()(                 
)|(log)|()(                 
)(log)();(
 
?
?
?
+
+
?=
k
jikjikji
k
jikjikji
k
kkji
wCpwCpwp
wCpwCpwp
CpCpCwwIGpair
)|(log)|()(                
)|(log)|()(                
)(log)()|;(
,,,
,,,
 
where p(wi) is the probability that word wi occurred, iw  
means that word wi doesn?t occur, p(Ck) is the probabil-
ity of the k-th class value, p(Ck|wi) is the conditional 
probability of the k-th class value given that wi occurred, 
p(wi,j) is the probability that wi and wj co-occurred, and 
iw  means that wi and wj doesn?t co-occur but wi or wj 
can occur (i.e. )(1)( ,, jiji wpwp ?= ). 
Given the above definition, MMR_FS computes in-
crementally the information gain scores when the 
parameter ? =1, and computes a maximal diversity 
among the features in R when ? =0. For intermediate 
values of ?  in the interval [0,1], a linear combination of 
both criteria is optimized. 
4 Experiments 
In order to compare the performance of MMR-based 
feature selection method with conventional IG and 
greedy feature selection method (Koller & Sahami?s 
method, labeled ?Greedy?), we evaluated the three fea-
ture selection methods with four different learning algo-
rithms: naive Bayes, TFIDF/Rocchio, Probabilistic 
Indexing (PrTFIDF [7]) and Maximum Entropy using 
Rainbow [6]. 
We also compared the performance of conventional 
machine learning algorithms using our feature selection 
method and SVM using all features. 
MMR-based feature selection and greedy feature se-
lection method (Koller & Sahami?s method) requires 
quadratic time with respect to the number of features. 
To reduce this complexity, for each data set, we first 
selected 1000 features using IG, and then we applied 
MMR-based feature selection and greedy feature selec-
tion method to the selected 1000 features. 
For all datasets, we did not remove stopwords. The 
results reported on all dataset are averaged over 10 
times of different test/training splits. A random subset 
of 20% of the data considered in an experiment was 
used for testing (i.e. we used Rainbow?s ?--test-set=0.2? 
and ?--test=10? options), because Rainbow does not 
support 10-fold cross validation. 
MMR-based feature selection method needs to tune 
for ? . It appears that a tuning method based on held-out 
data is needed here. We tested our method using 11 ?  
values (i.e. 0, 0.1, 0.2, ?, 1) and selected the best ?  
value. 
4.1 Reuters-21578 
The Reuters-21578 corpus contains 21578 articles taken 
from the Reuters newswire. Each article is typically 
designated into one or more semantic categories such as 
?earn?, ?trade?, ?corn? etc., where the total number of 
categories is 114. 
Following [3], we constructed a subset from Reuter 
corpus. The subset is comprised of articles on the topic 
?coffee?, ?iron-steel?, and ?livestock?.  
4.2 WebKB 
This data set contains WWW-pages collected from 
computer science departments of various universities in 
January 1997 by the World Wide Knowledge Base 
(WebKb) project of the CMU text learning group. The 
8282 pages were manually classified into 7 categories: 
?course?, ?department?, ?faculty?, ?project?, ?staff?, ?stu-
dent? and ?other?. Following [1], we discarded the cate-
gories ?other?, ?department? and ?staff?. The remaining 
part of the corpus contains 4199 documents in four 
categories. 
4.3 Experimental Results 
 
Figure 1 displays the performance curves for four dif-
ferent machine learning algorithms on the subset of 
Reuters after term selection using MMR-based feature 
selection (number of features is 25). When the parame-
ter ? =0.5, most machine learning algorithms have best 
performance and significant improvements compared to 
conventional information gain (i.e. ? =1) and SVM us-
ing all features. 
 
Table 1. WebKB. 
 
 
Table 1 shows the performance of four machine 
learning algorithms on WebKB using three feature se-
lection methods and all features (41763 terms). In this 
data set, again MMR-based feature selection has best 
performance and significant improvements compared to 
greedy method and IG. Using MMR-based feature se-
lection, for example, the vocabulary is reduced from 
41763 terms to 200 (a 99.5% reduction), and the accu-
racy is improved from 85.26% to 90.49% in Na?ve 
Bayes. Using greedy method and IG, however, the accu-
racy is improved from 85.26% to about 87% in Na?ve 
Figure 1. MMR feature selection for four machine 
learning algorithms on Reuters (#features=25).
Bayes. PrTFIDF is most sensitive to feature selection 
method. Using MMR-based feature selection the best 
accuracy is 82.47%. Using greedy method and IG, how-
ever, the best accuracy is only 72~74%. In this dataset, 
however, MMR-based feature selection does not pro-
duce improvements of conventional machine learning 
algorithms over SVM. 
The observation in Reuters and WebKB are highly 
consistent. MMR-based feature selection is consistently 
more effective than greedy method and IG on two data 
sets, and sometimes produces improvements even over 
the best SVM. 
5 Conclusion 
In this paper, we proposed a MMR-based feature selec-
tion method which strives to reduce redundancy be-
tween features while maintaining information gain in 
selecting appropriate features for text categorization. 
We carried out extensive experiments to verify the 
proposed method. Based on the experiment results, we 
can verify that MMR-based feature selection is more 
effective than Koller & Sahami?s method, which is one 
kind of greedy methods, and conventional information 
gain which is commonly used in feature selection for 
text categorization. Besides, MMR-based feature selec-
tion method sometimes produces improvements of con-
ventional machine learning algorithms over SVM which 
is known to give the best classification accuracy. 
A disadvantage in using MMR-based feature selection 
is that the computational cost of computing the pairwise 
information gain (i.e. IGpair) is quadratic time with 
respect to the number of features. To reduce this compu-
tational cost, we can use MMR-based feature selection 
method on the reduced feature set resulting from IG as 
our experiments in section 4. Another drawback of our 
method is the need to tune for ? . It appears that a tun-
ing method based on held-out data is needed here 
References 
[1] Andrew Mccallum and Kamal Nigam. 1998.  A 
Comparison of Event Models for Naive Bayes Text 
Classification. In AAAI-98 Workshop on Learning 
for Text Categorization. 
[2] David D. Lewis and Marc Ringuette. 1994.  A Com-
parison of Two Learning Algorithms for Text Cate-
gorization. In Proceedings of SDAIR-94, 3rd Annual 
Symposium on Document Analysis and Information 
Retrieval. 
[3] Daphne Koller and Mehran Sahami. 1996.  Toward 
Optimal Feature Selection. In Proceedings of ICML-
96, 13th International Conference on Machine Learn-
ing. 
[4] Hinrich Sch?tze and David A. Hull, and Jan O. 
Pedersen. 1995.  A Comparison of Classifiers and 
Document Representations for the Routing Problem. 
In Proceedings of the 18th Annual International 
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval. 
[5] Jaime Carbonell and Jade Goldstein. 1998.  The Use 
of MMR, Diversity-Based Reranking for Reordering 
Documents and Producing Summaries. In Proceed-
ings of the 21st ACM-SIGIR International Confer-
ence on Research and Development in Information 
Retrieval. 
[6] McCallum and Andrew Kachites. 1996.  Bow: A 
toolkit for statistical language modelling, text re-
trieval, classification and clustering. 
http://www.cs.cmu.edu/~mccallum/bow. 
[7] Thorsten Joachims. 1997.  A probabilistic analysis 
of the Rocchio algorithm with TFIDF for text catego-
rization. In Proceedings of ICML-97, 14th Interna-
tional Conference on Machine Learning. 
[8] Yiming Yang and Jan O. Pedersen. 1997.  A Com-
parative Study on Feature Selection in Text Catego-
rization. In Proceedings of ICML-97, 14th 
International Conference on Machine Learning. 
[9] Yiming Yang and Xin Liu. 1999.  A re-examination 
of text categorization methods. In Proceedings of the 
22nd ACM-SIGIR International Conference on Re-
search and Development in Information Retrieval. 
[10] Thorsten Joachims. 1998.  Text Categorization with 
Support Vector Machines: Learning with Many Rele-
vant Features. In Proceedings of ECML-98, 10th 
European Conference on Machine Learning. 
[11] Thorsten Joachims. 2001.  A Statistical Learning 
Model of Text Classification for Support Vector Ma-
chines. In Proceedings of the 24th ACM-SIGIR In-
ternational Conference on Research and 
Development in Information Retrieval. 
[12] William S. Cooper. 1991.  Some Inconsistencies 
and Misnomers in Probabilistic Information Re-
trieval. In Proceedings of the 14th ACM SIGIR In-
ternational Conference on Research and 
Development in Information Retrieval. 
[13] Mehran Sahami. 1998.  Using Machine Learning to 
Improve Information Access. PhD thesis, Stanford 
University. 
[14] Stephen Della Pietra, Vincent Della Pietra, and 
John Lafferty. 1997.  Inducing Features of Random 
Fields. IEEE Transactions on Pattern Analysis and 
Machine Intelligence. 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 69?72,
New York, June 2006. c?2006 Association for Computational Linguistics
MMR-based Active Machine Learning  
for Bio Named Entity Recognition 
Seokhwan Kim1 Yu Song2 Kyungduk Kim1 Jeong-Won Cha3 Gary Geunbae Lee1
1 Dept. of Computer Science and Engineering, POSTECH, Pohang, Korea 
2 AIA Information Technology Co., Ltd. Beijing, China 
3 Dept. of Computer Science, Changwon National University, Changwon, Korea 
megaup@postech.ac.kr, Song-Y.Song@AIG.com, getta@postech.ac.kr
jcha@changwon.ac.kr, gblee@postech.ac.kr
Abstract 
This paper presents a new active learning 
paradigm which considers not only the 
uncertainty of the classifier but also the 
diversity of the corpus. The two measures 
for uncertainty and diversity were com-
bined using the MMR (Maximal Marginal 
Relevance) method to give the sampling 
scores in our active learning strategy. We 
incorporated MMR-based active machine-
learning idea into the biomedical named-
entity recognition system. Our experimen-
tal results indicated that our strategies for 
active-learning based sample selection 
could significantly reduce the human ef-
fort. 
1 Introduction 
Named-entity recognition is one of the most ele-
mentary and core problems in biomedical text min-
ing. To achieve good recognition performance, we 
use a supervised machine-learning based approach 
which is a standard in the named-entity recognition 
task. The obstacle of supervised machine-learning 
methods is the lack of the annotated training data 
which is essential for achieving good performance. 
Building a training corpus manually is time con-
suming, labor intensive, and expensive. Creating 
training corpora for the biomedical domain is par-
ticularly expensive as it requires domain specific 
expert knowledge. 
One way to solve this problem is through active 
learning method to select the most informative 
samples for training. Active selection of the train-
ing examples can significantly reduce the neces-
sary number of labeled training examples without 
degrading the performance. 
Existing work for active learning explores two 
approaches: certainty or uncertainty-based methods 
(Lewis and Gale 1994; Scheffer and Wrobel 2001; 
Thompson et al 1999) and committee-based 
methods (Cohn et al 1994; Dagan and Engelson 
1995; Freund et al 1997; Liere and Tadepalli 
1997). Uncertainty-based systems begin with an 
initial classifier and the systems assign some un-
certainty scores to the un-annotated examples. The 
k examples with the highest scores will be anno-
tated by human experts and the classifier will be 
retrained. In the committee-based systems, diverse 
committees of classifiers were generated. Each 
committee member will examine the un-annotated 
examples. The degree of disagreement among the 
committee members will be evaluated and the ex-
amples with the highest disagreement will be se-
lected for manual annotation. 
Our efforts are different from the previous ac-
tive learning approaches and are devoted to two 
aspects: we propose an entropy-based measure to 
quantify the uncertainty that the current classifier 
holds. The most uncertain samples are selected for 
human annotation. However, we also assume that 
the selected training samples should give the dif-
ferent aspects of learning features to the classifica-
tion system. So, we try to catch the most 
representative sentences in each sampling. The 
divergence measures of the two sentences are for 
the novelty of the features and their representative 
levels, and are described by the minimum similar-
ity among the examples. The two measures for un-
certainty and diversity will be combined using the 
MMR (Maximal Marginal Relevance) method 
(Carbonell and Goldstein 1998) to give the sam-
pling scores in our active learning strategy. 
69
We incorporate MMR-based active machine-
learning idea into the POSBIOTM/NER (Song et 
al. 2005) system which is a trainable biomedical 
named-entity recognition system using the Condi-
tional Random Fields (Lafferty et al 2001) ma-
chine learning technique to automatically identify 
different sets of biological entities in the text. 
2 MMR-based Active Learning for Bio-
medical Named-entity Recognition 
2.1 Active Learning 
We integrate active learning methods into the 
POSBIOTM/NER (Song et al 2005) system by the 
following procedure: Given an active learning 
scoring strategy S and a threshold value th, at each 
iteration t, the learner uses training corpus TMt   to 
train the NER module Mt. Each time a user wants 
to annotate a set of un-labeled sentences U, the 
system first tags the sentences using the current 
NER module Mt. At the same time, each tagged 
sentence is assigned with a score according to our 
scoring strategy S. Sentences will be marked if its 
score is larger than the threshold value th. The tag 
result is presented to the user, and those marked 
ones are rectified by the user and added to the 
training corpus. Once the training data accumulates 
to a certain amount, the NER module Mt will be 
retrained. 
2.2 Uncertainty-based Sample Selection 
We evaluate the uncertainty degree that the current 
NER module holds for a given sentence in terms of 
the entropy of the sentence. Given an input se-
quence o, the state sequence set S is a finite set. 
And  is the probability distribu-
tion over S. By using the equation for CRF 
(Lafferty et al 2001) module, we can calculate the 
probability of any possible state sequence s given 
an input sequence o. Then the entropy of  
is defined to be: 
Sso|s ??   ),(p
)( o|s?p
? ???=
s
o|so|s )]([log)( 2 PPH  
The number of possible state sequences grows 
exponentially as the sentence length increases. In 
order to measure the uncertainty by entropy, it is 
inconvenient and unnecessary to compute the 
probability of all the possible state sequences. In-
stead we implement N-best Viterbi search to find 
the N state sequences with the highest probabilities. 
The entropy H(N) is defined as the entropy of the 
distribution of the N-best state sequences: 
? ??= = ?
?
= ?
?
???
?
???
?
?=
N
i
N
i i
i
N
i i
i
P
P
P
P
NH
1
1
2
1
)(
)(
log
)(
)(
)(
o|s
o|s
o|s
o|s .  (1) 
The range of the entropy H(N) is [0, 
N
1
log 2? ] which varies according to different N. 
We could use the equation (2) to normalize the 
H(N) to [0, 1]. 
N
NH
NH
1
log
)(
)(
2?
=? .  (2) 
2.3 Diversity-based Sample Selection 
We measure the sentence structure similarity to 
represent the diversity and catch the most represen-
tative ones in order to give more diverse features to 
the machine learning-based classification systems. 
We propose a three-level hierarchy to represent 
the structure of a sentence. The first level is NP 
chunk, the second level is Part-Of-Speech tag, and 
the third level is the word itself. Each word is rep-
resented using this hierarchy structure. For exam-
ple in the sentence "I am a boy", the word "boy" is 
represented as w
r
=[NP, NN, boy]. The similarity 
score of two words is defined as: 
)()(
),(2
)(
21
21
21 wDepthwDepth
wwDepth
wwsim rr
rrrr
+
?=?  
Where ),( 21 wwDepth
rr
 is defined from the top 
level as the number of levels that the two words are 
in common. Under our three-level hierarchy 
scheme above, each word representation has depth 
of 3. 
The structure of a sentence S is represented as 
the word representation vectors ],  ,,[ 21 Nwww
rKrr . 
We measure the similarity of two sentences by the 
standard cosine-similarity measure. The similarity 
score of two sentences is defined as: 
2211
21
21 ),(
SSSS
SS
SSsimilarity rrrr
rrrr
??
?= , 
?? ?=?
i j
ji wwsimSS )( 2121
rrrr
. 
70
2.4 MMR Combination for Sample Selection 
We would like to score the sample sentences with 
respect to both the uncertainty and the diversity. 
The following MMR (Maximal Marginal Rele-
vance) (Carbonell and Goldstein 1998) formula is 
used to calculate the active learning score: 
),(Similaritymax                   
)1(),(yUncertaint)(
jiTs
i
def
i
ss
Mssscore
Mj??
???= ??   (3) 
where si is the sentence to be selected, Uncertainty 
is the entropy of si given current NER module M, 
and Similarity indicates the divergence degree be-
tween the si and the sentence sj in the training cor-
pus TM of M. The combination rule could be 
interpreted as assigning a higher score to a sen-
tence of which the NER module is uncertain and 
whose configuration differs from the sentences in 
the existing training corpus. The value of parame-
ter ?  coordinates those two different aspects of 
the desirable sample sentences. 
After initializing a NER module M and an ap-
propriate value of the parameter? , we can assign 
each candidate sentence a score under the control 
of the uncertainty and the diversity. 
3 Experiment and Discussion 
3.1 Experiment Setup 
We conducted our active learning experiments us-
ing pool-based sample selection (Lewis and Gale 
1994). The pool-based sample selection, in which 
the learner chooses the best instances for labeling 
from a given pool of unlabelled examples, is the 
most practical approach for problems in which 
unlabelled data is relatively easily available. 
For our empirical evaluation of the active learn-
ing methods, we used the training and test data 
released by JNLPBA (Kim et al 2004). The train-
ing corpus contains 2000 MEDLINE abstracts, and 
the test data contains 404 abstracts from the 
GENIA corpus. 100 abstracts were used to train 
our initial NER module. The remaining training 
data were taken as the pool. Each time, we chose k 
examples from the given pool to train the new 
NER module and the number k varied from 1000 
to 17000 with a step size 1000. 
We test 4 different active learning methods: Ran-
dom selection, Entropy-based uncertainty selection, 
Entropy combined with Diversity, and Normalized 
Entropy (equation (2)) combined with Diversity. 
When we compute the active learning score using 
the entropy based method and the combining 
methods we set the values of parameter N (from 
equation (1)) to 3 and ?  (from equation (3)) to 0.8 
empirically. 
 
Fig1. Comparison of active learning strategies with the ran-
l in the y-axis shows the 
per
bin  
ies consistently outperform 
the
dom selection 
3.2 Results and Analyses 
The initial NER module gets an F-score of 52.54, 
while the F-score performance of the NER module 
using the whole training data set is 67.19. We plot-
ted the learning curves for the different sample 
selection strategies. The interval in the x-axis be-
tween the curves shows the number of examples 
selected and the interva
formance improved. 
We compared the entropy, entropy combined 
with sentence diversity, normalized entropy com-
ed with sentence diversity and random selection.
The curves in Figure 1 show the relative per-
formance. The F-score increases along with the 
number of selected examples and receives the best 
performance when all the examples in the pool are 
selected. The results suggest that all three kinds of 
active learning strateg
 random selection.  
The entropy-based example selection has im-
proved performance compared with the random 
selection. The entropy (N=3) curve approaches to 
the random selection around 13000 sentences se-
lected, which is reasonable since all the methods 
choose the examples from the same given pool. As 
71
the number of selected sentences approaches the 
pool size, the performance difference among the 
different methods gets small. The best performance 
of the entropy strategy is 67.31 when 17000 exam-
ple
the
 normalized combined strategy 
behaves the worst. 
4 Conclusion 
ction could significantly reduce 
the human effort. 
by Minis-
try of Commerce, Industry and Energy. 
s are selected. 
Comparing with the entropy curve, the com-
bined strategy curve shows an interesting charac-
teristic. Up to 4000 sentences, the entropy strategy 
and the combined strategy perform similarly. After 
the 11000 sentence point, the combined strategy 
surpasses the entropy strategy. It accords with our 
belief that the diversity increases the classifier's 
performance when the large amount of samples is 
selected.  The normalized combined strategy dif-
fers from the combined strategy. It exceeds the 
other strategies from the beginning and maintains 
 best performance up until 12000 sentence point. 
   The entropy strategy reaches 67.00 in F-score 
when 11000 sentences are selected. The combined 
strategy receives 67.17 in F-score while 13000 sen-
tences are selected, while the end performance is 
67.19 using the whole training data. The combined 
strategy reduces 24.64 % of training examples 
compared with the random selection. The normal-
ized combined strategy achieves 67.17 in F-score 
when 11000 sentences are selected, so 35.43% of 
the training examples do not need to be labeled to 
achieve almost the same performance as the end 
performance. The normalized combined strategy's 
performance becomes similar to the random selec-
tion strategy at around 13000 sentences, and after 
14000 sentences the
 
We incorporate active learning into the biomedical 
named-entity recognition system to enhance the 
system's performance with only small amount of 
training data. We presented the entropy-based un-
certainty sample selection and combined selection 
strategies using the corpus diversity. Experiments 
indicate that our strategies for active-learning 
based sample sele
Acknowledgement  
This research was supported as a Brain Neuroin-
formatics Research Program sponsored 
References 
Carbonell J., & Goldstein J. (1998). The Use of MMR, 
Diversity-Based Reranking for Reordering Docu-
ments and Producing Summaries. In Proceedings of 
the 21st Annual International ACM-SIGIR Confer-
ence on Research and Development in Information 
Retrieval, pages 335-336. 
Cohn, D. A., Atlas, L., & Ladner, R. E. (1994). Improv-
ing generalization with active learning, Machine 
Learning, 15(2), 201-221. 
Dagan, I., & Engelson S. (1995). Committee-based 
sampling for training probabilistic classifiers. In Pro-
ceedings of the Twelfth International Conference on 
Machine Learning, pages 150-157, San Francisco, 
CA, Morgan Kaufman. 
Freund Y., Seung H.S., Shamir E., & Tishby N. (1997). 
Selective sampling using the query by committee al-
gorithm, Machine Learning, 28, 133-168. 
Kim JD., Ohta T., Tsuruoka Y., & Tateisi Y. (2004). 
Introduction to the Bio-Entity Recognition Task at 
JNLPBA, Proceedings of the International Workshop 
on Natural Language Processing in Biomedicine and 
its Application (JNLPBA). 
Lafferty, J., McCallum, A., & Pereira, F. (2001). Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the 
18th International Conf. on Machine Learning, pages 
282-289, Williamstown, MA, Morgan Kaufmann. 
Lewis D., & Gale W. (1994). A Sequential Algorithm 
for Training Text Classifiers, In: Proceedings of the 
Seventeenth Annual International ACM-SIGIR Con-
ference on Research and Development in Information 
Retrieval. pp. 3-12, Springer-Verlag. 
Liere, R., & Tadepalli, P. (1997). Active learning with 
committees for text categorization, In proceedings of 
the Fourteenth National Conference on Artificial In-
telligence, pp. 591-596 Providence, RI. 
Scheffer T., & Wrobel S. (2001). Active learning of 
partially hidden markov models. In Proceedings of 
the ECML/PKDD Workshop on Instance Selection. 
Song Y., Kim E., Lee G.G., & Yi B-k. (2005). 
POSBIOTM-NER: a trainable biomedical named-
entity recognition system. Bioinformatics, 21 (11): 
2794-2796. 
Thompson C.A., Califf M.E., & Mooney R.J. (1999). 
Active Learning for Natural Language Parsing and 
Information Extraction, In Proceedings of the Six-
teenth International Machine Learning Conference, 
pp.406-414, Bled, Slovenia. 
72
NAACL HLT Demonstration Program, pages 7?8,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
POSSLT: A Korean to English Spoken Language Translation System 
 
 
Donghyeon Lee, Jonghoon Lee, Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science & Technology (POSTECH) 
San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea 
{semko, jh21983, gblee}@postech.ac.kr 
 
 
 
 
Abstract 
The POSSLT 1  is a Korean to English 
spoken language translation (SLT) system. 
Like most other SLT systems, automatic 
speech recognition (ASR), machine trans-
lation (MT), and text-to-speech (TTS) are 
coupled in a cascading manner in our 
POSSLT. However, several novel tech-
niques are applied to improve overall 
translation quality and speed. Models 
used in POSSLT are trained on a travel 
domain conversational corpus. 
1 Introduction 
Spoken language translation (SLT) has become 
more important due to globalization. SLT systems 
consist of three major components: automatic 
speech recognition (ASR), statistical machine 
translation (SMT), text-to-speech (TTS). Currently, 
most of SLT systems are developed in a cascading 
method. Simple SLT systems translate a single best 
recognizer output, but, translation quality can be 
improved using the N-best hypotheses or lattice 
provided by the ASR (Zhang et. al., 2004; Saleem 
et. al., 2004). 
In POSSLT, we used an N-best hypothesis re-
ranking based on both ASR and SMT features, and 
divided the language model of the ASR according 
to the specific domain situation. To improve the 
Korean-English SMT quality, several new tech-
                                                          
1 POSSLT stands for POSTECH Spoken Language Transla-
tion system 
niques can be applied (Lee et. al., 2006-b). The 
POSSLT applies most of these techniques using a 
preprocessor. 
2 System Description 
The POSSLT was developed by integrating ASR, 
SMT, and TTS. The system has a pipelined archi-
tecture as shown in Fig. 1. LM loader, preproces-
sor and re-ranking module are newly developed to 
improve the translation quality and speed for 
POSSLT. 
 
 
Figure 1: Overview of POSSLT 
2.1 ASR 
The system used HTK-based continuous speech 
recognition engine properly trained for Korean. 
The acoustic model, lexical model and language 
model of Korean are trained for conversational 
corpus. The phonetic set for Korean has 48 pho-
neme-like-units, and we used three-state tri-phone 
hidden Markov models and trigram language mod-
7
els. Pronunciation lexicons are automatically built 
by a Korean grapheme-to-phoneme (G2P) tool 
(Lee et. al., 2006-a). We used an eojeol2 as a basic 
recognition unit for lexical and language models, 
because an eojeol-based recognition unit has the 
higher accuracy than the morpheme-based one. 
The ASR produces the N-best hypotheses deter-
mined through the decoding process, which are 
used as the input of SMT. 
2.2 SMT 
We implemented a Korean-English phrase-based 
SMT decoder based on Pharaoh (Koehn, 2004). 
The decoder needs a phrase translation model for 
the Korean-English pair and a language model for 
English. We used the Pharaoh training module and 
GIZA++ (Och and Ney, 2000) to construct the 
phrase translation table. For language modeling, 
SRILM toolkit (Stolcke, 2002) was used to build a 
trigram language model. 
2.3 TTS 
We used Microsoft SAPI 5.1 TTS engine for Eng-
lish TTS. The final best translation is pronounced 
using the engine. 
2.4 LM Loader 
In cascading SLT systems, SMT coverage depends 
on the used ASR. In order to increase the ASR 
coverage, our system loads and unloads the ASR 
language models dynamically. In our system which 
uses a travel corpus, language models are built for 
ten domain situation categories such as an airport, 
a hotel, a shopping, etc. Besides user utterances, 
user selection of the situation is needed as an input 
to decide which language model have to be loaded 
in advance. By using the divided language models, 
many benefits such as fast decoding, higher accu-
racy and more coverage can be obtained. 
2.5 Preprocessor 
In the Korean-English SMT task, there have been 
developed several techniques for improving the 
translation quality such as changing spacing units 
into morphemes, adding POS tag information, and 
deleting useless words (Lee et. al., 2006-b). 
                                                          
2 Eojeol is a spacing unit in Korean and typically consists of 
more than one morpheme. 
However, for these techniques, Part-Of-Speech 
(POS) tagger is needed. If the final analyzed form 
of an eojeol (in the form of a sequence of mor-
phemes plus POS tags) is defined as a word in the 
ASR lexicon, the transformed sentences are direct-
ly generated by the ASR only, so POS tagger er-
rors can be removed from the system. Preprocessor 
also removes useless words in SMT in the trans-
formed sentences produced by the ASR. 
2.6 Re-ranking Module  
We implemented a re-ranking module to make a 
robust SLT system against the speech recognition 
errors. The re-ranking module uses several fea-
tures: ASR acoustic model scores, ASR language 
model scores, and SMT translation scores. Finally, 
the re-ranking module sorts the N-best lists by 
comparing the total scores. 
Acknowledgements  
This research was supported by the MIC (Ministry of 
Information and Communication), Korea, under the 
ITRC (Information Technology Research Center) sup-
port program supervised by the IITA (Institute of In-
formation Technology Assessment; IITA-2005-C1090-
0501-0018) 
References  
A. Stolcke. 2002. SRILM ? An Extensible Language Modeling 
Toolkit. Proc. of ICSLP. 
F. J. Och and H. Ney. 2000. Improved statistical alignment 
models. Proc. of 38th Annual Meeting of the ACL, page 
440-447, Hongkong, China, October 2000. 
Jinsik Lee, Seungwon Kim, Gary Geunbae Lee. 2006-a. Gra-
pheme-to-Phoneme Conversion Using Automatically Ex-
tracted Associative Rules for Korean TTS System. Proc. of 
Interspeech-ICSLP. 
Jonghoon Lee, Donghyeon Lee, Gary Geunbae Lee. 2006-b. 
Improving Phrase-based Korean-English Statistical Ma-
chine Translation. Proc. of Interspeech-ICSLP. 
P. Koehn. 2004. Pharaoh: A Beam Search Decoder for 
Phrase-based Statistical Machine Translation Models. 
Proc. of AMTA, Washington DC. 
R. Zhang, G. Kikui, H. Yamamoto, T. Watanabe, F. Soong, 
and W. K. Lo. 2004. A unified approach in speech-to-
speech translation: Integrating features of speech recogni-
tion and machine translation. Proc. of Coling 2004, Geve-
va. 
S. Saleem, S. Chen Jou, S. Vogel, and T.Schultz. 2004. Using 
word lattice information for a tighter coupling in speech 
translation systems. Proc. of ICSLP 2004, Jeju, Korea. 
8
Proceedings of NAACL HLT 2009: Short Papers, pages 89?92,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Agenda Graph Construction from Human-Human Dialogs 
using Clustering Method 
 
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science and Technology 
Pohang, South Korea 
{lcj80,hugman,getta,gblee}@postech.ac.kr 
 
  
 
Abstract 
Various knowledge sources are used for spo-
ken dialog systems such as task model, do-
main model, and agenda. An agenda graph is 
one of the knowledge sources for a dialog 
management to reflect a discourse structure. 
This paper proposes a clustering and linking 
method to automatically construct an agenda 
graph from human-human dialogs. Prelimi-
nary evaluation shows our approach would be 
helpful to reduce human efforts in designing 
prior knowledge. 
1 Introduction 
Data-driven approaches have been long applied for spo-
ken language technologies. Although a data-driven ap-
proach requires time-consuming data annotation, the 
training is done automatically and requires little human 
supervision. These advantages have motivated the de-
velopment of data-driven dialog modelings (Williams 
and Young, 2007, Lee et al, 2009). In general, the data-
driven approaches are more robust and portable than 
traditional knowledge-based approaches. However, var-
ious knowledge sources are still used in many spoken 
dialog systems that have been developed recently. These 
knowledge sources contain task model, domain model, 
and agenda which are powerful representation to reflect 
the hierarchy of natural dialog control. In the spoken 
dialog systems, these are manually designed for various 
purposes including dialog modeling (Bohus and Rud-
nicky, 2003, Lee et al, 2008), search space reduction 
(Young et al, 2007), domain knowledge (Roy and Sub-
ramaniam, 2006), and user simulation (Schatzmann et 
al., 2007). 
We have proposed an example-based dialog modeling 
(EBDM) framework using an agenda graph as prior 
knowledge (Lee et al, 2008). This is one of the data-
driven dialog modeling techniques and the next system 
action is determined by selecting the most similar dialog 
examples in dialog example database. In the EBDM 
framework for task-oriented dialogs, agenda graph is 
manually designed to address two aspects of a dialog 
management: (1) Keeping track of the dialog state with 
a view to ensuring steady progress towards task comple-
tion, and (2) Supporting n-best recognition hypotheses 
to improve the robustness of dialog manager. However, 
manually building such graphs for various applications 
may be labor intensive and time consuming. Thus, we 
have tried to investigate how to build this graph auto-
matically. Consequently, we sought to solve the prob-
lem by automatically building the agenda graph using 
clustering method from an annotated dialog corpus. 
2 Related Work  
Clustering techniques have been widely used to build 
prior knowledge for spoken dialog systems. One of 
them is automatic construction of domain model (or 
topic structure) which is one of the important resources 
to handle user?s queries in call centers. Traditional ap-
proach to building domain models is that the analysts 
manually generate a domain model through inspection 
of the call records. However, it has recently been pro-
posed to use an unsupervised technique to generate do-
main models automatically from call transcriptions (Roy 
and Subramaniam, 2006). In addition, there has been 
research on how to automatically learn models of task-
oriented discourse structure using dialog act and task 
information (Bangalore et al, 2006). Discourse struc-
ture is necessary for dialog state-specific speech recog-
nition and language understanding to improve the 
performance by predicting the next possible dialog 
states. In addition, the discourse structure is essential to 
determine whether the current utterance in the dialog is 
part of the current subtask or starts a new task. 
89
More recently, it has been proposed stochastic dialog 
management such as the framework of a partially ob-
servable Markov decision process (POMDP). This 
framework is statistically data-driven and theoretically 
principled dialog modeling. However, detailed dialog 
states in the master space should be clustered into gen-
eral dialog states in summary space to scale up 
POMDP-based dialog management for practical appli-
cations (Williams and Young, 2007). To address this 
problem, an unsupervised automatic clustering of dialog 
states has been introduced and investigated in POMDP-
based dialog manager (Lefevre and Mori, 2007).  
In this paper, we are also interested in exploring me-
thods that would automatically construct the agenda 
graph as prior knowledge for the EBDM framework. 
3 Agenda Graph 
In this section, we begin with a brief overview of 
EBDM framework and agenda graph. The basic idea of 
the EBDM is that the next system action is predicted by 
finding semantically similar user utterance in the dialog 
state space. The agenda graph was adapted to take into 
account the robustness problem for practical applica-
tions. Agenda graph G is a simply a way of encoding 
the domain-specific dialog control to complete the task. 
G is represented by a directed acyclic graph (DAG) 
(Figure 1). An agenda is one of the subtask flows, which 
is a possible path from root node to terminal node. G is 
composed of nodes (v) which correspond to possible 
intermediate steps in the process of completing the spe-
cified task, and edges (e) which connect nodes. In other 
words, v corresponds to dialog state to achieve domain-
specific subtask in its expected agenda. Each node in-
cludes three different components: (1) A precondition 
that must be true before the subtask is executed; (2) A 
description of the node that includes its label and iden-
tifier; and (3) Links to nodes that will be executed at the 
subsequent turn. In this system, this graph is used to 
rescore n-best ASR hypotheses and to interpret the dis-
course state such as new task, next task, and new sub-
task based on topological position on the graph. In the 
agenda graph G, each node holds a set of relevant dialog 
examples which may appear in the corresponding dialog 
states when a precondition of the node is true. To de-
termine the next system action, the dialog manager first 
generates possible candidate nodes with n-best hypo-
theses by using a discourse interpretation algorithm 
based on the agenda graph, and then selects the focus 
node which is the most likely dialog state given the pre-
vious dialog state. Finally the best example in the focus 
node is selected to determine appropriate system action. 
Human efforts are required to manually design the 
agenda graph to integrate it into the EBDM framework. 
However, it is difficult to define all possible precondi-
tion rules and to assign the transition probabilities to 
each link based only on the discretion of the system 
developer. To solve these problems, we tried to con-
struct the agenda graph from the annotated dialog cor-
pus using clustering technique. 
4 Clustering and Linking 
4.1 Node Clustering 
Each precondition has been manually defined to map 
relevant dialog examples into each node. To avoid this, 
the dialog examples are automatically grouped into the 
closest cluster (or node) by a node clustering. In this 
section, we explain a feature extraction and clustering 
method for constructing the agenda graph. 
4.1.1 Feature Extraction 
Each dialog example should be converted into a feature 
vector for a node clustering. To represent the feature 
vectors, we first extract all n-grams which occur more 
frequently than a threshold and do not contain any stop 
word as word-level features. We also extract utterance-
level and discourse-level features from the annotated 
dialog corpus to reflect semantic and contextual infor-
mation because a dialog state can be characterized using 
semantic and contextual information derivable from the 
annotations. The utterance is thus characterized by the 
set of various features as shown in Table 1. 
 
Figure 1: Example of an agenda graph for building 
guidance domain 
Feature Types Features #Size 
Word-level  
features 
unigram 175 
bigram 573 
trigram 1034 
Utterance-level  
features 
dialog act (DA) 9 
main goal (MG) 16 
slot filling status 8 
system act (SA) 26 
Discourse-level  
features 
previous DA 10 
previous MG 17 
previous SA 27 
Table 1: List of feature sets 
90
For a set of N dialog examples X={xi|i=1,..,N}, the 
binary feature vectors are represented by using a set of 
features from the dialog corpus. To calculate the dis-
tance of two feature vectors, we used a cosine measure 
as a binary vector distance measure: 
ji
ji
ji xx
xxxxd ?
??? )(1),(
 
where xi and xj denoted two feature vectors. However, 
each feature vector contains small number of non-zero 
terms (<20 features) compared to the feature space 
(>2000 features). Therefore, most pairs of utterances 
share no common feature, and their distance is close to 
1.0. To address this sparseness problem, the distance 
between two utterances can be computed by checking 
only the non-zero terms of corresponding feature vec-
tors (Liu, 2005). 
4.1.2 Clustering 
After extracting feature vectors from the dialog corpus, 
we used K-means clustering algorithm which is the sim-
plest and most commonly used algorithm employing a 
squared error criterion. At the initialization step, one 
cluster mean is randomly selected in the data set and k-1 
means are iteratively assigned by selecting the farthest 
point from pre-selected centers as the following equa-
tion:  
? ???
???
?? ??
??
1
1
,maxarg k
i
iXxk
uxdu
 
where each cluster ck is represented as a mean vector uk. 
At the assignment step, each example is assigned to the 
nearest cluster 
tc? by minimizing the distance of cluster 
mean uk and dialog example xt. 
? ?? ?tkKkt xudc ,minarg? 1 ???
 
The responsibilities rkt of each cluster ck are calcu-
lated for each example xt as the following rule: 
? ?? ?? ?? ?? ?? ??? l tl tkkt xud
xudr ,exp
,exp
?
?
 
where ? is the stiffness and usually assigned to 1. 
During the update step, the means are recomputed us-
ing the current cluster membership by reflecting their 
responsibilities: 
?
??
t kt
t tkt
k r
xru
 
4.2 Node Linking 
From the node clustering step, node vk for cluster ck is 
obtained from the dialog corpus and each node contains 
similar dialog examples by the node clustering algo-
rithm. Next, at the node linking step, each node should 
be connected with an appropriate transition probability 
to build the agenda graph which is a DAG (Figure 2). 
This linking information can come from the dialog cor-
pus because the task-oriented dialogs consist of sequen-
tial utterances to complete the tasks. Using sequences of 
dialog examples obtained with the dialog corpus, rela-
tive frequencies of all outgoing edges are calculated to 
weight directed edges: 
)(
)(),(
i
ji
ji vxn
vvxnvvf ?
???
 
where ? ?ivxn ?  represents the number of dialog exam-
ples in vi and ? ?ji vvxn ??  denotes the number of di-
alog examples having directed edge from vi to vj. Next 
some edges are pruned when the weight falls below a 
pre-defined threshold ?, and the cycle paths are removed 
by deleting minimal edge in cycle paths through a 
depth-first traversal. Finally the transition probability 
can be estimated by normalizing relative frequencies 
with the remained edges. 
?? l li jiij vvf
vvfvvp ),(
),()|(
 
5 Experiment & Result 
 A spoken dialog system for intelligent robot was devel-
oped to provide information about building (e.g., room 
number, room name, room type) and people (e.g., name, 
phone number, e-mail address).  If the user selects a 
specific room to visit, then the robot takes the user to 
the desired room. For this system, we collect a human-
human dialog corpus of about 880 user utterances from 
214 dialogs which were based on a set of pre-defined 10 
subjects relating to building guidance task. Then, we 
designed an agenda graph and integrated it into the 
EBDM framework. In addition, a simulated environ-
ment with a user simulator and an ASR channel (Jung et 
 
Figure 2: Node Linking Algorithm 
91
al., 2008) was developed to evaluate our approach by 
simulating a realistic scenario. 
First we measured the clustering performance to veri-
fy our approach for constructing the agenda graph.  We 
used the manually clustered examples by a set of pre-
condition rules as the reference clusters. Table 2 shows 
error rates when different feature sets are used for K-
means clustering in which K is equal to 10 because a 
hand-crafted graph included 10 nodes. The error rate 
was significantly reduced when using all feature sets. 
 
We also evaluated the dialog system performance 
with the agenda graphs which are manually (HC-AG) or 
automatically designed (AC-AG). We also used 10-best 
recognition hypotheses with 20% word error rate 
(WER) for a dialog management and 1000 simulated 
dialogs for an automatic evaluation. In this result, al-
though the system with HC-AG slightly outperforms the 
system with AC-AG, we believe that AC-AG can be 
helpful to manage task-oriented dialogs with less human 
costs for designing the hand-crafted agenda graph. 
 
6 Conclusion & Discussion  
In this paper, we address the problem of automatic 
knowledge acquisition of agenda graph to structure 
task-oriented dialogs. We view this problem as a first 
step in clustering the dialog states, and then in linking 
between each cluster based on the dialog corpus. The 
experiment results show that our approach can be appli-
cable to easily build the agenda graph for prior know-
ledge. 
There are several possible subjects for further re-
search on our approach. We can improve the clustering 
performance by using a distance metric learning algo-
rithm to consider the correlation between features. We 
can also discover hidden links in the graph by exploring 
new dialog flows with random walks. 
Acknowledgement 
This research was supported by the MKE (Ministry of 
Knowledge Economy), Korea, under the ITRC (Infor-
mation Technology Research Center) support program 
supervised by the IITA (Institute for Information Tech-
nology Advancement) (IITA-2009-C1090-0902-0045). 
References  
Bangalore, S., Fabbrizio, G.D. and Stent, A. 2006. Learning 
the structure of task-driven human-human dialogs. Proc. of 
the Association for Computational Linguistics, 201-208. 
Bohus, B. and Rudnicky, A. 2003. RavenClaw: Dialog Man-
agement Using Hierarchical Task Decomposition and an 
Expectation Agenda. Proc. of the European Conference on 
Speech, Communication and Technology, 597-600. 
Jung, S., Lee, C., Kim, K. and Lee, G.G. 2008. An Integrated 
Dialog Simulation Technique for Evaluating Spoken Dialog 
Systems. Proc. of Workshop on Speech Processing for Safe-
ty Critical Translation and Pervasive Applications, Interna-
tional Conference on Computational Linguistics, 9-16. 
Lee, C., Jung, S. and Lee, G.G. 2008. Robust Dialog Man-
agement with N-best Hypotheses using Dialog Examples 
and Agenda. Proc. of the Association for Computational 
Linguistics, 630-637. 
Lee, C., Jung, S., Kim, S. and Lee, G.G. 2009. Example-based 
Dialog Modeling for Practical Multi-domain Dialog System. 
Speech Communication, 51(5):466-484. 
Lefevre, F. and Mori, R.D. 2007. Unsupervised State Cluster-
ing for Stochastic Dialog Management. Proc. of the IEEE 
Workshop on Automatic Speech Recognition and Under-
standing, 550-555. 
Liu, Z. 2005. An Efficient Algorithm for Clustering Short 
Spoken Utterances. Proc. of the IEEE International Confe-
rence on Acoustics, Speech and Signal Processing, 593-596. 
Roy, S. and Subramaniam, L.V. 2006. Automatic generation 
of domain models for call centers from noisy transcriptions. 
Proc. of the Association for Computational Linguistics, 737-
744. 
Schatzmann, J., Thomson, B., Weilhammer, K., Ye, H. and 
Young, S. 2007. Agenda-based User Simulation for Boot-
strapping a POMDP Dialogue System. Proc. of the Human 
Language Technology/North American Chapter of the Asso-
ciation for Computational Linguistics, 149-152. 
Williams, J.D. and Young, S. 2007. Partially observable Mar-
kov decision processes for spoken dialog systems. Comput-
er Speech and Language, 21:393-422. 
Young, S., Schatzmann, J., Weilhammer, K. and Ye, H. 2007. 
The Hidden Information State Approach to Dialog Man-
agement. Proc. of the IEEE International Conference on 
Acoustics, Speech and Signal Processing, 149-152. 
 
System TCR (%) AvgUserTurn 
Using HC-AG 92.96 4.41 
Using AC-AG 89.95 4.39 
Table 3: Task completion rate (TCR) and average 
user turn (AvgUserTurn) (WER=20%) 
Feature sets Error rate (%) 
Word-level features 46.51 
+Utterance-level features 34.63 
+Discourse-level features 31.20 
Table 2: Error rates for node clustering (K=10) 
 
92
Proceedings of NAACL HLT 2009: Short Papers, pages 169?172,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Local Tree Alignment-based Soft Pattern Matching Approach for
Information Extraction
Seokhwan Kim, Minwoo Jeong, and Gary Geunbae Lee
Department of Computer Science and Engineering
Pohang University of Science and Technology
San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Korea
{megaup, stardust, gblee}@postech.ac.kr
Abstract
This paper presents a new soft pattern match-
ing method which aims to improve the recall
with minimized precision loss in information
extraction tasks. Our approach is based on a
local tree alignment algorithm, and an effec-
tive strategy for controlling flexibility of the
pattern matching will be presented. The ex-
perimental results show that the method can
significantly improve the information extrac-
tion performance.
1 Introduction
The goal of information extraction (IE) is to ex-
tract structured information from unstructured natu-
ral language documents. Pattern induction to gener-
ate extraction patterns from a number of training in-
stances is one of the most widely applied approaches
for IE.
A number of pattern induction approaches have
recently been researched based on the dependency
analysis (Yangarber, 2003) (Sudo et al, 2001)
(Greenwood and Stevenson, 2006) (Sudo et al,
2003). The natural language texts in training in-
stances are parsed by dependency analyzer and con-
verted into dependency trees. Each subtree of a de-
pendency tree is considered as a candidate of ex-
traction patterns. An extraction pattern is gener-
ated by selecting the subtree which indicates the de-
pendency relationships of each labeled slot value
in the training instance and agrees on the selec-
tion criteria defined by each pattern representation
model. A number of dependency tree-based pat-
tern representation models have been proposed. The
predicate-argument (SVO) model allows subtrees
containing only a verb and its direct subject and
object as extraction pattern candidates (Yangarber,
2003). The chain model represents extraction pat-
terns as a chain-shaped path from each target slot
value to the root node of the dependency tree (Sudo
et al, 2001). A couple of chain model patterns shar-
ing the same verb are linked to each other and con-
struct a linked-chain model pattern (Greenwood and
Stevenson, 2006). The subtree model considers all
subtrees as pattern candidates (Sudo et al, 2003).
Regardless of the applied pattern representation
model, the methods have concentrated on extracting
only exactly equivalent subtrees of test instances to
the extraction patterns, which we call hard pattern
matching. While the hard pattern matching policy
is helpful to improve the precision of the extracted
results, it can cause the low recall problem. In or-
der to tackle this problem, a number of soft pattern
matching approaches which aim to improve recall
with minimized precision loss have been applied to
the linear vector pattern models by introducing a
probabilistic model (Xiao et al, 2004) or a sequence
alignment algorithm (Kim et al, 2008).
In this paper, we propose an alternative soft
pattern matching method for IE based on a local
tree alignment algorithm. While other soft pattern
matching approaches have been able to handle the
matching among linear vector instances with fea-
tures from tree structures only, our method aims to
directly solve the low recall problem of tree-to-tree
pattern matching by introducing the local tree align-
ment algorithm which is widely used in bioinformat-
ics to analyze RNA secondary structures. Moreover,
169
(a) Example pattern
(b) Dependency Tree of the example sentence
(c) Local alignment-based tree pattern matching
Figure 1: An example of local alignment-based tree pat-
tern matching
we present an effective policy for controlling degree
of flexibility in the pattern matching by setting the
optimal threshold values for each extracted pattern.
2 Methods
The low recall problem of information extraction
based on hard pattern matching is caused by lack
of flexibility in pattern matching. For example, the
tree pattern in Figure 1(a) cannot be matched with
the tree in Figure 1(b) by considering only exactly
equivalent subtrees, because the first tree has an ad-
ditional root node ?said? which is not in the second
one. However, the matching between two trees can
be performed by omitting just a node as shown in
Figure 1(c).
In order to improve and control the degree of flex-
ibility in tree pattern matching, we have adopted a
local tree alignment approach as the pattern match-
ing method instead of hard pattern matching strat-
egy. The local tree alignment problem is to find the
most similar subtree between two trees.
We have adopted the Hochsmann algorithm
(Hochsmann et al, 2003) which is a local tree align-
ment algorithm used in bioinformatics to analyze
RNA secondary structures. The goal of the Hochs-
mann algorithm is to find the local closed forest
alignment which maximizes the similarity score for
ordered trees. The algorithm can be implemented
by a dynamic programming approach which solves a
problem based on the previous results of its subprob-
lems. The main problem of Hochsmann algorithm
is to compute the similarity score between two sub-
forests according to the defined order from the sin-
gle node level to the entire tree level. The similarity
score is defined based on three tree edit operations
which are insertion, deletion, and replacement (Tai,
1979). For each pair of subforests, the maximum
similarity score among three edit operations is com-
puted, and the kind and the position of performed
edit operations are recorded.
The adaptation of Hochsmann algorithm to the IE
problem is performed by redefining the ?-function,
the similarity score function between two nodes, as
follows:
?(v,w) =
?
?????
?????
1 if lnk(v)=lnk(w),
and lbl(v)=lbl(w),
?(p(w), p(v)) if lbl(v)=<SLOT>,
0 otherwise.
where v and w are nodes to be compared, lnk(v) is
the link label of v, lbl(v) is the node label of v, and
p(v) denotes a parent node of v. While general local
tree alignment problems consider only node labels
to compute the node-level similarities, our method
considers not only node labels, but also link labels to
the head node, because the class of link to the head
node is important as the node label itself for depen-
dency trees. Moreover, the method should consider
the alignment of slot value nodes in the tree patterns
for adopting information extraction tasks. If the pat-
tern node v is a kind of slot value nodes, the similar-
ity score between v and w is inherited from parents
of both nodes.
After computing for all pairs of subforests, the
optimal alignment is obtained by trace-back based
on the recorded information of edit operation which
maximizes the similarity score for each subforest
pair. On the optimal alignment, the target node
aligned to a slot value node on the pattern is regarded
as an argument candidate of the extraction. Each ex-
170
traction candidate has its confidence score which is
computed from the alignment score, defined as:
score(TPTN, TTGT) = S(TPTN, TTGT)|TPTN|
where |T | denotes the total number of nodes in tree
T and S(T1, T2) is the similarity score of both trees
computed by Hochsmann algorithm.
Only the extraction candidates with alignment
score larger than the given threshold value, ?, are
accepted and regarded as extraction results. For the
simplest approach, the same threshold value, ?, can
be applied to all the patterns. However, we assumed
that each pattern has its own optimal threshold value
as its own confidence score, which is different from
other patterns? threshold values. The optimal thresh-
old value ?i and the confidence score confi for the
pattern Pi are defined as:
?i = argmax
0.5<??1.0
{evalfscore (Dtrain, Pi, ?)}
confi = max0.5<??1.0 {evalfscore (Dtrain, Pi, ?)}
where evalfscore(D,P, ?) is the evaluation result in
F-score of the extraction for the data set D using the
pattern P with the threshold value ?. For each pat-
tern, the threshold value which maximizes the eval-
uation result in F-score for the training data set and
the maximum evaluation result in F-score are as-
signed as the optimal threshold value and the con-
fidence score for the pattern respectively.
3 Experiment
In order to evaluate the effectiveness of our method,
we performed an experiment for the scenario tem-
plate extraction task on the management succession
domain in MUC-6. The task aims to extract sce-
nario template instances which consist of person-in,
person-out, position, organization slot values from
news articles about management succession events.
We used a modified version of the MUC-6 corpus
including 599 training documents and 100 test doc-
uments described by Soderland (1999). While the
scenario templates on the original MUC-6 corpus
are labeled on each document, this version has sce-
nario templates for each sentence.
All the sentences in both training and test
documents were converted into dependency trees
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
F-
sc
or
e
Proportion of Patterns Used
SOFT(SUBTREE)
SOFT(LINKED)
SOFT(CHAIN)
HARD(LINKED)
HARD(CHAIN)
HARD(SUBTREE)
SOFT/HARD(SVO)
Figure 2: Comparison of soft pattern matching strategy
with the hard pattern matching
by Berkeley Parser1 and LTH Constituent-to-
Dependency Conversion Tool2. From the depen-
dency trees and scenario templates on the training
data, we constructed pattern candidate sets for four
types of pattern representation models which are
SVO, chain, linked-chain, and subtree models. For
each pattern candidate, corresponding confidence
score and optimal threshold value were computed.
The pattern candidates for each pattern represen-
tation model were arranged in descending order of
confidence score. According to the arranged order,
each pattern was matched with test documents and
the extracted results were accumulated. Extracted
templates for test documents are evaluated by com-
paring with the answer templates on the test corpus.
The curves in Figure 2 show the relative perfor-
mance of the pattern matching strategies for each
pattern representation model. The results suggest
that soft pattern matching strategy with optimal
threshold values requires less number of patterns
for the performance saturation than the hard pat-
tern matching strategy for all pattern models except
the SVO model. For the SVO model, the result of
soft pattern matching strategy is equivalent to that
of hard pattern matching strategy. It is because most
of patterns represented in SVO model are relatively
shorter than those represented in other models.
In order to evaluate the flexibility controlling
strategy, we compared the result of optimally de-
termined threshold values with the cases of using
1http://nlp.cs.berkeley.edu/pages/Parsing.html
2http://nlp.cs.lth.se/pennconverter/
171
? SVO Chain Linked-Chain SubtreeP R F P R F P R F P R F
0.7 32.1 18.0 23.1 27.6 55.0 36.8 26.8 57.0 36.4 26.6 58.0 36.5
0.8 32.1 18.0 23.1 43.8 35.0 38.8 43.4 36.0 39.3 44.7 34.0 38.6
0.9 32.1 18.0 23.1 45.2 33.0 38.1 43.8 35.0 38.9 45.2 33.0 38.2
1.0 (hard) 32.1 18.0 23.1 45.2 33.0 38.1 43.8 35.0 38.9 45.2 33.0 38.2
optimal 32.1 18.0 23.1 36.0 49.0 41.5 40.7 48.0 44.0 43.0 46.0 44.4
Table 1: Experimental Results
various fixed threshold values. Table 1 represents
the final results for all pattern representation mod-
els and threshold values. For the SVO model, all
the results are equivalent regardless of the thresh-
old strategy because of extremely short length of the
patterns. For the other pattern models, precisions are
increased and recalls are decreased by increasing the
threshold. The maximum performances in F-score
are achieved by our optimal threshold determining
strategy for all pattern representation models. The
experimental results of our method show the better
recall than the cases of hard pattern matching and
controlled precision than the cases of extremely soft
pattern matching.
4 Conclusion
We presented a local tree alignment based soft pat-
tern matching approach for information extraction.
The softness of the pattern matching method is con-
trolled by the threshold value of the alignment score.
The optimal threshold values are determined by self-
evaluation on the training data. Experimental results
indicate that our soft pattern matching approach is
helpful to improve the pattern coverage and our
threshold learning strategy is effective to reduce the
precision loss followed by the soft pattern matching
method.
The goal of local tree alignment algorithm is to
measure the structural similarity between two trees.
It is similar to the kernel functions in the tree kernel
method which is another widely applied approach to
solve the IE problems. In the future, we plan to in-
corporate our alignment-based soft pattern matching
method into the tree kernel method for IE.
Acknowledgments
This work was supported by the Korea Science and
Engineering Foundation(KOSEF) grant funded by
the Korea government(MEST) (No. R01-2008-000-
20651-0)
References
Mark A. Greenwood and Mark Stevenson. 2006. Im-
proving semi-supervised acquisition of relation extrac-
tion patterns. In Proceedings of Workshop on Informa-
tion Extraction Beyond The Document, pp. 29?35.
Matthias Hochsmann, Thomas Toller, Robert Giegerich,
and Stefan Kurtz. 2003. Local similarity in rna sec-
ondary structures. In Proceedings of the IEEE Com-
puter Society Bioinformatics Conference , pp. 159?68.
Seokhwan Kim, Minwoo Jeong, and Gary Geunbae
Lee. 2008. An alignment-based pattern representa-
tion model for information extraction. In Proceedings
of the ACM SIGIR ?08, pp. 875?876.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1):233?272.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic pattern acquisition for japanese in-
formation extraction. In Proceedings of the first inter-
national conference on Human language technology
research, pp. 1?7.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the ACL ?03, pp. 224?231.
Kuo-Chung Tai. 1979. The tree-to-tree correction prob-
lem. Journal of the ACM (JACM), 26(3):422?433.
Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cas-
cading use of soft and hard matching pattern rules
for weakly supervised information extraction. In Pro-
ceedings of COLING ?04, pp. 542?548.
Roman Yangarber. 2003. Counter-training in discovery
of semantic patterns. In Proceedings of the ACL ?03,
pp. 343?350.
172
Automatic Acquisition of Named Entity Tagged Corpus from World Wide
Web
Joohui An
Dept. of CSE
POSTECH
Pohang, Korea 790-784
minnie@postech.ac.kr
Seungwoo Lee
Dept. of CSE
POSTECH
Pohang, Korea 790-784
pinesnow@postech.ac.kr
Gary Geunbae Lee
Dept. of CSE
POSTECH
Pohang, Korea 790-784
gblee@postech.ac.kr
Abstract
In this paper, we present a method that
automatically constructs a Named En-
tity (NE) tagged corpus from the web
to be used for learning of Named En-
tity Recognition systems. We use an NE
list and an web search engine to col-
lect web documents which contain the
NE instances. The documents are refined
through sentence separation and text re-
finement procedures and NE instances are
finally tagged with the appropriate NE cat-
egories. Our experiments demonstrates
that the suggested method can acquire
enough NE tagged corpus equally useful
to the manually tagged one without any
human intervention.
1 Introduction
Current trend in Named Entity Recognition (NER) is
to apply machine learning approach, which is more
attractive because it is trainable and adaptable, and
subsequently the porting of a machine learning sys-
tem to another domain is much easier than that of a
rule-based one. Various supervised learning meth-
ods for Named Entity (NE) tasks were successfully
applied and have shown reasonably satisfiable per-
formance.((Zhou and Su, 2002)(Borthwick et al,
1998)(Sassano and Utsuro, 2000)) However, most
of these systems heavily rely on a tagged corpus for
training. For a machine learning approach, a large
corpus is required to circumvent the data sparseness
problem, but the dilemma is that the costs required
to annotate a large training corpus are non-trivial.
In this paper, we suggest a method that automati-
cally constructs an NE tagged corpus from the web
to be used for learning of NER systems. We use an
NE list and an web search engine to collect web doc-
uments which contain the NE instances. The doc-
uments are refined through the sentence separation
and text refinement procedures and NE instances are
finally annotated with the appropriate NE categories.
This automatically tagged corpus may have lower
quality than the manually tagged ones but its size
can be almost infinitely increased without any hu-
man efforts. To verify the usefulness of the con-
structed NE tagged corpus, we apply it to a learn-
ing of NER system and compare the results with the
manually tagged corpus.
2 Automatic Acquisition of an NE Tagged
Corpus
We only focus on the three major NE categories (i.e.,
person, organization and location) because others
are relatively easier to recognize and these three cat-
egories actually suffer from the shortage of an NE
tagged corpus.
Various linguistic information is already held in
common in written form on the web and its quantity
is recently increasing to an almost unlimited extent.
The web can be regarded as an infinite language re-
source which contains various NE instances with di-
verse contexts. It is the key idea that automatically
marks such NE instances with appropriate category
labels using pre-compiled NE lists. However, there
should be some general and language-specific con-
Web documents
W1W2W3
?
URL1URL2URL3
?
Web search engine
Web robot
Sentence separator
Textrefinement
S1S2S3
?
1.html2.html
?
1.ans2.ans
?
NE list Web page URL
Separated sentences
Refined sentences
NE taggeneration
S1(t)S2(t)S3(t)
?
NE taggedcorpus
Figure 1: Automatic generation of NE tagged corpus
from the web
siderations in this marking process because of the
word ambiguity and boundary ambiguity of NE in-
stances. To overcome these ambiguities, the auto-
matic generation process of NE tagged corpus con-
sists of four steps. The process first collects web
documents using a web search engine fed with the
NE entries and secondly segments them into sen-
tences. Next, each sentence is refined and filtered
out by several heuristics. An NE instance in each
sentence is finally tagged with an appropriate NE
category label. Figure 1 explains the entire proce-
dure to automatically generate NE tagged corpus.
2.1 Collecting Web Documents
It is not appropriate for our purpose to randomly col-
lect documents from the web. This is because not all
web documents actually contain some NE instances
and we also do not have the list of all NE instances
occurring in the web documents. We need to col-
lect the web documents which necessarily contain
at least one NE instance and also should know its
category to automatically annotate it. This can be
accomplished by using a web search engine queried
with pre-compiled NE list.
As queries to a search engine, we used the list
of Korean Named Entities composed of 937 per-
son names, 1,000 locations and 1,050 organizations.
Using a Part-of-Speech dictionary, we removed am-
biguous entries which are not proper nouns in other
contexts to reduce errors of automatic annotation.
For example, ?E?(kyunggi, Kyunggi/business con-
ditions/a game)? is filtered out because it means a lo-
cation (proper noun) in one context, but also means
business conditions or a game (common noun) in
other contexts. By submitting the NE entries as
queries to a search engine1, we obtained the max-
imum 500 of URL?s for each entry. Then, a web
robot visits the web sites in the URL list and fetches
the corresponding web documents.
2.2 Splitting into Sentences
Features used in the most NER systems can be clas-
sified into two groups according to the distance from
a target NE instance. The one includes internal fea-
tures of NE itself and context features within a small
word window or sentence boundary and the other in-
cludes name alias and co-reference information be-
yond a sentence boundary. In fact, it is not easy to
extract name alias and co-reference information di-
rectly from manually tagged NE corpus and needs
additional knowledge or resources. This leads us to
focus on automatic annotation in sentence level, not
document level. Therefore, in this step, we split the
texts of the collected documents into sentences by
(Shim et al, 2002) and remove sentences without
target NE instances.
2.3 Refining the Web Texts
The collected web documents may include texts ac-
tually matched by mistake, because most web search
engines for Korean use n-gram, especially, bi-gram
matching. This leads us to refine the sentences to ex-
clude these erroneous matches. Sentence refinement
is accomplished by three different processes: sep-
aration of functional words, segmentation of com-
pound nouns, and verification of the usefulness of
the extracted sentences.
An NE is often concatenated with more than one
josa, a Korean functional word, to compose a
Korean word. Therefore we need to separate the
functional words from an NE instance to detect the
boundary of the NE instance and this is achieved
by a part-of-speech tagger, POSTAG, which can
detect unknown words (Lee et al, 2002). The
separation of functional words gives us another
benefit that we can resolve the ambiguities between
an NE and a common noun plus functional words
1We used Empas (http://www.empas.com)
Person Location Organization
Training Automatic 29,042 37,480 2,271Manual 1,014 724 1,338
Test Manual 102 72 193
Table 1: Corpus description (number of NE?s) (Au-
tomatic: Automatically annotated corpus, Manual:
Manually annotated corpus
and filter out erroneous matches. For example,
?E??(kyunggi-do)? can be interpreted as
either ?E??(Kyunggi Province)? or ?E?+?(a
game also)? according to its context. We can remove
the sentence containing the latter case.
A josa-separated Korean word can be a com-
pound noun which only contains a target NE as a
substring. This requires us to segment the compound
noun into several correct single nouns to match with
the target NE. If the segmented single nouns are not
matched with a target NE, the sentence can be fil-
tered out. For example, we try to search for an NE
entry, ???(Fin.KL, a Korean singer group)? and
may actually retrieve sentences including ????
?(surfing club)?. The compound noun, ??????,
can be divided into ???(surfing)? and ???(club)?
by a compound-noun segmenting method (Yun et
al., 1997). Since both ???? and ???? are not
matched with our target NE, ????, we can delete
the sentences. Although a sentence has a correct tar-
get NE, if it does not have context information, it is
not useful as an NE tagged corpus. We also removed
such sentences.
2.4 Generating an NE tagged corpus
The sentences selected by the refining process ex-
plained in previous section are finally annotated with
the NE label. We acquired the NE tagged corpus in-
cluding 68,793 NE instances through this automatic
annotation process. We can annotate only one NE
instance per sentence but almost infinitely increase
the size of the corpus because the web provides un-
limited data and our process is fully automatic.
3 Experimental Results
3.1 Usefulness of the Automatically Tagged
Corpus
For effectiveness of the learning, both the size and
the accuracy of the training corpus are important.
Training corpus Precision Recall F-measure
Seeds only 84.13 42.91 63.52
Manual 80.21 86.11 83.16
Automatic 81.45 85.41 83.43
Manual + Automatic 82.03 85.94 83.99
Table 2: Performance of the decision list learning
Generally, the accuracy of automatically created NE
tagged corpus is worse than that of hand-made cor-
pus. Therefore, it is important to examine the useful-
ness of our automatically tagged corpus compared
to the manual corpus. We separately trained the de-
cision list learning features using the automatically
annotated corpus and hand-made one, and compared
the performances. Table 1 shows the details of the
corpus used in our experiments.2
Through the results in Table 2, we can verify that
the performance with the automatic corpus is supe-
rior to that with only the seeds and comparable to
that with the manual corpus.Moreover, the domain
of the manual training corpus is same with that of
the test corpus, i.e., news and novels, while the do-
main of the automatic corpus is unlimited as in the
web. This indicates that the performance with the
automatic corpus should be regarded as much higher
than that with the manual corpus because the per-
formance generally gets worse when we apply the
learned system to different domains from the trained
ones. Also, the automatic corpus is pretty much self-
contained since the performance does not gain much
though we use both the manual corpus and the auto-
matic corpus for training.
3.2 Size of the Automatically Tagged Corpus
As another experiment, we tried to investigate how
large automatic corpus we should generate to get the
satisfiable performance. We measured the perfor-
mance according to the size of the automatic cor-
pus. We carried out the experiment with the deci-
sion list learning method and the result is shown in
Table 3. Here, 5% actually corresponds to the size of
the manual corpus. When we trained with that size
of the automatic corpus, the performance was very
low compared to the performance of the manual cor-
pus. The reason is that the automatic corpus is com-
2We used the manual corpus used in Seon et al (2001) as
training and test data.
Corpus size (words) Precision Recall F-measure
90,000 (5%) 72.43 6.94 39.69
448,000 (25%) 73.17 41.66 57.42
902,000 (50%) 75.32 61.53 68.43
1,370,000 (75%) 78.23 77.19 77.71
1,800,000 (100%) 81.45 85.41 83.43
Table 3: Performance according to the corpus size
Corpus size (words) Precision Recall F-measure
700,000 79.41 81.82 80.62
1,000,000 82.86 85.29 84.08
1,200,000 83.81 86.27 85.04
1,300,000 83.81 86.27 85.04
Table 4: Saturation point of the performance for
?person? category
posed of the sentences searched with fewer named
entities and therefore has less lexical and contextual
information than the same size of the manual cor-
pus. However, the automatic generation has a big
merit that the size of the corpus can be increased al-
most infinitely without much cost. From Table 3,
we can see that the performance is improved as the
size of the automatic corpus gets increased. As a
result, the NER system trained with the whole au-
tomatic corpus outperforms the NER system trained
with the manual corpus.
We also conducted an experiment to examine the
saturation point of the performance according to the
size of the automatic corpus. This experiment was
focused on only ?person? category and the result is
shown in Table 4. In the case of ?person? category,
we can see that the performance does not increase
any more when the corpus size exceeds 1.2 million
words.
4 Conclusions
In this paper, we presented a method that automat-
ically generates an NE tagged corpus using enor-
mous web documents. We use an internet search en-
gine with an NE list to collect web documents which
may contain the NE instances. The web documents
are segmented into sentences and refined through
sentence separation and text refinement procedures.
The sentences are finally tagged with the NE cat-
egories. We experimentally demonstrated that the
suggested method could acquire enough NE tagged
corpus equally useful to the manual corpus without
any human intervention. In the future, we plan to ap-
ply more sophisticated natural language processing
schemes for automatic generation of more accurate
NE tagged corpus.
Acknowledgements
This research was supported by BK21 program of
Korea Ministry of Education and MOCIE strategic
mid-term funding through ITEP.
References
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Exploiting Diverse
Knowledge Sources via Maximum Entropy in Named
Entity Recognition. In Proceedings of the Sixth Work-
shop on Very Large Corpora, pages 152?160, New
Brunswick, New Jersey. Association for Computa-
tional Linguistics.
Gary Geunbae Lee, Jeongwon Cha, and Jong-Hyeok
Lee. 2002. Syllable Pattern-based Unknown Mor-
pheme Segmentation and Estimation for Hybrid Part-
Of-Speech Tagging of Korean. Computational Lin-
guistics, 28(1):53?70.
Manabu Sassano and Takehito Utsuro. 2000. Named
Entity Chunking Techniques in Supervised Learning
for Japanese Named Entity Recognition. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 705?711,
Germany.
Choong-Nyoung Seon, Youngjoong Ko, Jeong-Seok
Kim, and Jungyun Seo. 2001. Named Entity Recog-
nition using Machine Learning Methods and Pattern-
Selection Rules. In Proceedings of the Sixth Natural
Language Processing Pacific Rim Symposium, pages
229?236, Tokyo, Japan.
Junhyeok Shim, Dongseok Kim, Jeongwon Cha,
Gary Geunbae Lee, and Jungyun Seo. 2002. Multi-
strategic Integrated Web Document Pre-processing for
Sentence and Word Boundary Detection. Information
Processing and Management, 38(4):509?527.
Bo-Hyun Yun, Min-Jeung Cho, and Hae-Chang Rim.
1997. Segmenting Korean Compound Nouns using
Statistical Information and a Preference Rule. Jour-
nal of Korean Information Science Society, 24(8):900?
909.
GuoDong Zhou and Jian Su. 2002. Named Entity
Recognition using an HMM-based Chunk Tagger. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
473?480, Philadelphia, USA.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 412?419,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploiting Non-local Features for Spoken Language Understanding
Minwoo Jeong and Gary Geunbae Lee
Department of Computer Science & Engineering
Pohang University of Science and Technology,
San 31 Hyoja-dong, Nam-gu
Pohang 790-784, Korea
{stardust,gblee}@postech.ac.kr
Abstract
In this paper, we exploit non-local fea-
tures as an estimate of long-distance de-
pendencies to improve performance on the
statistical spoken language understanding
(SLU) problem. The statistical natural
language parsers trained on text perform
unreliably to encode non-local informa-
tion on spoken language. An alternative
method we propose is to use trigger pairs
that are automatically extracted by a fea-
ture induction algorithm. We describe a
light version of the inducer in which a sim-
ple modification is efficient and success-
ful. We evaluate our method on an SLU
task and show an error reduction of up to
27% over the base local model.
1 Introduction
For most sequential labeling problems in natural
language processing (NLP), a decision is made
based on local information. However, processing
that relies on the Markovian assumption cannot
represent higher-order dependencies. This long-
distance dependency problem has been considered
at length in computational linguistics. It is the key
limitation in bettering sequential models in vari-
ous natural language tasks. Thus, we need new
methods to import non-local information into se-
quential models.
There are two types of method for using non-
local information. One is to add edges to structure
to allow higher-order dependencies and another is
to add features (or observable variables) to encode
the non-locality. An additional consistent edge of
a linear-chain conditional random field (CRF) ex-
plicitly models the dependencies between distant
occurrences of similar words (Sutton and McCal-
lum, 2004; Finkel et al, 2005). However, this
approach requires additional time complexity in
inference/learning time and it is only suitable for
representing constraints by enforcing label consis-
tency. We wish to identify ambiguous labels with
more general dependency without additional time
cost in inference/learning time.
Another approach to modeling non-locality is
to use observational features which can capture
non-local information. Traditionally, many sys-
tems prefer to use a syntactic parser. In a language
understanding task, the head word dependencies
or parse tree path are successfully applied to learn
and predict semantic roles, especially those with
ambiguous labels (Gildea and Jurafsky, 2002). Al-
though the power of syntactic structure is impres-
sive, using the parser-based feature fails to encode
correct global information because of the low ac-
curacy of a modern parser. Furthermore the inac-
curate result of parsing is more serious in a spoken
language understanding (SLU) task. In contrast
to written language, spoken language loses much
information including grammar, structure or mor-
phology and contains some errors in automatically
recognized speech.
To solve the above problems, we present one
method to exploit non-local information ? the trig-
ger feature. In this paper, we incorporate trig-
ger pairs into a sequential model, a linear-chain
CRF. Then we describe an efficient algorithm to
extract the trigger feature from the training data it-
self. The framework for inducing trigger features
is based on the Kullback-Leibler divergence cri-
terion which measures the improvement of log-
likelihood on the current parameters by adding a
new feature (Pietra et al, 1997). To reduce the
cost of feature selection, we suggest a modified
412
version of an inducing algorithm which is quite ef-
ficient. We evaluate our method on an SLU task,
and demonstrate the improvements on both tran-
scripts and recognition outputs. On a real-world
problem, our modified version of a feature selec-
tion algorithm is very efficient for both perfor-
mance and time complexity.
2 Spoken Language Understanding as a
Sequential Labeling Problem
2.1 Spoken Language Understanding
The goal of SLU is to extract semantic mean-
ings from recognized utterances and to fill the
correct values into a semantic frame structure.
A semantic frame (or template) is a well-formed
and machine readable structure of extracted in-
formation consisting of slot/value pairs. An ex-
ample of such a reference frame is as follows.
<s> i wanna go from denver to new york on
november eighteenth </s>
FROMLOC.CITY NAME = denver
TOLOC.CITY NAME = new york
MONTH NAME = november
DAY NUMBER = eighteenth
This example from air travel data (CU-
Communicator corpus) was automatically gener-
ated by a Phoenix parser and manually corrected
(Pellom et al, 2000; He and Young, 2005). In this
example, the slot labels are two-level hierarchi-
cal; such as FROMLOC.CITY NAME. This hier-
archy differentiates the semantic frame extraction
problem from the named entity recognition (NER)
problem.
Regardless of the fact that there are some
differences between SLU and NER, we can
still apply well-known techniques used in NER
to an SLU problem. Following (Ramshaw
and Marcus, 1995), the slot labels are drawn
from a set of classes constructed by extending
each label by three additional symbols, Begin-
ning/Inside/Outside (B/I/O). A two-level hierar-
chical slot can be considered as an integrated flat-
tened slot. For example, FROMLOC.CITY NAME
and TOLOC.CITY NAME are different on this slot
definition scheme.
Now, we can formalize the SLU prob-
lem as a sequential labeling problem, y? =
argmaxy P (y|x). In this case, input word se-
quences x are not only lexical strings, but also
multiple linguistic features. To extract semantic
frames from utterance inputs, we use a linear-
chain CRF model; a model that assigns a joint
probability distribution over labels which is con-
ditional on the input sequences, where the distri-
bution respects the independent relations encoded
in a graph (Lafferty et al, 2001).
A linear-chain CRF is defined as follows. Let
G be an undirected model over sets of random
variables x and y. The graph G with parameters
? = {?, . . .} defines a conditional probability for
a state (or label) sequence y = y1, . . . , yT , given
an input x = x1, . . . , xT , to be
P?(y|x) = 1Zx exp
( T?
t=1
?
k
?kfk(yt?1, yt,x, t)
)
where Zx is the normalization factor that makes
the probability of all state sequences sum to one.
fk(yt?1, yt,x, t) is an arbitrary linguistic feature
function which is often binary-valued in NLP
tasks. ?k is a trained parameter associated with
feature fk. The feature functions can encode any
aspect of a state transition, yt?1 ? yt, and the
observation (a set of observable features), x, cen-
tered at the current time, t. Large positive val-
ues for ?k indicate a preference for such an event,
while large negative values make the event un-
likely.
Parameter estimation of a linear-chain CRF is
typically performed by conditional maximum log-
likelihood. To avoid overfitting, the 2-norm reg-
ularization is applied to penalize on weight vec-
tor whose norm is too large. We used a limited
memory version of the quasi-Newton method (L-
BFGS) to optimize this objective function. The
L-BFGS method converges super-linearly to the
solution, so it can be an efficient optimization
technique on large-scale NLP problems (Sha and
Pereira, 2003).
A linear-chain CRF has been previously applied
to obtain promising results in various natural lan-
guage tasks, but the linear-chain structure is defi-
cient in modeling long-distance dependencies be-
cause of its limited structure (n-th order Markov
chains).
2.2 Long-distance Dependency in Spoken
Language Understanding
In most sequential supervised learning prob-
lems including SLU, the feature function
fk(yt?1, yt,xt, t) indicates only local information
413
for practical reasons. With sufficient local context
(e.g. a sliding window of width 5), inference and
learning are both efficient.
However, if we only use local features, then
we cannot model long-distance dependencies.
Thus, we should incorporate non-local infor-
mation into the model. For example, figure
1 shows the long-distance dependency problem
in an SLU task. The same two word to-
kens ?dec.? should be classified differently,
DEPART.MONTH and RETURN.MONTH. The
dotted line boxes represent local information at the
current decision point (?dec.?), but they are ex-
actly the same in two distinct examples. More-
over, the two states share the same previous
sequence (O, O, FROMLOC.CITY NAME-B,
O, TOLOC.CITY NAME-B, O). If we cannot
obtain higher-order dependencies such as ?fly?
and ?return,? then the linear-chain CRF cannot
classify the correct labels between the two same
tokens. To solve this problem, we propose an ap-
proach to exploit non-local information in the next
section.
3 Incorporating Non-local Information
3.1 Using Trigger Features
To exploit non-local information to sequential la-
beling for a statistical SLU, we can use two ap-
proaches; a syntactic parser-based and a data-
driven approach. Traditionally, information ex-
traction and language understanding fields have
usually used a syntactic parser to encode global
information (e.g. parse tree path, governing cat-
egory, or head word) over a local model. In a se-
mantic role labeling task, the syntax and semantics
are correlated with each other (Gildea and Juraf-
sky, 2002), that is, the global structure of the sen-
tence is useful for identifying ambiguous semantic
roles. However the problem is the poor accuracy
of the syntactic parser with this type of feature. In
addition, recognized utterances are erroneous and
the spoken language has no capital letters, no ad-
ditional symbols, and sometimes no grammar, so
it is difficult to use a parser in an SLU problem.
Another solution is a data-driven method, which
uses statistics to find features that are approxi-
mately modeling long-distance dependencies. The
simplest way is to use identical words in history or
lexical co-occurrence, but we wish to use a more
general tool; triggering. The trigger word pairs
are introduced by (Rosenfeld, 1994). A trigger
pair is the basic element for extracting informa-
tion from the long-distance document history. In
language modeling, n-gram based on the Marko-
vian assumption cannot represent higher-order de-
pendencies, but it can automatically extract trigger
word pairs from data. The pair (A ? B) means
that word A and B are significantly correlated, that
is, when A occurs in the document, it triggers B,
causing its probability estimate to change.
To select reasonable pairs from arbitrary word
pairs, (Rosenfeld, 1994) used averaged mutual in-
formation (MI). In this scheme, the MI score of
one pair is MI(A;B) =
P (A,B) log P (B|A)P (B) + P (A, B?) log
P (B?|A)
P (B?) +
P (A?, B) log P (B|A?)P (B?) + P (A?, B?) log
P (B?|A?)
P (B?) .
Using the MI criterion, we can select corre-
lated word pairs. For example, the trigger pair
(dec.?return) was extracted with score 0.001179
in the training data1. This trigger word pair can
represent long-distance dependency and provide a
cue to identify ambiguous classes. The MI ap-
proach, however, considers only lexical colloca-
tion without reference labels y, and MI based se-
lection tends to excessively select the irrelevant
triggers. Recall that our goal is to find the signif-
icantly correlated trigger pairs which improve the
model. Therefore, we use a more appropriate se-
lection method for sequential supervised learning.
3.2 Selecting Trigger Feature
We present another approach to extract relevant
triggers and exploit them in a linear-chain CRF.
Our approach is based on an automatic feature in-
duction algorithm, which is a novel method to se-
lect a feature in an exponential model (Pietra et al,
1997; McCallum, 2003). We follow McCallum?s
work which is an efficient method to induce fea-
tures in a linear-chain CRF model. Following the
framework of feature inducing, we start the algo-
rithm with an empty set, and iteratively increase
the bundle of features including local features and
trigger features. Our basic assumption, however,
is that the local information should be included
because the local features are the basis of the de-
cision to identify the classes, and they reduce the
1In our experiment, the pair (dec.?fly) cannot be selected
because this MI score is too low. However, the trigger pair is
a binary type feature, so the pair (dec.?return) is enough to
classify the two cases in the previous example.
414
1999dec.onchicagotodenverfromfly... 10th
1999dec.onchicagotodenverfrom... 10threturn ...
...
DEPART.MONTH
RETURN.MONTH
Figure 1: An example of a long-distance dependency problem in spoken language understanding. In
this case, a word token ?dec.? with local feature set (dotted line box) is ambiguous for determining the
correct label (DEPART.MONTH or RETURN.MONTH).
mismatch between training and testing tasks. Fur-
thermore, this assumption leads us to faster train-
ing in the inducing procedure because we can only
consider additional trigger features.
Now, we start the inducing process with local
features rather than an empty set. After training
the base model ?(0), we should calculate the gains,
which measure the effect of adding a trigger fea-
ture, based on the local model parameter ?(0). The
gain of the trigger feature is defined as the im-
provement in log-likelihood of the current model
?(i) at the i-th iteration according to the following
formula:
G??(i)(g) = max? G?(i)(g, ?)
= max?
{
L?(i)+g,? ? L?(i)
}
where ? is a parameter of a trigger feature to
be found and g is a corresponding trigger feature
function. The optimal value of ? can be calculated
by Newton?s method.
By adding a new candidate trigger, the equation
of the linear-chain CRF model is changed to an
additional feature model as P?(i)+g,?(y|x) =
P?(i)(y|x) exp
(?T
t=1 ?g(yt?1, yt,x, t)
)
Zx(?(i), g, ?)
.
Note that Zx(?(i), g, ?) is the marginal sum over
all states of y?. Following (Pietra et al, 1997; Mc-
Callum, 2003), the mean field approximation and
agglomerated features allows us to treat the above
calculation as the independent inference problem
rather than sequential inference. We can evaluate
the probability of state y with an adding trigger
pair given observation x separately as follows.
P?(i)+g,?(y|x, t) =
P?(i)(y|x, t) exp (?g(yt,x, t))
Zx(?(i), g, ?)
Here, we introduce a second approximation. We
use the individual inference problem over the un-
structured maximum entropy (ME) model whose
state variable is independent from other states in
history. The background of our approximation is
that the state independent problem of CRF can
be relaxed to ME inference problem without the
state-structured model. In the result, we calculate
the gain of candidate triggers, and select trigger
features over a light ME model instead of a huge
computational CRF model2.
We can efficiently assess many candidate trig-
ger features in parallel by assuming that the old
features remain fixed while estimating the gain.
The gain of trigger features can be calculated on
the old model that is trained with the local and
added trigger pairs in previous iterations. Rather
than summing over all training instances, we only
need to use the mislabeled N tokens by the cur-
rent parameter ?(i) (McCallum, 2003). From mis-
classified instances, we generate the candidates of
trigger pairs, that is, all pairs of current words and
others within the sentence. With the candidate fea-
ture set, the gain is
G??(i)(g) = N??E?[g]
?
N?
j=1
log (E?(i) [exp(??g)|xj ])?
??2
2?2 .
Using the estimated gains, we can select a small
portion of all candidates, and retrain the model
with selected features. We iteratively perform the
selection algorithm with some stop conditions (ex-
cess of maximum iteration or no added feature up
to the gain threshold). The outline of the induction
2The ME model cannot represent the sequential structure
and the resulting model is different from CRF. Nevertheless,
we empirically prove that the effect of additional trigger fea-
tures on both ME and approximated CRF (without regarding
edge-state) are similar (see the experiment section).
415
Algorithm InduceLearn(x,y)
triggers ? {?} and i ? 0
while |pairs| > 0 and i < maxiter do
?(i) ? TrainME(x,y)
P (ye|xe) ? Evaluate(x,y,?(i))
c ? MakeCandidate(xe)
G?(i) ? EstimateGain(c, P (ye|xe))
pairs ? SelectTrigger(c, G?(i))
x ? UpdateObs(x, pairs)
triggers ? triggers ? pairs and i ? i+ 1
end while
?(i+1) ? TrainCRF(x,y)
return ?(i+1)
Figure 2: Outline of trigger feature induction al-
gorithm
algorithms is described in figure 2. In the next sec-
tion, we empirically prove the effectiveness of our
algorithm.
The trigger pairs introduced by (Rosenfeld,
1994) are just word pairs. Here, we can gen-
eralize the trigger pairs to any arbitrary pairs of
features. For example, the feature pair (of?B-
PP) is useful in deciding the correct answer
PERIOD OF DAY-I in ?in the middle of the day.?
Without constraints on generating the pairs (e.g.
at most 3 distant tokens), the candidates can be
arbitrary conjunctions of features3. Therefore we
can explore any features including local conjunc-
tion or non-local singleton features in a uniform
framework.
4 Experiments
4.1 Experimental Setup
We evaluate our method on the CU-Communicator
corpus. It consists of 13,983 utterances. The se-
mantic categories correspond to city names, time-
related information, airlines and other miscella-
neous entities. The semantic labels are automat-
ically generated by a Phoenix parser and manually
corrected. In the data set, the semantic category
has a two-level hierarchy: 31 first level classes
and 7 second level classes, for a total of 62 class
combinations. The data set is 630k words with
29k entities. Roughly half of the entities are time-
related information, a quarter of the entities are
3In our experiment, we do not consider the local conjunc-
tions because we wish to capture the effect of long-distance
entities.
city names, a tenth are state and country names,
and a fifth are airline and airport names. For
the second level hierarchy, approximately three
quarters of the entities are ?NONE?, a tenth are
?TOLOC?, a tenth are ?FROMLOC?, and the re-
maining are ?RETURN?, ?DEPERT?, ?ARRIVE?,
and ?STOPLOC.?
For spoken inputs, we used the open source
speech recognizer Sphinx2. We trained the recog-
nizer with only the domain-specific speech corpus.
The reported accuracy for Sphinx2 speech recog-
nition is about 85%, but the accuracy of our speech
recognizer is 76.27%; we used only a subset of the
data without tuning and the sentences of this sub-
set are longer and more complex than those of the
removed ones, most of which are single-word re-
sponses.
All of our results have averaged over 5-fold
cross validation with an 80/20 split of the data.
As it is standard, we compute precision and re-
call, which are evaluated on a per-entity basis and
combined into a micro-averaged F1 score (F1 =
2PR/(P+R)).
A final model (a first-order linear chain CRF)
is trained for 100 iterations with a Gaussian prior
variance of 20, and 200 or fewer trigger features
(down to a gain threshold of 1.0) for each round of
inducing iteration (100 iterations of L-BFGS for
the ME inducer and 10?20 iterations of L-BFGS
for the CRF inducer). All experiments are imple-
mented in C++ and executed on Linux with XEON
2.8 GHz dual processors and 2.0 Gbyte of main
memory.
4.2 Empirical Results
We list the feature templates used by our experi-
ment in figure 3. For local features, we use the
indicators for specific words at location i, or lo-
cations within five words of i (?2,?1, 0,+1,+2
words on current position i). We also use the part-
of-speech (POS) tags and phrase labels with par-
tial parsing. Like words, the two basic linguis-
tic features are located within five tokens. For
comparison, we exploit the two groups of non-
local syntax parser-based features; we use Collins
parser and extract this type of features from the
parse trees. The first consists of the head word
and POS-tag of the head word. The second group
includes governing category and parse tree paths
introduced by semantic role labeling (Gildea and
Jurafsky, 2002). Following the previous studies
416
Local feature templates
-lexical words
-part-of-speech (POS) tags
-phrase chunk labels
Grammar-based feature templates
-head word / POS-tag
-parse tree path and governing category
Trigger feature templates
-word pairs (wi ? wj), |i? j| > 2
-feature pairs between words, POS-tags, and
chunk labels (fi ? fj), |i? j| > 2
-null pairs (? ? wj)
Figure 3: Feature templates
of semantic role labeling, the parse tree path im-
proves the classification performance of semantic
role labeling. Finally, we use the trigger pairs that
are automatically extracted from the training data.
Avoiding the overlap of local features, we add the
constraint |i? j| > 2 for the target word wj . Note
that null pairs are equivalent to long-distance sin-
gleton word features wj .
To compute feature performance, we begin with
word features and iteratively add them one-by-one
so that we achieve the best performance. Table 1
shows the empirical results of local features, syn-
tactic parser-based features, and trigger features
respectively. The two F1 scores for text tran-
scripts (Text) and outputs recognized by an au-
tomatic speech recognizer (ASR) are listed. We
achieved F1 scores of 94.79 and 71.79 for Text and
ASR inputs using only word features. The perfor-
mance is decreased by adding the additional local
features (POS-tags and chunk labels) because the
pre-processor brings more errors to the system for
spoken dialog.
The parser-based and trigger features are added
to two baselines: word only and all local features.
The result shows that the trigger feature is more
robust to an SLU task than the features generated
from the syntactic parser. The parse tree path and
governing category show a small improvement of
performance over local features, but it is rather in-
significant (word vs. word+path, McNemar?s test
(Gillick and Cox, 1989); p = 0.022). In contrast,
the trigger features significantly improve the per-
formance of the system for both Text and ASR
inputs. The differences between the trigger and
the others are statistically significant (McNemar?s
test; p < 0.001 for both Text and ASR).
Table 1: The result of local features, parser-based
features and trigger features
Feature set F1 (Text) F1 (ASR)
word (w) 94.79 71.79
w + POStag (p) 94.57 71.61
w + chunk (c) 94.70 71.64
local (w+p+c) 94.41 71.60
w + head (h) 94.55 71.76
w + path (t) 95.07 72.17
w + h + t 94.84 72.09
local + head (h) 94.17 71.39
local + path (t) 94.80 71.89
local + h + t 94.51 71.67
w + trigger 96.18 72.95
local + trigger 96.04 72.72
Next, we compared the two trigger selection
methods; mutual information (MI) and feature in-
duction (FI). Table 2 shows the experimental re-
sults of the comparison between MI and FI ap-
proaches (with the local feature set; w+p+c). For
the MI-based approach, we should calculate an av-
eraged MI for each word pair appearing in a sen-
tence and cut the unreliable pairs (down to thresh-
old of 0.0001) before training the model. In con-
trast, the FI-based approach selects reliable trig-
gers which should improve the model in train-
ing time. Our method based on the feature in-
duction algorithm outperforms simple MI-based
methods. Fewer features are selected by FI, that
is, our method prunes the event pairs which are
highly correlated, but not relevant to models. The
extended feature trigger (fi ? fj) and null trig-
gers (? ? wj) improve the performance over word
trigger pairs (wi ? wj), but they are not statisti-
cally significant (vs. (fi ? fj); p = 0.749, vs.
({?, wi} ? wj); p = 0.294). Nevertheless, the
null pairs are effective in reducing the size of trig-
ger features.
Figure 4 shows a sample of triggers selected by
MI and FI approaches. For example, the trigger
?morning ? return? is ranked in first of FI but
66th of MI. Moreover, the top 5 pairs of MI are
not meaningful, that is, MI selects many functional
word pairs. The MI approach considers only lexi-
cal collocation without reference labels, so the FI
method is more appropriate to sequential super-
vised learning.
Finally, we wish to justify that our modified
417
Table 2: Result of the trigger selection methods
Method Avg. # triggers F1 (Text) F1 (ASR) McNemar?s test (vs. MI)
MI (wi ? wj) 1,713 95.20 72.12 -
FI (wi ? wj) 702 96.04 72.72 p < 0.001
FI (fi ? fj) 805 96.04 72.76 p < 0.001
FI ({?, wi} ? wj) 545 96.14 72.80 p < 0.001
Mutual Information Feature Induction
[1] from?like [1] morning?return
[2] on?to [2] morning?on
[3] to?i [3] morning?to
[4] on?from [4] afternoon?on
[5] from?i [5] afternoon?return
[41] afternoon?return [6] afternoon?to
[66] morning?return [15] morning?leaving
[89] morning?leaving [349] december?return
[1738] london?fly [608] illinois?airport
Figure 4: A sample of triggers extracted by two
methods
version of an inducing algorithm is efficient and
maintains performance without any drawbacks.
We proposed two approximations: starting with
local features (Approx. 1) and using an unstruc-
tured model on the selection stage (Approx. 2),
Table 3 shows the results of variant versions of
the algorithm. Surprisingly, the selection crite-
rion based on ME (the unstructured model) is bet-
ter than CRF (the structured model) not only for
time cost but also for the performance on our ex-
periment4. This result shows that local informa-
tion provides the fundamental decision clues. Our
modification of the algorithm to induce features
for CRF is sufficiently fast for practical usage.
5 Related Work and Discussion
The most relevant previous work is (He and
Young, 2005) who describes an generative ap-
proach ? hidden vector state (HVS) model. They
used 1,178 test utterances with 18 classes for 1st
level label, and published the resulting F1 score
of 88.07. Using the same test data and classes,
we achieved the 92.77 F1-performance, as well
4In our analysis, 10?20 iterations for each round of in-
ducing procedure are insufficient in optimizing the model in
CRF (empty) inducer. Thus, the resulting parameters are
under-fitted and selected features are infeasible. We need
more iteration to fit the parameters, but they require too much
learning time (> 1 day).
as 39% of error reduction compared to the previ-
ous result. Our system uses a discriminative ap-
proach, which directly models the conditional dis-
tribution, and it is sufficient for classification task.
To capture long-distance dependency, HVS uses a
context-free model, which increases the complex-
ity of models. In contrast, we use non-local trigger
features, which are relatively easy to use without
having additional complexity of models.
Trigger word pairs are introduced and success-
fully applied in a language modeling task. (Rosen-
feld, 1994) demonstrated that the trigger word
pairs improve the perplexity in ME-based lan-
guage models. Our method extends this idea to
sequential supervised learning problems. Our trig-
ger selection criterion is based on the automatic
feature inducing algorithm, and it allows us to gen-
eralize the arbitrary pairs of features.
Our method is based on two works of fea-
ture induction on an exponential model, (Pietra et
al., 1997) and (McCallum, 2003). Our induction
algorithm builds on McCallum?s method which
presents an efficient procedure to induce features
on CRF. (McCallum, 2003) suggested using only
the mislabeled events rather than the whole train-
ing events. This intuitional suggestion has offered
us fast training. We added two additional approx-
imations to reduce the time cost; 1) an inducing
procedure over a conditional non-structured infer-
ence problem rather than an approximated sequen-
tial inference problem, and 2) training with a local
feature set, which is the basic information to iden-
tify the labels.
In this paper, our approach describes how to
exploit non-local information to a SLU prob-
lem. The trigger features are more robust than
grammar-based features, and are easily extracted
from the data itself by using an efficient selection
algorithm.
418
Table 3: Comparison of variations in the induction algorithm (performed on one of the 5-fold validation
sets); columns are induction and total training time (h:m:s), number of trigger and total features, and
f-score on test data.
Inducer type Approx. Induction/total time # triggers/features F1 (Text) F1 (ASR)
CRF (empty) No approx. 3:55:01 / 5:27:13 682 / 2,693 90.23 67.60
CRF (local) Approx. 1 1:25:28 / 2:56:49 750 / 5,241 94.87 71.65
ME (empty) Approx. 2 20:57 / 1:54:22 618 / 2,080 94.85 71.46
ME (local) Approx. 1+2 6:30 / 1:36:14 608 / 5,099 95.17 71.81
6 Conclusion
We have presented a method to exploit non-local
information into a sequential supervised learning
task. In a real-world problem such as statistical
SLU, our model performs significantly better than
the traditional models which are based on syntac-
tic parser-based features. In comparing our se-
lection criterion, we find that the mutual informa-
tion tends to excessively select the triggers while
our feature induction algorithm alleviates this is-
sue. Furthermore, the modified version of the al-
gorithm is practically fast enough to maintain its
performance particularly when the local features
are offered by the starting position of the algo-
rithm.
In this paper, we have focused on a sequential
model such as a linear-chain CRF. However, our
method can also be naturally applied to arbitrary
structured models, thus the first alternative is to
combine our methods with a skip-chain CRF (Sut-
ton and McCallum, 2004). Applying and extend-
ing our approach to other natural language tasks
(which are difficult to apply a parser to) such as in-
formation extraction from e-mail data or biomed-
ical named entity recognition is a topic of future
work.
Acknowledgements
We thank three anonymous reviewers for helpful
comments. This research was supported by the
MIC (Ministry of Information and Communica-
tion), Korea, under the ITRC (Information Tech-
nology Research Center) support program super-
vised by the IITA (Institute of Information Tech-
nology Assessment). (IITA-2005-C1090-0501-
0018)
References
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of ACL?05, pages 363?370.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of ICASSP, pages 532?535.
Y. He and S. Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech &
Language, 19(1):85?106.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML, pages 282?289.
A. McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of UAI,
page 403.
B. L. Pellom, W. Ward, and S. S. Pradhan. 2000. The
cu communicator: An architecture for dialogue sys-
tems. In Proceedings of ICSLP.
S. Della Pietra, V. J. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Trans.
Pattern Anal. Mach. Intell, 19(4):380?393.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In 3rd
Workshop on Very Large Corpora, pages 82?94.
R. Rosenfeld. 1994. Adaptive statistical language
modeling: A maximum entropy approach. Tech-
nical report, School of Computer Science Carnegie
Mellon University.
F. Sha and F. Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT/NAACL?03.
C. Sutton and A. McCallum. 2004. Collective segmen-
tation and labeling of distant entities in information
extraction. In ICML Workshop on Statistical Rela-
tional Learning.
419
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 61?64,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Joint Statistical Model for Simultaneous Word Spacing and 
 Spelling Error Correction for Korean 
Hyungjong Noh* Jeong-Won Cha** Gary Geunbae Lee* 
*Department of Computer Science and Engineering 
Pohang University of Science & Technology (POSTECH) 
San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea 
 
** Changwon National University 
Department of Computer information & Communication 
9 Sarim-dong, Changwon Gyeongnam, Korea 641-773  
 
nohhj@postech.ac.kr jcha@changwon.ac.kr gblee@postech.ac.kr 
Abstract 
This paper presents noisy-channel based 
Korean preprocessor system, which cor-
rects word spacing and typographical errors. 
The proposed algorithm corrects both er-
rors simultaneously. Using Eojeol transi-
tion pattern dictionary and statistical data 
such as Eumjeol n-gram and Jaso transition 
probabilities, the algorithm minimizes the 
usage of huge word dictionaries. 
1 Introduction 
With increasing usages of messenger and SMS, we 
need an efficient text normalizer that processes 
colloquial style sentences. As in the case of general 
literary sentences, correcting word spacing error 
and spelling error is the very essential problem 
with colloquial style sentences. 
In order to correct word spacing errors, many 
algorithms were used, which can be divided into 
statistical algorithms and rule-based algorithms. 
Statistical algorithms generally use character n-
gram (Eojeol 1  or Eumjeol 2  n-gram in Korean) 
(Kang and Woo, 2001; Kwon, 2002) or noisy-
channel model (Gao et. al., 2003). Rule-based al-
gorithms are mostly heuristic algorithms that re-
flect linguistic knowledge (Yang et al, 2005) to 
solve word spacing problem. Word spacing prob-
lem is treated especially in Japanese or Chinese, 
                                                 
1 Eojeol is a Korean spacing unit which consists of one or 
more Eumjeols (morphemes). 
2 Eumjeol is a Korean syllable. 
which does not use word boundary, or Korean, 
which is normally segmented into Eojeols, not into 
words or morphemes. 
The previous algorithms for spelling error cor-
rection basically use a word dictionary. Each word 
in a sentence is compared to word dictionary en-
tries, and if the word is not in the dictionary, then 
the system assumes that the word has spelling er-
rors. Then corrected candidate words are suggested 
by the system from the word dictionary, according 
to some metric to measure the similarity between 
the target word and its candidate word, such as 
edit-distance (Kashyap and Oommen, 1984; Mays 
et al, 1991). 
But these previous algorithms have a critical li-
mitation: They all corrected word spacing errors 
and spelling errors separately. Word spacing algo-
rithms define the problem as a task for determining 
whether to insert the delimiter between characters 
or not. Since the determination is made according 
to the characters, the algorithms cannot work if the 
characters have spelling errors. Likewise, algo-
rithms for solving spelling error problem cannot 
work well with word spacing errors. 
To cope with the limitation, there is an algo-
rithm proposed for Japanese (Nagata, 1996). Japa-
nese sentence cannot be divided into words, but 
into chunks (bunsetsu in Japanese), like Eojeol in 
Korean. The proposed system is for sentences rec-
ognized by OCR, and it uses character transition 
probabilities and POS (part of speech) tag n-gram. 
However it needs a word dictionary and takes long 
time for searching many character combinations. 
61
We propose a new algorithm which can correct 
both word spacing error and spelling error simulta-
neously for Korean. This algorithm is based on 
noisy-channel model, which uses Jaso3  transition 
probabilities and Eojeol transition probabilities to 
create spelling correction candidates. Candidates 
are increased in number by inserting the blank cha-
racters on the created candidates, which cover the 
spacing error correction candidates. We find the 
best candidate sentence from the networks of Ja-
so/Eojeol candidates. This method decreases the 
size of Eojeol transition pattern dictionary and cor-
rects the patterns which are not in the dictionary. 
The remainder of this paper is as follows: Sec-
tion 2 describes why we use Jaso transition prob-
ability for Korean. Section 3 describes the pro-
posed model in detail. Section 4 provides the ex-
periment results and analyses. Finally, section 5 
presents our conclusion. 
2 Spelling Error Correction with Jaso 
Transition4 Probabilities 
We can use Eumjeol transition probabilities or Jaso 
transition probabilities for spelling error correction 
for Korean. We choose Jaso transition probabilities 
because there are several advantages. Since an 
Eumjeol is a combination of 3 Jasos, the number of 
all possible Eumjeols is much larger than that of all 
possible Jasos. In other words, Jaso-based 
language model is smaller than Eumjeol-based 
language model. Various errors in Eumjeol (even if 
they do not appear as an Eumjeol pattern in a 
training corpus) can be corrected by correction in 
Jaso unit. Also, Jaso transition probabilities can be 
extracted from relatively small corpus. This merit 
is very important since we do not normally have 
such a huge corpus which is very hard to collect, 
since we have to pair the spelling errors with 
corresponding corrections.  
We obtain probabilities differently for each 
case: single Jaso transition case, two Jaso?s transi-
tion case, and more than two Jasos transition case. 
In single Jaso transition case, the spelling errors 
are corrected by only one Jaso transition (e.g. 
??????? / ???). The case of correcting 
by deleting Jaso is also one of the single Jaso tran-
                                                 
3 Jaso is a Korean character. 
4 ?Transition? means the correct character is changed to other 
character due to some causes, such as typographical errors. 
sition case (??????? / ??X5). The Jaso 
transition probabilities are calculated by counting 
the transition frequencies in a training corpus. 
In two Jaso?s transition case, the spelling errors 
are corrected by adjacent two Jasos transition 
(????? / ???X?). In this case, we treat 
two Jaso?s as one transition unit. The transition 
probability calculation is the same as above. 
In more than two Jaso?s transition case, the spel-
ling errors cannot be corrected only by Jaso transi-
tion (????). In this case, we treat the whole 
Eojeols as one transition unit, and build an Eojeol 
transition pattern dictionary for these special cases. 
3 A Joint Statistical Model for Word 
Spacing and Spelling Error Correction 
3.1 Problem Definition 
Given a sentence T  which includes both word 
spacing errors and spelling errors, we create 
correction candidates C  from T , and find the best 
candidate that has the highest transition 
probability from C . 
'C
).|(maxarg' TCPC C=               (1) 
3.2 Model Description 
A given sentence T  and candidates  consist of 
Eumjeol  and the blank character . 
C
is ib
nnbsbsbsbsT ...332211= . 
....332211 nnbsbsbsbsC =                (2) 
(n is the number of Eumjeols) 
Eumjeol  consists of 3 Jasos, Choseong (on-
set), Jungseong (nucleus), and Jongseong (coda). 
The empty Jaso is defined as ?X?.  is ?
is
ib B ? when 
the blank exists, and ?? ? when the blank does not 
exist. 
321 iiii jjjs = .                        (3) 
( : Choseong, : Jungseong, : Jongseong) 1ij 2ij 3ij
Now we apply Bayes? Rule for : 'C
)|(maxarg' TCPC C=  
).()|(maxarg
)(/)()|(maxarg
CPCTP
TPCPCTP
C
C
=
=
             (4) 
                                                 
5 ?X? indicates that there is no Jaso in that position. 
62
)(CP  can be obtained using trigrams of Eum-
jeols (with the blank character) that  includes. C
?
=
??=
n
i
iii cccPCP
1
21 )|()( ,  or b .    (5) sc =
And  can be written as multiplication 
of each Jaso transition probability and the blank 
character transition probability. 
)|( CTP
)|()|(
1
'?
=
=
n
i
ii ssPCTP  
.)]|()|()|()|([
1
''
33
'
22
'
11?
=
=
n
i
iiiiiiii bbPjjPjjPjjP  
(6) 
We use logarithm of  in implementa-
tion. Figure 1 shows how the system creates the 
Jaso candidates network. 
)|( TCP
 
Figure 1: An example6 of Jaso candidate network. 
 
In Figure 1, the topmost line is the sequence of 
Jasos of the input sentence. Each Eumjeol in the 
sentence is decomposed into 3 Jasos as above, and 
each Jaso has its own correction candidates. For 
example, Jaso ??? at 4th column has its candidates 
???, ??? and ?X?. And two jaso?s ?X?? at 13th 
and 14th column has its candidates ????, 
????, ????, ????, and ????. The undermost 
gray square is an Eojeol (which is decomposed into 
Jasos) candidate ???X?????X? created 
from ???X??X?. Each jaso candidate has its 
own transition probability, 7)|(log 'ikik jjP , that is 
used for calculating . )|( TCP
In order to calculate , we need Eumjeol-
based candidate network. Hence, we convert the 
above Jaso candidate network into Eumjeol/Eojeol 
candidate network. Figure 2 shows part of the final 
)(CP
                                                 
6 The example sentence is ??????????????. 
7 In real implementation, we used ?a*logP(jik|j?ik) + b? by 
determining constants a and b with parameter optimization  
(a = 1.0, b = 3.0). 
network briefly. At this time, the blank characters 
? B ? and ? ? ? are inserted into each Eum-
jeol/Eojeol candidates. To find the best path from 
the candidates, we conduct viterbi-search from 
leftmost node corresponding to the beginning of 
the sentence. When Eumjeol/Eojeol candidates are 
selected, the algorithm prunes the candidates ac-
cording to the accumulated probabilities, doing 
beam search. Once the best path is found, the sen-
tence corrected by both spacing and spelling errors 
is extracted by backtracking the path. In Figure 2, 
thick squares represent the nodes selected by the 
best path.  
 
Figure 2: A final Eumjeol/Eojeol candidate network8
4  Experiments and Analyses 
4.1  Corpus Information 
 Table 1: Corpus information 
 
Table 1 shows the information of corpus which is 
used for experiments. All corpora are obtained 
from Korean web chatting site log. Each corpus 
has pair of sentences, sentences containing errors 
and sentences with those errors corrected. Jaso 
transition patterns and Eojeol transition patterns 
are extracted from training corpus. Also, Eumjeol 
n-grams are also obtained as a language model. 
                                                 
8 The final corrected sentence is ??? ??? ??? 
??? ???. 
 Training Test 
Sentences 60076 6006 
Eojeols 302397 30376 
Error Sentences (%) 15335  (25.53) 
1512 
 (25.17) 
Error Eojeols (%) 31297 (10.35) 
3111 
(10.24) 
63
4.2  Experiment Results and Analyses 
 We used two separate Eumjeol n-grams as lan-
guage models for experiments. N-gram A is ob-
tained from only training corpus and n-gram B is 
obtained from all training and test corpora. All ac-
curacies are measured based on Eojeol unit. 
Table 2 shows the results of word spacing error 
correction only for the test corpus. 
 Table 2: The word spacing error correction results 
 
The results of both word spacing error and spell-
ing error correction are shown in Table 3. Error 
containing test corpus (the blank characters are all 
deleted) was applied to this evaluation. 
 Table 3: The joint model results 
 
Table 4 shows the results of the same experi-
ment, without deleting the blank characters in the 
test corpus. The experiment shows that our joint 
model has a flexibility of utilizing already existing 
blanks (spacing) in the input sentence. 
 Table 4: The joint model results without deleting the 
exist spaces 
  
As shown above, the performance is dependent 
of the language model (n-gram) performance. Jaso 
transition probabilities can be obtained easily from 
small corpus because the number of Jaso is very 
small, under 100, in contrast with Eumjeol. 
 Using the existing blank information is also an 
important factor. If test sentences have no or few 
blank characters, then we simply use joint algo-
rithm to correct both errors. But when the test sen-
tences already have some blank characters, we can 
use the information since some of the spacing can 
be given by the user. By keeping the blank charac-
ters, we can get better accuracy because blank in-
sertion errors are generally fewer than the blank 
deletion errors in the corpus. 
5 Conclusions 
 We proposed a joint text preprocessing model 
that can correct both word spacing and spelling 
errors simultaneously for Korean. To our best 
knowledge, this is the first model which can handle 
inter-related errors between spacing and spelling in 
Korean. The usage and size of the word dictionar-
ies are decreased by using Jaso statistical prob-
abilities effectively. 
6 Acknowledgement 
This work was supported in part by MIC & IITA 
through IT Leading R&D Support Project. 
References 
 Jianfeng Gao, Mu Li and Chang-Ning Huang. 2003. 
Improved Source-Channel Models for Chinese Word 
Segmentation. Proceedings of the 41st Annual Meet-
ing of the ACL, pp. 272-279 
Seung-Shik Kang and Chong-Woo Woo. 2001. Auto-
matic Segmentation of Words Using Syllable Bigram 
Statistics. Proceedings of 6th Natural Language Proc-
essing Pacific Rim Symposium, pp. 729-732 
R. L Kashyap, B. J. Oommen. 1984. Spelling Correc-
tion Using Probabilistic Methods. Pattern Recogni-
tion Letters, pp. 147-154 
Oh-Wook Kwon. 2002. Korean Word Segmentation and 
Compound-noun Decomposition Using Markov 
Chain and Syllable N-gram. The Journal of the 
Acoustical Society of Korea, pp. 274-283.  
Mu Li, Muhua Zhu, Yang Zhang and Ming Zhou. 2006. 
Exploring Distributional Similarity Based Models for 
Query Spelling Correction. Proceedings of the 21st 
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pp. 1025-
1032 
Eric Mays, Fred J. Damerau and Robert L. Mercer. 
1991. Context Based Spelling Correction. IP&M, pp. 
517-522.  
Masaaki Nagata. 1996. Context-Based Spelling Correc-
tion for Japanese OCR. Proceedings of the 16th con-
ference on Computational Linguistics, pp. 806-811 
Christoper C. Yang and K. W. Li. 2005. A Heuristic 
Method Based on a Statistical Approach for Chinese 
Text Segmentation. Journal of the American Society 
for Information Science and Technology, pp. 1438-
1447. 
 n-gram A n-gram B 
Accuracy 91.03% 96.00% 
System n-gram A n-gram B
Basic joint model 88.34% 93.83%
System n-gram A n-gram B
Baseline 89.35% 89.35%
Basic joint model with keep-
ing the blank characters 90.35% 95.25%
64
Proceedings of ACL-08: HLT, pages 630?637,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Robust Dialog Management with N-best Hypotheses Using Dialog Examples
and Agenda
Cheongjae Lee, Sangkeun Jung and Gary Geunbae Lee
Pohang University of Science and Technology
Department of Computer Science and Engineering
Pohang, Republic of Korea
{lcj80,hugman,gblee}@postech.ac.kr
Abstract
This work presents an agenda-based approach
to improve the robustness of the dialog man-
ager by using dialog examples and n-best
recognition hypotheses. This approach sup-
ports n-best hypotheses in the dialog man-
ager and keeps track of the dialog state us-
ing a discourse interpretation algorithm with
the agenda graph and focus stack. Given
the agenda graph and n-best hypotheses, the
system can predict the next system actions
to maximize multi-level score functions. To
evaluate the proposed method, a spoken dia-
log system for a building guidance robot was
developed. Preliminary evaluation shows this
approach would be effective to improve the ro-
bustness of example-based dialog modeling.
1 Introduction
Development of spoken dialog systems involves hu-
man language technologies which must cooperate
to answer user queries. Since the performance in
human language technologies such as Automatic
Speech Recognition (ASR) and Natural Language
Understanding (NLU)1 have been improved, this ad-
vance has made it possible to develop spoken dialog
systems for many different application domains.
Nevertheless, there are major problems for practi-
cal spoken dialog systems. One of them which must
be considered by the Dialog Manager (DM) is the
error propagation from ASR and NLU modules. In
1Through this paper, we will use the term natural language
to include both spoken language and written language
general, errors in spoken dialog systems are preva-
lent due to errors in speech recognition or language
understanding. These errors can cause the dialog
system to misunderstand a user and in turn lead to
an inappropriate response. To avoid these errors, a
basic solution is to improve the accuracy and robust-
ness of the recognition and understanding processes.
However, it has been impossible to develop perfect
ASR and NLU modules because of noisy environ-
ments and unexpected input. Therefore, the devel-
opment of robust dialog management has also been
one of the most important goals in research on prac-
tical spoken dialog systems.
In the dialog manager, a popular method to deal
with these errors is to adopt dialog mechanisms for
detecting and repairing potential errors at the con-
versational level (McTear et al, 2005; Torres et al,
2005; Lee et al, 2007). In human-computer com-
munication, the goal of error recovery strategy is
to maximize the user?s satisfaction of using the sys-
tem by guiding for the repair of the wrong informa-
tion by human-computer interaction. On the other
hand, there are different approaches to improve the
robustness of dialog management using n-best hy-
potheses. Rather than Markov Decision Processes
(MDPs), partially observable MDPs (POMDPs) po-
tentially provide a much more powerful framework
for robust dialog modeling since they consider n-
best hypotheses to estimate the distribution of the
belief state (Williams and Young, 2007).
In recent, we proposed another data-driven ap-
proach for the dialog modeling called Example-
based Dialog Modeling (EBDM) (Lee et al, 2006a).
However, difficulties occur when attempting to de-
630
ploy EBDM in practical spoken dialog systems in
which ASR and NLU errors are frequent. Thus,
this paper proposes a new method to improve the ro-
bustness of the EBDM framework using an agenda-
based approach and n-best recognition hypotheses.
We consider a domain-specific agenda to estimate
the best dialog state and example because, in task-
oriented systems, a current dialog state is highly cor-
related to the previous dialog state. We have also
used the example-based error recovery approach to
handle exceptional cases due to noisy input or unex-
pected focus shift.
This paper is organized as follows. Previous re-
lated work is described in Section 2, followed by the
methodology and problems of the example-based di-
alog modeling in Section 3. An agenda-based ap-
proach for heuristics is presented in Section 4. Fol-
lowing that, we explain greedy selection with n-best
hypotheses in Section 5. Section 6 describes the
error recovery strategy to handle unexpected cases.
Then, Section 7 provides the experimental results of
a real user evaluation to verify our approach. Finally,
we draw conclusions and make suggestions for fu-
ture work in Section 8.
2 Related Work
In many spoken dialog systems that have been devel-
oped recently, various knowledge sources are used.
One of the knowledge sources, which are usually
application-dependent, is an agenda or task model.
These are powerful representations for segmenting
large tasks into more reasonable subtasks (Rich and
Sidner, 1998; Bohus and Rudnicky, 2003; Young et
al., 2007). These are manually designed for various
purposes including dialog modeling, search space
reduction, domain knowledge, and user simulation.
In Collagen (Rich and Sidner, 1998), a plan tree,
which is an approximate representation of a partial
SharedPlan, is composed of alternating act and plan
recipe nodes for internal discourse state representa-
tion and discourse interpretation.
In addition, Bohus and Rudnicky (2003) have pre-
sented a RavenClaw dialog management which is
an agenda-based architecture using hierarchical task
decomposition and an expectation agenda. For mod-
eling dialog, the domain-specific dialog control is
represented in the Dialog Task Specification layer
using a tree of dialog agents, with each agent han-
dling a certain subtask of the dialog task.
Recently, the problem of a large state space in
POMDP framework has been solved by grouping
states into partitions using user goal trees and on-
tology rules as heuristics (Young et al, 2007).
In this paper, we are interested in exploring algo-
rithms that would integrate this knowledge source
for users to achieve domain-specific goals. We used
an agenda graph whose hierarchy reflects the natu-
ral order of dialog control. This graph is used to both
keep track of the dialog state and to select the best
example using multiple recognition hypotheses for
augmenting previous EBDM framework.
3 Example-based Dialog Modeling
Our approach is implemented based on Example-
Based Dialog Modeling (EBDM) which is one of
generic dialog modelings. We begin with a brief
overview of the EBDM framework in this sec-
tion. EBDM was inspired by Example-Based Ma-
chine Translation (EBMT) (Nagao, 1984), a trans-
lation system in which the source sentence can be
translated using similar example fragments within a
large parallel corpus, without knowledge of the lan-
guage?s structure. The idea of EBMT can be ex-
tended to determine the next system actions by find-
ing similar dialog examples within the dialog cor-
pus. The system action can be predicted by finding
semantically similar user utterances with the dialog
state. The dialog state is defined as the set of relevant
internal variables that affect the next system action.
EBDM needs to automatically construct an example
database from the dialog corpus. Dialog Example
DataBase (DEDB) is semantically indexed to gen-
eralize the data in which the indexing keys can be
determined according to state variables chosen by
a system designer for domain-specific applications
(Figure 1). Each turn pair (user turn, system turn) in
the dialog corpus is mapped to semantic instances in
the DEDB. The index constraints represent the state
variables which are domain-independent attributes.
To determine the next system action, there are three
processes in the EBDM framework as follows:
? Query Generation: The dialog manager
makes Structured Query Language (SQL)
631
Figure 1: Indexing scheme for dialog example database on building guidance domain
statement using discourse history and NLU re-
sults.
? Example Search: The dialog manager
searches for semantically similar dialog exam-
ples in the DEDB given the current dialog state.
If no example is retrieved, some state variables
can be ignored by relaxing particular variables
according to the level of importance given the
dialog?s genre and domain.
? Example Selection: The dialog manager se-
lects the best example to maximize the ut-
terance similarity measure based on lexico-
semantic similarity and discourse history simi-
larity.
Figure 2 illustrates the overall strategy of EBDM
framework for spoken dialog systems. The EBDM
framework is a simple and powerful approach
to rapidly develop natural language interfaces for
multi-domain dialog processing (Lee et al, 2006b).
However, in the context of spoken dialog system for
domain-specific tasks, this framework must solve
two problems: (1) Keeping track of the dialog state
with a view to ensuring steady progress towards task
completion, (2) Supporting n-best recognition hy-
potheses to improve the robustness of dialog man-
ager. Consequently, we sought to solve these prob-
Figure 2: Strategy of the Example-Based Dialog
Modeling (EBDM) framework.
lems by integrating the agenda graph as a heuristic
which reflects the natural hierarchy and order of sub-
tasks needed to complete the task.
4 Agenda Graph
In this paper, agenda graph G is simply a way of
encoding the domain-specific dialog control to com-
plete the task. An agenda is one of the subtask flows,
which are possible paths from root node to terminal
node. G is composed of nodes (v) which correspond
to possible intermediate steps in the process of com-
pleting the specified task, and edges (e) which con-
632
Figure 3: Example of an agenda graph for a building
guidance.
nect nodes. In other words, v corresponds to user
goal state to achieve domain-specific subtask in its
expected agenda. Each node includes three different
components: (1) A precondition that must be true
before the subtask is executed; (2) A description of
the node that includes its label and identifier; and
(3) Links to nodes that will be executed at the subse-
quent turn. For every edge eij = (vi, vj), we defined
a transition probability based on prior knowledge of
dialog flows. This probability can be assigned based
on empirical analysis of human-computer conversa-
tions, assuming that the users behave in consistent,
goal-directed ways. Alternatively, it can be assigned
manually at the discretion of the system developer
to control the dialog flow. This heuristic has ad-
vantages for practical spoken dialog system because
a key condition for successful task-oriented dialog
system is that the user and system know which task
or subtask is currently being executed. To exem-
plify, Figure 3 illustrates part of the agenda graph for
PHOPE, a building guidance robot using the spoken
dialog system. In Figure 3, G is represented by a
Directed Acyclic Graph (DAG), where each link in
the graph reflects a transition between one user goal
state and the next. The set of paths in G represent
an agenda designed by the system developer. We
adapted DAG representation because it is more in-
tuitive and flexible than hierarchical tree represen-
tation. The syntax for graph representation in our
system is described by an XML schema (Figure 4).
4.1 Mapping Examples to Nodes
In the agenda graph G, each node v should hold
relevant dialog examples corresponding to user goal
states. Therefore, the dialog examples in DEDB are
Figure 4: XML description for the agenda graph
mapped to a user goal state when a precondition of
the node is true. Initially, the root node of the DAG is
the starting state, where there is no dialog example.
Then, the attributes of each dialog example are ex-
amined via the preconditions of each user goal node
by breadth-first traversal. If the precondition is true,
the node holds relevant that may appear in the user?s
goal state. The method of selecting the best of these
examples will be described in 5.
4.2 Discourse Interpretation
Inspired by Collagen (Rich and Sidner, 1998; Lesh
et al, 2001), we investigated a discourse interpre-
tation algorithm to consider how the current user?s
goal can contribute to the current agenda in a focus
stack according to Lochbaum?s discourse interpreta-
tion algorithm (Lochbaum, 1998). The focus stack
takes into account the discourse structure by keeping
track of discourse states. In our system, the focus
stack is a set of user goal nodes which lead to com-
pletion of the subtask. The top on the focus stack is
the previous node in this set. The focus stack is up-
dated after every utterance. To interpret the type of
the discourse state, this breaks down into five main
cases of possible current node for an observed user?s
goal:
? NEW TASK: Starting a new task to complete a
new agenda (Child of the root).
? NEW SUB TASK: Starting a new subtask to
partially shift focus (A different child of the
parent).
633
? NEXT TASK: Working on the next subtask con-
tributing to current agenda (Its child node).
? CURRENT TASK: Repeating or modifying the
observed goal on the current subtask (Current
node).
? PARENT TASK: Modifying the observation on
the previous subtask (Parent node).
Nodes in parentheses denote the topological position
of the current node relative to the top node on the
focus stack. If NEXT TASK is selected, the current
node is pushed to the focus stack. NEXT TASK cov-
ers totally focused behavior, i.e., when there are no
unexpected focus shifts. This occurs when the cur-
rent user utterance is highly correlated to the pre-
vious system utterance. The remaining four cases
cover various types of discourse state. For example,
NEW SUB TASK involves starting a new subtask to
partially shift focus, thereby popping the previous
goal off the focus stack and pushing a new user goal
for the new subtask. NEW TASK, which is placed
on the node linked to root node, involves starting a
new task to complete a new agenda. Therefore, a di-
alog is re-started and the current node is pushed onto
the focus stack with the current user goal as its first
element.
If none of the above cases holds, the discourse in-
terpretation concludes that the current input should
be rejected because we expect user utterances to be
correlated to the previous turn in a task-oriented do-
main. Therefore, this interpretation does not con-
tribute to the current agenda on the focus stack due
to ASR and NLU errors that are due to noisy envi-
ronments and unexpected input. These cases can be
handled by using an error recovery strategy in Sec-
tion 6.
Figure 5 shows some examples of pseudo-codes
used in the discourse interpretation algorithm to
select the best node among possible next nodes.
S,H ,and G denote the focus stack, hypothesis, and
agenda graph, respectively. The INTERPRET al-
gorithm is initially called to interpret the current dis-
course state. Furthermore, the essence of a discourse
interpretation algorithm is to find candidate nodes of
possible next subtask for an observed user goal, ex-
pressed in the definition of GENERATE. The SE-
LECT algorithm selects the best node to maximize
Figure 5: Pseudo-codes for the discourse interpreta-
tion algorithm
the score function based on current input and dis-
course structure given the focus stack. The details
of how the score of candidate nodes are calculated
are explained in Section 5.
5 Greedy Selection with n-best Hypotheses
Many speech recognizers can generate a list of plau-
sible hypotheses (n-best list) but output only the
most probable one. Examination of the n-best list
reveals that the best hypothesis, the one with the
lowest word error rate, is not always in top-1 posi-
tion but sometimes in the lower rank of the n-best
list. Therefore, we need to select the hypothesis
that maximizes the scoring function among a set of
n-best hypotheses of each utterance. The role of
agenda graph is for a heuristic to score the discourse
state to successfully complete the task given the fo-
cus stack.
The current system depends on a greedy policy
which is based on immediate transitions rather than
full transitions from the initial state. The greedy
selection with n-best hypotheses is implemented as
follows. Firstly, every hypothesis hi is scanned and
all possible nodes are generated using the discourse
interpretation. Secondly, the multi-level score func-
tions are computed for each candidate node ci given
a hypothesis hi. Using the greedy algorithm, the
node with the highest score is selected as the user
goal state. Finally, the system actions are predicted
by the dialog example to maximize the example
score in the best node.
The generation of candidate nodes is based
on multiple hypotheses from the previous EBDM
634
framework. This previous EBDM framework chose
a dialog example to maximize the utterance similar-
ity measure. However, our system generates a set of
multiple dialog examples with each utterance sim-
ilarity over a threshold given a specific hypothesis.
Then, the candidate nodes are generated by match-
ing to each dialog example bound to the node. If the
number of matching nodes is exactly one, that node
is selected. Otherwise, the best node which would
be pushed onto the focus stack must be selected us-
ing multi-level score functions.
5.1 Node Selection
The node selection is determined by calculating
some score functions. We defined multi-level score
functions that combine the scores of ASR, SLU, and
DM modules, which range from 0.00 to 1.00. The
best node is selected by greedy search with multiple
hypotheses H and candidate nodes C as follows:
c? = argmax
hi?H,ci?C
?SH(hi) + (1? ?)SD(ci|S)
where H is a list of n-best hypotheses and C is a
set of nodes to be generated by the discourse in-
terpretation. For the node selection, we divided the
score function into two functions SH(hi), hypothe-
sis score, and SD(ci|S), discourse score, where ci is
the focus node to be generated by single hypothesis
hi.
We defined the hypothesis score at the utterance
level as
SH(hi) = ?Srec(hi) + ?Scont(hi)
where Srec(hi) denotes the recognition score which
is a generalized confidence score over the confi-
dence score of the top-rank hypothesis. Scont(hi)
is the content score in the view of content manage-
ment to access domain-specific contents. For exam-
ple, in the building guidance domain, theses contents
would be a building knowledge database including
room name, room number, and room type. The score
is defined as:
Scont(hi) =
?
?
?
N(Chi )
N(Cprev) if Chi ? Cprev
N(Chi )
N(Ctotal) if Chi * Cprev
where Cprev is a set of contents at the previous turn
and Ctotal is a set of total contents in the content
database. Chi denotes a set of focused contents by
hypothesis hi at the current turn. N(C) represents
the number of contents C. This score reflects the
degree of content coherence because the number of
contents of interest has been gradually reduced with-
out any unexpected focus shift. In the hypothesis
score, ? and ? denote weights which depend on the
accuracy of speech recognition and language under-
standing, respectively.
In addition to the hypothesis score, we defined the
discourse score SD at the discourse level to consider
the discourse structure between the previous node
and current node given the focus stack S. This score
is the degree to which candidate node ci is in focus
with respect to the previous user goal and system ut-
terance. In the agenda graph G, each transition has
its own probability as prior knowledge. Therefore,
when ci is NEXT TASK, the discourse score is com-
puted as
SD(ci|S) = P (ci|c = top(S))
where P (ci|c = top(S)) is a transition probabil-
ity from the top node c on the focus stack S to the
candidate node ci. However, there is a problem for
cases other than NEXT TASK because the graph has
no backward probability. To solve this problem, we
assume that the transition probability may be lower
than that of the NEXT TASK case because a user
utterance is likely to be influenced by the previous
turn. Actually, when using the task-oriented dialog
system, typical users stay focused most of the time
during imperfect communication (Lesh et al, 2001).
To assign the backward transition probability, we
obtain the minimum transition probability Pmin(S)
among from the top node on the focus stack S to
its children. Then, the discourse score SD can be
formalized when the candidate node ci does not cor-
respond to NEXT TASK as follows:
SD(ci|S) = max{Pmin(S)? ?Dist(ci, c), 0}
where ? is a penalty of distance between candi-
date node and previous node, Dist(ci, c), according
to type of candidate node such as NEW TASK and
NEW SUB TASK. The simplest case is to uniformly
assign ? to a specific value.
To select the best node using the node score, we
use ? (0 ? ? ? 1) as an interpolation weight
635
between the hypothesis score Sh and the discourse
score SD. This weight is empirically assigned ac-
cording to the characteristics of the dialog genre and
task. For example, ? can set lower to manage the
transactional dialog in which the user utterance is
highly correlated to the previous system utterance,
i.e., a travel reservation task, because this task usu-
ally has preference orders to fill slots.
5.2 Example Selection
After selecting the best node, we use the example
score to select the best dialog example mapped into
this node.
e? = argmax
ej?E(c?)
?Sutter(h?, ej)+(1??)Ssem(h?, ej)
where h? is the best hypothesis to maximize the
node score and ej is a dialog example in the best
node c?. Sutter(h, ej) denotes the value of the utter-
ance similarity of the user?s utterances between the
hypothesis h and dialog example ej in the best node
c? (Lee et al, 2006a).
To augment the utterance similarity used in the
EBDM framework, we also defined the semantic
score for example selection, Ssem(h, ej):
Ssem(h, ej) = # of matching index keys# of total index keys
The semantic score is the ratio of matching index
keys to the number of total index keys between hy-
pothesis h and example record ej . This score re-
flects that a dialog example is semantically closer to
the current utterance if the example is selected with
more index keys. After processing of the node and
example selection, the best example is used to pre-
dict the system actions. Therefore, the dialog man-
ager can predict the next actions with the agenda
graph and n-best recognition hypotheses.
6 Error Recovery Strategy
As noted in Section 4.2, the discourse interpretation
sometimes fails to generate candidate nodes. In ad-
dition, the dialog manager should confirm the cur-
rent information when the score falls below some
threshold. For these cases, we adapt an example-
based error recovery strategy (Lee et al, 2007). In
this approach, the system detects that something is
wrong in the user?s utterance and takes immediate
steps to address the problem using some help mes-
sages such as UtterHelp, InfoHelp, and UsageHelp
in the example-based error recovery strategies. We
also added a new help message, AgendaHelp, that
uses the agenda graph and the label of each node to
tell the user which subtask to perform next such as
?SYSTEM: Next, you can do the subtask 1)Search
Location with Room Name or 2)Search Location
with Room Type?.
7 Experiment & Result
First we developed the spoken dialog system for
PHOPE in which an intelligent robot can provide in-
formation about buildings (i.e., room number, room
location, room name, room type) and people (i.e.,
name, phone number, e-mail address, cellular phone
number). If the user selects a specific room to visit,
then the robot takes the user to the desired room.
For this system, ten people used the WOZ method to
collect a dialog corpus of about 500 utterances from
100 dialogs which were based on a set of pre-defined
10 subjects relating to domain-specific tasks. Then,
we designed an agenda graph and integrated it into
the EBDM framework.
In an attempt to quantify the impact of our ap-
proach, five Korean users participated in a prelimi-
nary evaluation. We provided them with pre-defined
scenarios and asked them to collect test data from
50 dialogs, including about 150 utterances. After
processing each dialog, the participants completed
a questionnaire to assess their satisfaction with as-
pects of the performance evaluation. The speech
recognition hypotheses are obtained by using the
Hidden Markov model Toolkit (HTK) speech rec-
ognizer adapted to our application domain in which
the word error rate (WER) is 21.03%. The results of
the Task Completion Rate (TCR) are shown in Table
1. We explored the effects of our agenda-based ap-
proach with n-best hypotheses compared to the pre-
vious EBDM framework which has no agenda graph
and supports only 1-best hypothesis.
Note that using 10-best hypotheses and the
agenda graph increases the TCR from 84.0% to
90.0%, that is, 45 out of 50 dialogs were com-
pleted successfully. The average number of turns
(#AvgTurn) to completion was also shorter, which
636
shows 4.35 turns per a dialog using the agenda graph
and 10-best hypotheses. From these results, we con-
clude that the the use of the n-best hypotheses with
the agenda graph is helpful to improve the robust-
ness of the EBDM framework against noisy inputs.
System #AvgTurn TCR (%)
1-best(-AG) 4.65 84.0
10-best(+AG) 4.35 90.0
Table 1: Task completion rate according to using the
AG (Agenda Graph) and n-best hypotheses for n=1
and n=10.
8 Conclusion & Discussion
This paper has proposed a new agenda-based ap-
proach with n-best recognition hypotheses to im-
prove the robustness of the Example-based Dialog
Modeling (EBDM) framework. The agenda graph
can be thought of as a hidden cost of applying our
methodology. However, an explicit agenda is nec-
essary to successfully achieve the purpose of using
spoken dialog system. Our preliminary results indi-
cate this fact that the use of agenda graph as heuris-
tics can increase the TCR. In addition, our approach
is robust to recognition errors because it maintains
multiple hypotheses for each user utterance.
There are several possible subjects for further re-
search on our approach. First, the optimal interpo-
lation weights should be determined. This task will
require larger dialog corpora by using user simula-
tion. Second, the cost of designing the agenda graph
should be reduced. We have focused on developing a
system to construct this graph semi-automatically by
applying dialog state clustering and utterance clus-
tering to achieve hierarchical clustering of dialog ex-
amples. Finally, future work will include expanding
our system to other applications, such as navigation
systems for automobiles.
Acknowledgement
This work was supported by grant No. RTI04-02-06
from the Regional Technology Innovation Program
and by the Intelligent Robotics Development Pro-
gram, one of the 21st Century Frontier R&D Pro-
grams funded by the Ministry of Commerce, Indus-
try and Energy (MOICE) of Korea.
References
Bohus, B. and Rudnicky A. 2003. RavenClaw: Dia-
log Management Using Hierarchical Task Decompo-
sition and an Expectation Agenda. Proceedings of the
European Conference on Speech, Communication and
Technology, 597?600.
Grosz, B.J. and Kraus, S. 1996. Collaborative Plans
for Complex Group Action. Artificial Intelligence,
86(2):269?357.
Lee, C., Jung, S., Eun, J., Jeong, M., and Lee, G.G.
2006. A Situation-based Dialogue Management using
Dialogue Examples. Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, 69?72.
Lee, C., Jung, S., Jeong, M., and Lee, G.G. 2006.
Chat and Goal-oriented Dialog Together: A Unified
Example-based Architecture for Multi-domain Dialog
Management. Proceedings of the IEEE Spoken Lan-
guage Technology Workshop, 194-197.
Lee, C., Jung, S., and Lee, G.G. 2007. Example-based
Error Reocvery Strategy For Spoken Dialog System.
Proceedings of the IEEE Automatic Speech Recogni-
tion and Understanding Workshop, 538?543.
Lesh, N., Rich, C., and Sidner, C. 2001. Collaborat-
ing with focused and unfocused users under imper-
fect communication. Proceedings of the International
Conference on User Modeling, 63?74.
Lochbaum, K.E. 1998. A Collaborative Planning Model
of Intentional Structure. Computational Linguistics,
24(4):525?572.
McTear, M., O?Neil, I., Hanna, P., and Liu, X.
2005. Handling errors and determining confirmation
strategies-An object-based approach. Speech Commu-
nication, 45(3):249?269.
Nagao, M. 1984. A Frame Work of a Mechnical Trans-
latino between Japanese and English by Analogy Prin-
ciple. Proceedings of the international NATO sympo-
sium on artificial and human intelligence, 173?180.
Rich, C. and Sidner, C.. 1998. Collagen: A Collab-
oration Agent for Software Interface Agents. Jour-
nal of User Modeling and User-Adapted Interaction,
8(3):315?350.
Torres, F., Hurtado, L.F., Garcia, F., Sanchis, E., and
Segarra, E. 2005. Error Handling in a Stochastic
Dialog System through Confidence Measure. Speech
Communication, 45(3):211?229.
Williams, J.D. and Young, S. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech Language, 21(2):393-422.
Young, S., Schatzmann, J., Weilhammer, K., and Ye, H..
2007. The Hidden Information State Approach to Di-
alog Management. Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, 149?152.
637
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 17?20,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Hybrid Approach to User Intention Modeling for Dialog Simulation 
 
 
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science and Technology(POSTECH) 
{hugman, lcj80, getta, gblee}@postech.ac.kr 
  
Abstract 
This paper proposes a novel user intention si-
mulation method which is a data-driven ap-
proach but able to integrate diverse user dis-
course knowledge together to simulate various 
type of users. In Markov logic framework, lo-
gistic regression based data-driven user inten-
tion modeling is introduced, and human dialog 
knowledge are designed into two layers such 
as domain and discourse knowledge, then it is 
integrated with the data-driven model in gen-
eration time. Cooperative, corrective and self-
directing discourse knowledge are designed 
and integrated to mimic such type of users. 
Experiments were carried out to investigate 
the patterns of simulated users, and it turned 
out that our approach was successful to gener-
ate user intention patterns which are not only 
unseen in the training corpus and but also per-
sonalized in the designed direction.  
1 Introduction 
User simulation techniques are widely used for learn-
ing optimal dialog strategies in a statistical dialog 
management framework and for automated evaluation 
of spoken dialog systems. User simulation can be 
layered into the user intention level and user surface 
(utterance) level. This paper proposes a novel inten-
tion level user simulation technique.  
In recent years, a data-driven user intention model-
ing is widely used since it is domain- and language 
independent. However, the problem of data-driven 
user intention simulation is the limitation of user pat-
terns. Usually, the response patterns from data-driven 
simulated user tend to be limited to the training data. 
Therefore, it is not easy to simulate unseen user inten-
tion patterns, which is quite important to evaluate or 
learn optimal dialog policies. Another problem is poor 
user type controllability in a data-driven method. 
Sometimes, developers need to switch testers between 
various type of users such as cooperative, uncoopera-
tive or novice user and so on to expose their dialog 
system to various users. 
For this, we introduce a novel data-driven user in-
tention simulation method which is powered by hu-
man dialog knowledge in Markov logic formulation 
(Richardson and Domingos, 2006) to add diversity 
and controllability to data-driven intention simulation. 
2 Related work 
Data-driven intention modeling approach uses statis-
tical methods to generate the user intention given dis-
course information (history). The advantage of this 
approach lies in its simplicity and in that it is domain- 
and language independency. N-gram based approach-
es (Eckert et al, 1997, Levin et al, 2000) and other 
approaches (Scheffler and Young, 2001, Pietquin and 
Dutoit, 2006, Schatzmann et al, 2007) are  introduced. 
There has been some work on combining rules with 
statistical models especially for system side dialog 
management (Heeman, 2007, Henderson et al, 2008). 
However, little prior research has tried to use both 
knowledge and data-driven methods together in a sin-
gle framework especially for user intention simulation.  
In this research, we introduce a novel data-driven 
user intention modeling technique which can be di-
versified or personalized by integrating human dis-
course knowledge which is represented in first-order 
logic in a single framework. In the framework, di-
verse type of user knowledge can be easily designed 
and selectively integrated into data-driven user inten-
tion simulation. 
3 Overall architecture  
The overall architecture of our user simulator is 
shown in Fig. 1. The user intention simulator accepts 
the discourse circumstances with system intention as 
input and generates the next user intention. The user 
utterance simulator constructs a corresponding user 
sentence to express the given user intention. The si-
mulated user sentence is fed to the automatic speech 
recognition (ASR) channel simulator, which then adds 
noises to the utterance. The noisy utterance is passed 
to a dialog system which consists of spoken language 
understanding (SLU) and dialog management (DM) 
modules. In this research, the user utterance simulator 
and ASR channel simulator are developed using the 
method of  (Jung et al, 2009). 
17
 
4 Markov logic 
Markov logic is a probabilistic extension of finite 
first-order logic (Richardson and Domingos, 2006). A 
Markov Logic Network (MLN) combines first-order 
logic and probabilistic graphical models in a single 
representation.  
An MLN can be viewed as a template for construct-
ing Markov networks. From the above definition, the 
probability distribution over possible worlds x speci-
fied by the ground Markov network is given by  
 
 
 
where F is the number  of formulas in the MLN and 
ni(x) is the number of true groundings of Fi in x. As 
formula weights increase, an MLN increasingly re-
sembles a purely logical KB, becoming equivalent to 
one in the limit of all infinite weights. General algo-
rithms for inference and learning in Markov logic are 
discussed in (Richardson and Domingos, 2006). 
Since Markov logic is a first-order knowledge base 
with a weight attached to each formula, it provides a 
theoretically fine framework integrating a statistically 
learned model with logically designed and inducted 
human knowledge. So the framework can be used for 
building up a hybrid user modeling with the advan-
tages of knowledge-based and data-driven models.  
5 User intention modeling in Markov 
logic 
The task of user intention simulation is to generate 
subsequent user intentions given current discourse 
circumstances. Therefore, user intention simulation 
can be formulated in the probabilistic form 
P(userIntention | context).  
In this research, we define the user intention state 
userIntention = [dialog_act, main_goal, compo-
nent_slot], where dialog_act is a domain-independent 
label of an utterance at the level of illocutionary force 
(e.g. statement, request, wh_question) and main_goal 
is the domain-specific user goal of an utterance (e.g. 
give_something, tell_purpose). Component slots 
represent domain-specific named-entities in the utter-
ance. For example, in the user intention state for the 
utterance ?I want to go to city hall? (Fig. 2), the com-
bination of each slot of semantic frame represents the 
user intention symbol. In this example, the state sym-
bol is ?request+search_loc+[loc_name]?. Dialogs on 
car navigation deal with support for the information 
and selection of the desired destination. 
The first-order language-based predicates which 
are related with discourse context information and 
with generating the next user intention are as follows: 
 
For example, after the following fragment of dialog 
for the car navigation domain,  
 
the discourse context which is passed to the user si-
mulator is illustrated in Fig. 3. 
Notice that the context information is composed of 
semantic frame (SF), discourse history (DH) and pre-
vious system intention (SI). ?isFilledComponent? 
predicate indicates which component slots are filled 
during the discourse.  ?updatedEntity? predicate is 
true if the corresponding named entity is newly up-
dated. ?hasSystemAct? and ?hasSystemActAttr? 
predicate represent previous system intention and 
mentioned attributes.  
 
 
SF 
hasIntention(?ct_01?, ?request+search_loc+loc_name?) 
hasDialogAct(?ct_01?,?wh_question?) 
hasMainGoal(?ct_01?, ?search_loc?) 
hasEntity(?ct_01?, ?loc_keyword?) 
DH 
isFilledComponent(?ct_01?, ?loc_keyword) 
!isFilledComponent(?ct_01?, ?loc_address) 
!isFilledComponent(?ct_01?, ?loc_name?) 
!isFilledComponent(?ct_01?, ?route_type?) 
updatedEntity(?ct_01?, ?loc_keyword?) 
SI 
hasNumDBResult(?ct_01?, ?many?) 
hasSystemAct(?ct_01?, ?inform?) 
hasSystemActAttr(?ct_01?, ?address,name?) 
Fig. 3 Example of discourse context in car navigation domain. 
SF=Semantic Frame, DH=Discourse History, SI=System Inten-
tion. 
raw user utterance I want to go to city hall. 
dialog_act request 
main_goal search_loc 
component.[loc_name] cityhall 
Fig. 2 Semantic frame for user intention simulation on 
car navigation domain. 
 
Fig. 1 Overall architecture of dialog simulation  
User(01) : Where are Chinese restaurants? 
// dialog_act=wh_question 
// main_goal=search_loc 
// named_entity[loc_keyword]=Chinese_restaurant 
Sys(01) : There are Buchunsung and Idongbanjum in 
Daeidong. 
// system_act=inform 
// target_action_attribute=name,address 
? User intention simulation related  predicates 
 GenerateUserIntention(context,userIntention) 
? Discourse context related predicates 
 hasIntention(context, userIntention) 
 hasDialogAct(context, dialogAct) 
 hasMainGoal(context, mainGoal) 
 hasEntity(context, entity) 
 isFilledComponent(context,entity) 
 updatedEntity(contetx, entity) 
 hasNumDBResult(context, numDBResult) 
 hasSystemAct(context, systemAct) 
 hasSystemActAttr(context, sytemActAttr) 
  isSubTask(context, subTask) 
 
1
1( ) exp( ( ))F i iiP X x w n xZ ?? ? ?
 
18
5.1 Data-driven user intention modeling in 
Markov logic 
The formulas are defined between the predicates 
which are related with discourse context information 
and corresponding user intention. The formulas for 
user intention modeling based on logistic regression 
are as follows: 
?ct, pui, ui hasIntention(ct, pui)1   
=>  GenerateUserIntention(ct, ui) 
?ct, da, ui hasDialogAct(ct, da) => GenerateUserIntention(ct,ui) 
?ct, mg, ui hasMainGoal(ct, mg) => GenerateUserIntention(ct,ui) 
?ct, en, ui hasEntity(ct, en) =>GenerateUserIntention(ct,ui) 
?ct, en, ui isFilledComponent(ct,en) 
=> GenerateUserIntention(ct,ui)  
?ct, en, ui updatedEntity(ct, en) => GenerateUserIntention(ct,ui) 
?ct, dbr, ui hasNumDBResult(ct, dbr)  
=> GenerateUserIntention(ct, ui) 
?ct, sa, ui hasSystemAct(ct, sa) =>GenerateUserIntention(ct, ui) 
?ct, attr, ui hasSystemActAttr(ct, attr) 
       =>  GenerateUserIntention(ct, ui) 
The weights of each formula are estimated from 
the data which contains the evidence (context) and 
corresponding user intention of next turn (userInten-
tion). 
5.2 User knowledge 
In this research, the user knowledge, which is used for 
deciding user intention given discourse context, is 
layered into two levels: domain knowledge and dis-
course knowledge. Domain- specific and ?dependent 
knowledge is described in domain knowledge. Dis-
course knowledge is more general and abstracted 
knowledge. It uses the domain knowledge as base 
knowledge. The subtask which is one of domain 
knowledge are defined as follows 
 
?isSubTask? implies which subtask corresponds 
to the current context. ?subTaskHasIntention? 
describes which subtask has which user intention. 
?moveTo? predicate implies the connection from sub-
task to subtask node. 
Cooperative, corrective and self-directing discourse 
knowledge is represented in Markov logic to mimic 
following users.  
? Cooperative User: A user who is cooperative with a 
system by answering what the system asked.  
? Corrective User: A user who try to correct the mis-
behavior of system by jumping to or repeating spe-
cific subtask. 
? Self-directing User: A user who tries to say what 
he/she want to without considering system?s sugges-
tion.  
Examples of discourse knowledge description for 
three types of user are shown in Fig. 4. 
                                                 
1 ct: context, ui: user intention, pui: previous user intention, da: 
dialog act, mg: main goal, en: entity, dbr:DB result, sa: system 
action, attr: target attribute of system action 
Both the formulas from data-driven model and 
formulas from discourse knowledge are used for con-
structing MLN in generation time. 
In inference, the discourse context related predi-
cates are given to MLN as true, then probabilities of 
predicate ?GenerateUserIntention? over candi-
date user intention are calculated. One of example 
evidence predicates was shown in Fig. 3. All of the 
predicates of Fig. 3 are given to MLN as true. From 
the network, the probability of P(userIntention | con-
text) is calculated. 
 
 
6 Experiments 
137 dialog examples from a real user and a dialog 
system in the car navigation domain were used to 
train the data-driven user intention simulator. The 
SLU and DM are built in the same way of (Jung et al, 
2009). After the training, simulations collected 1000 
dialog samples at each word error rate (WER) setting 
(WER=0 to 40%). The simulator model can be varied 
according to the combination of knowledge. We can 
generate eight different simulated users from A to H 
as Fig. 5. 
The overall trend of simulated dialogs are ex-
amined by defining an average score function similar 
to the reward score commonly used in reinforcement 
learning-based dialog systems for measuring both a 
cost and task success. We give 20 points for the suc-
cessful dialog state and penalize 1 point for each ac-
tion performed by the user to penalize longer dialogs.  
 A B C D E F G H 
Statistical model (S) O O O O O O O O 
Cooperative(CPR)  O   O O  O 
Corrective(COR)   O  O  O O 
Self-directing(SFD)    O  O O O 
Fig. 5 Eight different users (A to H) according to the 
combination of knowledge.  
? Subtask related predicates 
 subTaskHasIntention(subTask,userIntetion) 
 moveTo(subtask, subTask) 
 isCompletedSubTask (context, subTask) 
 isSubtask(context,subTask) 
 
Cooperative Knoweldge 
 // If system asks to specify an address explicitly, coop-
erative users would specify the address by jumping to 
the address setting subtask. 
? ct, st  isSubTask(ct, st) ^  
hasSytemAct(ct, ?specify?) ^ 
          hasSystemActAttr(ct, ?address?) 
           => moveTo(st, ?AddressSetting?) 
Corrective Knowledge 
 // If the current subtask fails, corrective users would 
repeat current subtask. 
? ct, st isSubTask(ct, st)^  
? isCompletedSubTask(ct, st) ^  
subTaskHasIntention(st, ui)  
=> GenerateUserIntention(ct,ui) 
Self-directing Knowledge 
 // Self-directing users do not make an utterance which 
is not relevant with the next subtask in their knowledge. 
? ct, st  isSubTask(ct, st) ^  
? moveTo(st, nt) ^ 
           subTaskHasIntention(nt, ui) 
 => ? GenerateUserIntention(ct, ui) 
Fig. 4 Example of cooperative, corrective and self-
directing discourse knowledge.  
19
Fig. 6 shows that simulated user C which has cor-
rective knowledge with statistical model show signifi-
cantly different trend over the most of word error rate 
settings. For the cooperative user (B), the difference is 
not as large and not statistically significant. It can be 
analyzed that the cooperative user behaviors are rela-
tively common patterns in human-machine dialog 
corpus. So, these behaviors can be already learned in 
statistical model (A).  
Using more than two type of knowledge together 
shows interesting result. Using cooperative know-
ledge with corrective knowledge together (E) shows 
much different result than using each knowledge 
alone (B and C). In the case of using self-directing 
knowledge with cooperative knowledge (F), the aver-
age scores are partially increased against base line 
scores. However, using corrective knowledge with 
self-directing knowledge does not show different re-
sult.  It can be thought that the corrective knowledge 
and self-directing knowledge are working as contra-
dictory policy in deciding user intention. Three dis-
course knowledge combined user shows very interest-
ing result. H shows much higher improvement over 
all simulated users, and the differences are significant 
results at p ? 0.001.  
To verify the proposed user simulation method can 
simulate the unseen events, the unseen rates of units 
were calculated. Fig. 7 shows the unseen unit rates of 
intention sequence. The unseen rate of n-gram varies 
according to the simulated user. Notice that simulated 
user C, E and H generates higher unseen n-gram pat-
terns over all word error settings. These users com-
monly have corrective knowledge, and the patterns 
seem to not be present in the corpus. But the unseen 
patterns do not mean poor intention simulation. High-
er task completion rate of C, E and H imply that these 
users actually generate corrective user response to 
make a successful conversation. 
7 Conclusion 
This paper presented a novel user intention simulation 
method which is a data-driven approach but able to 
integrate diverse user discourse knowledge together to 
simulate various type of user.  A logistic regression 
model is used for the statistical user intention model 
in Markov logic. Human dialog knowledge is sepa-
rated into domain and discourse knowledge, and co-
operative, corrective and self-directing discourse 
knowledge are designed to mimic such type user. The 
experiment results show that the proposed user inten-
tion simulation framework actually generates natural 
and diverse user intention patterns what the developer 
intended.  
Acknowledgments 
This research was supported by the MKE (Ministry of 
Knowledge Economy), Korea, under the 
ITRC(Information Technology Research Center) sup-
port program supervised by the IITA(Institute for In-
formation Technology Advancement) (IITA-2009-
C1090-0902-0045). 
 
 
References  
Eckert, W., Levin, E. and Pieraccini, R. 1997. User model-
ing for spoken dialogue system evaluation. Automatic 
Speech Recognition and Understanding:80-87. 
Heeman, P. 2007. Combining reinforcement learning with 
information-state update rules. NAACL. 
Henderson, J., Lemon, O. and Georgila, K. 2008. Hybrid 
reinforcement/supervised learning of dialogue policies 
from fixed data sets. Comput. Linguist., 34(4):487-511. 
Jung, S., Lee, C., Kim, K. and Lee, G.G. 2009. Data-driven 
user simulation for automated evaluation of spoken dialog 
systems. Computer Speech & Lan-
guage.doi:10.1016/j.csl.2009.03.002. 
Levin, E., Pieraccini, R. and Eckert, W. 2000. A stochastic 
model of human-machine interaction for learning dialog-
strategies. IEEE Transactions on Speech and Audio 
Processing, 8(1):11-23. 
Pietquin, O. and Dutoit, T. 2006. A Probabilistic Frame-
work for Dialog Simulation and Optimal Strategy Learn-
ing. IEEE Transactions on Audio, Speech and Language 
Processing, 14(2):589-599. 
Richardson, M. and Domingos, P. 2006. Markov logic net-
works. Machine Learning, 62(1):107-136. 
Schatzmann, J., Thomson, B. and Young, S. 2007. Statistic-
al User Simulation with a Hidden Agenda. SIGDial. 
Scheffler, K. and Young, S. 2001. Corpus-based dialogue 
simulation for automatic strategy learning and evaluation. 
NAACL Workshop on Adaptation in Dialogue Sys-
tems:64-70. 
 
Fig. 7 Unseen user intention sequence rate and task com-
pletion rate over simulated users at word error rate of 10. 
WER(%) 
model  
0 10 20 30 40 
A:S (base line) 
14.22 
(0.00) 
9.13 
(0.00) 
5.55 
(0.00) 
1.33 
(0.00) 
-1.16 
(0.00) 
B:S+CPR 
14.39 
(0.17) 
9.78 
(0.65) 
5.38 
(-0.17) 
2.32? 
(0.99) 
-1.00 
(0.16) 
C:S+COR 
14.61? 
(0.40) 
10.91
?
 
(1.78) 
7.28
?
 
(1.74) 
2.62? 
(1.30) 
-0.81 
(0.35) 
D:S+SFD 
15.70
?
 
(1.48) 
10.10? 
(0.97) 
5.51 
(-0.04) 
1.89 
(0.56) 
-0.96
?
 
(0.20) 
E:S+CPR+COR 
14.75? 
(0.53) 
10.93
?
 
(1.79) 
6.88? 
(1.33) 
2.94
?
 
(1.61) 
-1.06? 
(0.11) 
F:S+CPR+SFD 
15.75
?
 
(1.54) 
10.16? 
(1.02) 
5.80 
(0.26) 
1.88 
(0.56) 
-0.03? 
(1.13) 
G:S+COR+SFD 
14.39 
(0.17) 
9.18 
(0.05) 
5.04 
(-0.50) 
1.63 
(0.31) 
-1.52 
(-0.36) 
H:S+CPR+COR+SFD 15.70
?
 
(1.48) 
12.19
?
 
(3.05) 
9.20
?
 
(3.65) 
5.12
?
 
(3.80) 
1.32
?
 
(2.48) 
Fig. 6 Average scores of user intention models over used discourse 
knowledge. The relative improvements against statistical models 
are described between parentheses. Bold cells indicate the im-
provements are higher than 1.0.  
? : significantly different from the base line, p = 0.05,  
? : significantly different from the base line, p = 0.01,  
?
 : significantly different from the base line, p ? 0.001 
20
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 81?84,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Realistic Grammar Error Simulation using Markov Logic 
 
Sungjin Lee 
Pohang University of Science and 
Technology 
Pohang, Korea 
junion@postech.ac.kr  
Gary Geunbae Lee 
Pohang University of Science and 
Technology 
Pohang, Korea 
gblee@postech.ac.kr 
 
  
 
Abstract 
The development of Dialog-Based Computer-
Assisted Language Learning (DB-CALL) sys-
tems requires research on the simulation of 
language learners. This paper presents a new 
method for generation of grammar errors, an 
important part of the language learner simula-
tor. Realistic errors are generated via Markov 
Logic, which provides an effective way to 
merge a statistical approach with expert know-
ledge about the grammar error characteristics 
of language learners. Results suggest that the 
distribution of simulated grammar errors gen-
erated by the proposed model is similar to that 
of real learners. Human judges also gave con-
sistently close judgments on the quality of the 
real and simulated grammar errors. 
1 Introduction 
Second Language Acquisition (SLA) researchers 
have claimed that feedback provided during con-
versational interaction facilitates the acquisition 
process. Thus, interest in developing Dialog-
Based Computer Assisted Language Learning 
(DB-CALL) systems is rapidly increasing. How-
ever, developing DB-CALL systems takes a long 
time and entails a high cost in collecting learnerV? 
data. Also, evaluating the systems is not a trivial 
task because it requires numerous language 
learners with a wide range of proficiency levels 
as subjects.  
While previous studies have considered user 
simulation in the development and evaluation of 
spoken dialog systems (Schatzmann et al, 2006), 
they have not yet simulated grammar errors be-
cause those systems were assumed to be used by 
native speakers, who normally produce few 
grammar errors in utterances. However, as tele-
phone-based information access systems become 
more commonly available to the general public, 
the inability to deal with non-native speakers is 
becoming a serious limitation since, at least for 
some applications, (e.g. tourist information, le-
gal/social advice) non-native speakers represent 
a significant portion of the everyday user popula-
tion. Thus, (Raux and Eskenazi, 2004) conducted 
a study on adaptation of spoken dialog systems 
to non-native users. In particular, DB-CALL sys-
tems should obviously deal with grammar errors 
because language learners naturally commit nu-
merous grammar errors. Thus grammar error si-
mulation should be embedded in the user simula-
tion for the development and evaluation of such 
systems. 
In Foster?s (2007) pioneering work, she de-
scribed a procedure which automatically intro-
duces frequently occurring grammatical errors 
into sentences to make ungrammatical training 
data for a robust parser. However the algorithm 
cannot be directly applied to grammar error gen-
eration for language learner simulation for sever-
al reasons. First, it either introduces one error per 
sentence or none, regardless of how many words 
of the sentence are likely to generate errors. 
Second, it determines which type of error it will 
create only by relying on the relative frequencies 
of error types and their relevant parts of speech. 
This, however, can result in unrealistic errors. As 
exemplified in Table 1, when the algorithm tries 
to create an error by deleting a word, it would 
probably omit the word ?go? because verb is one 
of the most frequent parts of speech omitted re-
sulting in an unrealistic error like the first simu-
lated output. However, Korean/Japanese lan-
guage learners of English tend to make subject-
verb agreement errors, omission errors of the 
preposition of prepositional verbs, and omission 
errors of articles because their first language 
does not have similar grammar rules so that they 
may be slow on the uptake of such constructs. 
Thus, they often commit errors like the second 
simulated output.  
81
This paper develops an approach to statistical 
grammar error simulation that can incorporate 
this type of knowledge about language learners? 
error characteristics and shows that it does in-
deed result in realistic grammar errors. The ap-
proach is based on Markov logic, a representa-
tion language that combines probabilistic graphi-
cal models and first-order logic (Richardson and 
Domingos, 2006). Markov logic enables concise 
specification of very complex models. Efficient 
open-source Markov logic learning and inference 
algorithms were used to implement our solution.  
We begin by describing the overall process of 
grammar error simulation and then briefly re-
viewing the necessary background in Markov 
logic. We then describe our Markov Logic Net-
work (MLN) for grammar error simulation. Fi-
nally, we present our experiments and results. 
2 Overall process of grammar error si-
mulation 
The task of grammar error simulation is to gen-
erate an ill-formed sentence when given a well-
formed input sentence. The generation procedure 
involves three steps: 1) Generating probability 
over error types for each word of the well-
formed input sentence through MLN inference 2) 
Determining an error type by sampling the gen-
erated probability for each word 3) Creating an 
ill-formed output sentence by realizing the cho-
sen error types (Figure 1).  
3 Markov Logic 
Markov logic is a probabilistic extension of finite 
first-order logic (Richardson and Domingos, 
2006). An MLN is a set of weighted first-order 
clauses. Together with a set of constants, it de-
fines a Markov network with one node per 
ground atom and one feature per ground clause. 
The weight of a feature is the weight of the first-
order clause that originated it. The probability of 
a state x in such a network is given by ?(?)  =
 (1/?) ??? (? ?? ?? (?)? ), where ?  is a normali-
zation constant, ??  is the weight of the ?th clause, ??  =  1 if the ?th clause is true, and ??  =  0 oth-
erwise.  
Markov logic makes it possible to compactly 
specify probability distributions over complex 
relational domains. We used the learning and 
inference algorithms provided in the open-source 
Alchemy package (Kok et al, 2006).  In particu-
lar, we performed inference using the belief 
propagation algorithm (Pearl, 1988), and genera-
tive weight learning. 
4 An MLN for Grammar Error Simula-
tion 
This section presents our MLN implementation 
which consists of three components: 1) Basic 
formulas based on parts of speech, which are 
comparable to Foster?s method 2) Analytic for-
mulas drawn from expert knowledge obtained by 
error analysis on a learner corpus 3) Error limit-
ing formulas that penalize statistical model?s 
over-generation of nonsense errors.  
4.1 Basic formulas 
Error patterns obtained by error analysis, which 
might capture a lack or an over-generalization of 
knowledge of a particular construction, cannot 
explain every error that learners commit. Be-
cause an error can take the form of a perfor-
mance slip which can randomly occur due to 
carelessness or tiredness, more general formulas 
are needed as a default case. The basic formulas 
are represented by the simple rule: 
y ????????, ?, +??? ? ?)?????????, ?, +??) 
where all free variables are implicitly universally 
quantified. The ?+??, +??? notation signifies 
that the MLN contains an instance of this rule for 
each (part of speech, error type) pair. The evi-
Input sentence 
He wants to go to a movie theater 
Unrealistic simulated output 
He wants to to a movie theater 
Realistic simulated output 
He want go to movie theater 
Table 1: Examples of simulated outputs  
Figure 1: An example process of grammar error simulation 
82
dence predicate in this case is ??)?????, ?, ??), 
which is true iff the ?th position of the sentence ? 
has the part of speech ??. The query predicate is ?)?????????, ?, ??). It is true iff the ?th position 
of the sentence ? has the error type ??, and infer-
ring it returns the probability that the word at 
position ? would commit an error of type ??.  
4.2 Analytic formulas 
On top of the basic formulas, analytic formulas 
add concrete knowledge of realistic error charac-
teristics of language learners. Error analysis and 
linguistic differences between the first language 
and the second language can identify various 
error sources for each error type. We roughly 
categorize the error sources into three groups for 
explanation: 1) Over-generalization of the rules 
of the second language 2) Lack of knowledge of 
some rules of the second language 3) Applying 
rules and forms of the first language into the 
second language. 
Often, English learners commit pluralization 
error with irregular nouns. This is because they 
over-generalize the pluralization rule, i.e. attach-
ing ?s/es?, so that they apply the rule even to ir-
regular  nouns such  as ?fish? and ?feet? etc. This 
characteristic is captured by the simple formula: 
y ?????????????????????, ?? ? ????????, ?, ???? 
? ?)?????????, ?, ?_?????_?) 
where ?????????????????????, ?? is true iff the ?th word of the sentence ? is an irregular plural 
and N_NUM_SUB is the abbreviation for substi-
tution by noun number error.  
One trivial error caused by a lack of know-
ledge of the second language is using the singu-
lar noun form for weekly events: 
y ??????, ? ? 1, ???  ? ?????????, ??? ????????, ?, ????? ?)?????????, ?, ?_?????_?) 
where ??????, ? ? 1, ??? is true iff the ? ? 1th 
word is ?on? and ?????????, ?? is true iff the ?th word of the sentence ? is a noun describing 
day like Sunday(s). Another example is use of 
plurals behind ?every? due to the ignorance that a 
noun modified by ?every? should be singular: 
y ??????, ??, ?????? ? ???????????????, ??, ??? 
? ?)?????????, ??, ?_?????_?) 
where ???????????????, ??, ???  is true iff the ??th word is the determiner of the ??th word. 
An example of errors by applying the rules of 
the first language is that Korean/Japanese often 
allows omission of the subject of a sentence; thus, 
they easily commit the subject omission error. 
The following formula is for the case: 
y ?????????, ??? ?)?????????, ?,?_???_???) 
where ?????????, ?? is true iff the ?th word is the 
subject and N_LXC_DEL is the abbreviation for 
deletion by noun lexis error.1 
4.3 Error limiting formulas 
A number of elementary formulas explicitly 
stated as hard formulas prevent the MLN from 
generating improbable errors that might result 
from over-generations of the statistical model. 
For example, a verb complement error should not 
have a probability at the words that are not com-
plements of a verb: 
y ! ????????????????, ??, ??? 
? ! ?)?????????, ??, ?_???_???). 
where ?!? denotes logically ?not? and ?.? at the 
end signifies that it is a hard formula. Hard formu-
las are given maximum weight during inference. ????????????????, ??, ???  is true iff the ?? th 
word is a complement of the verb at the ??th po-
sition and V_CMP_SUB is the abbreviation for 
substitution by verb complement error. 
5 Experiments  
Experiments used the NICT JLE Corpus, which 
is speech samples from an English oral profi-
ciency interview test, the ACTFL-ALC Standard 
Speaking Test (SST). 167 of the files are error 
annotated. The error tagset consists of 47 tags 
that are described in Izumi (2005). We appended 
structural type of errors (substitution, addition, 
deletion) to the original error types because 
structural type should be determined when creat-
ing an error. For example, V_TNS_SUB consists 
of the original error type V_TNS (verb tense) and 
structural type SUB (substitution).  Level-
specific language learner simulation was accom-
plished by dividing the 167 error annotated files 
into 3 level groups: Beginner(level1-4), Interme-
diate(level5-6), Advanced(level7-9).  
The grammar error simulation was compared 
with real learnerV? errors and the baseline model 
using only basic formulas comparable to Foster?s 
algorithm, with 10-fold cross validations per-
formed for each group. The validation results 
were added together across the rounds to com-
pare the number of simulated errors with the 
number of real errors. Error types that occurred 
less than 20 times were excluded to improve re-
liability. Result graphs suggest that the distribu-
tion of simulated grammar errors generated by 
the proposed model using all formulas is similar 
to that of real learners for all level groups and the 
                                                 
1
 Because space is limited, all formulas can be found at 
http://isoft.postech.ac.kr/ges/grm_err_sim.mln 
83
proposed model outperforms the baseline model 
using only the basic formulas. The Kullback-
Leibler divergences, a measure of the difference 
between two probability distributions, were also 
measured for quantitative comparison. For all 
level groups, the Kullback-Leibler divergence of 
the proposed model from the real is less than that 
of the baseline model (Figure 2). 
Two human judges verified the overall realism 
of the simulated errors. They evaluated 100 ran-
domly chosen sentences consisting of 50 sen-
tences each from the real and simulated data. The 
sequence of the test sentences was mixed so that 
the human judges did not know whether the 
source of the sentence was real or simulated. 
They evaluated sentences with a two-level scale 
(0: Unrealistic, 1: Realistic). The result shows 
that the inter evaluator agreement (kappa) is 
moderate and that both judges gave relatively 
close judgments on the quality of the real and 
simulated data (Table 2). 
6 Summary and Future Work  
This paper introduced a somewhat new research 
topic, grammar error simulation. Expert know-
ledge of error characteristics was imported to 
statistical modeling using Markov logic, which 
provides a theoretically sound way of encoding 
knowledge into probabilistic first order logic. 
Results indicate that our method can make an 
error distribution more similar to the real error 
distribution than the baseline and that the quality 
of simulated sentences is relatively close to that 
of real sentences in the judgment of human eva-
luators. Our future work includes adding more 
expert knowledge through error analysis to in-
crementally improve the performance. Further-
more, actual development and evaluation of a 
DB-CALL system will be arranged so that we 
may investigate how much the cost of collecting 
data and evaluation would be reduced by using 
language learner simulation. 
Acknowledgement  
This research was supported by the MKE (Ministry of 
Knowledge Economy), Korea, under the ITRC (In-
formation Technology Research Center) support pro-
gram supervised by the IITA (Institute for Informa-
tion Technology Advancement) (IITA-2009-C1090-
0902-0045). 
References  
Foster, J. 2007. Treebanks Gone Bad: Parser evalua-
tion and retraining using a treebank of ungrammat-
ical sentences. IJDAR, 10(3-4), 129-145. 
Izumi, E et al 2005. Error Annotation for Corpus of 
Japanese Learner English. In Proc. International 
Workshop on Linguistically Interpreted Corpora  
Kok, S. et al 2006. The Alchemy system for statistic-
al relational AI. http://alchemy.cs.washington.edu/. 
Pearl, J. 1988. Probabilistic Reasoning in Intelligent 
Systems Morgan Kaufmann. 
Raux, A. and Eskenazi, M. 2004. Non-Native Users in 
the Let's Go!! Spoken Dialogue System: Dealing 
with Linguistic Mismatch, HLT/NAACL. 
Richardson, M. and Domingos, P. 2006. Markov logic 
networks. Machine Learning, 62(1):107-136. 
Schatzmann, J. et al 2006. A survey of statistical user 
simulation techniques for reinforcement-learning 
of dialogue management strategies, The Know-
ledge Engineering ReviewVolProceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 281?284,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Efficient Inference of CRFs for Large-Scale Natural Language Data
Minwoo Jeong
??
Chin-Yew Lin
?
Gary Geunbae Lee
?
?
Pohang University of Science & Technology, Pohang, Korea
?
Microsoft Research Asia, Beijing, China
?
{stardust,gblee}@postech.ac.kr
?
cyl@microsoft.com
Abstract
This paper presents an efficient inference algo-
rithm of conditional random fields (CRFs) for
large-scale data. Our key idea is to decompose
the output label state into an active set and an
inactive set in which most unsupported tran-
sitions become a constant. Our method uni-
fies two previous methods for efficient infer-
ence of CRFs, and also derives a simple but
robust special case that performs faster than
exact inference when the active sets are suffi-
ciently small. We demonstrate that our method
achieves dramatic speedup on six standard nat-
ural language processing problems.
1 Introduction
Conditional random fields (CRFs) are widely used in
natural language processing, but extending them to
large-scale problems remains a significant challenge.
For simple graphical structures (e.g. linear-chain), an
exact inference can be obtained efficiently if the num-
ber of output labels is not large. However, for large
number of output labels, the inference is often pro-
hibitively expensive.
To alleviate this problem, researchers have begun to
study the methods of increasing inference speeds of
CRFs. Pal et al (2006) proposed a Sparse Forward-
Backward (SFB) algorithm, in which marginal distribu-
tion is compressed by approximating the true marginals
using Kullback-Leibler (KL) divergence. Cohn (2006)
proposed a Tied Potential (TP) algorithm which con-
strains the labeling considered in each feature function,
such that the functions can detect only a relatively small
set of labels. Both of these techniques efficiently com-
pute the marginals with a significantly reduced runtime,
resulting in faster training and decoding of CRFs.
This paper presents an efficient inference algorithm
of CRFs which unifies the SFB and TP approaches. We
first decompose output labels states into active and in-
active sets. Then, the active set is selected by feasible
heuristics and the parameters of the inactive set are held
a constant. The idea behind our method is that not all
of the states contribute to the marginals, that is, only a
?
Parts of this work were conducted during the author?s
internship at Microsoft Research Asia.
small group of the labeling states has sufficient statis-
tics. We show that the SFB and the TP are special cases
of our method because they derive from our unified al-
gorithm with a different setting of parameters. We also
present a simple but robust variant algorithm in which
CRFs efficiently learn and predict large-scale natural
language data.
2 Linear-chain CRFs
Many versions of CRFs have been developed for use
in natural language processing, computer vision, and
machine learning. For simplicity, we concentrate on
linear-chain CRFs (Lafferty et al, 2001; Sutton and
McCallum, 2006), but the generic idea described here
can be extended to CRFs of any structure.
Linear-chain CRFs are conditional probability dis-
tributions over label sequences which are conditioned
on input sequences (Lafferty et al, 2001). Formally,
x = {x
t
}
T
t=1
and y = {y
t
}
T
t=1
are sequences of in-
put and output variables. Respectively, where T is the
length of sequence, x
t
? X and y
t
? Y where X is the
finite set of the input observations and Y is that of the
output label state space. Then, a first-order linear-chain
CRF is defined as:
p
?
(y|x) =
1
Z(x)
T
?
t=1
?
t
(y
t
, y
t?1
,x), (1)
where ?
t
is the local potential that denotes the factor
at time t, and ? is the parameter vector. Z(x) is a
partition function which ensures the probabilities of all
state sequences sum to one. We assume that the poten-
tials factorize according to a set of observation features
{?
1
k
} and transition features {?
2
k
}, as follows:
?
t
(y
t
, y
t?1
,x) =?
1
t
(y
t
,x) ??
2
t
(y
t
, y
t?1
), (2)
?
1
t
(y
t
,x) =e
?
k
?
1
k
?
1
k
(y
t
,x)
, (3)
?
2
t
(y
t
, y
t?1
) =e
?
k
?
2
k
?
2
k
(y
t
,y
t?1
)
, (4)
where {?
1
k
} and {?
2
k
} are weight parameters which we
wish to learn from data.
Inference is significantly challenging both in learn-
ing and decoding CRFs. Time complexity is O(T |Y|
2
)
for exact inference (i.e., forward-backward and Viterbi
algorithm) of linear-chain CRFs (Lafferty et al, 2001).
The inference process is often prohibitively expensive
281
when |Y| is large, as is common in large-scale tasks.
This problem can be alleviated by introducing approx-
imate inference methods based on reduction of the
search spaces to be explored.
3 Efficient Inference Algorithm
3.1 Method
The key idea of our proposed efficient inference
method is that the output label state Y can be decom-
posed to an active set A and an inactive set A
c
. Intu-
itively, many of the possible transitions (y
t?1
? y
t
) do
not occur, or are unsupported, that is, only a small part
of the possible labeling set is informative. The infer-
ence algorithm need not precisely calculate marginals
or maximums (more generally, messages) for unsup-
ported transitions. Our efficient inference algorithm
approximates the unsupported transitions by assigning
them a constant value. When |A| < |Y|, both train-
ing and decoding times are remarkably reduced by this
approach.
We first define the notation for our algorithm. Let
A
i
be the active set and A
c
i
be the inactive set of output
label i where Y
i
= A
i
? A
c
i
. We define A
i
as:
A
i
= {j|?(y
t
= i, y
t?1
= j) > ?} (5)
where ? is a criterion function of transitions (y
t?1
?
y
t
) and ? is a hyperparameter. For clarity, we define the
local factors as:
?
1
t,i
, ?
1
t
(y
t
= i,x), (6)
?
2
j,i
, ?
2
t
(y
t?1
= j, y
t
= i). (7)
Note that we can ignore the subscript t at ?
2
t
(y
t?1
=
j, y
t
= i) by defining an HMM-like model, that is,
transition matrix ?
2
j,i
is independent of t.
As exact inference, we use the forward-backward
procedure to calculate marginals (Sutton and McCal-
lum, 2006). We formally describe here an efficient
calculation of ? and ? recursions for the forward-
backward procedure. The forward value ?
t
(i) is the
sum of the unnormalized scores for all partial paths that
start at t = 0 and converge at y
t
= i at time t. The
backward value ?
t
(i) similarly defines the sum of un-
normalized scores for all partial paths that start at time
t + 1 with state y
t+1
= j and continue until the end
of the sequences, t = T + 1. Then, we decompose the
equations of exact ? and ? recursions as follows:
?
t
(i) = ?
1
t,i
?
?
?
j?A
i
(
?
2
j,i
? ?
)
?
t?1
(j) + ?
?
?
, (8)
?
t?1
(j) =
?
i?A
j
?
1
t,i
(
?
2
j,i
? ?
)
?
t
(i) + ?
?
i?Y
?
1
t,i
?
t
(i),
(9)
where ? is a shared transition parameter value for set
A
c
i
, that is, ?
2
j,i
= ? if j ? A
c
i
. Note that
?
i
?
t
(i) = 1
(Sutton and McCallum, 2006). Because all unsup-
ported transitions in A
c
i
are calculated simultaneously,
the complexities of Eq. (8) and (9) are approximately
O(T |A
avg
||Y|) where |A
avg
| is the average number of
states in the active set, i.e.,
1
T
?
T
t=1
|A
i
|. The worst
case complexity of our ? and ? equations is O(T |Y|
2
).
Similarly, we decompose a ? recursion for the
Viterbi algorithm as follows:
?
t
(i) = ?
1
t,i
{
max
(
max
j?A
i
?
2
j,i
?
t?1
(j),max
j?Y
??
t?1
(j)
)}
,
(10)
where ?
t
(i) is the sum of unnormalized scores for the
best-scored partial path that starts at time t = 0 and
converges at y
t
= i at time t. Because ? is constant,
max
j?Y
?
t?1
(j) can be pre-calculated at time t ? 1.
By analogy with Eq. (8) and (9), the complexity is ap-
proximately O(T |A
avg
||Y|).
3.2 Setting ? and ?
To implement our inference algorithm, we need a
method of choosing appropriate values for the setting
function ? of the active set and for the constant value
? of the inactive set. These two problems are closely
related. The size of the active set affects both the com-
plexity of inference algorithm and the quality of the
model. Therefore, our goal for selecting ? and ? is
to make a plausible assumption that does not sacrifice
much accuracy but speeds up when applying large state
tasks. We describe four variant special case algorithms.
Method 1: We set ?(i, j) = Z(L) and ? = 0 where
L is a beam set, L = {l
1
, l
2
, . . . , l
m
} and the sub-
partition function Z(L) is approximated by Z(L) ?
?
t?1
(j). In this method, all sub-marginals in the inac-
tive set are totally excluded from calculation of the cur-
rent marginal. ? and ? in the inactive sets are set to 0
by default. Therefore, at each time step t the algorithm
prunes all states i in which ?
t
(i) < ?. It also generates
a subset L of output labels that will be exploited in next
time step t + 1.
1
This method has been derived the-
oretically from the process of selecting a compressed
marginal distribution within a fixed KL divergence of
the true marginal (Pal et al, 2006). This method most
closely resembles SFB algorithm; hence we refer an al-
ternative of SFB.
Method 2: We define ?(i, j) = |?
2
j,i
?1| and ? = 1.
In practice, unsupported transition features are not pa-
rameterized
2
; this means that ?
k
= 0 and ?
2
j,i
= 1
if j ? A
c
i
. Thus, this method estimates nearly-exact
1
In practice, dynamically selecting L increases the num-
ber of computations, and this is the main disadvantage of
Method 1. However, in inactive sets ?
t?1
(j) = 0 by de-
fault; hence, we need not calculate ?
t?1
(j). Therefore, it
counterbalances the extra computations in ? recursion.
2
This is a common practice in implementation of input
and output joint feature functions for large-scale problems.
This scheme uses only supported features that are used at
least once in the training examples. We call it the sparse
model. While a complete and dense feature model may per-
282
CRFs if the hyperparameter is ? = 0; hence this cri-
terion does not change the parameter. Although this
method is simple, it is sufficiently efficient for training
and decoding CRFs in real data.
Method 3: We define ?(i, j) = E
p?
??
2
k
(i, j)? where
E
p?
?z? is an empirical count of event z in training data.
We also assign a real value for the inactive set, i.e.,
? = c ? R, c 6= 0, 1. The value c is estimated in the
training phase; hence, c is a shared parameter for the
inactive set. This method is equivalent to TP (Cohn,
2006). By setting ? larger, we can achieve faster infer-
ence, a tradeoff exists between efficiency and accuracy.
Method 4: We define the shared parameter as a func-
tion of output label y in the inactive set, i.e., c(y). As in
Method 3, c(y) is estimated during the training phase.
When the problem expects different aspects of unsup-
ported transitions, this method would be better than us-
ing only one parameter c for all labels in inactive set.
4 Experiment
We evaluated our method on six large-scale natu-
ral language data sets (Table 1): Penn Treebank
3
for part-of-speech tagging (PTB), phrase chunk-
ing data
4
(CoNLL00), named entity recognition
data
5
(CoNLL03), grapheme-to-phoneme conversion
data
6
(NetTalk), spoken language understanding data
(Communicator) (Jeong and Lee, 2006), and fine-
grained named entity recognition data (Encyclopedia)
(Lee et al, 2007). The active set is sufficiently small in
Communicator and Encyclopedia despite their large
numbers of output labels. In all data sets, we selected
the current word, ?2 context words, bigrams, trigrams,
and prefix and suffix features as basic feature templates.
A template of part-of-speech tag features was added for
CoNLL00, CoNLL03, and Encyclopedia. In particu-
lar, all tasks except PTB and NetTalk require assigning
a label to a phrase rather than to a word; hence, we used
standard ?BIO? encoding. We used un-normalized log-
likelihood, accuracy and training/decoding times as our
evaluation measures. We did not use cross validation
and development set for tuning the parameter because
our goal is to evaluate the efficiency of inference algo-
rithms. Moreover, using the previous state-of-the-art
features we expect the achievement of better accuracy.
All our models were trained until parameter estima-
tion converged with a Gaussian prior variance of 4.
During training, a pseudo-likelihood parameter estima-
tion (Sutton and McCallum, 2006) was used as an ini-
tial weight (estimated in 30 iterations). We used com-
plete and dense input/output joint features for dense
model (Dense), and only supported features that are
used at least once in the training examples for sparse
form better, the sparse model performs well in practice with-
out significant loss of accuracy (Sha and Pereira, 2003).
3
Penn Treebank3: Catalog No. LDC99T42
4
http://www.cnts.ua.ac.be/conll2000/chunking/
5
http://www.cnts.ua.ac.be/conll2003/ner/
6
http://archive.ics.uci.edu/ml/
Table 1: Data sets: number of sentences in the train-
ing (#Train) and the test data sets (#Test), and number
of output labels (#Label). |A
?=1
avg
| denotes the average
number of active set when ? = 1, i.e., the supported
transitions that are used at least once in the training set.
Set #Train #Test #Label |A
?=1
avg
|
PTB 38,219 5462 45 30.01
CoNLL00 8,936 2,012 22 6.59
CoNLL03 14,987 3,684 8 4.13
NetTalk 18,008 2,000 51 22.18
Communicator 13,111 1,193 120 3.67
Encyclopedia 25,348 6,336 279 3.27
model (Sparse). All of our model variants were based
on Sparse model. For the hyper parameter ?, we empir-
ically selected 0.001 for Method 1 (this preserves 99%
of probability density), 0 for Method 2, and 4 for Meth-
ods 3 and 4. Note that ? for Methods 2, 3, and 4 indi-
cates an empirical count of features in training set. All
experiments were implemented in C++ and executed in
Windows 2003 with XEON 2.33 GHz Quad-Core pro-
cessor and 8.0 Gbyte of main memory.
We first show that our method is efficient for learning
CRFs (Figure 1). In all learning curves, Dense gener-
ally has a higher training log-likelihood than Sparse.
For PTB and Encyclopedia, results for Dense are not
available because training in a single machine failed
due to out-of-memory errors. For both Dense and
Sparse, we executed the exact inference method. Our
proposed method (Method 1?4) performs faster than
Sparse. In most results, Method 1 was the fastest, be-
cause it was terminated after fewer iterations. How-
ever, Method 1 sometimes failed to converge, for ex-
ample, in Encyclopedia. Similarly, Method 3 and 4
could not find the optimal solution in the NetTalk data
set. Method 2 showed stable results.
Second, we evaluated the accuracy and decoding
time of our methods (Table 2). Most results obtained
using our method were as accurate as those of Dense
and Sparse. However, some results of Method 1, 3,
and 4 were significantly inferior to those of Dense and
Sparse for one of two reasons: 1) parameter estimation
failed (NetTalk and Encyclopedia), or 2) approximate
inference caused search errors (CoNLL00 and Com-
municator). The improvements of decoding time on
Communicator and Encyclopedia were remarkable.
Finally, we compared our method with two open-
source implementations of CRFs: MALLET
7
and
CRF++
8
. MALLET can support the Sparse model, and
the CRF++ toolkit implements only the Dense model.
We compared them with Method 2 on the Commu-
nicator data set. In the accuracy measure, the re-
sults were 91.56 (MALLET), 91.87 (CRF++), and 91.92
(ours). Our method performs 5?50 times faster for
training (1,774 s for MALLET, 18,134 s for CRF++,
7
Ver. 2.0 RC3, http://mallet.cs.umass.edu/
8
Ver. 0.51, http://crfpp.sourceforge.net/
283
0 10000 20000 30000 40000?
1400
00
?
1000
00
Training time (sec)
Log?
likelih
ood
SparseMethod 1Method 2Method 3Method 4
(a) PTB
0 500 1500 2500?
1000
0
?
6000
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(b) CoNLL00
0 500 1000 1500?1
4000
?
1000
0?
6000
?
2000
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(c) CoNLL03
0 1000 3000 5000
?
4100
0
?
3900
0
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(d) NetTalk
0 1000 3000 5000?75
00
?
6500
?
5500
?
4500
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(e) Communicator
0 100000 200000 300000
?
3000
0?
2000
0?
1000
0
Training time (sec)
Log?
likelih
ood
SparseMethod 1Method 2Method 3Method 4
(f) Encyclopedia
Figure 1: Result of training linear-chain CRFs: Un-normalized training log-likelihood and training times are
compared. Dashed lines denote the termination of training step.
Table 2: Decoding result; columns are percent accuracy (Acc), and decoding time in milliseconds (Time) measured
per testing example. ?
?
? indicates that the result is significantly different from the Sparse model. N/A indicates
failure due to out-of-memory error.
Method
PTB CoNLL00 CoNLL03 NetTalk Communicator Encyclopedia
Acc Time Acc Time Acc Time Acc Time Acc Time Acc Time
Dense N/A N/A 96.1 0.89 95.8 0.26 88.4 0.49 91.6 0.94 N/A N/A
Sparse 96.6 1.12 95.9 0.62 95.9 0.21 88.4 0.44 91.9 0.83 93.6 34.75
Method 1 96.8 0.74 95.9 0.55
?
94.0 0.24
?
88.3 0.34 91.7 0.73
?
69.2 15.77
Method 2 96.6 0.92
?
95.7 0.52 95.9 0.21
?
87.4 0.32 91.9 0.30 93.6 4.99
Method 3 96.5 0.84
?
94.2 0.51 95.9 0.24
?
78.2 0.29
?
86.7 0.30 93.7 6.14
Method 4 96.6 0.85
?
92.1 0.51 95.9 0.24
?
77.9 0.30 91.9 0.29 93.3 4.88
and 368 s for ours) and 7?12 times faster for decod-
ing (2.881 ms for MALLET, 5.028 ms for CRF++, and
0.418 ms for ours). This result demonstrates that learn-
ing and decoding CRFs for large-scale natural language
problems can be efficiently solved using our method.
5 Conclusion
We have demonstrated empirically that our efficient in-
ference method can function successfully, allowing for
a significant speedup of computation. Our method links
two previous algorithms, the SFB and the TP. We have
also showed that a simple and robust variant method
(Method 2) is effective in large-scale problems.
9
The
empirical results show a significant improvement in
the training and decoding speeds especially when the
problem has a large state space of output labels. Fu-
ture work will consider applications to other large-scale
problems, and more-general graph topologies.
9
Code used in this work is available at
http://argmax.sourceforge.net/.
References
T. Cohn. 2006. Efficient inference in large conditional ran-
dom fields. In Proc. ECML, pages 606?613.
M. Jeong and G. G. Lee. 2006. Exploiting non-local fea-
tures for spoken language understanding. In Proc. of COL-
ING/ACL, pages 412?419, Sydney, Australia, July.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML, pages 282?289.
C. Lee, Y. Hwang, and M. Jang. 2007. Fine-grained named
entity recognition and relation extraction for question an-
swering. In Proc. SIGIR Poster, pages 799?800.
C. Pal, C. Sutton, and A. McCallum. 2006. Sparse forward-
backward using minimum divergence beams for fast train-
ing of conditional random fields. In Proc. ICASSP.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL/HLT, pages 134?141.
C. Sutton and A. McCallum. 2006. An introduction to condi-
tional random fields for relational learning. In Lise Getoor
and Ben Taskar, editors, Introduction to Statistical Rela-
tional Learning. MIT Press, Cambridge, MA.
284
Corpus -Based  Learn ing  of  Compound Noun Index ing  * 
Byung-Kwan Kwak, 
Jee-Hyub Kim, 
and Geunbae Lee t 
NLP Lab., Dept. of CSE 
Pohang University of 
Science & Technology 
(POSTECH) 
{nerguri,gblee} @postech.ac.kr 
J ung  Yun  Seo 
NLP Lab., 
Dept. of Computer  Science 
Sogang University 
seojy@ccs.sogang.ac.kr 
Abst ract  
In this paper, we present a corpus- 
based learning method that can 
index diverse types of compound 
nouns using rules automatically ex- 
tracted from a large tagged corpus. 
We develop an efficient way of ex- 
tracting the compound noun index- 
ing rules automatically and perform 
extensive experiments to evaluate 
our indexing rules. The automatic 
learning method shows about the 
same performance compared with 
the manual inguistic approach but 
is more portable and requires no 
human efforts. We also evaluate 
the seven different filtering meth- 
ods based on both the effectiveness 
and the efficiency, and present a 
new method to solve the problems of 
compound noun over-generation a d 
data sparseness in statistical com- 
pound noun processing. 
1 In t roduct ion  
Compound nouns are more specific and ex- 
pressive than simple nouns, so they are more 
valuable as index terms and can increase 
the precision in search experiments. There 
are many definitions for the compound noun 
which cause ambiguities as to whether a given 
continuous noun sequence is a compound 
noun or not. We, therefore, need a clean 
" This research was supported by KOSEF special 
purpose basic research (1997.9 - 2000.8 #970-1020- 
301-3) 
t Corresponding author 
definition of compound nouns in terms of in- 
formation retrieval, so we define a compound 
noun as "any continuous noun sequence that 
appears frequently in documents." 1 
In Korean documents, compound nouns are 
represented in various forms (shown in Table 
1), so there is a difficulty in indexing all types 
of compound nouns. Until now, there have 
been much works on compound noun index- 
ing, but they still have limitations of cover- 
ing all types of compound nouns and require 
much linguistic knowledge to accomplish this 
goal. In this paper, we propose a corpus- 
based learning method for compound noun 
indexing which can extract he rules automat- 
ically with little linguistic knowledge. 
Table 1: Various types of Korean compound 
noun with regard to "jeong-bo geom-saeg (in- 
formation retrieval)" 
jeong-bo-geom-saeg (information-retrieval) 
jeong-bo-eui geom-saeg (retrieval of information) 
jeong-bo geom-saeg (information retrieval) 
jeong-bo-leul geom-saeg-ha-neun 
(retrieving information) 
jeong-bo-geom-saeg si-seu-tem 
(information-retrieval system) 
As the number of the documents i growing 
retrieval, efficiency also becomes as important 
as effectiveness. To increase the efficiency, we 
focus on reducing the number of indexed spu- 
rious compound nouns. We perform experi- 
ments on several filtering methods to find the 
algorithm that can reduce spurious compound 
nouns most efficiently. 
1 The frequency threshold can be adjusted accord- 
ing to application systems. 
57 
The remainder of this paper ? is organized 
as follows. Section 2 describes previous com- 
pound noun indexing methods for Korean and 
compound noun filtering methods. We show 
overall compound noun indexing system ar- 
chitecture in Section 3, and expl~.~n each mod- 
ule of the system in Section 4 and 5 in de- 
tail. We evaluate our method with standard 
Korean test collections in Section 6. Finally, 
concluding remarks are given in Section 7. 
2 P rev ious  Research  
2.1 Compound Noun Indexing 
There have been two different methods 
for compound noun indexing: statistical 
and linguistic. In one Statistical method, 
(Fagan, 1989) indexed phrases using six 
different parameters, including information 
on co-occurrence of phrase elements, rela- 
tive location of phrase elements, etc., and 
achieved reasonable performance. However, 
his method couldn't reveal consistent sub- 
stantial improvements on five experimental 
document collections in effectiveness. (Strza- 
lkowski et al, 1996; Evans and Zhai, 1996) 
indexed subcompounds from complex noun 
phrases using noun-phrase analysis. These 
methods need to find the head-modifier rela- 
tions from noun phrases and therefore require 
difficult syntactic parsing in Korean. 
For Korean, in one statistical method, (Lee 
and Ahn, 1996) indexed general Korean nouns 
using n-grams without linguistic knowledge 
and the experiment results showed that the 
proposed method might be Mmost as effec- 
tive as the linguistic noun indexing. How- 
ever, this method can generate many spuri- 
ous n-grarn~ which decrease the precision in 
search performance. In linguistic methods, 
(Kim, 1994) used five manually chosen com- 
pound noun indexing rule patterns based on 
linguistic knowledge. However, this method 
cannot index the diverse types of compound 
nouns. (Won et al, 2000) used a full parser 
and increased the precision in search experi- 
ments. However, this linguistic method can- 
not be applied to unrestricted texts robustly. 
In summary, the previous methods, 
whether they are statistical or linguistic, 
have their own shortcomings. Statistical 
methods require signiAcant amounts of 
co-occurrence information for reasonable 
performance and can not index the diverse 
types of compound nouns. Linguistic meth- 
ods need compound noun indexing rules 
described by human and sometimes result 
in meaningless compound nouns, which 
decreases the performance of information 
retrieval systems. They cannot also cover the 
various types of compound nouns because of 
the limitation of human linguistic knowledge. 
In this paper, we present a hybrid method 
that uses linguistic rules but these rules are 
automatically acquired from a large corpus 
through statistical learning. Our method gen- 
erates more diverse compound noun index- 
ing rule patterns than the previous tandard 
methods (Kim, 1994; Lee et ah, 1997), be- 
cause previous methods use only most gen- 
eral rule patterns (shown in Table 2) and are 
based solely on human linguistic knowledge. 
Table 2: Typical hand-written compound 
noun indexing rule patterns for Korean 
Noun without case makers / Noun 
Noun with a genitive case maker / Noun 
Noun with a nominal case maker or 
an accusative case maker \[ 
Verbal common oun or adjectival common noun 
Noun with an adnominal ending \] Noun 
Noun within predicate particle phrase / Noun 
(The two nouns before and after a slash 
in the pattern can form a single compound noun.) 
2.2 Compound Noun F i l ter ing 
Compound noun indexing methods, whether 
they are statistical or linguistic, tend to gen- 
erate spurious compound nouns when they 
are actually applied. Since an information re- 
trieval system can be evaluated by its effec- 
tiveness and also by its efficiency (van Rijs- 
bergen, 1979), the spurious compound nouns 
should be efficiently filtered. (Kando et al, 
1998) insisted that, for Japanese, the smaller 
the number of index terms is, the better the 
performance of the information retrieval sys- 
tem should be. 
58 
For Korean, (Won et al, 2000) showed 
that segmentation f compound nouns is more 
efficient than compound noun synthesis in 
search performance. There have been many 
works on compound noun filtering methods; 
(Kim, 1994) used mutual information only, 
and (Yun et al, 1997) used mutual informa- 
tion and relative frequency of POS (Part-Of- 
Speech) pairs together. (Lee et ai., 1997) used 
stop word dictionaries which were constructed 
manually. Most of the previous methods for 
compound noun filtering utilized only one 
consistent method for generated compound 
nouns irrespective of the different origin of 
compound noun indexing rules, and the meth- 
ods cause many problems due to data sparse- 
hess in dictionary and training data. Our 
approach solves the data sparseness problem 
by using co-occurrence information on auto- 
matically extracted compound noun elements 
together with a statistical precision measure 
which fits best to each rule. 
3 Overa l l  Sys tem Arch i tec ture  
The compound noun indexing system pro- 
posed in this paper Consists of two major 
modules: one for automatically extracting 
compound noun indexing rules (in Figure 1) 
and the other for indexing documents, fil- 
tering the automatically generated compound 
nouns, and weighting the indexed compound 
nouns (in Figure 2). 
Compound ~ Tagged Corpus 1 
Compound ~ R  
Noun Statistical 
Information 
~ Roles with 
Precision 
Extracted Rules I 
Filtered Rules 
Figure 1: Compound noun indexing-rule ex- 
traction module (control flow =~, data flow 
Compound Noun~-----~-~ Indexing I ~_Indexing~'~ / 
Rules wig f--.......?-.~ 
Compound 
//Compound Noun\[ s~i~ \[ Infonnadon \[ 
Find \[ Compound I Nouns , I 
Weighted 
. Compound 
Nouns 
Figure 2: Compound noun indexing, filtering, 
and weighting module (control flow =~, data 
flow ~) 
4 Automat ic  Ext rac t ion  of  
Compound Noun Index ing  Ru les  
There are three major steps in automatically 
extracting compound noun indexing rules. 
The first step is to collect compound noun 
statistical information, and the second step is 
to extract the rules from a large tagged cor- 
pus using the collected statistical information. 
The final step is to learn each rule's precision.. 
4.1 Col lect ing Compound Noun 
Statist ics 
We collect initial compound noun seeds which 
were gathered from various types of well- 
balanced ocuments uch as ETRI Kemong 
encyclopaedia 2 nd many dictionaries on the 
Internet, and we collected 10,368 seeds, as 
shown in Table 3. The small number of seeds 
are bootstrapped to extract the Compound 
noun indexing rules for various corpora. 
Table 3: Collected compound noun seeds 
No. of 2 3 Total 
component elements 
ETRI Kemong encyclomedia 5,100 2,088 7,188 
Internet dictionaries 2,071 1,109 3,180 
To collect more practical statistics on the 
compound nouns, we made a 1,000,000 eo- 
jeol(Korean spacing unit which corresponds 
2 Courteously provided by ETRI, Korea. 
59 
to an English word or phrase) tagged cor- 
pus for a compound noun indexing experi- 
ment from a large document set (Korean In- 
formation Base). We collected complete com- 
pound nouns (a continuous noun sequence 
composed of at least two nouns on the condi- 
tion that both the preceding and the following 
POS of the sequence axe not nouns (Yoon et 
al., 1998)) composed of 1 - 3 no, ms from the 
tagged training corpus (Table 4). 
Table 4: Statistics for complete compound 
nouns 
No. of 1 2 3 
component elements 
Vocabulary 264,359 200,455 63,790 
4.2  Ext rac t ing  Index ing  Ru les  
We define a template (in Table 5) to extract 
the compound noun indexing rules from a 
POS tagged corpus. 
The template means that if a front- 
condition-tag, a rear-condition-tag, and sub- 
string-tags are coincident with input sentence 
tags, the lexical item in the synthesis position 
of the sentence can be indexed as a compound 
noun as "x /  y (for 3-noun compounds, x / 
y / z)". The tags used in the template are 
POS (Part-Of-Speech) tags and we use the 
POSTAG set (Table 17). 
The following is an algorithm to extract 
compound noun indexing rules from a large 
tagged corpus using the two-noun compound 
seeds and the template defined above. The 
rule extraction scope is limited to the end 
of a sentence or, if there is a conjunctive 
ending (eCC) in the sentence, only to the 
conjunctive nding of the sentence. A rule 
extraction example is shown in Figure 3. 
Algorithm 1: Extracting compound noun 
indexing rules (for 2-noun compounds) 
Read Template 
Read Seed 
(Consist of Constituent 1 / Constituent 2) 
TokeD/ze Seed into Constituents 
Put Constituent 1 into Key1 and Constituent 2 
? into Key2 
While (Not(End of Documents)) 
{ 
Read Initial Tag of Sentence 
While (Not(End of Sentence or eCC)) 
{ 
Read NeIt Tag of Sentence 
If (Read Tag =ffi Key1) 
{ 
While (Not(End of Sentence or eCC)) 
( 
Read Next Tag of Sentence 
If (Current Tag == Key2) 
Write Rule according 
to the Template 
} 
} 
} 
The next step is to  refine the extracted 
rules to select he proper ones. We used a rule 
filtering algorithm (Algorithm 2) using the 
frequency together with the heuristics that 
the rules with negative lexical items (shown in 
Table 6) will make spurious compound nouns. 
Algorithm 2: Filtering extracted rules us- 
ing frequency and heuristics 
I. For each compound noun seed, select 
the rules whose frequency is greater than 2. 
2. Among rules selected by step 1, select 
only rules that are extracted 
at least by 2 seeds. 
3. Discard rules which contain 
negative lexical items. 
Table 5: The template to extract the com- 
pound noun indexing rules 
,o  
front-condition-tag I 
sub-string-tags (tag 1 tag 2 ... tag n-1 tag n) \[ 
rear-condition-tag I 
synthesis locations (x y) 
lexicon x / lexicon y 
(for 3-noun compounds, 
synthesis locations (x, y, z) 
lexicon x / lexicon y / lexicon z) 
Table 6: Negative 
negative items (tags) 
je-oe(MC) (exclude) 
eobs(E) (not exist) 
mos-ha(D) (can not) 
lexical item examples 
example phrases 
no-jo-leul je-oe-han hoe-eui 
(meeting excluding union) 
sa-gwa-ga eobs~neun na-mu 
(tree without apple) 
dog-lib-eul mos-han gug-ga 
(country that 
cannot be liberated) 
We automatically extracted and filtered out 
60 
Tagged ~t~ 
B,~baI-Ib, 
MC< .kong-bo > 
.iC<tmb. 
MC< geom-sacg > 
fron~com~illm1_~g I sub_s~ring_mgs (~ I lag2 ... tag n-I ~n)  
~rcar_cond~iol~. I~ syn~lcsls location (x y) ~> lexicon x I Icxlco~ y 
(i.formafioa~?uicvall 
Indcxlag Rule: 
B I MC.jC<leul> MC I y I l 3 
Figure 3: Rule Extraction Process Example 
2,036 rules from the large tagged corpus (Ko- 
rean Information Base, 1,000,000 eojeol) us- 
ing the above Algorithm 2. Among the ill- 
tered rules, there are 19 rules with negative 
lexical items and we finally selected 2,017 
rules. Table 7 shows a distribution of the final 
rules according to the number of elements in 
their sub-string-tags. 
Table 7: Distribution of extracted rules by 
number of elements in sub-string-tags 
No. Distribution Example 
2 tags 79.6 % MC MC 
3 tags 12.6 % MC jO(eui) MC 
4 tags 4.7 % MC y eCNMG MC 
5 tags 1.5 % MC MC jO(e) 
DI<sog-ha-neun) MC 
over 6 tags 1.6 % 
The automatically extracted rules have 
more rule patterns and lexical items than 
human-made rules so they can cover more 
diverse types of compound nouns (Table 8). 
When checking the overlap between the two 
rule collections, we found that the manual in- 
guistic rules are a subset of our automatically 
generated statistical rules. Table 9 shows 
some of the example rules newly generated 
from our extraction algorithm, which were 
originally missing in the manual rule patterns. 
4.3 Learning the Precision o f  
Ext racted  Rules 
In the proposed method, we use the precision 
of rules to solve the compound noun over- 
generation and the data sparseness problems. 
The precision of a rule can be defined by 
Table 8: Comparison between the automati- 
cally extracted rules and the manual rules 
Method 
Manual 
linguistic 
method 
Our method 
No. of No. of 
general lexical terms 
rule patterns used in rule patterns 
16 
23 78 
Table 9: Examples of newly added rule pat- 
terns 
Rule 
Noun + bound noun / Noun 
Noun + suffix / Noun 
Noun + suffix + assignment verb + 
adnominal ending / Noun 
counting how many indexed compound noun 
candidates generated by the rule are actual 
compound nouns: 
Yactuat 
Prec(rule) = Ncandidate 
where Prec(rule) is the precision of a rule, 
Ndctual is the number of actual compound 
nouns, and Ncandidat e is the number of com- 
pound noun candidates generated by the au- 
tomatic indexing rules. 
To  calculate the precision, we need a defin- 
ing measurement for compound noun identi- 
fication. (Su et al, 1994) showed that the 
average mutual information of a compound 
noun tends to be higher than that of a non- 
compound noun, so we try to use the mutual 
information as the measure for identifying the 
compound nouns. If the mutual information 
of the compound noun candidate is higher 
than the average mutual information of the 
compound noun seeds, we decide that it is 
a compound noun. For mutual information 
(MI), we use two different equations: one for 
two-element compound nouns (Church and 
Hanks, 1990) and the other for three-element 
compound nouns (Suet  al., 1994). The equa- 
tion for two-element compound nouns is as 
follow: 
P(x,y) 
I(x;y) = log 2 P(x) x P(y) 
61 
where x and y are two words in the corpus, 
and I(x; y) is the mutual information of these 
two words (in this order). Table 10 shows 
the average MI value of the two and three 
elements. 
Table 10: Average value of the mutual infor- 
mation (MI) of compound noun seeds 
.Number of elements \[ 2 I 3 
Average MI 3.56 3.62 
The MI was calculated from the statistics of 
the complete compound nouns collected from 
the tagged training corpus (see Section 4.1). 
However, complete compound nouns are 
continuous noun sequences and cause the 
data sparseness problem. Therefore, we need 
to expand the statistics. Figure 4 shows 
the architecture of the precision learning 
module by expanding the statistics of the 
complete compound nouns along with an 
algorithmic explanation (Algorithm 3) of the 
process. Table 11 shows the improvement in
the average precision during the repetitive 
execution of this learning process. 
Norm Statistical ) 
Compound Norm of Rules ~ '~ Rule incision (step 5) (s~ 2.7) l~v v\[ (step s) 
Figure 4: Learning the precision of the com- 
pound noun indexing rules (The steps are 
shown in Algorithm 3) 
Algorithm 3: 
i. Calculate all rules' initial precision 
using initial complete compound noun 
statistical information. 
2. Calculate the average precision 
of the rules. 
3. Multiply a rule's precision by 
the frequency of the compound noun made 
by the  ru le .  
We ca l l  th i s  va lue  the  mod i f ied  f requency  
(MF). 
4. Collect the same compound nouns, and 
sum all the modified frequencies 
for each compound noun. 
5. If the sunm~ed modified frequency is greater 
than a threshold, add this compound noun 
to the complete compound noun 
s ta t i s t i ca l  information. 
6. Calculate all rules' precision again 
using the changed complete compound noun 
s ta t i s t i ca l  in fo rmat ion .  
7. Calculate the average precision of the rules. 
8. If the average precision of the rules is 
equal to the previous average precision, 
stop. Othervise, go to step 2. 
Table 11: Improvement in the average preci- 
sion of rules 
Learning 1 2 3 4 5 6 
cycles 
Avg. prec. 0.19 0.23 0.39 0.44 0.45 0.45 
of rules 
5 Compound Noun Index ing ,  
F i l te r ing ,  and  Weight ing  
In this section, we explai n how to use the au- 
tomatically extracted rules to actually index 
the compound nouns, and describe how to fil- 
ter and weight the indexed compound nouns. 
5.1 Compound Noun I ndex ing  
To index compound nouns from documents, 
we use a natural anguage processing engine, 
SKOPE (Standard KOrean Processing En- 
gine) (Cha et al, 1998), which processes doc- 
uments by analysing words into morphemes 
and tagging part-of-speeches. The tagging 
results are compared with the automatically 
learned compound noun indexing rules and, if 
they are coincident with each other, we index 
them as compound nouns. Figure 5 shows a 
process of the compound noun indexing with 
an example. 
5.2 Compound Noun F i l ter ing 
Among the indexed compound nouns above, 
still there can be meaningless compound 
nouns, which increases the number of index 
terms and the search time. To solve com- 
pound noun over-generation problem, we ex- 
periment with seven different filtering meth- 
ods (shown in Table 12) by analyzing their 
62 
?. " Tagging Result: 
bbal-li jeong-bo-leul B<bbal-li> 
geom-saeg-ha-netm ~ Auaty~ ~.  ~ MC<jeong-bo >
(~evmg I \~'~?_"2:? ?"/ ~ jc<,e.,> 
information I \ tagging / /1..~ ? :-t.~..~ \] ~ --  - /  / I.MU< geom-saeg > 
/ I eCNMG<neun> 
"~d~"~g"l~ ~es-- \ ] 'X~mpoun"~ Indexed 
i 1,2 . 
Complete l /  ~ geom-saeg 
_ C?mp~No~ ~ - -  (mf , ,~o . /  
Statistical Information \] retrieval) 
Figure 5: Compound noun indexing process 
relative effectiveness and efficiency, as shown 
in Table 16. These methods can be divided 
into three categories: first one using MI, sec- 
ond one using the frequency of the compound 
nouns (FC), and the last one using the fre- 
quency of the compound noun elements (FE). 
MI (Mutual Information) is a measure of word 
association, and used under the assumption 
that a highly associated word n-gram is more 
likely to be a compound noun. FC is used 
under the assumption that a frequently en- 
countered word n-gram is more likely to be a 
compound than a rarely encountered n-gram. 
FE is ;used under the assumption that a word 
n-gram with a frequently encountered specific 
element is more likely to be a compound. In 
the method of C, D, E, and F, each threshold 
was decided by calculating the average num- 
ber of compound nouns of each method. 
Table 12: Seven different filtering methods 
(MI) A. Mutual information of compound 
noun elements (0) 
(MI) B. Mutual information of compound 
noun elements 
(average of MI of compound noun seeds) 
(FC) C. Frequency of compound nouns 
in the training corpus (4) 
(FC) D. Frequency of compound nouns 
in the test corpus (2) 
(FE) E. Frequency of compound noun heads 
in the training corpus (5) 
(FE) F. Frequency of compound noun modifiers 
in the training corpus (5) 
G. No filtering 
(The value in parantheses is a threshold.) 
Among these methods, method B gener- 
ated the smallest number of compound nouns 
best efficiency and showed the reasonable f- 
fectiveness (Table 16). On the basis of this 
filtering method, we develop a smoothing 
method by combining the precision of rules 
with the mutual information of the compound 
noun elements, and propose our final filtering 
method (H) as follows: 
P(x, y) + ~ ? Precision T(x, y) = log 2 P(x) x P(y) 
where a is a weighting coefficient and Preci- 
sion is the applied rules learned in Section 4.3. 
For the three-element compound nouns, the 
MI part is replaced with the three-element MI
equation 3 (Su et al, 1994). 
6 Exper iment  Resu l ts  
To calculate the similarity between a docu- 
ment and a query, we use the p-norm retrieval 
model (Fox, 1983) and use 2.0 as the p-value. 
We also use fhe  component nouns in a com- 
pound as the indexing terms. We follow the 
standard TREC evaluation schemes (Salton 
and Buckley, 1991). For single index terms, 
we use the weighting method atn.ntc (Lee, 
1995). 
6.1 Compound Noun Index ing  
Exper iments  
This experiment shows how well the proposed 
method can index diverse types of compound 
nouns than the previous popular methods 
which use human-generated compound noun 
indexing rules (Kim, 1994; Lee et al, 1997). 
For simplicity, we filtered the generated com- 
pound nouns using the mutual information of 
the compound noun elements with a thresh- 
old of zero (method A in Table 12). 
Table 13 shows that the terms indexed by 
previous linguistic approach are a subset of 
the ones made by our statistical approach. 
This means that the proposed method can 
cover more diverse compound nouns than the 
3 
PD (x, ~, z) 
I(x;y;z) = log 2 Px(x,y,z) 
63 
Table 13: Compound noun indexing coverage 
experiment (With a 200,000 eojeol Korean In- 
formation Base) 
Manual 
linguistic 
rule patterns 
Our 
automatic 
rule patterns 
No. of 
generated actual 22,276 30,168 
compound nouns. (+35.4 %) 
No. of 
generated actual 7,892 
compound nouns 
without overlap 
manual inguistic rule method. We perform a 
retrieval experiment to evaluate the automat- 
ically extracted rules. Table 144 and table 155 
show that our method has slightly better re- 
call and l l -point average precision than the 
manual inguistic rule method. 
Table 14: Compound noun indexing effective- 
ness experiment I 
Avg. recall 
Manual linguistic 
rule patterns 
82.66 
Our automatic 
rule patterns 
83.62 
(+1.16 %) 
ll-pt. 42.24 42.33 
avg. precision (+0.21%) 
No. of 504,040 515,801 
index terms (+2.33 %) 
Table 15: Compound noun indexing effective- 
ness experiment II
Avg. recall 
ll-pt, avg. 
precision 
No. of 
index terms 
Manual linguistic 
rule patterns 
86.32 
34.33 
1,242,458 
Our automatic 
rule patterns 
87.50 
(+1.35 %) 
34.54 
(+0.61%) 
1,282,818 
(+3.15 %) 
4 With KTSET2.0 test collections (Courteously 
provided by KT, Korea. (4,410 documents and 50 
queries)) 
s With KRIST2.0 test collection (Courteously pro- 
vided by KORDIC, Korea. (13,514 documents and 30 
queries)) 
6.2 Retrieval Experiments Using 
Various Filtering Methods 
In this experiment, we compare the seven fil- 
tering methods to find out which one is the 
best in terms of effectiveness and efficiency. 
For this experiment, we used our automatic 
rules for the compound noun indexing, and 
the test collection KTSET2.0. To check the 
effectiveness, we used recall and l l -point  av- 
erage precision. To check the efficiency, we 
used the number of index terms. Table 16 
shows the results of the various filtering ex- 
periments. 
From Table 16, the methods using mu- 
tual information reduce the number of in- 
dex terms, whereas they have lower precision. 
The reason of this lower precision is that MI 
has a bias, i.e., scoring in favor of rare terms 
over common terms, so MI seems to have a 
problem in its sensitivity to probabil ity es- 
t imation error (Yang and Pedersen, 1997). 
In this experiment 6, we see that method B 
generates the smallest number of compound 
nouns (best efficiency) and our final propos- 
ing method H has the best recall and precision 
? (effectiveness) with the  reasonable number ? of 
compound nouns (efficiency). We can con- 
clude that the filtering method H is the best, 
considering the effectiveness and the efficiency 
at the same time. 
7 Conc lus ion  
In this paper, we presented a method to ex- 
tract the compound noun indexing rules au- 
tomatically from a large tagged corpus, and 
showed that this method can index compound 
nouns appearing in diverse types of docu- 
ments. 
In the view of effectiveness, this method is 
slightly better than the previous linguistic ap- 
proaches but requires no human effort. 
The proposed method also uses no parser 
and no rules described by humans, there- 
fore, it can be applied to unrestricted texts 
very robustly and has high domain porta- 
6 Our Korean NLQ (Natural Lan- 
guage Querying) demo system (located in 
'http:/ /nlp.postech.ac.kz /Resarch/POSNLQ/') 
can be tested. 
64 
Table 16: Retrieval experiment 
A B C 
Average 83.62 83.62 83.62 
recall (+0.00) (+0.00) 
ll-pt, avg. 42.45 42.42 42.49 
precision (-0.07) (+0.09) 
Precision 52.11 52.44 52.07 
at 10 Docs. 
No. of 515,80 508,20 514,54 5~ 
index terms (-1.47) (-0.24) (-+ 
results 
D 
83.62 
(+0.00) 
42.55 
(+0.24) 
52.80 
47,27 
+6.10) 
of various filtering 
E F 
83.62 
(+0.00) 
42.72 
(+0.64) 
52.26 
572,36 
(+10.97) 
83.62 
(+0.00) 
42.48 
(+0.07) 
51.89 
574,04 
(+11.29) 
; methods 
G 
84.32 
(+0.84) 
42.48 
(+0.07) 
52.81 
705,98 
(+36.87) 
H 
84.32 
(.+0.84) 
42.75 
(+o.71) 
52.98 
509,90 
(-1.14) 
bility. We also presented a filtering method 
to solve the compound noun over-generation 
problem. Our proposed filtering method (H) 
shows good retrieval performance both in the 
view of the effectiveness and the efficiency. 
In the future, we need to perform some 
experiments on much larger commercial 
databases to test the practicality of our 
method. 
. Finally, our method doesn't  require lan- 
guage dependent knowledge, so it needs to be 
verified whether it can be easily applied to 
other languages. 
References 
Jeongwon Cha, Geunbae Lee, and Jong-Hyeok 
Lee. 1998. Generalized unknown morpheme 
guessing for hybrid pos tagging of korean. 
In Proceedings of SIXTH WORKSHOP ON 
VERY LARGE CORPORA in Coling-ACL 98. 
K. W. Church and P. Hanks. 1990. Word associ- 
ation norms, mutual information, and lexicog- 
raphy. Computational Linguistics, 16(1):22-29. 
David A. Evans and Chengxiang Zhai. 1996. 
Noun-phrase analysis in unrestricted text for 
information retrieval. In Proceedingof the 3~th 
Annual Meetinof the Association for Computa- 
tional Linguistics, Santa Cruz, CA, pages 17- 
24. 
Joel L. Fagan. 1989. The effectiveness of a non- 
syntactic approach to automatic phrase index- 
ing for document retrieval. JASIS, 40(2):115- 
132. 
E. A. Fox. 1983. Extending the Boolean and Vec- 
tor Space Models of Information Retrieval with 
P-norm Queries and Multiple Concept Types. 
Ph.D. thesis, Cornell Univ. 
Noriko Kando, Kyo Kageura, Masaharu Yoshoka, 
and Keizo Oyama. 1998. Phrase processing 
methods for japanase text retrieval. SIGIR fo- 
rum, 32(2):23-28. 
Pan Koo Kim. 1994. The automatic indexing 
of compound words from korean text based on 
mutual information. Journal of KISS (in Ko- 
rean), 21(7):1333-1340. 
Joon Ho Lee and Jeong Soo Ahn. 1996. Using 
n-grams for korean text retrieval. In SIGIR'96, 
pages 216-224. 
Hyun-A Lee, Jong-Hyeok Lee, and Geunbae Lee. 
1997. Noun phrase indexing using clausal 
segmentation. Journal of KISS (in Korean), 
24(3):302-311. 
Joon Ho Lee. 1995. Combining multiple vidence 
from different properties of weighting schemes. 
In SIGIR'95, pages 180-188. 
Gerard Salton and Chris Buckley. 1991. 
Text retrieval conferences evaluation pro- 
gram. In .ftp://)2p.cs.corneU.edu/pub/smart/, 
trec_eval.7.0beta.tar.gz. 
Tomek Strzalkowski, Louise Guthrie, Jussi Karl- 
gren, Jura Leistensnider, Fang Lin, Jose Perez- 
Carballo, Troy Straszheim, Jin Wang, and Jon 
Wilding. 1996. Natural anguage information 
retrieval: Trec-5 report. In The Fifth Text 
REtrieval conference (TREC-5), NIST Special 
publication, pages 500-238. 
Keh-Yih Su, Mind-Wen Wu, and Jing-Shin 
Chang. 1994. A corpus-based approach to au- 
tomatic ompound extraction. In Proceedings 
of ACL 94, pages 242-247. 
C. J. van Rijsbergen. 1979. Information Re- 
trieval. University of Computing Science, 
Lodon. 
Hyungsuk Won, Mihwa Park, and Geunbae Lee. 
2000. Integrated multi-level indexing method 
for compound noun processing. In Journal o.f 
KISS, 27(1) (in Korean), pages 84-95. 
65 
Tag 
MC 
T 
B 
DI 
I 
js 
eGS 
eCNMM 
eCC 
+ 
so  
s. 
sf 
Table 17: The POS (Part-Of-Speech) set of POSTAG 
common noun 
pronoun 
adverb 
irregular verb 
assignment verb 
auxiliary particle 
prefmal ending 
nominal ending 
conjunctive ending 
prefix 
other symbol 
sentence closer 
foreign word 
MP 
G 
K 
HI~ 
E 
jo 
eCNDI 
eCNMG 
Y 
S c 
s -  
sh 
Description 
proper noun 
adnoun 
interjection 
regular adjective 
existential predicate 
other particle 
attx conj ending 
adnomina l  end ing  
predicative particle 
suffix 
left parenthesis 
sentence connection 
Chinese character 
Tag 
MD 
S 
DR 
HI 
jc 
eGE 
eCNDC 
eCNB 
b 
su  
s '  
s ,  
Description 
bound noun 
numeral 
regular verb 
irregular adjective 
case particle 
final ending 
quote conj ending 
adverbial ending 
auxiliary verb 
unit symbol 
right parenthesis 
sentence comma 
Yiming Yang and Jan O. Pedersen. 1997. A com- 
parative study on feature selection in text cat- 
egorization. In Douglas H. Fisher, editor, Pro- 
ceedings of ICML-97, l~th International Con- 
ference on Machine Learning, pages 412--420, 
Nashville, US. Morgan Kaufmann Publishers, 
San Francisco, US. 
Jun-Tae Yoon, Eui-Seok Jong, and Mansuk Song. 
1998. Analysis of korean compound noun in- 
dexing using lexical information between ouns. 
Journal of KISS (in Korean), 25(11):1716- 
1725. 
Bo-Hyun Yun, Yong-Jae Kwak, and Hae-Chang 
Rim. 1997. A korean information retrieval 
model alleviating syntactic term mismatches. 
In Proceedings ofthe Natural Language Process- 
ing Pacific Rim Symposium, pages 107-112. 
66 
 
	MAYA: A Fast Question-answering System Based On A Predictive 
Answer Indexer* 
Harksoo Kim, Kyungsun Kim 
Dept. of Computer Science, 
Sogang University 
1 Sinsu-Dong, Mapo-Gu, Seoul, 
121-742, Korea 
{ hskim, kksun } 
@nlpzodiac.sogang.ac.kr 
Gary Geunbae Lee 
Dept. of Computer Science 
and Engineering, 
Pohang University of 
Science and Technology 
San 31, Hyoja-Dong, 
Pohang, 790-784, Korea 
gblee@postech.ac.kr 
Jungyun Seo 
Dept. of Computer Science, 
Sogang University 
1 Sinsu-Dong, Mapo-Gu, 
Seoul, 121-742, Korea 
seojy@ccs.sogang.ac.kr 
(Currently Visiting CSLI Stanford University) 
 
 
 
 
Abstract 
We propose a Question-answering 
(QA) system in Korean that uses a 
predictive answer indexer. The 
predictive answer indexer, first, 
extracts all answer candidates in a 
document in indexing time. Then, it 
gives scores to the adjacent content 
words that are closely related with each 
answer candidate. Next, it stores the 
weighted content words with each 
candidate into a database. Using this 
technique, along with a complementary 
analysis of questions, the proposed QA 
system can save response time because 
it is not necessary for the QA system to 
extract answer candidates with scores 
on retrieval time. If the QA system is 
combined with a traditional 
Information Retrieval system, it can 
improve the document retrieval 
precision for closed-class questions 
after minimum loss of retrieval time. 
1 Introduction? 
Information Retrieval (IR) systems have been 
applied successfully to a large scale of search 
area in which indexing and searching speed is 
important. Unfortunately, they return a large 
                                                          
?
 This research was partly supported by BK21 program of 
Ministry of Education and Technology Excellency 
Program of Ministry of Information and 
Telecommunications. 
amount of documents that include indexing 
terms in a user?s query. Hence, the user should 
carefully look over the whole text in order to 
find a short phrase that precisely answers his/her 
question. 
Question-answering (QA), an area of IR, is 
attracting more attention, as shown in the 
proceedings of AAAI (AAAI, 1999) and TREC 
(TREC, http://trec.nist.gov/overview.html). A 
QA system searches a large collection of texts, 
and filters out inadequate phrases or sentences 
within the texts. By using the QA system, a user 
can promptly approach to his/her answer phrases 
without troublesome tasks. However, most of 
the current QA systems (Ferret et al, 1999; Hull, 
1999; Srihari and Li, 1999; Prager et al, 2000) 
have two problems as follows: 
  It cannot correctly respond to all of the users? 
questions. It can answer the questions that are 
included in the pre-defined categories such as 
person, date, time, and etc. 

It requires more indexing or searching time than 
traditional IR systems do because it needs a 
deep linguistic knowledge such as syntactic or 
semantic roles of words. 
 
To solve the problems, we propose a QA 
system using a predictive answer indexer - 
MAYA (MAke Your Answer). We can easily 
add new categories to MAYA by only 
supplementing domain dictionaries and rules. 
We do not have to revise the searching engine of 
MAYA because the indexer is designed as a 
separate component that extracts candidate 
answers. In addition, a user can promptly obtain 
answer phrases on retrieval time because 
MAYA indexes answer candidates in advance. 
Most of the previous approaches in IR have 
been focused on the method to efficiently 
represent terms in a document because they 
want to index and search a large amount of data 
in a short time (Salton et al, 1983; Salton and 
McGill, 1983; Salton 1989). These approaches 
have been applied successfully to the 
commercial search engines (e.g. 
http://www.altavista.com) in World Wide Web 
(WWW). However, in a real sense of 
information retrieval rather than document 
retrieval, a user still needs to find an answer 
phrase within the vast amount of the retrieved 
documents although he/she can promptly find 
the relevant documents by using these engines. 
Recently, several QA systems are proposed to 
avoid the unnecessary answer finding efforts 
(Ferret et al, 1999; Hull, 1999; Moldovan et al 
1999; Prager et al, 1999; Srihari and Li, 1999). 
Recent researches have combined the 
strengths between a traditional IR system and a 
QA system (Prager et al, 2000; Prager et al, 
1999; Srihari and Li, 1999). Most of the 
combined systems access a huge amount of 
electronic information by using IR techniques, 
and they improve precision rates by using QA 
techniques. In detail, they retrieve a large 
amount of documents that are relevant to a 
user?s query by using a well-known TF  IDF. 
Then, they extract answer candidates within the 
documents, and filter out the candidates by 
using an expected answer type and some rules 
on the retrieval time. Although they have been 
based on shallow NLP techniques (Sparck-Jones, 
1999), they consume much longer retrieval time 
than traditional IR systems do because of the 
addictive efforts mentioned above. To save 
retrieval time, MAYA extracts answer 
candidates, and computes the scores of the 
candidates on indexing time. On retrieval time, 
it just calculates the similarities between a user?s 
query and the candidates. As a result, it can 
minimize the retrieval time. 
This paper is organized as follows. In Section 
2, we review the previous works of the QA 
systems. In Section 3, we describe the applied 
NLP techniques, and present our system. In 
Section 4, we analyze the result of our 
experiments. Finally, we draw conclusions in 
Section 5. 
2 Previous Works 
The current QA approaches can be classified 
into two groups; text-snippet extraction systems 
and noun-phrase extraction systems (also called 
closed-class QA) (Vicedo and Ferr?ndex, 2000). 
The text-snippet extraction approaches are 
based on locating and extracting the most 
relevant sentences or paragraphs to the query by 
assuming that this text will probably contain the 
correct answer to the query. These approaches 
have been the most commonly used by 
participants in last TREC QA Track (Ferret et al, 
1999; Hull, 1999; Moldovan et al, 1999; Prager 
et al, 1999; Srihari and Li, 1999). ExtrAns 
(Berri et al, 1998) is a representative QA 
system in the text-snippet extraction approaches. 
The system locates the phrases in a document 
from which a user can infer an answer. However, 
it is difficult for the system to be converted into 
other domains because the system uses syntactic 
and semantic information that only covers a very 
limited domain (Vicedo and Ferr?ndex, 2000). 
The noun-phrase extraction approaches are 
based on finding concrete information, mainly 
noun phrases, requested by users? closed-class 
questions. A closed-class question is a question 
stated in natural language, which assumes some 
definite answer typified by a noun phrase rather 
than a procedural answer. MURAX (Kupiec, 
1993) is one of the noun-phrase extraction 
systems. MURAX uses modules for the shallow 
linguistic analysis: a Part-Of-Speech (POS) 
tagger and finite-state recognizer for matching 
lexico-syntactic pattern. The finite-state 
recognizer decides users? expectations and 
filters out various answer hypotheses. For 
example, the answers to questions beginning 
with the word Who are likely to be people?s 
name. Some QA systems participating in Text 
REtrieval Conference (TREC) use a shallow 
linguistic knowledge and start from similar 
approaches as used in MURAX (Hull, 1999; 
Vicedo and Ferr?ndex, 2000). These QA 
systems use specialized shallow parsers to 
identify the asking point (who, what, when, 
where, etc). However, these QA systems take a 
long response time because they apply some 
rules to each sentence including answer 
candidates and give each answer a score on 
retrieval time. 
MAYA uses shallow linguistic information 
such as a POS tagger, a lexico-syntactic parser 
similar to finite-state recognizer in MURAX and 
a Named Entity (NE) recognizer based on 
dictionaries. However, MAYA returns answer 
phrases in very short time compared with those 
previous systems because the system extracts 
answer candidates and gives each answer a score 
using pre-defined rules on indexing time. 
3 MAYA Q/A approach 
MAYA has been designed as a separate 
component that interfaces with a traditional IR 
system. In other words, it can be run without IR 
system. It consists of two engines; an indexing 
engine and a searching engine. 
The indexing engine first extracts all answer 
candidates from collected documents. For 
answer extraction, it uses the NE recognizer 
based on dictionaries and the finite-state 
automata. Then, it gives scores to the terms that 
surround each candidate. Next, it stores each 
candidate and the surrounding terms with scores 
in Index DataBase (DB). For example, if n 
surrounding terms affects a candidate, n pairs of 
the candidate and terms are stored into DB with 
n scores. As shown in Figure 1, the indexing 
engine keeps separate index DBs that are 
classified into pre-defined semantic categories 
(i.e. users? asking points or question types). 
The searching engine identifies a user?s 
asking point, and selects an index DB that 
includes answer candidates of his/her query. 
Then, it calculates similarities between terms of 
his/her query and the terms surrounding the 
candidates.  The similarities are based on p-
Norm model (Salton et al, 1983). Next, it ranks 
the candidates according to the similarities. 
 
      	 
 
        
      Multilingual Question Answering with High Portability on 
Relational Databases 
 
Hanmin Jung 
Department of Computer Science and 
Engineering 
Pohang University of Science and Technology 
San 31, Hyoja-dong, Nam-gu, Pohang, 
Kyungbuk, Korea (790-784) 
Telephone: +82-54-279-5581 
jhm@postech.ac.kr 
Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science and Technology 
San 31, Hyoja-dong, Nam-gu, Pohang, Kyungbuk, 
Korea (790-784) 
Telephone: +82-54-279-5581 
gblee@postech.ac.kr 
 
Abstract  
This paper describes a highly-portable 
multilingual question answering system on 
multiple relational databases. We apply 
semantic category and pattern-based 
grammars, into natural language interfaces to 
relational databases. Lexico-semantic pattern 
(LSP) and multi-level grammars achieve 
portability of languages, domains, and 
DBMSs. The LSP-based linguistic 
processing does not require deep analysis 
that sacrifices robustness and flexibility, but 
can handle delicate natural language 
questions. To maximize portability, we drive 
various dependent parts into two tight 
corners, i.e., language-dependent part into 
front linguistic analysis, and 
domain-dependent and database-dependent 
parts into backend SQL query generation. 
Experiments with 779 queries generate only 
constraint-missing errors, which can be 
easily corrected by adding new terms, of 
2.25% for English and 5.67% for Korean. 
Introduction 
As a natural language (NL) interface, question 
answering [7] on relational databases 1  allows 
users to access information stored in databases by 
requests in natural language [16], and generates 
as output natural language sentences, tables, and 
graphical representation. The NL interface can be 
combined with other interfaces to databases: a 
                                                   
1  We also call it NLIDB (Natural Language Interface to 
DataBases). 
formal query language interface directly using 
SQL, a form-based interface with fields to input 
query patterns, and a graphical interface using a 
keyboard and a mouse to access tables. The NL 
interface does not require the learning of formal 
query languages, and it easily represents negation 
and quantification [4], and provides discourse 
processing [8]. 
The use of natural language has both advantages 
and disadvantages. Including general NLP 
problems such as quantifier scoping, PP 
attachment, anaphora resolution, and elliptical 
questions, current NLIDB has many 
shortcomings [1]: First, as a frequent complaint, 
it is difficult for users to understand which kinds 
of questions are actually allowed or not. Second, 
the user assumes that the system is intelligent; he 
or she thinks NLIDB has common sense, and can 
deduce facts. Finally, users do not know whether 
a failure is caused by linguistic coverage or by 
conceptual mismatch. Nevertheless, natural 
language does not need training in any 
communication media or predefined access 
patterns. 
NLIDB systems [2], one of the first applications 
of natural language processing, including 
?LUNAR? were developed from the 1970s [23]. 
In the 1980s, research focuses on intermediate 
representation and portability, and attempts to 
interface with various systems. CHAT-80 [22] 
transforms an English query into PROLOG 
representation, and ASK [20] teaches users new 
words and concepts. From 1990s, commercial 
systems based on linguistic theories such as 
GPSG, HPSG, and PATR-II appear [13], and 
some systems attempt to semi-automatically 
construct domain knowledge. MASQUE/SQL 
[1] uses a semi-automatic domain editor, and 
LOQUI [3], a commercial system, adopts GPSG 
grammar. Meanwhile, Demers introduces a 
lexicalist approach for natural language to SQL 
translation [6], and as the CoBase project of 
UCLA, Meng and Chu combine information 
retrieval and a natural language interface [14]. 
The major problems of the previous systems are 
as follows. First, they do not effectively reflect 
the vocabulary used in the description of database 
attributes into linguistic processing. Second, they 
require users to pose natural language queries at 
one time using a single sentence rather than give 
the flexibility by dialog-based query processing. 
The discordance between attribute vocabulary 
and linguistic processing vocabulary causes the 
portability problem of domain knowledge from 
knowledge acquisition bottleneck; the systems 
need extensive efforts by some experts who are 
highly experienced in linguistics as well as in the 
domain and the task. 
Androutsopoulos [1] [2], which are mainly 
referenced for this section, classifies NLIDB 
approaches into the following four major 
categories. 
Pattern matching systems: Some of the early 
systems exclude linguistic processing. They are 
easy to implement, but have many critical 
limitations caused by linguistic shallowness [17]. 
Syntax-based systems: They syntactically 
analyze user questions, and use grammars that 
transform parsed trees to SQL queries [23]. 
However, the mapping rules are difficult and 
tedious to devise, which drops the portability of 
languages and domains. 
Semantic grammar systems: The systems adopt 
techniques interleaving syntactic and semantic 
processing, and generate SQL queries from the 
result [19] [21]. They are useful to rapidly 
develop parsers in limited domains, but are not 
ported well to new domains due to hard-wired 
and domain-dependent semantic information 
[18]. 
Intermediate representation language 
systems: Most current systems place an 
intermediate logical query between NL question 
and SQL [5]. The processes before the 
intermediate query are defined as the linguistic 
front-end (LFE), and the other processes as the 
database back-end (DBE). This architecture has 
the merits that LFE is DBMS-independent and an 
inference module can be placed between LFE and 
DBE. However, the limitation of parsing and 
semantic analysis requires semantic 
post-processing. Nevertheless, it is difficult to 
achieve high quality analysis for database 
applications. 
On the contrary, we apply linguistic processing 
based on lexico-semantic patterns (LSP), a 
prominent method verified in text-based question 
answering [10] [12], into NLIDB, and propose 
multi-level grammars to represent query 
structures and to translate into SQL queries. Our 
system is a hybridization of the pattern matching 
system and the intermediate representation 
language system. However, our LSP-based 
pattern covers lexical to semantic matching, and 
the multi-level grammars for intermediate 
representation evidently separate the database 
back-end from the linguistic front-end. Thus, our 
method has the ability to divide LFE and DBE, 
but promises greater adaptability due to the 
hybrid linguistic analysis and the 
pattern-matching characteristics. 
The LSP-based linguistic processing does not 
require deep analysis that sacrifices robustness 
and flexibility, but handles delicate NL questions. 
To maximize portability of languages, domains, 
and DBMSs, we drive the various dependent 
parts into two tight corners, i.e., the 
language-dependent part into front linguistic 
analysis, and the domain-dependent and 
database-dependent parts into backend SQL 
query generation. In our LSP description, 
attribute vocabularies are also represented as 
semantic classes that represent semantic meaning 
of words. Thus, the domain-dependent attributes 
and values are automatically extracted from 
databases, and get registered in a semantic 
category dictionary. 
1 LSP-based Linguistic Processing and 
Multi-level Grammars 
A lexico-semantic pattern (LSP) is the structure 
where linguistic entries and semantic types can 
be used in combinations to abstract certain 
sequences of words in a text [12] [15]. Linguistic 
entries consist of words, phrases and 
part-of-speech tags, such as ?television,? ?3D 
Surround,? and ?NP2.? Semantic types consist of 
attribute names, semantic tags (categories) 3 and 
user-defined semantic classes 4 , such as 
?@model,? ?@person,? and ?%each.? 
LSP-based language processing simplifies the 
natural language interface due to the following 
characteristics: First, linguistic elements from 
lexicons to semantic categories offer flexibility in 
representing natural language. Second, simple 
LSP matching without fragile high-level analyses 
assures a robust linguistic model. Third, the use 
of common semantic types among different 
languages reduces the burden of cross-linguistic 
portability, i.e., enhances multilingual expansion. 
Finally, separation between dictionary and rules 
easily enriches domain knowledge by 
minimizing the conflict to describe the rules. 
Multi-level grammars are designed to construct 
intermediate representation as the source of SQL 
query generation. The grammars interpret 
lexico-semantic patterns obtained from the 
linguistic front-end, i.e., morphological analysis, 
and build attribute-value trees for database 
back-end. We introduce three-level grammars 
that include lexico-semantic patterns to describe 
their rules: a QT5 grammar to determine question 
types, an AV 6 -TYPE grammar to construct 
attribute-value nodes (see section 2.1), and an 
AV-OP grammar to find the relations between 
the nodes (see section 2.2). Using the QT 
grammar, query-to-LSP transfer makes a 
lexico-semantic pattern from a given question [9]. 
The lexico-semantic patterns enhance 
information abstraction through many-to-one 
mapping between questions and a 
lexico-semantic pattern. 
2 System Configuration 
To handle the two major problems of previous 
NLIDB systems, i.e., 1) the discordance between 
attribute vocabulary and linguistic processing 
                                                   
2 Part-of-speech tag for a proper noun 
3 ?@? is the start symbol for the semantic tags. 
4  ?%? is the start symbol for the user-defined semantic 
classes. 
5 Question type 
6 Attribute and value 
vocabulary, which causes laborious low 
portability in multiple environments, and 2) the 
absence of query refinement supporting 
session-based dialog, our system effectively 
develops LSP-based linguistic processing, 
multi-level grammars, and SQL query refinement. 
To maximize portability of multiple 
environments, such as languages, domains, and 
DBMSs, each environment-dependent module is 
clearly defined and confined. 
Our system consists of four phases (figure 2.1): 
morphological analysis, QT/AV Processing, 
AV-tree construction, and SQL query generation. 
The QT/AV processing determines the question 
types for a given question, and constructs 
attribute-value nodes from the question. This 
phase includes linguistic front-end processes, 
such as morphological analysis and named entity 
recognition. The AV-tree construction phase 
finds the relation between the nodes obtained 
from the previous phase, and produces an 
attribute-value tree that is independent of 
DBMSs. The last phase, SQL query generation, 
translates the attribute-value tree into a 
DBMS-dependent SQL query. 
 
 
[Figure 2.1] System Architecture 
2.1 Morphological Analysis and QT/AV 
Processing 
We separate all processes and resources using a 
linguistic dependency approach. Morphological 
analysis and all the resources, including 
grammars and dictionaries, are all language 
dependent. Morphological analysis for each 
language produces a sequence of (word, lemma, 
POS) pairs. After the analysis, system shares all 
processes independent of the source language; 
this increases linguistic portability by pushing the 
language-dependent processes to the earlier 
stage. 
To acquire the named entities for target databases, 
the system looks up the category dictionary 
which includes main semantic information. The 
category dictionary consists of four components: 
semantic tags, user-defined semantic classes, 
part-of-speech (POS) tags, and lexical forms. The 
structure of semantic tags is a flat form. In a 
lexico-semantic pattern, each semantic tag 
follows a ?@? symbol. For example, a semantic 
tag ?@item 7 ? includes the words, such as 
????,? ????,? and ?????? in Korean, 
and ?VCR? and ?video? in English. User-defined 
semantic classes are the tags for syntactically or 
semantically similar lexical groups. For example, 
a user-defined semantic class ?%each? includes 
the words, such as ??,? ????,? ???,? and 
??? in Korean, and ?each? and ?every? in 
English. The category dictionary has the highest 
priority to construct the lexico-semantic pattern 
for a sentence. In the absence of an entry, the 
part-of-speech tag of the current morpheme 
becomes the component of the LSP. 
A question type indicates ordering clauses, 
including ?ASC? and ?DESC,? or column 
functions such as ?SUM(),? ?AVG(),? and 
?MIN().? By applying a QT grammar, a question 
type and a target attribute, i.e., the argument of 
the question type, are obtained. The following 
shows the process from user query to SQL 
template. 
[User query] 
?How much is the cheapest among 34 inches?? 
[LSP pattern] 
%how %much %be %dt %most-cheap %among 
num @unit_length sent 
[QT grammar] 
?? 
^8%how%much%bedt%most-cheap 
qo_min|qt_price 
@corpin%asc 
qo_aso|qt_corp 
                                                   
7 ?item? attribute for ?audio-video? product database 
8 Symbol designating the beginning position of a regular 
expression 
%corp%make@type%and@type%together
 qo_intersect|qt_corp 
?? 
[Question type] 
MIN(price) 
[SQL template] 
SELECT MIN(price) FROM ? WHERE ? 
AV-TYPE grammar finds all the pairs of 
attributes and values by applying lexico-semantic 
patterns. Like QT grammar, LSP-based condition 
and action exist. The action consists of an 
attribute and a value operator. In SQL, the two 
components are represented like ([attribute] 
[value operator] [value]) in WHERE clauses, for 
example, (price > 300,000). The pairs of attribute 
and value become the nodes of the AV-tree, 
which is the source to generate the SQL query. 
The following examples demonstrate the method 
to obtain pairs using the AV-TYPE grammar. 
[User query] 
?Choose only Samsung?s among 25 inches, 21 
inches, and 29 inches products.? 
[LSP pattern] 
? @corp ? num @unit_length ? num 
@unit_length ? num @unit_length ? 
[AV-TYPE grammar] 
?? 
engj%start 
@model|vo_begin 
num@unit_length 
@size|vo_like 
%betweennum%andnum  
 @price|vo_between 
%each%price   
 @price|vo_group 
@corp    
 @corp|vo_like 
@price%elower   
 @vop_mod9|vo_elower 
?? 
[Pairs of attribute and value] 
(corp like ?%Samsung%?) 
(size like ?25inch?) 
(size like ?21inch?) 
(size like ?29inch?) 
                                                   
9 Action to modify the current value operation 
2.2 AV-tree Construction 
Binary operators (AND, OR, NOR, NAND ?) 
connect the pairs of attributes and values that are 
the nodes of the AV-tree. AV-OP grammar 
describes the relations using lexico-semantic 
patterns like other grammars. The condition part 
of the grammar consists of attributes and 
conjunctions (see examples below), whereas the 
action part consists of binary operators and the 
attributes? index in postfix notation. 
[Pairs of attribute and value from section 2.1] 
(corp like ?%Samsung%?) 
(size like ?25inch?) 
(size like ?21inch?) 
(size like ?29inch?) 
[AV-OP grammar] 
?? 
@corp@size@size%and@size
 1|2|3|5|bo_or|bo_or|bo_and 
@model%and@model@type
 1|3|bo_or|4|bo_and 
@type@model@pname
 1|2|bo_and|3|bo_and 
?? 
[Prefix notation and AV-tree] 
 
To handle negative expressions, we follow the 
two steps: First, we determine each negation?s 
scope from the input sentence. Second, we insert 
a ?NOT? unary operator into the maximal AV 
subtree that the negation?s scope covers. For 
example, the input sentence ?0 no 1 LG 2 
Electronics 3 and 4 Philips 5? has a negation with 
[0, 5] scope, and three subtrees: [1, 5] for ?LG 
Electronics and Philips,? [1, 3] for ?LG 
Electronics,? and [4, 5] for ?Philips.? Since the 
negation?s scope covers all the subtrees, ?NOT? 
operator is put onto the subtree with [1, 5]. 
Therefore, (NOT ((corp like ?%LG 
Electronics%?) OR (corp like ?%Philips%?))) 
becomes the constraint of the ?WHERE? clause. 
A query expansion dictionary defines the 
conceptual sets of values, where the set can 
definitely determine its values in a given DB, 
such as ?imported product? and ?Japanese 
companies.? The conceptual sets are different 
from user-defined terms in that the meaning of 
user-defined terms varies with users, such as 
?large size of TV? and ?high-priced audio.? For 
the user-defined terms, we maintain the user?s 
profile for each user, and for the conceptual sets, 
maintain a query expansion dictionary for the 
system. 
2.3 SQL Query Generation 
Domains and DBMSs affect the generation of 
SQL queries. For multiple domains, our system 
automatically determines the domain that the 
user requests; first, find the domains to which 
each attribute-value node belongs. Second, select 
one or more domains using the combination of 
domains from the first step. When two or more 
domains are chosen, the domains get combined 
with attributes in the SQL query to be generated, 
such as ?(saa.size >= ?29inch?)? and ?(bb.corp 
like ?%SONY%?).? Where the attribute of 
question type is ambiguous, the selected domains 
also help to fix the attribute. We generate an SQL 
query in the order of question types (SELECT ? 
FROM), tables (FROM ? WHERE), constraints 
(WHERE ?), sub-query, and connection with 
two SQLs. 
For supporting session-based dialog, we preserve 
the SQL query created from the previous 
questions, and re-generate new SQL query for the 
current successive question. First, the system 
checks whether the current question has the same 
domain as the previous question or not. When the 
two domains are equal, the previous generated 
SQL query becomes a constraint part of the 
current SQL query, for example, ?? FROM 
(SELECT ?) WHERE ? .? Otherwise, a default 
SQL query10 will be generated. 
 
                                                   
10 ?SELECT * FROM table-name WHERE 1 (or id>=0),? 
where id is the primary key. 
3 Portability of Languages, Domains, and 
DBMSs 
Natural language interfaces to databases should 
consider the portability of languages, domains, 
and DBMSs. Previous systems are short of one or 
more portability factors; systems with heavy 
linguistic components have trouble in expanding 
into other languages, and systems with simple 
pattern matching are insufficient to deal with 
multiple domains and DBMSs. On the other hand, 
our system adopts robust LSP-based language 
processing and multi-level grammars to 
structurally analyze the user?s query. 
Portability of languages: In our system, only 
both morphological analysis and the resources 
including dictionaries and grammars are 
language-dependent. Because the resources 
include lexico-semantic patterns that represent 
linguistic characteristics, both the linguistic 
front-end and the database back-end can be 
clearly divided. Where the languages to handle 
are similar to each other, as in word order and 
linguistic structure, many rules of grammars can 
be shared without consideration of the specific 
languages. Like English and Korean11, however, 
if the two languages are quite different, then the 
shared portion naturally decreases. We separate 
out the language-dependent morphological 
analysis and the subsequent processes as soon as 
possible to easily expand to other languages. To 
add another language, only new morphological 
analysis and linguistic resources need to be 
appended to the existing system. Heavy linguistic 
processes like syntactic and semantic analysis 
used by the previous systems inevitably delay the 
point of linguistic separation. 
Portability of domains: A new domain with the 
new DB schema affects both the linguistic 
front-end and the database back-end. To reduce 
the influence, our system deals with the 
domain-related information only in resources and 
SQL query generation. The other processes, such 
as morphological analysis, QT/AV processing, 
and AV-tree construction are all 
domain-independent. Domain category 
dictionary and grammars localize the attribute 
                                                   
11 Korean and Japanese are very similar in that the two are 
agglutinative languages and have SOV structures, whereas, 
English and some of the European languages are inflective 
and have SVO structures. 
names, but the general category dictionary is 
independent of domains because it is designed to 
handle common named entities. In order to 
manage the multiple domains, SQL query 
generation should consider domain information. 
However, a single domain reduces the burden of 
domain portability because the processing has no 
relation with the SQL query. 
Portability of DBMSs: The format and the 
descriptive power of SQL queries vary from 
DBMSs, such as in attribute names, sub-query 
operation, types of operations, case sensitivity, 
and constraint syntax. In our system, any 
linguistic processes and resources do not involve 
SQL query generation, which eventually 
increases DBMS portability. SQL query 
generation has alternatives to produce 
DBMS-dependent SQL only in some sub-parts, 
such as sub-query generation and combination 
with SQL queries. Until an SQL query begins to 
be generated, current DBMS does not influence 
any processes and resources. 
4 Experiment Results 
Pursuing high portability in languages, domains, 
and DBMSs, we implemented a multi-lingual 
question answering system on relational 
databases. The target languages are English and 
Korean, which are completely different in 
linguistic structures. Our system dealt with two 
domains for Korean: first, an audio-video product 
database with 418 entries automatically 
populated from an information extraction system 
[11], and second, a price comparison database 
with 1964 entries and multiple schemas from a 
BestBuyer 12  comparison shopping mall. For 
multiple languages, we manually translated all 
the Korean entries into English. Oracle 8.0.5 and 
MySQL 3.23.22 were used as two different 
DBMSs. 
Our system processes the user question from a 
Web browser, and produces an SQL query. Next, 
CGI (Common Gateway Interface) sends the 
query to DBMSs. For the result retrieved from 
databases, the user can ask a new question or 
make a context-based refinement of the question. 
                                                   
12 http://www.bestbuyer.co.kr/mainbbr/index.php3 
For training, five graduate students prepare 192 
questions for each language (see appendix A). 
The questions include negation, quantifiers, 
multiple conjunctions, multiple question types, 
various lexicography, user-defined terms, 
synonyms, and many value operators. Table 4.1 
shows the current linguistic resources 
constructed from the training set for both 
languages. 
Resources English Korean 
Domain category 
dictionary13 
2,612 
entries 
2,847 
entries 
General category 
dictionary14 
63,121 
entries 
67,280 
entries 
QT grammar 56 entries 14 entries 
AV-TYPE 
grammar 
96 entries 70 entries 
AV-OP grammar 94 entries 93 entries 
[Table 4.1] Resources for the training set 
For the test, we gather 779 unique queries (355 
for English and 424 for Korean) and 111 
refinement queries (19 for English and 92 for 
Korean) from the system log for about four 
months (see appendix A and B). Our system does 
not fail for the questions because of the 
LSP-based robustness15, but some SQL queries 
with wrong constraints (2.25% for English) are 
caused by undefined terms, such as ?wide TV? 
and ?voice multiplex,? and by an illegal unit such 
as ?cm? and ?mm.? 16  In Korean, the rate of 
wrong constraints rises to 5.67% that are mainly 
caused by the irregular transiterations of the 
foreign words, for example, ????,? 
?????,? ?????,? ????,? 
?????,? ?????,? ????,? and ???? 
for ?TV.? However, all the above errors can be 
easily corrected by adding new terms. This 
phenomenon is also true for multi-level 
grammars. For a new linguistic expression, we 
simply decompose it and disperse the 
components throughout the grammars. 
                                                   
13 Most of the entries are automatically extracted from target 
databases. 
14 Reuse the existing dictionaries used for open-domain text 
question answering 
15 When the system does not find any proper constraint, it 
produces a default SQL query with null constraint. 
16 Our databases use only ?inch? for the size, thus a unit 
converter needs to cover the errors. 
Conclusion 
We developed a multilingual question answering 
system on relational databases and demonstrated 
high performance and high portability in 
languages, domains, and DBMSs. LSP-based 
linguistic processing and multi-level grammars 
preserve robustness and adaptability without 
losing the precise interpretation of user queries. 
In order to overcome previous problems, 
including the discordance between attribute 
vocabulary and linguistic processing vocabulary, 
and the absence of query refinement supporting 
session-based dialog, we introduced automatic 
linguistic dictionary construction from database 
attribute terms, LSP-based linguistic processing, 
multi-level grammars, and SQL query 
refinement. 
By using lexico-semantic patterns, we separate 
language-dependent processes from the others at 
the earliest stage, and use the multi-level 
grammars to produce sophisticated 
attribute-value tree structures to connect the 
attribute vocabulary and the linguistic processing 
vocabulary. To treat the multiple domains and 
DBMSs, only SQL query generation and related 
resources are involved. This minimization of the 
environment-dependent parts enables our system 
to be widely ported on multiple environments. 
Future works include expansion to other 
languages, including Japanese and Chinese. 
References  
[1] I. Androutsopoulos, G. Ritchie and P. 
Thanisch, ?MASQUE/SQL ? An Efficient and 
Portable Natural Language Query Interface for 
Relational Databases,? Proc. of the 6th 
International Conference on Industrial & 
Engineering Applications of Artificial 
Intelligence and Expert Systems, 1993. 
[2] I. Androutsopoulos, G. Ritchie, and P. 
Thanisch, ?Natural Language Interfaces to 
Databases ? An Introduction,? Natural 
Language Engineering, Vol. 1, No. 1, 1995. 
[3] J. Binot, L. Debille, D. Sedlock, and B. 
Vandecapelle, ?Natural Language Interfaces: 
A New Philosophy,? SunExpert Magazine, 
January, 1991. 
[4] P. Cohen, The Role of Natural Language in a 
Multimodal Interface, Technical Note 514, 
Computer Dialogue Laboratory, SRI 
International, 1991. 
[5] R. Dale, H. Moisl, and H. Somers (Eds.), ?A 
Handbook of Natural Language Processing,? 
Marcel Dekker Inc., 2000. 
[6] P. Demers, A Lexical Approach to Natural 
Language Front-end Database, 
http://www.cs.stu.ca/research/groups/NLL, 
1999. 
[7] S. Harabagiu, D. Moldovan, M. Pasca, R. 
Mihalcea, M. Surdeanu, R. Bunescu, R. Girju, 
V. Rus, and P. Morarescu, ?The Role of 
Lexico-Semantic Feedback in Open-Domain 
Textual Question-Answering,? Proc. of the 39th 
Annual Meeting and 10th Conference of the 
European Chapter, 2001. 
[8] G. Hendrix, ?Natural Language Interface 
(Panel),? Computational Linguistics, Vol. 8, 
No. 2, 1982. 
[9] H. Jung, G. Lee, W. Choi, K. Min and J. Seo, 
?A Multi-lingual Question answering System 
on Relational Databases,? Proc. of the 13th 
Conference on Hangeul and Korean 
Information Processing (Korean), 2001. 
[10] H. Kim, K. Kim, G. Lee, and J. Seo, 
?MAYA: A Fast Question-answering System 
Based on a Predictive Answer Indexer,? Proc. 
of the Workshop Open-Domain Question 
Answering, the 39th Annual Meeting of ACL, 
2001. 
[11] D. Kim, J. Cha and G. Lee, Learning 
Information Extraction Patterns for the Web 
Data Mining, Proc. of the 13th Conference on 
Hangeul and Korean Information Processing 
(Korean), 2001. 
[12] G. Lee, J. Seo, S. Lee, H. Jung, B. Cho, C. 
Lee, B. Kwak, J. Cha, D. Kim, J. Ahn, H. Kim 
and K. Kim, ?SiteQ: Engineering High 
Performance QA System Using 
Lexico-Semantic Pattern Matching and 
Shallow NLP,? Proc. of the 10th Text REtrieval 
Conference, 2001. 
[13] P. McFetridge, F. Popowich and D. Fass, 
?An Analysis of Compounds in HPSG 
(Head-driven Phrase Structure Grammar) for 
Database Queries,? Data & Knowledge 
Engineering, Vol. 20, 1996. 
[14] F. Meng and W. Chu, Database Query 
Formation from Natural Language using 
Semantic Modeling and Statistical Keyword 
Meaning Disambiguation, CSD-TR 990003, 
University of California, 1999. 
[15] A. Mikheev and S. Finch, ?Towards a 
Workbench for Acquisition of Domain 
Knowledge from Natural Language,? Proc. of 
the 7th Conference of the European Chapter of 
the Association for Computational Linguistics, 
1995. 
[16] C. Senturk, Natural Language Interfaces to 
Databases, In the course of Digital Libraries, 
E6998-003, 1997. 
[17] A. Shankar and W. Yung, gNarLI: A 
practical Approach to Natural Language 
Interfaces to Databases, Term Report, Harvard 
University, 2000. 
[18] D. Silberberg and R. Semmel, ?Role-Based 
Semantics for Conceptual-Level Queries,? 
Proc. of the 5th KRDB Workshop, 1998. 
[19] M. Templeton and J. Burger, ?Problems in 
Natural Language Interface to DBMS with 
Examples from EUFID,? Proc. of the 1st 
Conference on Applied Natural Language 
Processing, 1983. 
[20] B. Thompson and F. Thompson, ?ASK is 
Transportable in Half a Dozen Ways,? ACM 
Transactions on Office Information Systems, 
Vol. 3, No. 2, 1985. 
[21] D. Waltz, ?An English Language Question 
Answering System for a Large Relational 
Database,? Communications of the ACM, Vol. 
21, No. 7, 1978. 
[22] D. Warren and F. Pereira, ?An Efficient 
Easily Adaptable System for Interpreting 
Natural Language Queries,? Computational 
Linguistics, Vol. 8, 1982. 
[23] W. Woods, R. Kaplan, and B. Webber, The 
Lunar Sciences Natural Language Information 
System: Final Report, BBN Report 2378, Bolt 
Beranek and Newman Inc., Cambridge, 
Massachusetts, 1972. 
POSBIOTM-NER in the shared task of BioNLP/NLPBA 2004    
Yu Song, Eunju Kim, Gary Geunbae Lee, Byoung-kee Yi 
Department of CSE, 
Pohang University of Science and Technology (POSTECH)  
Pohang, Korea 790-784 
{songyu, hosuabi, gblee, bkyi} @postech.ac.kr 
 
Abstract 
Two classifiers -- Support Vector Machine 
(SVM) and Conditional Random Fields (CRFs) are 
applied here for the recognition of biomedical 
named entities. According to their different 
characteristics, the results of two classifiers are 
merged to achieve better performance. We propose 
an automatic corpus expansion method for SVM 
and CRF to overcome the shortage of the annotated 
training data. In addition, we incorporate a 
keyword-based post-processing step to deal with 
the remaining problems such as assigning an 
appropriate named entity tag to the word/phrase 
containing parentheses. 
1 Introduction 
Recently, with the rapid growth in the number of 
published papers in biomedical domain, many NLP 
(Natural Language Processing) researchers have 
been interested in a task of automatic extraction of 
facts from biomedical articles. The first and 
fundamental step is to extract the named entities. 
And recently several SVM-based named entity 
recognition models have been proposed. Lee et. al. 
([Lee et. al., 2003]) proposed a two-phrase SVM 
recognition model. Yamamoto et. al. ([Yamamoto 
et. al., 2003]) proposed a SVM-based recognition 
method which uses various morphological 
information and input features such as base noun 
phrase information, stemmed forms of a word, etc. 
However, notable limitation of SVM is its low 
speed both for training and recognition. 
On the other hand, conditional random fields 
(CRFs) ([Lafferty, 2001]) is a probabilistic 
framework for labelling and segmenting sequential 
data, which is much faster  comparing with SVM. 
The conditional probability of the label sequence 
can depend on arbitrary, non-independent features 
of the observation sequence without forcing the 
model to account for the distribution of those 
dependencies. Named entity recognition problem 
can be taken as assigning the named entity class 
tag sequences to the input sentences. We adopt 
CRF to be the complementary scheme of SVM. 
 In natural language processing, supervised 
machine-learning based approach is a kind of 
standard and its efficiency is proven in various task 
fields. However, the most problematic point of 
supervised learning methods is that the size of 
training data is essential to achieve good 
performance, but building a training corpus by 
human labeling is time consuming, labor intensive, 
and expensive. To overcome this problem, various 
attempts have been proposed to acquire a training 
data set in an easy and fast way. Some approaches 
focus on minimally-supervised style learning and 
some approaches try to expand or acquire the 
training data automatically or semi-automatically. 
Using virtual examples, i.e., artificially created 
examples, is a type of method to expand the 
training data in an automatic way ([Niyogi et al 
1998] [Sasano, 2003] [Scholkopf et. al., 1996]. In 
this paper, we propose an automatic corpus 
expansion method both for SVM and CRF based 
biological named entity recognition using virtual 
example idea. 
The remainder of this paper is organized as 
follows: Section 2 introduces named entity 
recognition (NER) part: two machine learning 
approaches with some justification, feature set 
used in NER and virtual examples generation.  In 
section 3, we present some keyword-based post-
processing methods. The experiment results and 
analysis will be presented in section 4. Finally, 
conclusion is provided in section 5.  
2 Named Entity Recognition 
The training corpus is provided in IOB notion. 
The IOB notation is used where named entities are 
not nested and therefore do not overlap. Words 
outside of named entities are tagged with ?O?, 
while the first word in a named entity is tagged 
with B-[entity class], and further named entity 
words receive tag I-[entity class] for inside. We 
define the named entity recognition problem as a 
classification problem, assigning an appropriate 
classification tag for each token in the input 
sentences.  
To simplify the classification problem, we assign 
each token only with I-[entity class]/O. Then we 
convert the tag of the initial token of a consecutive 
100
sequence of predicted named entity tokens to B-
[entity class]. 
2.1 SVM 
Support Vector Machine (SVM) is a well-known 
machine learning technique showing a good 
performance in several classification problems. 
However, SVM has suffered from low speed and 
unbalanced distributed data.     
Named entity token is a compound token that 
consists of the constituents of some other named 
entities, and all other un-related tokens are 
considered as outside tokens. Due to the 
characteristics of SVM, this unbalanced 
distribution of training data can cause a drop-off in 
classification coverage. 
In order to resolve this low coverage and low 
speed problem together, we filter out possible 
outside tokens in the training data through two 
steps. First, we eliminate tokens that are not 
constituents of a base noun phrase, assuming that 
every named entity token should be inside of a 
base noun phrase boundary. Second, we exclude 
some tokens according to their part-of-speech tags. 
We build a stop-part-of-speech tag list by 
collecting tags which have a small chance of being 
a named entity token, such as predeterminer, 
determiner, etc. 
2.2 CRF 
Conditional random fields (CRFs) ([Wallach, 
2004] is a probabilistic framework for labelling 
and segmenting a sequential data. Let ),( EVG be 
a graph such that VvYvY ?= )( , and there is a 
node Vv? corresponding to each of the random 
variable representing an element Yv of Y . Then 
),( YX  is a conditional random field, and when 
conditioned on X  , the random variables Yv  obey 
the Markov property with respect to the graph: 
),~,,|(),,||( vwYwXYvpvwYwXYvp =?  
where vw ~  means that w and v are neighbours in 
G. 
Let X  and Y  be jointly distributed random 
variables respectively representing observation 
sequences and corresponding label sequences. A 
CRF is an undirected graphical model, globally 
conditioned on X (the observation sequence). 
We try to use this CRF model to our NER as a 
complementary method for both speed and 
coverage. SVM predicts the named entities based 
on feature information of words collected in a 
predefined window size while CRF predicts them 
based on the information of the whole sentence. 
So, CRF can handle the named entities with 
outside tokens which SVM always tags as ?O?. 
2.3 Feature set 
As an input to the classifier, we use a bit-vector 
representation, each dimension of which indicates 
whether the input matches with the corresponding 
feature.  
The followings are the basic input features: 
 
z Surface word - only in the case that the 
previous/current/next words are in the 
surface word dictionary. 
z word feature - orthographical feature of 
the previous/current/next words. 
z prefix/suffix - prefixes/suffixes which are 
contained in the current word among the 
entries in the prefix/suffix dictionary. 
z part-of-speech tag - POS tag of the 
previous/current/next words. 
z Base noun phrase tag - base noun tag of 
the previous/current/next words. 
z previous named entity tag - named entity 
tag which is assigned for previous word. 
This feature is only for SVM. 
 
The surface word dictionary is constructed from 
the words that occur more than one time in the 
training part of the corpus.  
2.4 Automatic Corpus Expansion using 
Virtual Examples  
To achieve good results in machine learning 
based classification, it is important to use training 
data which is sufficient not only in the quality but 
also in the quantity. But making the training data 
by hand requires considerable man-power and 
takes a long time. Expanding the training data 
using virtual examples is an attempt for corpus 
expansion in the biomedical domain. 
We expand the training data by augmenting 
the set of virtual examples generated using some 
prior knowledge on the training data. We use the 
fact that the syntactic role of a named entity is a 
noun and the basic syntactic structure of a sentence 
is preserved if we replace a noun with another 
noun in the sentence. Based on this linguistic 
paradigmatic relation, we can generate a new 
sentence by replacing each named entity by 
another named entity which is in the named entity 
dictionary of the corresponding class. Then we 
augment the sentence into the original training data. 
If we apply this replacement processes n times for 
each sentence in the original corpus, then we can 
obtain a virtual corpus about n+1 times bigger than 
the original one. Since the virtual corpus 
strengthens the right information which may not be 
observed in the original corpus, it is helpful to 
extend the coverage of a recognition model and 
101
also helpful to improve the recognition 
performance. 
3 Keyword based post-processing 
We notice that some words occur more 
frequently in the specific entity class. For example, 
the word ?genes? appears in class DNA 590 times 
while in other entity class appears less than 10 
times. The information provided by these key 
words not only impacts the named entity prediction 
part but also shows great power in post-processing 
part. Once keywords appear at specific position in 
a named entity, we can surely decide the entity 
class of this named entity. 
3.1 Words containing parentheses or ?and? 
 It is difficult but significant to decide whether 
parentheses or ?and? are part of named entity or 
not. Parentheses occur in the named entity more 
than 700 times in the training data. Both SVM and 
CRF cannot work well while dealing with this 
problem. 
Once a specific keyword appears at the right side 
of ?)?, we can tell that the parentheses belong to a 
named entity.  The named entity tag information 
can also be determined by the keyword. For 
example, in Table 1, the left column is the result of 
the NER module. At post-processing stage, the 
word ?genes? is detected on the right side of ?)?, 
then this pair of parentheses and keyword ?genes? 
are included in the current named entity.  
 
Before After 
text tag text tag 
(        O (  I-DNA 
VH      I-DNA VH       I-DNA 
)        O   )         I-DNA 
genes    O genes      I-DNA 
    Table 1: An example for the usage of 
keywords. 
 
A keyword list for parentheses is collected from 
the training corpus, including the named entity 
tag information.  It not only solves the 
parentheses named entity tag problem but also 
helps to correct the wrong named entity assigned 
to the words between parentheses by the previous 
step. The word ?and? can be treated similarly as 
the parenthesis case.  
3.2 Correcting the wrong named entity tag 
Some keywords occur in one specific type of 
named entities with high frequency. We employ 
the information provided by those keywords in 
correcting the wrongly assigned named entity tag. 
First a list of high frequency keywords with 
class information is collected. Once a keyword is 
predicted as another type of named entity, all the 
words in the current named entity boundary will be 
corrected as the corresponding named entity type 
as the keyword. For example, the keywords 
?protein? and ?proteins?, in a very rare case, 
belong to other named entity class rather than the 
class ?PROTEIN?.  
4 Experiment Result and analysis 
4.1 Corpus  
The shared task BioNLP/NLPBA 2004 provides 
2000 MEDLINE abstracts from the GENIA ([Ohta 
et. al., 2002]) bio-named entity corpus version 3.02. 
There are total 5 entity classes: DNA, RNA, 
protein, cell_line and cell_type.  
4.2 Experiment results and analysis 
CLASS Recall/Precision/F-score 
Full R64.80   P67.82   F66.28
Left R69.99   P73.25   F71.58
ALL 
Right R73.25   P76.67   F74.92
Full R65.50   P73.04   F69.07
Left R71.26   P79.46   F75.13
Protein 
Right R72.23   P80.54   F76.16
Full R53.77   P61.40   F57.33
Left R56.39   P64.40   F60.13
Cell_Line 
Right R63.57   P72.60   F67.79
Full R58.60   P61.65   F60.08
Left R64.27   P67.61   F65.90
DNA 
Right R66.79   P70.27   F68.48
Full R65.49   P62.71   F64.07
Left R67.26   P64.41   F65.80
RNA 
Right R75.22   P72.03   F73.59
Full R70.45   P59.45   F64.48
Left R74.46   P62.83   F68.15
Cell_Type
Right R84.52   P71.32   F77.36
 Table 2: Final result of POSBIOTM-NER (with 
no abstract boundary information). 
 
Method Full:   Recall/Precision/F-score
SVM.base R62.01   P65.80   F63.85 
SVM+V R63.91   P66.89   F65.37 
CRF.base R64.90   P61.33   F63.06 
CRF+V R65.78   P61.06   F63.34 
Final R64.80   P67.82   F66.28 
Table 3: Step by step result 
102
From table 3, we can see that after using virtual 
samples, both the precision and recall increased, 
especially for SVM. In CRF, even though the full 
f-score did not increase the full F-score much, but 
for RNA class, after using virtual samples, the f-
score has increased 3%. 
A CRF has different characteristics from SVM, 
and is good at handling different kinds of data. So, 
we simply merge the results of two machine 
learning approaches, by using the CRF results to 
extend the boundaries of named entities predicted 
by SVM. After merging the results of the baseline 
of SVM and CRF (without using virtual samples) 
the f-score reaches to 64.58, while the f-score of 
SVM alone is 63.85. The final score in Table 3 is 
the merged results with the virtual samples. 
Although we have improved our system by 
using virtual samples, CRF and SVM as 
complementary means and post-processing, we 
still have some problems to solve, such as correct 
named entity boundary detection. It is more 
difficult to correctly predict the left boundary of 
named entities than the right boundary. From the 
analysis of the results, we usually predict ?human? 
and ?factor? as the beginning and end of a named 
entity, but, it is even difficult for human to decide 
correctly whether it is a part of a named entity or 
not. 
5 Conclusion and Future Works 
In this paper, we propose a general method for 
named entity recognition in the biomedical domain. 
Various morphological, part-of-speech and base 
noun phrase features are incorporated to recognise 
the named entities. We use two different kinds of 
machine learning techniques, SVM and CRF, with 
merged results. We also developed a virtual sample 
technique to overcome the training data shortage 
problem. Finally, we present a   keyword-based 
heuristic post-processing which increases both 
precision and recall. 
 As shown in the experiment results, more 
correct detection of the named entity boundary is 
required, especially the detection of left boundary. 
6 Acknowledgements 
This research is supported by BIT Fusion project 
(by MOST). 
References  
J.Lafferty, A.McCallum, and F.Pereira. 
Conditional random fields: probabilistic models 
for segmenting and labelling sequence data. In 
International Conference on Machine Learning, 
2001. 
Ki-Joong Lee, Young-Sook Hwang, and Hae-
Chang Rim. Two-phase biomedical NE 
recognition based on SVMs. Proceedings of 
ACL 2003 Workshop on Natural Language 
Processing in Biomedicine,2003. 
P.Niyogi, F.Girosi, and T.Poggio. Incorporating 
prior information in machine learning by 
creating virtual examples. Proceedings of 
IEEE volume 86, pages 2196-2207, 1998 
 
Manabu Sasano. Virtual examples for text 
classification with support vector machines. 
Proceedings of 2003 Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP 2003), 2003. 
Bernhard Scholkopf, Chris Burges, and Vladimir 
Vapnik. Incorporating invariances in support 
vector learning machines.  Artificial Neural 
Networks- ICANN96,1112:47-52,1996. 
Hanna M.Wallach. Conditional Random Fields: 
An Introduction. 2004 
T.Ohta, Y. Tateisi, J.Kim, H. Mima and 
J.Tsujiii.2002. The GENIA corpus: An 
annotated research abstract corpus in 
molecular biology domain. In Proceedings of 
Human Language Technology Conference, 
2002. 
Kaoru Yamamoto, Taku Kudo, Akihiko Konagaya, 
and Yuji Matusmoto. Protein name tagging 
for biomedical annotation in text. Proceedings 
of ACL 2003 Workshop on Natural Language 
Processing in Biomedicine, 2003. 
 
103
Using Higher-level Linguistic Knowledge for Speech Recognition Error
Correction in a Spoken Q/A Dialog
Minwoo Jeong
Department of Computer
Science and Engineering,
POSTECH, Pohang, Korea
stardust@postech.ac.kr
Byeongchang Kim
Division of Computer and
Multimedia Engineering,
Uiduk University,
Gyeongju, Korea
bckim@uiduk.ac.kr
Gary Geunbae Lee
Department of Computer
Science and Engineering,
POSTECH, Pohang, Korea
gblee@postech.ac.kr
Abstract
Speech interface is often required in many
application environments such as telephone-
based information retrieval, car navigation sys-
tems, and user-friendly interfaces, but the low
speech recognition rate makes it difficult to ex-
tend its application to new fields. Several ap-
proaches to increase the accuracy of the recog-
nition rate have been researched by error cor-
rection of the recognition results, but previ-
ous approaches were mainly lexical-oriented
ones in post error correction. We suggest
an improved syllable-based model and a new
semantic-oriented approach to correct both se-
mantic and lexical errors, which is also more
accurate for especially domain-specific speech
error correction. Through extensive experi-
ments using a speech-driven in-vehicle telem-
atics information retrieval, we demonstrate
the superior performance of our approach and
some advantages over previous lexical-oriented
approaches.
1 Introduction
New application environments such as telephone-based
retrieval, car navigation systems, and mobile information
retrieval, often require speech interface to conveniently
process user queries. In these environments, keyboard
input is inconvenient or sometimes impossible because
of spatial limitation on mobile devices and instability in
manipulating the devices.
However, because of the low recognition rate in current
speech recognition systems, the performance of speech
applications such as speech-driven information retrieval
(IR) and question answering (QA), and speech dialogue
systems is very low. The performance of the serially con-
nected spoken QA system, based on the QA system from
text input which has 76% performance and the output of
the ASR which operated at a 30% WER, was only 7%
(Harabagiu et al, 2002). (Harabagiu et al, 2002) ex-
poses several fundamental flaws of this simple combina-
tion of an automatic speech recognition (ASR) and QA
system, including the importance of named entity infor-
mation, and the inadequacies of current speech recogni-
tion technology based on n-gram language models.
The major problem of speech-driven IR and QA is the
decreasing of the performance due to the recognition er-
rors in ASR systems. Erroneously recognized spoken
queries drop the precision and recall of IR and QA sys-
tem. Some authors investigated the relation of ASR er-
rors and precision of IR (Barnett et al, 1997; Crestani,
2000). They evaluated the effectiveness of the IR systems
through various error rates using 35 queries of TREC.
Their researches show that the increasing word error rate
(WER) quickly decreases the precision of IR. Another
group investigated the performance of spoken queries in
NTCIR collections (Fujii et al, 2002A). They evaluated
a variety of speakers, and calculated the error rate with
respect to a query term, which is a keyword used for the
retrieval. They showed that the WER of the query terms
was generally higher than that of the general words ir-
respective of the speakers. In other words, recognition
of content words related to the IR and QA performance
was more difficult than that of normal words. So, they
introduced a method to improve the precision of speech-
driven IR by suggesting a new type of IR system tightly-
integrated with a speech input interface (Fujii et al,
2002B). In their system, document collection provides an
adaptation of the language model of the ASR, which re-
sults in a drop of the word error rate.
For this reason, some appropriate adaptation tech-
niques are required for overcoming speech recognition
errors such as post error correction. ASR error correc-
tion can be one of the domain adaptation techniques to
improve the recognition accuracy, and the primary advan-
Figure 1: Adaptation via Post Error Correction
tage of the error correction approach is its independence
of the specific speech recognizer. If the speech recog-
nizer can be regarded as a black-box, we can perform ro-
bust and flexible domain adaptation through the post error
correction process. Figure 1 shows the paradigm of this
post error correction approach.
One approach in post error correction, which is a
straightforward and intuitive method to robustly handle
many kinds of recognition errors, was rule-based ap-
proach (Kaki et al, 1998). (Kaki et al, 1998) collected
many lexical error patterns that occurred in a speech
translation system in Japanese. They could correct any
type of errors by matching the strings in the transcription
with lexical error patterns in the database. However, their
approach has a disadvantage in that the correction is only
feasible to the trained (or collected) lexical error patterns.
Another approach has been based on a statistical
method utilizing the probabilistic information of words
in a spoken dialogue situation and the language models
adapted to the application domain (Ringger and Allen,
1996). (Ringger and Allen, 1996) applied the noisy chan-
nel model to the correction of the errors in speech recog-
nition. They simplified a statistical machine translation
(MT) model called an IBM model (Brown et al, 1990),
and tried to construct a general post-processor that can
correct errors generated by any speech recognizer. The
model consists of two parts: a channel model, which ac-
counts for errors made by the ASR, and the language
model, which accounts for the likelihood of a sequence of
words being uttered. They trained the channel model and
the language model both using some transcriptions from
TRAINS-95 dialogue system which is a train traveling
planning system (Allen et al, 1996). Here, the channel
model has the distribution that an original word may be
recognized as an erroneous word. They use the proba-
bility of mistakenly recognized words, the co-occurrence
information extracted from the words and their neighbor-
ing words, and the tagged word bi-grams, which are all
lexical clues in error strings.
Such approaches based on lexical information of words
have shown some successful results, but they still have
major drawbacks; The performance of such systems de-
pends on the size and the quality of speech recognition
result, or on the database of collected error strings since
they are directly dependent on lexical items. The error
patterns constructed are available but not enough, be-
cause it is expensive to collect them; so in many cases,
they fail to recover the original strings from the lexical
specific error patterns. Also, since they are sensitive to
the error patterns, they occasionally mis-identify a cor-
rect word as an error word.
We suggest a more improved and robust semantic-
oriented error correction approach, which can be in-
tegrated into previous fragile lexical-based approaches.
In our approach, in addition to lexical information, we
use high level syntactic and semantic information of the
words in a speech transcription. We obtain semantic in-
formation from a knowledge base such as general the-
sauri and a special domain dictionary that we construct
by ourselves to contain some domain specific knowledge
to the target application.
In the next section, we first describe a general noisy
channel model for ASR error correction and discuss some
problems with them. We then introduce our improved
channel model especially for Korean language in section
3. We also propose a new high-level error correction
model using syntactic and semantic knowledge in section
4. We prove the feasibility of our approach through some
experiments in section 5, and draw some conclusions in
section 6.
2 Noisy Channel Error Correction Model
The noisy channel error correction framework has been
applied to a wide range of problems, such as spelling
correction, statistical machine translation, and ASR error
correction (Brill and Moore, 2000; Brown et al, 1990;
Ringger and Allen, 1996). The key idea of noisy chan-
nel model is that we can model some channel properties
through estimating the posterior probabilities.
The problem of ASR error correction can be stated
in this model as follows: For an input sentence, O =
o1, o2, . . . , on produced as the output sequence of ASR,
find the best word sequence,W? = w1, w2, . . . , wn, that
maximizes the posterior probability P (W |O). Then, ap-
plying Bayes? rule and dropping the constant denomina-
tor, we can rewrite as:
W? = arg max
W
P (W |O) = arg max
W
P (W )P (O|W ) (1)
Now, we have a noisy channel model for ASR er-
ror correction, with two components, the source model
P (W ) and the channel model P (O|W ). The probability
P(W) is given by the language model and can be decom-
posed as:
P (W ) =
?
i
P (wi|w1,i?1) (2)
Figure 2: Example of Word-based Channel Model
The distribution P (W ) can be defined using n-grams,
structured language model (Chelba, 1997), or any other
tool in the statistical language modeling.
Next, the conditional probability, P (O|W ) reflects the
channel characteristics of the ASR environment. If we as-
sume that the output word sequence produced under ASR
are independent of one another, we have the following
formula:
P (O|W ) =
?
i
P (o1,i|w1,i) =
?
i
P (oi|wi) (3)
So,
W? = arg max
W
P (W )P (O|W )
= arg max
W
(
?
i
P (wi|w1,i?1)
?
i
P (oi|wi))(4)
However, this simple one-to-one model is not suitable
to handling split or merged errors, which frequently ap-
pear in an ASR output, because we assume that the out-
put word sequence are independent of one another. For
example, 1figure 2 shows a split or a merged error prob-
lem. To solve this problem, Ringger and Allen used the
fertility of pre-channel word (Ringger and Allen, 1996).
Following (Brown et al, 1990), we refer to the num-
ber of post-channel words oi produced by a pre-channel
word wi as a fertility. They simplified the fertility model
of IBM statistical MT model-4, and permitted the fer-
tility within 2 windows such as P (oi?1, oi|wi) for two-
to-one channel probability, and P (oi|wi, wi+1) for one-
to-two channel probability. So, the fertility model can
deal with (TO LEAVE, TOLEDO) substitution. But this
improved fertility model only slightly increased the ac-
curacy in experiments (Ringger and Allen, 1996), and
we think the major reason is due to the data-sparseness
problem. Because substitution probability is based on the
whole word-level, this fertility model requires enormous
training data. We call the model a word-based channel
model, because this model is based on the word-to-word
transformation. The word-based model focused on inter-
word substitutions, so it requires enough results of ASR
and transcription pairs. Considering the cost of building
the enough amount of correction pairs, we need a smaller
unit than a word for overcoming the data-sparseness.
1This example is from (Ringger and Allen, 1996).
3 Syllable-based Channel Model
We suggest an improved channel model for smaller train-
ing data. If we can use smaller unit such as letter,
phoneme or syllable than word, relatively smaller training
set is needed. For dealing with intra-word transformation,
we suggest a syllable-based channel model, which can
deal with syllable-to-syllable transformation. This model
is especially reasonable for Korean. In some agglutina-
tive languages such as Korean, syllable is a basic unit of
written form like a Chinese character. In Korean, the av-
erage number of syllables in one word is about three or
four.
3.1 The Model
Suppose S = s1, s2, . . . , sn is a syllable sequence of
ASR output and W = w1, w2, . . . , wm is a source word
sequence, then our purpose is to find the best word se-
quence W? as follows:
W? = arg max
W
P (W |S) (5)
We can apply the same Bayes? rule and decompose the
syllable-to-word channel model into syllable-to-syllable
channel model.
P (w|s) = P (s|w)P (w)P (s) ? P (s|w)P (w)
? P (s|x)P (x|w)P (w) (6)
So, final formula can be written as:
W? = arg max
W
(P (W )P (X|W )P (S|X)) (7)
Here, P (S|X) is the probability of a syllable-to-
syllable transformation, where X = x1, x2, . . . , xn is
a source syllable sequence. P (X|W ) is a word model,
which can convert syllable lattice into word lattice. The
conversion can be done efficiently by dictionary look-up.
This model is similar to a standard hidden markov
model (HMM) of continuous speech recognition. In
speech recognition system, P (S|X) can be an acoustic
model in signal-to-phoneme level, and P (X|W ) can be
a pronunciation dictionary. Then, we applied the fertility
into our syllable-to-syllable channel model. We set the
maximum 2-fertility of syllable, which was determined
experimentally.
3.2 Training the Model
To train the model, we need a training data consisting
of {X,S} pairs which are manually transcribed strings
and ASR outputs. And, we align the pair based on mini-
mizing the edit distance between xi and si by dynamic
Figure 3: Example of Syllable-based Channel Model
programming. 2Figure 3 shows an alignment for the
syllable-model (For understanding, we use an English ex-
ample and a letter-to-letter alignment. In Korean, each
syllable is clearly distinguished much like a letter in En-
glish.). For example, (TO LEAVE, TOLEDO) pair in pre-
vious section can be divided into (TO, TO), (L, L), (EA,
E), and (VE, DO) with fertility 2.
We can then calculate the probability of each sub-
stitution P (si|xi) by Maximum-Likelihood Estimation
(MLE). Let C(xi) be the frequency of source syllable,
and C(xi, si) be the frequency of events where xi substi-
tute si. Then,
PMLE(si|xi) =
C(xi, si)
C(xi)
(8)
The total number of theoretical unique syllables is
about ten thousands in Korean, but the number of syl-
lables, which appeared at least one time, is about 2,300
in a corpus which has about 3 billion syllables. Thus, we
used Witten-Bell method for smoothing unseen substitu-
tions (Witten and Bell, 1991). Let T (xi) be the number
of substitution types, and N be the number of syllables in
a training data. For Witten-Bell discounting, we should
define Z(xi), which is the number of syllable xi with
count zero. Then, we can write as follows:
PWB(si|xi) =
T (xi)
Z(xi)(N + T (xi))
, if C(xi, si) = 0 (9)
3.3 Decoding the Model
Given a syllable sequence S, we want to find
arg maxW (P (W )P (X|W )P (S|X)). This will be to re-
turn an N-best list of candidates according to the models,
and then rescore these candidates by taking into account
the language model probabilities. To rescore the candi-
dates, we used Viterbi search algorithm to find the best
sequence. For implementation of candidate generation,
we store the syllable channel probabilities P (si|xi) as a
hash-table to pop them easily and fast. The system can
generate a candidate word sequence network using sylla-
ble channel model and a lexicon. And then, we can find
optimal sequence which has the best probability through
Viterbi decoding by including a language model.
2We omitted detail character-level match lines to simplify.
The whole word match is depicted in bold lines, while no-line
means character-level match errors.
Figure 4: Common semantic category values
4 Using Syntactic and Semantic
Knowledge
In some similar areas such as spelling error correction
or optical character recognition (OCR) error correction,
NLP researchers traditionally identified five levels of er-
rors in a text: (1) a lexical level, (2) a syntactic level,
(3) a semantic level, (4) a discourse structure level, and
(5) a pragmatic level (Kukich, 1992). In spelling cor-
rection and OCR error correction problem, correction
schemes mainly have focused on non-word errors at the
lexical level, which is an isolated word correction prob-
lem. However, errors of speech recognition tend to be
continuous word errors which should be better classi-
fied into syntactic and semantic level errors, because the
recognizer only produces word sequences existing in a
lexicon. So, this section presents a more syntax and
semantic-oriented approach to correct erroneous outputs
of a speech recognizer using a domain knowledge which
provides syntactic and semantic information. We fo-
cus on continuous word error detection and correction,
using syntactic and semantic knowledge, and pipeline
this high-level error correction method with the syllable-
based channel model.
4.1 Lexico-Semantic Pattern
A lexico-semantic pattern (LSP) is a structure where lin-
guistic entries and semantic types are used in combina-
tion to abstract certain sequences of the words in a text.
It has been used in the area of natural language interface
for database (NLIDB) (Jung et al, 2003) and a TREC
QA system for the purpose of matching the user query
with the appropriate answer types at syntax/semantic
level (Kim et al, 2001; Lee et al, 2001). In an LSP,
linguistic entries consist of words, phrases and part-of-
speech (POS) tags, such as ?YMCA,? ?Young Men?s
Christian Association,? and ?NNP.?3 Semantic types con-
3Part-of-speech tag denoting a proper noun which is used in
Penn TreeBank (Marcus et al, 1994).
Phrases LSP
Reading trainer
Fairy tale trainer %hobby @position
Recreation coach
Table 1: Example of a template abstracted by LSP
sist of common semantic classes and domain-specific (or
user-defined) semantic classes. The common semantic
tags again include attribute-values in databases, such as
?@corp? for a company name like ?IBM,? and pre-define
83 semantic category values, such as ?@location? for lo-
cation names like ?New York? (Jung et al, 2003). Fig-
ure 4 shows an example of predefined common semantic
category values which will be used in an ontology dictio-
nary.
In domain-specific application, well defined semantic
concepts are required, and the domain-specific seman-
tic classes represent these requirements. The domain-
specific semantic classes include special attribute names
in databases, such as ?%action? for ?active? and ?inac-
tive,? and semantic category names, such as ?%hobby? for
?reading? and ?recreation,? for which the user wants a spe-
cific meaning in the application domain. Moreover, we
used the classes to abstract out several synonyms into a
single concept. For example, a domain-specific semantic
class ?%question? represents some words, such as ?ques-
tion?, ?query?, ?asking?, and ?answer.?
The domain dictionary is a subset of the general se-
mantic category dictionary, and focuses only on the nar-
row extent of the knowledge it concerns, since it is im-
possible to cover all the knowledge of the world in imple-
menting an application. On the other hand, the ontology
dictionary for common semantic classes reflects the pure
general knowledge of the world; hence it performs a sup-
plementary role to extract semantic information. The do-
main dictionary provides the specific vocabulary which is
used in semantic representation tasks of a user query and
the template database.
4.2 Construction of a Domain Knowledge
For semantic-oriented error correction, we constructed a
domain knowledge, which consists of a domain dictio-
nary, an ontology dictionary, and template queries that
are similar to question types in a QA system (Lee et
al., 2001). Query sentences are semantically abstracted
by LSP?s and are automatically collected for the template
database.
Because Fujii et al (Fujii et al, 2002B) have shown
the importance of the language model which well de-
scribes the domain knowledge, we reflect the domain
information with a template database: database of tem-
plate queries of the source statements which are used
Figure 5: Process of Semantic-oriented Error Correction
for the actual error detection and correction task after
speech recognition. The template queries are automati-
cally acquired by the Query-to-LSP translation from the
source statements using two semantic category dictionar-
ies: domain dictionary and an ontology dictionary. As-
suming that some speech statements for a specific target
domain are predefined, a record of the template database
is composed of a fixed number of LSP elements, such as
POS tags, semantic tags, and domain-specific semantic
classes. Table 1 shows an example of template abstracted
by LSP conversion in a predefined domain of ?on-line ed-
ucation.?
Query-to-LSP translation transforms a given query into
a corresponding LSP, and the LSP?s enhance the cov-
erage of extraction by information abstraction through
many-to-one mapping between queries and an LSP. The
words in a query sentence are converted into the LSP
through several steps. First, a morphological analysis is
performed, which segments a sentence of words into mor-
phemes, and adds POS tags to the morphemes (Lee et al,
2002). NE recognition discovers all the possible seman-
tic types for each word by consulting a domain dictionary
and an ontology dictionary. NE tagging selects a seman-
tic type for each word so that a sentence can be mapped
into a suitable LSP sequence by searching several types
in the semantic dictionaries (An et al, 2003).
4.3 Semantic-oriented Error Correction Process
Now, we will show the working mechanism of post error
correction of a speech recognition result using the domain
knowledge of template database and domain-specific dic-
tionary. Figure 5 is a schematic diagram of the post error
correction process.
The overall process is divided into two stages: a syn-
tactic/semantic recovery and a lexical recovery stage. In
the semantic error detection stage, a recognized query is
converted into the corresponding LSP. The converted LSP
may be ill-formed depending on the errors in the rec-
ognized query. Semantic error correction is performed
by replacing these syntactic and/or semantic errors us-
ing a semantic confusion table. We used a pre-collected
template database to recover the semantic level errors,
and the technique for searching most similar templates
are based on a minimum edit distance dynamic program-
ming search, which has been used as a similarity search
in many areas such as spelling correction, OCR post cor-
rection, and DNA sequence analysis (Wagner and Fis-
cher, 1974). The semantic confusion table provides the
matching cost, which can be semantic similarity, to the
dynamic programming search process. The ?minimum
edit distance? between two words is originally defined as
the minimum number of deletions, insertions, and sub-
stitutions required to transform one word into the other.
We compute the minimum edit distances between the er-
roneous LSP?s and the template LSP?s in the template
database using the similarity cost functions at the seman-
tic level, and select, as the final template query, the one
which has the minimum distance among them. At this
stage, replaced LSP elements can provide some clues of
the recognition errors and the original query?s meaning
to the next lexical recovery stage. Moreover, candidate
error boundary can also be detected by this procedure.
After this procedure, lexical recovery is performed in
the next stage. Recovered semantic tags and the erro-
neous queries produced by ASR are the clues of lexi-
cal recovery. Erroneous query and recovered template
query are aligned by dynamic programming again, after
which some lexical candidates are generated by our im-
proved syllable-based channel model. Figure 6 4 shows
an example of semantic error correction process using the
same data in TRAIN-95 (Allen et al, 1996).
5 Experiments
5.1 Experimental Setup
We performed several experiments on the domain of in-
vehicle telematics IR related to navigation question an-
swering services. The speech transcripts used in the ex-
periments were composed of 462 queries, which were
collected by 1 male speaker in a real application. We
also used two Korean speech recognizers: a speech rec-
ognizer made by LG-Elite (LG Electronics Institute of
Technology) and a Korean commercial speech recog-
nizer, ByVoice (refer to http://www.voicetech.co.kr). For
4In corrected sentence, note that word ?A? is not recovered
because this word is meaningless functional word.
Figure 6: Example of Semantic-oriented Error Correction
our semantic-oriented error correction, we constructed a
domain knowledge for our target domain. We constructed
3,195 entries of domain dictionary, 13,154 entries of on-
tology dictionary, and 436 semantic templates generated
automatically using domain dictionary and ontology dic-
tionary.
We implemented both word-based and syllable-based
model for comparison, and combined the system of
syllable-based lexical correction with the LSP-based se-
mantic error correction. For experiments, we use trigrams
language model generated by SRILM toolkit (Stolcke,
2002), and a training program for channel model made
by ourselves. And, we divided the 462 queries into 6 dif-
ferent sets, and evaluated the results of 6-fold cross vali-
dation for each model.
5.2 Results
To measure error correction performance, we use word
error rate (WER) and term error rate (TER):
WER = |Sw| + |Iw| + |Dw||Wtruth|
(10)
TER = |St| + |It| + |Dt||Ttruth|
(11)
|Wtruth| is the number of original words, and |Ttruth|
is the number of query term (or keyword) in original
words, that is, an error rate of content words directly re-
lated to the performance of IR and QA system (Fujii et
al., 2002A).
Table 2, 3 present the experiments results of WER of
baseline ASR, word-based channel model, our syllable-
based channel model and combined syllable-based chan-
nel model with the LSP semantic correction model. The
performances of baseline systems were about 79% ?
81% on the utterances in in-vehicle telematics IR domain.
This result shows that the semantic error correction of
Test set 1 2 3 4 5 6 AVG.
Baseline 18.1% 21.6% 19.4% 22.8% 19.9% 19.2% 20.17%
Word-based 12.8% 20.3% 15.5% 17.5% 16.7% 17.7% 16.75%
Syllable-based 10.6% 16.0% 11.5% 16.1% 14.0% 10.6% 13.13%
Syllable + LSP 9.7% 14.9% 10.4% 15.3% 13.0% 10.7% 12.33%
Table 2: Result of LG-Elite Recognizer
Test set 1 2 3 4 5 6 AVG.
Baseline 20.5% 18.8% 19.8% 17.0% 16.9% 17.8% 18.47%
Word-based 19.9% 14.8% 18.4% 16.2% 15.3% 15.1% 16.75%
Syllable-based 16.7% 13.8% 17.0% 13.3% 12.7% 12.2% 14.28%
Syllable + LSP 15.3% 13.4% 15.8% 12.9% 11.3% 11.8% 13.42%
Table 3: Result of ByVoice
speech recognition result is a viable approach to improve
the performance.
Using both baseline ASR systems, we achieved 39%
and 27% of error reduction rate. In comparison with the
previous word-based model, our new approaches have
more accurate error correction performance in this do-
main. Table 4 shows the result of the experiments for
TER. The result of TER shows that baseline ASR systems
alone are not appropriate to process the user?s queries in
speech-driven IR, QA or dialog understanding system.
However, with a post error correction, the error reduc-
tion rate of TER is much higher than that of WER. And
we achieved better performance than word-based model.
With this result, our methods are considered to be more
appropriate in speech-driven IR and QA applications.
Compared with the word-based noisy channel model that
has been the best approach in the error correction so far,
our semantic-oriented error correction suggests alterna-
tive more successful methods for speech recognition error
correction.
Baseline Word-
based
Syllable-
based
Syllable
+ LSP
LG-Elite 56.4 % 31.5% 30.1% 26.7%
ByVoice 64.1% 34.1% 32.8% 27.6%
Table 4: Result of Term Error Rate
6 Conclusion and Future Works
We proposed an improved syllable-based noisy channel
model and combined higher level linguistic knowledge
for semantic-oriented approach in a speech recognition
error correction, which shows a superior performance in
domain-specific IR applications.
The previous works only focused on inter-word level
error correction, commonly depending on a large amount
of training corpus for the error correction model and the
language model. So, previous approaches require enor-
mous results of ASR and are dependent on specific speak-
ers and environments. On the other hand, our method
takes in far smaller training corpus, and it is possible to
implement the method easily and in a short time to ob-
tain the better error correction rate because it utilizes the
semantic information of the application domain.
And our semantic-oriented approach has more advan-
tages over lexical based ones, since it is less sensitive to
each error pattern. Also, the approach has a broader cov-
erage of error patterns, since several similar common er-
ror strings in the semantic ground can be reduced to one
semantic error pattern, which enables us to improve the
probability of recovering from erroneous recognition re-
sults.
And, because the LSP scheme transforms pure lexical
entries into abstract semantic categories, the size of the
error pattern database can be reduced remarkably, and
it also increases the coverage and robustness compared
with the previous pure lexical entries that can only deal
with the morphological variants.
With all these facts, the LSP correction has a high
possibility of generating semantically correct correction
due to the massive use of semantic contexts. Hence, it
shows a high performance, especially when combined
with domain-specific speech-driven natural language IR
and QA systems.
Future work should include the end-performance ex-
periments with IR or QA application for our error correc-
tion model.
7 Acknowledgements
This work was partly supported by Jungki KeoJeom
Project (MOCIE, ITEP), and by 21C Frontier Project
(MOST).
References
James F. Allen, Bradford W. Miller, Eric K. Ringger, and
Teresa Sikorski. 1996. A Robust System for Natural
Spoken Dialogue. In Proceedings of the 34th Annual
Meeting of the ACL
Juhui An, Seungwoo Lee, and Gary Geunbae Lee. 2003.
Automatic acquisition of Named Entity tagged corpus
from World Wide Web. In Proceedings of the 41st an-
nual meeting of the ACL (poster presentation).
J. Barnett, S. Anderson, J. Broglio, M. Singh, R. Hud-
son, and S.W. Kuo. 1997. Experiments in spoken
queries for documents retrieval. In Proceedings of Eu-
rospeech, (3):1323-1326.
Eric Brill and Robert C. Moore. 2000. An Improved
Error Model for Noisy Channel Spelling Correction.
ACL2000, 286-293.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P.
S. Roossin. 1990. A Statistical Approach to Machine
Translation. Computational Linguistics, 16(2):79-85
Ciprian Chelba. 1997. A Structured Language Model. In
Proceedings of the Thirty-Fifth Annual Meeting of the
ACL and Eighth Conference of the European Chapter
of the ACL, 498-503.
F. Crestani. 2000. Word recognition errors and relevance
feedback in spoken query processing In Proceedings
of the 2000 Flexible Query Answering Systems Confer-
ence, 267-281.
Atsushi Fujii, Katunobu Itou, and Tetsuya Ishikawa.
2002A. Speech-driven Text Retrieval: Using Target
IR Collections for Statistical Language Model Adapta-
tion in Speech Recognition. Anni R. Coden and Eric
W. Brown and Savitha Srinivasan (Eds.) Information
Retrieval Techniques for Speech Application (LNCS
2273), 94-104.
Atsushi Fujii, Katunobu Itou, and Tetsuya Ishikawa.
2002B. A method for open-vocabulary speech-driven
text retrieval. In Proceedings of the 2002 conference
on Empirical Methods in Natural Language Process-
ing, 188-195.
Sanda Harabagiu, Dan Moldovan, and Joe Picone. 2002.
Open-Domain Voice-Activated Question Answering.
COLING2002, (1):321-327, Taipei.
Hanmin Jung, Gary Geunbae Lee, Wonseug Choi,
KyungKoo Min, and Jungyun Seo. 2003. Multi-
lingual question answering with high portability on re-
lational databases. IEICE transactions on information
and systems, E-86D(2):306-315.
Satoshi Kaki, Eiichiro Sumita, and Hitoshi Iida. 1998.
A Method for Correcting Speech Recognition Using
the Statistical features of Character Co-occurrence.
COLING-ACL?98, 653-657.
Haksoo Kim, Kyungsun Kim, Gary Geunbae Lee, and
Jungyun Seo. 2001. MAYA: A Fast Question-
Answering System Based on a Predictive Answer In-
dexer. In Proceedings of the 39th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?01), Workshop on Open-Domain Question An-
swering
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys,
24(4):377-439.
Geunbae Lee, Jungyun Seo, Seungwoo Lee, Hanmin
Jung, Bong-Hyun Cho, Changki Lee, Byung-Kwan
Kwak, Jeongwon Cha, Dongseok Kim, JooHui An,
Harksoo Kim, and Kyungsun Kim. 2001. SiteQ: Engi-
neering High Performance QA System Using Lexico-
Semantic Pattern Matching and Shallow NLP. In Pro-
ceedings of the 10th Text Retrieval Conference (TREC-
10), Washington D.C.
Gary Geunbae Lee, Jeongwon Cha, and Jong-Hyeok
Lee. 2002. Syllable pattern-based unknown mor-
pheme segmentation and estimation for hybrid part-of-
speech tagging of Korean. Computational Linguistics,
28(1):53-70.
Mitchell P. Marcus and Beatrice Santorini and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313-330.
Eric K. Ringger and James F. Allen. 1996. A fertility
model for post correction of continuous speech recog-
nition ICSLP?96, 897-900.
Andreas Stolcke 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of Intl. Conf.
on Spoken Language Processing, (2):901-904, Denver,
Co. (http://www.speech.sri.com/projects/srilm/)
Robert A. Wagner and Michae J. Fischer. 1974. The
String-to-String Correction Problem. Journal of the
ACM, 21(1):168-173.
I. Witten and T. Bell. 1991. The Zero-Frequency Prob-
lem: Estimating the Probabilities of Novel Events in
Adaptive Text Compression. In IEEE Transactions on
Information Theory, 37(4).
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 120?127,
Columbus, June 2008. c?2008 Association for Computational Linguistics
A Frame-Based Probabilistic Framework for Spoken Dialog Manage-
ment Using Dialog Examples 
 
 
Kyungduk Kim, Cheongjae Lee, Sangkeun Jung and Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science & Technology (POSTECH) 
San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea 
{getta, lcj80, hugman, gblee}@postech.ac.kr 
 
 
 
 
 
 
Abstract 
This paper proposes a probabilistic framework 
for spoken dialog management using dialog 
examples. To overcome the complexity prob-
lems of the classic partially observable Mar-
kov decision processes (POMDPs) based 
dialog manager, we use a frame-based belief 
state representation that reduces the complexi-
ty of belief update. We also used dialog ex-
amples to maintain a reasonable number of 
system actions to reduce the complexity of the 
optimizing policy. We developed weather in-
formation and car navigation dialog system 
that employed a frame-based probabilistic 
framework. This framework enables people to 
develop a spoken dialog system using a prob-
abilistic approach without complexity prob-
lem of POMDP. 
1 Introduction 
A robust dialog manager is an essential part of 
spoken dialog systems, because many such sys-
tems have failed in practice due to errors in speech 
recognition. Speech recognition errors can be 
propagated to spoken language understanding 
(SLU), so the speech input must be considered er-
ror-prone from a standpoint of dialog management. 
Therefore robust dialog managers are necessary to 
develop practical spoken dialog systems. 
One approach to dialog management uses the 
partially observable Markov decision process 
(POMDP) as a statistical framework, because this 
approach can model the uncertainty inherent in 
human-machine dialog (Doshi and Roy, 2007). 
The dialog manager uses a probabilistic, rather 
than deterministic, approach to manage dialog. As 
more information becomes available, the dialog 
manager updates its belief states. A POMDP-based 
dialog manager can learn the optimized policy that 
maximizes expected rewards by reinforcement 
learning. 
But applying classic POMDP to a practical di-
alog system incurs a scalability problem. The com-
putational complexity of updating belief states and 
optimizing the policy increases rapidly with the 
size of the state space in a slot-filling dialog task. 
To solve this scalability problem, the method of 
compressing states or mapping the original state 
space to summarized space can be used (Williams 
and Young, 2006; Roy et al,2005), but these algo-
rithms tend to approximate the state space exces-
sively. The complexity problem of POMDP comes 
from updating beliefs that are out of the user?s in-
tention, and from calculating the reward of system 
actions that do not satisfy user?s objective. 
In this paper, we propose a new probabilistic 
framework for spoken dialog management using 
dialog examples. We adopted a frame-based belief 
state representation to reduce the complexity of 
belief update. Furthermore, we used an example-
based approach to generate only a reasonable 
number of system action hypotheses in a new 
framework. We developed a dialog system by us-
ing our new framework in weather information 
service and car navigation service. 
120
2 Overview 
We try to address two problems of applying 
POMDP to slot-filling dialog management. 1) 
Computational complexity of belief update: it is 
difficult to maintain and update all belief states at 
every turn of dialog since there are too many di-
alog states in slot-filling dialog tasks. 2) Computa-
tional complexity of policy optimizing: optimizing 
complexity depends on both the space size of di-
alog states, and the number of available machine 
actions. In slot-filling dialog tasks, a system action 
can have various slot values so that the system 
needs to choose an action among a large number of 
action hypotheses. 
In our new probabilistic framework (Figure 1), 
we try to solve these problems. Our approach uses 
1) the frame-based belief state representation to 
solve the computational complexity problem of 
belief update and 2) the dialog examples to gener-
ate action hypotheses to solve the computational 
complexity of policy optimizing by reducing the 
number of system action hypotheses. First, the sys-
tem groups belief states dynamically using frame-
based belief state representation according to us-
er?s utterance and its SLU result. Then the system 
uses an example-based approach to generate only 
system action hypotheses that are suitable for cur-
rent belief states. If there are too many hypotheses 
for calculating expected utility, the system prunes 
them away until only a reasonable number of hy-
potheses remains. The following describes the de-
tails of each system?s component and the dialog 
managing process. 
User?s Utterance
SLU Result
Frame-bas d Belief 
St te Re r sentation
Dialog 
Example DB
Calculating
Utilities
System action
User?s Intention,
Semantic Frame,
Dialog History
Pruning 
Hypotheses
Lexico-semantic 
Similarity
Generating
Hypotheses
 
Figure 1. Overview of the system operation. Bold ar-
rows indicate the control flow. Thin arrows indicate the 
data flow.  
3 Frame-based Belief State Representation 
We assumed that the machine?s internal represen-
tation of the dialog state sm consists of three com-
ponents: user?s goal su, user?s last action au and 
dialog history sd. This section briefly describes the 
basic introduction of POMDP framework and ex-
plains each component of machine?s internal state 
in the standpoint of our frame-based probabilistic 
framework. 
3.1 POMDP for spoken dialog management 
A POMDP is defined as a tuple that consists of six 
substates: (S, A, P, R, ?, O) where S is a set of 
state, A is a set of action, P is a transition proba-
bility P(s?|s,a), R is a reward function R(s,a,s?), ? 
is a set of observation and O is an observation 
model P(o|s,a). The current state is not determinis-
tic in a POMDP framework while it is determined 
as a specific state in a Markov decision process 
(MDP) framework. In a POMDP, the probability 
distribution over all states s?S, which is referred 
as a belief state b(s), is maintained instead of de-
terministic state. At each time instant t, the system 
chooses an action a?A, and this causes the system 
to move from current state s to next state s? with 
the transition probability P(s? |s,a). Then, the sys-
tem is granted a reward R(s,a) while the system 
receives an observation o with probability of 
P(o|s?,a). The system computes the belief state in 
the next time instance b?(s?) as a following: 
 
? ????? s sbassPasoPksb )(),|(),|()(  
 
where k is a normalizing factor. This process is 
referred as belief update. 
Optimizing a POMDP policy is a process of 
finding a mapping function from belief states to 
actions that maximizes the expected reward. The 
system should compute a value function over be-
lief spaces to find optimized actions. However, 
unlike as in a MDP, each value in a POMDP is a 
function of an entire probability distribution and 
belief spaces are very complex, so that a POMDP 
has a scale problem of computing the exact value 
function. 
A POMDP for spoken dialog system is well 
formulated in (Williams and Young, 2007). First, a 
state s can be factored to three substates: (su, au, sd) 
121
where su is a user goal state, au is a user action, and 
sd is a dialog history. A system action am and user 
action au can be cast as action a and observation o 
respectively. With some independence assumption 
between variables, the belief update equation can 
be rewritten as following: 
 
,),,(           
),,|(),|(           
),|()|
~
(         
),,()(
?
? ?
???
?????
?????
???????
u
u d
a
duu
s s
mdudmuu
muuuu
duu
sasb
asasPassP
asaPaaPk
sasbb
 
where 
ua~?  is an automatic speech recognizer (ASR) 
and SLU recognition result of user action. In our 
framework, belief update is done based on this eq-
uation. But applying this directly to a spoken di-
alog system can have a problem because the 
probabilities used in the equation are hard to esti-
mate from the corpus due to the data sparseness. 
Therefore, we adopted Young?s (2007) belief up-
date formula that is simplified from the original 
equation. 
3.2 User goal state 
In a slot-filling dialog system, the user?s goal can 
be represented as a fully-filled frame in which all 
slots of the frame contain values specified by the 
user?s intention. Therefore, if a dialog system has 
W slots and each slot can have a value among V 
candidates, then VW user goals can be represented 
as frames. This means that the number of user 
goals is related exponentially to the number of 
slots. This number of user goals is intractable in 
practical dialog systems. 
Therefore, a method is needed to reduce the size 
of the state space rather than maintaining all belief 
states. To do this, we developed a frame-based be-
lief state representation in which the system dy-
namically groups set of equivalent states to a high-
level frame state. Frame state, which is a similar 
concept to the partition in the hidden information 
state (HIS) approach (Young et al 2007) 
represents the indistinguishable classes of user?s 
goals. The biggest difference between frame-based 
representation and partition-based representation is 
that the former uses only user input to split the 
frame state, whereas the latter uses the user input 
and external ontology rules such as a prior proba-
bility for belief of split partition. Therefore, the 
frame-based representation has relatively high do-
main portability because it does not need that kind 
of external domain dependent information. 
In the frame-based belief state representation, a 
partially-filled frame state represents the current 
user?s goal state for which the unfilled slot can be 
filled in the future, while a fully-filled frame state 
represents  co plete user?s goal state. Figure 2 
describes an example of the ubsumption relation-
ship between partially filled frames and fully filled 
frames.  
 
Figure 2. Subsumption relationship between partially 
filled frame and fully filled frame. The left frame is par-
tially filled and three frames in the right side are fully 
filled. 
 
At the start of a dialog, all states belong to the 
root frame state f0. As the dialog progresses, this 
root frame state is split into smaller frame states 
whenever the value of a slot is filled by the user?s 
input (Figure 3). First, if the user?s input [A=a] 
fills the slot of the root frame state f0, then it splits 
into two frame states: f1, which includes all user 
goal states with the slot A having ?a? as a value; 
and {f0-f1}, which is the relative complement of f1. 
Next, if the user?s input [B=b] is entered to the 
system, each frame f1 and {f0-f1} is split into small-
er frame states. The system updates not all belief 
states but only the beliefs of the frame states, so 
that the computational complexity remains rela-
tively small.  
If each user?s goal has uniform distribution, the 
belief of frame state b(f) can be calculated as fol-
lows:  
# of user goals contained in frame ( ) # of all user goals
fb f ?
 
This can be computed as follows:  
 
122
t
0
t
1
t
2
.
.
.
Root
Frame State
f
0
0( ) 1b f ?
{f
0
 - f
1
}
0 1
49
({ })
50
b f f? ?
f
1
(A = a)
1
1
( )b f
 f
3
(A = a)
(B = b)
3 2
1
( )
50
b f ?
{f
1 
? f
3
}
(A = a)
1 3 2
49
({ })
50
b f f? ?
 f
2
(B = b)
2 2
49
( )
50
b f ?
{{f
0
 - f
1
} - f
2
}
2
0 1 2 2
49
({{ } })
50
b f f f? ? ?
A = a
A != a
f
1 { f0 - f1}
f
0
A = a
B = b
A = a
B != b
A != a
B = b
A != a
B != b
f
3
{f
1 
? f
3
}
 f
2
{{f
0
 - f
1
} - f
2
}
User Input
A = a
User Input
B = b
User Input
C = c
Frame Splitting State Space
 
Figure 3. Splitting frame states and their beliefs with three user?s inputs. f0, f1, f2, ? denote frame states and b(f) 
means the belief of frame state f. A, B, C are the slot labels and a, b, c are the respective values of these slots. 
 
 
where Sfilled means the set of slots that are filled by 
the user?s input in frame state f, and SnotFilled means 
the set of empty slots. Vs denotes the set of availa-
ble values for slot s, and Vs? stands for the set of 
values for slot s that were specified by the user in 
other frame states. 
3.3 User action 
The SLU result of current user's utterance is used 
for the user action. The result frame of SLU con-
sists of a speech act, a main goal, and several 
named-entity component slots for each user's utter-
ance. The speech act stands for the surface-level 
speech act per single utterance and the main goal 
slot is assigned from one of the predefined classes 
which classify the main application actions in a 
specific domain such as ?search the weather 
(SEARCH_WEATHER)? or ?search the tempera-
ture (SEARCH_TEMPERATURE)? in the weather 
information service domain. The tasks for filling 
the named-entity component slots, such as, name 
of the city, name of the state, are viewed as a se-
quence labeling task. The Figure 4 shows some 
examples of predefined classes for SLU semantic 
frame in weather information service dialog system 
Our SLU module was developed based on the 
concept spotting approach, which aims to extract 
only the essential information for predefined mean-
ing representation slots, and was implemented by 
applying a conditional random field model (Lee et 
al., 2007).  
 
 
Figure 4 Example predefined classes for semantic frame 
of SLU in weather information service dialog system. 
 
3.4 Dialog history 
Similar to the traditional frame-based dialog 
management approach, a frame can represent the 
history of the dialog. The difference between the 
traditional frame-based dialog manager and our 
framework is that traditional frame-based dialog 
123
manager maintains only one frame while our 
framework can maintain multiple dialog hypothes-
es. Moreover, each hypothesis in our framework 
can have a probability as in the belief state of the 
classic POMDP.  
4 Example-based System Action Genera-
tion 
4.1 Example-based system action hypothesis 
generation 
It is impossible to consider all of the system ac-
tions as hypotheses because the number of possible 
actions is so large. We used an example-based ap-
proach to generate a reasonable number of system 
action hypotheses as hinted in (Lee et al, 2006). In 
this approach, the system retrieves the best dialog 
example from dialog example database (DEDB) 
which is semantically indexed from a dialog cor-
pus. To query a semantically close example for the 
current situation, the system uses the user?s inten-
tion (speech act and main goal), semantic frame 
(component slots) and discourse history as search 
key constraints (Lee et al, 2006). These search 
keys can be collected with SLU output (e.g., user 
intention and semantic frame) and discourse histo-
ry in a dialog manager. Figure 5 describes an ex-
ample of search key for DEDB on a weather 
information service system.  
 
User?s utterance  What will the temperature be tomorrow?  
                     Weather_Type  Time_Date  
Search key 
constraints  
Speech Act = wh_question  
Main Goal = search_temperature  
WEATHER_TYPE = 1 (filled) 
TIME_DATE = 1 (filled) 
LOC_CITY = 0 (unfilled) 
LOC_STATE = 0 (unfilled) 
Lexico-semantic  
Input 
What will the [WEATHER_TYPE] be 
[TIME_DATE]? 
Figure 5. Example search key constraints for dialog 
example database.  
 
For each frame state f1, ?, fn, the system gene-
rates one or more system action hypotheses by 
querying the DEDB respectively. Queried actions 
may inconsistent with the current frame state be-
cause the situation of indexed dialog examples 
may different from current dialog situation. There-
fore, the system maps the contents of dialog exam-
ple to information of current frame state. Slot 
values of frame state and information from content 
database (e.g., weather information database) are 
used for making the action consistent. If the system 
retrieves more than a threshold number of system 
action hypotheses using the search key constrains, 
then the system should prune away dialog exam-
ples to maintain only a reasonable number of hypo-
theses. We used lexico-semantic similarity 
between the user utterance and the retrieved exam-
ples to limit the number of hypotheses. To measure 
the lexico-semantic similarity, we first replace the 
slot values in the user utterance by its slot names to 
generate lexico-semantic input, and calculate the 
normalized edit distance between that input and 
retrieved examples (Figure 5). In the normalized 
edit distance, we defined following cost function 
C(i,j) to give a weight to the term which is re-
placed by its slot name. 
 
1, 2,
1, 2, 1, 2, _
1, 2, 1, 2, _
0  if                                       
( , ) 1  if  and ,  
1.5  if  and ,
i j
i j i j slot name
i j i j slot name
w w
C i j w w w w S
w w w w S
? ?
?? ? ??
? ? ??
 
 
where w1,i is ith word of user?s utterance, w2,j is jth 
word of dialog example?s utterance, and Sslot_name is 
the set of slot names. According to the lexico-
semantic similarity, the system appends the top Nh-
ranked hypotheses to the final action hypotheses 
(where Nh is the rank threshold). 
Many existing systems used heuristics or rule-
based approaches to reduce the number of system 
action hypotheses (Young et al, 2007). But these 
methods are not flexible enough to handle all di-
alog flows because a system developer should de-
sign new heuristics or rules whenever the system 
needs to support a new kind of dialog flow. The 
example-based approach, on the contrary, can in-
stantly refine the control of dialog flows by adding 
new dialog examples. This is a great advantage 
when a system developer wants to change or refine 
a dialog control flow. 
4.2 Calculating Expected Utilities 
We adopted the principle of maximum expected 
utility to determine the optimized system actions 
among the hypotheses (Paek and Horvitz, 2004). 
124
* argmax ( | )
argmax ( | ) ( , )
argmax ( ) ( , )
m
a
a h
h
a EU a
P H h u a h
b h u a h
?
?
?
? ?
?
?
?
?
 
 
where ? denotes all information about the envi-
ronment, u(a,h) means the utility of taking an ac-
tion when the internal state of the machine is h, 
which consists of three substates, (f, au, sd) : f is a 
frame state, au is a user?s last action, and sd is a 
dialog history. The utility function u(a,h) can be 
specific to each application. We defined a 
handcrafted utility function to calculate the ex-
pected utility. 
5 Experiments 
We performed two evaluations. 1) Real user evalu-
ation: we measured the user satisfaction with vari-
ous factors by human. 2) Simulated user 
evaluation: we implemented user simulator to 
measure the system performance with a large 
number of dialogs. We built dialog corpora in two 
domains: weather information service and car na-
vigation.  
5.1 Real user evaluation 
We built a dialog corpus in weather information 
service to measure the performance of the dialog 
system using our approach by real user evaluation. 
This corpus consists of 99 dialogs with 503 user 
utterances (turns). User?s utterances were anno-
tated with the semantic frame including speech 
acts, main goal and component slots for training 
the SLU module and indexing the DEDB. 
To evaluate the preliminary performance, four 
test volunteers among computer science people 
evaluated our dialog system with five different 
weather information-seeking tasks. The volunteers 
typed their utterances with a keyboard rather than 
using a real ASR because it is hard to control the 
WER. We employed a simulated ASR error chan-
nel by generating random errors to evaluate the 
performance of dialog management under various 
levels of WER. We will explain the details of our 
ASR channel simulator in Section 5.2. The WER is 
controlled by this ASR channel simulator while the 
volunteers were interacting with computer. To 
measure the user perception of task completion 
rate (TCR), the volunteers evaluated the system?s 
response in each dialog to measure the success turn 
rate (STR) and decided whether the entire dialog 
was successful or not. We evaluated the perfor-
mance of our dialog system based on criteria out-
lined in (Litman and Pan, 2004) by measuring user 
satisfaction, which is defined with a linear combi-
nation of three measures: TCR, Mean Recognition 
Accuracy (MRA), and STR. 
 
User Satisfaction = ?TCR +?STR + ?MRA 
 
In our evaluation, we set ?, ? and ? to 1/3, so 
that the maximum value of the user satisfaction is 
one.  
 
 
Figure 6 Dialog system performance with various word 
error rates in weather information seeking tasks. Dotted 
line is TCR; dashed line is STR; solid line is user satis-
faction. 
 
TCR, STR and user satisfaction decreased with 
WER. User satisfaction has relatively high value 
when the WER is smaller than 20% (Figure 6). If 
the WER is equal or over 20%, user satisfaction 
has small value because the TCR decreases rapidly 
in this range. 
Generally, TCR has a higher value than STR, 
because although a dialog turn may fail, users still 
have a chance to use other expressions which can 
be well recognized by the system. As a result of 
this, even when some dialog turns fail, the task can 
be completed successfully. 
TCR decreases rapidly when WER ?20%. 
When WER is high, the probability of losing the 
125
information in a user utterance is also large. Espe-
cially, if words contain important meaning, i.e., 
values of component slots in SLU, it is difficult for 
the system to generate a proper response. 
STR is 0.83 when WER is zero, i.e., although all 
user inputs are correctly recognized, the system 
sometimes didn?t generate proper outputs. This 
failure can be caused by SLU errors or malfunction 
of the dialog manager. SLU errors can be propa-
gated to the dialog manager, and this leads the sys-
tem to generate a wrong response because SLU 
results are inputs of dialog manger. 
If the WER is 20%, user satisfaction is relatively 
small because TCR decreases rapidly in this range. 
This means that our approach is useful in a system 
devoted to providing weather information, and is 
relatively robust to speech errors if the WER is less 
than 20%. 
5.2 Simulated user evaluation 
We built another dialog corpus in car navigation 
service to measure the performance of the dialog 
system by simulated user evaluation. This corpus 
consists of 123 dialogs with 510 user utterances 
(turns). The SLU result frame of this corpus has 7 
types of speech acts, 8 types of main goals, and 5 
different component slots. 
The user simulator and ASR channel simulator has 
been used for evaluating the proposed dialog man-
agement framework. The user simulator has two 
components: an Intention Simulator and a Surface 
Simulator. The Intention Simulator generates the 
next user intention given current discourse context, 
and the Surface Simulator generates user sentence 
to express the generated intention.  
ASR channel simulator simulates the speech 
recognition errors including substitution, deletion, 
and insertions errors. It uses the phoneme confu-
sion matrix to estimate the probability distribution 
for error simulation. ASR channel simulator dis-
torts the generated user utterance from Surface Si-
mulator. By simulating user intentions, surface 
form of user sentence and ASR channel, we can 
test the robustness of the proposed dialog system in 
both speech recognition and speech understanding 
errors. 
We defined a final state of dialog to automati-
cally measure TCR of a simulated dialog. If a di-
alog flow reaches the final state, the evaluator 
regards that the dialog was successfully completed. 
TCRs and average dialog lengths were measured 
under various WER conditions that were generated 
by ASR channel simulator. Until the SLU result is 
an actual input of the dialog manager, we also 
measured the SLU accuracy. If a SLU result is 
same as a user?s intention of the Intention Simula-
tor, then the evaluator considers that the result is 
correct. Unlike in the real user evaluation, the di-
alog system could be evaluated with relatively 
large amount of simulated dialogs in the simulated 
user evaluation. 5000 simulated dialogs were gen-
erated for each WER condition. 
 
 
Figure 7 TCR, SLU accuracy, and average dialog length 
of the dialog system under various WER conditions. 
 
We found that the SLU accuracy and TCR li-
nearly decreased with the WER. Similar in the 
human evaluation, TCR is about 0.9 when WER is 
zero, and it becomes below 0.7 when WER is 
higher than 20%. Average dialog length, on con-
trary, increased with WER, and it has similar val-
ues when WER is less than 10% although it 
increased relatively rapidly when WER is higher 
than 15%. 
 
6 Conclusions 
This paper proposed a new probabilistic method to 
manage the human-machine dialog by using the 
frame-state belief state representation and the ex-
ample-based system action hypothesis generation. 
The frame-based state representation reduces the 
computational complexity of belief update by 
grouping the indistinguishable user goal states. 
And the system generates the system action hypo-
126
theses with the example-based approach in order to 
refine the dialog flows easily. In addition, this ap-
proach employed the POMDP formalism to main-
tain belief distribution over dialog states so that the 
system can be robust to speech recognition errors 
by considering the uncertainty of user?s input. 
A prototype system using our approach has been 
implemented and evaluated by real and simulated 
user. According to the preliminary evaluation, our 
framework can be a useful approach to manage a 
spoken dialog system. 
We plan to progress the research on adopting a 
formalized online search to determine the optimal 
system action (Ross and Chaib-draa, 2007). With 
the online searching, system doesn?t need to be-
have the useless computation because this ap-
proach searches only possible path. We expect that 
this property of the online searching show the syn-
ergetic effect on dialog management if it combines 
with example-based approach. 
Similar to example-based approach, the case-
based reasoning approach (Eliasson, 2006) can be 
helpful for our future research. Some properties 
such as using previous cases to process current 
case can be shared with our approach. We think 
that some other properties including the concept of 
online learning can be useful for making our ap-
proach concrete 
Acknowledgments 
This research was supported by the MKE (Min-
istry of Knowledge Economy), Korea, under the 
ITRC (Information Technology Research Center) 
support program supervised by the IITA (Institute 
for Information Technology Advancement) (IITA-
2008-C1090-0801-0045) 
References  
Changki Lee, Jihyun Eun, Minwoo Jeong, and Gary 
Geunbae Lee, Y. Hwang, M. Jang, ?A multi-strategic 
concept-spotting approach for robust understanding 
of spoken Korean,? ETRI Journal, vol. 29, No.2, pp. 
179-188, 2007. 
 
Cheongjae Lee, Sangkeun Jung, Jihyun Eun, Minwoo 
Jeong and Gary Geunbae Lee, ?A situation-based di-
alogue management using dialogue examples,? in 
Proceedings of International conference on Acoustics, 
Speech, and Signal Processing, Toulouse, 2006. 
 
Diane J. Litman and Shimei Pan, ?Empirically evaluat-
ing an adaptable spoken dialogue system,? in Pro-
ceedings of the 8th International Conference on 
Spoken Language Processing, pp. 2145-2148, 2004. 
 
Finale Doshi and Nicholas Roy, ?Efficient Model 
Learning for Dialog Management,? in Proceeding of 
the ACM/IEEE international conference on Human-
robot interaction, Washington DC, 2007. 
 
Jason D. Williams and Steve Young, "Scaling POMDPs 
for dialog management with composite summary 
point-based value iteration (CSPBVI)," in Proceed-
ings of AAAI Workshop on Statistical and Empirical 
Approaches for Spoken Dialogue Systems, Boston, 
2006. 
 
Jason D. Williams and Steve Young, " Partially Observ-
able Markov Decision Processes for Spoken Dialog 
Systems." Computer Speech and Language 21(2): 
231-422, 2007 
 
Karolina Eliasson, ?The Use of Case-Based Reasoning 
in a Human-Robot Dialog System?, Licentiate of 
Engineering Thesis of Link?ping Institute of Tech-
nology at Link?ping University, 2006 
 
Nicholas Roy, Geoffrey Gordon, and Sebastian Thrun, 
?Finding approximate pomdp solutions through be-
lief compression,? Journal of Artificial Intelligence 
Research, vol. 23, pp.1?40, 2005. 
 
Spt?phane Ross, Brahim Chaib-draa, ?AEMS: An Any-
time Online Search Algorithm for Approximate Poli-
cy Refinement in Large POMDPs?, in Proceedings 
of the 20th International Joint Conference on Artifi-
cial Intelligence, 2007 
 
Steve Young, Jost Schatzmann, Karl Weilhammer and 
Hui Ye, "The hidden information state approach to 
dialog management," in Proceedings of International 
Conference on Acoustics, Speech, and Signal 
Processing, Honolulu, 2007. 
 
Tim Paek and Eric Horvitz, ?Optimizing automated call 
routing by integrating spoken dialog models with 
queuing models,? in Proceedings of HLT-NAACL, pp. 
41-48, Boston, 2004. 
 
 
127
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 564?571,
Beijing, August 2010
A Cross-lingual Annotation Projection Approach
for Relation Detection
Seokhwan Kim?, Minwoo Jeong?, Jonghoon Lee?, Gary Geunbae Lee?
?Department of Computer Science and Engineering,
Pohang University of Science and Technology
{megaup|jh21983|gblee}@postech.ac.kr
?Saarland University
m.jeong@mmci.uni-saarland.de
Abstract
While extensive studies on relation ex-
traction have been conducted in the last
decade, statistical systems based on su-
pervised learning are still limited because
they require large amounts of training data
to achieve high performance. In this pa-
per, we develop a cross-lingual annota-
tion projection method that leverages par-
allel corpora to bootstrap a relation detec-
tor without significant annotation efforts
for a resource-poor language. In order to
make our method more reliable, we intro-
duce three simple projection noise reduc-
tion methods. The merit of our method is
demonstrated through a novel Korean re-
lation detection task.
1 Introduction
Relation extraction aims to identify semantic re-
lations of entities in a document. Many rela-
tion extraction studies have followed the Rela-
tion Detection and Characterization (RDC) task
organized by the Automatic Content Extraction
project (Doddington et al, 2004) to make multi-
lingual corpora of English, Chinese and Ara-
bic. Although these datasets encourage the de-
velopment and evaluation of statistical relation
extractors for such languages, there would be a
scarcity of labeled training samples when learn-
ing a new system for another language such as
Korean. Since manual annotation of entities and
their relations for such resource-poor languages
is very expensive, we would like to consider in-
stead a weakly-supervised learning technique in
order to learn the relation extractor without sig-
nificant annotation efforts. To do this, we propose
to leverage parallel corpora to project the relation
annotation on the source language (e.g. English)
to the target (e.g. Korean).
While many supervised machine learning ap-
proaches have been successfully applied to the
RDC task (Kambhatla, 2004; Zhou et al, 2005;
Zelenko et al, 2003; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005; Zhang et al, 2006),
few have focused on weakly-supervised relation
extraction. For example, (Zhang, 2004) and (Chen
et al, 2006) utilized weakly-supervised learning
techniques for relation extraction, but they did
not consider weak supervision in the context of
cross-lingual relation extraction. Our key hypoth-
esis on the use of parallel corpora for learning
the relation extraction system is referred to as
cross-lingual annotation projection. Early stud-
ies of cross-lingual annotation projection were ac-
complished for lexically-based tasks; for exam-
ple part-of-speech tagging (Yarowsky and Ngai,
2001), named-entity tagging (Yarowsky et al,
2001), and verb classification (Merlo et al, 2002).
Recently, there has been increasing interest in ap-
plications of annotation projection such as depen-
dency parsing (Hwa et al, 2005), mention de-
tection (Zitouni and Florian, 2008), and semantic
role labeling (Pado and Lapata, 2009). However,
to the best of our knowledge, no work has reported
on the RDC task.
In this paper, we apply a cross-lingual anno-
tation projection approach to binary relation de-
tection, a task of identifying the relation between
two entities. A simple projection method propa-
gates the relations in source language sentences to
564
word-aligned target sentences, and a target rela-
tion detector can bootstrap from projected annota-
tion. However, this automatic annotation is unre-
liable because of mis-classification of source text
and word alignment errors, so it causes a critical
falling-off in annotation projection quality. To al-
leviate this problem, we present three noise reduc-
tion strategies: a heuristic filtering; an alignment
correction with dictionary; and an instance selec-
tion based on assessment, and combine these to
yield a better result.
We provide a quantitive evaluation of our
method on a new Korean RDC dataset. In our
experiment, we leverage an English-Korean par-
allel corpus collected from the Web, and demon-
strate that the annotation projection approach and
noise reduction method are beneficial to build an
initial Korean relation detection system. For ex-
ample, the combined model of three noise reduc-
tion methods achieves F1-scores of 36.9% (59.8%
precision and 26.7% recall), favorably comparing
with the 30.5% shown by the supervised base-
line.1
The remainder of this paper is structured as fol-
lows. In Section 2, we describe our cross-lingual
annotation projection approach to relation detec-
tion task. Then, we present the noise reduction
methods in Section 3. Our experiment on the pro-
posed Korean RDC evaluation set is shown in Sec-
tion 4 and Section 5, and we conclude this paper
in Section 6.
2 Cross-lingual Annotation Projection
for Relation Detection
The annotation projection from a resource-rich
language L1 to a resource-poor language L2 is
performed by a series of three subtasks: annota-
tion, projection and assessment.
The annotation projection for relation detection
can be performed as follows:
1) For a given pair of bi-sentences in parallel cor-
pora between a resource-rich language L1 and
a target language L2, the relation detection task
is carried out for the sentence in L1.
1The dataset and the parallel corpus are available on the
author?s website,
http://isoft.postech.ac.kr/?megaup/research/resources/.
2) The annotations obtained by analyzing the sen-
tence in L1 are projected onto the sentence in
L2 based on the word alignment information.
3) The projected annotations on the sentence in
L2 are utilized as resources to perform the re-
lation detection task for the language L2.
2.1 Annotation
The first step to projecting annotations from L1
onto L2 is obtaining annotations for the sentences
in L1. Since each instance for relation detection
is composed of a pair of entity mentions, the in-
formation about entity mentions on the given sen-
tences should be identified first. We detect the
entities in the L1 sentences of the parallel cor-
pora. Entity identification generates a number of
instances for relation detection by coupling two
entities within each sentence. For each instance,
the existence of semantic relation between entity
mentions is explored, which is called relation de-
tection. We assume that there exist available mod-
els or systems for all annotation processes, includ-
ing not only an entity tagger and a relation de-
tector themselves, but also required preprocessors
such as a part-of-speech tagger, base-phrase chun-
ker, and syntax parser for analyzing text in L1.
Figure 1 shows an example of annotation pro-
jection for relation detection of a bitext in En-
glish and Korean. The annotation of the sentence
in English shows that ?Jan Mullins? and ?Com-
puter Recycler Incorporated? are entity mentions
of a person and an organization, respectively. Fur-
thermore, the result indicates that the pair of en-
tities has a semantic relationship categorized as
?ROLE.Owner? type.
2.2 Projection
In order to project the annotations from the sen-
tences in L1 onto the sentences in L2, we utilize
the information of word alignment which plays
an important role in statistical machine transla-
tion techniques. The word alignment task aims
to identify translational relationships among the
words in a bitext and produces a bipartite graph
with a set of edges between words with transla-
tional relationships as shown in Figure 1. In the
same manner as the annotation in L1, entities are
565
????????
(keom-pyu-teo-ri-sa-i-keul-reo)
?
(ui)
??
(sa-jang)
?
(eun)
? ??
(ra-go)
???
(mal-haet-da)
Mullins, owner of Incorporated said that ...
?
(jan)
???
(meol-rin-seu)
Jan Computer Recycler
ROLE.Owner
PER ORG
ORG PER
ROLE.Owner
Figure 1: An example of annotation projection for relation detection of a bitext in English and Korean
considered as the first units to be projected. We as-
sume that the words of the sentences in L2 aligned
with a given entity mention in L1 inherit the infor-
mation about the original entity in L1.
After projecting the annotations of entity men-
tions, the projections for relational instances fol-
low. A projection is performed on a projected in-
stance in L2 which is a pair of projected entities
by duplicating annotations of the original instance
in L1.
Figure 1 presents an example of projection of a
positive relational instance between ?Jan Mullins?
and ?Computer Recycler Incorporated? in the
English sentence onto its translational counter-
part sentence in Korean. ?Jan meol-rin-seu? and
?keom-pyu-teo-ri-sa-i-keul-reo? are labeled as en-
tity mentions with types of a person?s name and an
organization?s name respectively. In addition, the
instance composed of the two projected entities is
annotated as a positive instance, because its orig-
inal instance on the English sentence also has a
semantic relationship.
As the description suggests, the annotation pro-
jection approach is highly dependant on the qual-
ity of word alignment. However, the results of au-
tomatic word alignment may include several noisy
or incomplete alignments because of technical dif-
ficulties. We present details to tackle the problem
by relieving the influence of alignment errors in
Section 3.
2.3 Assessment
The most important challenge for annotation pro-
jection approaches is how to improve the robust-
ness against the erroneous projections. The noise
produced by not only word alignment but also
mono-lingual annotations in L1 accumulates and
brings about a drastic decline in the quality of pro-
jected annotations.
The simplest policy of utilizing the projected
annotations for relation detection in L2 is to con-
sider that all projected instances are equivalently
reliable and to employ entire projections as train-
ing instances for the task without any filtering. In
contrast with this policy, which is likely to be sub-
standard, we propose an alternative policy where
the projected instances are assessed and only the
instances judged as reliable by the assessment are
utilized for the task. Details about the assessment
are provided in Section 3.
3 Noise Reduction Strategies
The efforts to reduce noisy projections are consid-
ered indispensable parts of the projection-based
relation detection method in a resource-poor lan-
guage. Our noise reduction approach includes the
following three strategies: heuristic-based align-
ment filtering, dictionary-based alignment correc-
tion, and assessment-based instance selection.
3.1 Heuristic-based Alignment Filtering
In order to improve the performance of annotation
projection approaches, we should break the bottle-
neck caused by the low quality of automatic word
alignment results. As relation detection is carried
out for each instance consisting of two entity men-
tions, the annotation projection for relation detec-
tion concerns projecting only entity mentions and
566
their relational instances. Since this is different
from other shallower tasks such as part-of-speech
tagging, base phrase chunking, and dependency
parsing which should consider projections for all
word units, we define and apply some heuristics
specialized to projections of entity mentions and
relation instances to improve robustness of the
method against erroneous alignments, as follows:
? A projection for an entity mention should
be based on alignments between contiguous
word sequences. If there are one or more
gaps in the word sequence in L2 aligned
with an entity mention in the sentence in
L1, we assume that the corresponding align-
ments are likely to be erroneous. Thus, the
alignments of non-contiguous words are ex-
cluded in projection.
? Both an entity mention in L1 and its projec-
tion in L2 should include at least one base
noun phrase. If no base noun phrase oc-
curs in the original entity mention in L1, it
may suggest some errors in annotation for
the sentence in L1. The same case for the
projected instance raises doubts about align-
ment errors. The alignments between word
sequences without any base noun phrase are
filtered out.
? The projected instance in L2 should sat-
isfy the clausal agreement with the original
instance in L1. If entities of an instance
are located in the same clause (or differ-
ent clauses), its projected instance should be
in the same manner. The instances without
clausal agreement are ruled out.
3.2 Dictionary-based Alignment Correction
The errors in word alignment are composed of
not only imprecise alignments but also incomplete
alignments. If an alignment of an entity among
two entities of a relation instance is not provided
in the result of the word alignment task, the pro-
jection for the corresponding instance is unavail-
able. Unfortunately, the above-stated alignment
filtering heuristics for improving the quality of
projections make the annotation loss problems
worse by filtering out several alignments likely to
be noisy.
In order to solve this problem, a dictionary-
based alignment correction strategy is incorpo-
rated in our method. The strategy requires a bilin-
gual dictionary for entity mentions. Each entry of
the dictionary is a pair of entity mention in L1 and
its translation or transliteration in L2. For each
entity to be projected from the sentence in L1,
its counterpart in L2 is retrieved from the bilin-
gual dictionary. Then, we seek the retrieved entity
mention from the sentence in L2 by finding the
longest common subsequence. If a subsequence
matched to the retrieved mention is found in the
sentence in L2, we make a new alignment between
it and its original entity on the L1 sentence.
3.3 Assessment-based Instance Selection
The reliabilities of instances projected via a series
of independent modules are different from each
other. Thus, we propose an assessment strategy
for each projected instance. To evaluate the reli-
ability of a projected instance in L2, we use the
confidence score of monolingual relation detec-
tion for the original counterpart instance in L1.
The acceptance of a projected instance is deter-
mined by whether the score of the instance is
larger than a given threshold value ?. Only ac-
cepted instances are considered as the results of
annotation projection and applied to solve the re-
lation detection task in target language L2.
4 Experimental Setup
To demonstrate the effectiveness of our cross-
lingual annotation projection approach for rela-
tion detection, we performed an experiment on
relation detection in Korean text with propagated
annotations from English resources.
4.1 Annotation
The first step to evaluate our method was annotat-
ing the English sentences in a given parallel cor-
pus. We use an English-Korean parallel corpus
crawled from an English-Korean dictionary on the
web. The parallel corpus consists of 454,315 bi-
sentence pairs in English and Korean 2. The En-
glish sentences in the parallel corpus were prepro-
2The parallel corpus collected and other resources are all
available in our website
http://isoft.postech.ac.kr/?megaup/research/resources/
567
cessed by the Stanford Parser 3 (Klein and Man-
ning, 2003) which provides a set of analyzed re-
sults including part-of-speech tag sequences, a de-
pendency tree, and a constituent parse tree for a
sentence.
The annotation for English sentences is di-
vided into two subtasks: entity mention recogni-
tion and relation detection. We utilized an off-
the-shelf system, Stanford Named Entity Recog-
nizer 4 (Finkel et al, 2005) for detecting entity
mentions on the English sentences. The total
number of English entities detected was 285,566.
Each pair of recognized entities within a sentence
was considered as an instance for relation detec-
tion.
A classification model learned with the train-
ing set of the ACE 2003 corpus which con-
sists of 674 documents and 9,683 relation in-
stances was built for relation detection in English.
In our implementation, we built a tree kernel-
based SVM model using SVM-Light 5 (Joachims,
1998) and Tree Kernel Tools 6 (Moschitti, 2006).
The subtree kernel method (Moschitti, 2006) for
shortest path enclosed subtrees (Zhang et al,
2006) was adopted in our model. Our rela-
tion detection model achieved 81.2/69.8/75.1 in
Precision/Recall/F-measure on the test set of the
ACE 2003 corpus, which consists of 97 docu-
ments and 1,386 relation instances.
The annotation of relations was performed by
determining the existence of semantic relations
for all 115,452 instances with the trained model
for relation detection. The annotation detected
22,162 instances as positive which have semantic
relations.
4.2 Projection
The labels about entities and relations in the En-
glish sentences of the parallel corpora were propa-
gated into the corresponding sentences in Korean.
The Korean sentences were preprocessed by our
part-of-speech tagger 7 (Lee et al, 2002) and a de-
pendency parser implemented by MSTParser with
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://nlp.stanford.edu/software/CRF-NER.shtml
5http://svmlight.joachims.org/
6http://disi.unitn.it/?moschitt/Tree-Kernel.htm
7http://isoft.postech.ac.kr/?megaup/research/postag/
Filter Without assessing With assessing
none 97,239 39,203
+ heuristics 31,652 12,775
+ dictionary 39,891 17,381
Table 1: Numbers of projected instances
a model trained on the Sejong corpus (Kim, 2006).
The annotation projections were performed on
the bi-sentences of the parallel corpus followed
by descriptions mentioned in Section 2.2. The
bi-sentences were processed by the GIZA++ soft-
ware (Och and Ney, 2003) in the standard con-
figuration in both English-Korean and Korean-
English directions. The bi-direcional alignments
were joined by the grow-diag-final algorithm,
which is widely used in bilingual phrase extrac-
tion (Koehn et al, 2003) for statistical machine
translation. This system achieved 65.1/41.6/50.8
in Precision/Recall/F-measure in our evaluation
of 201 randomly sampled English-Korean bi-
sentences with manually annotated alignments.
The number of projected instances varied with
the applied strategies for reducing noise as shown
in Table 1. Many projected instances were fil-
tered out by heuristics, and only 32.6% of the in-
stances were left. However, several instances were
rescued by dictionary-based alignment correction
and the number of projected instances increased
from 31,652 to 39,891. For all cases of noise re-
duction strategies, we performed the assessment-
based instance selection with a threshold value ?
of 0.7, which was determined empirically through
the grid search method. About 40% of the pro-
jected instances were accepted by instance selec-
tion.
4.3 Evaluation
In order to evaluate our proposed method, we pre-
pared a dataset for the Korean RDC task. The
dataset was built by annotating the information
about entities and relations in 100 news docu-
ments in Korean. The annotations were performed
by two annotators following the guidelines for the
ACE corpus processed by LDC. Our Korean RDC
corpus consists of 835 sentences, 3,331 entity
mentions, and 8,354 relation instances. The sen-
568
Model w/o assessing with assessingP R F P R F
Baseline 60.5 20.4 30.5 - - -
Non-filtered 22.5 6.5 10.0 29.1 13.2 18.2
Heuristic 51.4 15.5 23.8 56.1 22.9 32.5
Heuristic + Dictionary 55.3 19.4 28.7 59.8 26.7 36.9
Table 2: Experimental Results
tences of the corpus were preprocessed by equiva-
lent systems used for analyzing Korean sentences
for projection. We randomly divided the dataset
into two subsets with the same number of in-
stances for use as a training set to build the base-
line system and for evaluation.
For evaluating our approach, training instance
sets to learn models were prepared for relation
detection in Korean. The instances of the train-
ing set (half of the manually built Korean RDC
corpufs) were used to train the baseline model.
All other sets of instances include these baseline
instances and additional instances propagated by
the annotation projection approach. The train-
ing sets with projected instances are categorized
into three groups by the level of applied strategies
for noise reduction. While the first set included
all projections without any noise reduction strate-
gies, the second included only the instances ac-
cepted by the heuristics. The last set consisted of
the results of a series of heuristic-based filtering
and dictionary-based correction. For each training
set with projected instances, an additional set was
derived by performing assessment-based instance
selection.
We built the relation detection models for all
seven training sets (a baseline set, three pro-
jected sets without assessing, and three pro-
jected sets with assessing). Our implementations
are based on the SVM-Light and Tree Kernel
Tools described in the former subsection. The
shortest path dependency kernel (Bunescu and
Mooney, 2005) implemented by the subtree kernel
method (Moschitti, 2006) was adopted to learn all
models.
The performance for each model was evaluated
with the predictions of the model on the test set,
which was the other half of Korean RDC corpus.
We measured the performances of the models on
true entity mentions with true chaining of coref-
erence. Precision, Recall and F-measure were
adopted for our evaluation.
5 Experimental Results
Table 2 compares the performances of the differ-
ent models which are distinguished by the applied
strategies for noise reduction. It shows that:
? The model with non-filtered projections
achieves extremely poor performance due to
a large number of erroneous instances. This
indicates that the efforts for reducing noise
are urgently needed.
? The heuristic-based alignment filtering helps
to improve the performance. However, it is
much worse than the baseline performance
because of a falling-off in recall.
? The dictionary-based correction to our pro-
jections increased both precision and recall
compared with the former models with pro-
jected instances. Nevertheless, it still fails to
achieve performance improvement over the
baseline model.
? For all models with projection, the
assessment-based instance selection boosts
the performances significantly. This means
that this selection strategy is crucial in
improving the performance of the models
by excluding unreliable instances with low
confidence.
? The model with heuristics and assessments
finally achieves better performance than the
baseline model. This suggests that the pro-
jected instances have a beneficial influence
569
on the relation detection task when at least
these two strategies are adopted for reducing
noises.
? The final model incorporating all proposed
noise reduction strategies outperforms the
baseline model by 6 in F-measure. This is
due to largely increased recall by absorbing
more useful features from the well-refined
set of projected instances.
The experimental results show that our pro-
posed techniques effectively improve the perfor-
mance of relation detection in the resource-poor
Korean language with a set of annotations pro-
jected from the resource-rich English language.
6 Conclusion
This paper presented a novel cross-lingual annota-
tion projection method for relation extraction in a
resource-poor language. We proposed methods of
propagating annotations from a resource-rich lan-
guage to a target language via parallel corpora. In
order to relieve the bad influence of noisy projec-
tions, we focused on the strategies for reducing the
noise generated during the projection. We applied
our methods to the relation detection task in Ko-
rean. Experimental results show that the projected
instances from an English-Korean parallel corpus
help to improve the performance of the task when
our noise reduction strategies are adopted.
We would like to introduce our method to the
other subtask of relation extraction, which is re-
lation categorization. While relation detection is
a binary classification problem, relation catego-
rization can be solved by a classifier for multi-
ple classes. Since the fundamental approaches
of the two tasks are similar, we expect that our
projection-based relation detection methods can
be easily adapted to the relation categorization
task.
For this further work, we are concerned about
the problem of low performance for Korean,
which was below 40 for relation detection. The re-
lation categorization performance is mostly lower
than detection because of the larger number of
classes to be classified, so the performance of
projection-based approaches has to be improved
in order to apply them. An experimental result
of this work shows that the most important factor
in improving the performance is how to select the
reliable instances from a large number of projec-
tions. We plan to develop more elaborate strate-
gies for instance selection to improve the projec-
tion performance for relation extraction.
Acknowledgement
This research was supported by the MKE (The
Ministry of Knowledge Economy), Korea, un-
der the ITRC (Information Technology Research
Center) support program supervised by the NIPA
(National IT Industry Promotion Agency) (NIPA-
2010-C1090-1031-0009).
References
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, page 724731.
Chen, Jinxiu, Donghong Ji, Chew Lim Tan, and
Zhengyu Niu. 2006. Relation extraction using la-
bel propagation based semi-supervised learning. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 129?136, Sydney, Australia. Associ-
ation for Computational Linguistics.
Culotta, Aron and Jaffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of ACL, volume 4.
Doddington, George, Alexis Mitchell, Mark Przy-
bocki, Lance Ramshaw, Stephanie Strassel, and
Ralph Weischedel. 2004. The automatic content
extraction (ACE) programtasks, data, and evalua-
tion. In Proceedings of LREC, volume 4, page
837840.
Finkel, Jenny R., Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
volume 43, page 363.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
570
Joachims, Thorsten. 1998. Text categorization with
support vector machines: Learning with many rele-
vant features. In Proceedings of the European Con-
ference on Machine Learning, pages 137?142.
Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive poster and demonstra-
tion sessions, page 22, Barcelona, Spain. Associa-
tion for Computational Linguistics.
Kim, Hansaem. 2006. Korean national corpus in the
21st century sejong project. In Proceedings of the
13th NIJL International Symposium, page 4954.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 423?430, Sap-
poro, Japan. Association for Computational Lin-
guistics.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, vol-
ume 1, pages 48?54.
Lee, Gary Geunbae, Jeongwon Cha, and Jong-Hyeok
Lee. 2002. Syllable pattern-based unknown mor-
pheme segmentation and estimation for hybrid part-
of-speech tagging of korean. Computational Lin-
guistics, 28(1):53?70.
Merlo, Paola, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 207?214, Philadelphia,
Pennsylvania. Association for Computational Lin-
guistics.
Moschitti, Alessandro. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of EACL06.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Pado, Sebastian and Mirella Lapata. 2009.
Cross-lingual annotation projection of semantic
roles. Journal of Artificial Intelligence Research,
36(1):307340.
Yarowsky, David and Grace Ngai. 2001. Inducing
multilingual POS taggers and NP bracketers via ro-
bust projection across aligned corpora. In Second
meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
technologies 2001, pages 1?8, Pittsburgh, Pennsyl-
vania. Association for Computational Linguistics.
Yarowsky, David, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, pages 1?
8, San Diego. Association for Computational Lin-
guistics.
Zelenko, Dmitry, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083?1106.
Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832, Sydney, Australia. Associ-
ation for Computational Linguistics.
Zhang, Zhu. 2004. Weakly-supervised relation clas-
sification for information extraction. In Proceed-
ings of the thirteenth ACM international conference
on Information and knowledge management, pages
581?588, Washington, D.C., USA. ACM.
Zhou, Guodong, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, page
434.
Zitouni, Imed and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 600?609, Honolulu,
Hawaii. Association for Computational Linguistics.
571
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 48?53,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Graph-based Cross-lingual Projection Approach for
Weakly Supervised Relation Extraction
Seokhwan Kim
Human Language Technology Dept.
Institute for Infocomm Research
Singapore 138632
kims@i2r.a-star.edu.sg
Gary Geunbae Lee
Dept. of Computer Science and Engineering
Pohang University of Science and Technology
Pohang, 790-784, Korea
gblee@postech.ac.kr
Abstract
Although researchers have conducted exten-
sive studies on relation extraction in the last
decade, supervised approaches are still limited
because they require large amounts of training
data to achieve high performances. To build
a relation extractor without significant anno-
tation effort, we can exploit cross-lingual an-
notation projection, which leverages parallel
corpora as external resources for supervision.
This paper proposes a novel graph-based pro-
jection approach and demonstrates the mer-
its of it by using a Korean relation extrac-
tion system based on projected dataset from
an English-Korean parallel corpus.
1 Introduction
Relation extraction aims to identify semantic rela-
tions of entities in a document. Although many
supervised machine learning approaches have been
successfully applied to relation extraction tasks (Ze-
lenko et al, 2003; Kambhatla, 2004; Bunescu and
Mooney, 2005; Zhang et al, 2006), applications of
these approaches are still limited because they re-
quire a sufficient number of training examples to ob-
tain good extraction results. Several datasets that
provide manual annotations of semantic relation-
ships are available from MUC (Grishman and Sund-
heim, 1996) and ACE (Doddington et al, 2004)
projects, but these datasets contain labeled training
examples in only a few major languages, includ-
ing English, Chinese, and Arabic. Although these
datasets encourage the development of relation ex-
tractors for these major languages, there are few la-
beled training samples for learning new systems in
other languages, such as Korean. Because manual
annotation of semantic relations for such resource-
poor languages is very expensive, we instead con-
sider weakly supervised learning techniques (Riloff
and Jones, 1999; Agichtein and Gravano, 2000;
Zhang, 2004; Chen et al, 2006) to learn the rela-
tion extractors without significant annotation efforts.
But these techniques still face cost problems when
preparing quality seed examples, which plays a cru-
cial role in obtaining good extractions.
Recently, some researchers attempted to use ex-
ternal resources, such as treebank (Banko et al,
2007) and Wikipedia (Wu and Weld, 2010), that
were not specially constructed for relation extraction
instead of using task-specific training or seed exam-
ples. We previously proposed to leverage parallel
corpora as a new kind of external resource for rela-
tion extraction (Kim et al, 2010). To obtain training
examples in the resource-poor target language, this
approach exploited a cross-lingual annotation pro-
jection by propagating annotations that were gener-
ated by a relation extraction system in a resource-
rich source language. In this approach, projected
annotations were determined in a single pass pro-
cess by considering only alignments between entity
candidates; we call this action direct projection.
In this paper, we propose a graph-based projec-
tion approach for weakly supervised relation extrac-
tion. This approach utilizes a graph that is con-
stucted with both instance and context information
and that is operated in an iterative manner. The goal
of our graph-based approach is to improve the ro-
bustness of the extractor with respect to errors that
are generated and accumulated by preprocessors.
48
fE (<Barack Obama, Honolulu>) = 1
fK  ( <  ?? zj  ,   ??F>  > ) = 1
?? zj
(beo-rak-o-ba-ma)
&r
(e-seo)
?
(neun)
??F>
(ho-nol-rul-ru)
???
(ha-wa-i)
2
:.
(tae-eo-nat-da)
?
(ui)
Barack Obama was born in Honolulu Hawaii, .
(beo-rak-o-ba-ma) (ho-nol-rul-ru)
Figure 1: An example of annotation projection for rela-
tion extraction of a bitext in English and Korean
2 Cross-lingual Annotation Projection for
Relation Extraction
Relation extraction can be considered to be a classi-
fication problem by the following classifier:
f
(
ei, ej
)
=
{
1 if ei and ej have a relation,
?1 otherwise. ,
where ei and ej are entities in a sentence.
Cross-lingual annotation projection intends to
learn an extractor ft for good performance with-
out significant effort toward building resources for
a resource-poor target language Lt. To accomplish
that goal, the method automatically creates a set of
annotated text for ft, utilizing a well-made extractor
fs for a resource-rich source language Ls and a par-
allel corpus of Ls and Lt. Figure 1 shows an exam-
ple of annotation projection for relation extraction
with a bi-text in Lt Korean and Ls English. Given an
English sentence, an instance ?Barack Obama, Hon-
olulu? is extracted as positive. Then, its translational
counterpart ?beo-rak-o-ba-ma, ho-nol-rul-ru? in the
Korean sentence also has a positive annotation by
projection.
Early studies in cross-lingual annotation projec-
tion were accomplished for various natural lan-
guage processing tasks (Yarowsky and Ngai, 2001;
Yarowsky et al, 2001; Hwa et al, 2005; Zitouni and
Florian, 2008; Pado and Lapata, 2009). These stud-
ies adopted a simple direct projection strategy that
propagates the annotations in the source language
sentences to word-aligned target sentences, and a
target system can bootstrap from these projected an-
notations.
For relation extraction, the direct projection strat-
egy can be formularized as follows: ft
(
eit, e
j
t
)
=
fs
(
A(eit), A(e
j
t )
)
, where A(et) is the aligned entity
of et. However, these automatic annotations can be
unreliable because of source text mis-classification
and word alignment errors; thus, it can cause a criti-
cal falling-off in the annotation projection quality.
Although some noise reduction strategies for pro-
jecting semantic relations were proposed (Kim et al,
2010), the direct projection approach is still vulner-
able to erroneous inputs generated by submodules.
We note two main causes for this limitation: (1)
the direct projection approach considers only align-
ments between entity candidates, and it does not
consider any contextual information; and, (2) it is
performed by a single pass process. To solve both of
these problems at once, we propose a graph-based
projection approach for relation extraction.
3 Graph Construction
The most crucial factor in the success of graph-
based learning approaches is how to construct a
graph that is appropriate for the target task. Das
and Petrov (Das and Petrov, 2011) proposed a graph-
based bilingual projection of part-of-speech tagging
by considering the tagged words in the source lan-
guage as labeled examples and connecting them to
the unlabeled words in the target language, while re-
ferring to the word alignments. Graph construction
for projecting semantic relationships is more com-
plicated than part-of-speech tagging because the unit
instance of projection is a pair of entities and not a
word or morpheme that is equivalent to the align-
ment unit.
3.1 Graph Vertices
To construct a graph for a relation projection, we
define two types of vertices: instance vertices V and
context vertices U .
Instance vertices are defined for all pairs of en-
tity candidates in the source and target languages.
Each instance vertex has a soft label vector Y =
[ y+ y? ], which contains the probabilities that
the instance is positive or negative, respectively. The
larger the y+ value, the more likely the instance has
a semantic relationship. The initial label values of an
instance vertex vijs ? Vs for the instance
?
eis, e
j
s
?
in
the source language are assigned based on the con-
fidence score of the extractor fs. With respect to the
target language, every instance vertex vijt ? Vt has
49
the same initial values of 0.5 in both y+ and y?.
The other type of vertices, context vertices, are
used for identifying relation descriptors that are con-
textual subtexts that represent semantic relationships
of the positive instances. Because the characteristics
of these descriptive contexts vary depending on the
language, context vertices should be defined to be
language-specific. In the case of English, we define
the context vertex for each trigram that is located be-
tween a given entity pair that is semantically related.
If the context vertices Us for the source language
sentences are defined, then the units of context in
the target language can also be created based on the
word alignments. The aligned counterpart of each
source language context vertex is used for generat-
ing a context vertex uit ? Ut in the target language.
Each context vertex us ? Us and ut ? Ut also has
y+ and y?, which represent how likely the context
is to denote semantic relationships. The probability
values for all of the context vertices in both of the
languages are initially assigned to y+ = y? = 0.5.
3.2 Edge Weights
The graph for our graph-based projection is con-
structed by connecting related vertex pairs by
weighted edges. If a given pair of vertices is likely to
have the same label, then the edge connecting these
vertices should have a large weight value.
We define three types of edges according to com-
binations of connected vertices. The first type of
edges consists of connections between an instance
vertex and a context vertex in the same language.
For a pair of an instance vertex vi,j and a context
vertex uk, these vertices are connected if the context
sequence of vi,j contains uk as a subsequence. If
vij is matched to uk, the edge weight w
(
vi,j , uk)
)
is assigned to 1. Otherwise, it should be 0.
Another edge category is for the pairs of context
vertices in a language. Because each context vertex
is considered to be an n-gram pattern in our work,
the weight value for each edge of this type represents
the pattern similarity between two context vertices.
The edge weight w(uk, ul) is computed by Jaccard?s
coefficient between uk and ul.
While the previous two categories of edges are
concerned with monolingual connections, the other
type addresses bilingual alignments of context ver-
tices between the source language and the target lan-
guage. We define the weight for a bilingual edge
connecting uks and ult as the relative frequency of
alignments, as follows:
w(uks , u
l
t) = count
(
uks , u
l
t
)
/
?
umt
count
(
uks , u
m
t
)
,
where count (us, ut) is the number of alignments
between us and ut across the whole parallel corpus.
4 Label Propagation
To induce labels for all of the unlabeled vertices on
the graph constructed in Section 3, we utilize the
label propagation algorithm (Zhu and Ghahramani,
2002), which is a graph-based semi-supervised
learning algorithm.
First, we construct an n ? n matrix T that rep-
resents transition probabilities for all of the vertex
pairs. After assigning all of the values on the ma-
trix, we normalize the matrix for each row, to make
the element values be probabilities. The other input
to the algorithm is an n ? 2 matrix Y , which indi-
cates the probabilities of whether a given vertex vi is
positive or not. The matrix T and Y are initialized
by the values described in Section 3.
For the input matrices T and Y , label propagation
is performed by multiplying the two matrices, to up-
date the Y matrix. This multiplication is repeated
until Y converges or until the number of iterations
exceeds a specific number. The Y matrix, after fin-
ishing its iterations, is considered to be the result of
the algorithm.
5 Implementation
To demonstrate the effectiveness of the graph-based
projection approach for relation extraction, we de-
veloped a Korean relation extraction system that was
trained with projected annotations from English re-
sources. We used an English-Korean parallel cor-
pus 1 that contains 266,892 bi-sentence pairs in En-
glish and Korean. We obtained 155,409 positive in-
stances from the English sentences using an off-the-
shelf relation extraction system, ReVerb 2 (Fader et
al., 2011).
1The parallel corpus collected is available in our website:
http://isoft.postech.ac.kr/?megaup/acl/datasets
2http://reverb.cs.washington.edu/
50
Table 1: Comparison between direct and graph-based
projection approaches to extract semantic relationships
for four relation types
Type Direct Graph-basedP R F P R F
Acquisition 51.6 87.7 64.9 55.3 91.2 68.9
Birthplace 69.8 84.5 76.4 73.8 87.3 80.0
Inventor Of 62.4 85.3 72.1 66.3 89.7 76.3
Won Prize 73.3 80.5 76.7 76.4 82.9 79.5
Total 63.9 84.2 72.7 67.7 87.4 76.3
The English sentence annotations in the parallel
corpus were then propagated into the correspond-
ing Korean sentences. We used the GIZA++ soft-
ware 3 (Och and Ney, 2003) to obtain the word align-
ments for each bi-sentence in the parallel corpus.
The graph-based projection was performed by the
Junto toolkit 4 with the maximum number of itera-
tions of 10 for each execution.
Projected instances were utilized as training ex-
amples to learn the Korean relation extractor. We
built a tree kernel-based support vector machine
model using SVM-Light 5 (Joachims, 1998) and
Tree Kernel tools 6 (Moschitti, 2006). In our model,
we adopted the subtree kernel method for the short-
est path dependency kernel (Bunescu and Mooney,
2005).
6 Evaluation
The experiments were performed on the manu-
ally annotated Korean test dataset. The dataset
was built following the approach of Bunescu and
Mooney (Bunescu and Mooney, 2007). The dataset
consists of 500 sentences for four relation types: Ac-
quisition, Birthplace, Inventor of, and Won Prize. Of
these, 278 sentences were annotated as positive in-
stances.
The first experiment aimed to compare two sys-
tems constructed by the direct projection (Kim et al,
2010) and graph-based projection approach. Table 1
shows the performances of the relation extraction of
the two systems. The graph-based system achieved
better performances in precision and recall than the
3http://code.google.com/p/giza-pp/
4http://code.google.com/p/junto/
5http://svmlight.joachims.org/
6http://disi.unitn.it/ moschitt/Tree-Kernel.htm
Table 2: Comparisons of our projection approach to
heuristic and Wikipedia-based approaches
Approach P R F
Heuristic-based 92.31 17.27 29.09
Wikipedia-based 66.67 66.91 66.79
Projection-based 67.69 87.41 76.30
system with direct projection for all of the four re-
lation types. It outperformed the baseline system by
an F-measure of 3.63.
To demonstrate the merits of our work against
other approaches based on monolingual external re-
sources, we performed comparisons with the fol-
lowing two baselines: heuristic-based (Banko et
al., 2007) and Wikipedia-based approaches (Wu and
Weld, 2010). The heuristic-based baseline was built
on the Sejong treebank corpus (Kim, 2006) and the
Wikipedia-based baseline used Korean Wikipedia
articles 7. Table 2 compares the performances of the
two baseline systems and our method. Our proposed
projection-based approach obtained better perfor-
mance than the other systems. It outperformed the
heuristic-based system by 47.21 and the Wikipedia-
based system by 9.51 in the F-measure.
7 Conclusions
This paper presented a novel graph-based projection
approach for relation extraction. Our approach per-
formed a label propagation algorithm on a proposed
graph that represented the instance and context fea-
tures of both the source and target languages. The
feasibility of our approach was demonstrated by our
Korean relation extraction system. Experimental re-
sults show that our graph-based projection helped to
improve the performance of the cross-lingual anno-
tation projection of the semantic relations, and our
system outperforms the other systems, which incor-
porate monolingual external resources.
In this work, we operated the graph-based pro-
jection under very restricted conditions, because of
high complexity of the algorithm. For future work,
we plan to relieve the complexity problem for deal-
ing with more expanded graph structure to improve
the performance of our proposed approach.
7We used the Korean Wikipedia database dump as of June
2011.
51
Acknowledgments
This research was supported by the MKE(The
Ministry of Knowledge Economy), Korea, un-
der the ITRC(Information Technology Research
Center) support program (NIPA-2012-(H0301-12-
3001)) supervised by the NIPA(National IT Industry
Promotion Agency) and Industrial Strategic technol-
ogy development program, 10035252, development
of dialog-based spontaneous speech interface tech-
nology on mobile platform, funded by the Ministry
of Knowledge Economy(MKE, Korea).
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
Proceedings of the fifth ACM conference on Digital li-
braries, pages 85?94.
M. Banko, M. J Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extrac-
tion from the web. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence,
pages 2670?2676.
R. Bunescu and R. Mooney. 2005. A shortest path de-
pendency kernel for relation extraction. In Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 724?731.
R. Bunescu and R. Mooney. 2007. Learning to extract
relations from the web using minimal supervision. In
Proceedings of the 45th annual meeting of the Associ-
ation for Computational Linguistics, volume 45, pages
576?583.
J. Chen, D. Ji, C. L Tan, and Z. Niu. 2006. Relation ex-
traction using label propagation based semi-supervised
learning. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 129?136.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 600?609.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The auto-
matic content extraction (ACE) program?tasks, data,
and evaluation. In Proceedings of LREC, volume 4,
pages 837?840.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identify-
ing relations for open information extraction. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 1535?1545.
R. Grishman and B. Sundheim. 1996. Message under-
standing conference-6: A brief history. In Proceedings
of the 16th conference on Computational linguistics,
volume 1, pages 466?471.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Natural language engineer-
ing, 11(3):311?325.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142.
N. Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions, pages 22?25.
S. Kim, M. Jeong, J. Lee, and G. G Lee. 2010. A cross-
lingual annotation projection approach for relation de-
tection. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 564?571.
H. Kim. 2006. Korean national corpus in the 21st cen-
tury sejong project. In Proceedings of the 13th NIJL
International Symposium, pages 49?54.
A. Moschitti. 2006. Making tree kernels practical for
natural language learning. In Proceedings of the 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, volume 6, pages
113?120.
F. J Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
S. Pado and M. Lapata. 2009. Cross-lingual annotation
projection of semantic roles. Journal of Artificial In-
telligence Research, 36(1):307?340.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the National Conference on Artifi-
cial Intelligence, pages 474?479.
F. Wu and D. Weld. 2010. Open information extraction
using wikipedia. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 118?127.
D. Yarowsky and G. Ngai. 2001. Inducing multilingual
POS taggers and NP bracketers via robust projection
across aligned corpora. In Proceedings of the Second
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 1?8.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. In-
ducing multilingual text analysis tools via robust pro-
jection across aligned corpora. In Proceedings of the
52
First International Conference on Human Language
Technology Research, pages 1?8.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. The Journal of Ma-
chine Learning Research, 3:1083?1106.
M. Zhang, J. Zhang, J. Su, and G. Zhou. 2006. A com-
posite kernel to extract relations between entities with
both flat and structured features. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associa-
tion for Computational Linguistics, pages 825?832.
Z. Zhang. 2004. Weakly-supervised relation classifica-
tion for information extraction. In Proceedings of the
thirteenth ACM international conference on Informa-
tion and knowledge management, pages 581?588.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. School
Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA,
Tech. Rep. CMU-CALD-02-107.
I. Zitouni and R. Florian. 2008. Mention detection cross-
ing the language barrier. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 600?609.
53
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 328?332,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Meta Learning Approach to Grammatical Error Correction 
 Hongsuck Seo1, Jonghoon Lee1, Seokhwan Kim2, Kyusong Lee1 Sechun Kang1, Gary Geunbae Lee1 1Pohang University of Science and Technology 2Institute for Infocomm Research {hsseo, jh21983}@postech.ac.kr, kims@i2r.a-star.edu.sg {kyusonglee, freshboy, gblee}@postech.ac.kr     Abstract We introduce a novel method for grammatical error correction with a number of small corpora. To make the best use of several corpora with different characteristics, we employ a meta-learning with several base classifiers trained on different corpora. This research focuses on a grammatical error correction task for article errors. A series of experiments is presented to show the effectiveness of the proposed approach on two different grammatical error tagged corpora. 1. Introduction As language learning has drawn significant attention in the community, grammatical error correction (GEC), consequently, has attracted a fair amount of attention. Several organizations have built diverse resources including grammatical error (GE) tagged corpora. Although there are some publicly released GE tagged corpora, it is still challenging to train a good GEC model due to the lack of large GE tagged learner corpus. The available GE tagged corpora are mostly small datasets having different characteristics depending on the development methods, e.g. spoken corpus vs. written corpus. This situation forced researchers to utilize native corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers 
focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al, 2000; Lee, 2004; Nagata et al, 2006; Han et al, 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al, 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received less focus. In this paper, we present a novel approach to the GEC task using meta-learning. We focus mainly on article errors for two reasons. First, articles are one of the most significant sources of GE for the learners with various L1 backgrounds. Second, the effective features for article error correction are already well engineered allowing for quick analysis of the method. Our approach is distinguished from others by integrating the predictive models trained on several GE tagged learner corpora, rather than just one GE tagged corpus. Moreover, the framework is compatible to any classification technique. In this study, we also use a native corpus employing Dahlmeier and Ng?s approach. We demonstrate the effectiveness of the proposed method against baseline models in article error correction tasks. 
328
The remainder of this paper is organized as follows: Section 2 explains our proposed method. The experiments are presented in Section 3. Finally, Section 4 concludes the paper. 2. Method Our method predicts the type of article for a noun phrase within three classes: null, definite, and indefinite. A correction arises when the prediction disagrees with the observed article. The meta-learning technique is applied to this task to deal with multiple corpora obtained from different sources. A meta-classifier decides the final output based on the intermediate results obtained from several base classifiers. Each base classifier is trained on a different corpus than are the other classifiers. In this work, the feature extraction processes used for the base classifiers are identical to each other for simplicity, although they need not necessarily be identical. The meta-classifier takes the output scores of the base classifiers as its input and is trained on the held-out development data (Figure 1a). During run time, the trained classifiers are organized in the same manner. For the given features, the base classifiers independently calculate the score, then the meta-classifier makes the final decision based on the scores (Figure 1b). 2.1. Meta-learning Meta-learning is a sequential learning process following the output of other base learners (classifiers). Normally, different classifiers successfully predict results on different parts of the 
input space, so researchers have often tried to combine different classifiers together (Breiman, 1996; Cohen et al, 2007; Zhang, 2007; Ayd?n, 2009; Menahem et al, 2009). To capitalize on the strengths and compensate for the weaknesses of each classifier, we build a meta-learner that takes an input vector consisting of the outputs of the base classifiers. The performance of meta-learning can be improved using output probabilities for every class label from the base classifiers. The meta-classifier for the proposed method consists of multiple linear classifiers. Each classifier takes an input vector consisting of the output scores of each base classifier and calculates a score for each type of article. The meta-classifier finally takes the class having the maximum score. A common design of an ensemble is to train different base classifiers with the same dataset, but in this work one classification technique was used with different datasets each having different characteristics. Although only one classification method was used in this work, different methods each well-tuned to the individual corpora may be used to improve the performance. We employed the meta-learning method to generate synergy among corpora with diverse characteristics. More specifically, it is shown by cross validation that meta-learning performs at a level that is comparable to the best base classifier (Dzeroski and Zenko, 2004). 2.2. Base Classifiers In the meta-learning framework, the performance of the base classifiers is important because the improvement in base classification generally enha-
Figure 1: Overview of the proposed method 
329
nces the overall performance. The base classifiers can be expected to become more informative as more data are provided. We followed the structural learning approach (Ando and Zhang, 2005), which trains a model from both a native corpus and a GE tagged corpus (Dahlmeire and Ng, 2011), to improve the base classifiers by the additional information extracted from a native corpus. Structural learning is a technique which trains multiple classifiers with common structure. The common structure chooses the hypothesis space of each individual classifier and the individual classifiers are trained separately once the hypothesis space is determined. The common structure can be obtained from auxiliary problems which are closely related to the main problems. A word selection problem is a task to predict the appropriate word given the surrounding context in a native corpus and is a closely related auxiliary problem of the GEC task. We can obtain the common structure from the article selection problem and use it for the correction problem. In this work, all the base classifiers used the same least squares loss function for structural learning.  We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al 2005) to extract the features. 2.3. Evaluation Metric The effectiveness of the proposed method is evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances. Precision is the ratio of the suggested corrections that agree with the tagged answer to the total number of the suggested corrections whereas recall is the ratio of the suggested corrections that agree with the tagged answer to the total number of corrections in the corpus. 3. Experiments 3.1. Datasets In this work we used a native corpus and two GE tagged corpora. For the native corpus, we used                                                             1 http://nlp.stanford.edu/software/corenlp.shtml 
news data2 which is a large English text extracted from news articles. The First Certificate in English exams in the Cambridge Learner Corpus 3 (hereafter, CLC-FCE; Yannakoudakis et al, 2011) and the Japanese Learner English corpus (Izumi et. al., 2005) were used for the GE tagged corpora. We extracted noun phrases from each corpus by parsing the text of the respective corpora. (1) We parsed the native corpus from the beginning until approximately a million noun phrases are extracted. (2) About 90k noun phrases containing ~3,300 mistakes in article usage were extracted from the entire CLC-FCE corpus, and (3) about 30k noun phrases containing ~2,500 mistakes were extracted from the JLE corpus.  The extracted noun phrases were used for our training and test data. We hold out 10% of the data for the test. We applied 20% under-sampling to the training instances that do not have any errors to alleviate data imbalance in the training set. We emphasize the fact that the two learner corpora differ from each other in three aspects. The first aspect is the styles of the texts: the CLC is literary whereas the JLE is colloquial. The second is the error rate: about 3.5% for CLC-FCE and   8.5% for JLE. Finally, the third is the distribution of L1 languages of the learners: the learners of the CLC corpus have various L1 backgrounds whereas the learners of the JLE consist of only Japanese. These experiments demonstrate the effectiveness of the proposed method relying on the diversity of the corpora. The native corpus was used to find the common structure using structural learning and two GE tagged learner corpora are used to train the base classifiers by structural learning with the common structure obtained from the news corpus. We trained three classifiers for comparison; (1) the classifier (INTEG) trained with the integrated training set of the two GE tagged corpora, and two base classifiers used for the ensemble: (2) the base classifier (CB) trained only with the CLC-FCE and (3) the other base classifier (JB) trained with the JLE. 3.2. Results The accuracy obtained from the word selection task with the news corpus was 76.10%. Upon                                                             2 http://www.statmt.org/wmt09/translation-task.html 3 http://www.ilexir.com/ 
330
obtaining the parameters of the word selection task, the structural parameter ?  was calculated by singular value decomposition and was used for the structural learning of the main GEC task. We used three different test data sets: the CLC-FCE, the JLE and an integrated test set of the two. The accuracy (Acc.) and the precision (Prec.) of the INTEG was poorer than CB on the CLC-FCE test set (Table 1), whereas INTEG outperformed JB on the JLE test (Table 2).  Some instances extracted from the CLC-FCE corpus have similar characteristics to the instances from the JLE corpus. This overlap of instances affected the performance in both positive and negative ways. Prediction of instances similar to those in the JLE was enhanced. Consequently, INTEG model demonstrated better accuracy and precision for the JLE test set. Unfortunately, for the CLC test set, the instances resulted in lower accuracy and precision. The proposed model is able to alleviate this model bias due to similar instances observed in the INTEG model. The accuracy of the proposed model consistently increased by over 10% for all three data sets. The relative performance gain in terms of F1-score (F1) was 15% on the integrated set. This performance gain stems from the over   25% relative improvement of the precision (Table 1, 2 and 3). We believe the improvement comes from the contribution of reconfirming procedures performed 
by the meta-classifier. When the prediction of the two base classifiers conflicts with each other, the meta-classifier tends to choose the one with a higher confidence score; this choice improves the accuracy and precision because known features generate a higher confidence whereas unseen or less-weighted features generate a lower score. Although the proposed model introduced a tradeoff between precision and recall (Rec.), this tradeoff was tolerable in order to improve the overall F1-score. Since GEC is a task where false alarm is critical, obtaining high precision is very important. The low precision on the whole experiments is due to the data imbalance. Instances in the dataset are mostly not erroneous, e.g., only 3.5% of erroneous instances for the CLC corpus. The standard for correct prediction is also very strict and does not allow multiple answers. Performance can be evaluated in a more realistic way by applying a softer standard, e.g., by evaluating manually. 4. Conclusion We have presented a novel approach to grammatical error correction by building a meta-classifier using multiple GE tagged corpora with different characteristics in various aspects. The experiments showed that building a meta-classifier overcomes the interference that occurs when training with a set of heterogeneous corpora. The proposed method also outperforms the base classifier themselves tested on the same class of test set as the training set with which the base classifiers are trained. A better automatic evaluation metric would be needed as further research. Acknowledgments Industrial Strategic technology development program, 10035252, development of dialog-based spontaneous speech interface technology on mobile platform, funded by the Ministry of Knowledge Economy (MKE, Korea).   
Model Acc. Prec. Rec. F1 INTEG 73.37 4.69 72.39 8.82 CB 77.20 5.39 71.17 10.03 Proposed 86.99 6.17 45.77 10.88 Table 1: Best results for GEC task on CLC-FCE test set.  Model Acc. Prec. Rec. F1 INTEG 78.87 14.88 85.47 25.35 JB 78.02 14.49 86.32 24.82 Proposed 89.61 19.28 46.60 27.27 Table 2: Best results for GEC task on JLE test set. Model Acc. Prec. Rec. F1 INTEG 74.64 6.84 77.86 12.58 Proposed 87.50 8.61 46.12 14.52 Table 3: Best results for GEC task on the integrated set of CLC-FCE and JLE test sets.  
331
References  R.K. Ando and T. Zhang. 2005. A framework for learn- ing predictive structures from multiple tasks and un- labeled data. Journal of Machine Learning Research, 6, pp. 1817-1853. U. Ayd?n, S. Murat, Olcay T Y?ld?z, A. Ethem, 2009, Incremental construction of classifier and discriminant ensembles, Information Science, 179 (9), pp. 144-152. L. Breiman, 1996, Bagging predictors, Machine Learning, pp. 123?140. S. Cohen, L. Rokach, O. Maimon, 2007, Decision tree instance space decomposition with grouped gain-ratio, Information Science, 177 (17), pp. 3592?3612. D. Dahlmeier, H. T. Ng, 2011, Grammatical error correction with alternating structure optimization, In Proceedings of the 49th Annual Meeting of the ACL-HLT 2011, pp. 915-923. R. De Felice. 2008. Automatic Error Detection in Non- native English. Ph.D. thesis, University of Oxford. S. Dzeroski, B. Zenko, 2004, Is combining classifiers with stacking better than selecting the best one?, Machine Learning, 54 (3), pp. 255?273. J. R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43nd Annual Meeting of the ACL, pp. 363-370. N.R. Han, M. Chodorow, and C. Leacock. 2006. De- tecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(02), pp. 115-129. N.R. Han, J. Tetreault, S.H. Lee, and J.Y. Ha. 2010. Using an error-annotated learner corpus to develop an ESL/EFL error correction system. In Proceedings of LREC. D. Klein and C.D. Manning. 2003a. Accurate unlexical- ized parsing. In Proceedings of ACL, pp. 423-430. D. Klein and C.D. Manning. 2003b. Fast exact inference with a factored model for natural language processing. Advances in Neural Information Processing Systems (NIPS 2002), 15, pp. 3-10. K. Knight and I. Chander. 1994. Automated postediting of documents. In Proceedings of AAAI, pp. 779-784. J. Lee. 2004. Automatic article restoration. In Proceed- ings of HLT-NAACL, pp. 31-36. R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A feedback-augmented method for detecting errors in 
the writing of learners of English. In Proceedings of COLING-ACL, pp. 241--248. A. Mariko, 2007, Grammatical errors across proficiency levels in L2 spoken and written English, The Economic Journal of Takasaki City University of Economics, 49 (3, 4), pp. 117-129. E. Menahem, L. Rokach, Y. Elovici, 2009, Troika-An imporoved stacking schema for classification tasks, Information Science, 179 (24), pp. 4097-4122. G. Minnen, F. Bond, and A. Copestake. 2000. Memory- based learning for article generation. In Proceedings of CoNLL, pp. 43-48. E. Izumi, K. Uchimoto, H. Isahara, 2005, Error annotation for corpus of Japanese learner English, In Proceedings of the 6th International Workshop on Linguistically Interpreted Corpora, pp. 71-80. A. Rozovskaya and D. Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Pro- ceedings of HLT-NAACL, pp. 154-162. K. Toutanova and C. D. Manning. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In Proceedings of the Joint SIGDAT Conference on EMNLP/VLC-2000, pp. 63-70. H.Yannakoudakis, T. Briscoe, B. Medlock, 2011, A new dataset and method for automatically grading ESOL texts, In Proceedings of ACL, pp. 180-189. G. P. Zhang, 2007, A neural network ensemble method with jittered training data for time series forecasting, Information Sciences: An International Journal, 177 (23), pp. 5329?5346. 
332
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 9?16
Manchester, August 2008
An Integrated Dialog Simulation Technique for Evaluating Spoken Dialog
Systems
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Gary Geunbae Lee
Department of Computer Science and Engineering
Pohang University of Computer Science and Technology(POSTECH)
San 31, Hyoja-Dong, Pohang, 790-784, Korea
{hugman, lcj80, getta, gblee}@postech.ac.kr
Abstract
This paper proposes a novel integrated dialog
simulation technique for evaluating spoken di-
alog systems. Many techniques for simulat-
ing users and errors have been proposed for
use in improving and evaluating spoken dia-
log systems, but most of them are not easily
applied to various dialog systems or domains
because some are limited to specific domains
or others require heuristic rules. In this pa-
per, we propose a highly-portable technique for
simulating user intention, utterance and Au-
tomatic Speech Recognition (ASR) channels.
This technique can be used to rapidly build a
dialog simulation system for evaluating spo-
ken dialog systems. We propose a novel user
intention modeling and generating method that
uses a linear-chain conditional random field, a
data-driven domain specific user utterance sim-
ulation method, and a novel ASR channel sim-
ulation method with adjustable error recogni-
tion rates. Experiments using these techniques
were carried out to evaluate the performance
and behavior of previously developed dialog
systems designed for navigation dialogs, and
it turned out that our approach is easy to set up
and shows the similar tendencies of real users.
1 Introduction
Evaluation of spoken dialog systems is essential for de-
veloping and improving the systems and for assessing
their performance. Normally, humans are used to eval-
uate the systems, but training and employing human
evaluators is expensive. Furthermore, qualified human
users are not always immediately available. These in-
evitable difficulties of working with human users can
cause huge delay in development and assessment of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
spoken dialog systems. To avoid the problems that re-
sult from using humans to evaluate spoken dialog sys-
tems, developers have widely used dialog simulation,
in which a simulated user interacts with a spoken dia-
log system.
Many techniques for user intention, utterance and er-
ror simulation have been proposed. However, previ-
ously proposed simulation techniques cannot be eas-
ily applied to evaluate various dialog systems, because
some of these techniques are specially designed to work
with their own dialog systems, some require heuristic
rules or flowcharts, and others try to build user side
dialog management systems using specialized dialog
managing methods. These problems motivated us to
develop dialog simulation techniques which allow de-
velopers to build dialog simulation systems rapidly for
use in evaluating various dialog systems.
To be successful, a simulation approach should not
depend on specific domains or rules. Also it should not
be coupled to a specific dialog management method.
Furthermore, successful dialog simulation should fully
support both user simulation and environment simula-
tion. In user simulation, it must be capable of simu-
lating both user intentions and user utterances, because
user utterances are essential for testing the language un-
derstanding component of the dialog system. In addi-
tion to user simulation, environment simulation such as
ASR channel simulation is desirable because it allows
developers to test the dialog system in various acoustic
environments.
In this paper, we propose novel dialog simulation
techniques which satisfy these requirements. We in-
troduce a new user intention simulation method based
on the sequential graphical model, and a user utterance
simulator which can generate diverse natural user utter-
ances. The user intention and utterance simulators are
both fully data-driven approaches; therefore they have
high domain- and language portability. We also propose
a novel Automatic Speech Recognizer (ASR) channel
simulator which allows the developers to set the de-
sired speech recognition performance level. Through
a case study, we showed that our approach is feasible in
successful dialog simulation to evaluate spoken dialog
9
systems.
This paper is structured as follows. We first provide a
brief introduction of other dialog simulation techniques
and their differences from our approach in Section 2.
We then introduce the overall architecture and the de-
tailed methods of intention, utterance and ASR channel
simulation in Section 3. Experiments to test the simula-
tion techniques, and a case study are described in Sec-
tion 4. We conclude with a brief summary and suggest
directions for future work in Section 5.
2 Related Works
Dialog simulation techniques can be classified accord-
ing to the purpose of the simulation. One of the pur-
poses is to support the refinement of dialog strategies.
Some techniques use large amounts of simulated data
for a systematic exploration of the dialog state space
in the framework of reinforcement learning (Schatz-
mann et al, 2005; Schatzmann et al, 2007a). Other
techniques use simulation techniques to investigate and
improve the target dialog strategies by examining the
results heuristically or automatically (Chung, 2004;
Rieser and Lemon, 2006; Torres et al, 2008). A sec-
ond purpose of dialog simulation techniques is to eval-
uate the dialog system itself qualitatively. Eckert et al,
(1997) and Lo?pez-Co?zar et., (2003; 2006) used a dialog
simulation to evaluate whole dialog systems.
Dialog simulation techniques can also be classified
according to the layers of the simulation. Typically, di-
alog simulation can be divided into three layers: user
intention, user surface (utterance) and error simulation.
Some studies have focused only on the intention level
simulation (Rieser and Lemon, 2006; Schatzmann et
al., 2007b; Cuayahuitl et al, 2005). The main purpose
of those approaches was to collect and examine inten-
tion level dialog behavior for automatically learning di-
alog strategies. In this case, surface and error simula-
tions were neglected or simply accessed normally.
Another approach is to simulate both user intention
and surface. In this approach, user utterance generation
is designed to express a given intention. Chung (2004)
tried to use the natural language generation module
of (Seneff, 2002) to generate this surface. He used a
speech synthesizer to generate user utterances. Lo?pez-
Co?zar et., (2003; 2006) collected real human utter-
ances, and selected and played the voice to provide in-
put for the spoken dialog system. Both Chung (2004)
and Lo?pez-Co?zar et., (2003; 2006) used rule based in-
tention simulation. They used real ASR to recognize
the synthesized or played voice; hence, ASR channel
simulation is not needed in their techniques. Scheffler
and Young (2000; 2001) used the lattices which are de-
rived from the grammars used by the recognition en-
gine, but generated user utterances by associating the
lattice edges with intentions. During utterance gener-
ation, they simulated errors in recognition and under-
standing by probabilistic substitution on the selection of
the edge. Schatzmann et al, (2007a; 2007b) proposed a
statistical model for user utterance generation and error
simulation using agenda based intention simulation.
The existing rule-based techniques for simulating in-
tentions or surfaces are not appropriate in the sense of
portability criteria. In addition, specific dialog manag-
ing techniques based user simulators (e.g., (Torres et
al., 2008)) are not desirable because it is not easy to
implement these techniques for other developers. An-
other important criterion for evaluating dialog simula-
tion techniques for use in evaluating spoken dialog sys-
tems is the range of simulation layers. Simulations that
are restricted to only the intention level are not suffi-
cient to evaluate the whole dialog system. Domain and
language independent techniques for simulating both
intentions and utterances are needed, and ASR channel
simulation is desirable for evaluating the spoken dia-
log systems accurately because human-machine dialog
is heavily influenced by speech recognition errors.
3 Dialog Simulation Architecture for
Dialog System Evaluation
3.1 Overall Architecture
Typical spoken dialog systems deal with the dialog be-
tween a human user and a machine. Human users ut-
ter spoken language to express their intention, which is
recognized, understood and managed by ASR, Spoken
Language Understanding (SLU) and Dialog Manager
(DM). Conventionally, ASR has been considered to be
a component of dialog systems. However, in this re-
search, we do not include a real ASR module in the di-
alog system component because a real ASR takes only
fixed level of speech as an input. To use real voices,
we must either collect real human speech or generate
voices using a speech synthesizer. However, both ap-
proaches have limitations. When recording and play-
ing real human voices, the cost of data collection is
high and the simulator can simulate only the behav-
ior of the humans who were recorded. When using a
speech synthesizer, the synthesizer can usually generate
the speech of one person, on a limited variety of speech
behaviors; this means that the dialog system cannot be
evaluated under various conditions. Also, in both ap-
proaches, freely adjusting the speech recognition per-
formance level is difficult. In this research, instead of
using real speech we simulate the ASR channel and add
noises to a clean utterance from the user simulator to
mimic the speech recognition result.
The overall architecture of our dialog simulation sep-
arates the user simulator into two levels: user intention
simulator and utterance simulator (Fig. 1). The user
intention simulator accepts the discourse circumstances
with system intention as input and generates the next
user intention. The user utterance simulator constructs
a corresponding user sentence to express the given user
intention. The simulated user sentence is fed to the
ASR channel simulator, which then adds noises to the
utterance. This noisy utterance is passed to a dialog sys-
10
Dialog System
User Simulator
ASRChannelSimulator SLU
Dialog ManagerUser Intention Simulator
User Utterance Simulator
System IntentionDialog Logs
Evaluator EvaluationResults
= Simulated User = = System =
Figure 1: Overall architecture of dialog simulation
tem which consists of a SLU and a DM. The dialog sys-
tem understands the user utterance, manages dialog and
passes the system intention to the user simulator. User
simulator, ASR channel simulator and dialog system re-
peat the conversation until the user simulator generates
an end to the dialog.
After finishing simulating one dialog successfully,
this dialog is stored in Dialog Logs. If the dialog logs
contain enough dialogs, the evaluator uses the logs to
evaluate the performance of the dialog system.
3.2 User Intention Simulation
The task of user intention simulation is to generate sub-
sequent user intentions given current discourse circum-
stances. The intention is usually represented as ab-
stracted user?s goals and information on user?s utter-
ance (surface). In other words, generating the user?s
next semantic frame from the current discourse status
constitutes the user intention simulation.
Dialog is basically sequential behavior in which par-
ticipants use language to interact with each other. This
means that intentions of the user or the system are natu-
rally embedded in a sequential structure. Therefore, in
intention modeling we must consider how to model this
sequential property. Also, we must understand that the
user?s intention depends not only on previous n-gram
user and system intentions, but also on diverse dis-
course circumstances, including dialog goal, the num-
ber of items, and the number of filled component slots.
Sophisticated user intention modeling should be able to
reflect the discourse information.
To satisfy the sequential property and use rich
information for user intention modeling, we used
linear-chain Conditional Random Field (CRF) model
(Lafferty et al, 2001) for user intention modeling.
Let Y,X be random vectors, ? = {?
k
} ? R
K be a
parameter vector, and {f
k
(y, y
?
,x
t
)}
K
k=1
be a set of
real-valued feature functions. Then a linear-chain CRF
is a distribution of p(y|x) that takes the form
UI1
DI1
UI2
DI2
UIt
DIt
UIt+1
DIt+1
?
Figure 2: Conditional Random Fields for user intention
modeling. UI
t
: User Intention ; DI
t
: Discourse Infor-
mation for the tth user turn
p(y|x) =
1
Z(x)
exp
{
K
?
k=1
?
k
f
k
(y
t
, y
t?1
,x
t
)
} (1)
where Z(x) is an instance-specific normalization func-
tion.
Z(x) =
?
y
exp
{
K
?
k=1
?
k
f
k
(y
t
, y
t?1
,x
t
)
}
CRF is an undirected graphical model that defines a
single log-linear distribution over the joint probability
of an entire label sequence given a particular observa-
tion sequence. This single distribution removes the per-
state normalization requirement and allows entire state
sequences to be accounted for at once. This property is
well suited to model the entire sequence of intentions in
a dialog. Also, CRF is a conditional model, and not a
joint model (such as the Hidden Markov Model). Arbi-
trary facts can be captured to describe the observation
in the form of indicator functions. This means that CRF
allows us to use rich discourse information to model in-
tentions.
CRF has states and observations in each time line.
We represent the user intention as state and discourse
information as observations in CRF (Fig. 2). We rep-
resent the state as a semantic frame. For example in
the semantic frame representing the user intention for
the utterance ?I want to go to city hall? (Fig. 3), dia-
log act is a domain-independent label of an utterance at
the level of illocutionary force (e.g. statement, request,
wh question) and main goal is the domain-specific user
goal of an utterance (e.g. give something, tell purpose).
Component slots represent named entities in the utter-
ance. We use the cartesian product of each slot of se-
mantic frame to represent the state of the utterance in
our CRF model. In this example, the state symbol is
?request?search loc?loc name?.
For the observation, we can use various discourse
events because CRF allows using rich information by
interpreting each event as an indicator function. Be-
cause we pursue the portable dialog simulation tech-
nique, we separated the features of the discourse in-
formation into those that are domain independent and
those that are domain dependent. Domain independent
11
I want to go to city hall.requestsearch_loccityhall
I/PRP want/VB to/TO go/VB to/TO [loc_name]/[loc_name]PRP, VB, TO, VB, TO, [loc_name]
I, want, to, go, to, [loc_name]
Structure PRP ? VB ? TO ? VB ? TO ? [loc_name]I ? want ? to ? go ? to ? [loc_name]
Semantic Frame for User Inention Simulation
Preprocessing Information for User Utterance Simulation
Structure Tags
Word Vocabulary
processed  utterance
Generation Target for User Utterance Simulation
Word Sequence
raw user utterance dialog_act main_goal component.[loc_name] 
Figure 3: Example of semantic frame for user inten-
tion, and preprocessing and generation target for user
utterance simulation.
features include discourse information which is not rel-
evant to the specific dialog domain and system. For ex-
ample, previous system acts in Fig. 4 are not dependent
on specific dialog domain. The actual values of pre-
vious system acts could be dependent on each dialog
domain and system, but the label itself is independent
because every dialog system has system parts and corre-
sponding system acts. In contrast, domain specific dis-
course information exists for each dialog system. For
example, in the navigation domain (Fig. 4), the cur-
rent position of the user or the user?s favorite restau-
rant could be very important for generating the user?s
intention. This information is dependent on the spe-
cific domain and system. We handle these features as
?OTHER INFO?.
We trained the user intention model using dialog ex-
amples of human-machine. One training example con-
sists of a sequence of user intentions and discourse in-
formation features in a given dialog. We collected train-
ing examples and trained the intention model using a
typical CRF training method, a limited-memory quasi-
Newton code for unconstrained optimization (L-BFGS)
of (Liu and Nocedal, 1989).
To generate user intentions given specific discourse
circumstances, we calculate the probability of a se-
quence of user intentions from the beginning of the
dialog to the corresponding turn. For example, sup-
pose that we need to generate user intention at the
third turn (UI
3
) (Fig. 2). We have previously sim-
ulated user intentions UI
1
and UI
2
using DI
1
and
DI
2
. In this case, we calculate the probability of
UI
1
? UI
2
? UI
3
given DI
1
, DI
2
and DI
3
. No-
tice that DI
3
contains discourse information at the third
turn: it includes previous system intention, attributes
and other useful information. Using the algorithm (Fig.
5) we generate the user intention at turn t. The proba-
bility of P (UI
1
, UI
2
, . . . , UI
t
|DI
1
, DI
2
, . . . , DI
t
) is
calculated using the equation (1). In the genera-
tion of user intention at t turn, we do not select the
UI
t
which has higher probability. Instead, we se-
lect UI
t
randomly based on the probability distribution
PREV_1_SYS_ACT previous system action.
Ex) PREV_1_SYS_ACT=confirm
PREV_1_SYS_ACT_ATTRIBUTES previous system mentioned attributes. 
Ex) PREV_1_SYS_ACT_attributes=city_name
PREV_2_SYS_ACT previous system action. 
Ex) PREV_2_SYS_ACT=confirm
PREV_2_SYS_ACT_ATTRIBUTES previous system mentioned attributes.
Ex) PREV_2_SYS_ACT_attributes=city_name
SYSTEM_HOLDING_COMP_SLOT system recognized component slot. 
Ex) SYSTEM_HOLDING_COMP_SLOT=loc_name
OTHER_INFO other useful domain dependent information
Ex) OTHER_INFO(user_fav_rest)=gajokjung
Domain Independent Features
Domain Dependent Features
Figure 4: Example feature design for navigation do-
main
UI t   ? user intention at t turn
S  ? user intentions set (UI t  ? S )
UI 1 , UI 2 , ? , UI t-1  ? already simulated user intention sequence
DI 1 , DI 2 , ? , DI t  ? discourse information from 1 to t  turn
For each UI t  in S
       Calculate P( UI 1 , UI 2 , ?, UI t |DI 1 , DI 2 , ?, DI t )
UI t  ? random user intention from P( UI 1 , UI 2 , ?, UI t |DI 1 , DI 2 , ?, DI t )
Figure 5: User intention generation algorithm
P (UI
1
, UI
2
, . . . , UI
t
|DI
1
, DI
2
, . . . , DI
t
) because we
want to generate diverse user intention sequence given
the same discourse context. If we select UI
t
which has
highest probability, user intention simulator always re-
turns the same user intention sequence.
3.3 User Utterance Simulation
Utterance simulation generates surface level utterances
which express a given user intention. For example, if
users want to go somewhere and provide place name
information, we need to generate corresponding utter-
ances (e.g. ?I want to go to [place name] or ?Let?s go to
[place name]?). We approach the task of user utterance
simulation by assuming that the types of structures and
the vocabulary are limited when we make utterances to
express certain context and intention in a specific do-
main, and that humans express their intentions by re-
combining and re-aligning these structures and vocabu-
laries.
To model this process, we need to collect the types of
structures and vocabularies. For this, we need to define
the context space. We define the structure and vocabu-
lary space as a production of dialog act and main goal.
In an example of semantic frame for the utterance ?I
want to go to city hall? (Fig. 3), the structure and vocab-
ulary (SV) space ID is ?request # search loc?, which is
produced by the dialog act and the main goal. We col-
lect structure tags, which consist of a part of speech
tag, a component slot tag, and a vocabulary that cor-
responds to SV space. For example (Fig. 3), structure
12
1. Repeat generate  S t  based on PSV(S t+1 |S t ),    until S T  = <setence_end> , where S t  ? S  ,   t=1,2,3,?.T .2. Generate W t  based on PSV (W t |S t ), where    t =1,2,3,..,T  , W t  ? V3. The generation word sequence W ={W1,W2,..,WT} is inserted    into the set of generated utterance U4. Repeat 1  to 3  for Max_Generation_Number    times, Max_Generation_Number is given by developers
1.Rescore the utterance U k  in the set of U  by the measure2.Select top n-best
First Phase ? Generating Structures and Words given SV space 
Second Phase ? Selection by measure 
Figure 6: Algorithm of user utterance simulation
tags include PRP, VB, TO, VB as a part of speech tag
and [loc name] as a component slot tag. The vocab-
ulary includes I, want, to, go, and [loc name]. In the
vocabulary, every named-entity word is replaced with
its category name.
In this way, we can collect the structure tags and vo-
cabulary for each SV space from the dialog logs. For
the given SV space, we estimate probability distribu-
tions for statistical user utterance simulation using a
training process. For each space, we estimate tag tran-
sition probability P
SV
(S
t+1
|S
t
) and collect structure
tags set S
SV
and vocabularies V
SV
.
We devised a two-phase user utterance generation al-
gorithm (Fig. 6). Symbols are as follows. The detail
explanation of Fig. 6 will be followed in the next sub-
sections.
? S
SV
: structure tag set for given SV
? V
SV
: vocabularies for given SV
? S
i
: structure tag, i = 0, ..., T, S
i
? S
SV
? W
i
: word, i = 0, ..., T, W
i
? V
SV
? W
seq
: generated word sequence. W
seq
=
(W
1
,W
2
, ...,W
T
)
? U
k
: k-th sampled utterance,
k = 1, ..., Max Sampling Number, U
k
? U
3.3.1 First Phase - Generating Structure and
Word Sequence
We generate the structure tag S
1
based on the prob-
ability of P
SV
(S
1
| < sentence start >) and then
S
1
influences the generating of S
2
after P
SV
(S
2
|S
1
).
In this way, a structure tag chain is generated sequen-
tially based on the structure tag transition probability
P
SV
(S
t+1
|S
t
) until the last generated structure tag S
T
is < sentence end >. We assume that the current
structure tag has a first order Markov property, which
means that the structure tag is only influenced by the
previous structure tag. After the structure tags are
generated, the emission probability P
SV
(W
t
|S
t
)(w =
1, . . . , T ) is used to generate the word sequence given
the tag sequence. We iterate the process of generating
structures and word sequences sufficient times to gen-
erate many different structure tags and word sequences
which may occur in real human expressions. Select-
ing natural utterances from the generated utterances re-
quires an automatic evaluation metric.
3.3.2 Second Phase - Selection by the BLEU
measure
To measure the naturalness of the generated utter-
ances, we use the BLEU (Bilingual Evaluation Under-
study) score (Papineni et al, 2001) which is widely
used for automatic evaluation in Statistical Machine
Translation (SMT). In SMT, translated candidate sen-
tences are evaluated by comparing semantically equiv-
alent reference sentences which have been translated
by a human. Evaluation of the user utterance gener-
ation shares the same task of evaluation in SMT. We
can evaluate the naturalness of generated utterances by
comparing semantically equivalent reference utterances
collected by humans. Therefore, the BLEU score can
be adopted successfully to measure the naturalness of
the utterances.
The BLEU score is the geometric mean of the n-gram
precisions with a brevity penalty. The original BLEU
metric is used to evaluate translated sentences by com-
paring them to several reference sentences. We mod-
ified the BLEU metric to compare one generated ut-
terance with several reference utterances. To rescore
the generated utterances, we used the Structure and
Word interpolated BLEU score (SWB). After the first
phase, we obtain generated utterances which have both
structure and word sequence. To measure the natu-
ralness of a generated utterance, we check both struc-
tural and lexical naturalness. We calculated Struc-
ture Sequence BLEU score using the generated struc-
ture tags sequences instead of words sequences with the
reference structure tag sequences of the SV space in the
BLEU calculation process. The Word Sequence BLEU
is calculated by measuring BLEU score using the gener-
ated words sequence with the reference word sequences
of the SV space. SWB is calculated as:
SWB = ? ? Structure Sequence BLEU
+(1? ?) ? Word Sequence BLEU
In this study, we set ? = 0.5. Using SWB, we select
the top 20-best generated utterances and return a corre-
sponding generated utterance by selecting one of them
randomly.
3.4 ASR channel Simulation
ASR channel simulation generates speech recognition
errors which might occur in the real speech recognition
process. In this study, we simulate the ASR channel and
modify the generated clean utterance to a speech rec-
ognized erroneous utterance. Successful ASR channel
simulation techniques should have the following prop-
erties: the developer should be able to set the simu-
lated word error rate (WER) between 0% ? 100%; the
simulated errors should be generated based on realistic
13
phone-level and word-level confusions; and the tech-
nique should be easily adapted to new tasks, at low cost.
Our ASR channel simulation approach is designed
to satisfy these properties. The proposed ASR channel
simulation method involved four steps: 1) Determining
error position 2) Generating error types on error marked
words. 3) Generating ASR errors such as substitution,
deletion and insertion errors, and 4) Rescoring and se-
lecting simulated erroneous utterances (Fig. 7 for Ko-
rean language example).
In the first step, we used the WER to determine the
positions of erroneous words. For each word, we ran-
domly generate a number between 0 and 1. If this num-
ber is between 0 and WER, we mark the word Error
Word (1); otherwise we mark the word Clean Word (0).
In the second step, we generate ASR error types for the
error marked words based on the error type distribution.
In the third step, we generate various types of ASR er-
ror. In the case of deletion error, we simply delete the
error marked word from the utterance. In the case of
insertion error, we select one word from the pronunci-
ation dictionary randomly, and insert it before the error
marked word. In the case of substitution error, we use a
more complex process to select a substitutable word.
To select a substitutable word, we compare the
marked error word with the words from pronunciation
dictionary which are similar in syllable sequence and
phoneme sequence. First, we convert the final word
sequence from the user simulator into a phoneme se-
quence using a Grapheme-to-Phoneme (G2P) module
(Lee et al, 2006). Then, we extract a part of the
phoneme sequence which is similar to the error marked
word from the entire phoneme sequence of the ut-
terance. The reason for extracting a target phoneme
sequence corresponding to one word from the entire
phoneme sequence is that the G2P results vary between
the boundaries of words. Then, we separate the marked
word into syllables and compare their syllable-level
similarity to other words in the pronunciation dictio-
nary. We calculate a similarity score which interpolates
syllable and phoneme level similarity using following
equations.
Similarity = ? ? Syllable Alignment Score
+(1? ?) ? Phone Alignment Score
We used the dynamic global alignment algorithm of
(Needleman and Wunsch, 1970) for both syllable and
phoneme sequence alignment. This alignment algo-
rithm requires a weight matrix. As a weight matrix,
we used a vowel confusion matrix which is based on
the manner of articulation. We consider the position
(back/front, high/mid/low) of the tongue and the shape
(round/flat) of the lips. We select candidate words
which have higher similarity than an arbitrary thresh-
old ? and replace the error marked word with a random
word from this set. We repeat steps 1 to 3 many times
(usually 100) to collect error added utterances.
In the fourth step, we rescore the error added utter-
si-chung e ga go sip eo (I want to go to city hall)si-cheong e ga go sip eo0 1 1 0 1 0- del sub - sub -Generating Error Types and Positions
Generating Candidate Lists of Noisy Utterance si-cheong - geo-gi go si eosi-cheong - ga-ja go seo eosi-cheong - gat go seu eosi-cheong - geot go sil eo
Selecting Noisy Utterance si-cheong gat go seo eo
Error Generation
Ranking with LM score
1-Step2-Step
3-Step
4-Step
Figure 7: Example of ASR channel simulation
ances using the language model (LM) score. This LM
is trained using a domain corpus which is usually used
in ASR. We select top n-best erroneous utterances (we
set n=10) and choose one of them randomly. This utter-
ance is the final result of ASR channel simulator, and is
fed into the dialog system.
4 Experiments
We proposed a method that user intention, utterance and
ASR channel simulation to rapidly assemble a simula-
tion system to evaluate dialog systems. We conducted
a case study for the navigation domain Korean spoken
dialog system to test our simulation method and exam-
ine the dialog behaviors using the simulator. We used
100 dialog examples from real user and dialog system
to train user intention and utterance simulator. We used
the SLU method of (Jeong and Lee, 2006), and dia-
log management method of (Kim et al, 2008) to build
the dialog system. After trained user simulator, we per-
form simulation to collect 5000 dialog samples for each
WER settings (WER = 0 ? 40 %).
To verify the user intention and utterance simula-
tion quality, we let two human judges to evaluate 200
randomly chosen dialogs and 1031 utterances from the
simulated dialog examples (WER=0%). At first, they
evaluate a dialog with three scale (1: Unnatural, 2: Pos-
sible, 3: Natural), then evaluate the utterances of a dia-
log with three scale (1: Unclear, 2: Understandable, 3:
Natural).
The inter evaluator agreement (kappa) is 0.45 and
0.58 for dialog and utterance evaluation respectively,
which show the moderate agreement (Fig. 8). Both
judges show the positive reactions for the quality of user
intention and utterance, the simulated dialogs can be
possibly occurred, and the quality of utterance is close
to natural human utterance.
We also did regression analysis with the results of
human evaluation and the SWB score to find out the
relationship between SWB and human judgment. Fig.
9 shows the result of polynomial regression (order 3)
result. It shows that ?Unclear? utterance might have 0.5
14
Human 1 Human 2 Average Kappa
Dialog 2.38 2.22 2.30 0.45
Utterance 2.74 2.67 2.71 0.58
Figure 8: Human evaluation results on dialog and utter-
ance
0.34
0.44
0.54
0.64
0.74
0.84
0.94
1 1.5 2 2.5 3
SW
B S
co
re
Average human evaluation for user utterances
Figure 9: Relationship between SWB score and human
judgment
? 0.7 SWB score, ?Possible? and ?Natural? simulated
utterance might have over 0.75. It means that we can
simulate good user utterance if we constrain the user
simulator with the threshold around 0.75 SWB score.
To assess the ASR channel simulation quality, we
compared how SLU of utterances was affected by
WER. SLU was quantified according to sentence er-
ror rate (SER) and concept error rate (CER). Compared
to WER set by the developer, measured WER was the
same, SER increased more rapidly, and CER increased
more slowly (Fig. 10). This means that our simula-
tion framework models SLU errors effective as well as
speech recognition errors.
Fig. 11 shows the overall dialog system behaviors us-
ing the user simulator and ASR channel simulator. As
the WER rate increased, dialog system performance de-
creased and dialog length increased. This result is sim-
ilar as observed to the dialog behaviors in real human-
0 10 20 30 40 50
WER measured 0 10 19.71 29 39.06 49.21
SER 0 33.28 56.6 70.91 81.29 88
CER 1.9087 10.1069 18.3183 26.1619 34.4322 41.755
0
10
20
30
40
50
60
70
80
90
100
err
 rat
e(%
)
Figure 10: Relationship between given WER and mea-
sured other error rates. X-axis = WER fixed by ASR
channel(%)
7.7
8.1
8.5
8.9
9.3
0.50
0.60
0.70
0.80
0.90
1.00
0 5 10 15 20 25 30 35 40
Avg
. Di
alog
 Len
gth
 (tu
rns
)
SLU
 acc
ura
cy 
and
 TC
R
Word Error Rate (%)
SLU
TCR
Length
Figure 11: Dialog simulation result on navigation do-
main
machine dialog.
5 Conclusion
This paper presented novel and easy to build dialog sim-
ulation methods for use in evaluation of spoken dia-
log systems. We proposed methods of simulating utter-
ances and user intentions to replace real human users,
and introduced an ASR channel simulation method that
acts as a real speech recognizer. We introduce a method
of simulating user intentions which is based on the CRF
sequential graphical model, and an utterance simulator
that generates user utterances. Both user intention and
utterance simulators use a fully data-driven approach;
therefore, they have high domain- and language porta-
bility. We also proposed a novel ASR channel sim-
ulator which allows the developers to set the speech
recognition performance level. We applied our meth-
ods to evaluate a navigation domain dialog system; ex-
perimental results show that the simulators successfully
evaluated the dialog system, and that simulated inten-
tion, utterance and errors closely match to those ob-
served in real human-computer dialogs. We will apply
our approach to other dialog systems and bootstrap new
dialog system strategy for the future works.
6 Acknowledgement
This research was supported by the Intelligent Robotics
Development Program, one of the 21st Century Frontier
R&D Programs funded by the Ministry of Knowledge
Economy of Korea.
References
Chung, G. 2004. Developing a flexible spoken dialog
system using simulation. Proc. ACL, pages 63?70.
Cuayahuitl, H., S. Renals, O. Lemon, and H. Shi-
modaira. 2005. Human-Computer Dialogue Sim-
ulation Using Hidden Markov Models. Automatic
15
Speech Recognition and Understanding, 2005 IEEE
Workshop on, pages 100?105.
Eckert, W., E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation.
Automatic Speech Recognition and Understanding,
1997. Proceedings., 1997 IEEE Workshop on, pages
80?87.
Jeong, M. and G. Lee. 2006. Jointly Predicting
Dialog Act and Named Entity for Statistical Spo-
ken Language Understanding. Proceedings of the
IEEE/ACL 2006 workshop on spoken language tech-
nology (SLT).
Kim, K., C. Lee, S Jung, and G. Lee. 2008. A
frame-based probabilistic framework for spoken di-
alog management using dialog examples. In the 9th
sigdial workshop on discourse and dialog (sigdial
2008), To appear.
Lafferty, J.D., A. McCallum, and F.C.N. Pereira. 2001.
Conditional Random Fields: Probabilistic Models
for Segmenting and Labeling Sequence Data. Pro-
ceedings of the Eighteenth International Conference
on Machine Learning table of contents, pages 282?
289.
Lee, J., S. Kim, and G.G. Lee. 2006. Grapheme-to-
Phoneme Conversion Using Automatically Extracted
Associative Rules for Korean TTS System. In Ninth
International Conference on Spoken Language Pro-
cessing. ISCA.
Liu, D.C. and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(1):503?528.
Lo?pez-Co?zar, R., A. De la Torre, JC Segura, and AJ Ru-
bio. 2003. Assessment of dialogue systems by
means of a new simulation technique. Speech Com-
munication, 40(3):387?407.
Lo?pez-Co?zar, Ramo?n, Zoraida Callejas, and Michael
Mctear. 2006. Testing the performance of spoken di-
alogue systems by means of an artificially simulated
user. Artif. Intell. Rev., 26(4):291?323.
Needleman, SB and CD Wunsch. 1970. A general
method applicable to the search for similarities in the
amino acid sequence of two proteins. J Mol Biol,
48(3):443?53.
Papineni, K., S. Roukos, T. Ward, and WJ Zhu.
2001. BLEU: a method for automatic evaluation of
MT. Research Report, Computer Science RC22176
(W0109-022), IBM Research Division, TJ Watson
Research Center, 17.
Rieser, V. and O. Lemon. 2006. Cluster-Based User
Simulations for Learning Dialogue Strategies. In
Ninth International Conference on Spoken Language
Processing. ISCA.
Schatzmann, J., K. Georgila, and S. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In 6th SIGdial
Workshop on Discourse and Dialogue. ISCA.
Schatzmann, J., B. Thomson, and S. Young. 2007a.
Error simulation for training statistical dialogue sys-
tems. Automatic Speech Recognition & Understand-
ing, 2007. ASRU. IEEE Workshop on, pages 526?
531.
Schatzmann, J., B. Thomson, and S. Young. 2007b.
Statistical User Simulation with a Hidden Agenda.
Proc. SIGDial, Antwerp, Belgium.
Scheffler, K. and S. Young. 2000. Probabilistic simula-
tion of human-machine dialogues. Proc. of ICASSP,
2:1217?1220.
Scheffler, K. and S. Young. 2001. Corpus-based dia-
logue simulation for automatic strategy learning and
evaluation. Proc. NAACL Workshop on Adaptation
in Dialogue Systems, pages 64?70.
Seneff, S. 2002. Response planning and generation
in the Mercury flight reservation system. Computer
Speech and Language, 16(3):283?312.
Torres, Francisco, Emilio Sanchis, and Encarna
Segarra. 2008. User simulation in a stochastic di-
alog system. Comput. Speech Lang., 22(3):230?255.
16
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 344?346,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
POMY: A Conversational Virtual Environment for Language Learning 
in POSTECH 
 
Hyungjong 
 Noh 
Kyusong  
Lee 
Sungjin  
Lee 
 
Gary Geunbae 
Lee 
Department of Computer Science and Engineering 
Pohang University of Science & Technology, Pohang, South Korea 
{nohhj, kyusonglee, junion, gblee}@postech.ac.kr 
  
 
 
Abstract 
This demonstration will illustrate an inter-
active immersive computer game, POMY, 
designed to help Korean speakers learn 
English. This system allows learners to ex-
ercise their visual and aural senses, receiv-
ing a full immersion experience to increase 
their memory and concentration abilities to 
a greatest extent. In POMY, learners can 
have free conversations with game charac-
ters and receive corrective feedback to their 
errors. Game characters show various emo-
tional expressions based on learners? input 
to keep learners motivated. Through this 
system, learners can repeatedly practice 
conversations in everyday life setting in a 
foreign language with no embarrassment. 
1 Introduction 
The needs for computer-based methods for learn-
ing language skills and components are increasing. 
One of the ultimate goals of computer-assisted 
language learning is to provide learners with an 
immersive environment that facilitates acquiring 
communicative competence. According to Second 
Language Acquisition (SLA) theories, there are 
some essential factors for improving learners? con-
versational skills: 1) comprehensible inputs and 
outputs, 2) corrective feedback, and 3) motivation 
and attitude. SLA theories imply that providing 
learners with the opportunity to have free conver-
sations with someone who can correct their errors 
is very important for successful acquisition of for-
eign languages. Moreover, motivation is another 
crucial factor; therefore a good CALL system 
should have elements which can interest learners 
[1]. 
Considering these requirements, we have devel-
oped a conversational English education frame-
work, POMY (POstech iMmersive English studY). 
The program allows users to exercise their visual 
and aural senses to receive a full immersion expe-
rience to develop into independent English as a 
Foreign Language (EFL) learners and increase 
their memory and concentration abilities to a 
greatest extent [2].  
 
 
Figure 1: Example screenshots of POMY: path-finding, post office, and market 
344
2 Demonstrated System 
In order to provide learners with immersive world, 
we have developed a virtual reality environment 
using the Unity 3D game engine1. For the domains 
that learners are exposed to, we select such do-
mains as path-finding, market, post office, library, 
and movie theater (Figure 1) to ensure having 
learners practice conversations in everyday life 
setting. To keep learners motivated and interested 
during learning sessions, learners are encouraged 
to accomplish several missions. For example, the 
first mission in the post office is to send a camera 
to one?s uncle in England. The package must be 
insured and delivered by the next week. In order to 
send the package, a learner must talk to Non-
Player Characters (NPCs) to fill in the zip-code 
properly.  
All NPCs can perceive the utterances of learners, 
especially Korean learners of English. Korean 
learners? production of the sound is different from 
those of native speakers, resulting in numerous 
pronunciation errors. Therefore, we have collected 
a Korean-English corpus to train acoustic models. 
In addition, since language learners commit nu-
merous grammatical errors, we should consider 
this to understand their utterances. Thus, we statis-
tically infer the actual learners' intention by taking 
not only the utterance itself but also the dialog con-
text into consideration, as human tutors do [1]. 
While free conversation is invaluable to the 
acquisition process, it is not sufficient for learners 
to fully develop their L2 proficiency. Corrective 
feedback to learners? grammatical errors is 
necessary for improving accuracy in their 
interlanguage. For this purpose, we designed a 
                                                          
1 http://unity3d.com/ 
special character, Ghost Tutor, which plays the 
role of English tutor and helps learners to use more 
appropriate words and expressions during the game. 
When a learner produces ungrammatical utterances, 
the Ghost Tutor provides both implicit and explicit 
negative and positive feedback in a form of 
elicitation or recast, which was manifested as 
effective ways in the second language acquisition 
processes [3].  To provide corrective feedback on 
grammatical errors, we use a method which con-
sists of two sub-models: the grammaticality check-
ing model and the error type classification model 
[4]. Firstly, we automatically generate grammatical 
errors that learners usually commit [5-6], and con-
struct error patterns based on the articulated errors. 
Then the grammaticality checking model classifies 
the recognized user speech based on the similarity 
between the error patterns and the recognition re-
sult using confidence scores. After that, the error 
type classification model chooses the error type 
based on the most similar error pattern and the er-
ror frequency extracted from a learner corpus. 
Finally, the human perception of NPC?s emo-
tional expressions plays a crucial role in human 
computer interaction. Thus, all NPCs are provided 
with a number of communicative animations such 
as talking, laughing, waving, crying, thinking, and 
getting angry (Figure 2).The total number of ani-
mations is over thirty from which the system can  
select one based on the response of a learner. The 
system generates positive expressions such as 
clapping and laughing when the learner answers 
correctly, and negative expressions such as crying 
and getting angry for incorrect answers.  
 
 
 
 
 
Figure 2: Various character animations 
345
Acknowledgments 
This work was supported by the Industrial Strate-
gic technology development program, 10035252, 
development of dialog-based spontaneous speech 
interface technology on mobile platform, funded 
by the Ministry of Knowledge Economy (MKE, 
Korea), and by Basic Science Research Program 
through the National Research Foundation of Ko-
rea (NRF) funded by the Ministry of Education, 
Science and Technology (2010-0019523). 
References  
Lee, S., Noh, H., Lee, J., Lee, K., Lee, G. G., Sagong, S., 
Kim, M. 2011. On the Effectiveness of Robot-
Assisted Language Learning, ReCALL Journal, 
Vol.23(1). 
Lee, S., Noh, H., Lee, J., Lee, K., Lee, G. G. 2010. 
POSTECH Approaches for Dialog-based English 
Conversation Tutoring. Proceedings of the APSIPA 
annual summit and conference, Singapore. 
Long, M. H., Inagaki, S., Ortega, L. 1998. The Role of 
Input and Interaction in Second Language Acquisi-
tion. The Modern Language Journal, 82, 357-371.  
Lee, S., Noh, H., Lee, K., Lee, G. G. 2011. Grammatical 
error detection for corrective feedback provision in 
oral conversations. Proceedings of the 25th AAAI 
conference on artificial intelligence (AAAI-11), San 
Francisco. 
Lee, S., Lee J., Noh, H., Lee, K., Lee, G. G, 2011. 
Grammatical Error Simulation for Computer-
Assisted Language Learning, Knowledge-Based Sys-
tems (to be published). 
Lee, S. and Lee, G. G. 2009. Realistic grammar error 
simulation using markov logic. Proceedings of the 
ACL, Singapore. 
 
346
Proceedings of the SIGDIAL 2013 Conference, pages 349?353,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Counseling Dialog System with 5W1H Extraction 
 
 
Sangdo Han, Kyusong Lee, Donghyeon Lee, Gary Geunbae Lee 
Department of Computer Science and Engineering, POSTECH, South Korea 
{hansd,kyusonglee,semko,gblee}@postech.ac.kr 
 
  
 
Abstract 
In this paper, we introduce our counseling dia-
log system. Our system interacts with users by 
recognizing what the users say, predicting the 
context, and following the users? feelings. For 
this interaction, our system follows three basic 
counseling techniques: paraphrasing, asking 
open questions, and reflecting feelings. To fol-
low counseling techniques, we extracted 
5W1H information and user emotions from 
user utterances, and we generated system ut-
terances while using the counseling techniques. 
We used the conditional random field algo-
rithm to extract 5W1H information, and con-
structed our counseling algorithm using a dia-
log strategy that was based on counseling 
techniques. A total of 16 adults tested our sys-
tem and rated it with a higher score as an in-
teractive communicator compared with the 
baseline system. 
1 Introduction 
Over the past 45 years, suicide rates have in-
creased by 60% worldwide.1 To prevent suicide, 
suicide people need to counsel with counselors. 
However, counseling with a human counselor 
requires a substantial cost, and in addition, there 
is a location restriction. Developing a counseling 
dialog system could be an effective solution to 
address this problem because the system has no 
limitations with respect to time and location. 
In this study, we present a counseling dialog 
system. The system interacts with users by rec-
ognizing what the users say, predicting the con-
text, and following the users? feelings. We used 
three counseling techniques for our system, to 
interact with the users. The system performs par-
aphrasing, asks open questions, and reflects feel-
ings. 
                                                 
1 
http://www.who.int/mental_health/prevention/suicide/suicid
eprevent/en/ 
Paraphrasing is a technique that paraphrases 
user utterances. For example, when a user utter-
ance is ?My dog picked up the ball?, then it 
could be paraphrased by ?Oh, your dog picked 
up the ball?. The technique of asking open ques-
tions is to ask some questions to the user, to ob-
tain more information. For example, when a user 
says ?I played computer games?, then the coun-
selor could say ?When did you play?? or ?Where 
did you play??. Finally, reflecting a feeling is a 
similar technique to paraphrasing, but it includes 
emotional comments. For example, when a user 
says ?My dog died. I?m so sad?, then the counse-
lor could say, ?Oh, your dog died. You look de-
pressed.? or ?You look so sad?. 
In our approach, we extract 5W1H (who, what, 
when, where, why, how) information and four 
basic emotions (happy, afraid, sad, and angry) 
from user utterances. We generate system utter-
ances using 5W1H information and basic emo-
tions. 
2 Counseling Techniques 
Counselors show empathy with clients by listen-
ing and understanding them. Clients feel com-
fortable by a counselor?s attention. Counselors 
listen, ask questions, answer questions, and con-
centrate on clients. Attention and empathy is im-
portant for counseling. Counselors show interest 
and care about the clients? emotions. Our coun-
seling dialog system also focused on attending 
and empathy. 
Many counseling techniques are used in coun-
seling. Basic attending, self-expression, and mi-
cro-training skills are introduced in Theron et al 
(2008). Basic attending and self-expression skills 
are about non-verbal behavior, such as tone of 
voice and eye contact. Micro-training skills are 
the basic verbal counseling techniques that are 
learned for counseling beginners: open and 
closed questions, minimal encouragement, para-
phrasing, reflection of feelings and summariza-
tion. 
349
We chose three micro-training skills to attend 
and show empathy with clients. These skills are 
open questions, paraphrasing, and reflection of 
feelings because they are basic techniques to 
show emphasize effectively. 
3 Related Work 
The SEMAINE project aims to build a Sensitive 
Artificial Listeners (SAL) ? conversational 
agents that are designed to interact with a human 
user through robust recognition and the genera-
tion of non-verbal behavior (Schr?der et al, 
2008). This system detects user emotions by 
multimodal sensors (camera, microphone). A 
virtual face in this system shows facial expres-
sions based on user emotions, and it encourages 
the user to speak by reacting and asking ques-
tions. These techniques could show empathy 
with users. However, it has limited verbal skills 
because SEMAINE does not have language un-
derstanding module. In our research, our system 
follows user utterances and generates system ut-
terances based on user?s 5W1H. 
4 Data Collection 
We generated 4,284 utterances by using fifty-
three 5W1H information sets and four basic 
emotions (Figure 1). Each utterance could be 
generated by using part of the 5W1H information 
and four emotions. 
 
Wh When Where What How Why
My 
om
Yesterday Park Key Lost
Her pocket
was punctured
Emotion
Sad
My mom lost key yesterday.
Yesterday, my mom lost key at the park.
Sadly, my mom lost key yesterday.
My mom lost key because her pocket was punctured.
Given Situation
Collected Corpus
 
Figure 1. Counseling Corpus Collecting Process 
 
We tagged each 5W1H element in each utter-
ance and the user intention for each utterance 
(Table 1). The system?s actions were labeled by 
following counseling strategies which will be 
discussed in section 5.3. 
 
Tagged Corpus User Intention System Action
<who>My mom</who> <how>lost</how> <what>a 
key</what> <when>yesterday</when>.
Inform_5W1H Ask_Open_Question
<when>Yesterday</when>, <who>my mom</who> 
<how>lost</how> <what>a key</what> at the 
<where>park</where>.
Inform_5W1H Paraphrase
<who>My mom</who> <how>lost</how> <what>a 
key</what> <when>yesterday</when>. I?m so sad.
Inform_5W1H_
Emotion
Reflect_Feeling
I?m so sad. Inform_Emotion Reflect_Feeling
Thank you. Thank Welcome
Good bye. Bye Bye
 
Table 1.  Corpus Tagging Examples 
 
User intentions we defined can be separated in 
two groups: ?counseling? and ?others?. Utterances 
in ?counseling? group include 5W1H information 
or emotional information. Utterances which do 
not including them are in ?others? group. Greet-
ings, thanks, and farewells are included (Table 2). 
 
Couns ing group Others group
Inform_5W1H,
Inform_emotion, 
Inform_5W1H_emotion, ?
Thank, Bye, Greeting, Agree, 
Disagree,?
 
Table 2. Two Separated Groups of User Intentions 
5 Method 
5.1 Architecture 
Our system architecture is given in graph 2. 
When a user inputs a sentence, a natural lan-
guage understanding (NLU) module understands 
the main action (the user?s intention) and extracts 
the 5W1H entities from the user?s utterance. The 
emotion detection module detects the user?s 
emotions using the emotional keyword diction-
ary. The dialog management module decides the 
system?s action from the main action and the 
5W1H information from the trained module from 
the example dialog corpus. The natural language 
generation (NLG) module generates the system 
utterance using a system utterance template. We 
can generate the system utterance by replacing 
5W1H slots with entities. 
 
User
Natural 
Language 
Understanding
Dialog 
Manager
Natural 
Language 
Generation
Dialog 
Template
Emotion 
Detector
Output
Emotional 
Keyword
 
Figure 2. Counseling Dialog System Hierarchy 
350
5.2 Natural Language Understanding 
In our approach, the NLU module understands 
the user utterance by classifying the main action 
and the 5W1H entities from the user utterance. 
To classify user intention, we used maximum 
entropy model (Ratnaparkhi, 1998) trained on a 
linguistically motivated features. We used a lexi-
cal word features for the utterance model. The 
lexical word features are lexical trigrams using 
previous, current, and next lexical words. To ex-
tract 5W1H entities, we used a conditional ran-
dom field (CRF) model (Laffery et al, 2001). 
We also used lexical word features (lexical tri-
grams) to train model. 
5.3 Dialog Management with Counseling 
Strategy 
When we extract 5W1H information or user 
emotions, the dialog management module keeps 
them in the emotion slot or in the six 5W1H slots. 
This slot information is discussed in a dialog. 
The dialog management module decides the 
system?s action by the main action, the 5W1H 
entities, and the user?s emotions. Dialog man-
agement follows the rules in figure 3, which is 
our dialog strategy for the counseling system. In 
figure 3, ?Counseling group?? node finds users 
intentions included in ?others group? (rejection or 
thanks could be included). The ?User Emotion 
Detection? node figures out whether the user ut-
terance is to include emotional keywords or 
whether the user emotion is already known by 
the discourse. The ?6 slot empty? node checks 
whether the user utterance includes at least one 
of the 5W1H elements or whether the 5W1H en-
tity is already known. The ?6 slot full? node de-
cides whether the user utterance with a discourse 
has all six 5W1H entries. From this strategy, we 
can notice that we cannot reflect a user?s feeling 
without the user?s emotion. We cannot ask open 
questions when all of the 5W1H slots are filled. 
 
Yes
No
No
No
No
No
Yes
No
Yes
YesYes
Yes
6 slot 
empty
6 slot 
full
6 slot 
empty
6 slot 
full
Counseling 
group?
User 
Utterance
User 
Emotion 
Detection
Particular 
System Actions
Ask Open 
Question
Reflect
Feeling
Ask Open 
Question
Reflect
Feeling
Paraphrase
Ask Open 
Question
Paraphrase
Reflect 
Feeling
Paraphrase
Ask Open 
Question
Paraphrase
 
Figure 3. Dialog Strategy Architecture 
5.4 Emotion Detection 
The emotion detection module decides the user?s 
emotion with respect to the four basic emotions. 
To detect the user?s emotions, we find emotional 
keywords in the user?s utterances. If any emo-
tional keyword appears in a user utterance, we 
decide that the user?s emotion which includes 
that keyword. For this approach, we made a dic-
tionary of emotional keywords for each basic 
emotion. The dictionary has approximately 15 
emotional keywords for each basic emotion (Ta-
ble 3). 
 
E otion Keywords
Happy happy, joy, glad, pleasure, ?
Sad sad, depress, grief, sorrow,?
Angry anger, rage, indignation, wrath, ?
Afraid afraid, fear, terror, dread, ?
 
Table 3. Emotional Keywords for Emotions 
5.5 Natural Language Generation 
The NLG module generates the system?s utter-
ances by selecting an utterance template from the 
template database. There are various system 
templates for each system action, for the 5W1H 
slot entities, and for the user?s emotions (Figure 
4). When the system template is chosen, we can 
generate system utterances by replacing the 
5W1H slot with a 5W1H discourse. ?O? means 
we should have the entity, ?X? means we do not 
have the entity, and ?*? means we do not care 
whether we have the entity or not. 
 
Who When Where What How Why Emotion System Action
O X O O O X O Reflect Emotion
O X O O O X * Paraphrase
System Action System Template
Reflect Emotion Oh, <who> <how> <what>. You look so sad.
Paraphrase Oh, did <who> <how> <what>?
System Utterance
Oh, your mom lost a key. You look so sad.
Oh, did your mom lost a key?
Who When Where What How Why
My 
mom
Yesterday Park Key Lost
Her pocket
was punctured
Emotion
Sad
 
Figure 4. System Utterance Generation 
 
Our overall counseling dialog example is shown 
in (Table 4). 
351
 User My mom lost a key yesterday.
System Oh, where did you mom lost a key?
User At the park.
System Your mom lost a key at the park, right?
User Yes, that?s right. I?m so sad.
System You look so sad. Cheer up.
User Thank you.
System My pleasure.
 
Table 4. Counseling Dialog Example 
6 Experiment 
We measured how systems show empathy with 
users. Our baseline system is a Korean chat-
oriented dialog system (Kim et al, 2012). The 
chat-oriented dialog system shows empathy by 
understanding user utterances and making a con-
versation. In our experiment, 7 basic situations 
are given for each person. Situations are ex-
plained by 5W1H, and users generated various 
utterances using that information. Each person 
generated approximately 100 utterances during 
30 minutes and made estimates for each system. 
We recruited 16 volunteers to use our system and 
to estimate its effectiveness. Each user checked 
17 questions from 1 to 10. The questions ask us-
ers how does each system understand the user 
utterance, is it appropriate for counseling, and 
does it satisfy the users (Table 5). 
 
Question
Chat-
Oriented
Counseling
1-1. The system used counseling techniques: 
paraphrasing, open question, reflect feeling.
3.50 7.06
1-2. The system knows my emotion. 3.44 6.88
1-3. There was no break in the conversation. 2.63 6.88
1-4. The system acts like a counselor. 2.88 6.69
1-5. The system shows empathy with me. 4.69 7.31
1-6. I feel the system understands me. 2.56 6.50
2-1. The system understands what I said. 2.88 6.81
2-2. The system understands 5W1H information. 4.13 7.44
2-3. System utterances are appropriate. 2.75 6.94
2-4. System utterances have no problem. 3.50 5.50
3-1. I could speak about various situations. 4.31 6.38
3-2. I had a casual conversation. 4.75 6.88
3-3. Scenarios look expandable. 5.50 7.63
4-1. I satisfied overall conversation. 3.10 6.56
4-2. I satisfied overall counseling. 2.38 6.56
4-3. The system looks appropriate as a counselor. 2.50 6.38
4-4. I?ll recommend the system as a counselor to my
friends.
2.31 5.38
Mean 3.40 6.69
Standard Deviation 0.96 0.59
 
Table 5. Experiment Results 
 
Questions 1-1 to 1-6 ask users how each sys-
tem is appropriate as a counselor. Counseling 
system rated 6.89 for mean. Questions 2-1 to 2-4 
are about users? tterances understandability. In 
these questions, counseling system rated 6.67 on 
the average. Questions 3-1 to 3-3 show how var-
ious dialogs covered. Our system got 6.96 for 
mean. Finally, questions 4-1 to 4-4 are about 
overall satisfaction. These questions rated 6.22 
for mean. Our p-value through t-test was 
3.77*10-11. 
Counseling system got higher score than chat-
oriented system because users felt empathy better 
with our system than baseline system. As a coun-
selor, counseling system is much better than 
chat-oriented system. Our baseline system was 
not appropriate as a counselor because it rated 
3.39 for average. However, our system scored 
over 6.5 overall. It means our system is valuable 
as a counselor.  
7 Conclusion 
In this study, we introduced counseling tech-
niques that we used to implement counseling 
dialog system. The experimental results showed 
that our system shows empathy with users. Alt-
hough the results of this study bring us a step 
closer to implementing counseling dialog system, 
the results are only valid with 5W1H information 
in Korean. Our future works are to improve our 
counseling dialog system using new NLU mod-
ule which extracts 5W1H information from more 
general utterances, with new emotion detection 
method, and with more counseling techniques. 
 
Acknowledgments 
This research was supported by the Basic Sci-
ence Research Program through the National Re-
search Foundation of Korea(NRF) funded by the 
Ministry of Education, Science and Technolo-
gy(2012-0008835). 
This research was supported by the 
MSIP(Ministry of Science, ICT&Future Plan-
ning), Korea, under the ITRC(Information Tech-
nology Research Center) support program super-
vised by the NIPA(National IT Industry Promo-
tion Agency) (NIPA-2013-H0301-13-3002) 
References  
Kim, Y., Noh, H., & Lee, G. G. (2012). Dialog man-
agement on chatting system based on lexico-
syntactic patterns and named entity types. Proceed-
ings of Spring Conference of Korean Society of 
Speech Sciences, 41-42,  Seoul, Korea. 
352
Lafferty, J., McCallum, A., & Pereira, F. (2001). 
Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. 
Proceedings of the 18th International Confer-
ence on Machine Learning, 282-289. 
Ratnaparkhi, A. (1998). Maximum entropy models 
for natural language ambiguity resolution. 
Computer and Information Science, University 
of Pennsylvania, Philadelphia, USA.  
Schr?der, M., Cowie, R., Heylen, D., Pantic, M., Pe-
lachaud, C., & Shuller, B. (2008). Towards re-
sponsive sensitive artificial listeners. Workshop 
on Human-Computer Conversation, Bellagio, Italy. 
Theron, M. J. (2008). A manual for basic relational 
skills training in psychotherapy. Masters of Arts 
in Clinical Psychology, University of South Africa, 
South Africa. 
353
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 65?73,
Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational Linguistics
POSTECH Grammatical Error Correction System in the CoNLL-
2014 Shared Task 
 
 
Kyusong Lee, Gary Geunbae Lee 
Department of Computer Science and Engineering 
Pohang University of Science and Technology 
Pohang, Korea 
{Kyusonglee,gblee}@postech.ac.kr 
 
  
 
Abstract 
This paper describes the POSTECH gram-
matical error correction system. Various 
methods are proposed to correct errors 
such as rule-based, probability n-gram 
vector approaches and router-based ap-
proach. Google N-gram count corpus is 
used mainly as the correction resource. 
Correction candidates are extracted from 
NUCLE training data and each candidate 
is evaluated with development data to ex-
tract high precision rules and n-gram 
frames. Out of 13 participating teams, our 
system is ranked 4th on both the original 
and revised annotation.    
1 Introduction 
Automatic grammar error correction (GEC) is 
widely used by learners of English as a second 
language (ESL) in written tasks. Many methods 
have been proposed to correct grammatical errors; 
these include methods based on rules (Naber, 
2003), on statistical machine translation (Brockett 
et al., 2006), on machine learning, and on n-grams 
(Alam et al., 2006). Early research (Han et al., 
2006; De Felice, 2008; Knight & Chander, 1994; 
Nagata et al., 2006) on error correction for non-
native text was based on well-formed corpora.  
Most recent work (Cahill et al., 2013; 
Rozovskaya & Roth, 2011; Wu & Ng, 2013) has 
used machine learning methods that rely on a GE- 
tagged corpus such as NUCLE, Japanese English 
Learner corpus, and Cambridge Learner Corpus 
(Dahlmeier et al., 2013; Izumi et al., 2005; 
Nicholls, 2003), because well-formed and GE-
tagged approaches are closely related to each 
other, can be synergistically combined. Therefore, 
research using both types of data has also been 
conducted (Dahlmeier & Ng, 2011). Moreover, a 
meta-classification method using several GE-
tagged corpora and a native corpus has been pro-
posed to correct the grammatical errors (Seo et al., 
2012). A meta-classifier approach has been pro-
posed to combine a language model and error-spe-
cific classification for correction of article and 
preposition errors (Gamon, 2010). Web-scale 
well-formed corpora have been successfully ap-
plied to grammar error correction tasks instead of 
using error-tagged data (Bergsma et al., 2009; 
Gamon et al., 2009; Hermet et al., 2008). Espe-
cially in the CoNLL-2013 grammar error correc-
tion shared task, many of the high-ranked teams 
(Kao et al., 2013; Mark & Roth, 2013; Xing et al., 
2013) exploited the Google Web-1T n-gram cor-
pus. The major advantage of using these web-
scale corpora is that extremely large quantities of 
data are publicly available at no additional costs; 
thus fewer data sparseness problems arise com-
pared to previous approaches based on error-
tagged corpora. 
We also use the Google Web-1T n-gram corpus. 
We extract the candidate pairs (original erroneous 
text and its correction) from NUCLE training data. 
We use a router to choose the best frame to com-
pare the n-gram score difference between the orig-
inal and replacement in a given candidate pair.  
The intuition of our grammar error correction 
method is the following: First, if the uni-gram 
count is less than some threshold, we assume that 
the word is erroneous. Second, if the replacement 
word n-gram has more frequent than the original 
word n-gram, it presents strong evidence for cor-
rection. Third, depending on the candidate pair, 
tailored n-gram frames help to correct errors ac-
curately. Fourth, only high precision method and 
rules are applied. If correction precision on a can-
didate pair is less than 30% in development data, 
65
we do not make a correction for the candidate pair 
at runtime. 
In the CoNLL-Shared Task, objectives were 
presented yearly. In 2012, the objective was to 
correct article and preposition errors; in 2013, it 
was to correct article, preposition, noun number, 
verb form, and subject-verb agreement errors. 
This year, the objective is to correct all errors. 
Thus, our method should also correct prepro-
cessing and spelling errors.  Detailed description 
of the shared task set up, data set, and evaluation 
about the CoNLL-2014 Shared Task is explained 
in (Ng et al., 2014) 
2 Data and Recourse  
The Google Web-1T corpus contains 1012 words 
of running text and the counts for all 109 five-word 
sequences that appear > 40 times (Brants & Franz, 
2006). We used the NUS Corpus of Learner Eng-
lish (NUCLE) training data to extract the candi-
date pairs and CoNLL-2013 Official Shard Task 
test data as development data.  We used the Stan-
ford parser (De Marneffe & Manning, 2008) to 
extract part-of-speech, dependency, and constitu-
ency trees.  
3 Method 
3.1 Overall Process 
We correct the errors in the following order: 
Tokenizing ? spelling error correction ? punc-
tuation error correction ? N-gram Vector Ap-
proach for Noun number (Nn) ? Router-based 
                                                 
1 http://abisource.com/projects/enchant/ 
Correction (Deletion Correction ? Insertion Cor-
rection ? Replacement) for various error types ? 
Rule-based method for verb errors. Between each 
pair of step, we parse, tag, and tokenize again us-
ing the Stanford parser because the previous cor-
rection affects parsing, tagging, and tokenizing re-
sults.  
3.2 Preprocessing 
Because the correction task is no longer restricted 
to five error types, tokenizing and spelling error 
correction have become critical for error correc-
tion. To detect tokenizing error such as ?civiliza-
tions.It?, a re-tokenzing process is necessary. If a 
word contains a comma, punctuation (e.g., ?,? or 
?.?) and the word count in Google n-gram is less 
than some threshold (here, 1000), we tokenize the 
word, e.g., as ?civilizations . It?. We also correct 
spelling errors by referring to the Google n-gram 
word count. If the word uni-gram count is less 
than a threshold (here, 60000) and the part-of-
speech (POS) tag is not NNP or NNPS, we assume 
that the word has o  ne or more errors. The thresh-
old is set based on the development set. We use 
the Enchant Python Library to correct the spelling 
errors1. However, using only one best result is not 
very accurate. Thus, among the best results in the 
Enchant Python Library, we select the one best 
word, i.e. that word with the highest frequency in 
the Google n-gram corpus. Using NUCLE train-
ing data, rules are constructed for comma, punc-
tuation, and other errors (Table 3).  
 
 
 
Figure 1. Overall Process of Router-based Correction 
 
66
3.3 Candidate Generation 
Selecting appropriate correction candidates is crit-
ical for the precision of the method. In article and 
noun number correction, the number of candidates 
is small: ?a?,?an?,?the? in article correction, ?plural? 
or ?singular? in noun number correction. However, 
the number of correction candidates can be unlim-
ited in wrong collocation/idiom errors. Reducing 
the number of candidates is important in the gram-
mar error correction task.  
 
Nn Correction Candidate: noun number correc-
tion has just one replacement candidate. If the 
word is plural, its correction candidate is singular, 
and vice versa. The language tool2 can perform 
these changes. 
 
Other Correction Candidate: for corrections 
other than noun number, candidates are selected 
from the GE-tagged corpus. A total of 4206 pairs 
were extracted. We use the notation of candidate 
pair (o?r), which links the original word (o) and 
its correction candidate (r). In the deletion correc-
tion step, we determine whether or not the word 
should be deleted. In the insertion correction step, 
we select the insertion position in a sentence as a 
space between two words. If o is ?, insertion cor-
rection is required; if r is ?, the pair deletion cor-
rection is required. We use the Stanford constitu-
ency parser (De Marneffe & Manning, 2008) to 
extract a noun phrase; if it does not contain a de-
terminer or article, we insert one in front of the 
noun phrase; if the noun in the noun phrase is sin-
gular, ?the?, ?a?, and, ?an? are selected an insertion 
candidates; if the noun is plural, only ?the? is se-
lected as an insertion candidate. We only apply in-
sertion correction at ArtOrDet, comma errors, and 
preposition; we skip insertion correction for other 
error types because selecting an insertion position 
is difficult and if every position is selected as in-
sertion position, precision decrease. 
 
4 N-gram Approach 
We used the following notation. 
N(o) n-gram vector in original sentence 
N(r) n-gram vector in replacement sen-
tence 
n(o)i i th element in N(o) 
?(?)? i th element in N(r) 
N[i:j] n-gram vector from i th element to 
j th element 
                                                 
2http://www.languagetool.org 
 
Web-scale data have also been used successfully 
in many other research areas, such as lexical dis-
ambiguation (Bergsma et al., 2009). Most NLP 
systems resolve ambiguities with the help of a 
large corpus of text, e.g.: 
? The system tried to decide {among, between} 
the two confusable words.  
Disambiguation accuracy increases with the size 
of the corpus. Many systems incorporate the web 
count into their selection process. For the above 
example, a typical web-based system would query 
a search engine with the sequences ?decide among 
the? and ?decide between the? and select the can-
didate that returns the most hits. Unfortunately, 
this approach would fail when disambiguation re-
quires additional context. Bergsma (2009) sug-
gested using the context of samples of various 
lengths and positions. For example, from the 
above the example sentence, the following 5-gram 
patterns can be extracted: 
 
?  system tried to decide {among, between}  
?  tried to decide {among, between} the 
?  to decide {among, between} the two 
? decide {among, between} the two confusable 
? {among, between} the two confusable words 
 
Similarly, four 4-gram patterns, three 3-gram pat-
terns and two 2-gram patterns are extracted by 
spanning the target. A score for each pattern is cal-
culated by summing the log-counts. This method 
was successfully applied in lexical disambigua-
tion. Web-scale data were used with the count in-
formation specified as features. Kao et al. (2013) 
used a ?moving window (MW)? : 
 
???,?(w) = {???? , ? , ????+(??1), ? = 0, ? ? 1}  (1) 
 
where ?  denotes the position of the word, k the 
window size and w the original or replacement 
word at position ?. The window size is set to 2 to 
5 words. MW is the same concept as the SUMLM: 
 
??,?(?) = ? ?????(?????)
?????????(?)
(2) 
Both approaches apply the sum of all MWs in (1).  
Our approach is based on the MW method. The 
difference is that instead of summing all the MWs, 
we consider only one best MW which is referred 
to here as a frame. The following sentences 
 
67
demonstrate the case when the following words 
are the crucial features to correct errors: 
?  I will do it (in?at) home. 
?  We need (an??) equipment to solve problems. 
However, following sentences demonstrate the 
case when preceding words is the crucial feature 
to correct errors: 
?  One (are?is) deemed to death at a later stage . 
?  But data that (shows?show) the rising of life 
expectancies 
We investigated which frame is the best based on 
the development set, then router is trained to de-
cide on the frame depending on the candidate pair.  
 
4.1 Router-based N-gram Correction 
A frame is a sequence of words around the target 
position. A frame is divided into a preceding 
frame and a following frame. The target position 
can be either a position of a target word (Figure 
2a) or a position in which a candidate word is 
judged to be necessary (Figure 2b). Once the size 
(i.e., number of words) of frames is chosen, sev-
eral forms of frames (n; m) with different sizes of 
preceding (n) and following (m) words are possi-
ble. 
 
 
Figure 2.  Frame for n-gram 
 
The router is designed to take care of two stages 
(training, run-time) error correction. During train-
ing, the router selects the best frame for each can-
didate pair. By testing each candidate pair with 
each frame in the development data; the frame 
with the best precision is selected as the best 
frame among (1;1), (1;2), (1;3), (2;1),(2:2), etc.  
 At the end of the training stage, the router has 
a list of pairs (x) which matches the best frame (y) 
associated with it (Table 1) as a result of compar-
ing each candidate pair with one in the develop-
ment corpus. 
During runtime, the router assigns each candi-
date pair to the best frame to produce the output 
sentence (Figure 1). For example, for a sentence 
?This ability is not seen 40 years back where the 
technology advances were not as good as now .? 
the candidate pair for correction (back? ago) is 
suggested. The best frame assigned by the router 
for this pair (1;1), which is ?years back where?. 
The best candidate frame for this is ?year ago 
where?. At this point, we query the count of 
?years back where? and ?years ago where? from 
the Google N-gram Count Corpus; these counts 
are 46 and 1815 respectively. Because the count 
of ?years ago where? is greater than that of ?years 
back where?, the former is selected as the correct 
form. As a result, the sentence ?This ability is not 
seen 40 years back where the technology ad-
vances were not as good as now.? is corrected to 
?This ability is not seen 40 years ago where the 
technology advances were not as good as now.? 
Some words are allowed to have multiple best 
frames; in all the best frames, if a candidate word 
sequence is more frequent than an original word 
sequence in the Google count, then correction is 
made. The multiple frames are also trained from 
the development data set.  
4.2 Probability n-gram Vector 
We use the probability n-gram Vector approach to 
correct Nn. Most errors are corrected using the 
router-based method; however, training the router 
for every noun is difficult because the number of 
nouns is extremely large. Moreover, for noun 
number, we found that rather than considering one 
direction or one frame of n-gram, every direction 
of n-gram should be considered for better perfor-
mance such as forward, backward, and two-way. 
Thus, the probability n-gram vector algorithm is 
applied only in the noun number error correction.  
We propose the probability n-gram vector method 
to correct grammatical errors to consider both di-
rections, forward and backward. In a forward n-
gram, the probability of each word is estimated 
Table 1. Example of Trained Router 
x (o?r) y 
(another?other) (1;3) 
(less?fewer) (1;3) 
(rise?raise) (1;2) 
(back?ago) (1;1) 
(could?can) (2;1) 
(well?good) (2;1) 
(near??) No correction 
  
 
68
depending on the preceding word. On the other 
hand, in a backward n-gram the probability of 
each word is estimated depending on the follow-
ing words. When the probability of a candidate 
word is higher than original word, we replace the 
original with the candidate word in the correction 
step. 
Probability n-gram vectors are generated from the 
original word and a candidate word (Figure 3). 
Rather than using a single sequence of n-gram 
probability, we apply contexts of various lengths 
and positions. We applied the probability infor-
mation using the Google n-gram count infor-
mation as in the following equation: 
 P(??|???2, ???1) =
?(???2,???1??)
?(???2,???1)
 
 
Moreover, rather than calculating one word?s 
probability given n words such as 
P(??|???1, ???2, ???3), our model calculates the 
probability of m words given an n word sequence. 
The following is an example 4-gram with forward 
probability: 
? m = 3, n = 1 P(???2, ???1??|???3) 
? m = 2, n = 2 P(???1, ??|???3, ???2) 
? m = 1, n = 3 P(??|???3, ???2, ???1). 
We construct a 40-dimensional probability vector 
with forward and backward probabilities consid-
ering of twenty 5-grams, twelve 4-grams, six 3-
gram, and two 2-gram. Additionally, the elements 
of the n-gram vector are detailed in Table 2. 
 
Back-Off Model: A high-order n-gram is more 
effective than a low-order n-gram. Thus, we ap-
plied back-off methods (Katz, 1987) to assign 
higher priority to higher order probabilities. If all 
elements in 5-gram vectors are 0 for both the orig-
inal and candidate sentence, which means 
? {?(?)? + ?(?)?} = 0
19
?=0 , we consider 4-gram 
vectors (N[20:31]). If 4-gram vectors are 0, we con-
sider 3-gram vectors. Moreover, when the pro-
posed method calculates each of the forward, 
backward and two-way probabilities, the back-off 
method is used to get each score.  
 
Correction: Here, we explain the process of error 
correction using n-gram vectors. First, we gener-
ate Nn error candidates. Second, we construct the 
n-gram probability vector for each candidate. The  
back-off method is applied in N(o)+N(r), The vec-
tor contains various directions and ranges of prob-
abilities of words given a sample sentence. We 
then calculate forward n-gram score by summing 
even elements in the vector. We calculate the 
backward n-gram by summing odd elements in 
Table 2. Next, the two-way n-gram is calculated 
by summing all elements for both directions n-
gram. If forward, backward, and two-way n-
grams have higher probabilities for the candidate 
word, we select the candidate as corrected word 
(Figure 3). 
Table 2: The elements of n-gram vector  
5-GRAM 
?0 = ?(??|??+1??+2??+3??+4) backward 
?1 = ?(??|???4???3???2???1) forward 
?2 = ?(????+1|??+2??+3??+4) backward 
??.. 
4-GRAM 
?20 = ?(??|??+1??+2??+3) backward 
?21 = ?(??|???3???2???1) forward 
??.. 
 
3-GRAM 
?32 = ?(??|??+1??+2) backward 
?33 = ?(??|???2???1) forward 
?34 = ?(????+1|??+2) backward 
?35 = ?(???1??|???2) forward 
?36 = ?(???1??|??+1) backward 
?37 = ?(????+1|???1) forward 
2-GRAM 
?38 = ?(??|??+1) backward 
?39 = ?(??|???1) forward 
 
 
Figure 3. Overall process of Nn Correction 
69
 5 Verb Correction (Rule-based)  
There are several types of verb errors in non-na-
tive text such as verb tense, verb modal, missing  
verb, verb form, and subject-verb-agreement 
(SVA). Among these errors, we attempt to correct 
SVA errors using rule-based methods (Table 3). 
In non-native text, parsing and tagging errors are 
inevitable, and it may cause false alarm. Thus, in-
stead of dependency parsing to find subject and 
verb, we consider the preceding five words be-
cause erroneous sentences often contain depend-
ency errors. Moreover, in erroneous sentences, 
POS tagging accuracy is lower than native text. 
Thus, NN and VB are misclassified, as are VBZ 
and NNS. A rule is used that encodes the relevant 
linguistic knowledge that these words or POSs 
should not occur in the five positions preceding 
the VBZ: ?NN?, ?this?, ?it? ,?one?, ?VBG?. Moreover, 
words that preceded and follow ?which? should 
agree in verb form, as indicated in Rule3 and 
Rule4. 
 
6 Experiment 
The CoNLL-2014 training data consist of 1,397 
articles together with gold-standard annotation. 
Algorithm Rule1-Comma 
1: function rule1( toksent,  tokpos) 
2: for i ? 0 ? len(toksent) do 
3: if  toksent[i] in [ However?, ?Therefore?, ?Thus?] and not  toksent[i + 1] == ?,?  then 
4: toksent[i]= toksent[i] + ? ,? 
 
Algorithm Rule2-preposition 
1: function rule2( toksent,  tokpos) 
2: for i ? 0 ? len(toksent) do 
3: if  toksent[i] = ?according? and not  toksent [i+1] = ?to? 
4:  toksent [i+1] = ?to ?+  toksent [i+1] 
 
Algorithm Rule3-Subject Verb Agreement 
1: function rule3( toksent,  tokpos) 
2: for i ? 0 ? len(toksent) do 
3: if  toksent[i] is ?which? 
4: if  tokpos[i ? 1] == ?NNS? and  tokpos[i + 1] == ?VBZ?  then 
5: toksent[i + 1]= changeWordForm (toksent[i + 1], ?VBP?) 
6: else if  tokpos[i ? 1]  == ?NNS? and  tokpos[i + 1] == ?NNS? then 
7: toksent[i + 1]=  =changeWordForm(toksent[i + 1], ?VBP?) 
8: else if  tokpos[i ? 1] == ?NN? and  tokpos[i + 1]== ?are? then 
9: toksent[i + 1]=  = is 
10: else if  tokpos[i ? 1] == ?NN? and  tokpos[i + 1] in [?VBP?,?VB?,?NN?] then 
11: toksent[i + 1]=  = makePlural(toksent[i + 1]) 
 
Algorithm Rule4-Subject Verb Agreement 
1: function rule4( toksent,  tokpos) 
2: for i ? 0 ? len(toksent) do 
3: if not ( tokpos[i]is ?VBZ? and [?NN?,?this?,?it?,?one?,?VBG?] in  tokpos[i ? 5: i]) then 
4:  tokcand?changeWordForm( tokword[i], ?VBP?) 
5: else if not ( tokpos[i]is ?VBP? and [?I?,?we?,?they?,?and?] in  toksent[i ? 5: i]) then 
6:  tokcand ?changeWordForm( tokword[i], ?VBZ?) 
7: else if not ( tokpos[i]is ?NN? and [?be?,?ing?] in  toksent[i ? 5: i]) then 
8:  tokcand?changeWordForm( tokword[i], ?VBN?) 
9:         original = ngramCount( toksent), candidate =ngramCount(tokcand) 
10: If original < candidate then 
11: Return tokcand 
Table 3. Examples of Rules  
 
 
70
The documents are a subset of the NUS Corpus of 
Learner English (NUCLE). We use the Max-
Match (M2) scorer provided by the CoNLL-2014 
Shared Task. The M2 scorer works by using the 
set that maximally matches the set of gold-stand-
ard edits specified by the annotator as being equal 
to the set of system edits that are automatically 
computed and used in scoring (Dahlmeier & Ng, 
2012). The official evaluation metric is F0.5, 
weighting precision twice as much as recall. We 
achieve F0.5 of 30.88; precision of 34.51; recall 
of 21.73 in the original annotation (Table 4). After 
original official annotations announced by organ-
izers (i.e., only based on the annotations of the two 
annotators), another set of annotations is offered 
based on including the additional answers pro-
posed by the 3 teams (CAMB, CUUI, UMC). The 
improvement gap between the original annotation 
and the revised annotation of our team (POST) is 
5.89%.  We obtain the highest improvement rate 
except for the 3 proposed teams (Figure 4), F0.5 
of 36.77; precision of 41.28; recall of 25.59 in the 
revised annotation. Our system achieves the 4th 
highest scores of 13 participating teams based on 
both the original and revised annotations. To ana-
lyze the scores of each of the error types and mod-
ules, we apply the method of n-gram vector (Nn), 
rule-based (Verb, Mec), and router-based (others) 
separately in both the original and the revised an-
notation of all error types. We achieve high preci-
sion by rules at the Mec which indicates punctua-
tion, capitalization, spelling, and typos errors. Ad-
ditionally, the Nn type has the highest improve-
ment gap between the original and revised anno-
tation (17% ? 24.31 of F0.5).  In order for our 
team to improve the high precision in the rule-
based approach, we tested potential rules on the 
development data and kept a rule only if its preci-
sion on that data set was 30% or greater. When we 
trained router, the same strategy was conducted. 
If a frame could not achieve 30% precision, we 
assigned the candidate pair as ?no correction? in 
the router. These constraints achieve precision of 
30 % in most error types.  
7 Discussion  
Although preposition errors are frequently com-
mitted in non-native text, we mostly skip the cor-
rection of preposition error. This is because as-
signing prepositions correctly is extremely diffi-
cult, because (1) the preposition used can vary 
(e.g., Canada: ?on the weekend? vs. Britain ?at the 
weekend?); (2) in a given location, more than one 
preposition may be possible, and the choice af-
fects the meaning (e.g., ?on the wall?, vs. ?at the 
wall?). Verb errors can consist of many multi-
 
Figure 4. Improvement gap between the original annotation and revised annotation of each team 
 
0
2
4
6
8
10
 
Table 4. Performance on each error type 
 Original annotation  Revised annotation 
 Precision Recall F0.5  Precision Recall F0.5 
N-gram (Nn) 31.0 6.55 17.75  42.28 9.0 24.31 
Rule (Verb) 28.95 1.12 4.86  31.17 1.29 5.52 
Rule (Mec) 49.34 5.47 18.94  52.16 6.17 20.93 
Router (Others) 28.11 12.49 22.49  35.29 15.45 28.08 
All 34.51 21.73 30.88  41.28 25.59 36.77 
 
71
word errors due to errors of usages of passive and 
active voice. (e.g. release?be released). Our cur-
rent system cannot correct these multi-words er-
rors, for three reasons. First, if the original exam-
ple consists of one word and the optimal replace-
ment consists of two words, n-gram scores cannot 
be applied easily to compare probabilities be-
tween them. Second, the n-gram approach also 
fails if the distance between subject and verb is 
more than 5. Third, multiply dependent errors are 
critical for verb error correction. For example, 
noun number, determiner, and subject verb agree-
ment are often dependent upon each other: e.g. 
?And once this happens, privacy does not exist 
any more and people's (life?lives) (is?are) un-
der great threaten.? The correction order will be 
important when all error type must be corrected 
simultaneously.  
Grammar error correction is a challenging 
problem. In CoNNL-2013, more than half of the 
related teams obtained F-score < 10.0. This low 
performance in the grammar error correction can 
be explained by several reasons, which indicate 
the present limitations of grammar correction sys-
tems. 
Among a total of 4206 pairs, we only use small 
amount of candidate pairs, 215 pairs are used for 
candidate pairs. The other 3991 pairs are dis-
carded in the router training step because these 
pairs cannot be corrected by the n-gram approach. 
Various classification methods and statistical ma-
chine translation based methods will be investi-
gated in the router-based approach to find the tai-
lored methods for the given word. A demonstra-
tion and progress of our grammar error correction 
system is available to the public3.  
8 Conclusion 
We have described the POSTECH grammatical 
error correction system. We use the Google N-
gram count corpus to detect spelling errors, punc-
tuation, and comma errors. A rule-based method 
is used to correct verb, punctuation, comma errors 
and preposition errors. The Google corpus is also 
used for an n-gram vector approach and a router-
based approaches. Currently we use the router to 
select the best frame. In the future, we will train a 
router to select the best method among classifica-
tion, n-gram approach, statistical machine transla-
                                                 
3 http://isoft.postech.ac.kr/grammar 
tion-based method and pattern matching ap-
proaches. A machine learning method will be used 
to train the router with various features.   
Acknowledgements 
This research was supported by the MSIP(The Ministry 
of Science, ICT and Future Planning), Korea and Mi-
crosoft Research, under IT/SW Creative research pro-
gram supervised by the NIPA(National IT Industry 
Promotion Agency) (NIPA-2013- H0503-13-1006) 
and this research was supported by the Basic Science 
Research Program through the National Research 
Foundation of Korea(NRF) funded by the Ministry of 
Education, Science and Technology(2010-0019523). 
 
References 
Han, Na-Rae, Chodorow, Martin, & Leacock, Claudia. 
(2006). Detecting errors in English article 
usage by non-native speakers.  
Alam, Md Jahangir, UzZaman, Naushad, & Khan, 
Mumit. (2006). N-gram based statistical 
grammar checker for Bangla and English.  
Bergsma, Shane, Lin, Dekang, & Goebel, Randy. 
(2009). Web-Scale N-gram Models for 
Lexical Disambiguation. Paper presented at 
the IJCAI. 
Brants, Thorsten, & Franz, Alex. (2006). The Google 
Web 1T 5-gram corpus version 1.1. 
LDC2006T13.  
Brockett, Chris, Dolan, William B, & Gamon, Michael. 
(2006). Correcting ESL errors using phrasal 
SMT techniques. Paper presented at the 
Proceedings of the 21st International 
Conference on Computational Linguistics and 
the 44th annual meeting of the Association for 
Computational Linguistics. 
Cahill, Aoife, Madnani, Nitin, Tetreault, Joel, & 
Napolitano, Diane. (2013). Robust Systems 
for Preposition Error Correction Using 
Wikipedia Revisions. Paper presented at the 
Proceedings of NAACL-HLT. 
Dahlmeier, Daniel, & Ng, Hwee Tou. (2011). 
Grammatical error correction with 
alternating structure optimization. Paper 
presented at the Proceedings of the 49th 
Annual Meeting of the Association for 
Computational Linguistics: Human Language 
Technologies-Volume 1. 
Dahlmeier, Daniel, & Ng, Hwee Tou. (2012). Better 
evaluation for grammatical error correction. 
Paper presented at the Proceedings of the 
2012 Conference of the North American 
Chapter of the Association for Computational 
Linguistics: Human Language Technologies. 
 
72
Dahlmeier, Daniel, Ng, Hwee Tou, & Wu, Siew Mei. 
(2013). Building a large annotated corpus of 
learner English: The NUS corpus of learner 
English. Paper presented at the Proceedings of 
the Eighth Workshop on Innovative Use of 
NLP for Building Educational Applications. 
De Felice, Rachele. (2008). Automatic error detection 
in non-native English. University of Oxford.    
De Marneffe, Marie-Catherine, & Manning, 
Christopher D. (2008). The Stanford typed 
dependencies representation. Paper presented 
at the Coling 2008: Proceedings of the 
workshop on Cross-Framework and Cross-
Domain Parser Evaluation. 
Gamon, Michael. (2010). Using mostly native data to 
correct errors in learners' writing: a meta-
classifier approach. Paper presented at the 
Human Language Technologies: The 2010 
Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics. 
Gamon, Michael, Leacock, Claudia, Brockett, Chris, 
Dolan, William B, Gao, Jianfeng, Belenko, 
Dmitriy, & Klementiev, Alexandre. (2009). 
Using statistical techniques and web search to 
correct ESL errors. Calico Journal, 26(3), 
491-511.  
Hermet, Matthieu, D?silets, Alain, & Szpakowicz, Stan. 
(2008). Using the web as a linguistic resource 
to automatically correct lexico-syntactic 
errors.  
Izumi, Emi, Uchimoto, Kiyotaka, & Isahara, Hitoshi. 
(2005). Error annotation for corpus of 
Japanese learner English. Paper presented at 
the Proceedings of the Sixth International 
Workshop on Linguistically Interpreted 
Corpora. 
Kao, Ting-Hui, Chang, Yu-Wei, Chiu, Hsun-Wen, & 
Yen, Tzu-Hsi. (2013). CoNLL-2013 Shared 
Task: Grammatical Error Correction NTHU 
System Description. CoNLL-2013, 20.  
Katz, Slava. (1987). Estimation of probabilities from 
sparse data for the language model 
component of a speech recognizer. Acoustics, 
Speech and Signal Processing, IEEE 
Transactions on, 35(3), 400-401.  
Knight, Kevin, & Chander, Ishwar. (1994). Automated 
postediting of documents. Paper presented at 
the AAAI. 
Mark, Alla Rozovskaya Kai-Wei Chang, & Roth, 
Sammons Dan. (2013). The University of 
Illinois System in the CoNLL-2013 Shared 
Task. CoNLL-2013, 51, 13.  
Naber, Daniel. (2003). A rule-based style and grammar 
checker. Diploma Thesis 
Nagata, Ryo, Morihiro, Koichiro, Kawai, Atsuo, & Isu, 
Naoki. (2006). A feedback-augmented method 
for detecting errors in the writing of learners 
of English. Paper presented at the Proceedings 
of the 21st International Conference on 
Computational Linguistics and the 44th 
annual meeting of the Association for 
Computational Linguistics. 
Ng, Hwee Tou , Wu, Siew Mei , Briscoe, Ted , 
Hadiwinoto, Christian , Susanto, Raymond 
Hendy, & Bryant, Christopher (2014). The 
CoNLL-2014 Shared Task on Grammatical 
Error Correction. Paper presented at the the 
Eighteenth Conference on Computational 
Natural Language Learning: Shared Task 
(CoNLL-2014 Shared Task), Baltimore, 
Maryland, USA. 
Nicholls, Diane. (2003). The Cambridge Learner 
Corpus: Error coding and analysis for 
lexicography and ELT. Paper presented at the 
Proceedings of the Corpus Linguistics 2003 
conference. 
Rozovskaya, Alla, & Roth, Dan. (2011). Algorithm 
selection and model adaptation for ESL 
correction tasks. Urbana, 51, 61801.  
Seo, Hongsuck, Lee, Jonghoon, Kim, Seokhwan, Lee, 
Kyusong, Kang, Sechun, & Lee, Gary 
Geunbae. (2012). A meta learning approach 
to grammatical error correction. Paper 
presented at the Proceedings of the 50th 
Annual Meeting of the Association for 
Computational Linguistics: Short Papers-
Volume 2. 
Wu, Yuanbin, & Ng, Hwee Tou. (2013). Grammatical 
error correction using integer linear 
programming. Paper presented at the 
Proceedings of the 51st Annual Meeting of 
the Association for Computational 
Linguistics. 
Xing, Junwen, Wang, Longyue, Wong, Derek F, Chao, 
Lidia S, & Zeng, Xiaodong. (2013). UM-
Checker: A Hybrid System for English 
Grammatical Error Cor-rection. CoNLL-2013, 
34.  
Yannakoudakis, Helen, Briscoe, Ted, & Medlock, Ben. 
(2011). A New Dataset and Method for 
Automatically Grading ESOL Texts. Paper 
presented at the ACL. 
 
 
73
