Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 25?28,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
CASMACAT: A Computer-assisted Translation Workbench
V. Alabau
?
, C. Buck
?
, M. Carl
?
, F. Casacuberta
?
, M. Garc??a-Mart??nez
?
U. Germann
?
, J. Gonz
?
alez-Rubio
?
, R. Hill
?
, P. Koehn
?
, L. A. Leiva
?
B. Mesa-Lao
?
, D. Ortiz
?
, H. Saint-Amand
?
, G. Sanchis
?
, C. Tsoukala
?
?
PRHLT Research Center, Universitat Polit`ecnica de Val`encia
{valabau,fcn,jegonzalez,luileito,dortiz,gsanchis}@dsic.upv.es
?
Copenhagen Business School, Department of International Business Communication
{ragnar.bonk,mc.isv,mgarcia,bm.ibc}@cbs.dk
?
School of Informatics, University of Edinburgh
{cbuck,ugermann,rhill2,pkoehn,hsamand,ctsoukal}@inf.ed.ac.uk
Abstract
CASMACAT is a modular, web-based
translation workbench that offers ad-
vanced functionalities for computer-aided
translation and the scientific study of hu-
man translation: automatic interaction
with machine translation (MT) engines
and translation memories (TM) to ob-
tain raw translations or close TM matches
for conventional post-editing; interactive
translation prediction based on an MT en-
gine?s search graph, detailed recording and
replay of edit actions and translator?s gaze
(the latter via eye-tracking), and the sup-
port of e-pen as an alternative input device.
The system is open source sofware and in-
terfaces with multiple MT systems.
1 Introduction
CASMACAT
1
(Cognitive Analysis and Statistical
Methods for Advanced Computer Aided Trans-
lation) is a three-year project to develop an
advanced, interactive workbench for computer-
assisted translation (CAT). Currently, at the end of
the second year, the tool includes an array of inno-
vative features that combine to offer a rich, user-
focused working environment not available in any
other CAT tool.
CASMACAT works in close collaboration with
the MATECAT project
2
, another open-source web-
based CAT tool. However, while MATECAT is
concerned with conventional CAT, CASMACAT is
focused on enhancing user interaction and facili-
tating the real-time involvement of human trans-
lators. In particular, CASMACAT provides highly
interactive editing and logging features.
1
http://www.casmacat.eu
2
http://www.matecat.com
Through this combined effort, we hope to foster
further research in the area of CAT tools that im-
prove the translation workflow while appealing to
both professional and amateur translators without
advanced technical skills.
GUI
web
server
CAT
server
MT
server
Javascript      PHP
    Python
  Python
web socket
HTTP
HTTP
Figure 1: Modular design of the workbench: Web-
based components (GUI and web server), CAT
server and MT server can be swapped out.
2 Design and components
The overall design of the CASMACAT workbench
is modular. The system consists of four com-
ponents. (1) a front-end GUI implemented in
HTML5 and JavaScript; (2) a back-end imple-
mented in PHP; (3) a CAT server that manages the
editing process and communicates with the GUI
through web sockets; (4) a machine translation
(MT) server that provides raw translation of source
text as well as additional information, such as a
search graph that efficiently encodes alternative
translation options. Figure 1 illustrates how these
components interact with each other. The CAT
and MT servers are written in Python and inter-
act with a number of software components imple-
mented in C++. All recorded information (source,
translations, edit logs) is permanently stored in a
MySQL database.
These components communicate through a
well-defined API, so that alternative implementa-
tions can be used. This modular architecture al-
25
Figure 2: Translation view for an interactive post-editing task.
lows the system to be used partially. For instance,
the CAT and MT servers can be used separately as
part of a larger translation workflow, or only as a
front-end when an existing MT solution is already
in place.
2.1 CAT server
Some of the interactive features of CASMACAT
require real-time interaction, such as interactive
text-prediction (ITP), so establishing an HTTP
connection every time would cause a significant
network overhead. Instead, the CAT server relies
on web sockets, by means of Python?s Tornadio.
When interactive translation prediction is en-
abled, the CAT server first requests a translation
together with the search graph of the current seg-
ment from the MT server. It keeps a copy of the
search graph and constantly updates and visualizes
the translation prediction based on the edit actions
of the human translator.
2.2 MT server
Many of the functions of the CAT server require
information from an MT server. This information
includes not only the translation of the input sen-
tence, but also n-best lists, search graphs, word
alignments, and so on. Currently, the CASMACAT
workbench supports two different MT servers:
Moses (Koehn et al., 2007) and Thot (Ortiz-
Mart??nez et al., 2005).
The main call to the MT server is a request for
a translation. The request includes the source sen-
tence, source and target language, and optionally
a user ID. The MT server returns an JSON object,
following an API based on Google Translate.
3 Graphical User Interface
Different views, based on the MATECAT GUI,
perform different tasks. The translation view is
the primary one, used when translating or post-
editing, including logging functions about the
translation/post-editing process. Other views im-
plement interfaces to upload new documents or to
manage the documents that are already in the sys-
tem. Additionally, a replay view can visualize all
edit actions for a particular user session, including
eye tracking information, if available.
3.1 Post-Editing
In the translation view (Figure 2), the document
is presented in segments and the assistance fea-
tures provided by CASMACAT work at the segment
level. If working in a post-editing task without
ITP, up to three MT or TM suggestions are pro-
vided for the user to choose. Keyboard shortcuts
are available for performing routine tasks, for in-
stance, loading the next segment or copying source
text into the edit box. The user can assign different
status to each segment, for instance, ?translated?
for finished ones or ?draft? for segments that still
need to be reviewed. Once finished, the translated
document can be downloaded in XLIFF format.
3
In the translation view, all user actions re-
lated to the translation task (e.g. typing activity,
mouse moves, selection of TM proposals, etc.) are
recorded by the logging module, collecting valu-
able information for off-line analyses.
3.2 Interactive Translation Prediction
Here we briefly describe the main advanced CAT
features implemented in the workbench so far.
Intelligent Autocompletion: ITP takes place
every time a keystroke is detected by the sys-
tem (Barrachina et al., 2009). In such event, the
system produces a prediction for the rest of the
sentence according to the text that the user has al-
ready entered. This prediction is placed at the right
of the text cursor.
Confidence Measures: Confidence mea-
sures (CMs) have two main applications in
3
XLIFF is a popular format in the translation industry.
26
MT (Gonz?alez-Rubio et al., 2010). Firstly, CMs
allow the user to clearly spot wrong translations
(e.g., by rendering in red those translations
with very low confidence according to the MT
module). Secondly, CMs can also inform the user
about the translated words that are dubious, but
still have a chance of being correct (e.g., rendered
in orange). Figure 3 illustrates this.
Figure 3: Visualisation of Confidence Measures
Prediction Length Control: Providing the user
with a new prediction whenever a key is pressed
has been proved to be cognitively demanding (Al-
abau et al., 2012). Therefore, the GUI just displays
the prediction up to the first wrong word according
to the CMs provided by the system (Figure 4).
Figure 4: Prediction Length Control
Search and Replace: Most of CAT tools pro-
vide the user with intelligent search and replace
functions for fast text revision. CASMACAT fea-
tures a straightforward function to run search and
replacement rules on the fly.
Word Alignment Information: Alignment of
source and target words is an important part of
the translation process (Brown et al., 1993). To
display their correspondence, they are hihglighted
every time the user places the mouse or the text
cursor on a word; see Figure 5.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 5: Visualisation of Word Alignment
Prediction Rejection: With the purpose of eas-
ing user interaction, CASMACAT also supports a
one-click rejection feature (Sanchis-Trilles et al.,
2008). This feature invalidates the current predic-
tion made for the sentence that is being translated,
and provides the user with an alternate one.
3.3 Replay mode and logging functions
The CASMACAT workbench implements detailed
logging of user activity data, which enables both
automatic analysis of translator behaviour and
retrospective replay of a user session. Replay
takes place in the translation view of the GUI
and it displays the screen status of the recorded
translation/post-editing process. The workbench
also features a plugin to enrich the replay mode
with gaze data coming from an eye-tracker. This
eye-tracking integration is possible through a
project-developed web browser extension which,
at the moment, has only been fully tested with SR-
Research EyeLinks
4
.
4 E-pen Interaction
E-pen interaction is intended to be a complemen-
tary input rather than a substitution of the key-
board. The GUI features the minimum compo-
nents necessary for e-pen interaction; see Figure 6.
When the e-pen is enabled, the display of the cur-
rent segment is changed so that the source seg-
ment is shown above the target segment. Then the
drawing area is maximised horizontally, facilitat-
ing handwriting, particularly in tablet devices. An
HTML canvas is also added over the target seg-
ment, where the user?s drawings are handled. This
is achieved by means of MINGESTURES (Leiva
et al., 2013), a highly accurate, high-performance
gesture set for interactive text editing that can dis-
tinguish between gestures and handwriting. Ges-
tures are recognised on the client side so the re-
sponse is almost immediate. Conversely, when
handwritten text is detected, the pen strokes are
sent to the server. The hand-written text recog-
nition (HTR) server is based on iAtros, an open
source HMM decoder.
if any feature not
is available on your network
substitution
Figure 6: Word substitution with e-pen interaction
5 Evaluation
The CASMACAT workbench was recently evalu-
ated in a field trial at Celer Soluciones SL, a
language service provider based in Spain. The
trial involved nine professional translators work-
ing with the workbench to complete different post-
editing tasks from English into Spanish. The pur-
4
http://www.sr-research.com
27
pose of this evaluation was to establish which of
the workbench features are most useful to profes-
sional translators. Three different configurations
were tested:
? PE: The CASMACAT workbench was used
only for conventional post-editing, without
any additional features.
? IA: Only the Intelligent Autocompletion fea-
ture was enabled. This feature was tested sep-
arately because it was observed that human
translators substantially change the way they
interact with the system.
? ITP: All features described in Section 3.2
were included in this configuration, except-
ing CMs, which were deemed to be not accu-
rate enough for use in a human evaluation.
For each configuration, we measured the aver-
age time taken by the translator to produce the fi-
nal translation (on a segment basis), and the aver-
age number of edits required to produce the final
translation. The results are shown in Table 1.
Setup Avg. time (s) Avg. # edits
PE 92.2 ? 4.82 141.39 ? 7.66
IA 86.07 ? 4.92 124.29 ? 7.28
ITP 123.3 ? 29.72 137.22 ? 13.67
Table 1: Evaluation of the different configurations
of the CASMACAT workbench. Edits are measured
in keystrokes, i.e., insertions and deletions.
While differences between these numbers are
not statistically significant, the apparent slowdown
in translation with ITP is due to the fact that all
translators had experience in post-editing but none
of them had ever used a workbench featuring in-
telligent autocompletion before. Therefore, these
were somewhat unsurprising results.
In a post-trial survey, translators indicated that,
on average, they liked the ITP system the best.
They were not fully satisfied with the freedom of
interactivity provided by the IA system. The lack
of any visual aid to control the intelligent auto-
completions provided by the system made transla-
tors think that they had to double-check any of the
proposals made by the system when making only
a few edits.
6 Conclusions
We have introduced the current CASMACAT work-
bench, a next-generation tool for computer as-
sisted translation. Each of the features available
in the most recent prototype of the workbench has
been explained. Additionally, we have presented
an executive report of a field trial that evaluated
genuine users? performance while using the work-
bench. Although E-pen interaction has not yet
been evaluated outside of the laboratory, it will the
subject of future field trials, and a working demon-
stration is available.
Acknowledgements
Work supported by the European Union 7
th
Framework Program (FP7/2007-2013) under the
CASMACAT project (grant agreement n
o
287576).
References
Vicent Alabau, Luis A. Leiva, Daniel Ortiz-Mart??nez,
and Francisco Casacuberta. 2012. User evaluation
of interactive machine translation systems. In Proc.
EAMT, pages 20?23.
Sergio Barrachina et al. 2009. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, 35(1):3?28.
Peter Brown et al. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational linguistics, 19(2):263?311.
Jes?us Gonz?alez-Rubio, Daniel Ortiz-Mart??nez, and
Francisco Casacuberta. 2010. On the use of confi-
dence measures within an interactive-predictive ma-
chine translation system. In Proc. of EAMT.
Philipp Koehn et al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL,
pages 177?180.
Luis A. Leiva, Vicent Alabau, and Enrique Vidal.
2013. Error-proof, high-performance, and context-
aware gestures for interactive text edition. In Proc.
of CHI, pages 1227?1232.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2005. Thot: a toolkit to train
phrase-based statistical translation models. In Proc.
of MT Summit X, pages 141?148.
G. Sanchis-Trilles et al. 2008. Improving interactive
machine translation via mouse actions. In Proc. of
EMNLP, pages 485?494.
28
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 37?40,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Interactive Predictive Parsing using a Web-based Architecture?
Ricardo Sa?nchez-Sa?ez? Luis A. Leiva? Joan-Andreu Sa?nchez? Jose?-Miguel Bened???
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
{rsanchez,luileito,jandreu,jbenedi}@{?dsic,?iti}.upv.es
Abstract
This paper introduces a Web-based demon-
stration of an interactive-predictive framework
for syntactic tree annotation, where the user is
tightly integrated into the interactive parsing
system. In contrast with the traditional post-
editing approach, both the user and the sys-
tem cooperate to generate error-free annotated
trees. User feedback is provided by means of
natural mouse gestures and keyboard strokes.
1 Introduction
There is a whole family of problems within the pars-
ing world where error-free results, in the form of
perfectly annotated trees, are needed. Constructing
error-free trees is a necessity in many tasks, such
as handwritten mathematical expression recognition
(Yamamoto et al, 2006), or new gold standard tree-
bank creation (de la Clergerie et al, 2008). It is
a fact that current state-of-the-art syntactic parsers
provide trees that, although of excellent quality, still
contain errors. Because of this, the figure of a human
corrector who supervises the annotation process is
unavoidable in this kind of problems.
When using automatic parsers as a baseline for
building perfect syntactic trees, the role of the hu-
man annotator is to post-edit the trees and correct the
errors. This manner of operating results in the typ-
ical two-step process for error correcting, in which
the system first generates the whole output and then
?Work partially supported by the Spanish MICINN under
the MIPRCV ?Consolider Ingenio 2010? (CSD2007-00018),
MITTRAL (TIN2009-14633-C03-01), Prometeo (PROME-
TEO/2009/014) research projects, and the FPU fellowship
AP2006-01363. The authors wish to thank Vicent Alabau for
his invaluable help with the CAT-API library.
the user verifies or amends it. This paradigm is
rather inefficient and uncomfortable for the human
annotator. For example, in the creation of the Penn
Treebank annotated corpus, a basic two-stage setup
was employed: a rudimentary parsing system pro-
vided a skeletal syntactic representation, which then
was manually corrected by human annotators (Mar-
cus et al, 1994). Other tree annotating tools within
the two-step paradigm exist, such as the TreeBanker
(Carter, 1997) or the Tree Editor TrEd1.
With the objective of reducing the user effort and
making this laborious task easier, we devised an In-
teractive Predictive framework. Our aim is to put
the user into the loop, embedding him as a part of
the automatic parser, and allowing him to interact in
real time within the system. In this manner, the sys-
tem can use the readily available user feedback to
make predictions about the parts that have not been
validated by the corrector.
In this paper, we present a Web-based demo,
which implements the Interactive Predictive Parsing
(IPP) framework presented in (Sa?nchez-Sa?ez et al,
2009). User feedback (provided by means of key-
board and mouse operations) allows the system to
predict new subtrees for unvalidated parts of the an-
notated sentence, which in turn reduces the human
effort and improves annotation efficiency.
As a back-end for our demo, we use a more pol-
ished version of the CAT-API library, the Web-based
Computer Assisted Tool introduced in (Alabau et al,
2009). This library allows for a clean application de-
sign, in which both the server side (the parsing en-
gine) and the client side (which draws the trees, cap-
tures and interprets the user feedback, and requests
1http://ufal.mff.cuni.cz/?pajas/tred/
37
(a) System: output tree 1 (b) User: span modification (c) System: output tree 2
Figure 1: An interaction example on the IPP system.
parsed subtrees to the server) are independent. One
of the features that steam from the CAT-API library
is the ability for several annotators to work concur-
rently on the same problem-set, each in a different
client computer sharing the same parsing server.
Interactive predictive methods have been success-
fully demonstrated to ease the work of transcrip-
tors and translators in fields like Handwriting Text
Recognition (Romero et al, 2009; Toselli et al,
2008) and Statistical Machine Translation (Ortiz et
al., 2010; Vidal et al, 2006). This new paradigm
enables the collaboration between annotators across
the globe, granting them a physical and geographical
freedom that was inconceivable in the past.
2 Interactive Predictive Parsing
A tree t, associated to a string x1|x|, is composed
by substructures that are usually referred as con-
stituents. A constituent cAij is defined by the non-
terminal symbol A (either a syntactic label or a POS
tag) and its span ij (the starting and ending indexes
which delimit the part of the input sentence encom-
passed by the constituent).
Here follows a general formulation for the non-
interactive parsing scenario. Using a grammatical
model G, the parser analyzes the input sentence x =
{x1, . . . , x|x|} and produces the parse tree t?
t? = arg max
t?T
pG(t|x), (1)
where pG(t|x) is the probability of parse tree t given
the input string x using model G, and T is the set of
all possible parse trees for x.
In the interactive predictive scenario, after obtain-
ing the (probably incorrect) best tree t?, the user is
able to individually correct any of its constituents
cAij . The system reacts to each of the corrections in-
troduced by the human, proposing a new t?? that takes
into account the afore-mentioned corrections.
The action of modifying an incorrect constituent
(either setting the correct span or the correct label)
implicitly validates a subtree that is composed by
the partially corrected constituent, all of its ancestor
constituents, and all constituents whose end span is
lower than the start span of the corrected constituent.
We will name this subtree the validated prefix tree
tp. When the user replaces the constituent cAij with
the correct one c?Aij , the validated prefix tree is:
tp(c?Aij ) = {cBmn : m ? i, n ? j,
d(cBmn) ? d(c?Aij )} ?
{cDpq : p >= 1 , q < i}
(2)
with d(cBmn) being the depth of constituent cBmn.
When a constituent correction is performed, the
prefix tree tp(c?Aij ) is fixed and a new tree t?? that takes
into account the prefix is proposed
t?? = arg max
t?T
pG(t|x, tp(c?Aij )). (3)
Given that we are working with context-free
grammars, the only subtree that effectively needs to
be recalculated is the one starting from the parent of
the corrected constituent.
3 Demo outline
A preview version of the demonstration can be ac-
cessed at http://cat.iti.upv.es/ipp/.
The user is presented with the sentences in the se-
lected corpus, and starts parsing them one by one.
They make corrections in the trees both with the key-
board and the computer mouse. The user feedback
38
is decoded on the client side which in turn requests
subtrees to the parse engine.
Two kind of operations can be performed over
constituents: span modification (performed either by
dragging a line from the constituent to the word that
corresponds to the span?s upper index, or deleting
a tree branch by clicking on it), and label substitu-
tion (done by typing the correct one on its text field).
Modifying the span of a constituent invalidates its
label, so the server recalculates it as part of the suf-
fix. Modifying the label of a constituent validates its
span.
When the user is about to perform an opera-
tion, the affected constituent and the prefix that will
be validated are highlighted. The target span of
the modified constituent is visually shown as well.
When the user obtains the correctly annotated tree,
they can accept it by by clicking on a new sentence.
As already mentioned, the user is tightly inte-
grated into the interactive parsing process. They fol-
low a predetermined protocol in which they correct
and/or validate the annotated parse trees:
1. The parsing server proposes a full parse tree t
for the input sentence. The tree t is shown to
the user by the client (Fig. 1a).
2. The user finds the first2 incorrect constituent c
and starts amending it, either by changing its
label or changing its span (Fig. 1b, note how
the label is greyed out as it is discarded with
the span modification). This operation implic-
itly validates the prefix tree tp (highlighted in
green).
3. The system decodes the user feedback (i.e.,
mouse gestures or keyboard strokes) which can
either affect the label or the span of the incor-
rect constituent c:
(a) If the span of c is modified, the label is
not assumed to be correct. A partial con-
stituent c?, which includes span but no la-
bel, is decoded from the user feedback.
(b) If the label of c is modified, the span is
assumed to be correct. The corrected con-
stituent c? is decoded from the user feed-
back.
2The tree visiting order is left-to-right depth-first.
This step only deals with analysing the user
feedback, the parsing server will not be con-
tacted until the next step.
4. Either the partially corrected constituent c? or
the corrected constituent c? is then used by the
client to create a new extended consolidated
prefix that combines the validated prefix and the
user feedback: either tpc? or tpc?. The client
sends the extended prefix tree to the parsing
server and requests a suitable continuation for
the parse tree, or tree suffix ts:
(a) If the extended prefix is partial (tpc?), the
first element of ts is the label completing
c?, followed by the remaining calculated
whole constituents.
(b) If the extended prefix is complete (tpc?),
the parsing server produces a suitable tree
suffix ts which contains the remaining cal-
culated whole constituents.
5. The client concatenates the suffix returned by
the server to the validated extended prefix, and
shows the whole tree to the client (Fig. 1c).
6. These previous steps are iterated until a final,
perfect parse tree is produced by the server and
validated by the user.
Note that within this protocol, constituents can be
deleted or inserted by adequately modifying the span
of the left-neighbouring constituent.
4 Demo architecture
The proposed system coordinates client-side script-
ing with server-side technologies, by using the CAT-
API library (Alabau et al, 2009).
4.1 Server side
The server side of our system is a parsing en-
gine based on a customized CYK-Viterbi parser,
which uses a Probabilistic Context-Free Grammar in
Chomsky Normal Form obtained from sections 2 to
21 of the UPenn Treebank as a model (see (Sa?nchez-
Sa?ez et al, 2009) for details).
The client can request to the parsing server the
best subtree for any given span of the input string.
For each requested subtree, the client can either pro-
vide the starting label or not. If the starting subtree
39
label is not provided, the server calculates the most
probable label. The server also performs transparent
tree debinarization/binarization when communicat-
ing with the client.
4.2 Client side
The client side has been designed taking into ac-
count ergonomic issues in order to facilitate the in-
teraction.
The prototype is accessed through a Web browser,
and the only requirement is the Flash plugin (98% of
market penetration) installed in the client machine.
The hardware requirements in the client are very
low on the client side, as the parsing is process per-
formed remotely on the server side: any computer
(including netbooks) capable of running a modern
Web browser is enough.
Each validated user interaction is saved as a log
file on the server side, so a tree?s annotation session
can be later resumed.
4.2.1 Communication protocol
This demo exploits the WWW to enable the con-
nection of simultaneous accesses across the globe.
This architecture also provides cross-platform com-
patibility and requires neither computational power
nor disk space on the client?s machine.
Client and server communicate via asynchronous
HTTP connections, providing thus a richer interac-
tive experience ? no page refreshes is required when
parsing a new sentence. Moreover, the Web client
communicates with the IPP engine through binary
TCP sockets. Thus, response times are quite slow ? a
desired requirement for the user?s solace. Addition-
ally, cross-domain requests are possible, so the user
could switch between different IPP engines within
the same UI.
5 Evaluation results
We have carried out experiments that simulate user
interaction using section 23 of the Penn Treebank.
The results suggest figures ranging from 42% to
46% of effort saving compared to manually post-
editing the trees without an interactive system. In
other words, for every 100 erroneous constituents
produced by a parsing system, an IPP user would
correct only 58 (the other 42 constituents being au-
tomatically recalculated by the IPP system). Again,
see (Sa?nchez-Sa?ez et al, 2009) for the details on ex-
perimentation.
5.1 Conclusions and future work
We have introduced a Web-based interactive-
predictive system that, by using a parse engine in
an integrated manner, aids the user in creating cor-
rectly annotated syntactic trees. Our system greatly
reduces the human effort required for this task com-
pared to using a non-interactive automatic system.
Future work includes improvements to the client
side (e.g., confidence measures as a visual aid, mul-
timodality), as well as exploring other kinds of pars-
ing algorithms for the server side (e.g., adaptative
parsing).
References
V. Alabau, D. Ortiz, V. Romero, and J. Ocampo. 2009. A
multimodal predictive-interactive application for com-
puter assisted transcription and translation. In ICMI-
MLMI ?09, 227?228.
D. Carter. 1997. The TreeBanker. A tool for supervised
training of parsed corpora. In ENVGRAM?97, 9?15.
E.V. de la Clergerie, O. Hamon, D. Mostefa, C. Ayache,
P. Paroubek, and A. Vilnat. 2008. Passage: from
French parser evaluation to large sized treebank. In
LREC?08, 100:P2.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):313?330.
D. Ortiz, L. A. Leiva, V. Alabau, and F. Casacuberta.
2010. Interactive machine translation using a web-
based architecture. In IUI?10, 423?425.
V. Romero, L. A. Leiva, A. H. Toselli, and E. Vidal.
2009. Interactive multimodal transcription of text
imagse using a web-based demo system. In IUI?09,
477?478.
R. Sa?nchez-Sa?ez, J.A. Sa?nchez, and J.M. Bened??. 2009.
Interactive predictive parsing. In IWPT?09, 222?225.
A.H. Toselli, V. Romero, and E. Vidal. 2008. Computer
assisted transcription of text images and multimodal
interaction. In MLMI?08, 5237: 296?308.
E. Vidal, F. Casacuberta, L. Rodr??guez, J. Civera, and
C. Mart??nez. 2006. Computer-assisted translation us-
ing speech recognition. IEEE Trans. on Audio, Speech
and Language Processing, 14(3):941?951.
R. Yamamoto, S. Sako, T. Nishimoto, and S. Sagayama.
2006. On-line recognition of handwritten mathe-
matical expressions based on stroke-based stochastic
context-free grammar. In 10th Frontiers in Handwrit-
ing Recognition, 249?254.
40
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 68?73,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
An Interactive Machine Translation System with Online Learning
Daniel Ortiz-Mart??nez, Luis A. Leiva, Vicent Alabau,
Ismael Garc??a-Varea?, Francisco Casacuberta
ITI - Institut Tecnolo`gic d?Informa`tica, Universitat Polite`cnica de Vale`ncia
? Departamento de Sistemas Informa?ticos, Universidad de Castilla-La Mancha
{dortiz,luileito,valabau,fcn}@iti.upv.es, ?ismael.garcia@uclm.es
Abstract
State-of-the-art Machine Translation (MT)
systems are still far from being perfect. An
alternative is the so-called Interactive Ma-
chine Translation (IMT) framework, where
the knowledge of a human translator is com-
bined with the MT system. We present a sta-
tistical IMT system able to learn from user
feedback by means of the application of on-
line learning techniques. These techniques al-
low the MT system to update the parameters of
the underlying models in real time. According
to empirical results, our system outperforms
the results of conventional IMT systems. To
the best of our knowledge, this online learning
capability has never been provided by previ-
ous IMT systems. Our IMT system is imple-
mented in C++, JavaScript, and ActionScript;
and is publicly available on the Web.
1 Introduction
The research in the field of machine translation
(MT) aims to develop computer systems which are
able to translate text or speech without human in-
tervention. However, current translation technology
has not been able to deliver full automated high-
quality translations. Typical solutions to improve the
quality of the translations supplied by an MT system
require manual post-editing. This serial process pre-
vents the MT system from integrating the knowledge
of the human translator.
An alternative way to take advantage of the exist-
ing MT technologies is to use them in collaboration
with human translators within a computer-assisted
translation (CAT) or interactive framework (Isabelle
and Church, 1997). Interactivity in CAT has been
explored for a long time. Systems have been de-
signed to interact with linguists to solve ambiguities
or update user dictionaries.
An important contribution to CAT technology was
pioneered by the TransType project (Foster et al,
1997; Langlais et al, 2002). The idea proposed in
that work was to embed data driven MT techniques
within the interactive translation environment. Fol-
lowing the TransType ideas, Barrachina et al (2009)
proposed the so-called IMT framework, in which
fully-fledged statistical MT (SMT) systems are used
to produce full target sentences hypotheses, or por-
tions thereof, which can be accepted or amended
by a human translator. Each corrected text segment
is then used by the MT system as additional infor-
mation to achieve improved suggestions. Figure 1
shows an example of a typical IMT session.
The vast majority of the existing work on
IMT makes use of the well-known batch learning
paradigm. In the batch learning paradigm, the train-
ing of the IMT system and the interactive transla-
tion process are carried out in separate stages. This
paradigm is not able to take advantage of the new
knowledge produced by the user of the IMT system.
In this paper, we present an application of the online
learning paradigm to the IMT framework. In the on-
line learning paradigm, the training and prediction
stages are no longer separated. This feature is par-
ticularly useful in IMT since it allows to take into ac-
count the user feedback. Specifically, our proposed
IMT system can be extended with the new training
samples that are generated each time the user vali-
dates the translation of a given source sentence. The
online learning techniques implemented in our IMT
system incrementally update the statistical models
involved in the translation process.
2 Related work
There are some works on IMT in the literature that
try to take advantage of user feedback. One exam-
ple is the work by Nepveu et al (2004), where dy-
namic adaptation of an IMT system via cache-based
model extensions to language and translation models
is proposed. One major drawback of such proposal
is its inability to learn new words.
68
source(f ): Para ver la lista de recursos
reference(e?): To view a listing of resources
interaction-0
ep
es To view the resources list
interaction-1
ep To view
k a
es list of resources
interaction-2
ep To view a list
k list i
es list i ng resources
interaction-3
ep To view a listing
k o
es o f resources
accept ep To view a listing of resources
Figure 1: IMT session to translate a Spanish sentence into English. In interaction-0, the system suggests a translation
(es). In interaction-1, the user moves the mouse to accept the first eight characters ?To view ? and presses the a key
(k), then the system suggests completing the sentence with ?list of resources? (a new es). Interactions 2 and 3 are
similar. In the final interaction, the user accepts the current suggestion.
Recent research on IMT has proposed the use of
online learning as one possible way to successfully
incorporate user feedback in IMT systems (Ortiz-
Mart??nez et al, 2010). In the online learning setting,
models are trained sample by sample. For this rea-
son, such learning paradigm is appropriate for its use
in the IMT framework. The work by Ortiz-Mart??nez
et al (2010) implements online learning as incre-
mental learning. Specifically, an IMT system able
to incrementally update the parameters of all of the
different models involved in the interactive transla-
tion process is proposed. One previous attempt to
implement online learning in IMT is the work by
Cesa-Bianchi et al (2008). In that work, the authors
present a very constrained version of online learn-
ing, which is not able to extend the translation mod-
els due to the high time cost of the learning process.
We have adopted the online learning techniques
proposed in (Ortiz-Mart??nez et al, 2010) to imple-
ment our IMT system. We are not aware of other
IMT tools that include such functionality. For in-
stance, a prototype system for text prediction to help
translators is shown in (Foster et al, 2002). Addi-
tionally, Koehn (2009) presents the Caitra transla-
tion tool. Caitra aids linguists suggesting sentence
completions, alternative words or allowing users to
post-edit machine translation output. However, nei-
ther of these systems are able to take advantage of
the user validated translations.
3 Interactive Machine Translation
IMT can be seen as an evolution of the statistical ma-
chine translation (SMT) framework. In SMT, given
source string f , we seek for the target string e which
maximizes the posterior probability:
e? = argmax
e
Pr(e|f) (1)
Within the IMT framework, a state-of-the-art
SMT system is employed in the following way. For
a given source sentence, the SMT system automati-
cally generates an initial translation. A human trans-
lator checks this translation from left to right, cor-
recting the first error. The SMT system then pro-
poses a new extension, taking the correct prefix ep
into account. These steps are repeated until the
whole input sentence has been correctly translated.
In the resulting decision rule, we maximize over all
possible extensions es of ep:
e?s = argmax
es
Pr(es|ep, f) (2)
It is worth to note that the user interactions are at
character level, that is, for each submitted keystroke
the system provides a new extension (or suffix) to
the current hypothesis. A typical IMT session for a
given source sentence is depicted in Figure 1.
State-of-the-art SMT systems follow a log-linear
approach (Och and Ney, 2002), where the posterior
69
probability Pr(e | f) of Eq. (1) is used. Such log-
linear approach can be easily adapted for its use in
the IMT framework as follows:
e?s = argmax
es
{
M?
m=1
?mhm(ep, es, f)
}
(3)
where each hm(ep, es, f) is a feature function rep-
resenting a statistical model and ?m its correspond-
ing weight. Typically, a set of statistical generative
models are used as feature functions. Among this
feature functions, the most relevant are the language
and translation models. The language model is im-
plemented using statistical n-gram language mod-
els and the translation model is implemented using
phrase-based models.
The IMT system proposed here is based on a log-
linear SMT system which includes a total of seven
feature functions: an n-gram language model, a tar-
get sentence length model, inverse and direct phrase-
based models, source and target phrase length mod-
els and a reordering model.
4 Online Learning
In the online learning paradigm, learning proceeds
as a sequence of trials. In each trial, a sample is
presented to the learning algorithm to be classified.
Once the sample is classified, its correct label is told
to the learning algorithm.
The online learning paradigm fits nicely in the
IMT framework, since the interactive translation of
the source sentences generates new user-validated
training samples that can be used to extend the sta-
tistical models involved in the translation process.
One key aspect in online learning is the time re-
quired by the learning algorithm to process the new
training samples. One way to satisfy this constraint
is to obtain incrementally updateable versions of the
algorithms that are executed to train the statistical
models involved in the translation process. We have
adopted this approach to implement our IMT sys-
tem. Specifically, our proposed IMT system imple-
ments the set of training algorithms that are required
to incrementally update each component of the log-
linear model. Such log-linear model is composed of
seven components (see section 3). One key aspect of
the required training algorithms is the necessity to
replace the conventional expectation-maximization
(EM) algorithm by its incremental version (Neal and
Hinton, 1998). The complete details can be found in
(Ortiz-Mart??nez et al, 2010).
5 System Overview
In this section the main features of our prototype are
shown, including prototype design, interaction pro-
tocol, prototype functionalities and demo usage.
5.1 Prototype Design
Prototype architecture has been built on two main
aspects, namely, accessibility and flexibility. The
former is necessary to reach a larger number of po-
tential users. The latter allows researchers to test
different techniques and interaction protocols.
For that reason, we developed an CAT Appli-
cation Programming Interface (API) between the
client and the actual translation engine, by using
a network communication protocol and exposing a
well-defined set of functions.
Figure 2: IMT system architecture.
A diagram of the architecture is shown in Fig-
ure 2. On the one hand, the IMT client provides a
User Interface (UI) which uses the API to commu-
nicate with the IMT server through the Web. The
hardware requirements in the client are very low,
as the translation process is carried out remotely on
the server, so virtually any computer (including net-
books, tablets or 3G mobile phones) should be fairly
enough. On the other hand, the server, which is
unaware of the implementation details of the IMT
client, uses and adapts the statistical models that are
used to perform the translation.
5.2 User Interaction Protocol
The protocol that rules the IMT process has the fol-
lowing steps:
1. The system proposes a full translation of the
selected text segment.
70
Figure 3: Demo interface. The source text segments are automatically extracted from source document. Such segments
are marked as pending (light blue), validated (dark green), partially translated (light green), and locked (light red). The
translation engine can work either at full-word or character level.
2. The user validates the longest prefix of the
translation which is error-free and/or corrects
the first error in the suffix. Corrections are
entered by amendment keystrokes or mouse
clicks/wheel operations.
3. In this way, a new extended consolidated pre-
fix is produced based on the previous validated
prefix and the interaction amendments. Using
this new prefix, the system suggests a suitable
continuation of it.
4. Steps 2 and 3 are iterated until the user-desired
translation is produced.
5. The system adapts the models to the new vali-
dated pair of sentences.
5.3 Prototype Functionality
The following is a list of the main features that the
prototype supports:
? When the user corrects the solution proposed
by the system, a new improved suffix is pre-
sented to the user.
? The system is able to learn from user-validated
translations.
? The user is able to perform actions by means
of keyboard shortcuts or mouse gestures. The
supported actions on the proposed suffix are:
Substitution Substitute the first word or char-
acter of the suffix.
Deletion Delete the first word of the suffix.
Insertion Insert a word before the suffix.
Rejection The rejected word will not appear in
the following proposals.
Acceptance Assume that the current transla-
tion is correct and adapt the models.
? At any time, the user is able to visualize the
original document (Figure 4(a)), as well as a
properly formated draft of the current transla-
tion (Figure 4(b)).
? Users can select the document to be translated
from a list or upload their own documents.
5.4 Demo Description and Usage
This demo exploits the WWW to enable the connec-
tion of simultaneous accesses across the globe, coor-
dinating client-side scripting with server-side tech-
nologies. The interface uses web technologies such
as XHTML, JavaScript, and ActionScript; while the
IMT engine is written in C++.
The prototype is publicly available on the Web
(http://cat.iti.upv.es/imt/). To begin
with, the UI loads an index of all available transla-
tion corpora. Currently, the prototype can be tested
with the well-known Europarl corpora (Koehn,
2005). The user chooses a corpus and navigates to
the main interface page (Figure 3), where she in-
teractively translates the text segments one by one.
User?s feedback is then processed by the IMT server.
71
(a) Source document example, created from EuroParl corpus.
(b) Translated example document, preserving original format and highlighting non-translated sentences.
Figure 4: Translating documents with the proposed system.
All corrections are stored in plain text logs on the
server, so the user can retake them in any mo-
ment, also allowing collaborative translations be-
tween users. On the other hand, this prototype al-
lows uploading custom documents in text format.
Since the users operate within a web browser,
the system also provides crossplatform compatibil-
ity and requires neither computational power nor
disk space on the client?s machine. The communi-
cation between application and web server is based
on asynchronous HTTP connections, providing thus
a richer interactive experience (no page refreshes are
required.) Moreover, the Web server communicates
with the IMT engine through binary TCP sockets,
ensuring really fast response times.
6 Experimental Results
Experimental results were carried out using the Xe-
rox corpus (Barrachina et al, 2009), which con-
sists of translation of Xerox printer manual involv-
ing three different language pairs: French-English,
Spanish-English, and German-English. This corpus
has been extensively used in the literature to report
IMT results. The corpus consists of approximately
50,000 sentences pairs for training, 1,000 for devel-
opment, and 1,000 for test.
The evaluation criteria used in the experiments are
the key-stroke and mouse-action ratio (KSMR) met-
ric (Barrachina et al, 2009), which measures the
user effort required to generate error-free transla-
tions, and the well-known BLEU score, which con-
stitutes a measure of the translation quality.
The test corpora were interactively translated
from English to the other three languages, compar-
ing the performance of a batch IMT (baseline) and
the online IMT systems. The batch IMT system
is a conventional IMT system which is not able to
take advantage of user feedback after each trans-
lation is performed. The online IMT system uses
the translations validated by the user to adapt the
translation models at runtime. Both systems were
initialized with a log-linear model trained in batch
mode using the training corpus. Table 1 shows the
BLEU score and the KSMR for the batch and the
online IMT systems (95% confidence intervals are
shown). The BLEU score was calculated from the
first translation hypothesis produced by the IMT sys-
tem for each source sentence. All the obtained im-
provements with the online IMT system were statis-
tically significant. The average online training time
for each new sample presented to the system, and
the average response time for each user interaction
72
(that is, time that the system uses to propose new
extensions for corrected prefixes) are also shown in
Table 1, which are less than a tenth of a second and
around two tenths of a second respectively1. Ac-
cording to the reported response and online training
times, we can argue that the system proposed here is
able to be used on real time scenarios.
System BLEU KSMR LT/RT (s)
En-Sp
batch 55.1? 2.3 18.2? 1.1 ? /0.09
online 60.6? 2.3 15.8? 1.0 0.04 /0.09
En-Fr
batch 33.7? 2.0 33.9? 1.3 ? /0.14
online 42.2? 2.2 27.9? 1.3 0.09 /0.14
En-Ge
batch 20.4? 1.8 40.3? 1.2 ? /0.15
online 28.0? 2.0 35.0? 1.3 0.07 /0.15
Table 1: BLEU and KSMR results for the XEROX test
corpora using the batch and the online IMT systems, re-
porting the average online learning (LT) and the interac-
tion response times (RP) in seconds.
It is worth mentioning that the results presented
here significantly improve those presented in (Bar-
rachina et al, 2009) for other state-of-the-art IMT
systems using the same corpora.
7 Conclusions
We have described an IMT system with online learn-
ing which is able to learn from user feedback in real
time. As far as we know, to our knowledge, this
feature have never been provided by previously pre-
sented IMT prototypes.
The proposed IMT tool is publicly available
through the Web (http://cat.iti.upv.es/
imt/). Currently, the system can be used to inter-
actively translate the well-known Europarl corpus.
We have also carried out experiments with simulated
users. According to such experiments, our IMT
system is able to outperform the results obtained
by conventional IMT systems implementing batch
learning. Future work includes researching further
on the benefits provided by our online learning tech-
niques with experiments involving real users.
Acknowledgments
Work supported by the EC (FEDER/FSE), the Span-
ish Government (MEC, MICINN, MITyC, MAEC,
1All the experiments were executed in a PC with 2.40 GHz
Intel Xeon processor and 1GB of memory.
?Plan E?, under grants MIPRCV ?Consolider In-
genio 2010? CSD2007-00018, iTrans2 TIN2009-
14511, erudito.com TSI-020110-2009-439), the
Generalitat Valenciana (grant Prometeo/2009/014,
grant GV/2010/067), the Universitat Polite`cnica de
Vale`ncia (grant 20091027), and the Spanish JCCM
(grant PBI08-0210-7127).
References
S. Barrachina, O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Toma?s,
and E. Vidal. 2009. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, 35(1):3?28.
N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008.
Online learning algorithms for computer-assisted
translation. Deliverable D4.2, SMART: Stat. Multi-
lingual Analysis for Retrieval and Translation.
G. Foster, P. Isabelle, and P. Plamondon. 1997. Target-
text mediated interactive machine translation. Ma-
chine Translation, 12(1):175?194.
G. Foster, P. Langlais, and G. Lapalme. 2002. Transtype:
text prediction for translators. In Proc. HLT, pages
372?374.
P. Isabelle and K. Church. 1997. Special issue on
new tools for human translators. Machine Translation,
12(1?2).
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In Proc. of the MT Summit X,
pages 79?86, September.
P. Koehn. 2009. A web-based interactive computer aided
translation tool. In Proc. ACL-IJCNLP, ACLDemos,
pages 17?20.
P. Langlais, G. Lapalme, and M. Loranger. 2002.
Transtype: Development-evaluation cycles to boost
translator?s productivity. Machine Translation,
15(4):77?98.
R.M. Neal and G.E. Hinton. 1998. A view of the
EM algorithm that justifies incremental, sparse, and
other variants. In Proc. of the NATO-ASI on Learning
in graphical models, pages 355?368, Norwell, MA,
USA.
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004.
Adaptive language and translation models for interac-
tive machine translation. In Proc. EMNLP, pages 190?
197.
F. J. Och and H. Ney. 2002. Discriminative Training
and Maximum Entropy Models for Statistical Machine
Translation. In Proc. ACL, pages 295?302.
D. Ortiz-Mart??nez, I. Garc??a-Varea, and F. Casacuberta.
2010. Online learning for interactive statistical ma-
chine translation. In Proc. NAACL/HLT, pages 546?
554.
73
Workshop on Humans and Computer-assisted Translation, pages 10?15,
Gothenburg, Sweden, 26 April 2014. c?2014 Association for Computational Linguistics
Proofreading Human Translations with an E-pen
Vicent Alabau and Luis A. Leiva
PRHLT Research Center
Universitat Polite`cnica de Vale`ncia
{valabau,luileito}@prhlt.upv.es
Abstract
Proofreading translated text is a task
aimed at checking for correctness, con-
sistency, and appropriate writing style.
While this has been typically done with
a keyboard and a mouse, pen-based
devices set an opportunity for making
such corrections in a comfortable way,
as if proofreading on physical paper.
Arguably, this way of interacting with
a computer is very appropriate when
a small number of modifications are
required to achieve high-quality stan-
dards. In this paper, we propose a tax-
onomy of pen gestures that is tailored
to machine translation review tasks, af-
ter human translator intervention. In
addition, we evaluate the recognition
accuracy of these gestures using a cou-
ple of popular gesture recognizers. Fi-
nally, we comment on open challenges
and limitations, and discuss possible
avenues for future work.
1 Introduction
Currently, the workflow of many translation
agencies include a final reviewing or proof-
reading process
1
where the translators? work
is checked for correctness, consistency and
appropriate writing style. If the translation
quality is good enough, only a small amount
of changes would be necessary to reach a
high-quality result. However, the required
corrections are often spread sparingly and
unequally among the screen, which renders
1
The reviewing process can be seen as a detailed
proofreading process where the target sentence is also
compared against the source sentence for errors such as
mistranslations, etc. However, for the purpose of this
paper, we can use the terms reviewing and proofread-
ing indistinguishably.
mouse/keyboard interaction both inefficient
and unappealing.
As a result of the popularization of touch-
screen and pen-based devices, text-editing ap-
plications can be operated today in a simi-
lar way people interact with pen and paper.
This way of reviewing is arguably more natu-
ral and efficient than a keyboard or a mouse,
since the e-pen can be used both to locate and
correct an erroneous word, all at once. Ad-
ditionally, the expressiveness of e-pen interac-
tion provides an opportunity to integrate use-
ful gestures that are able correct other com-
mon mistakes, such as word reordering or cap-
italization.
2 Related Work
The first attempt that we are aware of to post-
edit text with an e-pen interface dates back to
the early seventies of the past century (Cole-
man, 1969). In that work, Coleman proposed
a set of unistroke gestures for post-editing.
Later on, the same corpus was used by (Ru-
bine, 1991) in his seminal work about gesture
recognition with excellent recognition results.
However, the gesture set is too simplistic to be
used in a real translation task today.
Most of the modern applications to generate
and edit textual content using ?digital ink? are
based on ad-hoc interaction protocols
2
and of-
ten do not ship handwriting recognition soft-
ware. To our knowledge, MyScript Notes Mo-
bile
3
is the closest system to provide a natural
onscreen paper-like interaction style, includ-
ing some text-editing gestures and a powerful
handwriting recognition software. However,
this application relies on spatial relations of
the ink strokes to perform handwriting recog-
2
http://appadvice.com/appguides/show/
handwriting-apps-for-ipad
3
http://www.visionobjects.com
10
nition. For instance, to insert a new word
in the middle of a sentence the user needs to
make room for space explicitly (i.e., if the word
has N characters, the user needs to perform an
Insert Space gesture N times). Moreover, the
produced text does not flow on the UI, i.e., it is
fixed to the position of the ink, which makes
it difficult to modify. As a result, this sys-
tem does not seem suitable for reviewing trans-
lations. Other comparable work is MinGes-
tures (Leiva et al., 2013), which proposes a
simplified set of gestures for interactive text
post-editing. Although MinGestures is very
efficient and accurate, it is also very limited in
expressiveness. Only basic edition capabilities
are allowed (insertion, deletion, and substitu-
tion). Thus, advanced e-pen gestures cannot
be used to improve the efficiency of the re-
viewer.
On the other hand, there are applications
for post-editing text where user interactions
are leveraged to propagate text corrections to
the rest of the sentence. CueTIP (Shilman et
al., 2006), CATTI (Romero et al., 2009) and
IMT (Alabau et al., 2014) are the most ad-
vanced representatives of this kind of applica-
tions. These systems allow the user to cor-
rect text either in the form of unconstrained
cursive handwriting or (limited) pen gestures.
Then, the corrections are leveraged by the sys-
tem to provide smart auto-completion capa-
bilities. This way, user interaction is not only
taken into account to amend the proposed cor-
rection but other mistakes in the surrounding
text are automatically amended as well. How-
ever, user interaction is limited in these cases.
In CueTIP, only one handwritten character
can be submitted at a time and only 4 ges-
tures can be performed (join, split, delete, and
substitution). In CATTI, the user can hand-
write text freely but is still limited to perform
4 gestures as well (substitute, insert, delete,
and reject). Finally, IMT does not support
gestures other than substitution. Although
the auto-completion capability is a very inter-
esting and promising topic, it should not be
considered for reviewing: given the locality of
the small amount of changes that are probably
needed, auto-completion can make more harm
than good.
Thus, in light of the current limitations of
state-of-the-art approaches, in this work we
present an exploratory research of how paper-
like interaction should be approached to allow
proofreading translated texts.
3 A Taxonomy of Proofreading
Gestures
Indicating text modifications on a sheet of
paper can be made in many different ways.
However, the lack of a consensus may lead
to misinterpretations. Fortunately, a series
of authoritative proofreading and copy-editing
symbols have been proposed (AMA, 2007;
CMO, 2010), even leading to an eventual stan-
dardization (BS, 2005; ISO, 1983).
We have studied the aforementioned author-
itative sources and have found that there is a
huge overlap in the proposed symbols, with
only minor variations. Moreover, such sym-
bols are meant to ease human-human com-
munication and therefore we need to adapt
them to ease human-computer communica-
tion. This way, we will focus on those sym-
bols that could be used to review using stroke-
based gestures. As such, we will study gestures
that allow to change the content and not the
formatting of the text. We can define the fol-
lowing high-level operations; see Figure 1:
Word change: change text?s written form.
Letter case: change word/character casing.
Punctuation: insert punctuation symbols.
Word combination: separate or join words.
Selection: select words or characters.
Text displacement: move text around.
It is worth noting that punctuation sym-
bols are represented explicitly in the litera-
ture, probably because of their importance in
copy-editing tasks. In addition, dot and hy-
phen symbols are represented differently from
other insertion symbols. The purpose of this
convention is to reduce visual ambiguity in hu-
man recognition. Finally, the selection opera-
tion is often devoted to spell out numbers or
abbreviations.
4 Preliminary Evaluation
The initial taxonomy (Figure 1) aims to be a
complete set of symbols for proofreading and
copy-editing onscreen. Nonetheless, the suc-
cess of these gestures will depend on the accu-
11
APOS QUOT DOTCOMMA SEMI COLON COLON
Punctuation
DELETE INSERT TEXT
Word change
LOWERCASE UPPERCASE CAMELCASE
Letter case
ENCIRCLE
Selection
REMOVE SPACE INSERT SPACE HYPHEN
Word combination
MOVE SELECTION FORWARD SWAP BLOCKS TRANSPOSE TEXT BLOCKSMOVE SELECTION BACKWARD
Text displacement
Figure 1: Initial taxonomy, based on de facto proofreading symbols.
racy of gesture recognizers, to correctly trans-
late gestures into commands.
As a first approach, we wanted to evalu-
ate these symbols with state-of-the-art gesture
recognizers. The initial taxonomy differs sig-
nificantly from other gesture sets in the liter-
ature (Anthony and Wobbrock, 2012; Vatavu
et al., 2012), in the sense that the symbols we
are researching are not expected to be drawn
in isolation. Instead, reviewers will issue a ges-
ture in a very specific context, and so a proof-
reading symbol may change its meaning. This
is specially true for symbols involving multiple
spans of text or block displacements: depend-
ing of the size of the span or the length of the
displacement, the aspect ratio and proportions
among the different parts of the gesture strokes
may vary. Thus, the final shape of the gesture
can be significantly different. An example is
given in Figure 2.
Lorem ipsum dolor sit amet
(a) Move forward with 1 selected word and 2 word
displacement.
Lorem ipsum dolor sit amet
(b) Move forward with 4 selected words and 1 word
displacement.
Figure 2: Examples of the same gesture ex-
ecuted with different proportions. As a re-
sult, the shapes of both gestures significantly
diverge from each other.
4.1 Gesture Samples Acquisition
We carried out a controlled study in a real-
world setup. We developed an application
that requested a set of random challenges to
the users (Figure 3). Then, we asked the
users if they would prefer to do the acquisi-
tion on a digitizer tablet or on a tablet com-
puter. On a 1 to 5 point scale, with 1 mean-
ing ?I prefer writing with a digitizer pen? and
5 ?I prefer writing with a pen-capable tablet?,
users indicated that they would prefer a tablet
computer (M=4.6, SD=0.8). Consequently,
we deployed the application into a Lenovo
ThinkPad tablet, which had to be operated
with an e-pen. To make the paper-like ex-
perience more realistic, the touchscreen func-
tionality was disabled, so that users could rest
their hands on the screen. Eventually, 12 users
aged 24?36 submitted 5 times each gesture fol-
lowing the aforementioned random challenges.
Figure 3: Acquisition application.
4.2 The Family of $ Recognizers
In HCI, there is a popular ?dollar series?
of template-matching gesture recognizers, us-
ing a nearest-neighbor classifier with scoring
functions based on Euclidean distance. The
$ recognizers present several advantages over
other classifiers based on more complex pat-
tern recognition algorithms. First, $ recogniz-
ers are easily understandable and fast to in-
tegrate or re-implement in different program-
ming languages. Second, they do not depend
on large amounts of training data to achieve
12
high accuracy, just on a small number of pre-
defined templates.
In particular, $N (Anthony and Wobbrock,
2012) and $P (Vatavu et al., 2012) can be
used to recognize multi-stroke gestures, so
they were the only suitable candidates to rec-
ognize our initial gesture taxonomy. On the
one hand, $N deals with multiple strokes by
recombining in every possible way the strokes
of the templates in order to generate new in-
stances of unistroke templates, and then ap-
ply either the $1 recognizer (Wobbrock et al.,
2007) or Protractor (Li, 2010). On the other
hand, $P considers gesture strokes as a cloud
of points, removing thus information about
stroke sequentiality. Then, the best match
is found using an approximation of the Hun-
garian algorithm, which pairs points from the
template with points of the query gesture.
4.3 Results
We evaluated three fundamental aspects of
the recognition process: accuracy, recognition
time and memory requirements to store the
whole set of templates. Aiming for a portable
recognizer that could work on most everyday
devices, we decided to use a JavaScript (ro-
tation invariant) version of the $ family rec-
ognizers. Experiments were executed as a
nodejs program on a Ubuntu Linux computer
with a 2.83 GHz Intel QuadCore
TM
and 4 GB
of RAM. We followed a leaving-one-out (LOO)
setup, i.e., each user?s set of gestures was used
as templates and tested against the rest of the
user?s gestures. All the values show the aver-
age of the different LOO runs.
Table 1 summarizes the experimental re-
sults. For the $N recognizer we found that,
by resampling to 32 points and 5 templates,
we can achieve very good recognition times
(0.7ms in average) but high recognition error
rate (23.6%). On the other hand, the $P rec-
ognizer behaves even worse, with 27.1% error
rate. Memory requirements are marginal but
recognition times increase more than one order
of magnitude.
It must be noted that the space needed by
$N to store just one template of n strokes is
n! ? 2
n
times the space for the original tem-
plate (Vatavu et al., 2012). This is actually
a huge waste of resources. For instance, one
template of the insert space gesture requires
Recognizer Error Time Mem. usage
$N 23.6% 0.7 ms 102 MB
$P 27.1% 45 ms 1.8 MB
Table 1: Results for $N and $P recognizers,
with gestures resampled to 32 points and using
5 templates per gesture.
3840 times the original size, assuming that the
user has introduced the minimum strokes re-
quired. With a resampling 8 points, $N needs
almost 33MB of RAM to store 5 templates per
gesture.
4.4 Error analysis
Surprised by the high error rates we decided
to delve into the results of the most accurate
setup so we could find the source of errors.
We observed that the most difficult gesture
to recognize was remove space, which rep-
resented 12% of the total number of errors;
being confused with comma and semi colon
more than 50% of the time, probably because
they are formed by two arcs. It was also con-
fused, though less frequently, with move se-
lection forward/backward. These ges-
tures, excepting the circle part, are also com-
posed by two arcs.
On the other hand, punctuation symbols ac-
counted for 37% of the errors, being mostly
confused with each other, as they have very
similar shapes. Finally, some errors are harder
to dissect. For instance, uppercase was
confused mainly with both move selection
(4.4% of the errors), and punctuation and dis-
placement operations were also confused with
each other at some time, despite their very dif-
ferent visual shapes and sizes. We suspect it
is because of the internal normalization proce-
dures of the $ recognizers.
5 Discussion
Our results suggest that the $ family of gesture
recognizers, although popular, are not appro-
priate for proofreading translated texts. Our
assumption is that the normalization proce-
dures of these recognizers?mainly scaling and
resampling? are not appropriate to gestures
for which the proportions of its constituent
parts may vary according to the context. For
13
Figure 4: One proposal for gesture set sim-
plification. A Pop-up menu could assist the
user to disambiguate among perceptually sim-
ilar gestures.
example, after resizing a move selection
forward that selects a small word and has
a long arrow, the final shape would be primar-
ily that of the arrow (Figure 2).
In the light of this analysis, several actions
can be taken for future work. Firstly, other
gesture recognizers should be explored that
can deal with stroke sequences without resam-
pling (Myers and Rabiner, 1981; Sezgin and
Davis, 2005;
?
Alvaro et al., 2013). However, it
must be remarked that response time is crucial
to ensure an adequate user experience. There-
fore, the underlying algorithms should be im-
plementable on thin clients, such as mobile de-
vices, with reasonable recognition times.
Secondly, it would be also necessary to re-
duce the set of gestures, but not at the expense
of reducing also expressiveness as Leiva et al.
(2013) did. For instance, taking advantage of
the interaction that computers can provide, we
can group punctuation operations, space, and
insert hyphen all into insert above and
below gestures. Both gestures would pop-
up a menu where the user could select deter-
ministically the symbol to insert; see Figure 4.
In the same manner, letter casing operations
could be grouped into a single selection cat-
egory, which would also provide a contextual
menu to trigger the right command. The re-
sulting set of gestures should be, in principle,
much easier to recognize.
Additionally, the current set of proofread-
ing gestures present further challenges. For
instance, we would need to identify the seman-
tics of the gestures, i.e., which elements in the
text are affected by the gesture and how the
system should proceed to accomplish the task.
6 Conclusions
In this work we have defined a set of gestures
that is suitable for the reviewing process of
human-translated text. We have performed
an evaluation on gestures generated by real
users that show that popular recognizers are
not able to achieve a satisfactory accuracy. In
consequence, we have identified a series of ar-
eas for improvement that could make e-pen
devices realizable in the near future.
7 Acknowledgments
This work is supported by the 7th Frame-
work Program of European Commission un-
der grant agreements 287576 (CasMaCat) and
600707 (tranScriptorium).
References
V. Alabau, A. Sanchis, and F. Casacuberta. 2014.
Improving on-line handwritten recognition in in-
teractive machine translation. Pattern Recogni-
tion, 47(3):1217?1228.
2007. AMA manual of style: A guide for authors
and editors. 10th ed. Oxford University Press.
L. Anthony and J. O. Wobbrock. 2012. $N-
protractor: a fast and accurate multistroke rec-
ognizer. In Proc. GI, pages 117?120.
2005. BS 5261-2:2005. Copy preparation and proof
correction.
2010. The Chicago manual of style. 16th ed. Uni-
versity Of Chicago Press.
M. L. Coleman. 1969. Text editing on a graphic
display device using hand-drawn proofreader?s
symbols. In Pertinent Concepts in Computer
Graphics, Proc. 2nd Univ. Illinois Conf. on
Computer Graphics, pages 283?290.
1983. ISO 5776:1983. Symbols for text correction.
L. A. Leiva, V. Alabau, and E. Vidal. 2013. Error-
proof, high-performance, and context-aware ges-
tures for interactive text edition. In Proc. CHI
EA, pages 1227?1232.
Y. Li. 2010. Protractor: a fast and accurate ges-
ture recognizer. In Proc. CHI, pages 2169?2172.
C. S. Myers and L. R. Rabiner. 1981. A compar-
ative study of several dynamic time-warping al-
gorithms for connected-word. Bell System Tech-
nical Journal.
14
V. Romero, L. A. Leiva, A. H. Toselli, and E. Vidal.
2009. Interactive multimodal transcription of
text images using a web-based demo system. In
Proc. IUI, pages 477?478.
D. Rubine. 1991. Specifying gestures by example.
In Proc. SIGGRAPH, pages 329?337.
T. M. Sezgin and R. Davis. 2005. HMM-based
efficient sketch recognition. In Proc. IUI, pages
281?283.
M. Shilman, D. S. Tan, and P. Simard. 2006.
CueTIP: a mixed-initiative interface for correct-
ing handwriting errors. In Proc. UIST, pages
323?332.
R. D. Vatavu, L. Anthony, and J. O. Wobbrock.
2012. Gestures as point clouds: A $P recognizer
for user interface prototypes. In Proc. ICMI,
pages 273?280.
J. O. Wobbrock, A. D. Wilson, and Y. Li. 2007.
Gestures without libraries, toolkits or training:
A $1 recognizer for user interface prototypes. In
Proc. UIST, pages 159?168.
F.
?
Alvaro, J.-A. Sa?nchez, and J.-M. Bened??. 2013.
Classification of on-line mathematical symbols
with hybrid features and recurrent neural net-
works. In Proc. ICDAR, pages 1012?1016.
15
