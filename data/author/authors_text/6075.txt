Stochastic Language Generation for Spoken Dialogue Systems 
Alice H. Oh 
Carnegie Mellon University 
5000 Forbes Ave. 
Pittsburgh, PA 15213 
aliceo+@cs.cmu.edu 
Alexander I. Rudnicky 
Carnegie Mellon University 
5000 Forbes Ave. 
Pittsburgh, PA 15213 
air+@cs.cmu.edu 
Abstract 
The two current approaches to language 
generation, Template-based and rule-based 
(linguistic) NLG, have limitations when 
applied to spoken dialogue systems, in part 
because they were developed for text 
generation. In this paper, we propose a new 
corpus-based approach to natural anguage 
generation, specifically designed for spoken 
dialogue systems. 
Introduct ion 
Several general-purpose rule-based generation 
systems have been developed, some of which 
are available publicly (cf. Elhadad, 1992). 
Unfortunately these systems, because of their 
generality, can be difficult to adapt to small, 
task-oriented applications. Bateman and 
Henschel (1999) have described a lower cost and 
more efficient generation system for a specific 
application using an automatically customized 
subgrammar. Busemann and Horacek (1998) 
describe a system that mixes templates and rule- 
based generation. This approach takes 
advantages of templates and rule-based 
generation as needed by specific sentences or 
utterances. Stent (1999) has proposed a similar 
approach for a spoken dialogue system. 
However, there is still the burden of writing and 
maintaining rammar rules, and processing time 
is probably too slow for sentences using 
grammar rules (only the average time for 
templates and rule-based sentences combined is 
reported in Busemann and Horacek, 1998), for 
use in spoken dialogue systems. 
Because comparatively less effort is needed, 
many current dialogue systems use template- 
based generation. But there is one obvious 
disadvantage: the quality of the output depends 
entirely on the set of templates. Even in a 
relatively simple domain, such as travel 
reservations, the number of templates necessary 
for reasonable quality can become quite large 
that maintenance becomes a serious problem. 
There is an unavoidfible trade-off between the 
amount of time and effort in creating and 
maintaining templates and the variety and 
quality of the output utterances. 
Given these shortcomings of the above 
approaches, we developed a corpus-based 
generation system, in which we model anguage 
spoken by domain experts performing the task of 
interest, and use that model to stochastically 
generate system utterances. We have applied this 
technique to sentence realization and content 
planning, and have incorporated the resulting 
generation component into a working natural 
dialogue system (see Figure 1). In this paper, we 
describe the technique and report the results of 
two evaluations. 
We used two corpora in the travel 
reservations domain to build n-gram language 
models. One corpus (henceforth, the CMU 
corpus) consists of 39 dialogues between a travel 
agent and clients (Eskenazi, et al 1999). 
Surface 
Real izat ion 
Content  
Planning 
Sentence 
Planning 
Dialogue Manager Generation Engine 
Figure 1 : Overall Architecture 
27 
query_arrive_city 
query_arrive_time 
query_confirm 
query_depart_date 
query_depart_time 
query_pay_by_card 
query_preferred_airport 
query_returndate 
query_return_time 
hotel car info 
hotel_hotel_chain 
hotel_hotel_info 
inform_airport 
inform_confirm_utterance 
inform_flight 
inform_flight_another 
inform_flight_earlier 
n form_flight_earliest 
inform_flight_later 
inform_flight_latest 
inform_not_avail 
inform_num_flights 
inform_price 
other 
Figure 2 : utterance classes 
airline 
arriveairport 
arriveSci:ty - 
arrive_date - 
arrive_time 
car company 
car price 
depart airport 
depart city 
depart_date 
depart_time 
flight_hum 
hotel_city 
hotel_price 
name 
num_flights 
pm 
price 
Figure 3 : word classes 
Another corpus (henceforth, the SRI corpus) 
consists of 68 dialogues between a travel agent 
and users in the SRI community (Kowtko and 
Price 1989). 
The utterances in the two corpora were 
tagged with utterance classes and word classes 
(see Figure 2 and Figure 3). The CMU corpus 
was manually tagged, and back-off trigram 
models built (using Clarkson and Rosenfeld, 
1997). These language models were used to 
automatically tag the SRI corpus; the tags were 
manually checked. 
1 Content P lanning 
In content planning we decide which attributes 
(represented as word classes, see Figure 3) 
should be included in an utterance. In a task- 
oriented dialogue, the number of attributes 
generally increases during the course of the 
dialogue. Therefore, as the dialogue progresses, 
we need to decide which ones to include at each 
system turn. If we include all of them every time 
(indirect echoing, see Hayes and Reddy, 1983), 
the utterances become overly lengthy, but if we 
remove all unnecessary attributes, the user may 
get confused. With a fairly high recognition 
error rate, this becomes an even more important 
issue. 
The problem, then, is to find a compromise 
between the two. We compared two ways to 
systematically generate system utterances with 
only selected attributes, such that the user hears 
repetition of some of the constraints he/she has 
specified, at appropriate points in the dialogue, 
without sacrificing naturalness and efficiency. 
The specific problems, then, are deciding what 
should be repeated, and when. We first describe 
a simple heuristic of old versus new information. 
Then we present a statistical approach, based on 
bigram models. 
1.1 First  approach:  old versus new 
As a simple solution, we can use the previous 
dialogue history, by tagging the attribute-value 
pairs as old (previously said by the system) 
information or new (not said by the system yet) 
information. The generation module would 
select only new information to be included in the 
system utterances. Consequently, information 
? given by the user is repeated only once in the 
dialogue, usually in the utterance immediately 
following the user utterance in which the new 
information was given 1. 
Although this approach seems to work fairly 
well, echoing user's constraints only once may 
not be the right thing to do. Looking at human- 
human dialogues, we observe that this is not 
very natural for a conversation; humans often 
repeat mutually known information, and they 
also often do not repeat some information at all. 
Also, this model does not capture the close 
relationship between two consecutive utterances 
within a dialogue. The second approach tries to 
address these issues. 
1.2 Second approach:  statist ical  mode l  
For this approach, we adopt the first of the two 
sub-maxims in (Oberlander, 1998) '?'Do the 
human thing". Oberlander (1998) talks about 
generation of referring expressions, but it is 
universally valid, at least within natural 
language generation, to say the best we can do is 
When the system utterance uses a template that does 
not contain the slots for the new information given in 
the previous user utterance, then that new 
information will be confirmed in the next available 
system utterance in which the template contains those 
slots. 
28 
to mimic human behavior. Hence, we built a 
two-stage statistical model of human-human 
dialogues using the CMU corpus. The model 
first predicts the number of attributes in the 
system utterance given the utterance class, then 
predicts the attributes given the attributes in the 
previous user utterance. 
1.2.1 The number of attributes model 
The first model will predict the number of 
attributes in a system utterance given the 
utterance class. The model is the probability 
distribution P(nk) = P(nklck), where nk is the 
number of attributes and Ck is the utterance class 
for system utte~anee k. 
1.2.2 The bigram model of the attributes 
This model will predict which attributes to use 
in a system utterance. Using a statistical model, 
what we need to do is find the set of attributes 
A* = {al, az . . . . .  an } such that 
A * = arg max F I  P(al, a2 ..... an) 
We assume that the distributions of the ai's 
are dependent on the attributes in the previous 
utterances. As a simple model, we look only at 
the utterance immediately preceding the current 
utterance and build a bigram model of the 
attributes. In other words, A* = arg max P(AIB), 
where B = {b l ,  b2 . . . . .  b in},  the set of m 
attributes in the preceding user utterance. 
If we took the above model and tried to 
apply it directly, we would run into a serious 
data sparseness problem, so we make two 
independence assumptions. The first assumption 
is that the attributes in the user utterance 
contribute independently to the probabilities of 
the attributes in the system utterance following 
it. Applying this assumption to the model above, 
we get the following: 
m 
A * = arg max ~ P(bk)P(A I bk) 
k=l 
The second independence assumption is that 
the attributes in the system utterance are 
independent of each other. This gives the final 
model that we used for selecting the attributes. 
m tl 
A*. = arg max ~ P(bk ) \ [ ' I  P(al I bk) 
k=l i=1 
29 
Although this independence assumption is 
an oversimplification, this simple model is a 
good starting point for our initial 
implementation f this approach. 
2 Stochastic Surface Realization 
We follow Busemann and Horacek (1998) in 
designing our generation engine with "different 
levels of granularity." The different levels 
contribute to the specific needs of the various 
utterance classes. For example, at the beginning 
of the dialogue, a system greeting can be simply 
generated by a "canned" expression. Other short, 
simple utterances can be generated efficiently by 
templates. In Busemann and Horacek (1998), the 
remaining output is generated by grammar rules. 
We replace the gefieration grammar with a 
simple statistical anguage model to generate 
more complex utterances. 
There are four aspects to our stochastic 
surface realizer: building language models, 
generating candidate utterances, scoring the 
utterances, and filling in the slots. We explain 
each of these below. 
2.1 Building Language Models 
Using the tagged utterances as described in 
the introduction, we built an unsmoothed n-gram 
language model for each utterance class. Tokens 
that belong in word classes (e.g., "U.S. 
Airways" in class "airline") were replaced by the 
word classes before building the language 
models. We selected 5 as the n in n-gram to 
introduce some variability in the output 
utterances while preventing nonsense utterances. 
Note that language models are not used here 
in the same way as in speech recognition. In 
speech recognition, the language model 
probability acts as a 'prior' in determining the 
most probable sequence of words given the 
acoustics. In other words, 
W* = arg max P(WIA) 
= arg max P(AI W)Pr(W) 
where W is the string of words, wl, ..., wn, and 
A is the acoustic evidence (Jelinek 1998). 
Although we use the same statistical tool, 
we compute and use the language model 
probability directly to predict he next word. In 
other words, the most likely utterance is W* = 
arg max P(WIu), where u is the utterance class. 
We do not, however, look for the most likely 
hypothesis, but rather generate each word 
randomly according to the distribution, as 
illustrated in the next section. 
2.2 Generat ing  Ut terances  
The input to NLG from the dialogue 
manager is a frame of attribute-value pairs. The 
first two attribute-value pairs specify the 
utterance class. The rest of the frame contains 
word classes and their values. Figure 4 is an 
example of an input frame to NLG. 
- act-query 
content depart_time 
depart_city New York 
arrive_city San Francisco 
depart_date 19991117 
} 
Figure 4 : an input frame to NLG 
The generation engine uses the appropriate 
language model for the utterance class and 
generates word sequences randomly according 
to the language model distributions. As in 
speech recognition, the probability of a word 
using the n-gram language model is 
P(wi) = P(wilwi.1, wi.2 . . . .  Wi.(n.1) , U) 
where u is the utterance class. Since we have 
built separate models for each of the utterance 
classes, we can ignore u, and say that 
P(wi) = P(wilw|.l, wi-2 . . . .  Wi.(n.1)) 
using the language model for u. 
Since we use unsmoothed 5,grams, we will 
not generate any unseen 5-grams (or smaller n- 
grams at the beginning and end of an utterance). 
This precludes generation of nonsense 
utterances, at least within the 5-word window. 
Using a smoothed n-gram would result in more 
randomness, but using the conventional back-off 
methods (Jelinek 1998), the probability mass 
assigned to unseen 5-grams would be very 
small, and those rare occurrences of unseen n- 
grams may not make sense anyway. There is the 
problem, as in speech recognition using n-gram 
language models, that long-distance dependency 
cannot be captured. 
= 
2.3 Scoring Utterances 
For each randomly generated utterance, we 
compute a penalty score. The score is based on 
the heuristics we've empirically selected. 
Various penalty scores are assigned for an 
utterance that 1. is too short or too long 
(determined by utterance-class dependent 
thresholds), 2. contains repetitions of any of the 
slots, 3. contains lots for which there is no valid 
value in the frame, or 4. does not have some 
required slots (see section 2 for deciding which 
slots are required). 
The generation engine generates a candidate 
utterance, scores it, keeping only the best-scored 
utterance up to that point. It stops and returns the 
best utterance when it finds an utterance with a 
zero penalty scoreTor uns out of time. 
2.4 Fil l ing Slots 
The last step is filling slots with the appropriate 
values. For example, the utterance "What time 
would you like to leave {depart_city}?" 
becomes "What time would you like to leave 
New York?". 
3 Evaluation 
It is generally difficult to empirically evaluate a
generation system. In the context of spoken 
dialogue systems, evaluation of NLG becomes 
an even more difficult problem. One reason is 
simply that there has been very little effort in 
building generation engines for spoken dialogue 
systems. Another reason is that it is hard to 
separate NLG from the rest of the system. It is 
especially hard to separate evaluation of 
language generation and speech synthesis. 
As a simple solution, we have conducted a
comparative evaluation by running two identical 
systems varying only the generation component. 
In this section we present results from two 
preliminary evaluations of our generation 
algorithms described in the previous ections. 
3.1 Content Planning: Experiment 
For the content planning part of the generation 
-system, we conducted a comparative evaluation 
of the two different generation algorithms: 
old/new and bigrams. Twelve subjects had two 
dialogues each, one with the old/new generation 
system, and another with the bigrams generation 
30 
system (in counterbalanced order); all other 
modules were held fixed. Afterwards, each 
subject answered seven questions on a usability 
survey. Immediately after, each subject was 
given transcribed logs of his/her dialogues and 
asked to rate each system utterance on a scale of 
1 to 3 (1 = good; 2 = okay; 3 = bad). 
3.2 Content Planning: Results 
For the usability survey, the results seem to 
indicate subjects' preference for the old/new 
system, but the difference is not statistically 
significant (p - 0.06). However, six out of the 
twelve subjects chose the bigram system to the 
question "Durqng-the session, which system's 
responses were easier to understand?" compared 
to three subjects choosing the old/new system. 
3.3 Surface Realization: Experiment 
For surface realization, we conducted a batch- 
mode evaluation. We picked six recent calls to 
our system and ran two generation algorithms 
(template-based generation and stochastic 
generation) on the input frames. We then 
presented to seven subjects the generated 
dialogues, consisting of decoder output of the 
user utterances and corresponding system 
responses, for each of the two generation 
algorithms. Subjects then selected the output 
utterance they would prefer, for each of the 
utterances that differ between the two systems. 
The results show a trend that subjects preferred 
stochastic generation over template-based 
generation, but a t-test shows no significant 
difference (p = 0.18). We are in the process of 
designing a larger evaluation. 
4 Conclusion 
We have presented a new approach to language 
generation for spoken dialogue systems. For 
content planning, we built a simple bigram 
model of attributes, and found that, in our first 
implementation, it performs as well as a 
heuristic of old vs. new information. For surface 
realization, we used an n-gram language model 
to stochastically generate ach utterance and 
found that the stochastic system performs at 
least as well as the template-based system. 
Our stochastic generation system has several 
advantages. One of those, an important issue for 
spoken dialogue systems, is the response time. 
With stochastic surface realization, the average 
generation time for the longest utterance class 
(10 - 20 words long) is about 200 milliseconds, 
which is much faster than any rule-based 
systems. Another advantage is that by using a 
corpus-based approach, we are directly 
mimicking the language of a real domain expert, 
rather than attempting to model it by rule. 
Corpus collection is usually the first step in 
building a dialogue system, so we are leveraging 
the effort rather than creating more work. This 
also means adapting this approach to new 
domains and even new languages will be 
relatively simple. 
The approach we present does require some 
amount of knowledge ngineering, though this 
appears to overlap with work needed for other 
parts of the dialogue system. First, defining the 
class of utterance and the attribute-value pairs 
requires care. Second, tagging the human-human 
corpus with the right classes and attributes 
requires effort. However, we believe the tagging 
effort is much less difficult than knowledge 
acquisition for most rule-based systems or even 
template-based systems. Finally, what may 
sound right for a human speaker may sound 
awkward for a computer, but we believe that 
mimicking a human, especially a domain expert, 
is the best we can do, at least for now. 
Acknowledgements 
We are thankful for significant contribution by 
other members of the CMU Communicator 
Project, especially Eric Thayer, Wei Xu, and 
Rande Shern. We would like to thank the 
subjects who participated in our evaluations. We 
also extend our thanks to two anonymous 
reviewers. 
References 
Bateman, J. and Henschel, R. (1999) From full 
generation to 'near-templates' without losing 
generality. In Proceedings of the KI'99 workshop, 
"May I Speak Freely?" 
Busemann, S. and Horacek, H. (1998) A flexible 
shallow approach to text generation. In 
Proceedings of the International Natural Language 
Generation Workshop. Niagara-on-the-Lake, 
Canada.. 
31 
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1986?1996,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Self-disclosure topic model for classifying and analyzing Twitter
conversations
JinYeong Bak
?
Department of Computer Science
KAIST
Daejeon, South Korea
jy.bak@kaist.ac.kr
Chin-Yew Lin
Microsoft Research
Beijing 100080, P.R. China
cyl@microsoft.com
Alice Oh
Department of Computer Science
KAIST
Daejeon, South Korea
alice.oh@kaist.edu
Abstract
Self-disclosure, the act of revealing one-
self to others, is an important social be-
havior that strengthens interpersonal rela-
tionships and increases social support. Al-
though there are many social science stud-
ies of self-disclosure, they are based on
manual coding of small datasets and ques-
tionnaires. We conduct a computational
analysis of self-disclosure with a large
dataset of naturally-occurring conversa-
tions, a semi-supervised machine learning
algorithm, and a computational analysis
of the effects of self-disclosure on subse-
quent conversations. We use a longitu-
dinal dataset of 17 million tweets, all of
which occurred in conversations that con-
sist of five or more tweets directly reply-
ing to the previous tweet, and from dyads
with twenty of more conversations each.
We develop self-disclosure topic model
(SDTM), a variant of latent Dirichlet al-
location (LDA) for automatically classi-
fying the level of self-disclosure for each
tweet. We take the results of SDTM and
analyze the effects of self-disclosure on
subsequent conversations. Our model sig-
nificantly outperforms several comparable
methods on classifying the level of self-
disclosure, and the analysis of the longitu-
dinal data using SDTM uncovers signifi-
cant and positive correlation between self-
disclosure and conversation frequency and
length.
1 Introduction
Self-disclosure is an important and pervasive so-
cial behavior. People disclose personal informa-
tion about themselves to improve and maintain
?
This work was done when JinYeong Bak was a visiting
student at Microsoft Research, Beijing, China.
relationships (Jourard, 1971; Joinson and Paine,
2007). A common instance of self-disclosure is
the start of a conversation with an exchange of
names and additional self-introductions. Another
example of self-disclosure, shown in Figure 1c,
where the information disclosed about a family
member?s serious illness, is much more personal
than the exchange of names. In this paper, we seek
to understand this important social behavior using
a large-scale Twitter conversation data, automati-
cally classifying the level of self-disclosure using
machine learning and correlating the patterns with
conversational behaviors which can serve as prox-
ies for measuring intimacy between two conversa-
tional partners.
Twitter conversation data, explained in more
detail in section 4.1, enable an extremely large
scale study of naturally-occurring self-disclosure
behavior, compared to traditional social science
studies. One challenge of such large scale study,
though, remains in the lack of labeled ground-
truth data of self-disclosure level. That is,
naturally-occurring Twitter conversations do not
come tagged with the level of self-disclosure in
each conversation. To overcome that challenge,
we propose a semi-supervised machine learning
approach using probabilistic topic modeling. Our
self-disclosure topic model (SDTM) assumes that
self-disclosure behavior can be modeled using a
combination of simple linguistic features (e.g.,
pronouns) with automatically discovered seman-
tic themes (i.e., topics). For instance, an utterance
?I am finally through with this disastrous relation-
ship? uses a first-person pronoun and contains a
topic about personal relationships.
In comparison with various other models,
SDTM shows the highest accuracy, and the result-
ing conversation frequency and length patterns on
self-disclosure are shown different over time. Our
contributions to the research community include
the following:
1986
? We present key features and prior knowl-
edge for identifying self-disclosure level, and
show relevance of it with experiment results
(Sec. 2).
? We present a topic model that explicitly in-
cludes the level of self-disclosure in a conver-
sation using linguistic features and the latent
semantic topics (Sec. 3).
? We collect a large dataset of Twitter conver-
sations over three years and annotate a small
subset with self-disclosure level (Sec. 4).
? We compare the classification accuracy of
SDTM with other models and show that it
performs the best (Sec. 5).
? We correlate the self-disclosure patterns and
conversation behaviors to show that there is
significant relationship over time (Sec. 6).
2 Self-Disclosure
In this section, we look at social science literature
for definition of the levels of self-disclosure. Us-
ing that definition, we devise an approach to au-
tomatically identify the levels of self-disclosure
in a large corpus of OSN conversations. We dis-
cuss three approaches, first, using first-person pro-
noun features, and second, extracting seed words
and phrases from the Twitter conversation cor-
pus, and third, extracting seed words and phrases
from an external corpus of anonymously posted
secrets, and we demonstrate the efficacy of those
approaches with an annotated corpus.
2.1 Self-disclosure (SD) level
To analyze self-disclosure, researchers categorize
self-disclosure language into three levels: G (gen-
eral) for no disclosure, M for medium disclosure,
and H for high disclosure (Vondracek and Von-
dracek, 1971; Barak and Gluck-Ofri, 2007). Ut-
terances that contain general (non-sensitive) in-
formation about the self or someone close (e.g.,
a family member) are categorized as M. Exam-
ples are personal events, past history, or future
plans. Utterances about age, occupation and hob-
bies are also included. Utterances that contain
sensitive information about the self or someone
close are categorized as H. Sensitive information
includes personal characteristics, problematic be-
haviors, physical appearance and wishful ideas.
Generally, these are thoughts and information that
(a) A G level Twitter conversation
(b) A M level Twitter conversation
(c) A H level Twitter conversation
Figure 1: An example of a Twitter conversation
(from annotated dataset) with G, M and H level of
self-disclosure.
one would keep as secrets to himself. All other
utterances, those that do not contain information
about the self or someone close are categorized
as G. Examples include gossip about celebrities
or factual discourse about current events. Figure
1 shows Twitter conversation examples with G,
M and H levels from annotated dataset (see Sec-
tion 4.2 for a detailed description of the annotated
dataset).
2.2 G Level of Self-Disclosure
An obvious clue of self-disclosure is the use of
first-person pronouns. For example, phrases such
as ?I live? or ?My name is? indicate that the ut-
terance contains personal information. In pre-
vious research, the simple method of counting
first-person pronouns was used to measure the de-
gree of self-disclosure (Joinson, 2001; Barak and
Gluck-Ofri, 2007). Consequently, the absence of a
first-person pronoun signals that the utterance be-
longs in the G level of self-disclosure. We ver-
ify this pattern with a dataset of Tweets annotated
with G, M, and H levels. We divide the annotated
Tweets into two classes, G and M/H. Then we com-
pute mutual information of each unigram, bigram,
or trigram feature to see which features are most
discriminative. As Table 1 shows, 18 out of 30
1987
Category Words/Expressions
Unigram my, I, I?m, I?ll, but, was, I?ve, love, dad, have
Bigram I love, I was, I have, my dad, go to, my mom,
with my, have to, to go, my mum
Trigram I have a, is going to, to go to, want to go, and I
was, going to miss, I love him, I think I, I was
like, I wish I
Table 1: High ranked words and expressions by
mutual information between G and M/H level in
annotated conversations.
most highly ranked discriminative features contain
a first-person pronoun.
2.3 M Level of Self-Disclosure
Utterances with M level include two types: 1)
information related with past events and future
plans, and 2) general information about self
(Barak and Gluck-Ofri, 2007). For the former, we
add as seed trigrams ?I have been? and ?I will?.
For the latter, we use seven types of information
generally accepted to be personally identifiable in-
formation (McCallister, 2010), as listed in the left
column of Table 2. To find the appropriate tri-
grams for those, we take Twitter conversation data
(described in Section 4.1) and look for trigrams
that begin with ?I? and ?my? and occur more than
200 times. We then check each one to see whether
it is related with any of the seven types listed in
the table. As a result, we find 57 seed trigrams for
M level. Table 2 shows several examples.
Type Trigram
Name My name is, My last name
Birthday My birthday is, My birthday party
Location I live in, I lived in, I live on
Contact My email address, My phone number
Occupation My job is, My new job
Education My high school, My college is
Family My dad is, My mom is, My family is
Table 2: Example seed trigrams for identifying M
level of SD. There are 51 of these used in SDTM.
2.4 H Level of Self-Disclosure
Utterances with H level express secretive wishes
or sensitive information that exposes self or some-
one close (Barak and Gluck-Ofri, 2007). These
are generally kept as secrets. With this intuition,
we crawled 26,523 posts from Six Billion Secrets
1
site where users post secrets anonymously
2
. We
1
http://www.sixbillionsecrets.com
2
This site is regularly monitored for spam.
Category Words - SECRET Words - Annotated
physical
appear-
ance
acne, hair, overweight,
stomach, chest, hand,
scar, thighs, chubby
ankle, face, toe,
skin
mental/
physical
condition
addicted, bulimia, doc-
tor, illness, alcoholic,
disease, drugs, pills
ache, epilepsy,
pain, chiropractor,
codeine
Table 3: Example words for identifying H level of
SD from secret posts (2nd column) and annotated
data (3rd column). Categories are hand-labeled.
call this external dataset SECRET. Unlike G and M
levels, evidence of H level of self-disclosure tends
to be topical, such as physical appearance, mental
and physical illnesses, and family problems, so we
take an approach of fitting a topic model driven by
seed words. A similar approach has been success-
ful in sentiment classification (Jo and Oh, 2011;
Kim et al., 2013).
A critical component of this approach is the set
of seed words with which to drive the discovery
of topics that are most indicative of H level self-
disclosure. To extract the seed words that express
secretive personal information, we compute mu-
tual information (Manning et al., 2008) with SE-
CRET and 24,610 randomly selected tweets. We
select 1,000 words with high mutual information
and filter out stop words. Table 3 shows some of
these words. To extract seed trigrams of secretive
wishes, we again look for trigrams that start with
?I? or ?my?, occur more than 200 times, and select
trigrams of wishful thinking, such as ?I want to?,
and ?I wish I?. In total, there are 88 seed words
and 8 seed trigrams for H.
Since SECRET is quite different from Twitter,
we must show that posts in SECRET are seman-
tically similar to the H level Tweets. Rather than
directly comparing SECRET posts and Tweets, we
use the same method of extracting discriminative
word features from the annotated H level Tweets
(see Section 4.2). Table 3 shows the seed words
extracted from SECRET as well as the annotated
Tweets. Because the annotated dataset consists of
only 200 conversations, the coverage of the topics
seems narrower than the much larger SECRETS,
but both datasets show similarities in the topics.
This, combined with the results of the model with
the two sets of seed words (see Section 5 for the
results), shows that SECRETS is an effective and
simple-to-obtain substitute for an annotated cor-
pus of H level of self-disclosure.
1988
??
??
??
CTN
??
?
?
?? 3?????? 3
Figure 2: Graphical model of SDTM
Notation Description
G; M ; H {general; medium; high} SD level
C; T ; N Number of conversations; tweets;
words
K
G
;K
M
;K
H
Number of topics for {G; M; H}
c; ct Conversation; tweet in conversation c
y
ct
SD level of tweet ct, G or M/H
r
ct
SD level of tweet ct, M or H
z
ct
Topic of tweet ct
w
ctn
n
th
word in tweet ct
? Learned Maximum entropy parameters
x
ct
First-person pronouns features
?
ct
Distribution over SD level of tweet ct
pi
c
SD level proportion of conversation c
?G
c
;?M
c
;?H
c
Topic proportion of {G; M; H} in con-
versation c
?G;?M ;?H Word distribution of {G; M; H}
?; ? Dirichlet prior for ?; pi
?G,?M ;?H Dirichlet prior for ?G;?M ;?H
n
cl
Number of tweets assigned SD level l
in conversation c
n
l
ck
Number of tweets assigned SD level l
and topic k in conversation c
n
l
kv
Number of instances of word v assigned
SD level l and topic k
m
ctkv
Number of instances of word v assigned
topic k in tweet ct
Table 4: Summary of notations used in SDTM
3 Self-Disclosure Topic Model
This section describes our model, the self-
disclosure topic model (SDTM), for classifying
self-disclosure level and discovering topics for
each self-disclosure level.
3.1 Model
In section 2, we discussed different approaches
to identifying each level of self-disclosure, based
on social science literature, annotated and unan-
notated Tweets, and an external corpus of se-
cret posts. In this section, we describe our
self-disclosure topic model, based on the widely
used latent Dirichlet allocation (Blei et al., 2003),
which incorporates those approaches.
Figure 2 illustrates the graphical model of
1. For each level l ? {G, M, H}:
For each topic k ? {1, . . . ,Kl}:
Draw ?lk ? Dir(?l)
2. For each conversation c ? {1, . . . , C}:
(a) Draw ?Gc ? Dir(?)
(b) Draw ?Mc ? Dir(?)
(c) Draw ?Hc ? Dir(?)
(d) Draw pic ? Dir(?)
(e) For each message t ? {1, . . . , T}:
i. Observe first-person pronouns features xct
ii. Draw ?ct ?MaxEnt(xct,?)
iii. Draw yct ? Bernoulli(?ct)
iv. If yct = 0 which is G level:
A. Draw zct ?Mult(?Gc )
B. For each word n ? {1, . . . , N}:
Draw word wctn ?Mult(?Gzct)
Else which can be M or H level:
A. Draw rct ?Mult(pic)
B. Draw zct ?Mult(?rctc )
C. For each word n ? {1, . . . , N}:
Draw word wctn ?Mult(?rctzct)
Figure 3: Generative process of SDTM.
SDTM and how those approaches are embodied
in it. The first approach based on the first-person
pronouns is implemented by the observed vari-
able x
ct
and the parameters ? from a maximum
entropy classifier for G vs. M/H level. The ap-
proach of seed words and phrases for levels M and
H is implemented by the three separate word-topic
probability vectors for the three levels of SD: ?
l
which has a Bayesian informative prior ?
l
where
l ? {G,M,H}, the three levels of self-disclosure.
Table 4 lists the notations used in the model and
the generative process, and Figure 3 describes the
generative process.
3.2 Classifying G vs M/H levels
Classifying the SD level for each tweet is done in
two parts, and the first part classifies G vs. M/H
levels with first-person pronouns (I, my, me). In
the graphical model, y is the latent variable that
represents this classification, and ? is the distri-
bution over y. x is the observation of the first-
person pronoun in the tweets, and? are the param-
eters learned from the maximum entropy classifier.
With the annotated Twitter conversation dataset
(described in Section 4.2), we experimented with
several classifiers (Decision tree, Naive Bayes)
and chose the maximum entropy classifier because
it performed the best, similar to other joint topic
models (Zhao et al., 2010; Mukherjee et al., 2013).
1989
3.3 Classifying M vs H levels
The second part of the classification, the M and the
H level, is driven by informative priors with seed
words and seed trigrams. In the graphical model,
r is the latent variable that represents this classi-
fication, and pi is the distribution over r. ? is a
non-informative prior for pi, and ?
l
is an informa-
tive prior for each SD level by seed words. For
example, we assign a high value for the seed word
?acne? for ?
H
, and a low value for ?My name is?.
This approach is the same as joint models of topic
and sentiment (Jo and Oh, 2011; Kim et al., 2013).
3.4 Inference
For posterior inference of SDTM, we use col-
lapsed Gibbs sampling which integrates out la-
tent random variables ?,pi,?, and ?. Then we
only need to compute y, r and z for each tweet.
We compute full conditional distribution p(y
ct
=
j
?
, r
ct
= l
?
, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x) for
tweet ct as follows:
p(y
ct
= 0, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x)
?
exp(?
0
? x
ct
)
?
1
j=0
exp(?
j
? x
ct
)
g(c, t, l
?
, k
?
),
p(y
ct
= 1, r
ct
= l
?
, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x)
?
exp(?
1
? x
ct
)
?
1
j=0
exp(?
j
? x
ct
)
(?
l
?
+ n
(?ct)
cl
?
) g(c, t, l
?
, k
?
),
where z
?ct
, r
?ct
,y
?ct
are z, r,y without tweet
ct, m
ctk
?
(?)
is the marginalized sum over word v of
m
ctk
?
v
and the function g(c, t, l
?
, k
?
) as follows:
g(c, t, l
?
, k
?
) =
?(
?
V
v=1
?
l
?
v
+ n
l
?
?(ct)
k
?
v
)
?(
?
V
v=1
?
l
?
v
+ n
l
?
?(ct)
k
?
v
+m
ctk
?
(?)
)
(
?
k
?
+ n
l
?
(?ct)
ck
?
?
K
k=1
?
k
+ n
l
?
ck
)
V
?
v=1
?(?
l
?
v
+ n
l
?
?(ct)
k
?
v
+m
ctk
?
v
)
?(?
l
?
v
+ n
l
?
?(ct)
k
?
v
)
.
4 Data Collection and Annotation
To test our self-disclosure topic model, we use a
large dataset of conversations consisting of Tweets
over three years such that we can analyze the re-
lationship between self-disclosure behavior and
conversation frequency and length over time. We
chose to crawl Twitter because it offers a prac-
tical and large source of conversations (Ritter et
al., 2010). Others have also analyzed Twitter con-
versations for natural language and social media
Users Dyads Conv?s Tweets
101,686 61,451 1,956,993 17,178,638
Table 5: Dataset of Twitter conversations. We
chose conversations consisting of five or more
tweets each. We chose dyads with twenty or more
conversations.
research (boyd et al., 2010; Danescu-Niculescu-
Mizil et al., 2011), but we collect conversations
from the same set of dyads over several months for
a unique longitudinal dataset. We also make sure
that each conversation is at least five tweets, and
that each dyad has at least twenty conversations.
4.1 Collecting Twitter conversations
We define a Twitter conversation as a chain of
tweets where two users are consecutively reply-
ing to each other?s tweets using the Twitter reply
button. We initialize the set of users by randomly
sampling thirteen users who reply to other users
in English from the Twitter public streams
3
. Then
we crawl each user?s public tweets, and look at
users who are mentioned in those tweets. It is
a breadth-first search in the network defined by
users as nodes and edges as conversations. We
run this search for dyads until the depth of four,
and filter out users who tweet in a non-English
language. We use an open source tool for de-
tecting English tweets
4
. To protect users? privacy,
we replace Twitter userid, usernames and url in
tweets with random strings. This dataset consists
of 101,686 users, 61,451 dyads, 1,956,993 conver-
sations and 17,178,638 tweets which were posted
between August 2007 to July 2013. Table 5 sum-
marizes the dataset.
4.2 Annotating self-disclosure level
To measure the accuracy of our model, we ran-
domly sample 301 conversations, each with ten or
fewer tweets, and ask three judges, fluent in En-
glish and graduate students/researchers, to anno-
tate each tweet with the level of self-disclosure.
Judges first read and discussed the definitions and
examples of self-disclosure level shown in (Barak
and Gluck-Ofri, 2007), then they worked sepa-
rately on a Web-based platform.
As a result of annotation, there are 122 G level
converstaions, 147 M level and 32 H level con-
3
https://dev.twitter.com/docs/api/
streaming
4
https://github.com/shuyo/ldig
1990
Figure 4: Screenshot of annotation web-based
platform. Annotators read a Twitter conversation
and annotate self-disclosure level to each tweet.
versations, and inter-rater agreement using Fleiss
kappa (Fleiss, 1971) is 0.68, which is substantial
agreement result (Landis and Koch, 1977).
5 Classification of Self-Disclosure Level
This section describes experiments and results of
SDTM as well as several other methods for classi-
fication of self-disclosure level.
We first start with the annotated dataset in sec-
tion 4.2 in which each tweet is annotated with SD
level. We then aggregate all of the tweets of a
conversation, and we compute the proportions of
tweets in each SD level. When the proportion of
tweets at M or H level is equal to or greater than
0.2, we take the level of the larger proportion and
assign that level to the conversation. When the
proportions of tweets at M or H level are both less
than 0.2, we assign G to the SD level. The reason
for setting 0.2 as the threshold is that a conversa-
tion containing tweets with H or M level of self-
disclosure usually starts with a greeting or a gen-
eral comment, and contains one or more questions
or comments before or after the self-disclosure
tweet.
We compare SDTM with the following methods
for classifying conversations for SD level:
? LDA (Blei et al., 2003): A Bayesian topic
model. Each conversation is treated as a doc-
ument. Used in previous work (Bak et al.,
2012).
? MedLDA (Zhu et al., 2012): A super-
vised topic model for document classifica-
tion. Each conversation is treated as a doc-
ument and response variable can be mapped
to a SD level.
? LIWC (Tausczik and Pennebaker, 2010):
Word counts of particular categories
5
. Used
in previous work (Houghton and Joinson,
2012).
? Bag of Words + Bigrams + Trigrams
(BOW+): A bag of words, bigram and tri-
gram features. We exclude features that ap-
pear only once or twice.
? Seed words and trigrams (SEED): Occur-
rences of seed words/trigrams from SECRET
which are described in section 3.3.
? SDTM with seed words from annotated
Tweets (SDTM?): To compare with SDTM
below using seed words from SECRET, this
uses seed words from the annotated data de-
scribed in section 2.4.
? ASUM (Jo and Oh, 2011): A joint model
of sentiments and topics. We map each SD
level to one sentiment and use the same seed
words/trigrams from SECRET as in SDTM
below. Used in previous work (Bak et al.,
2012).
? First-person pronouns (FirstP): Occurrence
of first-person pronouns which are described
in section 3.2. To identify first-person pro-
nouns, we tagged parts of speech in each
tweet with the Twitter POS tagger (Owoputi
et al., 2013).
? First-person pronouns + Seed words/trigrams
(FP+SE1): First-person pronouns and seed
words/trigrams from SECRET.
? Two stage classifier with First-person pro-
nouns + Seed words/trigrams (FP+SE2): A
5
personal pronouns, 3rd person singular words, family
words, human words, sexual words, etc
1991
Method Acc G F
1
M F
1
H F
1
Avg F
1
LDA 49.2 0.00 0.65 0.05 0.23
MedLDA 43.3 0.41 0.52 0.09 0.34
LIWC 49.2 0.34 0.61 0.18 0.38
BOW+ 54.1 0.50 0.59 0.15 0.41
SEED 54.4 0.52 0.60 0.14 0.42
ASUM 56.6 0.32 0.70 0.38 0.47
SDTM? 60.4 0.57 0.70 0.14 0.47
FirstP 63.2 0.63 0.69 0.10 0.47
FP+SE1 61.0 0.61 0.67 0.16 0.48
FP+SE2 60.4 0.64 0.69 0.17 0.50
SDTM 64.5 0.61 0.71 0.43 0.58
Table 6: SD level classification accuracies and F-
measures using annotated data. Acc is accuracy,
and G F
1
is F-measure for classifying the G level.
Avg F
1
is the macroaveraged value of G F
1
, M F
1
and H F
1
. SDTM outperforms all other methods
compared. The difference between SDTM and
FirstP is statistically significant (p-value < 0.05
for accuracy, < 0.0001 for Avg F
1
).
two stage classifier with first-person pro-
nouns and seed words/trigrams from SE-
CRET. In the first stage, the classifier identi-
fies G with first-person pronouns. Then in the
second stage, the classifier uses seed words
and trigrams to identify M and H levels.
? SDTM: Our model with first-person pro-
nouns and seed words/trigrams from SE-
CRET.
SEED, LIWC, LDA and FirstP cannot be used
directly for classification, so we use Maximum en-
tropy model with outputs of each of those models
as features
6
. BOW+ uses SVM with a radial ba-
sis kernel which performs better than all other set-
tings tried including maximum entropy. We split
the data randomly into 80/20 for train/test. We run
MedLDA, ASUM and SDTM 20 times each and
compute the average accuracies and F-measure for
each level. We run LDA and MedLDA with var-
ious number of topics from 80 to 140, and 120
topics shows best outputs. So we set 120 topics
for LDA, MedLDA and ASUM, 60; 40; 40 topics
for SDTM K
G
,K
M
and K
H
respectively which
is best perform from 40; 40; 40 to 60; 60; 60 top-
ics. We assume that a conversation has few topics
6
It performs better than other classifiers (C4.5, Naive-
Bayes, SVM with linear kernel, polynomial kernel and radial
basis)
and self-disclosure levels, so we set ? = ? = 0.1
(Tang et al., 2014). To incorporate the seed words
and trigrams into ASUM and SDTM, we initial-
ize ?
G
,?
M
and ?
H
differently. We assign a high
value of 2.0 for each seed word and trigram for
that level, and a low value of 10
?6
for each word
that is a seed word for another level, and a default
value of 0.01 for all other words. This approach
is the same as previous papers (Jo and Oh, 2011;
Kim et al., 2013).
As Table 6 shows, SDTM performs better than
the other methods for accuracy as well as F-
measure. LDA and MedLDA generally show
the lowest performance, which is not surprising
given these models are quite general and not tuned
specifically for this type of semi-supervised clas-
sification task. BOW which is simple word fea-
tures also does not perform well, showing espe-
cially low F-measure for the H level. LIWC and
SEED perform better than LDA, but these have
quite low F-measure for G and H levels. ASUM
shows better performance for classifying H level
than others, confirming the effectiveness of a topic
modeling approach to this difficult task, but not as
well as SDTM. FirstP shows good F-measure for
the G level, but the H level F-measure is quite low,
even lower than SEED. Combining first-person
pronouns and seed words and trigrams (FP+SE1)
shows better than each feature alone, and the two
stage classifier (FP+SE2) which is a similar ap-
proach taken in SDTM shows better results. Fi-
nally, SDTM classifies G and M level at a similar
accuracy with FirstP, FP+SE1 and FP+SE2, but
it significantly improves accuracy for the H level
compared to all other methods.
6 Relations of Self-Disclosure and
Conversation Behaviors
In this section, we investigate whether there is
a relationship between self-disclosure and con-
versation behaviors over time. Self-disclosure is
one way to maintain and improve relationships
(Jourard, 1971; Joinson and Paine, 2007). So
two people?s intimacy changes over time has rela-
tionship with self-disclosure in their conversation.
However, it is hard to identify intimacy between
users in large scale online social network. So we
choose conversation behaviors such as conversa-
tion frequency and length which can be treated as
proxies for measuring intimacy between two peo-
ple (Emmers-Sommer, 2004; Bak et al., 2012).
1992
With SDTM, we can automatically classify the
SD level of a large number of conversations, so
we investigate whether there is a similar relation-
ship between self-disclosure in conversations and
subsequent conversation behaviors with the same
partner on Twitter.
For comparing conversation behaviors over
time, we divided the conversations into two sets
for each dyad. For the initial period, we include
conversations from the dyad?s first conversation to
20 days later. And for the subsequent period,
we include conversations during the subsequent 10
days. We compute proportions of conversation for
each SD level for each dyad in the initial and
subsequent periods.
More specifically, we ask the following three
questions:
1. If a dyad shows high conversation frequency
at a particular time period, would they dis-
play higher SD in their subsequent conver-
sations?
2. If a dyad displays high SD level in their con-
versations at a particular time period, would
their subsequent conversations be longer?
3. If a dyad displays high overall SD level,
would their conversations increase in length
over time more than dyads with lower overall
SD level?
6.1 Experiment Setup
We first run SDTM with all of our Twitter con-
versation data with 150; 120; 120 topics for
SDTM K
G
,K
M
and K
H
respectively. The
hyper-parameters are the same as in section 5. To
handle a large dataset, we employ a distributed al-
gorithm (Newman et al., 2009), and run with 28
threads.
Table 7 shows some of the topics that were
prominent in each SD level by KL-divergence. As
expected, G level includes general topics such as
food, celebrity, soccer and IT devices, M level in-
cludes personal communication and birthday, and
finally, H level includes sickness and profanity.
We define a new measurement, SD level score
for a dyad in the period, which is a weighted sum
of each conversation with SD levels mapped to 1,
2, and 3, for the levels G, M, and H, respectively.
0 5 10 15 20 25 30 35Initial conversation frequency
2.00
2.02
2.04
2.06
2.08
2.10
2.12
2.14
Sub
seq
uen
t SD
 lev
el
Figure 5: Relationship between initial conversa-
tion frequency and subsequent SD level. The
solid line is the linear regression line, and the co-
efficient is 0.0020 with p < 0.0001, which shows
a significant positive relationship.
6.2 Does high frequency of conversation lead
to more self-disclosure?
We investigate whether the initial conversation
frequency is correlated with the SD level in the
subsequent period. We run linear regression with
the initial conversation frequency as the indepen-
dent variable, and SD level in the subsequent pe-
riod as the dependent variable.
The regression coefficient is 0.0020 with low p-
value (p < 0.0001). Figure 5 shows the scatter
plot. We can see that the slope of the regression
line is positive.
6.3 Does high self-disclosure lead to longer
conversations?
Now we investigate the effect of the self-
disclosure level to conversation length. We run
linear regression with the intial SD level score as
the independent variable, and the rate of change
in conversation length between initial period
and subsequent period as the dependent variable.
Conversation length is measured by the number of
tweets in a conversation.
The result of regression is that the independent
variable?s coefficient is 0.048 with a low p-value
(p < 0.0001). Figure 6 shows the scatter plot with
the regression line, and we can see that the slope
of regression line is positive.
1993
G level M level H level
101 184 176 36 104 82 113 33 19
chocolate obama league send twitter going ass better lips
butter he?s win email follow party bitch sick kisses
good romney game i?ll tumblr weekend fuck feel love
cake vote season sent tweet day yo throat smiles
peanut right team dm following night shit cold softly
milk president cup address account dinner fucking hope hand
sugar people city know fb birthday lmao pain eyes
cream good arsenal check followers tomorrow shut good neck
make going chelsea link facebook come dick cough arms
love time liverpool need followed i?ll kick bad head
yum party won message omg family face i?ve smirks
hot election football let right fun hoe need slowly
cookies gop united sure saw friends lmfao sore hair
banana paul final thanks page tonight nigga flu face
bread way away my email timeline plans bi today chest
Table 7: High ranked topics in each level by comparing KL-divergence with other level?s topics
1.0 1.5 2.0 2.5 3.0Initial SD level
0.10
0.05
0.00
0.05
0.10
0.15
# T
wee
ts in
 con
ver
sat
ion
 cha
nge
s pr
opo
rtio
n o
ver
 tim
e
Figure 6: Relationship between initial SD level
and conversation length changes over time. The
solid line is the linear regression line, and the co-
efficient is 0.048 with p < 0.0001, which shows a
significant positive relationship.
6.4 Is there a difference in conversation
length patterns over time depending on
overall SD level?
Now we investigate the conversation length
changes over time with three groups, low,
medium, and high, by overall SD level. Then
we investigate changes in conversation length over
time.
Figure 7 shows the results of this investigation.
First, conversations are generally lengthier when
SD level is high. This phenomenon is also ob-
0 5 10 15 20 25 30 35 40Conversation order
8.0
8.5
9.0
9.5
10.0
10.5
# T
wee
ts i
n c
onv
ers
atio
n
high mid low
Figure 7: Changes in conversation length over
time. We divide dyads into three groups by SD
level score as low, medium, and high. Conversa-
tion length noticeably increases over time in the
medium and high groups, but only slight in the low
group.
served in figure 6, but here we can see it as a
long-term persistent pattern. Second, conversation
length increases consistently and significantly for
the high and medium groups, but for the low SD
group, there is not a significant increase of conver-
sation length over time.
7 Related Work
Prior work on quantitatively analyzing self-
disclosure has relied on user surveys (Ledbetter et
1994
al., 2011; Trepte and Reinecke, 2013) or human
annotation (Barak and Gluck-Ofri, 2007; Court-
ney Walton and Rice, 2013). These methods con-
sume much time and effort, so they are not suit-
able for large-scale studies. In prior work clos-
est to ours, Bak et al. (2012) showed that a topic
model can be used to identify self-disclosure, but
that work applies a two-step process in which a
basic topic model is first applied to find the top-
ics, and then the topics are post-processed for bi-
nary classification of self-disclosure. We improve
upon this work by applying a single unified model
of topics and self-disclosure for high accuracy in
classifying the three levels of self-disclosure.
Subjectivity which is aspect of expressing opin-
ions (Pang and Lee, 2008; Wiebe et al., 2004) is
related with self-disclosure, but they are different
dimensions of linguistic behavior. Because there
indeed are many high self-disclosure tweets that
are subjective, but there are also counter examples
in annotated dataset. The tweet ?England manager
is Roy Hodgson.? is low self-disclosure and low
subjectivity, ?I have barely any hair left.? is high
self-disclosure but low subjectivity, and ?Senator
stop lying!? is low self-disclosure but high subjec-
tivity.
8 Conclusion and Future Work
In this paper, we have presented the self-disclosure
topic model (SDTM) for discovering topics and
classifying SD levels from Twitter conversation
data. We devised a set of effective seed words
and trigrams, mined from a dataset of secrets. We
also annotated Twitter conversations to make a
ground-truth dataset for SD level. With anno-
tated data, we showed that SDTM outperforms
previous methods in classification accuracy and F-
measure. We publish the source code of SDTM
and the dataset include annotated Twitter conver-
sations and SECRET publicly
7
.
We also analyzed the relationship between SD
level and conversation behaviors over time. We
found that there is a positive correlation be-
tween initial SD level and subsequent conversa-
tion length. Also, dyads show higher level of
SD if they initially display high conversation fre-
quency. Finally, dyads with overall medium and
high SD level will have longer conversations over
time. These results support previous results in so-
7
http://uilab.kaist.ac.kr/research/
EMNLP2014
cial psychology research with more robust results
from a large-scale dataset, and show the effective-
ness of computationally analyzing at SD behavior.
There are several future directions for this re-
search. First, we can improve our modeling for
higher accuracy and better interpretability. For
instance, SDTM only considers first-person pro-
nouns and topics. Naturally, there are other lin-
guistic patterns that can be identified by humans
but not captured by pronouns and topics. Sec-
ond, the number of topics for each level is varied,
and so we can explore nonparametric topic mod-
els (Teh et al., 2006) which infer the number of
topics from the data. Third, we can look at the
relationship between self-disclosure behavior and
general online social network usage beyond con-
versations. We will explore these directions in our
future work.
Acknowledgments
We would like to thank Jing Liu and Wayne Xin
Zhao for inspiring discussions, and the anony-
mous reviewers for helpful comments. Alice Oh
is supported by ICT R&D program of MSIP/IITP
[10041313, UX-oriented Mobile SW Platform].
References
JinYeong Bak, Suin Kim, and Alice Oh. 2012. Self-
disclosure and relationship strength in twitter con-
versations. In Proceedings of ACL.
Azy Barak and Orit Gluck-Ofri. 2007. Degree and
reciprocity of self-disclosure in online forums. Cy-
berPsychology & Behavior, 10(3):407?417.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
danah boyd, Scott Golder, and Gilad Lotan. 2010.
Tweet, tweet, retweet: Conversational aspects of
retweeting on twitter. In Proceedings of HICSS.
S Courtney Walton and Ronald E Rice. 2013. Medi-
ated disclosure on twitter: The roles of gender and
identity in boundary impermeability, valence, dis-
closure, and stage. Computers in Human Behavior,
29(4):1465?1474.
Cristian Danescu-Niculescu-Mizil, Michael Gamon,
and Susan Dumais. 2011. Mark my words!: Lin-
guistic style accommodation in social media. In
Proceedings of WWW.
Tara M Emmers-Sommer. 2004. The effect of com-
munication quality and quantity indicators on inti-
macy and relational satisfaction. Journal of Social
and Personal Relationships, 21(3):399?411.
1995
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
David J Houghton and Adam N Joinson. 2012.
Linguistic markers of secrets and sensitive self-
disclosure in twitter. In Proceedings of HICSS.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM.
Adam N Joinson and Carina B Paine. 2007. Self-
disclosure, privacy and the internet. The Oxford
handbook of Internet psychology, pages 237?252.
Adam N Joinson. 2001. Self-disclosure in
computer-mediated communication: The role of
self-awareness and visual anonymity. European
Journal of Social Psychology, 31(2):177?192.
Sidney M Jourard. 1971. Self-disclosure: An experi-
mental analysis of the transparent self.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A hierarchical aspect-sentiment
model for online reviews. In Proceedings of AAAI.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159?174.
Andrew M Ledbetter, Joseph P Mazer, Jocelyn M DeG-
root, Kevin R Meyer, Yuping Mao, and Brian Swaf-
ford. 2011. Attitudes toward online social con-
nection and self-disclosure as predictors of facebook
communication and relational closeness. Communi-
cation Research, 38(1):27?53.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Erika McCallister. 2010. Guide to protecting the confi-
dentiality of personally identifiable information. DI-
ANE Publishing.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Sharon Meraz. 2013. Public dialogue: Analysis of
tolerance in online discussions. In Proceedings of
ACL.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms
for topic models. Journal of Machine Learning Re-
search, 10:1801?1828.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of HLT-NAACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL.
Jian Tang, Zhaoshi Meng, Xuanlong Nguyen, Qiaozhu
Mei, and Ming Zhang. 2014. Understanding the
limiting factors of topic modeling via posterior con-
traction analysis. In Proceedings of The 31st In-
ternational Conference on Machine Learning, pages
190?198.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the american statistical associ-
ation, 101(476).
Sabine Trepte and Leonard Reinecke. 2013. The re-
ciprocal effects of social network site use and the
disposition for self-disclosure: A longitudinal study.
Computers in Human Behavior, 29(3):1102 ? 1112.
Sarah I Vondracek and Fred W Vondracek. 1971. The
manipulation and measurement of self-disclosure in
preadolescents. Merrill-Palmer Quarterly of Behav-
ior and Development, 17(1):51?58.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational linguistics,
30(3):277?308.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
EMNLP.
Jun Zhu, Amr Ahmed, and Eric P Xing. 2012. Medlda:
maximum margin supervised topic models. Journal
of Machine Learning Research, 13:2237?2278.
1996
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 60?64,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Self-Disclosure and Relationship Strength in Twitter Conversations
JinYeong Bak, Suin Kim, Alice Oh
Department of Computer Science
Korea Advanced Institute of Science and Technology
Daejeon, South Korea
{jy.bak, suin.kim}@kaist.ac.kr, alice.oh@kaist.edu
Abstract
In social psychology, it is generally accepted
that one discloses more of his/her personal in-
formation to someone in a strong relationship.
We present a computational framework for au-
tomatically analyzing such self-disclosure be-
havior in Twitter conversations. Our frame-
work uses text mining techniques to discover
topics, emotions, sentiments, lexical patterns,
as well as personally identifiable information
(PII) and personally embarrassing information
(PEI). Our preliminary results illustrate that in
relationships with high relationship strength,
Twitter users show significantly more frequent
behaviors of self-disclosure.
1 Introduction
We often self-disclose, that is, share our emotions,
personal information, and secrets, with our friends,
family, coworkers, and even strangers. Social psy-
chologists say that the degree of self-disclosure in a
relationship depends on the strength of the relation-
ship, and strategic self-disclosure can strengthen the
relationship (Duck, 2007). In this paper, we study
whether relationship strength has the same effect on
self-disclosure of Twitter users.
To do this, we first present a method for compu-
tational analysis of self-disclosure in online conver-
sations and show promising results. To accommo-
date the largely unannotated nature of online conver-
sation data, we take a topic-model based approach
(Blei et al, 2003) for discovering latent patterns that
reveal self-disclosure. A similar approach was able
to discover sentiments (Jo and Oh, 2011) and emo-
tions (Kim et al, 2012) from user contents. Prior
work on self-disclosure for online social networks
has been from communications research (Jiang et
al., 2011; Humphreys et al, 2010) which relies
on human judgements for analyzing self-disclosure.
The limitation of such research is that the data is
small, so our approach of automatic analysis of self-
disclosure will be able to show robust results over a
much larger data set.
Analyzing relationship strength in online social
networks has been done for Facebook and Twitter
in (Gilbert and Karahalios, 2009; Gilbert, 2012) and
for enterprise SNS (Wu et al, 2010). In this paper,
we estimate relationship strength simply based on
the duration and frequency of interaction. We then
look at the correlation between self-disclosure and
relationship strength and present the preliminary re-
sults that show a positive and significant correlation.
2 Data and Methodology
Twitter is widely used for conversations (Ritter et al,
2010), and prior work has looked at Twitter for dif-
ferent aspects of conversations (Boyd et al, 2010;
Danescu-Niculescu-Mizil et al, 2011; Ritter et al,
2011). Ours is the first paper to analyze the degree
of self-disclosure in conversational tweets. In this
section, we describe the details of our Twitter con-
versation data and our methodology for analyzing
relationship strength and self-disclosure.
2.1 Twitter Conversation Data
A Twitter conversation is a chain of tweets where
two users are consecutively replying to each other?s
tweets using the Twitter reply button. We identified
dyads of English-tweeting users who had at least
60
three conversations from October, 2011 to Decem-
ber, 2011 and collected their tweets for that dura-
tion. To protect users? privacy, we anonymized the
data to remove all identifying information. This
dataset consists of 131,633 users, 2,283,821 chains
and 11,196,397 tweets.
2.2 Relationship Strength
Research in social psychology shows that relation-
ship strength is characterized by interaction fre-
quency and closeness of a relationship between
two people (Granovetter, 1973; Levin and Cross,
2004). Hence, we suggest measuring the relation-
ship strength of the conversational dyads via the fol-
lowing two metrics. Chain frequency (CF) mea-
sures the number of conversational chains between
the dyad averaged per month. Chain length (CL)
measures the length of conversational chains be-
tween the dyad averaged per month. Intuitively, high
CF or CL for a dyad means the relationship is strong.
2.3 Self-Disclosure
Social psychology literature asserts that self-
disclosure consists of personal information and open
communication composed of the following five ele-
ments (Montgomery, 1982).
Negative openness is how much disagreement
or negative feeling one expresses about a situation
or the communicative partner. In Twitter conver-
sations, we analyze sentiment using the aspect and
sentiment unification model (ASUM) (Jo and Oh,
2011), based on LDA (Blei et al, 2003). ASUM
uses a set of seed words for an unsupervised dis-
covery of sentiments. We use positive and negative
emoticons from Wikipedia.org1. Nonverbal open-
ness includes facial expressions, vocal tone, bod-
ily postures or movements. Since tweets do not
show these, we look at emoticons, ?lol? (laughing
out loud) and ?xxx? (kisses) for these nonverbal ele-
ments. According to Derks et al (2007), emoticons
are used as substitutes for facial expressions or vocal
tones in socio-emotional contexts. We also consider
profanity as nonverbal openness. The methodology
used for identifying profanity is described in the next
section. Emotional openness is how much one dis-
closes his/her feelings and moods. To measure this,
1http://en.wikipedia.org/wiki/List of emoticons
we look for tweets that contain words that are iden-
tified as the most common expressions of feelings in
blogs as found in Harris and Kamvar (2009). Recep-
tive openness and General-style openness are diffi-
cult to get from tweets, and they are not defined pre-
cisely in the literature, so we do not consider these
here.
2.4 PII, PEI, and Profanity
PII and PEI are also important elements of self-
disclosure. Automatically identifying these is quite
difficult, but there are certain topics that are indica-
tive of PII and PEI, such as family, money, sick-
ness and location, so we can use a widely-used topic
model, LDA (Blei et al, 2003) to discover topics
and annotate them using MTurk2 for PII and PEI,
and profanity. We asked the Turkers to read the con-
versation chains representing the topics discovered
by LDA and have them mark the conversations that
contain PII and PEI. From this annotation, we iden-
tified five topics for profanity, ten topics for PII, and
eight topics for PEI. Fleiss kappa of MTurk result
is 0.07 for PEI, and 0.10 for PII, and those numbers
signify slight agreement (Landis and Koch, 1977).
Table 1 shows some of the PII and PEI topics. The
profanity words identified this way include nigga,
lmao, shit, fuck, lmfao, ass, bitch.
PII 1 PII 2 PEI 1 PEI 2 PEI 3
san tonight pants teeth family
live time wear doctor brother
state tomorrow boobs dr sister
texas good naked dentist uncle
south ill wearing tooth cousin
Table 1: PII and PEI topics represented by the high-
ranked words in each topic.
To verify the topic-model based approach to dis-
covering PII and PEI, we tried supervised classifi-
cation using SVM on document-topic proportions.
Precision and recall are 0.23 and 0.21 for PII, and
0.30 and 0.23 for PEI. These results are not quite
good, but this is a difficult task even for humans,
and we had a low agreement among the Turkers. So
our current work is in improving this.
2https://www.mturk.com
61
Se
nti
me
nt
0.26
0.28
0.30
0.32
0.34
0.36
l
ll
l
2 3 4
l pos
neg
neu No
nv
er
ba
l o
pe
nn
es
s
0.00
0.05
0.10
0.15 llll
2 3 4
l emoticon
lol
xxx Em
oti
on
al 
op
en
ne
ss
0.00
0.05
0.10
0.15
0.20
0.25
0.30
l
ll
l
2 3 4
l joy
sadness
others
Pr
ofa
nit
y
0.00
0.02
0.04
0.06
0.08
0.10
l
l
l
l
2 3 4
l profanity
PI
I, P
EI
0.00
0.01
0.02
0.03
0.04
l
l
l
l
2 3 4
l PII
PEI
(a) Chain Frequency
Se
nti
me
nt
0.26
0.28
0.30
0.32
0.34
0.36
ll
ll
l
ll
l
5 10 15 20 25
l pos
neg
neu No
nv
er
ba
l o
pe
nn
es
s
0.00
0.05
0.10
0.15
l
lllllll
5 10 15 20 25
l emoticon
lol
xxx Em
oti
on
al 
op
en
ne
ss
0.00
0.05
0.10
0.15
0.20
0.25
0.30
ll
ll
ll
l
l
5 10 15 20 25
l joy
sadness
others
Pr
ofa
nit
y
0.00
0.02
0.04
0.06
0.08
0.10
l
l
llllll
5 10 15 20 25
l profanity
PI
I, P
EI
0.00
0.01
0.02
0.03
0.04
l
lllllll
5 10 15 20 25
l PII
PEI
(b) Conversation Length
Figure 1: Degree of self-disclosure depending on various relationship strength metrics. The x axis shows relationship
strength according to tweeting behavior (chain frequency and chain length), and the y axis shows proportion of self-
disclosure in terms of negative openness, emotional openness, profanity, and PII and PEI.
3 Results and Discussions
Chain frequency (CF) and chain length (CL) reflect
the dyad?s tweeting behaviors. In figure 1, we can
see that the two metrics show similar patterns of
self-disclosure. When two users have stronger rela-
tionships, they show more negative openness, non-
verbal openness, profanity, and PEI. These patterns
are expected. However, weaker relationships tend
to show more PII and emotions. A closer look at the
data reveals that PII topics are related to cities where
they live, time of day, and birthday. This shows
that the weaker relationships, usually new acquain-
tances, use PII to introduce themselves or send triv-
ial greetings for birthdays. Higher emotional open-
ness in weaker relationships looks strange at first,
but similar to PII, emotion in weak relationships is
usually expressed as greetings, reactions to baby or
pet photos, or other shallow expressions.
It is interesting to look at outliers, dyads with very
strong and very weak relationship groups. Table 3
summarizes the self-disclosure behaviors of these
outliers. There is a clear pattern that stronger re-
lationships show more nonverbal openness, nega-
str1 str2 weak1 weak2 weak3
lmao sleep following ill love
lmfao bed thanks sure thanks
shit night followers soon cute
ass tired welcome better aww
smh awake follow want pretty
Table 2: Topics that are most prominent in strong (?str?)
and weak relationships.
tive openness, profanity use, and PEI. In figure 1,
emotional openness does not differ for the strong
and weak relationship groups. We can see why this
is when we look at the topics for the strong and
weak groups. Table 2 shows the topics that are
most prominent in the strong relationships, and they
include daily greetings, plans, nonverbal emotions
such as ?lol?, ?omg?, and profanity. In weak relation-
ships, the prominent topics illustrate the prevalence
of initial getting-to-know conversations in Twitter.
They welcome and greet each other about kids and
pets, and offer sympathies about feeling bad.
One interesting way to use our analysis is in iden-
62
strong weak
# relation 5,640 226,116
CF 14.56 1.00
CL 97.74 3.00
Emotion 0.21 0.22
Emoticon 0.162 0.134
lol 0.105 0.060
xxx 0.021 0.006
Pos Sent 0.31 0.33
Neg Sent 0.32 0.29
Neut Sent 0.27 0.29
Profanity 0.0615 0.0085
PII 0.016 0.019
PEI 0.022 0.013
Table 3: Comparing the top 1% and the bottom 1% rela-
tionships as measured by the combination of CF and CL.
From ?Emotion? to PEI, all values are average propor-
tions of tweets containing each self-disclosure behavior.
Strong relationships show more negative sentiment, pro-
fanity, and PEI, and weak relationships show more posi-
tive sentiment and PII. ?Emotion? is the sum of all emo-
tion categories and shows little difference.
tifying a rare situation that deviates from the gen-
eral pattern, such as a dyad linked weakly but shows
high self-disclosure. We find several such examples,
most of which are benign, but some do show signs
of risk for one of the parties. In figure 2, we show
an example of a conversation with a high degree of
self-disclosure by a dyad who shares only one con-
versation in our dataset spanning two months.
4 Conclusion and Future Work
We looked at the relationship strength in Twitter
conversational partners and how much they self-
disclose to each other. We found that people dis-
close more to closer friends, confirming the social
psychology studies, but people show more positive
sentiment to weak relationships rather than strong
relationships. This reflects the social norm toward
first-time acquaintances on Twitter. Also, emotional
openness does not change significantly with rela-
tionship strength. We think this may be due to the in-
herent difficulty in truly identifying the emotions on
Twitter. Identifying emotion merely based on key-
words captures mostly shallow emotions, and deeper
emotional openness either does not occur much on
Figure 2: Example of Twitter conversation in a weak re-
lationship that shows a high degree of self-disclosure.
Twitter or cannot be captures very well.
With our automatic analysis, we showed that
when Twitter users have conversations, they con-
trol self-disclosure depending on the relationship
strength. We showed the results of measuring the re-
lationship strength of a Twitter conversational dyad
with chain frequency and length. We also showed
the results of automatically analyzing self-disclosure
behaviors using topic modeling.
This is ongoing work, and we are looking to im-
prove methods for analyzing relationship strength
and self-disclosure, especially emotions, PII and
PEI. For relationship strength, we will consider not
only interaction frequency, but also network distance
and relationship duration. For finding emotions, first
we will adapt existing models (Vaassen and Daele-
mans, 2011; Tokuhisa et al, 2008) and suggest a
new semi-supervised model. For finding PII and
PEI, we will not only consider the topics, but also
time, place and the structure of questions and an-
swers. This paper is a starting point that has shown
some promising research directions for an important
problem.
5 Acknowledgment
We thank the anonymous reviewers for helpful com-
ments. This research is supported by Korean Min-
istry of Knowledge Economy and Microsoft Re-
search Asia (N02110403).
63
References
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
D. Boyd, S. Golder, and G. Lotan. 2010. Tweet, tweet,
retweet: Conversational aspects of retweeting on twit-
ter. In Proceedings of the 43rd Hawaii International
Conference on System Sciences.
C. Danescu-Niculescu-Mizil, M. Gamon, and S. Dumais.
2011. Mark my words!: linguistic style accommoda-
tion in social media. In Proceedings of the 20th Inter-
national World Wide Web Conference.
D. Derks, A.E.R. Bos, and J. Grumbkow. 2007. Emoti-
cons and social interaction on the internet: the impor-
tance of social context. Computers in Human Behav-
ior, 23(1):842?849.
S. Duck. 2007. Human Relationships. Sage Publications
Ltd.
E. Gilbert and K. Karahalios. 2009. Predicting tie
strength with social media. In Proceedings of the 27th
International Conference on Human Factors in Com-
puting Systems, pages 211?220.
E. Gilbert. 2012. Predicting tie strength in a new
medium. In Proceedings of the ACM Conference on
Computer Supported Cooperative Work.
M.S. Granovetter. 1973. The strength of weak ties.
American Journal of Sociology, pages 1360?1380.
J. Harris and S. Kamvar. 2009. We Feel Fine: An Al-
manac of Human Emotion. Scribner Book Company.
L. Humphreys, P. Gill, and B. Krishnamurthy. 2010.
How much is too much? privacy issues on twitter. In
Conference of International Communication Associa-
tion, Singapore.
L. Jiang, N.N. Bazarova, and J.T. Hancock. 2011. From
perception to behavior: Disclosure reciprocity and the
intensification of intimacy in computer-mediated com-
munication. Communication Research.
Y. Jo and A.H. Oh. 2011. Aspect and sentiment unifica-
tion model for online review analysis. In Proceedings
of International Conference on Web Search and Data
Mining.
S. Kim, J. Bak, and A. Oh. 2012. Do you feel what i feel?
social aspects of emotions in twitter conversations. In
Proceedings of the AAAI International Conference on
Weblogs and Social Media.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
pages 159?174.
D.Z. Levin and R. Cross. 2004. The strength of weak
ties you can trust: The mediating role of trust in effec-
tive knowledge transfer. Management science, pages
1477?1490.
B.M. Montgomery. 1982. Verbal immediacy as a behav-
ioral indicator of open communication content. Com-
munication Quarterly, 30(1):28?34.
A. Ritter, C. Cherry, and B. Dolan. 2010. Unsuper-
vised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 172?180.
A. Ritter, C. Cherry, and W.B. Dolan. 2011. Data-driven
response generation in social media. In Proceedings
of EMNLP.
R. Tokuhisa, K. Inui, and Y. Matsumoto. 2008. Emotion
classification using massive examples extracted from
the web. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 881?888.
F. Vaassen and W. Daelemans. 2011. Automatic emotion
classification for interpersonal communication. ACL
HLT 2011, page 104.
A. Wu, J.M. DiMicco, and D.R. Millen. 2010. Detecting
professional versus personal closeness using an enter-
prise social network site. In Proceedings of the 28th
International Conference on Human Factors in Com-
puting Systems.
64
Generating Baseball Summaries from Multiple Perspectives by Reordering
Content
Alice Oh
MIT CSAIL
32 Vassar St.
Cambridge, MA 02139 USA
aoh@mit.edu
Howard Shrobe
MIT CSAIL
32 Vassar St.
Cambridge, MA 02139 USA
hes@csail.mit.edu
Abstract
This paper presents a reordering algorithm for
generating multiple stories from different per-
spectives based on a single baseball game. We
take a description of a game and a neutral sum-
mary, reorder the content of the neutral sum-
mary based on event features, and produce two
summaries that the users rated as showing per-
spectives of each of the two teams. We de-
scribe the results from an initial user survey
that revealed the power of reordering on the
users? perception of perspective. Then we de-
scribe our reordering algorithm which was de-
rived from analyzing the corpus of local news-
paper articles of teams involved in the games
as well as a neutral corpus for the respective
games. The resulting reordering algorithm is
successful at turning a neutral article into two
different summary articles that express the two
teams? perspectives.
1 Introduction
Stories about events are written in many different
perspectives, or points-of-view. For example, fol-
lowing a baseball game, multiple articles are writ-
ten that summarize the game from different perspec-
tives. Although they are describing the same game,
readers feel differently about the articles and may
prefer to read a certain perspective over all the oth-
ers. We have explored what factors contribute to the
differences in perspective in these event summary
stories and how we can automatically plan content
to generate multiple summaries of a baseball game
written from different perspectives. The end goal of
this work is to build a system that takes as input a
factual description of a baseball game and a neutral
article about the game, then produces two other ar-
ticles, each from a particular team?s point of view.
There is previous work such as (Robin and McK-
eown, 1996) on automatic summary generation of
sports games, but our work goes further to generate
multiple summaries.
It is first necessary to define what is meant by per-
spective and multiple perspectives. The definition of
perspective in this work is somewhat different from
a more traditional meaning of perspective in litera-
ture, such as the third-person perspective discussed
in (Wiebe and Rapaport, 1988). Our definition is
much closer to that used in (Lin and Hauptmann,
2006), where they look at ideological perspectives
of online articles on political, social, and cultural is-
sues. They look at the political domain of the is-
sues between Israel and Palestine, and they try to
infer, for each online article, whether it is written
from the Israeli perspective or the Palestinian per-
spective. For our work, we are looking at the do-
main of baseball games, so we focus on the arti-
cle?s perspective in terms of the home team versus
the visiting team. We first assume that the two op-
posing perspectives are expressed in the local news-
paper articles of the two teams, and we assume that
the neutral perspective is expressed in the Associ-
ated Press articles published on an ESPN website
(www.espn.com). We confirmed these assumptions
via a user study, then we identified some key factors
contributing to an article having a certain perspec-
tive. The next section explains our corpus and user
studies.
173
2 Corpus
The Major League Baseball (MLB) has 30 teams
within the United States and Canada, and each team
plays approximately 160 games per season. We have
collected data for hundreds of games from the 2005
and 2006 MLB seasons. The corpus is divided into
two sets. The first is factual descriptions of the
games in quantitative form and simple natural lan-
guage text, and the second is journalistic writings
from online news sources.
2.1 Game Data
For every MLB game, the website of MLB
(www.mlb.com) publishes game data consisting of
two documents. The first is a game log (see figure
1) , which is a complete list of at-bats in the game,
where each at-bat is a set of pitches thrown from a
pitcher to a batter such that the batter either gets out
or advances to a base at the completion of the at-bat.
There are at least 3 at-bats per half of an inning (top
or bottom), and there are at least 9 innings per game
(except in extreme weather conditions), so there are
at least 54 at-bats, but usually more. In our corpus,
the average number of at-bats is 76.2 per game. The
second is a boxscore, which is a list of each bat-
ter and pitcher?s performance statistics for the game.
Currently we do not use the boxscore documents.
The game log is parsed using simple regular ex-
pression type patterns to turn each at-bat into a
feature vector. We have defined 22 features: in-
ningNumber, atBatNumber, pitchCount, homeScore,
visitScore, team, pitcher, batter, onFirst, onSec-
ond, onThird, outsAdded, baseHit, rbi, doubleplay,
runnersStranded, homerun, strikeOut, extraBase-
Hit, walk, error, typeOfPlay. Some of these features,
such as batter and typeOfPlay are extracted directly
from each line in the log that is being transformed
into a feature vector. Some of the features, such as
inningNumber, team, and pitcher are span multiple
contiguous at-bats and are extracted from the cur-
rent line or in one of the lines going back a few at-
bats. The remaining features, such as onFirst, out-
sAdded, and runnersStranded are derived from look-
ing at the feature vector of the previous at-bat and
following simple rules of the baseball game. For ex-
ample, onSecond is derived from looking at the pre-
vious feature vector?s onFirst value, and if the cur-
rent play is one that advances the runner one base,
the previous feature vector?s onFirst gets copied to
the current onSecond. While we tried to identify
features that are important for analyzing and gener-
ating multiple perspectives, later sections will show
that some of them were not used, as they were not
significant variables for our content ordering algo-
rithm.
2.2 Online Articles
In addition to game logs and boxscores, we collected
articles published on several online news sources.
The MLB website (www.mlb.com) publishes two
articles for every game, written for each of the
two teams in the game. Each team has a unique
sportswriter covering that team for the entire season,
so we use the MLB articles as one of our sources
with the home/visit team perspective. The ESPN
website (www.espn.com) also has articles for ev-
ery MLB game including the main summary arti-
cles from the Associated Press (AP). We use the AP
articles as our neutral source. We also collected
online local newspaper articles for MLB teams
in the American League East Division: Boston
Red Sox (The Boston Globe at www.boston.com),
New York Yankees (The New York Times at
www.nytimes.com), Baltimore Orioles (The Wash-
ington Post at www.washingtonpost.com), Toronto
Blue Jays (The Toronto Star at torontostar.com), and
Tampa Bay Devil Rays (The Tampa Tribune at tam-
patrib.com).
3 From Neutral to One-Sided Perspective
We are building a system that takes a neutral article
and turns it into an article with a one-sided perspec-
tive, so we looked at whether we can use the same
content of the neutral article and still produce a non-
neutral perspective. Surprisingly, looking at the ar-
ticles in terms of the game events, there were many
games where the three articles overlap quite a bit in
the (at-bats) that are mentioned in the articles. That
is, the neutral and the home/visit team articles all
describe the same set of game events, but they still
manage to present them such that the readers notice
the differences in perspective.
To compute the overlap of content, the articles
were first tagged with player names and part-of-
174
Figure 1: Pitch by Pitch Log of a Baseball Game
Games All Home Visit
41 215 23 21
Ave 5.24 0.56 0.51
Table 1: Number of at-bats described in all three articles,
at-bats only in the home team articles, and at-bats only in
the visit team articles for 41 games.
speech tags, and simple pattern matching heuristics
were used to automatically align the sentences in the
articles with game events. The player names were
downloaded from the MLB team sites accessible
from www.mlb.com, and the POS tagging was done
with the Stanford POS tagger (Toutanova and Man-
ninig, 2000). Pattern matching heuristics looked for
co-occurrences of tags and words within a certain
window (e.g., {player} AND ?homerun? within 3
words), and the results from applying those heuris-
tics were aligned with the at-bat feature vectors
computed from the game log. Testing on 45 arti-
cles hand-annotated by the first author, we achieved
a precision of 79.0% and recall of 79.2% for align-
ment. The average number of at-bats in those hand-
annotated articles was 8. The percentage of overlap-
ping content varies widely, mostly due to the way the
games unfolded. For example, many games are one-
sided where one team simply dominates, and there
are just not enough events that are positive for one of
the teams. For those games, the losing team?s news-
paper merely reports the result of the game without
describing the events of the game in detail. How-
ever, games that are close in score and number of
hits, we found a high overlap of content among all
three articles. Table 1 lists the number of at-bats re-
ported in common in all three articles.
Based on the corpus analyses we surveyed users
to see whether we can identify the important factors
that contribute to differences in perspective.
First, to confirm that the home team and the visit
team perspectives of the local team articles are cor-
rectly perceived, we simply presented the AP and
local newspaper articles to users and asked them to
guess which team the articles were written for. As
expected, users identified the local team perspec-
tive with ease and confidence. Then, we took out
all sentences except ones that describe the the game
events (at-bats). Player quotes, commentary about
the team or players? historical performances, and fi-
nancial and personal news were some of the con-
tent that were removed from the articles. Users were
asked to guess which team the articles were written
for, and again, they were able to identify the local
team perspectives. We then removed sentences de-
scribing game events that did not overlap with the
content in the neutral article, and again, users iden-
tified the local perspectives. Finally, we replaced all
the sentences with canned surface forms, such that
all the articles shared the same surface form of sen-
tences and preserved only the ordering of the con-
tent. This last experiment, albeit with less confi-
dence than the previous ones, still produced users?
perception of local perspective for the non-neutral
articles. 8 users participated in the study using 12
games, and table 2 summarizes the results of these
user surveys. All 8 users rated all 36 articles, 3 ar-
ticles for each game, but the ordering of the articles
was randomized. For all four conditions, users were
asked to rate each article on a scale of 1 to 5, where 1
is strongly home team perspective, 3 is neutral, and
5 is strongly visit team perspective.
4 Feature-based Ordering Strategies
Following the results from the user study, we used
a corpus-driven approach to identify the ordering
strategies that contribute to the different perspec-
tives. We looked at the games for which the three
175
Condition Home AP Visit
Original 1.75 2.75 4.06
Events Only 1.75 2.90 3.85
Overlapping 2.02 2.75 3.85
Ordering 2.18 2.83 3.83
Table 2: Users? ratings on how they perceived perspec-
tive. They rated using a 1 to 5 scale, where 1 is the home
team perspective, 3 is neutral, and 5 is the visiting team
perspective. For all lines, t-test for the users? ratings of
home team articles and visit tema articles show a statisti-
cally significant difference at the level p < 0.05.
articles have highly overlapping content and studied
how the content is organized differently. We seg-
mented the articles into topic segments (e.g, para-
graphs) and noticed that the three articles differ quite
a bit in the topics that hold the content together.
These topics can be expressed simply by the fea-
ture values that are shared among the at-bats that
appear together in the same segment. Below is an
example of two different orderings of at-bats based
on feature values. The first segment (lines 1a, 2a) of
the first ordering shares the same values for the fea-
tures pitcher, team, inning, andR (score added by
that play). The second segment (lines 3a, 4a) shares
pitcher, batter, and team.
Pitcher Batter Team inn type R
1a Johns Damon Bos 1 hr 1
2a Johns Ramir Bos 1 dbl 1
3a Schil Jeter Nyy 4 dbl 0
4a Schil Jeter Nyy 6 hr 2
The second ordering shows the same content ar-
ranged in different segments, where both segments
are organized based on the value of type of play.
This is a frequent pattern in our corpus that seems to
be responsible for the different perspectives of the
articles.
Pitcher Batter Team inn type R
1b Johns Damon Bos 1 hr 1
2b Schil Jeter Nyy 6 hr 2
3b Johns Ramir Bos 1 dbl 1
4b Schil Jeter Nyy 4 dbl 0
Since there are many features, we need to identify
the features to use for assigning the at-bats to appear
in the same segment. We used a simple counting of
most frequent feature values of the corpus to derive
these features. This comes from the intuition that
the players whose names appear most frequently in
the articles for a local newspaper tend to be impor-
tant topics for those stories. So we aggregate all
the local team articles and rank the feature values
including pitcher and batter names and play types
(e.g., homerun, single, strikeout). To turn a neutral
article into a local perspective article, we take the
at-bats that should appear in the article, look at the
feature values that are shared among them, and find
the highest-ranked feature value for that team. Any
remaining at-bats are arranged in chronological or-
der.
5 Conclusion
We presented a content ordering algorithm that takes
a neutral article of baseball and produces two other
articles from the two teams? perspectives. We
showed that just by reordering, we can induce dif-
ferent perspectives, and we used a corpus for dis-
covering the different ordering strategies. In the fu-
ture, we will refine our reordering algorithm, carry
out a full evaluation, and also look at other factors
that contribute to perspective such as content selec-
tion and surface realization. We will also look at an-
other domain, such as the socio-political conflict in
the Middle East discussed in (Lin and Hauptmann,
2006), to see whether similar reordering patterns ap-
pear in those articles.
References
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspectives?
A test of different perspectives based on statistical dis-
tribution divergence. Proceedings of the 42th annual
meeting on Association for Computational Linguistics.
Jacques Robin and Kathleen McKeown. 1996. Empir-
ically designing and evaluating a new revision-based
model for summary generation. Artificial Intelligence.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Corpora
(EMNLP/VLC-2000).
Janyce M. Wiebe and William J. Rapaport. 1988. A
computational theory of perspective and reference in
narrative. Proceedings of the 26th annual meeting on
Association for Computational Linguistics, 131?138.
176
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 42?49,
Baltimore, Maryland USA, 27 June 2014.
c
?2014 Association for Computational Linguistics
Self-disclosure topic model for Twitter conversations
JinYeong Bak
Department of Computer Science
KAIST
Daejeon, South Korea
jy.bak@kaist.ac.kr
Chin-Yew Lin
Microsoft Research Asia
Beijing 100080, P.R. China
cyl@microsoft.com
Alice Oh
Department of Computer Science
KAIST
Daejeon, South Korea
alice.oh@kaist.edu
Abstract
Self-disclosure, the act of revealing one-
self to others, is an important social be-
havior that contributes positively to inti-
macy and social support from others. It
is a natural behavior, and social scien-
tists have carried out numerous quantita-
tive analyses of it through manual tagging
and survey questionnaires. Recently, the
flood of data from online social networks
(OSN) offers a practical way to observe
and analyze self-disclosure behavior at an
unprecedented scale. The challenge with
such analysis is that OSN data come with
no annotations, and it would be impos-
sible to manually annotate the data for a
quantitative analysis of self-disclosure. As
a solution, we propose a semi-supervised
machine learning approach, using a vari-
ant of latent Dirichlet allocation for au-
tomatically classifying self-disclosure in a
massive dataset of Twitter conversations.
For measuring the accuracy of our model,
we manually annotate a small subset of
our dataset, and we show that our model
shows significantly higher accuracy and
F-measure than various other methods.
With the results our model, we uncover
a positive and significant relationship be-
tween self-disclosure and online conversa-
tion frequency over time.
1 Introduction
Self-disclosure is an important and pervasive so-
cial behavior. People disclose personal informa-
tion about themselves to improve and maintain
relationships (Jourard, 1971; Joinson and Paine,
2007). For example, when two people meet for
the first time, they disclose their names and in-
terests. One positive outcome of self-disclosure
is social support from others (Wills, 1985; Der-
lega et al., 1993), shown also in online social net-
works (OSN) such as Twitter (Kim et al., 2012).
Receiving social support would then lead the user
to be more active on OSN (Steinfield et al., 2008;
Trepte and Reinecke, 2013). In this paper, we seek
to understand this important social behavior using
a large-scale Twitter conversation data, automati-
cally classifying the level of self-disclosure using
machine learning and correlating the patterns with
subsequent OSN usage.
Twitter conversation data, explained in more de-
tail in section 4.1, enable a significantly larger
scale study of naturally-occurring self-disclosure
behavior, compared to traditional social science
studies. One challenge of such large scale study,
though, remains in the lack of labeled ground-
truth data of self-disclosure level. That is,
naturally-occurring Twitter conversations do not
come tagged with the level of self-disclosure in
each conversation. To overcome that challenge,
we propose a semi-supervised machine learning
approach using probabilistic topic modeling. Our
self-disclosure topic model (SDTM) assumes that
self-disclosure behavior can be modeled using a
combination of simple linguistic features (e.g.,
pronouns) with automatically discovered seman-
tic themes (i.e., topics). For instance, an utterance
?I am finally through with this disastrous relation-
ship? uses a first-person pronoun and contains a
topic about personal relationships.
In comparison with various other models,
SDTM shows the highest accuracy, and the result-
ing self-disclosure patterns of the users are cor-
related significantly with their future OSN usage.
Our contributions to the research community in-
clude the following:
? We present a topic model that explicitly in-
cludes the level of self-disclosure in a conver-
sation using linguistic features and the latent
semantic topics (Sec. 3).
42
? We collect a large dataset of Twitter conver-
sations over three years and annotate a small
subset with self-disclosure level (Sec. 4).
? We compare the classification accuracy of
SDTM with other models and show that it
performs the best (Sec. 5).
? We correlate the self-disclosure patterns of
users and their subsequent OSN usage to
show that there is a positive and significant
relationship (Sec. 6).
2 Background
In this section, we review literature on the relevant
aspects of self-disclosure.
Self-disclosure (SD) level: To quantitatively
analyze self-disclosure, researchers categorize
self-disclosure language into three levels: G (gen-
eral) for no disclosure, M for medium disclosure,
and H for high disclosure (Vondracek and Von-
dracek, 1971; Barak and Gluck-Ofri, 2007). Ut-
terances that contain general (non-sensitive) infor-
mation about the self or someone close (e.g., a
family member) are categorized as M. Examples
are personal events, past history, or future plans.
Utterances about age, occupation and hobbies are
also included. Utterances that contain sensitive in-
formation about the self or someone close are cat-
egorized as H. Sensitive information includes per-
sonal characteristics, problematic behaviors, phys-
ical appearance and wishful ideas. Generally,
these are thoughts and information that one would
generally keep as secrets to himself. All other
utterances, those that do not contain information
about the self or someone close are categorized
as G. Examples include gossip about celebrities or
factual discourse about current events.
Classifying self-disclosure level: Prior work
on quantitatively analyzing self-disclosure has re-
lied on user surveys (Trepte and Reinecke, 2013;
Ledbetter et al., 2011) or human annotation (Barak
and Gluck-Ofri, 2007). These methods consume
much time and effort, so they are not suitable for
large-scale studies. In prior work closest to ours,
Bak et al. (2012) showed that a topic model can
be used to identify self-disclosure, but that work
applies a two-step process in which a basic topic
model is first applied to find the topics, and then
the topics are post-processed for binary classifica-
tion of self-disclosure. We improve upon this work
by applying a single unified model of topics and
??
??
??
CTN
??
?
?
?? 3?????? 3
Figure 1: Graphical model of SDTM
self-disclosure for high accuracy in classifying the
three levels of self-disclosure.
Self-disclosure and online social network:
According to social psychology, when someone
discloses about himself, he will receive social sup-
port from those around him (Wills, 1985; Derlega
et al., 1993), and this pattern of self-disclosure
and social support was verified for Twitter con-
versation data (Kim et al., 2012). Social support
is a major motivation for active usage of social
networks services (SNS), and there are findings
that show self-disclosure on SNS has a positive
longitudinal effect on future SNS use (Trepte and
Reinecke, 2013; Ledbetter et al., 2011). While
these previous studies focused on small, qualita-
tive studies, we conduct a large-scale, machine
learning driven study to approach the question of
self-disclosure behavior and SNS use.
3 Self-Disclosure Topic Model
This section describes our model, the self-
disclosure topic model (SDTM), for classifying
self-disclosure level and discovering topics for
each self-disclosure level.
3.1 Model
We make two important assumptions based on our
observations of the data. First, first-person pro-
nouns (I, my, me) are good indicators for medium
level of self-disclosure. For example, phrases such
as ?I live? or ?My age is? occur in utterances that re-
veal personal information. Second, there are top-
ics that occur much more frequently at a particular
SD level. For instance, topics such as physical
appearance and mental health occur frequently at
level H, whereas topics such as birthday and hob-
bies occur frequently at level M.
Figure 1 illustrates the graphical model of
SDTM and how these assumptions are embodied
43
Notation Description
G; M ; H {general; medium; high} SD level
C; T ; N Number of conversations; tweets;
words
K
G
;K
M
;K
H
Number of topics for {G; M; H}
c; ct Conversation; tweet in conversation c
y
ct
SD level of tweet ct, G or M/H
r
ct
SD level of tweet ct, M or H
z
ct
Topic of tweet ct
w
ctn
n
th
word in tweet ct
? Learned Maximum entropy parame-
ters
x
ct
First-person pronouns features
?
ct
Distribution over SD level of tweet ct
pi
c
SD level proportion of conversation c
?G
c
;?M
c
;?H
c
Topic proportion of {G; M; H} in con-
versation c
?G;?M ;?H Word distribution of {G; M; H}
?; ? Dirichlet prior for ?; pi
?G,?M ;?H Dirichlet prior for ?G;?M ;?H
n
cl
Number of tweets assigned SD level l
in conversation c
n
l
ck
Number of tweets assigned SD level l
and topic k in conversation c
n
l
kv
Number of instances of word v as-
signed SD level l and topic k
m
ctkv
Number of instances of word v as-
signed topic k in tweet ct
Table 1: Summary of notations used in SDTM.
in it. The first assumption about the first-person
pronouns is implemented by the observed variable
x
ct
and the parameters ? from a maximum en-
tropy classifier for G vs. M/H level. The second
assumption is implemented by the three separate
word-topic probability vectors for the three lev-
els of SD: ?
l
which has a Bayesian informative
prior ?
l
where l ? {G,M,H}, the three levels
of self-disclosure. Table 1 lists the notations used
in the model and the generative process, Figure 2
describes the generative process.
3.2 Classifying G vs M/H levels
Classifying the SD level for each tweet is done in
two parts, and the first part classifies G vs. M/H
levels with first-person pronouns (I, my, me). In
the graphical model, y is the latent variable that
represents this classification, and ? is the distri-
bution over y. x is the observation of the first-
person pronoun in the tweets, and? are the param-
eters learned from the maximum entropy classifier.
With the annotated Twitter conversation dataset
(described in Section 4.2), we experimented with
several classifiers (Decision tree, Naive Bayes)
and chose the maximum entropy classifier because
it performed the best, similar to other joint topic
models (Zhao et al., 2010; Mukherjee et al., 2013).
1. For each level l ? {G, M, H}:
For each topic k ? {1, . . . ,Kl}:
Draw ?lk ? Dir(?l)2. For each conversation c ? {1, . . . , C}:
(a) Draw ?Gc ? Dir(?)(b) Draw ?Mc ? Dir(?)(c) Draw ?Hc ? Dir(?)(d) Draw pic ? Dir(?)
(e) For each message t ? {1, . . . , T}:
i. Observe first-person pronouns features xct
ii. Draw ?ct ?MaxEnt(xct,?)
iii. Draw yct ? Bernoulli(?ct)
iv. If yct = 0 which is G level:
A. Draw zct ?Mult(?Gc )B. For each word n ? {1, . . . , N}:
Draw word wctn ?Mult(?Gzct)Else which can be M or H level:
A. Draw rct ?Mult(pic)
B. Draw zct ?Mult(?rctc )C. For each word n ? {1, . . . , N}:
Draw word wctn ?Mult(?rctzct)
Figure 2: Generative process of SDTM.
3.3 Classifying M vs H levels
The second part of the classification, the M and the
H level, is driven by informative priors with seed
words and seed trigrams.
Utterances with M level include two types:
1) information related with past events and fu-
ture plans, and 2) general information about self
(Barak and Gluck-Ofri, 2007). For the former, we
add as seed trigrams ?I have been? and ?I will?.
For the latter, we use seven types of information
generally accepted to be personally identifiable in-
formation (McCallister, 2010), as listed in the left
column of Table 2. To find the appropriate tri-
grams for those, we take Twitter conversation data
(described in Section 4.1) and look for trigrams
that begin with ?I? and ?my? and occur more than
200 times. We then check each one to see whether
it is related with any of the seven types listed in
the table. As a result, we find 57 seed trigrams for
M level. Table 2 shows several examples.
Type Trigram
Name My name is, My last name
Birthday My birthday is, My birthday party
Location I live in, I lived in, I live on
Contact My email address, My phone number
Occupation My job is, My new job
Education My high school, My college is
Family My dad is, My mom is, My family is
Table 2: Example seed trigrams for identifying M
level of SD. There are 51 of these used in SDTM.
Utterances with H level express secretive wishes
or sensitive information that exposes self or some-
one close (Barak and Gluck-Ofri, 2007). These are
44
Category Keywords
physical
appearance
acne, hair, overweight, stomach, chest,
hand, scar, thighs, chubby, head, skinny
mental/physical
condition
addicted, bulimia, doctor, illness, alco-
holic, disease, drugs, pills, anorexic
Table 3: Example words for identifying H level of
SD. Categories are hand-labeled.
generally keep as secrests. With this intuition, we
crawled 26,523 secret posts from Six Billion Se-
crets
1
site where users post secrets anonymously.
To extract seed words that might express secre-
tive personal information, we compute mutual in-
formation (Manning et al., 2008) with the secret
posts and 24,610 randomly selected tweets. We
select 1,000 words with high mutual information
and filter out stop words. Table 3 shows some of
these words. To extract seed trigrams of secretive
wishes, we again look for trigrams that start with
?I? or ?my?, occur more than 200 times, and select
trigrams of wishful thinking, such as ?I want to?,
and ?I wish I?. In total, there are 88 seed words
and 8 seed trigrams for H.
3.4 Inference
For posterior inference of SDTM, we use col-
lapsed Gibbs sampling which integrates out la-
tent random variables ?,pi,?, and ?. Then we
only need to compute y, r and z for each tweet.
We compute full conditional distribution p(y
ct
=
j
?
, r
ct
= l
?
, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x) for
tweet ct as follows:
p(y
ct
= 0, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x)
?
exp(?
0
? x
ct
)
?
1
j=0
exp(?
j
? x
ct
)
g(c, t, l
?
, k
?
)
p(y
ct
= 1, r
ct
= l
?
, z
ct
= k
?
|y
?ct
, r
?ct
, z
?ct
,w,x)
?
exp(?
1
? x
ct
)
?
1
j=0
exp(?
j
? x
ct
)
(?
l
?
+ n
(?ct)
cl
?
) g(c, t, l
?
, k
?
)
where z
?ct
, r
?ct
,y
?ct
are z, r,y without tweet
ct, m
ctk
?
(?)
is the marginalized sum over word v of
m
ctk
?
v
and the function g(c, t, l
?
, k
?
) as follows:
g(c, t, l
?
, k
?
) =
?(
?
V
v=1
?
l
?
v
+ n
l
?
?(ct)
k
?
v
)
?(
?
V
v=1
?
l
?
v
+ n
l
?
?(ct)
k
?
v
+m
ctk
?
(?)
)
(
?
k
?
+ n
l
?
(?ct)
ck
?
?
K
k=1
?
k
+ n
l
?
ck
)
V
?
v=1
?(?
l
?
v
+ n
l
?
?(ct)
k
?
v
+m
ctk
?
v
)
?(?
l
?
v
+ n
l
?
?(ct)
k
?
v
)
1
http://www.sixbillionsecrets.com
4 Data Collection and Annotation
To answer our research questions, we need a
large longitudinal dataset of conversations such
that we can analyze the relationship between self-
disclosure behavior and conversation frequency
over time. We chose to crawl Twitter because it
offers a practical and large source of conversations
(Ritter et al., 2010). Others have also analyzed
Twitter conversations for natural language and so-
cial media research (Boyd et al., 2010; Danescu-
Niculescu-Mizil et al., 2011), but we collect con-
versations from the same set of dyads over several
months for a unique longitudinal dataset.
4.1 Collecting Twitter conversations
We define a Twitter conversation as a chain of
tweets where two users are consecutively replying
to each other?s tweets using the Twitter reply but-
ton. We identify dyads of English-tweeting users
with at least twenty conversations and collect their
tweets. We use an open source tool for detect-
ing English tweets
2
, and to protect users? privacy,
we replace Twitter userid, usernames and url in
tweets with random strings. This dataset consists
of 101,686 users, 61,451 dyads, 1,956,993 conver-
sations and 17,178,638 tweets which were posted
between August 2007 to July 2013.
4.2 Annotating self-disclosure level
To measure the accuracy of our model, we ran-
domly sample 101 conversations, each with ten
or fewer tweets, and ask three judges, fluent in
English, to annotate each tweet with the level of
self-disclosure. Judges first read and discussed
the definitions and examples of self-disclosure
level shown in (Barak and Gluck-Ofri, 2007), then
they worked separately on a Web-based platform.
Inter-rater agreement using Fleiss kappa (Fleiss,
1971) is 0.67.
5 Classification of Self-Disclosure Level
This section describes experiments and results of
SDTM as well as several other methods for classi-
fication of self-disclosure level.
We first start with the annotated dataset in sec-
tion 4.2 in which each tweet is annotated with SD
level. We then aggregate all of the tweets of a
conversation, and we compute the proportions of
tweets in each SD level. When the proportion of
2
https://github.com/shuyo/ldig
45
tweets at M or H level is equal to or greater than 0.2,
we take the level of the larger proportion and as-
sign that level to the conversation. When the pro-
portions of tweets at M or H level are both less than
0.2, we assign G to the SD level.
We compare SDTM with the following methods
for classifying tweets for SD level:
? LDA (Blei et al., 2003): A Bayesian topic
model. Each conversation is treated as a doc-
ument. Used in previous work (Bak et al.,
2012).
? MedLDA (Zhu et al., 2012): A super-
vised topic model for document classifica-
tion. Each conversation is treated as a doc-
ument and response variable can be mapped
to a SD level.
? LIWC (Tausczik and Pennebaker, 2010):
Word counts of particular categories. Used
in previous work (Houghton and Joinson,
2012).
? Seed words and trigrams (SEED): Occur-
rence of seed words and trigrams which are
described in section 3.3.
? ASUM (Jo and Oh, 2011): A joint model of
sentiment and topic using seed words. Each
sentiment can be mapped to a SD level. Used
in previous work (Bak et al., 2012).
? First-person pronouns (FirstP): Occurrence
of first-person pronouns which are described
in section 3.2. To identify first-person pro-
nouns, we tagged parts of speech in each
tweet with the Twitter POS tagger (Owoputi
et al., 2013).
SEED, LIWC, LDA and FirstP cannot be used
directly for classification, so we use Maximum en-
tropy model with outputs of each of those models
as features. We run MedLDA, ASUM and SDTM
20 times each and compute the average accuracies
and F-measure for each level. We set 40 topics
for LDA, MedLDA and ASUM, 60; 40; 40 top-
ics for SDTM K
G
,K
M
and K
H
respectively, and
set ? = ? = 0.1. To incorporate the seed words
and trigrams into ASUM and SDTM, we initial-
ize ?
G
,?
M
and ?
H
differently. We assign a high
value of 2.0 for each seed word and trigram for
that level, and a low value of 10
?6
for each word
that is a seed word for another level, and a default
Method Acc G F
1
M F
1
H F
1
Avg F
1
LDA 49.2 0.000 0.650 0.050 0.233
MedLDA 43.3 0.406 0.516 0.093 0.338
LIWC 49.2 0.341 0.607 0.180 0.376
SEED 52.0 0.412 0.600 0.178 0.397
ASUM 56.6 0.320 0.704 0.375 0.466
FirstP 63.2 0.630 0.689 0.095 0.472
SDTM 64.5 0.611 0.706 0.431 0.583
Table 4: SD level classification accuracies and F-
measures using annotated data. Acc is accuracy,
and G F
1
is F-measure for classifying the G level.
Avg F
1
is the average value of G F
1
, M F
1
and H
F
1
. SDTM outperforms all other methods com-
pared. The difference between SDTM and FirstP
is statistically significant (p-value < 0.05 for ac-
curacy, < 0.0001 for Avg F
1
).
value of 0.01 for all other words. This approach
is same as other topic model works (Jo and Oh,
2011; Kim et al., 2013).
As Table 4 shows, SDTM performs better than
other methods by accuracy and F-measure. LDA
and MedLDA generally show the lowest perfor-
mance, which is not surprising given these mod-
els are quite general and not tuned specifically
for this type of semi-supervised classification task.
LIWC and SEED perform better than LDA, but
these have quite low F-measure for G and H lev-
els. ASUM shows better performance for classi-
fying H level than others, but not for classifying
the G level. FirstP shows good F-measure for the
G level, but the H level F-measure is quite low,
even lower than SEED. Finally, SDTM has sim-
ilar performance in G and M level with FirstP, but
it performs better in H level than others. Classi-
fying the H level well is important because as we
will discuss later, the H level has the strongest rela-
tionship with longitudinal OSN usage (see Section
6.2), so SDTM is overall the best model for clas-
sifying self-disclosure levels.
6 Self-Disclosure and Conversation
Frequency
In this section, we investigate whether there is a
relationship between self-disclosure and conversa-
tion frequency over time. (Trepte and Reinecke,
2013) showed that frequent or high-level of self-
disclosure in online social networks (OSN) con-
tributes positively to OSN usage, and vice versa.
They showed this through an online survey with
46
Facebook and StudiVZ users. With SDTM, we
can automatically classify self-disclosure level of
a large number of conversations, so we investi-
gate whether there is a similar relationship be-
tween self-disclosure in conversations and subse-
quent frequency of conversations with the same
partner on Twitter. More specifically, we ask the
following two questions:
1. If a dyad displays high SD level in their con-
versations at a particular time period, would
they have more frequent conversations subse-
quently?
2. If a dyad shows high conversation frequency
at a particular time period, would they dis-
play higher SD in their subsequent conver-
sations?
6.1 Experiment Setup
We first run SDTM with all of our Twitter con-
versation data with 150; 120; 120 topics for
SDTM K
G
,K
M
and K
H
respectively. The
hyper-parameters are the same as in section 5. To
handle a large dataset, we employ a distributed al-
gorithm (Newman et al., 2009).
Table 5 shows some of the topics that were
prominent in each SD level by KL-divergence. As
expected, G level includes general topics such as
food, celebrity, soccer and IT devices, M level in-
cludes personal communication and birthday, and
finally, H level includes sickness and profanity.
For comparing conversation frequencies over
time, we divided the conversations into two sets
for each dyad. For the initial period, we include
conversations from the dyad?s first conversation to
60 days later. And for the subsequent period,
we include conversations during the subsequent 30
days.
We compute proportions of conversation for
each SD level for each dyad in the initial and
subsequent periods. Also, we define a new mea-
surement, SD level score for a dyad in the period,
which is a weighted sum of each conversation with
SD levels mapped to 1, 2, and 3, for the levels G,
M, and H, respectively.
6.2 Does self-disclosure lead to more frequent
conversations?
We investigate the effect of the level self-
disclosure on long-term use of OSN. We run lin-
ear regression with the intial SD level score as
1.0 1.5 2.0 2.5 3.0Initial SD level
1.0
0.5
0.0
0.5
1.0
1.5
# C
onv
ersa
ctio
n ch
ang
es p
rop
orti
on o
ver 
tim
e
Figure 3: Relationship between initial SD level
and conversation frequency changes over time.
The solid line is the linear regression line, and the
coefficient is 0.118 with p < 0.001, which shows
a significant positive relationship.
G level M level H level
Coeff (?) 0.094 0.419 0.464
p-value 0.1042 < 0.0001 < 0.0001
Table 6: Relationship between initial SD level
proportions and changes in conversation fre-
quency. For M and H levels, there is significant
positive relationship (p < 0.0001), but for the G
level, there is not (p > 0.1).
the independent variable, and the rate of change
in conversation frequency between initial period
and subsequent period as the dependent variable.
The result of regression is that the independent
variable?s coefficient is 0.118 with a low p-value
(p < 0.001). Figure 3 shows the scatter plot with
the regression line, and we can see that the slope
of regression line is positive.
We also investigate the importance of each SD
level for changes in conversation frequency. We
run linear regression with initial proportions of
each SD level as the independent variable, and
the same dependent variable as above. As ta-
ble 6 shows, there is no significant relationship
between the initial proportion of the G level and
the changes in conversation frequency (p > 0.1).
But for the M and H levels, the initial proportions
show positive and significant relationships with
the subsequent changes to the conversation fre-
quency (p < 0.0001). These results show that M
and H levels are correlated with changes to the fre-
quency of conversation.
47
G level M level H level
101 184 176 36 104 82 113 33 19
chocolate obama league send twitter going ass better lips
butter he?s win email follow party bitch sick kisses
good romney game i?ll tumblr weekend fuck feel love
cake vote season sent tweet day yo throat smiles
peanut right team dm following night shit cold softly
milk president cup address account dinner fucking hope hand
sugar people city know fb birthday lmao pain eyes
cream good arsenal check followers tomorrow shut good neck
Table 5: High ranked topics in each level by comparing KL-divergence with other level?s topics
0 20 40 60 80 100Initial conversation frequency
1.80
1.85
1.90
1.95
2.00
2.05
Sub
seq
uen
t SD
 lev
el
Figure 4: Relationship between initial conversa-
tion frequency and subsequent SD level. The
solid line is the linear regression line, and the co-
efficient is 0.0016 with p < 0.0001, which shows
a significant positive relationship.
6.3 Does high frequency of conversation lead
to more self-disclosure?
Now we investigate whether the initial conversa-
tion frequency is correlated with the SD level in
the subsequent period. We run linear regression
with the initial conversation frequency as the inde-
pendent variable, and SD level in the subsequent
period as the dependent variable.
The regression coefficient is 0.0016 with low p-
value (p < 0.0001). Figure 4 shows the scatter
plot. We can see that the slope of the regression
line is positive. This result supports previous re-
sults in social psychology (Leung, 2002) that fre-
quency of instant chat program ICQ and session
time were correlated to depth of SD in message.
7 Conclusion and Future Work
In this paper, we have presented the self-disclosure
topic model (SDTM) for discovering topics and
classifying SD levels from Twitter conversation
data. We devised a set of effective seed words and
trigrams, mined from a dataset of secrets. We also
annotated Twitter conversations to make a ground-
truth dataset for SD level. With annotated data, we
showed that SDTM outperforms previous methods
in classification accuracy and F-measure.
We also analyzed the relationship between SD
level and conversation frequency over time. We
found that there is a positive correlation between
initial SD level and subsequent conversation fre-
quency. Also, dyads show higher level of SD if
they initially display high conversation frequency.
These results support previous results in social
psychology research with more robust results from
a large-scale dataset, and show importance of
looking at SD behavior in OSN.
There are several future directions for this re-
search. First, we can improve our modeling for
higher accuracy and better interpretability. For
instance, SDTM only considers first-person pro-
nouns and topics. Naturally, there are patterns
that can be identified by humans but not captured
by pronouns and topics. Second, the number of
topics for each level is varied, and so we can
explore nonparametric topic models (Teh et al.,
2006) which infer the number of topics from the
data. Third, we can look at the relationship be-
tween self-disclosure behavior and general online
social network usage beyond conversations.
Acknowledgments
We thank the anonymous reviewers for helpful
comments. Alice Oh was supported by the IT
R&D Program of MSIP/KEIT. [10041313, UX-
oriented Mobile SW Platform]
48
References
JinYeong Bak, Suin Kim, and Alice Oh. 2012. Self-
disclosure and relationship strength in twitter con-
versations. In Proceedings of ACL.
Azy Barak and Orit Gluck-Ofri. 2007. Degree and
reciprocity of self-disclosure in online forums. Cy-
berPsychology & Behavior, 10(3):407?417.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010.
Tweet, tweet, retweet: Conversational aspects of
retweeting on twitter. In Proceedings of HICSS.
Cristian Danescu-Niculescu-Mizil, Michael Gamon,
and Susan Dumais. 2011. Mark my words!: Lin-
guistic style accommodation in social media. In
Proceedings of WWW.
Valerian J. Derlega, Sandra Metts, Sandra Petronio,
and Stephen T. Margulis. 1993. Self-Disclosure,
volume 5 of SAGE Series on Close Relationships.
SAGE Publications, Inc.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
David J Houghton and Adam N Joinson. 2012.
Linguistic markers of secrets and sensitive self-
disclosure in twitter. In Proceedings of HICSS.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of WSDM.
Adam N Joinson and Carina B Paine. 2007. Self-
disclosure, privacy and the internet. The Oxford
handbook of Internet psychology, pages 237?252.
Sidney M Jourard. 1971. Self-disclosure: An experi-
mental analysis of the transparent self.
Suin Kim, JinYeong Bak, and Alice Haeyun Oh. 2012.
Do you feel what i feel? social aspects of emotions
in twitter conversations. In Proceedings of ICWSM.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, and
Shixia Liu. 2013. A hierarchical aspect-sentiment
model for online reviews. In Proceedings of AAAI.
Andrew M Ledbetter, Joseph P Mazer, Jocelyn M DeG-
root, Kevin R Meyer, Yuping Mao, and Brian Swaf-
ford. 2011. Attitudes toward online social con-
nection and self-disclosure as predictors of facebook
communication and relational closeness. Communi-
cation Research, 38(1):27?53.
Louis Leung. 2002. Loneliness, self-disclosure, and
icq (? i seek you?) use. CyberPsychology & Behav-
ior, 5(3):241?251.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Erika McCallister. 2010. Guide to protecting the confi-
dentiality of personally identifiable information. DI-
ANE Publishing.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Sharon Meraz. 2013. Public dialogue: Analysis of
tolerance in online discussions. In Proceedings of
ACL.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms
for topic models. Journal of Machine Learning Re-
search, 10:1801?1828.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of HLT-NAACL.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of HLT-NAACL.
Charles Steinfield, Nicole B Ellison, and Cliff Lampe.
2008. Social capital, self-esteem, and use of on-
line social network sites: A longitudinal analy-
sis. Journal of Applied Developmental Psychology,
29(6):434?445.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of Language
and Social Psychology.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the american statistical associ-
ation, 101(476).
Sabine Trepte and Leonard Reinecke. 2013. The re-
ciprocal effects of social network site use and the
disposition for self-disclosure: A longitudinal study.
Computers in Human Behavior, 29(3):1102 ? 1112.
Sarah I Vondracek and Fred W Vondracek. 1971. The
manipulation and measurement of self-disclosure in
preadolescents. Merrill-Palmer Quarterly of Behav-
ior and Development, 17(1):51?58.
Thomas Ashby Wills. 1985. Supportive functions
of interpersonal relationships. Social support and
health, xvii:61?82.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
EMNLP.
Jun Zhu, Amr Ahmed, and Eric P Xing. 2012. Medlda:
maximum margin supervised topic models. Journal
of Machine Learning Research, 13:2237?2278.
49
