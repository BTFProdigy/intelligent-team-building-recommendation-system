Proceedings of the NAACL HLT 2012 Student Research Workshop, pages 41?47,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Using Ontology-based Approaches to Representing Speech Transcripts for Automated Speech Scoring   Miao Chen School of Information Studies Syracuse University Syracuse, NY 13244, USA mchen14@Syr.edu       Abstract 
This paper presents a thesis proposal on ap-proaches to automatically scoring non-native speech from second language tests. Current speech scoring systems assess speech by pri-marily using acoustic features such as fluency and pronunciation; however content features are barely involved. Motivated by this limita-tion, the study aims to investigate the use of content features in speech scoring systems. For content features, a central question is how speech content can be represented in appro-priate means to facilitate automated speech scoring. The study proposes using ontology-based representation to perform concept level representation on speech transcripts, and fur-thermore the content features computed from ontology-based representation may facilitate speech scoring. One baseline and two ontolo-gy-based representations are compared in ex-periments. Preliminary results show that ontology-based representation slightly im-proves performance of one content feature for automated scoring over the baseline system. 
1 Introduction With increasing number of language learners tak-ing second language tests, the resulting responses add a huge burden to testing agencies, and thus automated scoring has become a necessity for effi-ciency and objectivity. Speaking, an important as-pect for assessing second language speakers? proficiency, is selected as the context of the study. 
The general goal is to investigate new approaches to automatic scoring of second language speech. When giving a speaking test in computer-mediated environment, test-takers? responses are typically recorded as speech files. These files can be considered to contain two layers: sound and text. The sound is about the acoustic side of speech, whose features have been used to assess speaking proficiency in existing automated speech-scoring systems (Dodigovic, 2009; Zechner et al, 2009). However, the text side, which is about the content of speech, is by far not well addressed in scoring systems, mainly due to the imperfect per-formance of automatic speech recognizer systems. As content is an integral part of speech, adding content features to existing scoring systems may further enhance system performance, and thus this study aims to examine the use of content features in speech scoring systems. In order to acquire speech content, speech files need to be transcribed to text files, by human or Automatic Speech Recognition (ASR). The result-ed text files, namely, speech transcripts, are to be processed to extract content features. Moreover, representation of text content (e.g. in vectors) is important because it is the pre-requisite for compu-ting content features and building speech scoring models. Therefore this study focuses on represent-ing content of speech transcripts to facilitate auto-matic scoring of speech. Speech transcripts can be seen as a special type of text documents, and therefore document repre-sentation approaches shed light on representation of speech transcripts, such as Salton et al (1975), 
41
Deerwester et al (1990), Lewis (1992), Kaski (1997), He et al (2004), Arguello et al (2008), Hotho et al (2003a). On the other hand, written essays, the output of writing section of second lan-guage test, share great similarity with speech tran-scripts, and representation of essays also has implications on speech transcript representation, such as Burstein (2003), Attali & Burstein (2006), and Larkey & Croft (2003).  Existing document representation approaches are primarily statistical and corpus based, using words or latent variables mined from corpus as representation units in the vector. These approach-es exhibit two challenges: 1) meaningfulness of representation units. For example, synonymous words represent similar meaning and thus should be grouped as one representation unit. 2) unknown terms. Since words or latent variables in the vector are from training corpus, if an unknown term oc-curs in the testing corpus then it is difficult to de-termine the importance of the term in the training corpus because there is no prior knowledge of it in the training corpus. Ontology concepts, representation units at the concept level, have been less employed in content representation. Hotho et al (2003a) claim that on-tology concepts can help reveal concepts and se-mantics in documents, and thus we hypothesize ontology-based representation may facilitate ob-taining better content features for speech scoring. Ontologies can also complement the abovemen-tioned shortcomings of statistical and corpus based representations by providing meaningful represen-tation units and reasoning power between con-cepts.  The study compares baseline (statistical and corpus based) and ontology-based approaches. The criterion is representing the same speech tran-scripts using these approaches, computing content features based on the representations, and compar-ing performance of content features in predicting speaking proficiency.  2 Related Work  This section reviews work related to representation of speech transcripts, including document repre-sentation, essay scoring, and ontology-based repre-sentation in text processing.  Document representation has been an important topic in research areas such as natural language 
processing, information retrieval, and text mining, in which a number of representation approaches have been proposed.  The most common practice of text document representation is the Bag-Of-Words (BOW) ap-proach, illustrated in Salton et al (1975). The basic idea is that a document can be represented as a vector of words, with each dimension of the vector standing for one single word. Besides using explic-it words from documents, latent variables derived from document mining can be used for document representation as well, such as the Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) approaches. The representation units are latent concepts or topics and the documents are projected to the semantic space constructed from the latent concepts or topics (Deerwester et al, 1990; Blei, 2012). An important purpose of using the latent variables is to reduce dimensions in doc-ument representation and place documents in a more compact space. Some other dimension reduc-tion techniques include Subspace-based methods (Chen et al, 2003) and Self Organizing Map (Kaski 1997; Kaski, et al 1998). In the area of automatic essay scoring, essay content are represented to facilitate the scoring. The BOW approach is widely used in essay repre-sentation as well, including the e-rater system (Burstein, 2003; Attali & Burstein, 2006) and the experimental system in Larkey & Croft (2003). Representation in the BETSY system (Bayesian Essay Test Scoring System) also involves words, such as frequency of content words, along with specific phrases (Dikli, 2006). The exemplar sys-tem employing LSA representation is the Intelli-gent Essay Assessor system, which performs LSA on training essays and then projects essays to the vector space of latent concepts (Landauer et al, 2003). Besides representation approaches, content fea-ture computing in essay scoring is useful to content scoring of speech because they share great simi-larity. Content features can be derived by compu-ting cosine similarity between essay vectors, such as in e-rater (Attali & Burstein, 2006) and Intelli-gent Essay Assessor (Landauer et al, 2003). The e-rater content feature computing is adopted in this study to compute content features of speech tran-scripts. As mentioned in section 1, ontologies can be used to complement the challenges of statistical 
42
representation approaches. Ontology concepts have been successfully used in text processing tasks such as text classification and clustering and can help resolve the first challenge, meaningfulness of the representation. Hotho and Bloehdorn along with other people conducted a series of studies in using the ontologies (i.e. WordNet) for text catego-rization and clustering tasks (Bloehdorn & Hotho, 2004; Hotho et al, 2003a; Hotho et al, 2003b). The goals are to overcome several weaknesses, e.g. synonyms and generalization issues, of the bag-of-words representation by using ontology concept based representation. Basically concepts from on-tologies are used as units for text representation and then text processing is performed on top of the ontology-based representation. Explicit Semantic Approach (ESA), proposed by Gabrilovich and Markovitch (2007), is an approach to representing an arbitrary text snippet in vector of Wikipedia concepts for the convenience of further natural language processing. Each Wikipedia concept has text description, which is used to build an invert index to associate words with concepts. The invert index helps represent each word by vector of Wik-ipedia concepts, and eventually a document can be represented by weighted Wikipedia concepts by adding up the Wikipedia concept vectors of the words that the document contains.  Ontologies can also help resolve the second challenge of statistical representation, the unknown term issue. If a term occurs in the testing corpus but not the training corpus, then the importance of the term can be inferred from external knowledge such as ontologies. The semantic relations defined in ontologies connect relevant concepts and organ-ize them into a tree (i.e. WordNet) or a graph struc-ture (i.e. Wikipedia). Since paths usually exist between two individual concepts, ontologies can support inferences among concepts by using the paths and concept nodes between them. Moreover, semantic similarity between concepts, computed based on ontology knowledge, can be used to infer importance of unknown terms. WordNet (Fellbaum, 1998) and Wikipedia (Wikipedia, 2012) ontologies are two popular on-tologies for computing semantic similarity. A number of similarity approaches have been pro-posed for similarity calculation according to the different characteristics of the two ontologies (Lin, 1998; Pedersen et al, 2004; Resnik, 1999; Strube & Ponzetto, 2006).  
3 Methodology Experiments are conducted to compare ontology-based representations (experimental systems) and common representations (baseline systems). Two ontology-based methods are employed as the ex-perimental systems, one is about representing tran-scripts using ontology concepts (?ONTO?), and the other is about inferring weights of unknown terms using ontologies (?OntoReason?). For the baseline, we identify the BOW representation as a common text representation and use it in the baseline sys-tem. 3.1 Data Set The data set comes from an English proficiency test for non-native speakers. For the speaking sec-tion, test takers are asked to provide spontaneous speech responses to the prompts1 (test tasks). There are 4 prompts in the data set, all of which are inte-grated prompts. An integrated prompt is a test task that first provides test takers some materials to read or listen and then asks them to provide opinions or arguments towards the materials. The responses are then scored holistically by human raters based on a scoring rubric on a scale of 1 to 4, 4 being the highest score. For each score level, the scoring ru-bric contains guidelines of expected performance on various aspects of speaking ability such as pro-nunciation, fluency, and content.  The data set contains 1243 speech samples from 327 speakers in total. Manual and automatic meth-ods are used to obtain transcripts of the speech samples. For the manual way, each response is verbatim transcribed by human; and for the auto-matic way, each response is automatically tran-scribed by ASR with word error rate of 12.8%. Therefore two sets of transcripts are derived for the speech responses, the human transcripts set and the ASR set.  Since the representation approaches are prompt-specific in the study, meaning vector representa-tions are generated for each prompt, the data set is first split by prompts and then responses are split into training and testing sets within each prompt. Table 1 shows size of the data set and subsets:                                                              1 Prompts are test tasks assigned to test takers to elicit their speaking responses. 2 The feature is referred to as ?cos.w/6? in Attali and Burstein (2006) because there are usually 6 score levels, while here our 
43
Prompt Training Set Test Set Total A 143 176 319 (4/79/158/78) B 140  168 308 (7/86/146/69) C 139  172 311 (4/74/154/79) D 137  168 305 (8/75/141/81) Table 1. Size of data set and subsets. The numbers in parentheses are the number of documents on score lev-els 1-4. 3.2 Representation Approaches of Speech Transcripts One baseline approach and two ontology-based approaches are briefly introduced here and imple-mented in experiments. The approaches are used to generate vectors for computing content features. We also plan to employ other approaches in the future, as described in section 5.  3.2.1 Bag-of-words (baseline) It takes the view that essays can be represented in vector of words and the value of a word in a vector refers to its weighting on this dimension. It uses the representation method in the e-rater as well, including document-level representation for testing documents and score-level representation for train-ing documents (Attali & Burstein, 2006).  Within each prompt, each testing transcript is converted to a vector (document level representa-tion); training transcripts are grouped by their score levels and for each score level a vector is generated by aggregating all transcripts of this score level (score level representation). We decide to use the tfidf weighting schema with stop words removed after tuning options of the parameters. 3.2.2 Ontology-based Representation (experi-mental) ONTO-WordNet approach. Concepts from ontolo-gy are identified in speech transcripts and then used to generate concept-level vectors. In practice, concept mapping in transcripts varies according to characteristics of ontologies. The WordNet ontolo-gy, containing mostly single words, is used as one case in the study. In the future, we plan to try the Wikipedia ontology, which contains more phrases-based concepts, for ontology-based representation.  Synsets, groups of synonyms, are concepts in WordNet and used as ontology concepts here. Document text is split by whitespace and punctua-tions to a set of words. Then the words are 
matched to WordNet synsets. As a word may have multiple senses (synsets), it is necessary to decide which synset to use in WordNet. Therefore we try two sense selection strategies as in Hotho et al?s (2003a) study: 1) simply use the first sense in WordNet; and 2) do part-of-speech tagging on sen-tences and find the corresponding sense in Word-Net. We find the 1st strategy obtains better performance than the 2nd one and thus decide to use the 1st one. When constructing ontology-based vector, we include both concepts and words in the vector. 3.2.3 Ontology-based Reasoning (experimental) OntoReason-WordNet approach. This approach is also implemented by using WordNet. First, tran-scripts are represented by ontology concepts as in section 3.2.2. Then given an unknown concept in test transcripts, we identify its semantically similar concepts (N=5) in the training transcripts and then reason the weight of the unknown concept based on the weights of these similar concepts.  The reasoning makes use of semantic similarity between WordNet synsets. Concept similarity is computed using the edge-based path similarity (Pedersen et al, 2004). We select N=5 concepts from the training transcripts that are most similar to the unknown concept, and compute the weight of the unknown concept in the training transcripts by averaging the weights of the 5 similar concepts. 3.3 Content Feature Computation The baseline and experimental systems all generate vector representations for speech transcripts. The content features are computed based on vector rep-resentation, and all representation approaches em-ploy the same method of computing content features. We choose to use the two content features of the e-rater system, ?max.cos? and ?cos.w4?, as the feature computation method2 (Attali & Burstein, 2006).  The max.cos feature. This feature identifies which score level of training documents the testing document is closest to. It computes and compares the similarity between the test document and train-ing documents of each score level in vector space, and then makes the score level whose training doc-                                                            2 The feature is referred to as ?cos.w/6? in Attali and Burstein (2006) because there are usually 6 score levels, while here our data has 4 score levels therefore it is written as ?cos.w4?. 
44
uments are most similar to the test document as the feature value. The cos.w4 feature. This feature computes con-tent similarity between the test document and the highest level training documents in vector space. Since score 4 is the highest level in our data set of spoken responses, we compute the cosine similari-ty between the test vector and the score level 4 vector as the feature value. Given a speech transcript from the test set, we first convert it to a vector using one of the repre-sentation approaches, and then compute the max.cos and cos.w4 feature values as its content features.  3.4 Evaluation Representation approaches are evaluated based on their performance in predicting speaking proficien-cy of test takers. More specifically, a representa-tion approach generates a vector representation using specific representation units (e.g. words, concepts); for each test transcript, two content fea-tures are computed based on the vector representa-tion; Pearson correlation r is computed between each content feature and speaking proficiency to indicate the predictiveness of the content feature resulting from a specific representation. Higher correlation indicates higher predictiveness on speaking proficiency. Lastly, we compare content feature correlations of different representation ap-proaches. We consider that the higher the correla-tion is, the better the representation approach is. 4 Experiment Results    
In the preliminary stage, the BOW (baseline), ONTO-WordNet and OntoReason-WordNet (ex-perimental) approaches are implemented. Mean-while parameters are optimized to acquire the best parameter setup for each approach. Since the speech files are transcribed by both human and ASR, same experiments are run on both data sets to compare representation performance on differ-ent transcriptions. The correlations of the two con-tent features to speaking proficiency are computed for each representation. Tables 2 and 3 show corre-lations of the max.cos and cos.w4 features respec-tively: For the max.cos feature, the average correlation of the ONTO-WordNet approach outperforms the BOW baseline slightly but the correlation drops dramatically when using the OntoReason-WordNet approach, for both the human and ASR transcripts. For the cos.w4 feature, the average correlation of the ONTO-WordNet approach outperforms the BOW, and the OntoReason-WordNet further out-performs the ONTO-WordNet approach, for both the human and ASR transcripts. It shows some ev-idence that ontology-based representation can im-prove performance of both content features; the ontology-based reasoning increases performance of the cos.w4 feature but decreases the max.cos fea-ture correlation. Comparing the performance on human vs. ASR transcripts, the features extracted from the human transcripts exhibit better average correlations than the corresponding features from the ASR tran-scripts. The results also show that the correlation difference between human and ASR transcripts is moderate. It may indicate that the representation approaches can be employed on ASR transcripts to further automate the speech scoring process. 
Prompt Hum, BOW Hum, ONTO-WordNet Hum, Onto-Reason-WordNet ASR, BOW ASR, ONTO-WordNet ASR, Onto-Reason-WordNet A 0.320 0.333 0.038 0.293 0.286 0.014 B 0.348 0.352 0.350 0.308 0.338 0.339 C 0.366 0.373 0.074 0.396 0.386 0.106 D 0.343 0.323 0.265 0.309 0.309 0.265 Average 0.344 0.345 0.182 0.327 0.330 0.181 Table 2. Correlations between the max.cos feature and speaking proficiency (Hum=using human transcriptions; ASR=using ASR hypotheses).     
45
Prompt Hum, BOW Hum, ONTO-WordNet Hum, Onto-Reason-WordNet ASR, BOW ASR, ONTO-WordNet ASR, Onto-Reason-WordNet A 0.427 0.429 0.434 0.409 0.416 0.411 B 0.295 0.303 0.327 0.259 0.278 0.292 C 0.352 0.385 0.402 0.338 0.366 0.380 D 0.368 0.385 0.389 0.360 0.379 0.374 Average 0.361 0.376 0.388 0.342 0.360 0.364 Table 3. Correlations between the cos.w4 feature and speaking proficiency (Hum=using human transcriptions; ASR=using ASR hypotheses) 5 Future Work For future work, we will implement one more baseline (LSA) and two more ontology-based ap-proaches (ONTO-Wikipedia and OntoReason-Wikipedia) and analyze their performance. Latent semantic analysis (LSA). LSA decom-poses a term-by-document matrix generated from training transcripts to three sub-matrices. Then given a test transcript, documents can be projected to the latent semantic space based on the three sub-matrices. The rank k parameter needs to be decided as a parameter for dimensionality reduction pur-pose by tuning it on the training data.  Using Wikipedia as another case for ontology, two more experimental approaches will be imple-mented, one for ontology-based representation and the other for ontology-based reasoning.  ONTO-Wikipedia. Wikipedia concepts can be identified in transcripts in two ways: 1) directly find concepts in text window of 5 words; 2) con-vert a transcript in vectors of Wikipedia concepts using the Explicit Semantic Analysis method, which associates words to Wikipedia concepts and represents arbitrary text using the word-concept associations (Gabrilovich and Markovitch, 2007).  OntoReason-Wikpedia. The concept similarity between Wikipedia concepts is obtained by com-puting the cosine similarity of the text description of the concepts. The reasoning method of the un-known concept follows the one mentioned in the OntoReason-WordNet approach.  We will compute content features based on the-se new representations and evaluate the perfor-mance according to feature correlations. The current results examine effects of using the Word-Net ontology on predicting speaking proficiency, and these new experiments will answer whether the other type of ontology, Wikipedia, has positive effect in speaking proficiency prediction. We will 
also compare the effects of using different ontolo-gies for ontology-based representations. The study has implications on effects of differ-ent speech transcript representations in predicting speaking proficiency. Since content features are less well explored in automatic speech scoring compared to acoustic features, it also contributes to the understanding of the use and effects of content features in speech scoring. Acknowledgments The author would like to than Drs. Klaus Zech-ner and Jian Qin for their tremendous help and support on the dissertation study.  References  Arguello, J., Elsas, J. L., Callan, J., & Carbonell, J. G. (2008). Document representation and query expan-sion models for blog recommendation. Proceedings of the Second International Conference on Weblogs and Social Media (ICWSM 2008). Attali, Y., & Burstein, J. (2006). Automated essay scor-ing with e-rater? V. 2. The Journal of Technology, Learning and Assessment, 4(3). Blei, D. (2012). Introduction to probabilistic topic mod-els. Communications of the ACM, 77-84. Bloehdorn, S., & Hotho, A. (2004). Boosting for text classification with semantic features. Workshop on mining for and from the semantic web at the 10th ACM SIGKDD conference on knowledge discovery and data mining (KDD 2004).  Burstein, J. (2003). The E-rater? scoring engine: Au-tomated essay scoring with natural language pro-cessing. In M. D. Shermis, Burstein, J.C. (Ed.), Automated essay scoring: A cross-disciplinary per-spective (pp. 113-121). Mahwah, NJ: Lawrence Erl-baum Associates, Inc. Chen, L., Tokuda, N., & Nagai, A. (2003). A new dif-ferential LSI space-based probabilistic document classifier. Information Processing Letters, 88(5), 203-212. Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990). Indexing by latent 
46
semantic analysis. Journal of the American Society for information science, 41(6), 391-407. Dikli, S. (2006). An overview of automated scoring of essays. The Journal of Technology, Learning and As-sessment, 5(1). Dodigovic, M. (2009). Speech Processing Technology in Second Language Testing. Proceedings of the Conference on Language & Technology 2009.  Fellbaum, C. (Ed.). (1998). WordNet: An electronic lexical database. Cambridge, MA: The MIT press. Gabrilovich, E., & Markovitch, S. (2007). Computing semantic relatedness using wikipedia-based explicit semantic analysis. Proceedings of the 20th Interna-tional Joint Conference on Artificial Intelligence.  He, X., Cai, D., Liu, H., & Ma, W. Y. (2004). Locality preserving indexing for document representation. Proceedings of the 27th Annual International ACM SIGIR Conference.  Hotho, A., Staab, S., & Stumme, G. (2003a). Ontologies improve text document clustering. Proceedings of the Third IEEE International Conference on Data Mining (ICDM?03).  Hotho, A., Staab, S., & Stumme, G. (2003b). Text clus-tering based on background knowledge (Technical report, no.425.): Institute of Applied Informatics and Formal Description Methods AIFB, University of Karlsruche. Kaski, S. (1997). Computationally efficient approxima-tion of a probabilistic model for document represen-tation in the WEBSOM full-text analysis method. Neural processing letters, 5(2), 69-81. Kaski, S., Honkela, T., Lagus, K., & Kohonen, T. (1998). WEBSOM-Self-organizing maps of docu-ment collections1. Neurocomputing, 21(1-3), 101-117. Landauer, T. K., Laham, D., & Foltz, P. W. (2003). Au-tomated scoring and annotation of essays with the In-telligent Essay Assessor. In M. D. Shermis, Burstein, J.C. (Ed.), Automated essay scoring: A cross-disciplinary perspective (pp. 87?112). Mahwah, NJ: Lawrence Erlbaum Associates, Inc. Larkey, L. S., & Croft, W. B. (2003). A Text Categori-zation Approach to Automated Essay Grading. In M. D. Shermis & J. C. Burstein (Eds.), Automated Essay Scoring: A Cross-discipline Perspective: Mahwah, NJ, Lawrence Erlbaum. Lewis, D. D. (1992). Representation and learning in information retrieval. (Doctoral dissertation). Uni-versity of Massachusetts. Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet:: Similarity: measuring the relatedness of concepts. Proceedings of the Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-04).  Resnik, P. (1999). Semantic similarity in a taxonomy: An information-based measure and its application to 
problems of ambiguity in natural language. Journal of Artificial Intelligence, 11(1999), 95-130. Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic indexing. Communica-tions of the ACM, 18(11), 613-620. Strube, M., & Ponzetto, S. P. (2006). WikiRelated! Computing semantic relatedness using Wikipedia. Proceedings of the American Association for Artifi-cial Intelligence 2006, Boston, MA. Wikipedia: The free encyclopedia. (2012, Apr 1). FL: Wikimedia Foundation, Inc. Retrieved Apr 1, 2012, from http://www.wikipedia.org Zechner, K., Higgins, D., Xi, X., & Williamson, D. M. (2009). Automatic scoring of non-native spontaneous speech in tests of spoken English. Speech Communi-cation, 51(10), 883-895.  
47
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 722?731,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
    Computing and Evaluating Syntactic Complexity Features for  
              Automated Scoring of Spontaneous Non-Native Speech  
    
 
Miao Chen Klaus Zechner 
School of Information Studies NLP & Speech Group 
Syracuse University Educational Testing Service 
Syracuse, NY, USA Princeton, NJ, USA 
mchen14@syr.edu kzechner@ets.org 
 
 
 
 
Abstract 
This paper focuses on identifying, extracting 
and evaluating features related to syntactic 
complexity of spontaneous spoken responses as 
part of an effort to expand the current feature 
set of an automated speech scoring system in 
order to cover additional aspects considered 
important in the construct of communicative 
competence. 
Our goal is to find effective features, se-
lected from a large set of features proposed 
previously and some new features designed in 
analogous ways from a syntactic complexity 
perspective that correlate well with human rat-
ings of the same spoken responses, and to build 
automatic scoring models based on the most 
promising features by using machine learning 
methods. 
On human transcriptions with manually 
annotated clause and sentence boundaries, our 
best scoring model achieves an overall Pearson 
correlation with human rater scores of r=0.49 
on an unseen test set, whereas correlations of 
models using sentence or clause boundaries 
from automated classifiers are around r=0.2. 
1 Introduction 
Past efforts directed at automated scoring of 
speech have used mainly features related to fluen 
cy (e.g., speaking rate, length and distribution of 
pauses), pronunciation (e.g., using log-likelihood 
scores from the acoustic model of an Automatic 
Speech Recognition (ASR) system), or prosody 
(e.g., information related to pitch  contours or syl-
lable stress)  (e.g., Bernstein, 1999; Bernstein et 
al., 2000; Bernstein et al, 2010; Cucchiarini et al, 
1997; Cucchiarini et al, 2000; Franco et al, 2000a; 
Franco et al, 2000b; Zechner et al, 2007, Zechner 
et al, 2009). 
While this approach is a good match to most of 
the important properties related to low entropy 
speech (i.e., speech which is highly predictable), 
such as reading a passage aloud, it lacks many im-
portant aspects of spontaneous speech which are 
relevant to be evaluated both by a human rater and 
an automated scoring system. Examples of such 
aspects of speech, which are considered part of the 
construct1 of ?communicative competence (Bach-
man, 1990), include grammatical accuracy, syntac-
tic complexity, vocabulary diversity, and aspects of 
spoken discourse structure, e.g., coherence and 
cohesion. These different aspects of speaking pro-
ficiency are often highly correlated in a non-native 
speaker (Xi and Mollaun, 2006; Bernstein et al, 
2010), and so scoring models built solely on fea-
tures of fluency and pronunciation may achieve 
reasonably high correlations with holistic human 
rater scores. However, it is important to point out 
that such systems would still be unable to assess 
many important aspects of the speaking construct 
and therefore cannot be seen as ideal from a validi-
ty point of view.2
The purpose of this paper is to address one of 
these important aspects of spoken language in 
more detail, namely syntactic complexity. This 
paper can be seen as a first step toward including 
 
                                                          
1  A construct is a set of knowledge, skills, and abilities 
measured by a test. 
2 ?Construct validity? refers to the extent that a test measures 
what it is designed to measure, in this case, communicative 
competence via speaking. 
722
features related to this part of the speaking con-
struct into an already existing automated speech 
scoring system for spontaneous speech which so 
far mostly uses features related to fluency and pro-
nunciation (Zechner et al, 2009). 
We use data from the speaking section of the 
TOEFL? Practice Online (TPO) test, which is a 
low stakes practice test for non-native speakers 
where they are asked to provide six spontaneous 
speech samples of about one minute in length each 
in response to a variety of prompts. Some prompts 
may be simple questions, and others may involve 
reading or listening to passages first and then ans-
wering related questions. All responses were 
scored holistically by human raters according to 
pre-defined scoring rubrics (i.e., specific scoring 
guidelines) on a scale of 1 to 4, 4 being the highest 
proficiency level. 
In our automated scoring system, the first com-
ponent is an ASR system that decodes the digitized 
speech sample, generating a time-annotated hypo-
thesis for every response. Next, fluency and pro-
nunciation features are computed based on the 
ASR output hypotheses, and finally a multiple re-
gression scoring model, trained on human rater 
scores, computes the score for a given spoken re-
sponse (see Zechner et al (2009) for more details). 
We conducted the study in three steps: (1) finding 
important measures of syntactic complexity from 
second language acquisition (SLA) and English 
language learning (ELL) literature, and extending 
this feature set based on our observations of the 
TPO data in analogous ways; (2) computing fea-
tures based on transcribed speech responses and 
selecting features with highest correlations to hu-
man rater scores, also considering their compara-
tive values for native speakers taking the same test;  
and (3) building scoring models for the selected 
sub-set of the features to generate a proficiency 
score for each speaker, using all six responses of 
that speaker. 
In the remainder of the paper, we will address 
related work in syntactic complexity (Section 2), 
introduce the speech data sets of our study (Section 
3), describe the methods we used for feature ex-
traction (Section 4), provide the experiment design 
and results (Section 5), analyze and discuss the 
results in Section 6, before concluding the paper 
(Section 7). 
2 Related Work 
2.1 Literature on Syntactic Complexity 
Syntactic complexity is defined as ?the range of 
forms that surface in language production and the 
degree of sophistication of such forms? (Ortega, 
2003). It is an important factor in the second lan-
guage assessment construct as described  in Bach-
man?s (1990) conceptual model of language 
ability, and therefore is often used as an index of 
language proficiency and development status of L2 
learners. Various studies have proposed and inves-
tigated measures of syntactic complexity as well as 
examined its predictiveness for language profi-
ciency, in both L2 writing and speaking settings, 
which will be reviewed respectively. 
Writing 
Wolfe-Quintero et al (1998) reviewed a number of 
grammatical complexity measures in L2 writing 
from thirty-nine studies, and their usage for pre-
dicting language proficiency was discussed. Some 
examples of syntactic complexity measures are: 
mean number of clauses per T-unit3
                                                          
3 T-units are defined as ?shortest grammatically allowable 
sentences into which (writing can be split) or minimally 
terminable units? (Hunt, 1965:20). 
, mean length 
of clauses, mean number of verbs per sentence, etc. 
The various measures can be grouped into two cat-
egories: (1) clauses, sentences, and T-units in 
terms of each other; and (2) specific grammatical 
structures (e.g., passives, nominals) in relation to 
clauses, sentences, or T-units (Wolfe-Quintero et 
al., 1998). Three primary methods of calculating 
syntactic complexity measures are frequency, ratio, 
and index, where frequency is the count of occur-
rences of a specific grammatical structure, ratio is 
the number of one type of unit divided by the total 
number of another unit, and index is computing 
numeric scores by specific formulae (Wolfe-
Quintero et al, 1998). For example, the measure 
?mean number of clauses per T-unit? is obtained 
by using the ratio calculation method and the 
clause and T-unit grammatical structures. Some 
structures such as clauses and T-units only need 
shallow linguistic processing to acquire, while 
some require parsing. There are numerous combi-
nations for measures and we need empirical evi-
723
dence to select measures with the highest perfor-
mance. 
There have been a series of empirical studies 
examining the relationship of syntactic complexity 
measures to L2 proficiency using real-world data 
(Cooper, 1976; Larsen-Freeman, 1978; Perkins, 
1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003; 
Lu, 2010). The studies investigate measures that 
highly correlate with proficiency levels or distin-
guish between different proficiency levels. Many 
T-unit related measures were identified as statisti-
cally significant indicators to L2 proficiency, such 
as mean length of T-unit (Henry, 1996; Lu, 2010), 
mean number of clauses per T-unit (Cooper, 1976; 
Lu, 2010), mean number of complex nominals per 
T-unit (Lu, 2010), or the mean number of error-
free T-units per sentence (Ho-Peng, 1983). Other 
significant measures are mean length of clause (Lu, 
2010), or frequency of passives in composition 
(Kameen, 1979).   
Speaking 
Syntactic complexity analysis in speech mainly 
inherits measures from the writing domain, and the 
abovementioned measures can be employed in the 
same way on speech transcripts for complexity 
computation. A series of studies have examined 
relations between the syntactic complexity of 
speech and the speakers? holistic speaking profi-
ciency levels (Halleck, 1995; Bernstein et al, 
2010; Iwashita, 2006). Three objective measures of 
syntactic complexity, including mean T-unit 
length, mean error-free T-unit length, and percent 
of error-free T-units were found to correlate with 
holistic evaluations of speakers in Halleck (1995). 
Iwashita?s (2006) study on Japanese L2 speakers 
found that length-based complexity features (i.e., 
number of T-units and number of clauses per T-
unit) are good predictors for oral proficiency. In 
studies directly employing syntactic complexity 
measures in other contexts, ratio-based measures 
are frequently used. Examples are mean length of 
utterance (Condouris et al, 2003), word count or 
tree depth (Roll et al, 2007), or mean length of T-
units and mean number of clauses per T-unit 
(Bernstein et al, 2010). Frequency-based measures 
were used less, such as number of full phrases in 
Roll et al (2007). 
The speaking output is usually less clean than 
writing data (e.g., considering disfluencies such as 
false starts, repetitions, filled pauses etc.). There-
fore we may need to remove these disfluencies first 
before computing syntactic complexity features. 
Also, importantly, ASR output does not contain 
interpunctuation but both for sentential-based fea-
tures as well as for parser-based features, the 
boundaries of clauses and sentences need to be 
known. For this purpose, we will use automated 
classifiers that are trained to predict clause and 
sentence boundaries, as described in Chen et al 
(2010). With previous studies providing us a rich 
pool of complexity features, additionally we also 
develop features analogous to the ones from the 
literature, mostly by using different calculation 
methods. For instance, the frequency of Preposi-
tional Phrases (PPs) is a feature from the literature, 
and we add some variants such as number of PPs 
per clause as a new feature to our extended feature 
set. 
2.2 Devising the Initial Feature Set 
Through this literature review, we identified some 
important features that were frequently used in 
previous studies in both L2 speaking and writing, 
such as length of sentences and number of clauses 
per sentence. In addition, we also collected candi-
date features that were less frequently mentioned 
in the literature, in order to start with a larger field 
of potential candidate features. We further ex-
tended the feature set by inspecting our data, de-
scribed in the following section, and created 
suitable additional features by means of analogy. 
This process resulted in a set of 91 features, 11 of 
which are related to clausal and sentential unit 
measurements (frequency-based) and 80 to mea-
surements within such units (ratio-based). From 
the perspective of extracting measures, in our study, 
some measures can be computed using only clause 
and sentence boundary information, and some can 
be derived only if the spoken responses are syntac-
tically parsed. In our feature set, there are two 
types of features: clause and sentence boundary 
based (26 in total) and parsing based (65). The fea-
tures will be described in detail in Section 4. 
3 Data 
Our data set contains (1) 1,060 non-native speech 
responses of 189 speakers from the TPO test (NN 
set), and (2) 100 responses from 48 native speakers 
that took the same test (Nat set). All responses 
were verbatim transcribed manually and scored 
724
holistically by human raters. (We only made use of 
the scores for the non-native data set in this study, 
since we purposefully selected speakers with per-
fect or near perfect scores for the Nat set from a 
larger native speech data set.) As mentioned above, 
there are four proficiency levels for human scoring, 
levels 1 to 4, with higher levels indicating better 
speaking proficiency. 
The NN set was randomly partitioned into a 
training (NN-train) and a test set with 760 and 300 
responses, respectively, and no speaker overlap.  
 
Data 
Set 
Res-
ponses 
Speakers Responses per 
Speaker  
(average) 
NN-
train 
760 137 5.55 
Description: used to train sentence and 
clause boundary detectors, evaluate fea-
tures and train scoring models 
1: 
NN-
test-1-
Hum 
300 52 5.77 
Description: human transcriptions and 
annotations of sentence and clause boun-
daries 
2: 
NN-
test-2-
CB 
300 52 5.77 
Description: human transcriptions, au-
tomatically predicted clause boundaries 
3: 
NN-
test-3-
SB 
300 52 5.77 
Description: human transcriptions, au-
tomatically predicted sentence bounda-
ries 
4: 
NN-
test-4-
ASR-
CB 
300 52 5.77 
Description: ASR hypotheses, automati-
cally predicted clause boundaries 
5: 
NN-
test-5-
ASR-
SB 
300 52 5.77 
Description: ASR hypotheses, automati-
cally predicted sentence boundaries 
Table 1. Overview of non-native data sets. 
 
A second version of the test set contains ASR 
hypotheses instead of human transcriptions. The 
word error rate (WER4
                                                          
4 Word error rate (WER) is the ratio of errors from a string 
between the ASR hypothesis and the reference transcript, 
where the sum of substitutions, insertions, and deletions is 
) on this data set is 50.5%. 
We used a total of five variants of the test sets, as 
described in Table 1. Sets 1-3 are based on human 
transcriptions, whereas sets 4 and 5 are based on 
ASR output. Further, set 1 contains human anno-
tated clause and sentence boundaries, whereas the 
other 4 sets have clause or sentence boundaries 
predicted by a classifier. 
All human transcribed files from the NN data 
set were annotated for clause boundaries, clause 
types, and disfluencies by human annotators (see 
Chen et al (2010)). 
For the Nat data set, all of the 100 transcribed 
responses were annotated in the same manner by a 
human annotator. They are not used for any train-
ing purposes but serve as a comparative reference 
for syntactic complexity features derived from the 
non-native corpus. 
The NN-train set was used both for training 
clause and sentence boundary classifiers, as well as 
for feature selection and training of the scoring 
models. The two boundary detectors were machine 
learning based Hidden Markov Models, trained by 
using a language model derived from the 760 train-
ing files which had sentence and clause boundary 
labels (NN-train; see also Chen et al (2010)).  
Since a speaker?s response to a single test item 
can be quite short (fewer than 100 words in many 
cases), it may contain only very few syntactic 
complexity features we are looking for. (Note that 
much of the previous work focused on written lan-
guage with much longer texts to be considered.) 
However, if we aggregate responses of a single 
speaker, we have a better chance of finding a larger 
number of syntactic complexity features in the ag-
gregated file. Therefore we joined files from the 
same speaker to one file for the training set and the 
five test sets, resulting in 52 aggregated files in 
each test set. Accordingly, we averaged the re-
sponse scores of a single speaker to obtain the total 
speaker score to be used later in scoring model 
training and evaluation (Section 5).5
While disfluencies were used for the training of 
the boundary detectors, they were removed after-
wards from the annotated data sets to obtain a tran-
 
                                                                                           
divided by the length of the reference. To obtain WER in 
percent, this ratio is multiplied by 100.0. 
5 Although in most operational settings, features are derived 
from single responses, this may not be true in all cases. 
Furthermore, scores of multiple responses are often combined 
for score reporting, which would make such an approach 
easier to implement and argue for operationally. 
725
scription which is ?cleaner? and lends itself better 
to most of the feature extraction methods we use.  
4 Feature Extraction 
4.1 Feature Set 
As mentioned in Section 2, we gathered 91 candi-
date syntactic complexity features based on our 
literature review as initial feature set, which is 
grouped into two categories: (1) Clause and sen-
tence Boundary based features (CB features); and 
(2) Parse Tree based features (PT features). Clause 
based features are based on both clause boundaries 
and clause types and can be generated from human 
clause annotations, e.g., ?frequency of adjective 
clauses6
We first selected features showing high correla-
tion to human assigned scores. In this process the 
CB features were computed from human labeled 
clause boundaries in transcripts for best accuracy, 
and PT features were calculated from using parsing 
and other tools because we did not have human 
parse tree annotations for our data.  
 per one thousand words?, ?mean number 
of dependent clauses per clause?, etc. Parse tree 
based features refer to features that are generated 
from parse trees and cannot be extracted from hu-
man annotated clauses directly.  
We used the Stanford Parser (Klein and Man-
ning, 2003) in conjunction with the Stanford Tre-
gex package (Levy and Andrew, 2006) which 
supports using rules to extract specific configura-
tions from parse trees, in a package put together by 
Lu (Lu, 2011). When given a sentence, the Stan-
ford Parser outputs its grammatical structure by 
grouping words (and phrases) in a tree structure 
and identifies grammatical roles of words and 
phrases.  
Tregex is a tree query tool that takes Stanford 
parser trees as input and queries the trees to find 
subtrees that meet specific rules written in Tregex 
syntax (Levy and Andrew, 2006). It uses relational 
operators regulated by Tregex, for example, ?A << 
B? stands for ?subtree A dominates subtree B?. 
The operators primarily function in subtree prece-
dence, dominance, negation, regular expression, 
tree node identity, headship, or variable groups, 
among others (Levy and Andrew, 2006).   
                                                          
6 An adjective clause is a clause that functions as an adjective 
in modifying a noun. E.g., ?This cat is a cat that is difficult to 
deal with.? 
Lu?s tool (Lu, 2011), built upon the Stanford 
Parser and Tregex, does syntactic complexity anal-
ysis given textual data. Lu?s tool contributed 8 of 
the initial CB features and 6 of the initial PT fea-
tures, and we computed the remaining CB and PT 
features using Perl scripts, the Stanford Parser, and 
Tregex.  
Table 2 lists the sub-set of 17 features (out of 91 
features total) that were used for building the scor-
ing models described later (Section 5). 
4.2 Feature Selection 
We determined the importance of the features by 
computing each feature?s correlation with human 
raters? proficiency scores based on the training set 
NN-train. We also used criteria related to the 
speaking construct, comparisons with native 
speaker data, and feature inter-correlations. While 
approaches coming from a pure machine learning 
perspective would likely use the entire feature pool 
as input for a classifier, our goal here is to obtain 
an initial feature set by judicious and careful fea-
ture selection that can withstand the scrutiny of 
construct validity in assessment development. 
 
As noted earlier, the disfluencies in the training set 
had been removed to obtain a ?cleaner? text that 
looks somewhat more akin to a written passage and 
is easier to process by NLP modules such as pars-
ers and part-of-speech (POS) taggers. 7
                                                          
7 We are aware that disfluencies can provide valuable clues 
about spoken proficiency in and of themselves; however, this 
study is focused exclusively on syntactic complexity analysis, 
and in this context, disfluencies would distort the picture 
considerably due to the introduction of parsing errors, e.g. 
  The ex-
tracted features partly were taken directly from 
proposals in the literature and partly were slightly 
modified to fit our clause annotation scheme. In 
order to have a unified framework for computing 
syntactic complexity features, we used a combina-
tion of the Stanford Parser and Tregex for compu-
ting both clause- and sentence-based features as 
well as parse-tree-based features, i.e., we did not 
make use of the human clause boundary label an-
notations here. The only exception to this
726
  
  
is that we are using human clause and sentence 
labels to create a candidate set for the clause boun-
dary features evaluated by the Stanford Parser and 
Tregex, as explained in the following subsection. 
                                                          
8  Feature type: CB=Clause boundary based feature type, 
PT=Parse tree based feature type 
9A ?linguistically meaningful PP? (PP_ling) is defined as a PP 
immediately dominated by another PP in cases where a 
preposition contains a noun such as ?in spite of? or ?in front 
of?. An example would be ?she stood in front of a house? 
where ?in front of a house? would be parsed as two embedded 
PPs but only the top PP would be counted in this case.  
10 A ?linguistically meaningful VP? (VP_ling) is defined  as a 
verb phrase immediately dominated by a clausal phrase, in 
order to avoid VPs embedded in another VP, e.g., "should go 
to work" is identified as one VP instead of two embedded 
VPs. 
11 The ?P-based Sampson? is a raw production-based measure 
(Sampson, 1997), defined as "proportion of the daughters of a 
nonterminal node which are themselves nonterminal and 
nonrightmost, averaged over the nonterminals of a sentence". 
 
 
 
Clause and Sentence based Features (CB fea-
tures) 
 
Firstly, we extracted all 26 initial CB features di-
rectly from human annotated data of NN-train, us-
ing information from the clause and sentence type 
labels. The reasoning behind this was to create an 
initial pool of clause-based features that reflects 
the distribution of clauses and sentences as accu-
rately as possible, even though we did not plan to 
use this extraction method operationally, where the 
parser decides on clause and sentence types. After 
computing the values of each CB feature, we cal-
culated correlations between each feature and hu-
man-rated scores. Then we created an initial CB   
feature pool by selecting features that met two cri-
teria: (1) the absolute Pearson correlation coeffi-
cient with human scores was larger than 0.2; and 
(2) the mean value of the feature on non-native 
speakers was at least 20% lower than that for na-
Name Type8 Meaning  Correlation Regression 
MLS CB Mean length of sentences 0.329 0.101 
MLT CB Mean length of T-units 0.300 -0.059 
DC/C CB Mean number of dependent clauses per clause 0.291 2.873 
SSfreq CB Frequency of simple sentences per 1000 words -.0242 0.001 
MLSS CB Mean length of simple sentences 0.255 0.040 
ADJCfreq CB Frequency of adjective clauses per 1000 words 0.253 0.004 
Ffreq CB Frequency of fragments per 1000 words -0.386 -0.057 
MLCC CB Mean length of coordinate clauses 0.224 0.017 
CT/T PT Mean number of complex T-units per T-unit 0.248 0.908 
PP_ling/S PT Mean number of linguistically meaningful prepositional phrases (PP) per sentence9 0.310  0.423 
NP/S PT Mean number of noun phrases (NP) per sentence 0.244 -0.411 
CN/S PT Mean number of complex nominal per sentence 0.325 0.653 
VB _ling/T PT Mean number of linguistically meaningful10 0.273   verb phrases per T-unit -0.780 
PAS/S PT Mean number of passives per sentence 0.260 1.520 
DI/T PT Mean number of dependent infinitives per T-unit 0.325 1.550 
MLev PT Mean number of parsing tree levels per sentence 0.306 -0.134 
MPSam PT Mean P-based Sampson11 0.254  per sentence 0.234 
Table 2. List of syntactic complexity features selected to be included in building the scoring models. 
727
tive speakers in case of positive correlation and at 
least by 20% higher than for native speakers in 
case of negative correlation, using the Nat data set 
for the latter criterion. Note that all of these fea-
tures were computed without using a parser. This 
resulted in 13 important features. 
Secondly, Tregex rules were developed based 
on Lu?s tool to extract these 13 CB features from 
parsing results where the parser is provided with 
one sentence at a time. By applying the same selec-
tion criteria as before, except for allowing for cor-
relations above 0.1 and giving preference to 
linguistically more meaningful features, we found 
8 features that matched our criteria:  
MLS, MLT, DC/C, SSfreq, MLSS, ADJCfreq, 
Ffreq, MLCC 
All 28 pairwise inter-correlations between these 
8 features were computed and inspected to avoid 
including features with high inter-correlations in 
the scoring model. Since we did not find any inter-
correlations larger than 0.9, the features were con-
sidered moderately independent and none of them 
were removed from this set so it also maintains 
linguistic richness for the feature set.  
Due to the importance of T-units in complexity 
analysis, we briefly introduce how we obtain them 
from annotations. Three types of clauses labeled in 
our transcript can serve as T-units, including sim-
ple sentences, independent clauses, and conjunct 
(coordination) clauses. These clauses were identi-
fied in the human-annotated text and extracted as 
T-units in this phase. T-units in parse trees are 
identified using rules in Lu?s tool. 
 
Parse Tree based Features (PT features) 
 
We evaluated 65 features in total and selected fea-
tures with highest importance using the following 
two criteria (which are very similar as before): (1) 
the absolute Pearson correlation coefficient with 
human scores is larger than 0.2; and (2) the feature 
mean value on native speakers (Nat) is higher than 
on score 4 for non-native speakers in case of posi-
tive correlation, or lower for negative correlation. 
20 of 65 features were found to meet the require-
ments. 
Next, we examined inter-correlations between 
these features and found some correlations larger 
than 0.85.12
CT/T, PP_ling/S, NP/S, CN/S, VP_ling/T, PAS/S, 
DI/T, MLev, MPSam  
 For each feature pair exhibiting high 
inter-correlation, we removed one feature accord-
ing to the criterion that the removed feature should 
be linguistically less meaningful than the remain-
ing one. After this filtering, the 9 remaining PT 
features are: 
In summary, as a result of the feature selection 
process, a total of 17 features were identified as 
important features to be used in scoring models for 
predicting speakers? proficiency scores. Among 
them 8 are clause boundary based and the other 9 
are parse tree based. 
5 Experiments and Results 
In the previous section, we identified 17 syntactic 
features that show promising correlations with hu-
man rater speaking proficiency scores. These fea-
tures as well as the human-rated scores will be 
used to build scoring models by using machine 
learning methods. As introduced in Section 3, we 
have one training set (N=137 speakers with all of 
their responses combined) for model building and 
five testing sets (N=52 for each of them) for evalu-
ation.  
The publicly available machine learning pack-
age Weka was used in our experiments (Hall et al 
2009). We experimented with two algorithms in 
Weka: multiple regression (called ?LinearRegres-
sion? in Weka) and decision tree (called ?M5P?in 
Weka). The score values to be predicted are real 
numbers (i.e., non-integer), because we have to 
compute the average score of one speaker?s res-
ponses. Our initial runs showed that decision tree 
models were consistently outperformed by mul-
tiple regression (MR) models and thus decided to 
only focus on MR models henceforth. 
We set the ?AttributeSelectionMethod? parame-
ter in Weka?s LinearRegression algorithm to all 3 
of its possible values in turn: (Model-1) M5 me-
thod; (Model-2) no attribute selection; and (Model-
3) greedy method. The resulting three multiple re-
gression models were then tested against the five 
testing sets. Overall, correlations for all models for 
the NN-test-1-Hum set were between 0.45 and 
0.49, correlations for sets NN-test-2-CB and NN- 
                                                          
12 The reason for using a lower threshold than above was to 
obtain a roughly equal number of CB and PT features in the 
end. 
728
test-3-SB (human transcript based, and using au-
tomated boundaries) around 0.2, and for sets NN-
test-4-ASR-CB  and NN-test-5-ASR-SB (ASR hy-
potheses, and using automated boundaries), the 
correlations were not significant. Model-2 (using 
all 17 features) had the highest correlation on NN-
test-1-Hum and we provide correlation results of 
this model in Table 3. 
 
Test set 
Correlation 
coefficient 
Correlation significance 
(p < 0.05) 
NN-test-1-Hum 0.488 Significant 
NN-test-2-CB 0.220 Significant 
NN-test-3-SB 0.170 Significant 
NN-test-4-ASR-CB -0.025 Not significant 
NN-test-5-ASR-SB -0.013 Not significant 
Table 3. Multiple regression model testing results for 
Model-2. 
6 Discussion 
As we can see from the result table (Table 3) in the 
previous section, using only syntactic complexity 
features, based on clausal or parse tree information 
derived from human transcriptions of spoken test 
responses, can predict holistic human rater scores 
for combined speaker responses over a whole test 
with an overall correlation of r=0.49. While this is 
a promising result for this study with a focus on a 
broad spectrum of syntactic complexity features, 
the results also show significant limitations for an 
immediate operational use of such features. First, 
the imperfect prediction of clause and sentence 
boundaries by the two automatic classifiers causes 
a substantial degradation of scoring model perfor-
mance to about r=0.2, and secondly, the rather high 
error rate of the ASR system (50.5%) does not al-
low for the computation of features that would re-
sult in any significant correlation with human 
scores. We want to note here that while ASR sys-
tems can be found that exhibit WERs below 10% 
for certain tasks, such as restricted dictation in 
low-noise environments by native speakers, our 
ASR task is significantly harder in several ways: 
(1) we have to recognize non-native speak-
ers?rresponses where speakers have a number of 
different native language backgrounds; (2) the pro-
ficiency level of the test takers varies widely; and 
(3) the responses are spontaneous and uncon-
strained in terms of vocabulary. 
As for the automatic clause and sentence boun-
dary classifiers, we can observe (in Table 4) that 
although the sentence boundary classifier has a 
slightly higher F-score than the clause boundary 
classifier, errors in sentence boundary detection 
have more negative effects on the accuracy of 
score prediction than those made by the clause 
boundary classifier. In fact, the lower F-score of 
the latter is mainly due to its lower precision which 
indicates that there are more spurious clause boun-
daries in its output which apparently cause little 
harm to the feature extraction processes. 
Among the 17 final features, 3 of them are fre-
quency-based and the remaining 14 are ratio-
based, which mirrors our findings from previous 
work that frequency features have been used less 
successfully than ratio features. As for ratio fea-
tures, 5 of them are grammatical structure counts 
against sentence units, 4 are counts against T-units, 
and only 1 is based on counts against clause units. 
The feature set covers a wide range of grammatical 
structures, such as T-units, verb phrases, noun 
phrases, complex nominals, adjective clauses, 
coordinate clauses, prepositional phrases, etc. 
While this wide coverage provides for richness of 
the construct of syntactic complexity, some of the 
features exhibit relatively high correlation with 
each other which reduces their overall contribu-
tions to the scoring model?s performance. 
Going through the workflow of our system, we 
find at least five major stages that can generate 
errors which in turn can adversely affect feature 
computation and scoring model building. Errors 
may appear in each stage of our workflow, passing 
or even enlarging their effects from previous stages 
to later stages: 
1) grammatical errors by the speakers (test takers); 
2) errors by the ASR system; 
3) sentence/clause boundary detection errors; 
4) parser errors; and 
5) rule extraction errors. 
 
In future work we will need to address each er-
ror source to obtain a higher overall system per-
formance. 
 
729
Table 4. Performance of clause and sentence boundary 
detectors. 
7 Conclusion and Future Work 
In this paper, we investigated associations between 
speakers? syntactic complexity features and their 
speaking proficiency scores provided by human 
raters. By exploring empirical evidence from non-
native and native speakers? data sets of spontane-
ous speech test responses, we identified 17 features 
related to clause types and parse trees as effective 
predictors of human speaking scores. The features 
were implemented based on Lu?s L2 Syntactic 
Complexity Analyzer toolkit (Lu, 2011) to be au-
tomatically extracted from human or ASR tran-
scripts. Three multiple regression models were 
built from non-native speech training data with 
different parameter setup and were tested against 
five testing sets with different preprocessing steps. 
The best model used the complete set of 17 fea-
tures and exhibited a correlation with human 
scores of r=0.49 on human transcripts with boun-
dary annotations. 
When using automated classifiers to predict 
clause or sentence boundaries, correlations with 
human scores are around r=0.2. Our experiments 
indicate that by enhancing the accuracy of the two 
main automated preprocessing components, name-
ly ASR and automatic sentence and clause boun-
dary detectors, scoring model performance will 
increase substantially, as well. Furthermore, this 
result demonstrates clearly that syntactic complexi-
ty features can be devised that are able to predict 
human speaking proficiency scores. 
Since this is a preliminary study, there is ample 
space to improve all major stages in the feature 
extraction process. The errors listed in the previous 
section are potential working directions for prepro-
cessing enhancements prior to machine learning. 
Among the five types of errors, we can work on 
improving the accuracy of the speech recognizer, 
sentence and clause boundary detectors, parser, 
and feature extraction rules; as for the grammatical 
errors produced by test takers, we are envisioning 
to automatically identify and correct such errors. 
We will further experiment with syntactic com-
plexity measures to balance construct richness and 
model simplicity. Furthermore, we can also expe-
riment with additional types of machine learning 
models and tune parameters to derive scoring 
models with better performance. 
 
Acknowledgements 
The authors wish to thank Lei Chen and Su-Youn 
Yoon for their help with the sentence and clause 
boundary classifiers. We also would like to thank 
our colleagues Jill Burstein, Keelan Evanini, Yoko 
Futagi, Derrick Higgins, Nitin Madnani, and Joel 
Tetreault, as well as the four anonymous ACL re-
viewers for their valuable and helpful feedback and 
comments on our paper. 
References  
Bachman, L.F. (1990). Fundamental considerations in 
language testing. Oxford: Oxford University Press. 
Bernstein, J. (1999). PhonePass testing: Structure and 
construct. Menlo Park, CA: Ordinate Corporation. 
Bernstein, J., DeJong, J., Pisoni, D. & Townshend, B. 
(2000). Two experiments in automatic scoring of 
spoken language proficiency. Proceedings of In-
STILL 2000, Dundee, Scotland. 
Bernstein, J., Cheng, J., & Suzuki, M. (2010). Fluency 
and structural complexity as predictors of L2 oral 
proficiency. Proceedings of Interspeech 2010, Tokyo, 
Japan, September. 
Chen, L., Tetreault, J. & Xi, X. (2010). Towards using 
structural events to assess non-native speech. 
NAACL-HLT 2010. 5th Workshop on Innovative 
Use of NLP for Building Educational Applications, 
Los Angeles, CA, June. 
Condouris, K., Meyer, E. & Tagger-Flusberg, H. 
(2003). The relationship between standardized meas-
ures of language and measures of spontaneous speech 
in children with autism. American Journal of Speech-
Language Pathology, 12(3), 349-358. 
Cooper, T.C. (1976). Measuring written syntactic pat-
terns of second language learners of German. The 
Journal of Educational Research, 69(5), 176-183. 
Cucchiarini, C., Strik, H. & Boves, L. (1997). Automat-
ic evaluation of Dutch pronunciation by using speech 
recognition technology. IEEE Automatic Speech 
Recognition and Understanding Workshop, Santa 
Barbara, CA. 
Classifier Accu-
racy 
Preci-
sion 
Re-
call 
F score 
Clause boundary 0.954 0.721 0.748 0.734 
Sentence boundary 0.975    0.811 0.755 0.782    
730
Cucchiarini, C., Strik, H. & Boves, L. (2000). Quantita-
tive assessment of second language learners' fluency 
by means of automatic speech recognition technolo-
gy. Journal of the Acoustical Society of America, 
107, 989-999.  
Franco, H., Abrash, V., Precoda, K., Bratt, H., Rao, R. 
& Butzberger, J. (2000a). The SRI EduSpeak system: 
Recognition and pronunciation scoring for language 
learning. Proceedings of InSTiLL-2000 (Intelligent 
Speech Technology in Language Learning), Dundee, 
Scotland.  
Franco, H., Neumeyer, L., Digalakis, V. & Ronen, O. 
(2000b). Combination of machine scores for auto-
matic grading of pronunciation quality. Speech 
Communication, 30, 121-130.  
Hall, M., Frank, E., Holmes, G., Pfahringer, B., 
Reutemann, P. &  Witten, I.H. (2009). The WEKA 
Data Mining Software: An Update. SIGKDD Explo-
rations, 11(1). 
Halleck, G.B. (1995). Assessing oral proficiency: A 
comparison of holistic and objective measures. The 
Modern Language Journal, 79(2), 223-234. 
Henry, K. (1996). Early L2 writing development: A 
study of autobiographical essays by university-level 
students on Russian. The Modern Language Journal, 
80(3), 309-326. 
Ho-Peng, L. (1983). Using T-unit measures to assess 
writing proficiency of university ESL students. 
RELC Journal, 14(2), 35-43. 
Hunt, K. (1965). Grammatical structures written at three 
grade levels. NCTE Research report No.3. Cham-
paign, IL: NCTE. 
Iwashita, N. (2006). Syntactic complexity measures and 
their relations to oral proficiency in Japanese as a 
foreign language. Language Assessment Quarterly, 
3(20), 151-169. 
Kameen, P.T. (1979). Syntactic skill and ESL writing 
quality. In C. Yorio, K. Perkins, & J. Schachter 
(Eds.), On TESOL ?79: The learner in focus (pp.343-
364). Washington, D.C.: TESOL. 
Klein, D. & Manning, C.D. (2003). Fast exact inference 
with a factored model for a natural language parsing. 
In S.Becker, S. Thrun & K. Obermayer (Eds.), Ad-
vances in Neural Information Processing Systems 15 
(pp.3-10). Cambridge, MA: MIT Press. 
Larsen-Freeman, D. (1978). An ESL index of develop-
ment. Teachers of English to Speakers of Other Lan-
guages Quarterly, 12(4), 439-448. 
Levy, R. & Andrew, G. (2006). Tregex and Tsurgeon: 
Tools for querying and manipulating tree data struc-
tures. Proceedings of the Fifth International Confe-
rence on Language Resources and Evaluation.  
Lu, X. (2010). Automatic analysis of syntactic complex-
ity in second language writing. International Journal 
of Corpus Linguistics, 15(4), 474-496. 
Lu, X. (2011). L2 Syntactic Complexity Analyzer. Re-
trieved from 
http://www.personal.psu.edu/xxl13/downloads/l2sca.
html 
Ortega, L. (2003). Syntactic complexity measures and 
their relationship to L2 proficiency: A research syn-
thesis of college-level L2 writing. Applied Linguis-
tics, 24(4), 492-518. 
Perkins, K. (1980). Using objective methods of attained 
writing proficiency to discriminate among holistic 
evaluations. Teachers of English to Speakers of Oth-
er Languages Quarterly, 14(1), 61-69. 
Roll, M., Frid, J. & Horne, M. (2007). Measuring syn-
tactic complexity in spontaneous spoken Swedish. 
Language and Speech, 50(2), 227-245. 
Sampson, G. (1997). Depth in English grammar. Journal 
of Linguistics, 33, 131-151. 
Wolfe-Quintero, K., Inagaki, S. & Kim, H. Y. (1998). 
Second language development in writing: Measures 
of fluency, accuracy, & complexity. Honolulu, HI: 
University of Hawaii Press. 
Xi, X., & Mollaun, P. (2006).  Investigating the utility 
of analytic scoring for the TOEFL? Academic 
Speaking Test (TAST). TOEFL iBT Research Re-
port No. TOEFLiBT-01. 
Zechner, K., Higgins, D. & Xi, X. (2007). SpeechRa-
ter(SM): A construct-driven approach to score spon-
taneous non-native speech. Proceedings of the 2007 
Workshop of the International Speech Communica-
tion Association (ISCA) Special Interest Group on 
Speech and Language Technology in Education 
(SLaTE), Farmington, PA, October. 
Zechner, K., Higgins, D., Xi, X, & Williamson, D.M. 
(2009). Automatic scoring of non-native spontaneous 
speech in tests of spoken English. Speech Communi-
cation, 51 (10), October.  
 
731
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 86?94,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
     Using an Ontology for Improved Automated Content Scoring of Spontaneous Non-Native Speech   Miao Chen Klaus Zechner School of Information Studies Educational Testing Service Syracuse University 660 Rosedale Road Syracuse, NY 13244, USA Princeton, NJ 08541, USA mchen14@syr.edu kzechner@ets.org   Abstract 
This paper presents an exploration into auto-mated content scoring of non-native sponta-neous speech using ontology-based information to enhance a vector space ap-proach. We use content vector analysis as a baseline and evaluate the correlations between human rater proficiency scores and two co-sine-similarity-based features, previously used in the context of automated essay scoring. We use two ontology-facilitated approaches to improve feature correlations by exploiting the semantic knowledge encoded in WordNet: (1) extending word vectors with semantic con-cepts from the WordNet ontology (synsets); and (2) using a reasoning approach for esti-mating the concept weights of concepts not present in the set of training responses by ex-ploiting the hierarchical structure of WordNet. Furthermore, we compare features computed from human transcriptions of spoken respons-es with features based on output from an au-tomatic speech recognizer. We find that (1) for one of the two features, both ontologically based approaches improve average feature correlations with human scores, and that (2) the correlations for both features decrease on-ly marginally when moving from human speech transcriptions to speech recognizer output. 1 Introduction Currently, automated speech scoring systems mainly utilize features related to the acoustic as-pects of a spoken response of a test taker, for ex-ample, fluency, pronunciation, and prosody features (Cucchiarini et al, 2000, 2002; Franco et al, 2010; Zechner et al, 2009). In terms of the 
content aspect of speech, for highly predictable speech, such as reading a passage aloud, scoring of content reduces to measuring the reading accuracy of the read passage which is typically achieved by computing the string edit distance between the tar-get passage and the actual text read by the test tak-er, using the speech recognizer hypothesis as a proxy (Alwan et al, 2007; Balogh et al, 2007). For high entropy speech whose content is difficult to predict such as spontaneous speech in this study, on the other hand, content scoring has not been investigated much so far, mostly due to the diffi-culty of obtaining accurate word hypotheses for spontaneous non-native speech by Automated Speech Recognition (ASR) systems. In this paper, we use spoken responses from an English language spoken proficiency test where candidates, all non-native speakers of English, re-spond to four different prompts1 with a speaking time of one minute per response. For this study, we decide to use a baseline ap-proach for content scoring of spontaneous speech that was previously employed for a similar task in the context of automated essay scoring (Attali & Burstein, 2006), namely Content Vector Analysis (CVA) where every document is represented as a vector of word weights, based on their frequencies in a document or document collection. However, there are two issues with the CVA vector of words representation that we want to address with this study: (1) Similar words are treated in isolation and not grouped together. Words with similar meaning should be treated in the same way in an automated scoring system, so grouping word synonyms into semantic concepts can help with this issue. (2) The vector of word representation is based on an exist-                                                            1 Prompts are test tasks assigned to test takers to elicit spoken responses. 
86
ing corpus of training documents. When encoun-tering a word or concept in a test document that is not contained in the training set, it is difficult to decide the relevance of that word or concept.  We propose to use ontology-facilitated ap-proaches as solutions to these two issues, aiming at enriching speech content representations to im-prove speech content scoring.  Specifically, to ad-dress issue (1), we represent speech content by concept-level vectors, using the synsets (lists of synonymous words) of the WordNet ontology (Fellbaum, 1998; WordNet 3.0, 2010). As for issue (2), we expand the vector representation by infer-ring the importance (weight) of concepts not pre-sent in the training vectors based on their path distance to known concepts or words in the hierar-chical structure of the WordNet ontology. Since we only look at the content aspect of speech without considering the acoustic features in this study, we work on speech transcripts exclu-sively, both from human transcribers as well as from a state-of-the-art automated speech recogni-tion system, and compare results between the ideal human transcripts and the imperfect transcripts generated by the speech recognizer. For the pur-pose of simplified illustration, speech transcripts are often referred to as ?documents? in the paper as they are a special type of textual documents. The remainder of this paper is organized as fol-lows: in Section 2, we review related research in content scoring of texts, particularly student es-says; Section 3 describes the data set we use for this study and the ASR system; and Section 4 pre-sents the ontologically-facilitated methods we are using in detail. In Section 5, we present our exper-iments along with their results, followed by a dis-cussion in Section 6, and we conclude the paper with a summary and outlook in Section 7. 2 Related Work  There have been some effective approaches for test takers? written responses in language tests, namely in the area of Automated Essay Scoring (AES). AES has employed content vector analysis, i.e., vectors of words to represent text, for example, the e-rater system (Burstein, 2003; Attali & Burstein, 2006) and the experimental system in Larkey and Croft (2003). Representations in the BETSY sys-tem (Bayesian Essay Test Scoring System) also involve words, such as the frequency of content 
words, and also include specific phrases as well (Dikli, 2006). AES has also used latent concepts for text representation, such as the Intelligent Es-say Assessor system (Landauer et al, 2003). The latent concepts are generated by a statistical ap-proach called Latent Semantic Analysis (LSA), which constructs a semantic vector space and pro-jects essays to the new space.  Representing texts by vectors of words has also been a common practice in many research areas beyond AES, including information retrieval (Sal-ton et al, 1975; Croft et al, 2010). One of its weaknesses, however, is its difficulty in addressing issues such as synonyms and related terms. Differ-ent words, such as lawyer, attorney, counsel etc. can share similar meaning, while in a word vector representation they are treated as different dimen-sions; however, because they are conceptually sim-ilar, it makes more sense to group them into the same vector dimension. Ontologies are in a good position to resolve this issue because they organize words and terms under structured concepts, group terms with similar meaning together and also maintain various semantic relations between con-cepts. Therefore, text can be represented on a con-cept level by using ontology concepts as features. Recognizing concepts in documents can further reveal semantic relations between documents (Hotho et al, 2003a), thus can facilitate further text-related tasks such as clustering, information retrieval, as well as our speech scoring task. This type of representation has been tried in several studies (e.g., Hotho et al, 2003a; Hotho et al, 2003b; Bloehdorn & Hotho, 2004).  Hotho et al (2003a; 2003b) use ontology con-cepts to represent text and use the representation for document clustering. The studies employ the WordNet ontology, a general domain ontology. The experiments test three parameters of using an ontology for text representation: (1) whether con-cept features should be used alone or replace word features or be used together with word features; (2) word sense disambiguation strategies when using concepts; and (3) investigating the optimal level of word generalization in terms of the hierarchical structure of the ontology, i.e., how general the con-cepts should be. Some options of the first two pa-rameters will be implemented and tested in our experiment design below.  The vector representation approach of text doc-uments, either using words or concepts, can be 
87
used to measure the content similarity between essays. E-rater, for example, measures the similari-ty between test essays and training essays by com-puting the cosine similarity of their word vectors and by generating two content features based on this similarity metric. It uses multiple regression as its final scoring model, using both content features, as well as features related to other aspects of the essay, such as grammar and vocabulary usage (Burstein, 2003; Attali & Burstein, 2006). Intelli-gent Essay Assessor also employs cosine similarity between to-be-scored essays and training essays as basis of one content feature, and models the scor-ing process by normalization and regression analy-sis (Landauer et al, 2003). The IntelliMetric system uses a nonlinear and multidimensional modeling approach to reflect the complexity of the writing process as opposed to the general linear model (Dikli, 2006). Larkey and Croft (2003) em-ploy Bayesian classifiers for modeling, which is a type of text categorization technique. It treats essay scoring as a text categorization task, the purpose of which is to classify essays into score categories based on content features (i.e., if the scores range from 1-4, then there are four score categories).     Zechner and Xi (2008) report on experiments related to scoring of spontaneous speech responses where content vector analysis was used as one of several features in scoring models for two different item types. They found that while these content features performed reasonably well by themselves, they were not able to increase the overall scoring model performance over a baseline that did not use content features.      This paper will use CVA as a baseline for our experiment and investigate two ontology-based approaches to enhance the content representation and improve content feature performance. 3 Data  We use data from a test for English proficiency for non-native speakers of English. Candidates are asked to provide spontaneous speech responses to four prompts, with each of the responses being one minute in length. The four prompts are all integrat-ed prompts, meaning candidates are first given some materials to read or listen and then are asked to respond with their opinions or arguments to-wards the materials. The responses are scored ho-listically by human raters on a scale of 1 to 4, 4 
being the highest score. For holistic scoring, the human raters use a speech scoring rubric as the guideline of expected performance on aspects such as fluency, pronunciation, and content for each score level. Our data set contains 1243 speech samples in to-tal as responses to four different prompts, obtained from 327 speakers (note that not all speakers re-sponded to all prompts). Each response is verbatim transcribed by a human transcriber. The responses are grouped by their prompts since our experi-ments are prompt-specific. For responses of each prompt, we randomly split the responses into a training set (44%) and a test set (56%), making sure that response scores are distributed in a simi-lar proportion in both training and test sets. Each response is considered as a single document here. Table 1 shows the size of the two data sets. Prompt Training Set Test Set Total A 143 176 319 (4/79/158/78) B 140  168 308 (7/86/146/69) C 139  172 311 (4/74/154/79) D 137  168 305 (8/75/141/81) Table 1. Size of training and test data sets. The numbers in parentheses are the number of documents on score levels 1-4.  The training set is used for generating repre-sentative vectors of a prompt on different score levels, which are to be compared with test docu-ments. The test set is primarily used to compute content features for test documents and examine performance of approaches under different exper-iment setups. Besides human transcriptions of the speech files, we also obtained ASR output of the files, in order to examine performance of the proposed approach-es on imperfect output, in a fully automated opera-tional scenario where no human transcribers would be in the loop. Since the training set is used for deriving representative vectors for the four differ-ent prompts and we would like to generate accurate vectors based on human transcriptions, we do not use a separate training set for ASR data. Thus, we only obtain corresponding ASR output for the test set of each prompt. The ASR system we use for our experiments in this paper is a state-of-the-art gender-independent continuous density Hidden Markov Model speech recognizer, trained on about 30 hours of non-native 
88
spontaneous speech. Its word error rate on the test set used here is about 12.8%. 4 Method  We employ one baseline approach for word-level features and two experimental approaches for con-cept-level features to examine the effect of the WordNet ontology and concept-level features on content feature correlations. 4.1 Baseline Approach: Content Vector Analysis (CVA) We decide to use the two content features used by e-rater based on CVA analysis, called ?max.cos? and ?cos.w4? here (Attali & Burstein, 2006). The assumption behind this approach is that essays with similar human scores contain similar words; thus, they should share similar vector representa-tions in CVA. For our data, this assumption is held for the spoken test documents in the same way. Moreover, we conjecture this assumption is mostly true for high score responses as opposed to low score responses, because we expect high vocabu-lary uniformity in high score responses and more irrelevant and more diverse vocabulary in low score responses.  Before feature computation, some preprocessing is conducted on the speech transcripts. For each prompt, we group its training set into four groups according to their score levels (?score-level docu-ments?). Then we use the score-level documents of each prompt to generate a super vector as a repre-sentation for documents on this score level of this specific prompt. As a result, we have four score-level vectors under each prompt, generated from their training sets. While the score-level training vectors are produced using multiple documents of the same score level, vectors of test documents are generated on an individual document level. Given a test document that needs to be scored, we first convert it into the vector representation. Then we are ready to compute the two content features. Equation 1 provides the exact formula for the co-sine similarity measure used in all of our methods. 
(1)	 ? 	 ?
where n is the number of words and/or concepts in the score-level vector (from the training set docu-ments),  ?w ? ,?  are the word or concept weights of a score-level vector and w?,? are the word or concept weights of a test document (response transcrip-tion). ??,? are computed by term frequency and ?? ,? are computed in the same way after concatenating documents of the same score level as one large document. The max.cos feature. This feature measures which score level of documents the test document is most similar to in vector space by computing the cosine similarity with each score-level vector and then selecting the score level which has the largest cosine similarity to the test vector as feature value. Thus, this feature assumes integer values from 1 to 4 only. The cos.w4 feature2.  This feature measures con-tent similarity between the test document and the best quality documents in vector space. Since score 4 is the highest level in our data set of spoken re-sponses, we compute the cosine similarity between the test vector and the score level 4 vector as an indicator of how similar the test document is to the speech content of the test takers with highest profi-ciency.  The two features are evaluated based on their Pearson r correlation to human assigned scores. We evaluate the features in all experiments, as a way to observe how the two features? predictive-ness varies among different experiment setups. Note that since the max.cos feature assumes inte-ger values but the cos.w4 feature is real valued, we expect correlations to be higher for cos.w4 due to this difference, all other things being equal. 4.2 Ontology-facilitated Approaches We use two ontology-facilitated document repre-sentation approaches, which represent documents based on the WordNet ontology. The first approach matches words in a document to concepts and rep-resents documents by vectors of concepts, whereas the second one addresses the unknown word issue by inferring their weight based on the structure of the WordNet ontology. 
                                                            2 The feature is referred to as ?cos.w/6? in Attali and Burstein (2006) because there are usually 6 score levels, while here our data has 4 score levels therefore it is written as ?cos.w4?. ??? == = ni islni itni islit ww ww 1 2 ,1 2,1 ,,**
89
4.2.1 Ontology-facilitated representation ap-proach This representation uses concepts instead of the words as elements in the document vectors. Given a document, we map words in the document to concepts, using the synsets in WordNet. For exam-ple, chance and opportunity are different words, however they belong to the same WordNet synset (?opportunity.n.01?). This concept-level representa-tion groups words of similar meaning in the same vector dimension, thus making the vector space more succinct and semantically meaningful. The weighting scheme of concepts follows the one in the CVA approach. In this study, we focus on sin-gle words and match them to WordNet synsets; in future work, we consider matching multi-word ex-pressions to ontologies like Wikipedia (Wikipedia, 2011). Experiments show that including words and their corresponding WordNet synsets as vector dimensions has better performance than only in-cluding WordNet synsets for text clustering tasks (Hotho et al, 2003a) and the same result also oc-curs in our preliminary experiments. Therefore, we include both WordNet synsets and words in the vector representation. 4.2.2 Ontology-facilitated reasoning approach This approach is based on the ontology-facilitated representation and goes further to resolve the un-known word issue, i.e., handling words in test doc-uments that have not been seen in the training documents. First, test documents are converted to vectors of concepts plus words. If a concept in the test vector does not appear in the score level vector, its weight therefore is unknown, as well. We then estimate its weight based on structural information contained in the WordNet ontology. More specifically, given an unknown concept in the test document, we find the N most similar concepts to that unknown con-cept from the set of all concepts contained in the score level vector. We use a WordNet-based simi-larity estimate to measure similarity between con-cepts, namely the edge-based Path Similarity, which measures the length of a path from one con-cept to another concept in WordNet by computing the inverse of the shortest path between the two concepts (Pedersen et al, 2004). We submit that the estimated weight of the unknown concept in 
the test document vector should be close to the weights of its most similar concepts in the score level vector derived from the training documents. From this assumption, we propose estimating the unknown concept?s weight by averaging the weights of the N most similar concepts: (2) ??? ?( ??)/?????   with N denoting the number of similar concepts in a score level vector,   ?w? denoting the weights of these similar concepts, and w ??  standing for the resulting concept weight for the unknown concept in a test document. For example, a test document may be ?so radio also create a great impact on this uh people com-munication?. The words are matched to WordNet concepts, and we find that the concept synset ?im-pact.n.01? is an unknown concept to the score level 4 vector. From the dimensions of the score level 4 vector we find these three most similar concepts to the unknown concept: ?happening.n.01?, ?event.n.01?, and ?change.n.01?. We now can aver-age the weights of these three concepts in the score-level vector to use it as a weight estimate for the unknown concept ?impact.n.01?. We want to note that while this approach can es-timate weights for test document words or con-cepts contained in WordNet (but not in the training vectors), it cannot handle words that are not in-cluded in WordNet at all, such as many proper names, foreign words, etc. To address the latter as well, we would have to use a much larger and more comprehensive ontology, e.g., the online en-cyclopedia Wikipedia. 5 Experiments and Results We design experiments according to the above ap-proaches. The first experiment group is the base-line system using two features employed by e-rater, max.cos and cos.w4. The second and third experiment groups implement the two ontology-facilitated approaches, respectively. We first run CVA and compare several different parameter set-ups to optimize them for further experiments. 5.1 Parameter Optimization in CVA Experi-ments For the CVA method, we need to decide (1) which term weighting scheme to use, and (2) whether or not to use a list of stopwords to exclude common 
90
non-content words such as determiners or preposi-tions from consideration. We compare five com-monly used term weighting schemes, each one with or without using a stoplist, based on averaged correlations with human scores across all four prompts. The best results are obtained for the weighting scheme (TF/EDL)*IDF, where TF is the frequency of a term in a document, EDL is the Eu-clidean document length3, and IDF is the inverse document frequency of a term based on a collec-tion of documents. For this scheme, as for most others, there is almost no difference between using vs. not using a stoplist and we decide to use a stoplist for our experiments based on the tradition in the field. The selected term weighting scheme is applied in the same way for both the score-level vectors as well as the test document vectors. 5.2 Experiment Groups 5.2.1 Group 1: CVA As described above, we first convert the training sets to score level vectors and the test documents into test vectors with the TF/EDL*IDF weighting, and compute the max.cos and cos.w4 features for each test document.  5.2.2 Group 2: Ontology-facilitated Representa-tion We first match words in documents to WordNet concepts. There are several ways to achieve this (Hotho et al, 2003a). Given a word, it may corre-spond to multiple concepts in WordNet, in which each possibility is called a ?sense? in WordNet, and we need to decide which sense to use.  WordNet-Sense-1. In this study we employ a simple word sense disambiguation method by us-ing the first sense returned by WordNet. We send a word to WordNet synset search function, which returns all synstes of the word, and we select to use the first result because it is also the most frequently used sense for the word. After obtaining the senses and concepts for the words, the training sets and test documents are                                                             3 Given a vector of raw term frequencies (rtf?, rtf?,? , rtf?), its Euclidean length is computed in this way:  ?
?? ??????  
converted to vectors of WordNet concepts plus words, using TF/EDL*IDF weighting, the same one used by the CVA approach. We compute the max.cos and cos.w4 features in the same way as for the baseline CVA method.  5.2.3 Group 3: Ontology-facilitated Reasoning This approach, called here ?WordNet-Reasoning?, also extracts vectors of WordNet concepts plus words with the same term weighting scheme as before. For matching words to concepts, we still employ the WordNet?Sense-1 sense selection method. For unknown concepts, which appear in a test vector but not in any score level vectors, we infer their weights by using the reasoning approach proposed in section 4.2.2 with N=5 as the number of most similar concepts to the unknown concept4, located in the WordNet hierarchy. The score level vectors are expanded by the inferred unknown concepts. When we obtain the expanded score lev-el vectors, we compute the two content features from the vectors in the same way as before, and finally calculate feature correlations with human scores. 5.3 Results We run the three experiment groups on human and ASR transcriptions respectively and obtain the max.cos and cos.w4 feature values of test docu-ments in the experiments. As stated in 4.1, we compute the correlations between the two features and the human assigned scores for evaluating the approaches. Tables 2 and 3 (next page) list correlations of the two content features with human scores under different experiment setups. Significant differences on individual prompts between correlations of the two WordNet-based methods WordNet-Sense-1 and WordNet-Reasoning and the CVA baseline are denoted with * (p<0.05) and ** (p<0.01).  
                                                            4 We manually inspected some of the similar concepts of the unknown concepts and found the first 5 similar concepts were relevant to the unknown concepts, and thus made the decision of N=5. 
91
Prompt Hum, CVA Hum, WordNet-Sense-1 Hum, Word-Net-Reasoning ASR, CVA ASR, Word-Net-Sense-1 ASR, Word-Net-Reasoning A 0.320 0.333 0.038** 0.293 0.286 0.014** B 0.348 0.352 0.350 0.308 0.338 0.339 C 0.366 0.373 0.074** 0.396 0.386 0.106** D 0.343 0.323 0.265 0.309 0.309 0.265 Average 0.344 0.345 0.182 0.327 0.330 0.181 Table 2. Correlations between the max.cos feature and human scores (Hum=using human transcriptions; ASR=using ASR hypotheses).  Prompt Hum, CVA Hum, WordNet-Sense-1 Hum, Word-Net-Reasoning ASR, CVA ASR, Word-Net-Sense-1 ASR, Word-Net-Reasoning A 0.427 0.429 0.434 0.409 0.416 0.411 B 0.295 0.303 0.327* 0.259 0.278 0.292* C 0.352 0.385* 0.402** 0.338 0.366 0.380** D 0.368 0.385 0.389 0.360 0.379 0.374 Average 0.360 0.376 0.388 0.342 0.360 0.364 Table 3. Correlations between the cos.w4 feature and human scores (Hum=using human transcriptions; ASR=using ASR hypotheses) 6 Discussion 6.1 Results on Human Transcriptions On human transcriptions, Table 2 shows that the max.cos feature correlations increase, albeit not significantly, when using the method WordNet?Sense-1 on all prompts except for prompt D but decrease sometimes significantly when using the WordNet-Reasoning approach. The cos.w4 feature correlations, on the other hand, exhibit constant increases on all four prompts when using WordNet-Sense-1 and the increase on prompt C is significant. The average correlations further increase for all prompts when using WordNet-Reasoning and the increase is sig-nificant on prompts B and C (Table 3).  6.2 Results on ASR Output On the ASR output, for the max.cos feature, the average correlation barely changes when using the WordNet-Sense-1 method but decreases when us-ing WordNet-Reasoning with significant decrease on prompts A and C (Table 2).  For the cos.w4 feature, however, WordNet-Sense-1 improves correlations on all four prompts with 0.018 correlation increase on average but in-creases are not statistically significant on a prompt level. WordNet-Reasoning does not further im-prove correlations much beyond the correlations of WordNet-Sense-1, with a further 0.004 increase in 
average correlation. Compared to CVA, though, correlations for WordNet-Reasoning are signifi-cantly higher on prompts B and C (Table 3). 6.3 Overall Discussion Based on these observations, we find that for cos.w4, the WordNet-Sense-1 approach can im-prove average correlations compared to the CVA baseline on both ASR and human transcriptions. Hence, the extension of the document vectors by WordNet synsets has a positive impact on the ac-curacy of content scoring of the spoken responses by non-native speakers. Again looking at the cos.w4 feature, while the WordNet Reasoning approach works well on hu-man transcriptions to further improve correlations compared to WordNet-Sense-1, it does not consist-ently improve correlations on ASR output. This may indicate that WordNet-Reasoning is more sen-sitive to ASR errors than WordNet-Sense-1. For the max.cos feature, the correlation of WordNet-Reasoning decreases significantly from WordNet-Sense-1 on prompts A and C for both human and ASR transcriptions; moreover, in the WordNet-Reasoning approach the max.cos correla-tions vary greatly on the four prompts (Table 2). We conjecture that one reason for this finding may lie in the rather small sample size of the data set, as this is an exploratory study, and the differences across prompts may be smaller when using a sub-stantially larger data set. 
92
Comparing the average reduction in correlation between human and ASR transcriptions, we find an absolute drop in correlations of 0.017 between the CVA baseline for the max.cos and of 0.019 for the cos.w4 feature. Looking at the WordNet-Sense-1 approach for the cos.w4 feature, the average corre-lation of 0.376 for human transcriptions is reduced by 0.016 to 0.360 for ASR hypotheses. Hence, we observe that the imperfect speech recognition out-put does not cause a major degradation for this content feature; the degradations observed are all in the range of 5% relative (the ASR word error rate on the test set is about 13%.) Overall, the ontology-facilitated approaches are effective for the cos.w4 feature and seem to be less appropriate for the max.cos feature. We conjecture that the characteristics of the max.cos feature may be the reason for the poor performance of the on-tology-facilitated approaches on this feature. To compute this feature, we need to compare a test vector with vectors for each score level, and it is assumed that these vectors are representative vec-tors for documents at these score levels. In reality though, while the score level 4 vector is quite a good representative for the prompt topic (highest proficiency speakers), score level vectors of less proficient speakers are less uniform and more di-verse. The reason is that there are only a few ways to appropriately represent the correct topic in a good quality spoken response but there can be many different ways of generating responses that are not on topic. For example, the score level 1 vector contains vectors generated from score 1 documents, whose words are considered mostly irrelevant for the prompt. Then, given a test docu-ment, which also contains irrelevant words for the prompt but with little overlap to the level 1 score vector, the similarity between them would be very small. Thus, any ontological approach has to face this heterogeneous distribution of words in the score level vectors for responses with lower scores; any semantic generalizations are inherently more difficult compared to those on higher scoring re-sponses. For the cos.w4 feature, in contrast, only score level 4 vectors are used, and this problem does not surface here. Finally, we observe that average correlations of both features based on ASR hypotheses (except for WordNet-Reasoning for the max.cos feature) fall in the range of 0.32-0.37. This range is well in line with our better performing features in other dimen-
sions of spontaneous speech responses, e.g., fluen-cy, pronunciation, and prosody. 7 Conclusion and Future Work In this paper, we propose using ontology-facilitated approaches for content scoring of non-native spontaneous speech due to specific merits of ontologies. Two ontology-facilitated approaches are proposed and evaluated, and their results are compared against a CVA baseline. The results in-dicate that the ontology approaches can improve content feature correlations in some circumstances. As a summary, concept-level features and reason-ing-based approaches work well on the cos.w4 content feature where test documents are compared against a vector representing all training set docu-ments with the highest human score. For future work, we plan to investigate more so-phisticated reasoning approaches. For this study, we use a simple averaging method to infer the con-cept importance based on hierarchy-inferred simi-larity metrics. As a next step, we plan to infer weights according to different similarity metrics and differential weighting of the N closest terms. Another avenue for future research is to employ different ontologies, for example, Wikipedia, which contains more concepts and entities than WordNet and has a structure that has grown more organically and less from first principles.  Wikipe-dia also has a larger pool of multi-word expres-sions and we would like to explore how representations based on the Wikipedia ontology affects automated speech scoring performance. References  Alwan, A., Bai, Y., Black, M., Casey, L., Gerosa, M., Heritage, M., & Wang, S. (2007). A system for tech-nology based assessment of language and literacy in young children: The role of multiple information sources. Proceedings of the IEEE International Workshop on Multimedia signal Processing, Greece. Attali, Y., & Burstein, J. (2006). Automated essay scor-ing with e-rater? V. 2. The Journal of Technology, Learning and Assessment, 4(3).  Balogh, J., Bernstein, J., Cheng, J., & Townshend, B.  (2007). Automatic evaluation of reading accuracy: Assessing machine scores. Proceedings of the ISCA-SLaTE-2007 Workshop, Farmington, PA, October. Bloehdorn, S., & Hotho, A. (2004). Boosting for text classification with semantic features. Workshop on mining for and from the semantic web at the 10th 
93
ACM SIGKDD conference on knowledge discovery and data mining (KDD 2004).  Burstein, J. (2003). The E-rater? scoring engine: Au-tomated essay scoring with natural language pro-cessing. In M. D. Shermis, Burstein, J.C. (Ed.), Automated essay scoring: A cross-disciplinary per-spective (pp. 113-121). Mahwah, NJ: Lawrence Erl-baum Associates, Inc. Croft, W. B., Metzler, D., & Strohman, T. (2010). Search engines: Information retrieval in practice. Boston, MA: Addison-Wesley. Cucchiarini, C., Strik, H., & Boves, L. (2000). Quantita-tive assessment of second language learners? fluency by means of automatic speech recognition technolo-gy. Journal of the Acoustical Society of America, 107(2), 989-999. Cucchiarini, C., Strik, H., & Boves, L. (2002). Quantita-tive assessment of second language learners' fluen-cy: Comparisons between read and spontaneous speech. Journal of the Acoustical Society of Ameri-ca, 111(6), 2862-2873. Dikli, S. (2006). An overview of automated scoring of essays. The Journal of Technology, Learning and As-sessment, 5(1), 1-35. Fellbaum, C. (Ed.). (1998). WordNet: An electronic lexical database. Cambridge, MA: The MIT press. Franco, H., Bratt, H., Rossier, R., Gadde, V. R., Shriberg, E., Abrash, V., & Precoda, K. (2010). EduSpeak: A speech recognition and pronunciation scoring toolkit for computer-aided language  learn-ing applications. Language Testing, 27(3), 401-418. Hotho, A., Staab, S., & Stumme, G. (2003a). Ontologies improve text document clustering. Proceedings of the Third IEEE International Conference on Data Min-ing (ICDM?03).  Hotho, A., Staab, S., & Stumme, G. (2003b). Text clus-tering based on background knowledge (Technical report, no.425.): Institute of Applied Informatics and Formal Description Methods AIFB, University of Karlsruche. Landauer, T. K., Laham, D., & Foltz, P. W. (2003). Au-tomated scoring and annotation of essays with the In-telligent Essay Assessor. In M. D. Shermis, Burstein, J.C. (Ed.), Automated essay scoring: A cross-disciplinary perspective (pp. 87?112). Mahwah, NJ: Lawrence Erlbaum Associates, Inc. Larkey, L. S., & Croft, W. B. (2003). A Text Categori-zation Approach to Automated Essay Grading. In M. D. Shermis & J. C. Burstein (Eds.), Automated Essay Scoring: A Cross-discipline Perspective: Mahwah, NJ, Lawrence Erlbaum. Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet:: Similarity: measuring the relatedness of concepts. Proceedings of the Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-04).  
Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic indexing. Communica-tions of the ACM, 18(11), 613-620. Wikipedia: The free encyclopedia (2011). FL: Wiki-media Foundation, Inc. Retrieved Apr 26, 2012, from http://www.wikipedia.org WordNet 3.0 Reference Manual. (2010). Retrieved Apr 26, 2012 from http://wordnet.princeton.edu/wordnet/documentation/ Zechner, K., Higgins, D., Xi, X, & D. M. Williamson (2009). Automatic scoring of non-native spontaneous speech in tests of spoken English. Speech Communi-cation, 51(10), 883-895. Zechner, K., & X. Xi (2008). Towards Automatic Scor-ing of a Test of Spoken Language with Heterogene-ous Task Types. Proceedings of the ACL Workshop on Innovative Use of NLP for Building Educational Applications, Columbus, OH, June.   
94
