Proceedings of the Workshop on BioNLP: Shared Task, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Event Extraction from Trimmed Dependency Graphs
Ekaterina Buyko, Erik Faessler, Joachim Wermter and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, 07743 Jena, Germany
{ekaterina.buyko|erik.faessler|joachim.wermter|udo.hahn}@uni-jena.de
Abstract
We describe the approach to event extrac-
tion which the JULIELab Team from FSU
Jena (Germany) pursued to solve Task 1 in
the ?BioNLP?09 Shared Task on Event Ex-
traction?. We incorporate manually curated
dictionaries and machine learning method-
ologies to sort out associated event triggers
and arguments on trimmed dependency graph
structures. Trimming combines pruning ir-
relevant lexical material from a dependency
graph and decorating particularly relevant lex-
ical material from that graph with more ab-
stract conceptual class information. Given
that methodological framework, the JULIELab
Team scored on 2nd rank among 24 competing
teams, with 45.8% precision, 47.5% recall and
46.7% F1-score on all 3,182 events.
1 Introduction
Semantic forms of text analytics for the life sciences
have long been equivalent with named entity recog-
nition and interpretation, i.e., finding instances of se-
mantic classes such as proteins, diseases, or drugs.
For a couple of years, this focus has been comple-
mented by analytics dealing with relation extraction,
i.e., finding instances of relations which link one or
more (usually two) arguments, the latter being in-
stances of semantic classes, such as the interaction
between two proteins (PPIs).
PPI extraction is a complex task since cascades
of molecular events are involved which are hard to
sort out. Many different approaches have already
been tried ? pattern-based ones (e.g., by Blaschke
et al (1999), Hakenberg et al (2005) or Huang et
al. (2004)), rule-based ones (e.g., by Yakushiji et al
(2001), ?Saric? et al (2004) or Fundel et al (2007)),
and machine learning-based ones (e.g., by Katrenko
and Adriaans (2006), S?tre et al (2007) or Airola et
al. (2008)), yet without conclusive results.
In the following, we present our approach to solve
Task 1 within the ?BioNLP?09 Shared Task on Event
Extraction?.1 Task 1 ?Event detection and charac-
terization? required to determine the intended rela-
tion given a priori supplied protein annotations. Our
approach considers dependency graphs as the cen-
tral data structure on which various trimming oper-
ations are performed involving syntactic simplifica-
tion but also, even more important, semantic enrich-
ment by conceptual overlays. A description of the
component subtasks is provided in Section 2, while
the methodologies intended to solve each subtask
are discussed in Section 3. The system pipeline for
event extraction reflecting the task decomposition is
described in Section 4, while Section 5 provides the
evaluation results for our approach.
2 Event Extraction Task
Event extraction is a complex task that can be sub-
divided into a number of subtasks depending on
whether the focus is on the event itself or on the ar-
guments involved:
Event trigger identification deals with the large
variety of alternative verbalizations of the same
event type, i.e., whether the event is expressed in
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask/
19
a verbal or in a nominalized form (e.g., ?A is ex-
pressed? and ?the expression of A? both refer to the
same event type, viz. expression(A)). Since the
same trigger may stand for more than one event type,
event trigger ambiguity has to be resolved as well.
Event trigger disambiguation selects the correct
event name from the set of alternative event triggers.
Event typing, finally, deals with the semantic
classification of a disambiguated event name and the
assignment to an event type category.2
Argument identification is concerned with find-
ing all necessary participants in an event, i.e., the
arguments of the relation.
Argument typing assigns the correct semantic
category (entity class) to each of the determined par-
ticipants in an event (which can be considered as in-
stances of that class).
Argument ordering assigns each identified par-
ticipant its functional role within the event, mostly
Agent (and Patient/Theme).
The sentence ?Regulation of jun and fos gene ex-
pression in human monocytes by the macrophage
colony-stimulating factor?, e.g., contains mentions
of two Gene Expression events with respective
THEME arguments ?jun? and ?fos?, triggered in the
text by the literal phrase ?gene expression?.
Task 1 of the ?BioNLP?09 Shared Task on Event
Extraction? was defined in such a way as to iden-
tify a proper relation (event) name and link it with
its type, plus one or more associated arguments de-
noting proteins. To focus on relation extraction only
no automatic named entity recognition and interpre-
tation had to be performed (subtask ?argument typ-
ing? from above); instead candidate proteins were
already pre-tagged. The complexity of Task 1 was
raised by the condition that not only proteins were
allowed to be arguments but also were events.
3 Event Extraction Solution
Our event extraction approach is summarized in Fig-
ure 1 and consists of three major streams ? first, the
detection of lexicalized event triggers (cf. Section
3.1), second, the trimming of dependency graphs
which involves pruning irrelevant and semantically
enriching relevant lexical material (cf. Section 3.2),
2In our approach, event trigger disambiguation already im-
plies event typing.
Pre-processing
Argument Identi f icat ion with
Ensemble of Classifiers
Event Detection
Post-processing
     Trimming of
Dependency Graphs
  Typing of Putative
    Event Triggers
Figure 1: General Architecture of the Event Extraction
Solution of the JULIELab Team.
and, third, the identification of arguments for the
event under scrutiny (cf. Section 3.3). Event typ-
ing results from proper event trigger identification
(see Section 3.1.2), which is interlinked with the out-
come of the argument identification. We talk about
putative triggers because we consider, in a greedy
manner, all relevant lexical items (see Section 3.1.1)
as potential event triggers which might represent an
event. Only those event triggers that can eventually
be connected to arguments, finally, represent a true
event. To achieve this goal we preprocessed both the
original training and test data such that we enrich the
original training data with automatically predicted
event triggers in order to generate more negative ex-
amples for a more effective learning of true events.3
3.1 Event Trigger Identification
Looking at the wide variety of potential lexicalized
triggers for an event, their lacking discriminative
power relative to individual event types and their
inherent potential for ambiguity,4 we decided on
a dictionary-based approach whose curation princi-
ples are described in Section 3.1.1. Our disambigua-
tion policy for the ambiguous lexicalized event trig-
3Although the training data contains cross-sentence event
descriptions, our approach to event extraction is restricted to
the sentence level only.
4Most of the triggers are neither specific for molecular event
descriptions, in general, nor for a special event type. ?Induc-
tion?, e.g., occurs 417 times in the training data. In 162 of these
cases it acts as a trigger for Positive regulation, 6 times as a
trigger for Transcription, 8 instances trigger Gene expression,
while 241 occurrences do not trigger an event at all.
20
gers assembled in this suite of dictionaries, one per
event type, is discussed in Section 3.1.2.
3.1.1 Manual Curation of the Dictionaries
We started collecting our dictionaries from the
original GENIA event corpus (Kim et al, 2008a).
The extracted event triggers were then automatically
lemmatized5 and the resulting lemmata were subse-
quently ranked by two students of biology according
to their predictive power to act as a trigger for a par-
ticular event type. This expert assessment led us to
four trigger groups (for each event type these groups
were determined separately):
(1) Triggers are important and discriminative for
a specific event type. This group contains event trig-
gers such as ?upregulate? for Positive regulation.
(2) Triggers are important though not fully dis-
criminative for a particular event type; yet, this defi-
ciency can be overcome by other lexical cues within
the context of the same sentence. This group with in-
context disambiguators contains lexical items such
as ?proteolyse? for Protein catabolism.
(3) Triggers are non-discriminative for an event
type and even cannot be disambiguated by linguistic
cues within the context of the same sentence. This
group contains lexical items such as ?presence? for
Localization and Gene expression.
(4) Triggers are absolutely non-discriminative for
an event. This group holds general lexical triggers
such as ?observe?, ?demonstrate? or ?function?.
The final dictionaries used for the detection of
putative event triggers are a union of the first two
groups. They were further extended by biologists
with additional lexical material of the first group.
The dictionaries thus became event type-specific ?
they contain all morphological forms of the original
lemma, which were automatically generated using
the Specialist NLP Tools (2008 release).
We matched the entries from the final set of dic-
tionaries with the shared task data using the Ling-
pipe Dictionary Chunker.6 After the matching pro-
cess, some cleansing had to be done.7
5We used the lemmatizer from the Specialist NLP Tools
(http://lexsrv3.nlm.nih.gov/SPECIALIST/
index.html, 2008 release).
6http://alias-i.com/lingpipe/
7Event triggers were removed which (1) were found within
sentences without any protein annotations, (2) occurred within
3.1.2 Event Trigger Disambiguation
Preliminary experiments indicated that the dis-
ambiguation of event triggers might be beneficial
for the overall event extraction results since events
tend to be expressed via highly ambiguous triggers.
Therefore, we performed a disambiguation step pre-
ceding the extraction of any argument structures.
It is based on the importance of an event trig-
ger ti for a particular event type T as defined by
Imp(tTi ) := f(t
T
i )
P
i f(tTi )
, where f(tTi ) is the frequency
of the event trigger ti of the selected event type T
in a training corpus divided by the total amount of
all event triggers of the selected event type T in
that training corpus. The frequencies are measured
on stemmed event triggers. For example, Imp for
the trigger stem ?depend? amounts to 0.013 for the
event type Positive regulation, while for the event
type Regulation it yields 0.036 . If a text span con-
tains several event triggers with the same span off-
set, the event trigger with max(Imp) is selected and
other putative triggers are discarded. The trigger
stem ?depend? remains thus only for Regulation.
3.2 Trimming Dependency Graphs
When we consider event (relation) extraction as a se-
mantic interpretation task, plain dependency graphs
as they result from deep syntactic parsing might not
be appropriate to directly extract semantic informa-
tion from. This is due to two reasons - they contain
a lot of apparently irrelevant lexical nodes (from the
semantic perspective of event extraction) and they
also contain much too specific lexical nodes that
might better be grouped and further enriched se-
mantically. Trimming dependency graphs for the
purposes of event extraction, therefore, amounts to
eliminate semantically irrelevant and to semantically
enrich relevant lexical nodes (i.e., overlay with con-
cepts). This way, we influence the final representa-
tion for the machine learners we employ (in terms of
features or kernel-based representations) ? we may
avoid an overfitting of the feature or kernel spaces
with syntactic and lexical data and thus reduce struc-
tural information in a linguistically motivated way.
a longer event trigger, (3) overlapped with a longer trigger of
the same event type, (4) occurred inside an entity mention an-
notation.
21
3.2.1 Syntactic Pruning
Pruning targets auxiliary and modal verbs which
govern the main verb in syntactic structures such as
passives, past or future tense. We delete the aux-
iliars/modals as govenors of the main verbs from
the dependency graph and propagate the semantics-
preserving dependency relations of these nodes di-
rectly to the main verbs. Adhering to the depen-
dency tree format and labeling conventions set up
for the 2006 and 2007 CONLL shared tasks on de-
pendency parsing main verbs are usually connected
with the auxiliar by the VC dependency relation (see
Figure 2). Accordingly, in our example, the verb
?activate? is promoted to the ROOT in the depen-
dency graph and governs all nodes that were origi-
nally governed by the modal ?may?.
Figure 2: Trimming of Dependency Graphs.
3.2.2 Conceptual Decoration
Lexical nodes in the (possibly pruned) depen-
dency graphs deemed to be important for argument
extraction were then enriched with semantic class
annotations, instead of keeping the original lexical
(stem) representation (see Figure 2). The rationale
behind this decision was to generate more powerful
kernel-based or features representations (see Section
3.3.2 and 3.3.1).
The whole process is based on a three-tier task-
specific semantic hierarchy of named entity classes.
The top rank is constituted by the equivalent classes
Transcription factor, Binding site, and Promoter.
The second rank is occupied by MESH terms, and
the third tier assembles the named entity classes
Gene and Protein. Whenever a lexical item is cat-
egorized by one of these categories, the associated
node in the dependency graph is overlaid with that
category applying the ranking in cases of conflicts.
We also enriched the gene name mentions with
their respective Gene Ontology Annotations from
GOA.8 For this purpose, we first categorized GO
terms both from the ?molecular function? and from
the ?biological process? branch with respect to
their matching event type, e.g., Phosphorylation
or Positive regulation. We then mapped all gene
name mentions which occurred in the text to their
UNIPROT identifier using the gene name normalizer
GENO (Wermter et al, 2009). This identifier links a
gene with a set of (curated) GO annotations.
In addition, we inserted semantic information in
terms of the event trigger type and the experimen-
tal methods. As far as experimental methods are
concerned, we extracted all instances of them an-
notated in the GENIA event corpus. One student
of biology sorted the experimental methods relative
to the event categories under scrutiny. For example
?affinity chromatography? was assigned both to the
Gene expression and to the Binding category. For
our purposes, we only included those GO annota-
tions and experimental methods which matched the
event types to be identified in a sentence.
3.3 Argument Identification and Ordering
The argument identification task can be subdivided
into three complexity levels. Level (1) incorpo-
rates five event types (Gene expression, Transcrip-
tion, Protein catabolism, Localization, Phosphory-
lation) which involve a single participant with a
THEME role only. Level (2) is concerned with one
event type (Binding) that provides an n-ary argument
structure where all arguments occupy the THEME(n)
role. Level (3) comprises three event types (Posi-
tive regulation, Negative regulation, or an unspeci-
fied Regulation) that represent a regulatory relation
between the above-mentioned event classes or pro-
teins. These events have usually a binary structure,
with a THEME argument and a CAUSE argument.
For argument extraction, we built sentence-wise
pairs of putative triggers and their putative argu-
ment(s), the latter involving ontological informa-
tion about the event type. For Level (1), we built
pairs only with proteins, while for Level (3) we al-
8http://www.ebi.ac.uk/GOA
22
lowed all events as possible arguments. For Level
(2), Binding events, we generated binary (trigger,
protein) pairs as well as triples (trigger, protein1,
protein2) to adequately represent the binding be-
tween two proteins.9 Pairs of mentions not con-
nected by a dependency path could not be detected.
For the argument extraction we chose two ma-
chine learning-based approaches, feature-based and
a kernel-based one, as described below.10
3.3.1 Feature-based Classifier
We distinguished three groups of features. First,
lexical features (covering lexical items before, af-
ter and between both mentions (of the event trigger
and an argument) as described by Zhou and Zhang
(2007)); second, chunking features (concerned with
head words of the phrases between two mentions as
described by Zhou and Zhang (2007)); third, de-
pendency parse features (considering both the se-
lected dependency levels of the arguments (parents
and least common subsumer) as discussed by Ka-
trenko and Adriaans (2006), as well as a shortest de-
pendency path structure between the arguments as
used by Kim et al (2008b) for walk features).
For the feature-based approach, we chose the
Maximum Entropy (ME) classifier from MALLET.11
3.3.2 Graph Kernel Classifier
The graph kernel uses a converted form of depen-
dency graphs in which each dependency node is rep-
resented by a set of labels associated with that node.
The dependency edges are also represented as nodes
in the new graph such that they are connected to the
nodes adjacent in the dependency graph. Subgraphs
which represent, e.g., the linear order of the words
in the sentence can be added, if required. The entire
graph is represented in terms of an adjacency matrix
which is further processed to contain the summed
weights of paths connecting two nodes of the graph
(see Airola et al (2008) for details).
9We did not account for the binding of more than two pro-
teins as this would have led to a combinatory explosion of pos-
sible classifications.
10In our experiments, we used full conceptual overlaying
(see Section 3.2) for the kernel-based representation and partial
overlaying for the dependency parse features (only gene/protein
annotation was exploited here). Graph representations allow for
many semantic labels to be associated with a node.
11http://mallet.cs.umass.edu/index.php/
Main_Page
Figure 3: Graph Kernel Representation for a Trimmed
Dependency Graph ? (1) original representation, (2)
representation without graph dependency edge nodes
(weights (0.9, 0.3) taken from Airola et al (2008)).
For our experiments, we tried some variants of the
original graph kernel. In the original version each
dependency graph edge is represented as a node.
That means that connections between graph token
nodes are expressed through graph dependency edge
nodes (see Figure 3; (1)). To represent the connec-
tions between original tokens as direct connections
in the graph, we removed the edge nodes and each
token was assigned the edge label (its dependency
label; see Figure 3; (2)). Further variants included
encodings for (1) the shortest dependency path (sp)
between two mentions (argument and trigger)12 (2)
the complete dependency graph (sp-dep), and (3) the
complete dependency graph and linear information
(sp-dep-lin) (the original configuration from Airola
et al (2008)).
For the graph kernel, we chose the LibSVM
(Chang and Lin, 2001) Support Vector Machine as
classifier.
3.4 Postprocessing
The postprocessing step varies for the three different
Levels (see Section 3.3). For every event trigger of
Level (1) (e.g., Gene expression), we generate one
event per relation comprising a trigger and its argu-
ment. For Level (2) (Binding), we create a Binding
event with two arguments only for triples (trigger,
protein1, protein2). For the third Level, we create
for each event trigger and its associated arguments
e = n ? m events, for n CAUSE arguments and m
THEME arguments.
12For Binding we extracted the shortest path between two
protein mentions if we encounter a triple (trigger, protein1,
protein2).
23
4 Pipeline
The event extraction pipeline consists of two ma-
jor parts, a pre-processor and the dedicated event
extractor. As far as pre-processing is concerned,
we imported the sentence splitting, tokenization and
GDep parsing results (Sagae and Tsujii, 2007) as
prepared by the shared task organizers for all data
sets (training, development and test). We processed
this data with the OpenNLP POS tagger and Chun-
ker, both re-trained on the GENIA corpus (Buyko et
al., 2006). Additionally, we enhanced the original
tokenization by one which includes hyphenization
of lexical items such as in ?PMA-dependent?. 13
The data was further processed with the gene nor-
malizer GENO(Wermter et al, 2009) and a num-
ber of regex- and dictionary-based entity taggers
(covering promoters, binding sites, and transcrip-
tion factors). We also enriched gene name men-
tions with their respective Gene Ontology annota-
tions (see Section 3.2.2). The MESH thesaurus (ex-
cept chemical and drugs branch) was mapped on the
data using the Lingpipe Dictionary Chunker.14
After preprocessing, event extraction was started
distinguishing between the event trigger recognition
(cf. Section 3.1), the trimming of the dependency
graphs (cf. Section 3.2), and the argument extrac-
tion proper (cf. Section 3.3).15 We determined in
our experiments on the development data the perfor-
mance of every classifier type and its variants (for
the graph kernel), and of ensembles of the most per-
formant (F-Score) graph kernel variant and an ME
model.16 We present here the argument extraction
configuration used for the official run.17 For the
prediction of Phosphorylation, Localization, Pro-
tein catabolism types we used the graph kernel in
13This tokenization is more advantageous for the detection
of additional event triggers as it allows to generate depen-
dency relations from hyphenated terms. For example, in ?PMA-
dependent?, ?PMA? will be a child of ?dependent? linked by
the AMOD dependency relation, and ?dependent? receives the
original dependency relation of the ?PMA-dependent? token.
14http://alias-i.com/lingpipe/
15For the final configurations of the graph kernel, we opti-
mized the C parameter in the spectrum between 2?3 and 23 on
the final training data for every event type separately.
16In the ensemble configuration we built the union of positive
instances.
17We achieved with this configuration the best performance
on the development set.
its ?sp without dependency-edge-nodes? configura-
tion, while for the prediction of Transcription and
Gene expression events we used an ensemble of the
graph kernel in its ?sp with dependency-edge-nodes?
variant, and an ME model. For the prediction of
Binding we used an ensemble of the graph kernel
(?sp-dep with dependency-edge-nodes?) and an ME
model. For the prediction of regulatory events we
used ME models for each regulatory type.
5 Results
The baseline against which we compared our ap-
proach can be captured in a single rule. We extract
for every pair of a putative trigger and a putative ar-
gument the shortest dependency path between them.
If the shortest dependency path does not contain any
direction change, i.e., the argument is either a direct
child or a direct parent of the trigger, and if the path
does not contain any other intervening event trig-
gers, the argument is taken as the THEME role.
We performed evaluations on the shared task de-
velopment and test set. Our baseline achieved com-
petitive results of 36.0% precision, 34.0% recall,
35.0% F-score on the development set (see Table
1), and 30.4% precision, 35.7% recall, 32,8% F-
score on the test set (see Table 2). In particular
the one-argument events, i.e., Gene expression, Pro-
tein catabolism, Phosphorylation are effectively ex-
tracted with an F-score around 70.0%. More com-
plex events, in particular events of Level (3), i.e.,
(Regulation) were less properly dealt with because
of their strong internal complexity.
Event Class gold recall prec. F-score
Localization 53 75.47 30.30 43.24
Binding 248 33.47 20.80 25.66
Gene expression 356 76.12 75.07 75.59
Transcription 82 68.29 40.58 50.91
Protein catabolism 21 76.19 66.67 71.11
Phosphorylation 47 76.60 72.00 74.23
Regulation 169 14.20 15.09 14.63
Positive regulation 617 15.40 20.83 17.71
Negative regulation 196 11.73 13.22 12.43
TOTAL 1789 36.00 34.02 34.98
Table 1: Baseline results on the shared task development
data. Approximate Span Matching/Approximate Recur-
sive Matching.
24
Event Class gold recall prec. F-score gold recall prec. F-score
Localization 174 42.53 44.85 43.66 174 42.53 44.85 43.66
Binding 347 32.28 37.09 34.51 398 44.22 58.28 50.29
Gene expression 722 61.36 80.55 69.65 722 61.36 80.55 69.65
Transcription 137 39.42 35.06 37.11 137 39.42 35.06 37.11
Protein catabolism 14 71.43 66.67 68.97 14 71.43 66.67 68.97
Phosphorylation 135 65.93 90.82 76.39 135 65.93 90.82 76.39
EVT-TOTAL 1529 51.14 60.90 55.60 1580 53.54 65.89 59.08
Regulation 291 9.62 11.72 10.57 338 9.17 12.97 10.75
Positive regulation 983 10.38 11.33 10.83 1186 14.67 19.33 16.68
Negative regulation 379 14.25 19.22 16.36 416 14.18 21.00 16.93
REG-TOTAL 1653 11.13 12.96 11.98 1940 13.61 18.59 15.71
ALL-TOTAL 3182 30.36 35.72 32.82 3520 31.53 41.05 35.67
Table 2: Baseline results on the shared task test data. Approximate Span Matching/Approximate Recursive Matching
(columns 3-5). Event decomposition, Approximate Span Matching/Approximate Recursive Matching (columns 7-9).
The event extraction approach, in its final config-
uration (see Section 4), achieved a performance of
50.4% recall, 45.8% precision and 48.0% F-score on
the development set (see Table 4), and 45.8% recall,
47.5% precision and 46.7% F-score on the test set
(see Table 3). This approach clearly outperformed
the baseline with an increase of 14 percentage points
on the test data. In particular, the events of Level (2)
and (3) were more properly dealt with than by the
baseline. In the event decomposition mode (argu-
ment detection is evaluated in a decomposed event)
we achieved a performance of 49.4% recall, 56.2%
precision, and 52.6% F-score (see Table 3).
Our experiments on the development set showed
that the combination of the feature-based and the
graph kernel-based approach can boost the results up
to 6 percentage points F-score (for the Binding event
type). It is interesting that the combination for Bind-
ing increased recall without dropping precision. The
original graph kernel approach for Binding events
performs with 38.3% recall, 27.9% precision and
32.3% F-score on the development set. The com-
bined approach comes with a remarkable increase
of 14 percentage points in recall. The combination
could also boost the recall of the Gene expression
and Transcription by 15 percentage points and 5 per-
centage points, respectively, without seriously drop-
ping the precision (4 points for every type). For
the other event types, no improvements were found
when we combined both approaches.
5.1 Error Discussion
One expert biologist analyzed 30 abstracts randomly
extracted from the development error data. We de-
termined seven groups of errrors based on this anal-
ysis. The first group contains examples for which
an event should be determined, but a false argument
was found (e.g., Binding arguments were not prop-
erly sorted, or correct and false arguments were de-
tected for the same trigger) (44 examples). The sec-
ond group comprised examples where no trigger was
found (23 examples). Group (3) stands for cases
where no events were detected although a trigger
was properly identified (14 examples). Group (4)
holds examples detected in sentences which did not
contain any events (12 examples). Group (5) lists bi-
ologically meaningful analyses, actually very close
to the gold annotation, especially for the cascaded
regulatory events (12 examples), while Group (6) in-
corporates examples of a detected event with incor-
rect type (1 example). Group (7) gathers misleading
gold annotations (10 examples).
This assessment clearly indicates that a major
source of errors can be traced to the level of argu-
ment identification, in particular for Binding events.
The second major source has its offspring at the
level of trigger detection (we ignored, for exam-
ple, triggers such as ?in the presence of ?, ?when?,
?normal?). About 10% of the errors are due to a
slight difference between extracted events and gold
events. For example, in the phrase ?role for NF-
kappaB in the regulation of FasL expression? we
25
Event Class gold recall prec. F-score gold recall prec. F-score
Localization 174 43.68 77.55 55.88 174 43.68 77.55 55.88
Binding 347 49.57 35.25 41.20 398 63.57 54.88 58.91
Gene expression 722 64.82 80.27 71.72 722 64.82 80.27 71.72
Transcription 137 35.77 62.03 45.37 137 35.77 62.03 45.37
Protein catabolism 14 78.57 84.62 81.48 14 78.57 84.62 81.48
Phosphorylation 135 76.30 91.15 83.06 135 76.30 91.15 83.06
EVT-TOTAL 1529 57.49 63.97 60.56 1580 60.76 71.27 65.60
Regulation 291 31.27 30.13 30.69 338 35.21 37.54 36.34
Positive regulation 983 34.08 37.18 35.56 1186 40.64 49.33 44.57
Negative regulation 379 40.37 31.16 35.17 416 42.31 39.11 40.65
REG-TOTAL 1653 35.03 34.18 34.60 1940 40.05 44.55 42.18
ALL-TOTAL 3182 45.82 47.52 46.66 3520 49.35 56.20 52.55
Table 3: Offical Event Extraction results on the shared task test data of the JULIELab Team. Approximate
Span Matching/Approximate Recursive Matching (columns 3-5). Event decomposition, Approximate Span Match-
ing/Approximate Recursive Matching (columns 7-9).
could not extract the gold event Regulation of Regu-
lation (Gene expression (FasL)) associated with the
trigger ?role?, but we were able to find the (inside)
event Regulation (Gene expression (FasL)) associ-
ated with the trigger ?regulation?. Interestingly, the
typing of events is not an error source in spite of
the simple disambiguation approach. Still, our dis-
ambiguation strategy is not appropriate for the anal-
ysis of double-annotated triggers such as ?overex-
pression?, ?transfection?, etc., which are annotated
as Gene expression and Positive regulation and are
a major source of errors in Group (2). As Group
(6) is an insignificant source of errors in our ran-
domly selected data, we focused our error analysis
on the especially ambiguous event type Transcrip-
tion. We found from 34 errors that 14 of them were
due to the disambiguation strategy (in particular for
triggers ?(gene) expression? and ?induction?).
6 Conclusion
Our approach to event extraction incorporates man-
ually curated dictionaries and machine learning
methodologies to sort out associated event triggers
and arguments on trimmed dependency graph struc-
tures. Trimming combines pruning irrelevant lexi-
cal material from a dependency graph and decorat-
ing particularly relevant lexical material from that
graph with more abstract conceptual class informa-
tion. Given that methodological framework, the
JULIELab Team scored on 2nd rank among 24 com-
Event Class gold recall prec. F-score
Localization 53 71.70 74.51 73.08
Binding 248 52.42 29.08 37.41
Gene expression 356 75.28 81.46 78.25
Transcription 82 60.98 73.53 66.67
Protein catabolism 21 90.48 79.17 84.44
Phosphorylation 47 82.98 84.78 83.87
Regulation 169 37.87 36.78 37.32
Positive regulation 617 34.36 35.99 35.16
Negative regulation 196 41.33 33.61 37.07
TOTAL 1789 50.36 45.76 47.95
Table 4: Event extraction results on the shared task
development data of the official run of the JULIELab
Team. Approximate Span Matching/Approximate Recur-
sive Matching.
peting teams, with 45.8% precision, 47.5% recall
and 46.7% F1-score on all 3,182 events.
7 Acknowledgments
We wish to thank Rico Landefeld for his technical
support, Tobias Wagner and Rico Pusch for their
constant help and great expertise in biological is-
sues. This research was partially funded within the
BOOTSTREP project under grant FP6-028099 and
the CALBC project under grant FP7-231727.
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008. A
26
graph kernel for protein-protein interaction extraction.
In Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing, pages 1?9.
Christian Blaschke, Miguel A. Andrade, Christos Ouzou-
nis, and Alfonso Valencia. 1999. Automatic ex-
traction of biological information from scientific text:
Protein-protein interactions. In ISMB?99 ? Proceed-
ings of the 7th International Conference on Intelligent
Systems for Molecular Biology, pages 60?67.
Ekaterina Buyko, Joachim Wermter, Michael Poprat, and
Udo Hahn. 2006. Automatically adapting an NLP
core engine to the biology domain. In Proceedings
of the Joint BioLINK-Bio-Ontologies Meeting. A Joint
Meeting of the ISMB Special Interest Group on Bio-
Ontologies and the BioLINK Special Interest Group on
Text Data M ining in Association with ISMB, pages
65?68. Fortaleza, Brazil, August 5, 2006.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2007. Relex-relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Jo?rg Hakenberg, Ulf Leser, Conrad Plake, Harald Kirsch,
and Dietrich Rebholz-Schuhmann. 2005. LLL?05
challenge: Genic interaction extraction - identifica-
tion of language patterns based on alignment and finite
state automata. In Proceedings of the 4th Learning
Language in Logic Workshop (LLL05), pages 38?45.
Minlie Huang, Xiaoyan Zhu, Donald G. Payan, Kun-
bin Qu, and Ming Li. 2004. Discovering patterns
to extract protein-protein interactions from full texts.
Bioinformatics, 20(18):3604?3612.
Sophia Katrenko and Pieter W. Adriaans. 2006. Learn-
ing relations from biomedical corpora using depen-
dency trees. In Karl Tuyls, Ronald L. Westra, Yvan
Saeys, and Ann Nowe?, editors, KDECB 2006 ? Knowl-
edge Discovery and Emergent Complexity in Bioin-
formatics. Revised Selected Papers of the 1st Inter-
national Workshop., volume 4366 of Lecture Notes
in Computer Science, pages 61?80. Ghent, Belgium,
May 10, 2006. Berlin: Springer.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008a.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Seon-Ho Kim, Juntae Yoon, and Jihoon Yang. 2008b.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii. 2007. Syn-
tactic features for protein-protein interaction extrac-
tion. In Christopher J. O. Baker and Jian Su, editors,
LBM 2007, volume 319, pages 6.1?6.14.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and par ser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050.
Jasmin ?Saric?, Lars J. Jensen, Rossitza Ouzounova, Isabel
Rojas, and Peer Bork. 2004. Extracting regulatory
gene expression networks from pubmed. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 191, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Joachim Wermter, Katrin Tomanek, and Udo Hahn.
2009. High-performance gene name normalization
with GeNo. Bioinformatics, 25(6):815?821.
Akane Yakushiji, Yuka Tateisi, Yusuke Miyao, and
Jun?ichi Tsujii. 2001. Event extraction from biomed-
ical papers using a full parser. In Russ B. Altman,
A. Keith Dunker, Lawrence Hunter, Kevin Lauderdale,
and Teri E. Klein, editors, PSB 2001 ? Proceedings
of the 6th Pacific Symposium on Biocomputing, pages
408?419. Maui, Hawaii, USA. January 3-7, 2001. Sin-
gapore: World Scientific Publishing.
Guodong Zhou and Min Zhang. 2007. Extracting re-
lation information from text documents by exploring
various types of knowledge. Information Processing
& Management, 43(4):969?982.
27
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 235?242,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Proposal for a Configurable Silver Standard
Udo Hahn, Katrin Tomanek, Elena Beisswanger and Erik Faessler
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, 07743 Jena, Germany
http://www.julielab.de
Abstract
Among the many proposals to promote al-
ternatives to costly to create gold stan-
dards, just recently the idea of a fully au-
tomatically, and thus cheaply, to set up sil-
ver standard has been launched. However,
the current construction policy for such a
silver standard requires crucial parameters
(such as similarity thresholds and agree-
ment cut-offs) to be set a priori, based on
extensive testing though, at corpus com-
pile time. Accordingly, such a corpus is
static, once it is released. We here propose
an alternative policy where silver stan-
dards can be dynamically optimized and
customized on demand (given a specific
goal function) using a gold standard as an
oracle.
1 Introduction
Training natural language systems which rely on
(semi-)supervised machine learning algorithms,
or measuring the systems? performance requires
some standardized ground truth from which one
can learn or against which one evaluate, respec-
tively. Usually, a manually crafted gold stan-
dard is provided that is generated by human lan-
guage or domain experts after lots of iterative,
guideline-based training rounds. This procedure is
expensive, slow and yields only small, yet highly
trustable, amounts of meta data ? because human
experts are in the loop.
In the CALBC project,1 an alternative ap-
proach is currently under investigation (Rebholz-
Schuhmann et al, 2010a). The basic idea is to
generate the much needed ground truth automati-
cally. This is achieved by letting a flock of named
entity taggers run on a corpus, without impos-
ing any restriction on the type(s) being annotated.
1http://www.calbc.eu
The (most likely) heterogeneous results are auto-
matically homogenized subsequently, thus yield-
ing a consensus-based, machine-generated ground
truth. Considering the possible benefits (e.g., the
positive experience from boosting-style machine
learners (Freund, 1990)), but also being aware of
the possible drawbacks (varying quality of the dif-
ferent systems, skewed coverage of entity types,
different types of guidelines on which they were
trained, etc.), the CALBC consortium refers to
the outcome of this process as a silver standard
(Rebholz-Schuhmann et al, 2010a). This proce-
dure is inexpensive, fast, yields huge amounts of
meta data ? because computers are in the loop ?
but after all its applicability and validity has yet to
be determined experimentally.
The first silver standard corpus (SSC) that came
out of the CALBC project was generated by the
four main partners? named entity taggers.2 The
various contributions covered, among others, an-
notations for genes and proteins, chemicals, dis-
eases, etc (Rebholz-Schuhmann et al, 2010b). Af-
ter the submission of their runs, the SSC was gen-
erated by, first, harmonizing stretches of text in
terms of entity mention identification and, second,
by mapping these normalized mentions to agreed-
upon type systems (such as the MESH Semantic
Groups as described by Bodenreider and McCray
(2003) for entity type normalization). Basically,
the harmonization steps included rules when en-
tity mentions were considered to match or overlap
(using a cosine-based similarity criterion) and en-
tity types referred to the same class. For consensus
generation, finally, simple rules for majority votes
were established.
The CALBC consortium is fully aware of the
fact that the value of an SSC can only be assessed
2The CALBC consortium consists the Rebholz Group
from EBI (Hinxton, U.K.), the Biosemantics Group from
Erasmus (Rotterdam, The Netherlands), the JULIE Lab (Jena,
Germany), and LINGUAMATICS (Cambridge, U.K.).
235
by comparing, e.g., systems trained on such a sil-
ver standard with systems trained on a gold stan-
dard (preferably, though not necessarily, one that
is a subset of the document set which makes up the
SSC).
In the absence of such a gold standard, the
CALBC consortium has spent enormous efforts to
find out the most reasonable parameter settings
for, e.g., the cosine threshold (setting similar men-
tions apart from dissimilar ones) or the consen-
sus constraint (where a certain number of entity
types equally assigned by different taggers makes
one type the consensual silver one and discards all
alternative annotations). Once these criteria are
made effective, the SSC is completely fixed.
As an alternative, we are looking for a more
flexible solution. Our investigation was fuelled by
the following observations:
? The idiosyncrasies of guidelines (on which
(some) taggers were trained) do not necessar-
ily lead to semantically totally different enti-
ties although they differ literally to some de-
gree. Some guidelines prefer, e.g., ?human
IL-7 protein?, others favor ?human IL-7?,
and some lean towards ?IL-7?. As the cosine
measure tends to penalize a pair such as ?hu-
man IL-7 protein? and ?IL-7?, we intended
to avoid such a prescriptive mode and just
look at the type assignment for single tokens
as (parts of) entity mentions. thus avoiding
inconclusive mention boundary discussions.
? While we were counting, for all tokens of
the document set, the votes a single token re-
ceived from different taggers in terms of an-
notating this token with respect to some type,
we generated confidence data for meta data
assignments. Incorporating the distribution
of confidence values into the configuration
process, this allows us to get rid of a pri-
ori fixed majority criteria (e.g., two or three
out of five systems must agree on this token)
which are hard to justify in an absolute way.
Summarizing, we believe that the nature of di-
verging tasks to be solved, the levels of entity type
specificity to be reached, the sort of guidelines be-
ing preferred, etc. should allow prospective users
of a silver standard to customize one on their own
and not stick to one that is already prefabricated
without concrete application in mind.3
3There may be tasks where a ?long? entity such as ?hu-
As such an enterprise would be quite arbitrary
without a reference standard, we even go one step
further. We determine the suitability of, say, dif-
ferent voting scores and varying lexical extensions
of mentions by comparison to a gold standard so
that the ?optimal? configuration of a silver stan-
dard, given a set of goal-derived requirements,
can be automatically learned. In real-world ap-
plications, such gold standard annotations would
be delivered only for a fraction of the documents
contained in the entire corpus being tagged by a
flock of taggers. The gold standard is used to op-
timize parameters which are subsequently applied
to the aggregation of automatically annotated data.
Note that the gold standard is used for optimiza-
tion only, not for training. We call such a flexible,
dynamically adjustable silver standard a config-
urable Silver Standard Corpus (conSSC). In a sec-
ond step, we split the various conSSCs, re-trained
our NER tagger on these data sets and, by compar-
ison with the gold standard, were able to identify
the optimal conSSC for this task (which is not the
one (SSC I) made available by the CALBC consor-
tium for the first challenge round).4
2 Optimizing Silver Standards
In this section, we describe the constituent param-
eters of a wide spectrum of SSCs. Mostly, these
parameters were taken over from the design of the
SSC as developed by the CALBC project members.
Differing from that fixed SSC, we investigate the
impact of different parameter settings on the con-
struction of a collection of SSCs, and, first, eval-
uate their direct usefulness on a gold standard for
protein-gene annotations. Second, we also assess
their indirect usefulness by training NER classi-
fiers on these SSCs and evaluate the NERs? perfor-
mance on the gold standard. Thus, our approach
is entirely data-driven without the need for human
intervention in terms of choosing suitable param-
eter settings.
Technically, we first aggregate the votes from
the flock of taggers (in our experiments, we used
the four taggers from the CALBC project members
plus a second tagger of one of the members) for
each text token (for confidence-based decisions)
or at the entity level (for cosine-based decisions),
then we determine the confidence values of these
man IL-7 protein? may be appropriate, while for another task
a short one such as ?IL-7? is entirely sufficient.
4http://www.ebi.ac.uk/Rebholz-srv/
CALBC/challenge.html
236
aggregated votes, and, finally, we compute the
similarity of the various SSCs with the gold stan-
dard data in terms of F-scores (both exact and open
boundaries) and accuracy on the token level.
2.1 Calibrating Consensus
The metrical interpretation of consensus will be
based on thresholded votes for semantic groups at
the token level (cf. Section 2.1.1) and a cosine-
based measure to determine contiguous stretches
of entity mentions in the text (cf. Section 2.1.2).
2.1.1 Type Confidence and Type Voting
For each text token, we determine the entity type
assignment as generated by each NER tagger
which is part of the flock of CALBC taggers.5 We
count and aggregate these votes such that each en-
tity type has an associated type count value.
We then compute the ratio of systems agree-
ing on the same single type assignment and call
this the confidence attributed to a particular type
for some token. The confidence value will sub-
sequently be interpreted against the confidence
threshold [0, 1] that defines a measure of certainty
a type assignment should have in order to be ac-
cepted as consensual.
2.1.2 Cosine-based Similarity of Phrasal
Entity Mentions
As the above policy of token-wise annotation de-
couples contiguous entity mentions spanning over
more than one token, we also want to restitute this
phrasal structure. This is achieved by constructing
contiguous sequences of tokens that characterize a
phrasal entity mention at the text level to which the
same type label has been assigned. Since differ-
ent taggers tend to identify different spans of text
for the same entity type (as shown in the exam-
ple from Section 1) we have to account for similar
phrasal forms of named entity mentions.
This is achieved by constructing vectors which
represent entity mentions and by computing the
cosine between the different entity mention vec-
tors. Let E1 = T1T2T3 be an entity mention com-
prised of three tokens T1 to T3. Let E2 = T2T3 be
5Due to time constraints when we performed our experi-
ments, we make an extremely simplifying assumption: From
the whole range of possible entity types NER taggers may as-
sign to some token (cf. (Bodenreider and McCray, 2003)) we
have chosen the PRotein/GEne group for testing. Still, this
assumption does not do harm to the core of our hypotheses.
See also our discussion in Section 5.
an entity mention overlapping with E1 in the to-
kens T2 and T3. To decide whether E1 and E2 are
considered similar, we first construct two vectors
representing the entity mentions:
v(E1) = (f1, f2, f3)T
with fi = IDF (Ti) being the inverse document
frequency of the token Ti. We compute the in-
verse document frequency of tokens based on the
corpus which is subject to analysis. Analogously,
we construct the vector for E2
v(E2) = (0, f2, f3)T
filling in a zero for the IDF of T1 since it is not
covered by E2. The entity mentions E1 and E2
are considered equal or similar, if the cosine of
the two vectors is greater or equal a given thresh-
old, cos(v(E1), v(E2)) ? threshold.6 We then
compute the number of systems considering an en-
tity annotation as similar in the manner described
above. The annotation is accepted and thus en-
tered into the SSC, if a particular number of sys-
tems agree on one annotation. This approach was
previously developed by the CALBC project part-
ners (Rebholz-Schuhmann et al, 2010a).
The number of agreeing systems and the thresh-
old are the free parameters of this method and thus
subject to optimization.
2.2 Optimization of Silver Standard Corpora
In the experiments described in the next section,
we will consider alternative parametrizations for
Silver Standard Corpora, i.e., the required confi-
dence threshold or cosine threshold and the num-
ber of agreeing systems. We will then discuss two
variants for optimizing this collection of SSCs.
The first one directly uses the gold standard for op-
timization. The task will be to find that particular
parameter setting for an SSC which best fits the
data contained in the gold standard. Once these
parameters are determined they can be applied to
the complete CALBC document set (composed of
100,000 documents) to produce the final, quasi-
optimal SSC.
In another variant, we insert a classifier into this
loop. First, we train a classifier on a particular
6For final corpus creation, it must be decided which of the
matching entity mentions is entered into the reference SSC,
e.g. the longest or shortest entity annotation. In our exper-
iments, we always chose the shortest entity mention. How-
ever, preliminary experiments showed that the differences to
taking the longest entity mention were marginal.
237
SSC that is built from a particular parameter com-
bination. Next, this classifier is tested against the
gold standard. This is iterated through all parame-
ter combinations. Obviously, the best performing
classifier relative to the gold standard selects the
optimal SSC.
3 Experimental Setting
3.1 Gold Standard
We generated a new broad-coverage corpus com-
posed of 3,236 MEDLINE abstracts (35,519 sen-
tences or 941,890 tokens) dealing with gene
and protein mentions. Altogether, it comprises
57,889 named entity type annotations annotated
by one expert biologist. We created this new re-
source to have a consistent and (as far as pos-
sible) subdomain-independent protein-annotated
corpus.7
MEDLINE abstracts were annotated with (pro-
tein coding) genes, mRNAs and proteins. A
distinction was made between dedicated proteins
as they are recorded in the protein database
UNIPROT,8 protein complexes consisting of sev-
eral protein subunits (e.g., IL-2 receptor consist-
ing of ?, ?, and ? chain), and protein families or
groups (e.g., ?transcription factors?). Also enu-
merations of proteins and protein variants were an-
notated. Discontinuous annotations were avoided
as well as nested annotations (annotations embed-
ded in other annotations). However, gene/protein
mentions nested in terms other than gene/protein
mentions were annotated (e.g., protein mentions
nested in protein function descriptions such as
?ligase? in ?ligase activity?). Modifiers such as
species designators were excluded from annota-
tions whenever possible. Gene segments or pro-
tein fragments were also not annotated.
For our experiments, we did not distinguish be-
tween the different annotation classes (see Table
1) but merged all available annotations into one
class, viz. PRotein/GEne (PRGE).
3.2 Automatic Annotation of the Gold Standard
We then asked all four sites participating in the
CALBC project to automatically annotate the given
gold standard (made available without gold data,
7We are aware of other gene/protein-annotated corpora
such as PENNBIOIE (http://bioie.ldc.upenn.
edu/) or GENIA (http://www-tsujii.is.s.
u-tokyo.ac.jp/GENIA/home/wiki.cgi) that will
have to be taken into account in future studies as well.
8http://www.uniprot.org/
semantic type description
T028 Gene or Genome
T086 Nucleotide Sequence
T087 Amino Acid Sequence,
Amino Acid, Peptide
T116 Protein
T126 Enzyme
T192 Receptor
Table 1: Semantic types defining the PRGE group
(semantic type codes refer to the UMLS).
of course) using the same type of named entity tag-
ging machinery as was used to annotate CALBC?s
canonical SSC. The performance results of each
group?s system evaluated against the gold standard
are reported in Table 2. The data of each system
constitute the reference data sets and raw data for
all subsequent experiments on the configuration
and optimization of the silver standard.
The resulting raw material does thus not only
contain gene/protein annotations but also any
other entity types as supplied by the partners.
For our experiments on the gold standard, how-
ever, only the entity types subsumed by the PRGE
group (see Table 1) were considered and annota-
tions of all other types were discarded. The def-
inition of the PRGE group is identical to the one
proposed by Rebholz-Schuhmann et al (2010a).
For the experiments, the specific semantic types
(e.g., the UMLS concepts)9 were not considered,
only the semantic group PRGE was.
3.3 Evaluation Metrics
The following metrics were used to evaluate how
good the silver standard(s) fit(s) the provided gold
standard:
? segment-level recall, precision, and F-score
values with exact boundaries, the standard
way to evaluate NER taggers,
? segment-level recall, precision, and F-score,
but with relaxed boundary constraints. This
means that two entity mentions are consid-
ered to match when they overlap with at least
one token and have the same entity type as-
signed to them,
? accuracy measured on the token level.
These metrics can be considered as optimization
criteria.
9http://www.nlm.nih.gov/research/umls/
238
3.4 Tokenization
The CALBC partners? data do not necessarily
come with tokenization information and, more-
over, different partners/systems might have differ-
ent tokenizations. Since a common ground for
comparison is thus lacking we added a new, con-
sistent tokenization based on the JULIE Lab tok-
enizer (Tomanek et al, 2007b). This tokenizer is
optimized for biomedical documents with intrinsic
focus to keep complex biological terminological
units (such as ?IL-2?) unsegmented, but to split
up tokens that are not terminologically connected
(such as dividing ?IL-2-related? up into ?IL-2?,
?-? and ?related?). As a matter of fact, entity
boundaries do not necessarily coincide with token
boundaries. Our solution to this problem is as fol-
lows: Whenever a token partially overlaps with an
entity name, the full form of that token is consid-
ered to be associated with this entity. All data on
which we report here (silver and gold standards)
obey to this tokenization scheme.
3.5 Parameters Being Tested
The following parameter settings were considered
in our experiments:
? Four different values for confidence thresh-
olds indicating that 20% (0.2), 40% (0.4),
60% (0.6) or 80% (0.8) of all taggers agreed
on the same type annotation, viz. PRGE,
? Five different values for cosine thresholds
to identify overlapping entity mentions, viz.
(0.7, 0.8, 0.9, 0.95, 0.975), and two different
values for the number n of agreeing taggers,
viz. n ? 2 and n ? 3,
? Two tagger crowd scenarios, viz. one where
all five systems were involved, the other
where subsets of cardinality 2 of these
crowds were re-combined.10
4 Results
As already described in Section 2.2, we performed
two types of experiments. In the first experiment
(Section 4.1), we intend to find proper calibrations
of parameters for an optimal SSC as described in
Section 3.5. In the second experiment (Section
4.2), we incorporate an extrinsic task, training an
NER classifier on different parameter settings, as
a selector for the optimal SSC.
10We refrained from also testing combinations of 3 and 4
systems due to time constraints.
4.1 Intrinsic Calibration of Parameters
Full Merger of All Taggers. In this scenario,
we tested the merged results of the entire crowd of
CALBC taggers when compared to the gold stan-
dard and determined their performance scores (see
Table 3). We will discuss the results with respect
to the overlapping F-score, if not explicitly stated
otherwise.
Looking at the results of the runs involving dif-
ferent cosine thresholds, we witness a systematic
drawback when more than two systems are re-
quired to agree. Although precision is boosted in
this setting, recall is decreasing strongly which re-
sults in overall lower F-scores. When only two
systems are required to agree a comparatively
higher recall comes at the cost of lower preci-
sion. Yet, the F-score (both under exact as well
as overlap conditions) is always superior (ranging
between 75% and 73%) when compared to the 3-
agreement scenario. Note that the 2-agreement
condition for the highest threshold being tested
yields, without exception, better scores than the
best single system (cf. Table 2).
The best performing run in terms of F-score for
the confidence method results from a threshold of
0.2 with an F-score of 76%. Note that this F-
score lies 4 percentage points above the best per-
formance of a single system (cf. Table 2).
A threshold of 0.2 with five contributing sys-
tems results in a union of all annotations. Conse-
quently, this run benefits from a high recall com-
pared with the other runs. However, the run ex-
hibits the lowest precision rating (both for the ex-
act and overlap condition), which is due to the low
threshold being chosen. As can also be seen with
the confidence method at a threshold of 0.80, a
very high precision can be reached (99%) but at
the cost of extremely low recall.11 The methods
performing best in terms of overlapping F-score
also perform best in terms of exact F-score.
Selected Tagger Combinations: Twin Taggers.
In this scenario, we evaluated all twin combina-
tions of taggers against the gold standard regard-
ing the confidence criterion. In Table 4 we contrast
the two best performing and the two worst per-
forming tagger pairs for the confidence method.
The table reveals that there are some cases where
the taggers seem to complement each other, e.g.,
the twins SYS-1 and SYS-3, as well as SYS-3 and
11Exactly these kinds of alternatives offer flexibility for
choosing the most appropriate SSC given a specific task.
239
exactR exactP exactF overlapR overlapP overlapF systems
0.55 0.74 0.63 0.63 0.84 0.72 SYS-1
0.36 0.53 0.43 0.46 0.68 0.55 SYS-2
0.48 0.77 0.59 0.59 0.95 0.72 SYS-3
0.44 0.83 0.58 0.49 0.91 0.64 SYS-4
0.34 0.61 0.44 0.41 0.74 0.53 SYS-5
Table 2: Performance of single systems (SYS-1 to SYS-5) as evaluated against the gold standard (best
performance scores in bold face). Measurements are taken both for exact as well as overlapping recall
(R), precision (P) and F-score (F).
method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr. systems
cosine 0.94 0.53 0.71 0.61 0.66 0.87 0.75 0.70 2.00
cosine 0.93 0.40 0.79 0.53 0.49 0.96 0.65 0.70 3.00
cosine 0.94 0.54 0.71 0.61 0.65 0.87 0.74 0.80 2.00
cosine 0.93 0.41 0.80 0.54 0.48 0.95 0.64 0.80 3.00
cosine 0.94 0.54 0.72 0.62 0.65 0.86 0.74 0.90 2.00
cosine 0.93 0.41 0.81 0.54 0.48 0.95 0.64 0.90 3.00
cosine 0.94 0.54 0.73 0.62 0.64 0.86 0.74 0.95 2.00
cosine 0.93 0.41 0.83 0.55 0.47 0.95 0.63 0.95 3.00
cosine 0.94 0.55 0.75 0.64 0.64 0.86 0.73 0.97 2.00
cosine 0.93 0.42 0.85 0.56 0.47 0.95 0.63 0.97 3.00
confidence 0.95 0.58 0.73 0.65 0.68 0.85 0.76 0.20
confidence 0.94 0.44 0.83 0.58 0.50 0.94 0.66 0.40
confidence 0.93 0.32 0.88 0.47 0.35 0.97 0.52 0.60
confidence 0.91 0.16 0.91 0.27 0.17 0.99 0.30 0.80
Table 3: Merged annotations of the entire crowd of CALBC taggers (best performance scores per param-
eter setting in bold face). Parameters: threshold (confidence or cosine) and number of agreeing systems
(agr. systems).
SYS-4. In both cases, a confidence threshold of
0.2 yields the best F-score. Additionally, these F-
scores (81% and 78%) are even higher than the
single system?s F-scores (+9% up to +14%). This
comes with a significant increase in recall over
both systems (+13% to +28%) though at the cost
of lowered precision relative to the system with
the higher precision (?1% to ?10%). These re-
sults also outperform the best results of the exper-
imental runs where all systems were involved (see
Table 3). This indicates that a subset of all systems
might yield a better SSC than a combination of all
systems? outputs.
4.2 Extrinsic Calibration of Parameters
We employed a standard named entity tagger to as-
sess the impact of the different merging strategies
on a scenario near to a real-world application.12
12This tagger is based on Conditional Random Fields (Laf-
ferty et al, 2001) and employs a standard feature set used for
Each SSC variant (and thus each parameter com-
bination) was evaluated with this tagger in a 10-
fold cross validation. The SSC and the gold corpus
were split into ten parts of equal size. Nine parts of
the SSC constituted the training data of one cross
validation round, the corresponding tenth part of
the gold standard was used for evaluation. This
way, we tested how adequate a merged corpus was
with respect to the training of a classifier. Because
the cross validation has been very time consum-
ing, we did not consider specific combinations of
systems but always merged the annotations of all
five systems. The results are displayed in Table 5.
Interestingly, the highest recall, precision, and
F-score values (both for the exact and overlap con-
dition) are shared by the same parameter combi-
nations which also performed best in Section 4.1.
Hence, the use of a named entity tagger supports
the evaluation results when comparing the various
biomedical entity recognition (Settles, 2004).
240
ACC exactR exactP exactF overlapR overlapP overlapF systems threshold
0.95 0.62 0.69 0.65 0.76 0.85 0.81 SYS-1 + SYS-3 0.20
0.92 0.22 0.69 0.34 0.26 0.81 0.39 SYS-2 + SYS-5 0.60
0.95 0.55 0.75 0.63 0.67 0.91 0.78 SYS-3 + SYS-4 0.20
0.92 0.30 0.85 0.45 0.34 0.94 0.50 SYS-4 + SYS-5 0.60
Table 4: Twin pairs of taggers, contrasting the two best (in bold face) and the two worst performing pairs
obtained by the confidence method.
method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr. systems
cosine 0.94 0.46 0.69 0.56 0.58 0.86 0.69 0.70 2.00
cosine 0.93 0.32 0.77 0.45 0.39 0.94 0.55 0.70 3.00
cosine 0.94 0.46 0.69 0.56 0.57 0.86 0.69 0.80 2.00
cosine 0.93 0.32 0.78 0.46 0.39 0.94 0.55 0.80 3.00
cosine 0.94 0.46 0.70 0.56 0.57 0.85 0.68 0.90 2.00
cosine 0.93 0.32 0.79 0.46 0.38 0.93 0.54 0.90 3.00
cosine 0.94 0.47 0.71 0.56 0.56 0.85 0.68 0.95 2.00
cosine 0.93 0.33 0.80 0.47 0.38 0.93 0.54 0.95 3.00
cosine 0.94 0.47 0.73 0.57 0.56 0.85 0.67 0.97 2.00
cosine 0.93 0.33 0.82 0.47 0.38 0.93 0.54 0.97 3.00
confidence 0.94 0.50 0.72 0.59 0.60 0.85 0.70 0.20
confidence 0.93 0.36 0.82 0.50 0.41 0.93 0.56 0.40
confidence 0.92 0.25 0.87 0.39 0.28 0.95 0.43 0.60
confidence 0.91 0.12 0.89 0.20 0.12 0.96 0.22 0.80
Table 5: Performance of an NER tagger trained on an SSC, 10-fold cross validation, and all systems.
Parameters: threshold (confidence or cosine) and number of agreeing systems (agr. systems).
SSCs directly to the gold standard corpus. How-
ever, this result may be due to our particular exper-
imental setting and should not be taken as a gen-
eral rule. Instead, this issue should be studied on
additional gold standard corpora (cf. Section 5).
5 Discussion and Conclusions
The experiments reported in this paper strengthen
the empirical basis of the novel idea of a silver
standard corpus (SSC). While the originators of
the SSC have come up with a fixed SSC, our ex-
periments show that different parametrizations of
SSCs allow to dynamically configure or select an
optimal one given a gold standard for comparison
during this optimization.
Our experimental data reveals that the boosting
hypothesis (the combination of several classifiers
outperforms weaker single ones in terms of perfor-
mance) is confirmed for complete mergers as well
as selected twin pairs of taggers. We also have
evidence that boosting within the SSC paradigm
tends to increase precision whereas it seems to de-
crease recall. This general observation becomes
stronger and stronger when the size of the commit-
tees (i.e., the number of submitting classifiers) in-
creases. It is also particularly interesting that both
the intrinsic evaluation (groups of classifiers vs.
gold standard), as well as the extrinsic evaluation
of SSCs (groups of classifiers trained and tested on
mutually exclusive partitions of the gold standard)
reveal parallel patterns in terms of performance ?
this indicates a surprising level of stability of the
entire SSC approach.
In our view, the strongest finding from our ex-
periments is the possibility to calibrate an SSC ac-
cording to requirements derived from the goal of
annotation campaigns. In particular, one can adapt
parameters to a specific use case, e.g., building a
corpus with high precision when compared to the
gold standard. Through the evaluation of the pa-
rameter space, one can assess the costs of reach-
ing a specific goal. For instance, a precision of
99% can be reached, yet at the cost of the F-score
plunging to 30%; only slightly lowering the preci-
sion to 97% boosts the F-score by 22 points (see
last two rows in Table 3).
241
Also, when increasingly more annotation sets
become available (e.g., through the CALBC chal-
lenges) the problem of adversarial or extremely
bad performing systems is no longer a pressing is-
sue since with the optimization approach such sys-
tems are automatically sorted out when optimizing
over the set of possible system combinations.
While our experiments are but a first step to-
wards the consolidation of the SSC paradigm
some obvious limitations of our work have to be
overcome:
? experiments with different gold standards
have to be run as one might hypothesize that
different gold standards require different pa-
rameter settings for the optimal SSC,
? experiments with different NER taggers have
to be run (e.g., we plan to use an NER tag-
ger which prefers recall over precision, while
the one used for these experiments generally
yields higher precision than recall scores),
? test with crowds of taggers which generate
higher recall than precision.13
In our approach, a gold standard is needed to
find good parameters to build an SSC. A ques-
tion not addressed so far is how huge such a gold
standard must be to offer an appropriate size for
the optimization step. Finally, it might be particu-
larly rewarding to join efforts in reducing the de-
velopment costs for such a gold standards ? Active
Learning (e.g., Tomanek et al (2007a)) might be
one promising approach to break this bottleneck.
Since effective calibration of SSCs is in need of
reasonably sized and densely populated gold stan-
dards, by combining these lines of research we
claim that additional benefits for SSCs become vi-
able.
6 Acknowledgments
We wish to thank Kerstin Hornbostel for stim-
ulating and corrective remarks on the biological
grounding of this investigation. This research was
partially funded by the EC?s 7th Framework Pro-
gramme within the CALBC project (FP7-231727)
and the GERONTOSYS research initiative from the
13We used a gold standard in which some unusual entities
(e.g., protein families) had been annotated for which most
named entity taggers have not been trained. This might also
explain the generally overall low recall among the crowd of
taggers yielded in our experiments.
German Federal Ministry of Education and Re-
search (BMBF) under grant 0315581D within the
JENAGE project.
References
Olivier Bodenreider and Alexa T. McCray. 2003. Ex-
ploring semantic groups through visual approaches.
Journal of Biomedical Informatics, 36(6):414?432.
Yoav Freund. 1990. Boosting a weak learning algo-
rithm by majority. In COLT?90 ? Proceedings of the
3rd Annual Workshop on Computational Learning
Theory, pages 202?216.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML?01 ? Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno
Yepes, Erik van Mulligen, Ning Kang, Jan Kors,
David Milward, Peter Corbett, Ekaterina Buyko,
Elena Beisswanger, and Udo Hahn. 2010a. CALBC
Silver Standard Corpus. Journal of Bioinformatics
and Computational Biology, 8:163?179.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno
Yepes, Erik M. van Mulligen, Ning Kang, Jan
Kors, Peter Milward, David Corbett, Ekaterina
Buyko, Katrin Tomanek, Elena Beisswanger, and
Udo Hahn. 2010b. The CALBC Silver Standard
Corpus for biomedical named entities: A study in
harmonizing the contributions from four indepen-
dent named entity taggers. In LREC 2010 ? Pro-
ceedings of the 7th International Conference on
Language Resources and Evaluation.
Burr Settles. 2004. Biomedical named entity recog-
nition using conditional random fields and rich fea-
ture sets. In NLPBA/BioNLP 2004 ? COLING
2004 International Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applica-
tions, pages 107?110.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007a. An approach to text corpus construction
which cuts annotation costs and maintains corpus
reusability of annotated data. In EMNLP-CoNLL?07
? Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Language Learning, pages 486?
495.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007b. A reappraisal of sentence and token splitting
for life sciences documents. In K. A. Kuhn, J. R.
Warren, and T. Y. Leong, editors, MEDINFO?07 ?
Proceedings of the 12th World Congress on Medical
Informatics, number 129 in Studies in Health Tech-
nology and Informatics, pages 524?528. IOS Press.
242
