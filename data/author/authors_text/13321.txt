Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7?12,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Biases in Predicting the Human Language Model
Alex B. Fine
University of Illinois at Urbana-Champaign
abfine@illinois.edu
Austin F. Frank
Riot Games
aufrank@riotgames.com
T. Florian Jaeger
University of Rochester
fjaeger@bcs.rochester.edu
Benjamin Van Durme
Johns Hopkins University
vandurme@cs.jhu.edu
Abstract
We consider the prediction of three hu-
man behavioral measures ? lexical deci-
sion, word naming, and picture naming ?
through the lens of domain bias in lan-
guage modeling. Contrasting the predic-
tive ability of statistics derived from 6 dif-
ferent corpora, we find intuitive results
showing that, e.g., a British corpus over-
predicts the speed with which an Amer-
ican will react to the words ward and
duke, and that the Google n-grams over-
predicts familiarity with technology terms.
This study aims to provoke increased con-
sideration of the human language model
by NLP practitioners: biases are not lim-
ited to differences between corpora (i.e.
?train? vs. ?test?); they can exist as well
between corpora and the intended user of
the resultant technology.
1 Introduction
Computational linguists build statistical language
models for aiding in natural language processing
(NLP) tasks. Computational psycholinguists build
such models to aid in their study of human lan-
guage processing. Errors in NLP are measured
with tools like precision and recall, while errors in
psycholinguistics are defined as failures to model
a target phenomenon.
In the current study, we exploit errors of the lat-
ter variety?failure of a language model to predict
human performance?to investigate bias across
several frequently used corpora in computational
linguistics. The human data is revealing because
it trades on the fact that human language process-
ing is probability-sensitive: language processing
reflects implicit knowledge of probabilities com-
puted over linguistic units (e.g., words). For ex-
ample, the amount of time required to read a word
varies as a function of how predictable that word is
(McDonald and Shillcock, 2003). Thus, failure of
a language model to predict human performance
reveals a mismatch between the language model
and the human language model, i.e., bias.
Psycholinguists have known for some time that
the ability of a corpus to explain behavior depends
on properties of the corpus and the subjects (cf.
Balota et al (2004)). We extend that line of work
by directly analyzing and quantifying this bias,
and by linking the results to methodological con-
cerns in both NLP and psycholinguistics.
Specifically, we predict human data from
three widely used psycholinguistic experimental
paradigms?lexical decision, word naming, and
picture naming?using unigram frequency esti-
mates from Google n-grams (Brants and Franz,
2006), Switchboard (Godfrey et al, 1992), spoken
and written English portions of CELEX (Baayen
et al, 1995), and spoken and written portions
of the British National Corpus (BNC Consor-
tium, 2007). While we find comparable overall
fits of the behavioral data from all corpora un-
der consideration, our analyses also reveal spe-
cific domain biases. For example, Google n-
grams overestimates the ease with which humans
will process words related to the web (tech, code,
search, site), while the Switchboard corpus?a
collection of informal telephone conversations be-
tween strangers?overestimates how quickly hu-
mans will react to colloquialisms (heck, darn) and
backchannels (wow, right).
7
Figure 1: Pairwise correlations between log frequency es-
timates from each corpus. Histograms show distribution over
frequency values from each corpus. Lower left panels give
Pearson (top) and Spearman (bottom) correlation coefficients
and associated p-values for each pair. Upper right panels plot
correlations
2 Fitting Behavioral Data
2.1 Data
Pairwise Pearson correlation coefficients for log
frequency were computed for all corpora under
consideration. Significant correlations were found
between log frequency estimates for all pairs (Fig-
ure 1). Intuitive biases are apparent in the corre-
lations, e.g.: BNCw correlates heavily with BNCs
(0.91), but less with SWBD (0.79), while BNCs
correlates more with SWBD (0.84).
1
Corpus Size (tokens)
Google n-grams (web release) ? 1 trillion
British National Corpus (written, BNCw) ? 90 million
British National Corpus (spoken, BNCs) ? 10 million
CELEX (written, CELEXw) ? 16.6 million
CELEX (spoken, CELEXs) ? 1.3 million
Switchboard (Penn Treebank subset 3) ? 800,000
Table 1: Summary of the corpora under consideration.
2.2 Approach
We ask whether domain biases manifest as sys-
tematic errors in predicting human behavior. Log
unigram frequency estimates were derived from
each corpus and used to predict reaction times
(RTs) from three experiments employing lexical
1
BNCw and BNCs are both British, while BNCs and
SWBD are both spoken.
decision (time required by subjects to correctly
identify a string of letters as a word of English
(Balota et al, 1999)); word naming (time required
to read aloud a visually presented word (Spieler
and Balota, 1997); (Balota and Spieler, 1998));
and picture naming (time required to say a pic-
ture?s name (Bates et al, 2003)). Previous work
has shown that more frequent words lead to faster
RTs. These three measures provide a strong test
for the biases present in these corpora, as they
span written and spoken lexical comprehension
and production.
To compare the predictive strength of log fre-
quency estimates from each corpus, we fit mixed
effects regression models to the data from each
experiment. As controls, all models included (1)
mean log bigram frequency for each word, (2)
word category (noun, verb, etc.), (3) log mor-
phological family size (number of inflectional and
derivational morphological family members), (4)
number of synonyms, and (5) the first principal
component of a host of orthographic and phono-
logical features capturing neighborhood effects
(type and token counts of orthographic and phono-
logical neighbors as well as forward and backward
inconsistent words; (Baayen et al, 2006)). Mod-
els of lexical decision and word naming included
random intercepts of participant age to adjust for
differences in mean RTs between old (mean age
= 72) vs. young (mean age = 23) subjects, given
differences between younger vs. older adults? pro-
cessing speed (cf. (Ramscar et al, 2014)). (All
participants in the picture naming study were col-
lege students.)
2.3 Results
For each of the six panels corresponding to fre-
quency estimates from a corpus A, Figure 2 gives
the ?
2
value resulting from the log-likelihood ra-
tio of (1) a model containing A and an estimate
from one of the five remaining corpora (given on
the x axis) and (2) a model containing just the cor-
pus indicated on the x axis. Thus, for each panel,
each bar in Figure 2 shows the explanatory power
of estimates from the corpus given at the top of the
panel after controlling for estimates from each of
the other corpora.
Model fits reveal intuitive, previously undocu-
mented biases in the ability of each corpus to pre-
dict human data. For example, corpora of British
English tend to explain relatively little after con-
8
trolling for other British corpora in modeling lexi-
cal decision RTs (yellow). Similarly, Switchboard
provides relatively little explanatory power over
the other corpora in predicting picture naming
RTs (blue bars), possibly because highly image-
able nouns and verbs frequent in everyday interac-
tions are underrepresented in telephone conversa-
tions between people with no common visual ex-
perience. In other words, idiosyncratic facts about
the topics, dialects, etc. represented in each cor-
pus lead to systematic patterns in how well each
corpus can predict human data relative to the oth-
ers. In some cases, the predictive value of one
corpus after controlling for another?apparently
for reasons related to genre, dialect?can be quite
large (cf. the ?
2
difference between a model with
both Google and Switchboard frequency estimates
compared to one with only Switchboard [top right
yellow bar]).
In addition to comparing the overall predictive
power of the corpora, we examined the words
for which behavioral predictions derived from the
corpora deviated most from the observed behav-
ior (word frequencies strongly over- or under-
estimated by each corpora). First, in Table 2 we
give the ten words with the greatest relative differ-
ence in frequency for each corpus pair. For exam-
ple, fife is deemed more frequent according to the
BNC than to Google.
2
These results suggest that particular corpora
may be genre-biased in systematic ways. For in-
stance, Google appears to be biased towards termi-
nology dealing with adult material and technology.
Similarly, BNCw is biased, relative to Google, to-
wards Britishisms. For these words in the BNC
and Google, we examined errors in predicted lexi-
cal decision times. Figure 3 plots errors in the lin-
ear model?s prediction of RTs for older (top) and
younger (bottom) subjects.
The figure shows a positive correlation between
how large the difference is between the lexical de-
cision RT predicted by the model and the actu-
ally observed RT, and how over-estimated the log
frequency of that word is in the BNC relative to
Google (left panel) or in Google relative to the
BNC (right panel). The left panel shows that BNC
produces a much greater estimate of the log fre-
2
Surprisingly, fife was determined to be one of the words
with the largest frequency asymmetry between Switchboard
and the Google n-grams corpus. This was a result of lower-
casing all of the words in in the analyses, and the fact that
Barney Fife was mentioned several times in the BNC.
quency of the word lee relative to Google, which
leads the model to predict a lower RT for this word
than is observed (i.e., the error is positive; though
note that the error is less severe for older relative to
younger subjects). By contrast, the asymmetry be-
tween the two corpora in the estimated frequency
of sir is less severe, so the observed RT deviates
less from the predicted RT. In the right panel, we
see that Google assigns a much greater estimate
of log frequency to the word tech than the BNC,
which leads a model predicting RTs from Google-
derived frequency estimates to predict a far lower
RT for this word than observed.
3 Discussion
Researchers in computational linguistics often as-
sume that more data is always better than less
data (Banko and Brill, 2001). This is true in-
sofar as larger corpora allow computational lin-
guists to generate less noisy estimates of the av-
erage language experience of the users of compu-
tational linguistics applications. However, corpus
size does not necessarily eliminate certain types of
biases in estimates of human linguistic experience,
as demonstrated in Figure 3.
Our analyses reveal that 6 commonly used cor-
pora fail to reflect the human language model in
various ways related to dialect, modality, and other
properties of each corpus. Our results point to
a type of bias in commonly used language mod-
els that has been previously overlooked. This bias
may limit the effectiveness of NLP algorithms in-
tended to generalize to a linguistic domains whose
statistical properties are generated by humans.
For psycholinguists these results support an im-
portant methodological point: while each corpus
presents systematic biases in how well it predicts
human behavior, all six corpora are, on the whole,
of comparable predictive value and, specifically,
the results suggest that the web performs as well
as traditional instruments in predicting behavior.
This has two implications for psycholinguistic re-
search. First, as argued by researchers such as
Lew (2009), given the size of the Web compared to
other corpora, research focusing on low-frequency
linguistic events?or requiring knowledge of the
distributional characteristics of varied contexts?
is now more tractable. Second, the viability of
the web in predicting behavior opens up possibil-
ities for computational psycholinguistic research
in languages for which no corpora exist (i.e., most
9
CELEX written BNC written Google
CELEX spoken BNC spoken Switchboard
0
40
80
120
0
10
20
30
40
0
10
20
30
0
10
20
30
0
10
20
30
40
0
5
10
CE
LEX
 wr
itte
n
BN
C w
ritte
n
Go
ogle
CE
LEX
 sp
oke
n
BN
C s
pok
en
Sw
itch
boa
rd
CE
LEX
 wr
itte
n
BN
C w
ritte
n
Go
ogle
CE
LEX
 sp
oke
n
BN
C s
pok
en
Sw
itch
boa
rd
CE
LEX
 wr
itte
n
BN
C w
ritte
n
Go
ogle
CE
LEX
 sp
oke
n
BN
C s
pok
en
Sw
itch
boa
rd
Comparison
? ?2
task
lexical decision
picture naming
word naming
Pairwise model comparisons
Figure 2: Results of log likelihood ratio model comparisons. Large values indicate that the reference predictor (panel title)
explained a large amount of variance over and above the predictor given on the x-axis.
Google and BNC written
Standardized difference score
Err
or 
in l
ine
ar 
mo
de
l
cent dame
doleduke
fife
glen
god
gulf hall
hank
king
lee
lord march
nick
prime
prince
sir ward
cent damedoleduke
fife
glen
god
gulf
hallhank king
lee
lord march
nick
primeprince
sir ward
ass
bin
bug
butt cartchat click
code
darn
den
dialdikefileflip gayheckhop hunklink log
mail
map page
pee
prep
print
quote
ranchscript
search
self
sex
site
skipslotstore
suck
tag
tech
teens
thread
tiretoe
twain
webwhiz
wow
zip
ass
bin
bugbutt
cartchat click
codedarndendial
dike
file
flip
gay
heck
hop hunk
link log mailmap page
peepr p
print quoteranchscript
search
selfsex
site
skip
slot
store sucktag
tech
teens
thread
tire
toe
twain web
whizwow
zip
-0.1
0.0
0.1
0.2
0.3
0.4
-0.1
0.0
0.1
0.2
0.3
0.4
old
young
-3.5 -3.0 -2.5 2.5 3.0 3.5 4.0 4.5 5.0 5.5
Google < BNC written Google > BNC written
goog.f
-4
-2
0
2
Figure 3: Errors in the linear model predicting lexical decision RTs from log frequency are plotted against the standardized
difference in log frequency in the Google n-grams corpus versus the written portion of the BNC. Top and bottom panels show
errors for older and younger subjects, respectively. The left panel plots words with much greater frequency in the written
portion of the BNC relative to Google; the right panel plots words occurring more frequently in Google. Errors in the linear
model are plotted against the standardized difference in log frequency across the corpora, and word color encodes the degree to
which each word is more (red) or less (blue) frequent in Google. That the fit line in each graph is above 0 in the y-axis means
that on average these biased words in each domain are being over-predicted, i.e., the corpus frequencies suggest humans will
react (sometimes much) faster than they actually did in the lab.
10
Greater Lesser Top-10
google bnc.s web, ass, gay, tire, text, tool, code, woe, site, zip
google bnc.w ass, teens, tech, gay, bug, suck, site, cart, log, search
google celex.s teens, cart, gay, zip, mail, bin, tech, click, pee, site
google celex.w web, full, gay, bin, mail, zip, site, sake, ass, log
google swbd gay, thread, text, search, site, link, teens, seek, post, sex
bnc.w google fife, lord, duke, march, dole, god, cent, nick, dame, draught
bnc.w bnc.s pact, corps, foe, tract, hike, ridge, dine, crest, aide, whim
bnc.w celex.s staff, nick, full, waist, ham, lap, knit, sheer, bail, march
bnc.w celex.w staff, lord, last, nick, fair, glen, low, march, should, west
bnc.w swbd rose, prince, seek, cent, text, clause, keen, breach, soul, rise
celex.s google art, yes, pound, spoke, think, mean, say, thing, go, drove
celex.s bnc.s art, hike, pact, howl, ski, corps, peer, spoke, jazz, are
celex.s bnc.w art, yes, dike, think, thing, sort, mean, write, pound, lot
celex.s celex.w yes, sort, thank, think, jazz, heck, tape, well, fife, get
celex.s swbd art, cell, rose, spoke, aim, seek, shall, seed, text, knight
celex.w google art, plod, pound, shake, spoke, dine, howl, sit, say, draught
celex.w bnc.s hunch, stare, strife, hike, woe, aide, rout, yell, glaze, flee
celex.w bnc.w dike, whiz, dine, shake, grind, jerk, whoop, say, are, cram
celex.w celex.s wrist, pill, lawn, clutch, stare, spray, jar, shark, plead, horn
celex.w swbd art, rose, seek, aim, rise, burst, seed, cheek, grin, lip
swbd google mow, kind, lot, think, fife, corps, right, cook, sort, do
swbd bnc.s creek, mow, guess, pact, strife, tract, hank, howl, foe, nap
swbd bnc.w stuff, whiz, tech, lot, kind, creek, darn, dike, bet, kid
swbd celex.s wow, sauce, mall, deck, full, spray, flute, rib, guy, bunch
swbd celex.w heck, guess, right, full, stuff, lot, last, well, guy, fair
Table 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of fife,
lord, and duke, in the BNC are far greater than in Google).
languages). This furthers the arguments of the ?the
web as corpus? community (Kilgarriff and Grefen-
stette, 2003) with respect to psycholinguistics.
Finally, combining multiple sources of fre-
quency estimates is one way researchers may be
able to reduce the prediction bias from any sin-
gle corpus. This relates to work in automatically
building domain specific corpora (e.g., Moore and
Lewis (2010), Axelrod et al (2011), Daum?e III
and Jagarlamudi (2011), Wang et al (2014), Gao
et al (2002), and Lin et al (1997)). Those efforts
focus on building representative document collec-
tions for a target domain, usually based on a seed
set of initial documents. Our results prompt the
question: can one use human behavior as the tar-
get in the construction of such a corpus? Con-
cretely, can we build corpora by optimizing an ob-
jective measure that minimizes error in predicting
human reaction times? Prior work in building bal-
anced corpora used either rough estimates of the
ratio of genre styles a normal human is exposed to
daily (e.g., the Brown corpus (Kucera and Fran-
cis, 1967)), or simply sampled text evenly across
genres (e.g., COCA: the Corpus of Contemporary
American English (Davies, 2009)). Just as lan-
guage models have been used to predict reading
grade-level of documents (Collins-Thompson and
Callan, 2004), human language models could be
used to predict the appropriateness of a document
for inclusion in an ?automatically balanced? cor-
pus.
4 Conclusion
We have shown intuitive, domain-specific biases
in the prediction of human behavioral measures
via corpora of various genres. While some psy-
cholinguists have previously acknowledged that
different corpora carry different predictive power,
this is the first work to our knowledge to system-
atically document these biases across a range of
corpora, and to relate these predictive errors to do-
main bias, a pressing issue in the NLP community.
With these results in hand, future work may now
consider the automatic construction of a ?prop-
erly? balanced text collection, such as originally
desired by the creators of the Brown corpus.
Acknowledgments
The authors wish to thank three anonymous ACL
reviewers for helpful feedback. This research
was supported by a DARPA award (FA8750-13-2-
0017) and NSF grant IIS-0916599 to BVD, NSF
IIS-1150028 CAREER Award and Alfred P. Sloan
Fellowship to TFJ, and an NSF Graduate Research
Fellowship to ABF.
11
References
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 11).
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2). Linguis-
tic Data Consortium, Philadelphia.
R. H. Baayen, L. F. Feldman, and R. Schreuder.
2006. Morphological influences on the recognition
of monosyllabic monomorphemic words. Journal of
Memory and Language, 53:496?512.
D. A. Balota and D. H. Spieler. 1998. The utility of
item-level analyses in model evaluation: A reply to
Seidenberg & Plaut (1998). Psychological Science.
D. A. Balota, M. J. Cortese, and M. Pilotti. 1999. Item-
level analyses of lexical decision performance: Re-
sults from a mega-study. In Abstracts of the 40th An-
nual Meeting of the Psychonomics Society, page 44.
D. Balota, M. Cortese, S. Sergent-Marshall, D. Spieler,
and M. Yap. 2004. Visual word recognition for
single-syllable words. Journal of Experimental Psy-
chology:General, (133):283316.
M. Banko and E. Brill. 2001. Mitigating the paucity of
data problem. Human Language Technology.
E. Bates, S. D?Amico, T. Jacobsen, A. Szkely, E. An-
donova, A. Devescovi, D. Herron, CC Lu, T. Pech-
mann, C. Plh, N. Wicha, K. Federmeier, I. Gerd-
jikova, G. Gutierrez, D. Hung, J. Hsu, G. Iyer,
K. Kohnert, T. Mehotcheva, A. Orozco-Figueroa,
A. Tzeng, and O. Tzeng. 2003. Timed picture nam-
ing in seven languages. Psychonomic Bulletin & Re-
view, 10(2):344?380.
BNC Consortium. 2007. The British National Corpus,
version 3 (BNC XML Edition). Distributed by Ox-
ford University Computing Services on behalf of the
BNC Consortium.
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium (LDC).
Kevyn Collins-Thompson and James P. Callan. 2004.
A language modeling approach to predicting reading
difficulty. In HLT-NAACL, pages 193?200.
H. Daum?e III and J. Jagarlamudi. 2011. Domain
adaptation for machine translation by mining unseen
words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT 11).
M. Davies. 2009. The 385+ million word corpus of
contemporary american english (19902008+): De-
sign, architecture, and linguistic insights. Inter-
national Journal of Corpus Linguistics, 14(2):159?
190.
J. Gao, J. Goodman, M. Li, and K. F. Lee. 2002. To-
ward a unified approach to statistical language mod-
eling for chinese. In Proceedings of the ACM Trans-
actions on Asian Language Information Processing
(TALIP 02).
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone Speech Corpus for
Research and Development. In Proceedings of
ICASSP-92, pages 517?520.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the special issue on the web as corpus. Computa-
tional Linguistics, 29(3):333?348.
H. Kucera and W.N. Francis. 1967. Computational
analysis of present-day american english. provi-
dence, ri: Brown university press.
R. Lew, 2009. Contemporary Corpus Linguistics,
chapter The Web as corpus versus traditional cor-
pora: Their relative utility for linguists and language
learners, pages 289?300. London/New York: Con-
tinuum.
S. C. Lin, C. L. Tsai, L. F. Chien, K. J. Chen, and
L. S. Lee. 1997. Chinese language model adapta-
tion based on document classification and multiple
domain-specific language models. In Proceedings
of the 5th European Conference on Speech Commu-
nication and Technology.
S.A. McDonald and R.C. Shillcock. 2003. Eye
movements reveal the on-line computation of lexical
probabilities during reading. Psychological science,
14(6):648?52, November.
R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 10).
M. Ramscar, P. Hendrix, C. Shaoul, P. Milin, and R. H.
Baayen. 2014. The myth of cognitive decline: non-
linear dynamics of lifelong learning. Topics in Cog-
nitive Science, 32:5?42.
D. H. Spieler and D. A. Balota. 1997. Bringing com-
putational models of word naming down to the item
level. 6:411?416.
L. Wang, D.F. Wong, L.S. Chao, Y. Lu, and J. Xing.
2014. A systematic comparison of data selection
criteria for smt domain adaptation. The Scientific
World Journal.
12
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 18?26,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Is there syntactic adaptation in language comprehension?
Alex B. Fine, Ting Qian, T. Florian Jaeger and Robert A. Jacobs
Department of Brain and Cognitive Sciences
University of Rochester
Rochester, NY, USA
{afine, tqian, fjaeger, robbie}@bcs.rochester.edu
Abstract
In this paper we investigate the manner in
which the human language comprehension
system adapts to shifts in probability dis-
tributions over syntactic structures, given
experimentally controlled experience with
those structures. We replicate a classic
reading experiment, and present a model
of the behavioral data that implements a
form of Bayesian belief update over the
course of the experiment.
1 Introduction
One of the central insights to emerge from ex-
perimental psycholinguistics over the last half
century is that human language comprehension
and production are probability-sensitive. Dur-
ing language comprehension, language users ex-
ploit probabilistic information in the linguistic sig-
nal to make inferences about the speaker?s most
likely intended message. In syntactic compre-
hension specifically, comprehenders exploit statis-
tical information about lexical and syntactic co-
occurrence statistics. For instance, (1) is temporar-
ily ambiguous at the noun phrase the study, since
the NP can be parsed as either the direct object
(DO) of the verb acknowledge or as the subject
NP of a sentence complement (SC).
(1) The reviewers acknowledged the study...
? DO: ... in the journal.
? SC: ... had been revolutionary.
The ambiguity in the SC continuation is resolved
at had been, which rules out the direct object in-
terpretation of the study. Reading times at had
been?the so-called point of disambiguation?are
correlated with a variety of lexical-syntactic prob-
abilities. For instance, if the probability of a SC
is low, given the verb, subjects are garden-pathed
and will display longer reading times at had been.
Conversely, if the probability of a SC is high, the
material at the point of disambiguation is rela-
tively unsurprising (i.e. conveys less information),
and reading times will be short. Readers are also
sensitive to the probability of the post-verbal NP
occurring as the direct object of the verb. This is
often discussed in terms of plausibility?in (1), the
study is a plausible direct object of acknowledge
(relative to, say, the window), which will also con-
tribute to longer reading times in the event of a SC
continuation (Garnsey et al, 1997).
Thus, humans make pervasive use of proba-
bilistic cues in the linguistic signal. A question
that has received very little attention, however, is
how language users maintain or update their rep-
resentations of the probability distributions rele-
vant to language use, given new evidence?a phe-
nomenon we will call adaptation. That is, while
we know that language users have access to lin-
guistic statistics, we know little about the dynam-
ics of this knowledge in language users: is the
probabilistic information relevant to comprehen-
sion derived from experience during a critical pe-
riod of language acquisition, or do comprehenders
update their knowledge on the basis of experience
throughout adulthood? A priori, both scenarios
seem plausible?given the sheer number of cues
relevant to comprehension, it would be advanta-
geous to limit the resources devoted to acquiring
this knowledge; on the other hand, any learner?s
linguistic experience is bound to be incomplete, so
the ability to adapt to novel distributional patterns
in the linguistic input may prove to be equally use-
ful. The goal of this paper is to explore this is-
sue and to take an initial step toward providing a
computational framework for characterizing adap-
tation in language processing.
1.1 Adaptation in Sentence Comprehension
Both over time and across situations, humans are
exposed to linguistic evidence that, in principle,
18
ought to lead to shifts in our representations of the
relevant probability distributions. An efficient lan-
guage processing system is expected to take new
evidence into account so that behavior (decisions
during online production, predictions about up-
coming words, etc.) will be guided by accurate es-
timates of these probability distributions. At least
at the level of phonetic perception and produc-
tion, there is evidence that language users quickly
adapt to the statistical characteristics of the am-
bient language. For instance, over the course of
a single interaction, the speech of two interlocu-
tors becomes more acoustically similar, a phe-
nomenon known as spontaneous phonetic imita-
tion (Goldinger, 1998). Perhaps even more strik-
ingly, Clayards et al (2008) demonstrated that,
given a relatively small number of tokens, compre-
henders shift the degree to which they rely on an
acoustic cue as the variance of that cue changes,
reflecting adaptation to the distributions of proba-
bilistic cues in speech perception.
At the level of syntactic processing, belief up-
date/adaptation has only recently been addressed
(Wells et al, 2009; Snider and Jaeger, in prep). In
this study, we examine adaptation at the level of
syntactic comprehension. We provide a computa-
tional model of short- to medium-term adaptation
to local shifts in the statistics of the input. While
the Bayesian model presented can account for the
behavioral data, the quality of the model depends
on how control variables are treated. We discuss
the theoretical and methodological implications of
this result.
Section 2 describes the behavioral experiment,
a slight modification of the classic reading experi-
ment reported in Garnsey et al (1997). The study
reported in section 3 replicates the basic findings
of (Garnsey et al, 1997). In sections 4 and 5
we outline a Bayesian model of syntactic adapta-
tion, in which distributions over syntactic struc-
tures are updated at each trial based on the ev-
idence in that trial, and discuss the relationship
between the model results and control variables.
Section 6 concludes.
2 Behavioral Experiment
2.1 Participants
Forty-six members of the university community
participated in a self-paced reading study for pay-
ment. All were native speakers of English with
normal or corrected to normal vision, based on
self-report.
2.2 Materials
Subjects read a total of 98 sentences, of which 36
were critical items containing DO/SC ambiguities,
as in (1). These 36 sentences comprise a subset of
those used in Garnsey et al (1997). The stim-
uli were manipulated along two dimensions: first,
verbs were chosen such that the conditional prob-
ability of a SC, given the verb, varied. In Garnsey
et al (1997), this conditional probability was es-
timated from a norming study, in which subjects
completed sentence fragments containing DO/SC
verbs (e.g. the lawyer acknowledged...). We adopt
standard psycholinguistic terminology and refer
to this conditional probability as SC-bias. The
verbs used in the critical sentences in Garnsey et
al. (1997) were selected to span a wide range of
SC-bias values, from .01 to .9. Each sentence con-
tained a different DO/SC verb. In addition to SC-
bias, half of the sentences presented to each sub-
ject included the complementizer that, as in (2).
(2) The reviewers acknowledged that the
study had been revolutionary.
Sentences with a complementizer were included
as an unambiguous baseline (Garnsey et al 1997).
The presence of a complementizer was counter-
balanced, such that each subject saw half of the
sentences with a complementizer and all sen-
tences occurred with and without a complemen-
tizer equally often across subjects. All of the criti-
cal sentences contained a SC continuation. The 36
critical items were interleaved with 72 fillers that
included simple transitives and intransitives.
2.3 Procedure
Subjects read critical and filler sentences in a self-
paced moving window display (Just et al, 1982),
presented using the Linger experimental presen-
tation software (Rohde, 2005). Sentences were
presented in a noncumulative word-by-word self-
paced moving window. At the beginning of each
trial, the sentence appeared on the screen with all
non-space characters replaced by a dash. Using
their dominant hands, subjects pressed the space
bar to view each consecutive word in the sen-
tence. Durations between space bar presses were
recorded. At each press of the space bar, the
currently-viewed word reverted to dashes as the
next word was converted to letters. A yes/no com-
19
prehension question followed all experimental and
filler sentences.
2.4 Analysis
In keeping with standard procedure, we used
length-corrected residual per-word reading times
as our dependent measure. Following Garnsey et
al. (1997), we define the point of disambiguation
in the critical sentences as the two words follow-
ing the post-verbal NP (e.g. had been in (1) and
(2)). All analyses reported here were conducted on
residual reading times at this region. For a given
subject, residual reading times more than two stan-
dard deviations from that subject?s mean residual
reading time were excluded.
3 Study 1
Residual reading times at the point of disambigua-
tion were fit to a linear mixed effects regression
model. This model included the full factorial de-
sign (i.e. all main effects and all interactions) of
logged SC-bias (taken from the norming study re-
ported in Garnsey et al 1997) and complemen-
tizer presence. Additionally, the model included
random intercepts of subject and item. This was
the maximum random effect structure justified by
the data, based on comparison against more com-
plex models.1 All predictors in the model were
centered at zero in order to reduce collinearity.
P-values reported in all subsequent models were
calculated using MCMC sampling (where N =
10,000).
3.1 Results
This model replicated the findings reported by
Garnsey et al (1997). There was a significant
main effect of complementizer presence (? =
?3.2, t = ?2.5, p < .05)?reading times at
the point of disambiguation were lower when
the complementizer was present. Additionally,
there was a significant two-way interaction be-
tween complementizer presence and logged SC-
bias (? = 3.0, t = 2.5, p < .05)?SC-bias has a
stronger negative correlation with reading times in
the disambiguating region when the complemen-
tizer is absent, as expected. Additionally, Gar-
nsey et al (1997) found a main effect of SC-bias.
For us, this main effect did not reach significance
1For a detailed description of the procedure used,
see http://hlplab.wordpress.com/2009/05/14/random-effect-
should-i-stay-or-should-i-go/
(? = ?1.2, t = ?1.11, p = .5), possibly owing to
the fact that we tested a much smaller sample than
Garnsey et al (1997) (51 compared to 82 partici-
pants).
4 Study 2: Bayesian Syntactic
Adaptation
Reading times at the point of disambiguation in
these stimuli reflect, among other things, sub-
jects? estimates of the conditional probability
p(SC|verb) (Garnsey et al 1997), which we have
been calling SC-bias. Thus, we model the task fac-
ing subjects in this experiment as one of Bayesian
inference, where subjects are, when reading a sen-
tence containing the verb vi, inferring a posterior
probability P(SC|vi), i.e. the probability that a
sentence complement clause will follow a verb vi.
According to Bayes rule, we have:
p(SC|vi) =
p(vi|SC)p(SC)
p(vi)
(1)
In Equation (1), we use the relative frequency
of vi (estimated from the British National Corpus)
as the estimate for p(vi). The first term in the nu-
merator, p(vi|SC), is the likelihood, which we es-
timate by using the relative frequency of vi among
all verbs that can take a sentence complement as
their argument. These values are taken from the
corpus study by Roland et al (2007). Roland et al
(2007) report, among other things, the number of
times a SC occurs as the argument of roughly 200
English verbs. These values are reported across a
number of corpora. We use the values from the
BNC to compute p(vi|SC).
The prior probability of a sentence complement
clause, p(SC), is the estimate of interest in this
study. We hypothesize that, under the assumptions
of the current model, subjects update their esti-
mate for p(SC) based on the evidence presented
in each trial. As a result, the posterior probability
varies from trial to trial, not only because the verb
used in each stimulus is different, but also because
the belief about the probability of a sentence com-
plement is being updated based on the evidence in
each trial. We employ the beta-binomial model to
simulate this updating process, as described next.
4.1 Belief Update
We adopt an online training paradigm involving
an ideal observer learning from observations. Af-
ter observing a sentence containing a DO/SC verb,
20
we predict that subjects will update both the likeli-
hood p(vi|SC) for that verb, as well as the proba-
bility p(SC). Because each verb occurs only once
for a given subject, the effect of updating the first
quantity is impossible to measure in the current ex-
perimental paradigm. We therefore focus on mod-
eling how subjects update their belief of p(SC)
from trial to trial.
We make the simplifying assumption that the
only possible argument that DO/SC verbs can take
is either a direct object or a sentence complement
clause. Further, subjects are assumed to have an
initial belief about how probable a sentence com-
plement is, on a scale of 0 to 1. Let ? denote
this probability estimate, and p(?) the strength of
this estimate. From the perspective of an ideal
observer, p(?) will go up for ? > 0.5 when a
DO/SC verb is presented with a sentence comple-
ment as its argument. This framework assumes
that subjects do not compute ? by merely relying
on frequency (otherwise, ? will be simply the ra-
tio between SC and DO structures in a block of
trials), but they have a distribution P (?), where
each possible estimate of ? is associated with a
probability indicating the confidence on that es-
timate. In order to make our results comparable
to existing models, however, we use the expected
value of P (?) in each iteration of training as point
estimates. Therefore, for one subject, we have
36 estimated ?? values, each corresponding to the
changed belief after seeing a sentence containing
SC in an experiment of 36 trials. Because none
of the filler items included DO/SC verbs, we as-
sume that filler trials have no effect on subjects?
estimates of P (?).
Since all stimuli in our experiment have the SC
structure, the general expectation is the distribu-
tion P (?) will shift towards the end where ? = 1.
Our belief update model tries to capture the shape
of this shift during the course of the experiment.
Using Bayesian inference, we can describe the up-
dating process as the following, where ?i repre-
sents a particular belief of the value ?.
p(? = ?i|obs.) =
p(obs.|? = ?i)p(? = ?i)
p(obs.)
= p(obs.|? = ?i)p(? = ?i)? 1
0
p(obs.|?)p(?) d?
(2)
This posterior probability is hypothesized to re-
flect how likely a subject would consider the prob-
ability of SC to be ?i after being exposed to one
experimental item. We discretized ? to 100 evenly
spaced ?i values, ranging from 0 to 1. Thus, the
denominator can be calculated by marginalizing
over the 100 ?i values. The two terms in the nu-
merator in Equation (2) are estimated in the fol-
lowing manner.
Likelihood function p(obs.|? = ?i) is modeled
by a binomial distribution, where the parameters
are ?i (the probability of observing a SC clause)
and 1 ? ?i (the probability of observing a direct
object), and where the outcome is the experimen-
tal item presented to the subject. Therefore:
p(obs.|? = ?i) =
(nsc + ndo)!
nsc!ndo!
?nsci (1? ?i)ndo
(3)
In the current experiment, ndo is always 0 since
all stimuli contain the SC argument. In addition,
between-trial reading time differences are mod-
elled at one item a step for each subject so that nsc
is always 1 in each trial. It is in theory possible to
set nsc to other numbers.
The prior In online training, the posterior of the
previous iteration is used as the prior for the cur-
rent one. Nevertheless, the prior p(? = ?i) for
the very first iteration of training needs to be es-
timated. Here we assume a beta distribution with
parameters ? and ?. The probability of the prior
then is:
p(? = ?i) =
???1i (1? ?i)??1
B(?,?)
Intuitively, ? and ? capture the number of times
subjects have observed the SC and DO outcomes,
respectively, before the experiment. In the context
of our research, this model assumes that subjects?
beliefs about p(SC) and p(DO) are based on ??1
observations of SC and ? ? 1 observations of DO
prior to the experiment.
The values of the parameters of the beta distri-
bution were obtained by searching through the pa-
rameter space with an objective function based on
the Bayesian information criterion (BIC) score of
a regression model containing the log of the pos-
terior computed using the updated prior p(SC),
complementizer presence, and the two-way inter-
action. The BIC (Schwarz, 1978) is a measure
of model quality that weighs the models empirical
coverage against its parsimony (BIC = 2ln(L)+
21
k ? ln(n), where k is the number of parameters in
the model, n the number of data points, and L is
the models data likelihood). Smaller BIC indicate
better models. The ? and ? values yielding the
lowest BIC score are used.
In estimating ? and ?, we considered all pairs of
non-negative integers such that both values were
below 1000. The values of ? and ? used here were
1 and 177, respectively. These values do not im-
ply that subjects have seen only 1 SC and 177 DOs
prior to the experiment, but that only this many ob-
servations inform subjects? prior beliefs about this
distribution. The relationship between the choice
of the parameters of the beta distribution, ? and
?, and the BIC of the model used in the parameter
estimation is shown in Figure 1.
Beta
Alpha
BIC
Figure 1: The relationship between the BIC of the
model used in the parameter estimation step and
values of ? and ? in the beta distribution
Because we model subjects? estimates of
p(SC|vi) in terms of Bayesian inference, with a
continuously updated prior, p(SC), the value of
p(SC|vi) depends, in our model, on both verb-
specific statistics (i.e. the likelihood p(vi|SC) and
the probability of the verb p(vi)) and the point in
the experiment at which the trial containing that
verb is encountered. We can visualize this rela-
tionship in Figure 2, which shows the values given
by the model of p(SC|vi) for four particular dif-
ferent verbs, depending on the point in the experi-
ment at which the verb is seen.
The approach we take is hence fundamentally
Presentation Order
Pos
terio
r p(S
C|v)
0.2
0.4
0.6
0.8
0 10 20 30
confide deny
know
0 10 20 30
0.2
0.4
0.6
0.8
print
Figure 2: The relationship, for four of the verbs,
between the value of p(SC|vi) given by the model
as a function of when in the experiment vi is en-
countered
different from the approach commonly taken in
psycholinguistics, which is to use static estimates
of quantities such as p(SC|vi) derived from cor-
pora or norming studies.
4.2 Analysis
To test whether the model-derived values of
p(SC|vi) are a good fit for the behavioral data,
we fit residual reading times at the point of dis-
ambiguation using linear mixed effects regression.
The model included main effects of p(SC|vi)?as
given by the model just described?and comple-
mentizer presence, as well as the two-way inter-
action between these two predictors. Additionally,
there were random intercepts of subject and item.
p(SC|vi) was logged and centered at zero.
4.3 Results
There was a highly significant main effect of
the posterior probability p(SC|vi) yielded by the
beta-binomial model (? = ?40, t = ?21.2, p <
.001), as well as a main effect of complemen-
tizer presence (?4.5, t = ?3.7, p < .001).
The two-way interaction between complementizer
presence and the posterior probability from the
beta-binomial model did not reach significance
(? = 0.5, t = .5, p > .05). The reason is likely
that, in the analysis presented for Study 1, we can
interpret the interaction as indicating that when
22
SC-bias is high, the complementizer has less of
an effect; in our model, the posterior probabil-
ity p(SC|vi) is both generally higher and has less
variance than the same quantity when based on
corpus- or norming study estimates, since the prior
probability p(SC) is continuously increasing over
the course of the experiment. This would have the
effect of eliminating or at least obscuring the in-
teraction with complementizer presence.
The posterior p(SC|vi) has a much stronger
negative correlation with residual reading times
than the measure of SC-bias used in Study 1 (? =
?40 as opposed to ? = ?1.2).
4.4 Discussion
So far, we have replicated a classic finding in the
sentence processing literature (Study 1), provided
evidence that subjects? estimates of the conditional
probability p(SC|vi) change based on evidence
throughout the experiment, and that this process
is captured well by a model which implements a
form of incremental Bayesian belief update. We
take this as evidence that the language comprehen-
sion system is adaptive, in the sense that language
users continually update their estimates of proba-
bility distributions over syntactic structures.
5 Syntactic Adaptation vs. Motor
Adaptation
The results of the model presented in section 4
are amenable to (at least) two explanations. We
have hypothesized that, given exposure to new ev-
idence about probability distributions over syn-
tactic structures in English, subjects update their
beliefs about these probability distributions, re-
flected in reading times?a phenomenon we refer
to as syntactic adaptation. An alternative explana-
tion, however, is one that appeals to motor adap-
tation, rather than syntactic adaptation. Specifi-
cally, it could be that subjects are simply adapt-
ing to the task?rather than to changes in syntactic
distributions?as the experiment proceeds, lead-
ing to faster reading times.
We expect the effect of motor adaptation to
be captured by presentation order, or the point
in the experiment at which subjects encounter a
given stimulus. In particular, we predict a neg-
ative correlation between presentation order and
reading times. Unfortunately, in the current ex-
periment, presentation order and p(SC|vi) derived
from the Beta-binomial model are positively cor-
related (r = .6)?the latter increases with increas-
ing presentation order, since participants only see
SC continuations. The results we observed above
could hence also be due to an effect of presentation
order.
The expected shape of a possible effect of task
adaptation is not obvious. That is, it is not clear
whether the relationship between presentation or-
der and reading times will be linear. On the one
hand, linearity would be the default assumption
prior to theoretical considerations about the dis-
tributional properties of presentation order. On
the other hand, presentation order is a lower-
bounded variable, which often are distributed ap-
proximately log-normally. Additionally, it is pos-
sible that there may be a floor effect: participants
may get used to having to press the space bar to ad-
vance to the next word and may quickly get faster
at that procedure until RTs converge against the
minimal time it takes to program the motor move-
ment to press the space bar. Such an effect would
likely lead to an approximately log-linear effect of
presentation order.
We test for an effect of motor adaptation by ex-
amining the effect of presentation order on read-
ing times, comparing the effect of linear and log-
transformed presentation order.
5.1 Controlling for Presentation Order in the
Beta-binomial model
We test for separate effects of syntactic adaptation
and motor adaptation by conducting stepwise re-
gressions with two models containing the full fac-
torial design of the Beta-binomial posterior, com-
plementizer presence, and, for the first model, a
linear effect of presentation order and, for the
second model, log-transformed presentation order.
We conducted stepwise regressions using back-
ward elimination, starting with all predictors and
removing non-significant predictors (i.e. p > .1),
one at a time, until all non-significant predictors
are deleted.
For both the model including a linear effect
of presentation order and a model including log-
transformed presentation order, the final mod-
els resulting from the stepwise regression proce-
dure included only main effects of complemen-
tizer presence and log presentation order. These
models are summarized in Figure 1, which in-
cludes coefficient-based tests for significance of
each of the predictors (i.e. whether the coefficient
23
is significantly different from zero) as well as ?2-
based tests for significance (i.e. the difference be-
tween a model with that predictor and one with-
out). Comparing the two resulting models based
on the Bayesian Information Criterion, the model
containing log-transformed presentation order is a
better model than one with a linear effect of pre-
sentation order (BIClog = 37467; BICnon?log =
37510).
Pres. order untransformed
Coef. and ?2-based tests
Predictor ? p ?2 p
Comp. pres. ?4.3 < .05 4.9 < .05
Pres. order ?.7 < .001 28.2 < .001
Pres. order log-transformed
Coef. and ?2-based tests
Predictor ? p ?2 p
Comp. pres. ?4.3 < .05 4.8 < .05
Pres. order ?33.8 < .001 29.4 < .001
Table 1: Coefficient- and ?2-based tests for sig-
nificance of model resulting from stepwise regres-
sion
In sum, the beta-binomial derived posterior ap-
pears to have no predictive power after presenta-
tion order is controlled for. This result does not
depend on how presentation order is treated (i.e.
log-transformed or not).
5.2 The interaction between SC-bias and
presentation order
The results from the previous section suggest that
the Beta-binomial derived posterior carries no pre-
dictive power after presentation order is controlled
for. Is there any evidence at all for syntactic adap-
tation (as opposed to motor, or task, adaptation)?
To attempt to answer this, we analyzed the read-
ing data using the model reported in section 3,
with an additional main effect of presentation or-
der, as well as the interactions between presenta-
tion order and the other predictors in the model.
An overall decrease in reading times due to mo-
tor adaptation should surface as a main effect of
presentation order, as mentioned; syntactic adap-
tation, however, is predicted to show up as a two-
way interaction between SC-bias and presentation
order?since subjects only see SC continuations,
subjects should expect this outcome to become
more and more probable over the course of the ex-
periment, causing the correlation between SC-bias
and reading times to become weaker (thus we pre-
dict the interaction to have a positive coefficient).
To test for such an interaction, we performed
a stepwise regressions with two models contain-
ing the full factorial design of SC-bias, comple-
mentizer presence, and, for the first model, a lin-
ear effect of presentation order and, for the second
model, log-transformed presentation order. The
stepwise regression procedure here was identical
to the one reported in the previous section.
For both models, the remaining predictors were
main effects of presentation order, complemen-
tizer presence, and SC-bias, as well as a two-way
interaction between SC-bias and complementizer
presence and a two-way interaction between SC-
bias and presentation order. The results of these
models are given in Table 2.
Pres. order untransformed
Coef. and ?2-based tests
Predictor ? p ?2 p
SC-bias ?.4 = .8 11.5 < .001
Comp. pres. ?4.4 < .001 18.1 < .001
Pres. order ?.9 < .001 420.9 < .001
SC-bias:Comp. 2.6 < .05 5.3 < .05
SC-bias:Pres. Order .1 < .05 6.2 < .05
Pres. order log-transformed
Coef. and ?2-based tests
Predictor ? p ?2 p
SC-bias ?1.4 = .5 8.9 < .05
Comp. pres. ?4.6 < .001 19.3 < .001
Pres. order ?42.4 < .001 461.2 < .001
SC-bias:Comp. 2.6 < .05 5.2 < .05
SC-bias:Pres. Order 3.5 = .06 3.4 = .06
Table 2: Coefficient- and ?2-based tests for sig-
nificance of model resulting from stepwise regres-
sion
The main findings reported in Study 1 (i.e. a
main effect of complementizer presence and a
two-way interaction between SC-bias and com-
plementizer presence) are replicated here, and do
not depend on whether presentation order is log-
transformed. However, the interaction between
SC-bias and presentation order is less reliable
when presentation order is log-transformed, reach-
ing only marginal significance. In short, an ad-
equate account of the data requires reference to
both motor adaptation (in the form of a main effect
of presentation order, log-transformed) and syn-
tactic adaptation.
If subjects are improving at the task, and the
effect of presentation order represents a kind of
adaptation to the task of self-paced reading, we
would expect to find a main effect of presenta-
tion order on reading times at all regions. This
24
is the case?a strong negative correlation between
presentation order and reading times holds across
all regions. Evidence that the observed interac-
tion is due to syntactic belief update comes from
the fact that the interaction between SC-bias and
presentation order, unlike the main effect of pre-
sentation order, is limited to the disambiguating
region of the sentence. We performed the regres-
sion reported above on residual reading times at
the main verb (e.g. acknowledge), ambiguous (e.g.
the study), and disambiguating (e.g. had been) re-
gions. These analyses revealed, as expected, main
effects of presentation order across all regions. At
the verb and ambiguous regions, however, presen-
tation order did not interact with SC-bias.
Region ? p? value
Main effect of pres. order
Verb ?.95 < .001
Ambig. region ?.9 < .001
Disambig. region ?.9 < .001
Pres. order X SC-bias interaction
Verb .09 = .24
Ambig. region .04 = .37
Disambig. region .1 < .05
Table 3: Main effect of presentation order and in-
teraction of presentation order with SC-bias at dif-
ferent regions in the critical sentences
This finding provides initial evidence that sub-
jects adapt their linguistic expectations to the evi-
dence observed throughout the experiment. How-
ever, the interaction between presentation order
and SC-bias in this analysis is amenable to an al-
ternative interpretation: interactions between pre-
sentation order and other variables could emerge
if subjects? reaction times reach some minimum
value over the course of the experiment, causing
any other variable to become less strongly corre-
lated with the dependent measure as reaction times
approach that minimum value. Thus this interac-
tion could be an artefact of a floor effect.
To test the possibility that the SC-bias-
presentation order interaction is the result of a
floor effect, we compared the 1st, 5th, and 10th
fastest percentiles of residual reading times across
all regions. As shown in Figure 3, faster reading
times are observed at each quantile in at least one
other region. In other words, reading times in the
disambiguating region do not seem to be bounded
by motor demands associated with the task. We
hence tentatively conclude that the interaction be-
tween SC-bias and log-transformed presentation
order is not the result of a floor effect, although
this issue deserves further attention.
Figure 3: Minimum and upper boundary of 1st,
5th, and 10th percentile values of residual reading
times across all sentence regions
6 Conclusion
We hypothesized that the language comprehension
system rapidly adapts to shifts in the probability
distributions over syntactic structures on the ba-
sis of experience with those structures. To in-
vestigate this phenomenon, we modelled reading
times from a self-paced reading experiment us-
ing a Bayesian model of incremental belief up-
date. While an initial test of the Beta-binomial
model was encouraging, the predictions of the
Beta-binomial model are highly correlated with
presentation order in the current data set. This
means that it is hard to distinguish between adap-
tation to the task of self-paced reading and syntac-
tic adaptation. Indeed, model comparison suggests
that the Bayesian model does not explain a signif-
icant amount of the variance in reading times once
motor adaptation (as captured by stimulus presen-
tation order) is accounted for. In a secondary anal-
ysis, we did, however, find preliminary evidence
of syntactic adaptation. That is, while the Beta-
binomial model does not seem to capture syntac-
tic belief update adequately, there is evidence that
comprehenders continuously update their syntac-
tic distributions.
25
Teasing apart the effects of motor adaptation
and linguistic adaptation will require experimen-
tal designs in which these two factors are not as
highly correlated as in the present study. Ongoing
work addresses this issue.
Acknowledgements
The authors wish to thank Neal Snider and mem-
bers of the Human Language Processing lab, as
well as three anonymous ACL reviewers for help-
ful discussion and feedback. We are also very
grateful to Jeremy Ferris for help in collecting the
data reported here. This work was supported by
the University of Rochesters Provost Award for
Multidisciplinary Research and NSF grant BCS-
0845059 to TFJ.
References
John Anderson. 1990. The adaptive character of
thought. Lawrence Erlbaum.
Bock and Griffin. 2000. The persistence of structural
priming: Transient activation or implicit learning?
Journal of Experimental Psychology, 129(2):177?
192.
Chang, Dell, and Bock. 2006. Becoming syntactic.
Psychological Review, 113(2):234?272.
Clayards, Tanenhaus, Aslin, and Jacobs. 2008. Per-
ception of speech reflects optimal use of probabilis-
tic cues. Cognition, 108:804?809.
Garnsey, Pearlmutter, Myers, and Lotocky. 1997.
The contributions of verb bias and plausibility to
the comprehension of temporarily ambiguous sen-
tences. Journal of Memory and Language, (37):58?
93.
S.D. Goldinger. 1998. Echoes of echoes? an episodic
theory of lexical access. Psychological Review,
(105):251?279.
Florian Jaeger. in press. Redundancy and reduc-
tion: speakers manage syntactic information density.
Cognitive Psychology.
Just, Carpenter, and Woolley. 1982. Paradigms and
processes in reading comprehension. Journal of Ex-
perimental Psychology: General, 111:228?238.
Rohde. 2005. Linger experiment presentation soft-
ware. http://tedlab.mit.edu/ dr/Linger/.
Schwarz. 1978. Estimating the dimension of a model.
Annals of Statistics, 6:461?464.
Herbert Simon, 1987. 77K New Palgrave Dictionary
of Economics, chapter Bounded Rationality, pages
266?268. Macmillan, London.
Neal Snider and Florian Jaeger. in prep.
Thothathiri and Snedeker. 2008. Give and take:
Syntactic priming during language comprehension.
Cognition, 108:51?68.
Wells, Christiansen, Race, Acheson, and MacDonald.
2009. Experience and sentence comprehension:
Statistical learning and relative clause comprehen-
sion. Cognitive Psychology, 58:250?271.
26
