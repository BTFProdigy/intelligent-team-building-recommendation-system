TOWARDS DATA AND GOAL ORIENTED ANALYSIS:  
TOOL INTER-OPERABILITY AND COMBINATORIAL 
COMPARISON 
Yoshinobu Kano1      Ngan Nguyen1      Rune S?tre1       Kazuhiro Yoshida1 
Keiichiro Fukamachi1      Yusuke Miyao1       Yoshimasa Tsuruoka3   
Sophia Ananiadou2,3        Jun?ichi Tsujii1,2,3 
 
1Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester 
PO Box 88, Sackville St, MANCHESTER M60 1QD, UK 
 
3NaCTeM (National Centre for Text Mining), Manchester Interdisciplinary Biocentre, 
University of Manchester, 131 Princess St, MANCHESTER M1 7DN, UK 
 
{kano,nltngan,satre,kyoshida,keif,yusuke,tsujii} 
@is.s.u-tokyo.ac.jp 
{yoshimasa.tsuruoka,sophia.ananiadou}@manchester.ac.uk 
 
Abstract 
Recently, NLP researches have advanced 
using F-scores, precisions, and recalls with 
gold standard data as evaluation measures. 
However, such evaluations cannot capture 
the different behaviors of varying NLP 
tools or the different behaviors of a NLP 
tool that depends on the data and domain in 
which it works. Because an increasing 
number of tools are available nowadays, it 
has become increasingly important to grasp 
these behavioral differences, in order to 
select a suitable set of tools, which forms a 
complex workflow for a specific purpose. 
In order to observe such differences, we 
need to integrate available combinations of 
tools into a workflow and to compare the 
combinatorial results. Although generic 
frameworks like UIMA (Unstructured 
Information Management Architecture) 
provide interoperability to solve this 
problem, the solution they provide is only 
partial. In order for truly interoperable 
toolkits to become a reality, we also need 
sharable and comparable type systems with 
an automatic combinatorial comparison 
generator, which would allow systematic 
comparisons of available tools. In this 
paper, we describe such an environment, 
which we developed based on UIMA, and 
we show its feasibility through an example 
of a protein-protein interaction (PPI) 
extraction system. 
1 Introduction 
Recently, an increasing number of TM/NLP tools 
such as part-of-speech (POS) taggers (Tsuruoka et 
al., 2005), named entity recognizers (NERs) 
(Settles, 2005) syntactic parsers (Hara et al, 2005) 
and relation or event extractors (ERs) have been 
developed. Nevertheless, it is still very difficult to 
integrate independently developed tools into an 
aggregated application that achieves a specific 
task. The difficulties are caused not only by 
differences in programming platforms and 
different input/output data formats, but also by the 
lack of higher level interoperability among 
modules developed by different groups.  
859
UIMA, Unstructured Information Management 
Architecture (Lally and Ferrucci, 2004), which was 
originally developed by IBM and has recently 
become an open project in OASIS and Apache, 
provides a promising framework for tool 
integration. Although it has a set of useful 
functionalities, UIMA only provides a generic 
framework, thus it requires a user community to 
develop their own platforms with a set of actual 
software modules. A few attempts have already 
been made to establish platforms, e.g. the CMU 
UIMA component repository 1 , GATE 
(Cunningham et al, 2002) with its UIMA 
interoperability layer, etc.  
However, simply wrapping existing modules to 
be UIMA compliant does not offer a complete 
solution. Most of TM/NLP tasks are composite in 
nature, and can only be solved by combining 
several modules. Users need to test a large number 
of combinations of tools in order to pick the most 
suitable combination for their specific task. 
Although types and type systems are the only 
way to represent meanings in the UIMA 
framework, UIMA does not provide any specific 
types, except for a few purely primitive types. In 
this paper, we propose a way to design sharable 
type systems. A sharable type system designed in 
this way can provide the interoperability between 
independently developed tools with fewer losses in 
information, thus allowing for the combinations of 
tools and comparisons on these combinations. 
We show how our automatic comparison 
generator works based on a type system designed in 
that way. Taking the extraction of protein-protein 
                                                 
1 http://uima.lti.cs.cmu.edu/ 
interaction (PPI) as a typical example of a 
composite task, we illustrate how our platform 
helps users to observe the differences between 
tools and to construct a system for their own needs. 
2 Motivation and Background 
2.1 Goal and Data Oriented Evaluation, 
Module Selection and Inter-operability 
There are standard evaluation metrics for NLP 
modules such as precision, recall and F-value. For 
basic tasks like sentence splitting, POS tagging, 
and named-entity recognition, these metrics can be 
estimated using existing gold-standard test sets.  
Conversely, accuracy measurements based on 
the standard test sets are sometimes deceptive, 
since its accuracy may change significantly in 
practice, depending on the types of text and the 
actual tasks at hand. Because these accuracy 
metrics do not take into account the importance of 
the different types of errors to any particular 
application, the practical utility of two systems 
with seemingly similar levels of accuracy may in 
fact differ significantly. To users and developers 
alike, a detailed examination of how systems 
perform (on the text they would like to process) is 
often more important than standard metrics and 
test sets. Naturally, far greater weight is placed in 
measuring the end-to-end performance of a 
composite system than in measuring the 
performance of the individual components. 
In reality, because the selection of modules 
usually affects the performance of the entire 
system, it is crucial to carefully select modules that 
are appropriate for a given task. This is the main 
reason for having a collection of interoperable 
 
 
TOOL-SPECIFIC TYPES
PennPOS 
Penn verb1 ? ?
POS 
tcas.uima.Annotation 
-begin: int  -end: int 
SyntacticAnnotation SemanticAnnotation 
Sentence Phrase Token NamedEntity Relation 
-ent: FSArray<NamedEntity>
POSToken 
-pos: POS 
RichToken 
uima.jcas.cas.TOP 
UnknownPOS 
-base: String 
-posType: String 
ToolAToken
Verb Noun ?.. 
ToolBPOSToken
Protein 
ToolCProtein
ProteinProteinInteraction
ToolDPPI
Figure 1. Part of our type system 
860
modules. We need to show how the ultimate 
performance will be affected by the selection of 
different modules and show the best combination 
of modules in terms of the performance of the 
whole aggregated system for the task at hand. 
 Since the number of possible combinations of 
component modules is typically large, the system 
has to be able to enumerate and execute them 
semi-automatically. This requires a higher level of 
interoperability of individual modules than just 
wrapping them for UIMA.  
2.2 UIMA 
2.2.1 CAS and Type System 
The UIMA framework uses the ?stand-off 
annotation? style (Ferrucci et al, 2006). The raw 
text in a document is kept unchanged during the 
analysis process, and when the processing of the 
text is performed, the result is added as new stand-
off annotations with references to their positions in 
the raw text. A Common Analysis Structure (CAS) 
maintains a set of these annotations, which in itself 
are objects. The annotation objects in a CAS 
belong to types that are defined separately in a 
hierarchical type system. The features of an 
annotation2  object have values that are typed as 
well. 
2.2.2 Component and Capability 
Each UIMA Component has the capability 
property which describes what types of objects the 
component may take as the input and what types of 
objects it produces as the output. For example, a 
named entity recognizer detects named entities in 
                                                 
tools. Types should be defined in a distinct and 
2 In the UIMA framework, Annotation is a base type which 
has begin and end offset values. In this paper we call any 
objects (any subtype of TOP) as annotations. 
the text and outputs annotation objects of the type 
NamedEntity. 
It is possible to deploy any UIMA component as 
a SOAP web service, so that we can combine a 
remote component on a web service with the local 
component freely inside a UIMA-based system.  
3 Integration Platform and Comparators 
3.1 Sharable and Comparable Type System 
Although UIMA provides a set of useful 
functionalities for an integration platform of 
TM/NLP tools, users still have to develop the 
actual platform by using these functionalities 
effectively. There are several decisions for the 
designer to make an integration platform. 
Determining how to use types in UIMA is a 
crucial decision. Our decision is to keep different 
type systems by individual groups as they are, if 
necessary; we require that individual type systems 
have to be related through a sharable type system, 
which our platform defines. Such a shared type 
system can bridge modules with different type 
systems, though the bridging module may lose 
some information during the translation process.  
Whether such a sharable type system can be 
defined or not is dependent on the nature of each 
problem.  For example, a sharable type system for 
POS tags in English can be defined rather easily, 
since most of POS-related modules (such as POS 
taggers, shallow parsers, etc.) more or less follow 
the well established types defined by the Penn 
Treebank (Marcus et al, 1993) tag set. 
Figure 1 shows a part of our sharable type 
system. We deliberately define a highly organized 
type hierarchy as described above.  
Secondly we should consider that the type 
system may be used to compare a similar sort of 
Comparable Tools 
Sentence 
Detector
Deep 
Parser 
Named  
Entity 
Recognizer 
POS 
Tagger 
PPI 
Extractor 
AImed 
Collection 
Reader 
Comparator 
Evaluator 
Tokenizer 
Figure 2. PPI system workflow  
(conceptual) 
Figure 3.  
Basic example pattern
Comparable Tools
OpenNLP 
Sentence 
Detector 
Enju ABNER 
Stepp 
Tagger
UIMA 
Tokenizer
Figure 4.  
Complex tool example 
Comparable Tools 
GENIA 
Tagger 
OpenNLP 
Sentence 
Detector 
Enju NER 
POS 
Tagger
Tokenizer
Figure 5.  
Branch flow pattern 
Comparable Tools
OpenNLP 
S.D. 
UIMA 
Tokenizer
Enju ABNER 
Stepp 
Tagger
GENIA 
S.D. 
861
hierarchical manner. For example, both tokenizers 
and POS taggers output an object of type Token, 
but their roles are different when we assume a 
cascaded pipeline. We defined Token as a 
supertvpe, POSToken as subtypes of Token. Each 
tool should have an individual type to make clear 
which tool generated which instance, because each 
tool may have a slightly different definition. This 
is important because the capabilities are 
represented by these types, and the capabilities are 
the only attributes which are machine readable. 
3.2 General Combinatorial Comparison 
stem is defined in the previously 
tually shows the workflow of our 
wh
 pattern expansion mechanism which 
ge
cases, a single tool can play two or 
m
                                                
Generator 
Even if the type sy
described way, there are still some issues to 
consider when comparing tools. We illustrate these 
issues using the PPI workflow that we utilized in 
our experiments. 
Figure 2 concep
ole PPI system. If we can prepare two or more 
components for some type of the components in 
the workflow (e.g. two sentence detectors and three 
POS taggers), then we can make combinations of 
these tools to form a multiplied number of 
workflow patterns (2x3 = 6 patterns). See Table 1 
for the details of UIMA components used in our 
experiments. 
We made a
nerates possible workflow patterns automatically 
from a user-defined comparable workflow. A 
comparable workflow is a special workflow that 
explicitly specifies which set of components 
should be compared. Then, users just need to group 
comparable components (e.g. ABNER3 and MedT-
NER as a comparable NER group) without making 
any modifications to the original UIMA 
components. This aggregation of comparable 
components is controlled by our custom workflow 
controller.  
In some 
ore roles (e.g. the GENIA Tagger performs 
tokenization, POS tagging, and NER; see Figure 
4). It may be possible to decompose the original 
tool into single roles, but in most cases it is 
difficult and unnatural to decompose such a 
 
ponent requires two or more input 
ty
4 Experiments and Results 
 using our PPI 
e have several 
co
igure 6 show a part of the 
co
Table 2.   
3 In the example figures, ABNER requires Sentence to 
make the explanation clearer, though ABNER does not 
require it in actual usage. 
complex tool. We designed our comparator to 
detect possible input combinations automatically 
by the types of previously generated annotations, 
and the input capability of each posterior 
component. As described in the previous section, 
the component should have appropriate 
capabilities with proper types in order to permit 
this detection.  
When a com
pes (e.g. our PPI extractor requires outputs of a 
deep parser and a protein NER system), there 
could be different components used in the prior 
flow (e.g. OpenNLP and GENIA sentence 
detectors in Figure 5). Our comparator also 
calculates such cases automatically. 
 OO UO GOO U G A
UU 8 89 8
We have performed experiments
extraction system as an example (Kano et al, 
2008). It is similar to our BioCreative PPI system 
(S?tre et al, 2006) but differs in that we have 
deconstructed the original system into seven 
different components (Figure 2).  
As summarized in Table 1, w
mparable components and the AImed corpus as 
the gold standard data. In this case, possible 
combination workflow patterns are POSToken for 
36, PPI for 589, etc.   
Table 2, 3, 4 and F
mparison result screenshots between these 
patterns on 20 articles from the AImed corpus. In 
the tables, abbreviations like ?OOG? stands for a 
workflow of O(Sentence) -> O(Token) - 
Sentence
comparisons (%). 
Table 3. Part of Token
comparisons, 
precision/recall (%).
OOO UOS GOO 
UUO 87/74 81/68 85/68 
GUG 74/65 73/65 78/65 
GGO 92/95 81/84 97/95 
OGO 100/100 89/88 100/94 
G 0 0 - 85
U
 9/75 /75 8/70
GU 89/75 89/75 88/70
GG 92/95 91/95 97/95
OG 
86 - 0 7
A 6 6 60 -
O - 10 10/100 99/99 00/9481 0 7
Table 4. Part of POSToken comparisons, 
precision/recall (%) 
862
G(POSToken), where O stands for OpenNLP, G 
stands for Genia, U stands for UIMA, etc.  
When neither of the compared results include 
th
e comparison on Sentences 
sh
%  
0 
e gold standard data (AImed in this case), the 
comparison results show a similarity of the tools 
for this specific task and data, rather than an 
evaluation. Even if we lack an annotated corpus, it 
is possible to run the tools and compare the results 
in order to understand the characteristics of the 
tools depending on the corpus and the tool 
combinations.  
Although th
ows low scores of similarities, Tokens are 
almost the same; it means that input sentence 
boundaries do not affect tokenizations so much. 
POSToken similarities drop approximately 0-10
100 
  
                      100
Fi  6  NER (Protein) comp rison di
ences in 
5 Conclusion and Future Work 
ponents, 
 design, which the UIMA 
fra
   0  
gure . a stribution of 
precisions (x-axis, %) and recalls (y-axis, %). 
from the similarities in Token; the differ
Token are mainly apostrophes and punctuations; 
POSTokens are different because each POS 
tagger uses a slightly different set of tags: normal 
Penn tagset for Stepp tagger, BioPenn tagset 
(includes new tags for hyphenation) for GENIA 
tagger, and an original apostrophe tag for 
OpenNLP tagger. 
NLP tasks typically consist of many com
and it is necessary to show which set of tools are 
most suitable for each specific task and data. 
Although UIMA provides a general framework 
with much functionality for interoperability, we 
still need to build an environment that enables the 
combinations and comparisons of tools for a 
specific task.  
The type system
mework does not provide, is one of the most 
critical issues on interoperability. We have thus 
proposed a way to design a sharable and 
comparable type system. Such a type system allows 
for the automatic combinations of any UIMA 
compliant components and for the comparisons of 
these combinations, when the components have 
proper capabilities within the type system. We are 
Sentence Token POSToken RichToken Protein Phrase PPI
GENIA Tagger: Trained on the WSJ, GENIA and PennBioIE corpora (POS). Uses Maximum Entropy (Berger 
et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). Trained on GENIA corpus (Sentence 
Splitter). 
Enju: HPSG parser with predicate argument structures as well as phrase structures. Although trained with Penn 
Treebank, it can compute accurate analyses of biomedical texts owing to its method for domain adaptation (Hara 
et al, 2005). 
STePP Tagger: Based on probabilistic models, tuned to biomedical text trained by WSJ, GENIA (Kim et al, 
2003)  and PennBioIE corpora. 
MedT-NER: Statistical recognizer trained on the JNLPBA data. 
ABNER: From the University of Wisconsin (Settles, 2005), wrapped by the Center for Computational 
Pharmacology at the University of Colorado.  
Akane++: A new version of the AKANE system (Yakushiji, 2006), trained with SVMlight-TK (Joachims, 1999; 
Bunescu and Mooney, 2006; Moschitti, 2006) and the AImed Corpus. 
UIMA Examples: Provided in the Apache UIMA example. Sentence Splitter and Tokenizer. 
OpenNLP Tools: Part of the OpenNLP project (http://opennlp.sourceforge.net/), from Apache UIMA examples. 
AImed Corpus: 225 Medline abstracts with proteins and PPIs annotated (Bunescu and Mooney, 2006).   
Legend:         Input type(s) required for that tool          Input type(s) required optionally          Output type(s)  
Table 1. List of UIMA Components used in our experiment. 
863
preparing to make a portion of the components and 
services described in this paper publicly available 
(http://www-tsujii.is.s.u-tokyo.ac.jp/uima/). 
The final system shows which combination of 
co
or this work includes 
co
cknowledgments 
e wish to thank Dr. Lawrence Hunter?s text 
References 
Vincent J. Della Pietra, and Stephen 
IT 
 Mooney. 
on." Edited 
tcheva, and V. 
ls and 
m Lally, Daniel Gruhl, and Edward 
RC24122. (2006). 
ilistic disambiguation model of an 
t, 
e 
." MIT Press, (1999): 169-
ls 
ser: a tool comparator, using protein-protein 
i. "Introduction to the Bio-Entity 
d 
ics 
 i180-
le Application with the Unstructured Information 
l 43, 
ng a Large Annotated Corpus of 
ractical 
. (2006). 
oko 
 
cally tagging genes, proteins, and other entity 
rsity 
, 
ust Part-of-
tion 
University of Tokyo, (2006).  
mponents has the best score, and also generates 
comparative results. This helps users to grasp the 
characteristics and differences among tools, which 
cannot be easily observed by the widely used F-
score evaluations only. 
Future directions f
mbining the output of several modules of the 
same kind (such as NERs) to obtain better results, 
collecting other tools developed by other groups 
using the sharable type system, making machine 
learning tools UIMA compliant, and making grid 
computing available with UIMA workflows to 
increase the entire performance without modifying 
the original UIMA components. 
 
A
 
W
mining group at the Center for Computational 
Pharmacology for discussing with us and making 
their tools available for this research. This work 
was partially supported by NaCTeM (the UK 
National Centre for Text Mining), Grant-in-Aid for 
Specially Promoted Research (MEXT, Japan) and 
Genome Network Project (MEXT, Japan). 
NaCTeM is jointly funded by 
JISC/BBSRC/EPSRC. 
Berger, Adam L., 
A. Della Pietra. "A maximum entropy approach to 
natural language processing." Comput. Linguist. (M
Press) 22, no. 1 (1996): 39-71. 
Bunescu, Razvan, and Raymond
"Subsequence Kernels for Relation Extracti
by Weiss Y., Scholkopf B. and Platt J., 171-178. 
Cambridge, MA: MIT Press, (2006). 
Cunningham, H., D. Maynard, K. Bon
Tablan. "GATE: A framework and graphical 
development environment for robust NLP too
applications." Proceedings of the 40th Anniversary 
Meeting of the Association for Computational 
Linguistics. (2002). 
Ferrucci, David, Ada
Epstein. "Towards an Interoperability Standard for Text 
and Multi-Modal Analytics." IBM Research Report, 
Hara, Tadayoshi, Yusuke Miyao, and Jun'ichi Tsujii. 
"Adapting a probab
HPSG parser to a new domain." Edited by Dale Rober
Wong Kam-Fai, Su Jian and Yee Oi. Natural Languag
Processing IJCNLP 2005. Jeju Island, Korea: Springer-
Verlag, (2005). 199-210. 
Joachims, Thorsten. "Making large-scale support vector 
machine learning practical
184. 
Kano, Yoshinobu, et al "Filling the gaps between too
and u
interaction as an example." Proceedings of The Pacific 
Symposium on Biocomputing (PSB). Hawaii, USA, To 
appear, (2008). 
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa Tsuruoka, 
and Yuka Tateis
Recognition Task at JNLPBA." Proceedings of the 
International Workshop on Natural Language 
Processing. Geneva, Switzerland, (2004). 70-75. 
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, an
Jun'ichi Tsujii. "GENIA corpus - a semantically 
annotated corpus for bio-textmining." Bioinformat
(Oxford University Press) 19, no. suppl. 1 (2003):
i182. 
Lally, Adam, and David Ferrucci. "Building an 
Examp
Management Architecture." IBM Systems Journa
no. 3 (2004): 455-475. 
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann 
Marcinkiewicz. "Buildi
English: The Penn Treebank." Computational 
Linguistics 19, no. 2 (1993): 313-330. 
Moschitti, Alessandro. "Making Tree Kernels P
for Natural Language Learning." EACL
S?tre, Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiroh Matsubayashi, and Tom
Ohta. "AKANE System: Protein-Protein Interaction
Pairs in BioCreAtIvE2 Challenge." Proceedings of the 
Second BioCreative Challenge Evaluation Workshop. 
(2007). 
Settles, B. "ABNER: an open source tool for 
automati
names in text." Bioinformatics (Oxford Unive
Press) 21, no. 14 (2005): 3191-3192. 
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim
and Tomoko Ohta. "Developing a Rob
Speech Tagger for Biomedical Text." Advances in 
Informatics - 10th Panhellenic Conference on 
Informatics. Volos, Greece, (2005). 382-392. 
Yakushiji, Akane. "Relation Information Extrac
Using Deep Syntactic Analysis." PhD Thesis, 
864
Proceedings of the Workshop on BioNLP: Shared Task, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Overview of BioNLP?09 Shared Task on Event Extraction
Jin-Dong Kim? Tomoko Ohta? Sampo Pyysalo? Yoshinobu Kano? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{jdkim,okap,smp,kano,tsujii}@is.s.u-tokyo.ac.jp
Abstract
The paper presents the design and implemen-
tation of the BioNLP?09 Shared Task, and
reports the final results with analysis. The
shared task consists of three sub-tasks, each of
which addresses bio-molecular event extrac-
tion at a different level of specificity. The data
was developed based on the GENIA event cor-
pus. The shared task was run over 12 weeks,
drawing initial interest from 42 teams. Of
these teams, 24 submitted final results. The
evaluation results are encouraging, indicating
that state-of-the-art performance is approach-
ing a practically applicable level and revealing
some remaining challenges.
1 Introduction
The history of text mining (TM) shows that shared
tasks based on carefully curated resources, such
as those organized in the MUC (Chinchor, 1998),
TREC (Voorhees, 2007) and ACE (Strassel et al,
2008) events, have significantly contributed to the
progress of their respective fields. This has also been
the case in bio-TM. Examples include the TREC Ge-
nomics track (Hersh et al, 2007), JNLPBA (Kim et
al., 2004), LLL (Ne?dellec, 2005), and BioCreative
(Hirschman et al, 2007). While the first two ad-
dressed bio-IR (information retrieval) and bio-NER
(named entity recognition), respectively, the last two
focused on bio-IE (information extraction), seeking
relations between bio-molecules. With the emer-
gence of NER systems with performance capable of
supporting practical applications, the recent interest
of the bio-TM community is shifting toward IE.
Similarly to LLL and BioCreative, the
BioNLP?09 Shared Task (the BioNLP task, here-
after) also addresses bio-IE, but takes a definitive
step further toward finer-grained IE. While LLL and
BioCreative focus on a rather simple representation
of relations of bio-molecules, i.e. protein-protein
interactions (PPI), the BioNLP task concerns the
detailed behavior of bio-molecules, characterized as
bio-molecular events (bio-events). The difference in
focus is motivated in part by different applications
envisioned as being supported by the IE methods.
For example, BioCreative aims to support curation
of PPI databases such as MINT (Chatr-aryamontri
et al, 2007), for a long time one of the primary tasks
of bioinformatics. The BioNLP task aims to support
the development of more detailed and structured
databases, e.g. pathway (Bader et al, 2006) or Gene
Ontology Annotation (GOA) (Camon et al, 2004)
databases, which are gaining increasing interest
in bioinformatics research in response to recent
advances in molecular biology.
As the first shared task of its type, the BioNLP
task aimed to define a bounded, well-defined bio-
event extraction task, considering both the actual
needs and the state of the art in bio-TM technology
and to pursue it as a community-wide effort. The
key challenge was in finding a good balance between
the utility and the feasibility of the task, which was
also limited by the resources available. Special con-
sideration was given to providing evaluation at di-
verse levels and aspects, so that the results can drive
continuous efforts in relevant directions. The pa-
per discusses the design and implementation of the
BioNLP task, and reports the results with analysis.
1
Type Primary Args. Second. Args.
Gene expression T(P)
Transcription T(P)
Protein catabolism T(P)
Phosphorylation T(P) Site
Localization T(P) AtLoc, ToLoc
Binding T(P)+ Site+
Regulation T(P/Ev), C(P/Ev) Site, CSite
Positive regulation T(P/Ev), C(P/Ev) Site, CSite
Negative regulation T(P/Ev), C(P/Ev) Site, CSite
Table 1: Event types and their arguments. The type of the
filler entity is specified in parenthesis. The filler entity
of the secondary arguments are all of Entity type which
represents any entity but proteins: T=Theme, C=Cause,
P=Protein, Ev=Event.
2 Task setting
To focus efforts on the novel aspects of the event
extraction task, is was assumed that named entity
recognition has already been performed and the task
was begun with a given set of gold protein anno-
tation. This is the only feature of the task setting
that notably detracts from its realism. However,
given that state-of-the-art protein annotation meth-
ods show a practically applicable level of perfor-
mance, i.e. 88% F-score (Wilbur et al, 2007), we
believe the choice is reasonable and has several ad-
vantages, including focus on event extraction and ef-
fective evaluation and analysis.
2.1 Target event types
Table 1 shows the event types addressed in the
BioNLP task. The event types were selected from
the GENIA ontology, with consideration given to
their importance and the number of annotated in-
stances in the GENIA corpus. The selected event
types all concern protein biology, implying that they
take proteins as their theme. The first three types
concern protein metabolism, i.e. protein production
and breakdown. Phosphorylation is a representa-
tive protein modification event, and Localization and
Binding are representative fundamental molecular
events. Regulation (including its sub-types, Posi-
tive and Negative regulation) represents regulatory
events and causal relations. The last five are uni-
versal but frequently occur on proteins. For the bio-
logical interpretation of the event types, readers are
referred to Gene Ontology (GO) and the GENIA on-
tology.
The failure of p65 translocation to the nucleus . . .
T3 (Protein, 40-46)
T2 (Localization, 19-32)
E1 (Type:T2, Theme:T3, ToLoc:T1)
T1 (Entity, 15-18)
M1 (Negation E1)
Figure 1: Example event annotation. The protein an-
notation T3 is given as a starting point. The extraction
of annotation in bold is required for Task 1, T1 and the
ToLoc:T1 argument for Task 2, and M1 for Task 3.
As shown in Table 1, the theme or themes of all
events are considered primary arguments, that is, ar-
guments that are critical to identifying the event. For
regulation events, the entity or event stated as the
cause of the regulation is also regarded as a primary
argument. For some event types, other arguments
detailing of the events are also defined (Secondary
Args. in Table 1).
From a computational point of view, the event
types represent different levels of complexity. When
only primary arguments are considered, the first five
event types require only unary arguments, and the
task can be cast as relation extraction between a
predicate (event trigger) and an argument (Protein).
The Binding type is more complex in requiring the
detection of an arbitrary number of arguments. Reg-
ulation events always take a Theme argument and,
when expressed, also a Cause argument. Note that a
Regulation event may take another event as its theme
or cause, a unique feature of the BioNLP task com-
pared to other event extraction tasks, e.g. ACE.
2.2 Representation
In the BioNLP task, events are expressed using three
different types of entities. Text-bound entities (t-
entities hereafter) are represented as text spans with
associated class information. The t-entities include
event triggers (Localization, Binding, etc), protein
references (Protein) and references to other entities
(Entity). A t-entity is represented by a pair, (entity-
type, text-span), and assigned an id with the pre-
fix ?T?, e.g. T1?T3 in Figure 1. An event is ex-
pressed as an n-tuple of typed t-entities, and has
a id with prefix ?E?, e.g. E1. An event modifi-
cation is expressed by a pair, (predicate-negation-
or-speculation, event-id), and has an id with prefix
?M?, e.g. M1.
2
Item Training Devel. Test
Abstract 800 150 260
Sentence 7,449 1,450 2,447
Word 176,146 33,937 57,367
Event 8,597 / 8,615 1,809 / 1,815 3,182 / 3,193
Table 2: Statistics of the data sets. For events,
Task1/Task2 shown separately as secondary arguments
may introduce additional differentiation of events.
2.3 Subtasks
The BioNLP task targets semantically rich event ex-
traction, involving the extraction of several different
classes of information. To facilitate evaluation on
different aspects of the overall task, the task is di-
vided to three sub-tasks addressing event extraction
at different levels of specificity.
Task 1. Core event detection detection of typed,
text-bound events and assignment of given pro-
teins as their primary arguments.
Task 2. Event enrichment recognition of sec-
ondary arguments that further specify the
events extracted in Task 1.
Task 3. Negation/Speculation detection detection
of negations and speculation statements
concerning extracted events.
Task 1 serves as the backbone of the shared task and
is mandatory for all participants. Task 2 involves the
recognition of Entity type t-entities and assignment
of those as secondary event arguments. Task 3 ad-
dresses the recognition of negated or speculatively
expressed events without specific binding to text. An
example is given in Fig. 1.
3 Data preparation
The BioNLP task data were prepared based on the
GENIA event corpus. The data for the training and
development sets were derived from the publicly
available event corpus (Kim et al, 2008), and the
data for the test set from an unpublished portion of
the corpus. Table 2 shows statistics of the data sets.
For data preparation, in addition to filtering out
irrelevant annotations from the original GENIA cor-
pus, some new types of annotation were added to
make the event annotation more appropriate for the
purposes of the shared task. The following sections
describe the key changes to the corpus.
3.1 Gene-or-gene-product annotation
The named entity (NE) annotation of the GENIA
corpus has been somewhat controversial due to dif-
ferences in annotation principles compared to other
biomedical NE corpora. For instance, the NE an-
notation in the widely applied GENETAG corpus
(Tanabe et al, 2005) does not differentiate proteins
from genes, while GENIA annotation does. Such
differences have caused significant inconsistency in
methods and resources following different annota-
tion schemes. To remove or reduce the inconsis-
tency, GENETAG-style NE annotation, which we
term gene-or-gene-product (GGP) annotation, has
been added to the GENIA corpus, with appropriate
revision of the original annotation. For details, we
refer to (Ohta et al, 2009). The NE annotation used
in the BioNLP task data is based on this annotation.
3.2 Argument revision
The GENIA event annotation was made based on
the GENIA event ontology, which uses a loose typ-
ing system for the arguments of each event class.
For example, in Figure 2(a), it is expressed that
the binding event involves two proteins, TRAF2
and CD40, and that, in the case of CD40, its cy-
toplasmic domain takes part in the binding. With-
out constraints on the type of theme arguments,
the following two annotations are both legitimate:
(Type:Binding, Theme:TRAF2, Theme:CD40)
(Type:Binding, Theme:TRAF2,
Theme:CD40 cytoplasmic domain)
The two can be seen as specifying the same event
at different levels of specificity1. Although both al-
ternatives are reasonable, the need to have consis-
tent training and evaluation data requires a consis-
tent choice to be made for the shared task.
Thus, we fix the types of all non-event
primary arguments to be proteins (specifically
GGPs). For GENIA event annotations involving
themes other than proteins, additional argument
types were introduced, for example, as follows:
1In the GENIA event annotation guidelines, annotators are
instructed to choose the more specific alternative, thus the sec-
ond alternative for the example case in Fig. 2(a).
3
(a)
TRAF2 is a ? which binds to the CD40 cytoplasmic domain
GGP GGP PDR
(b)
HMG-I binds to GATA motifs
GGP DDR
(c)
alpha B2 bound the PEBP2 site within the GM-CSF promoter
GGP GGPDDR DDR
Figure 2: Entity annotation to example sentences
from (a) PMID10080948, (b) PMID7575565, and (c)
PMID7605990 (simplified).
(a)
Ah receptor recognizes the B cell transcription factor, BSAP
(b)
Grf40 binds to linker for activation of T cells (LAT)
(c)
expression of p21(WAF1/CIP1) and p27(KIP1)
(d)
included both p50/p50 and p50/p65 dimers
(e)
IL-4 Stat, also known as Stat6
Figure 3: Equivalent entities in example sentences from
(a) PMID7541987 (simplified), (b) PMID10224278, (c)
PMID10090931, (d) PMID9243743, (e) PMID7635985.
(Type:Binding, Theme1:TRAF2, Theme2:CD40,
Site2:cytoplasmic domain)
Note that the protein, CD40, and its domain, cyto-
plasmic domain, are associated by argument num-
bering. To resolve issues related to the mapping
between proteins and related entities systematically,
we introduced partial static relation annotation for
relations such as Part-Whole, drawing in part on
similar annotation of the BioInfer corpus (Pyysalo
et al, 2007). For details of this part of the revision
process, we refer to (Pyysalo et al, 2009).
Figure 2 shows some challenging cases. In (b),
the site GATA motifs is not identified as an argument
of the binding event, because the protein containing
it is not stated. In (c), among the two sites (PEBP2
site and promoter) of the gene GM-CSF, only the
more specific one, PEBP2, is annotated.
3.3 Equivalent entity references
Alternative names for the same object are fre-
quently introduced in biomedical texts, typically
through apposition. This is illustrated in Figure 3(a),
where the two expressions B cell transcription fac-
tor and BSAP are in apposition and refer to the
same protein. Consequently, in this case the fol-
lowing two annotations represent the same event:
(Type:Binding, Theme:Ah receptor,
Theme:B cell transcription factor)
(Type:Binding, Theme:Ah receptor, Theme:BSAP)
In the GENIA event corpus only one of these is an-
notated, with preference given to shorter names over
longer descriptive ones. Thus of the above exam-
ple events, the latter would be annotated. How-
ever, as both express the same event, in the shared
task evaluation either alternative was accepted as
correct extraction of the event. In order to im-
plement this aspect of the evaluation, expressions
of equivalent entities were annotated as follows:
Eq (B cell transcription factor, BSAP)
The equivalent entity annotation in the revised GE-
NIA corpus covers also cases other than simple ap-
position, illustrated in Figure 3. A frequent case in
biomedical literature involves use of the slash sym-
bol (?/?) to state synonyms. The slash symbol is
ambiguous as it is used also to indicate dimerized
proteins. In the case of p50/p50, the two p50 are
annotated as equivalent because they represent the
same proteins at the same state. Note that although
rare, also explicitly introduced aliases are annotated,
as in Figure 3(e).
4 Evaluation
For the evaluation, the participants were given the
test data with gold annotation only for proteins. The
evaluation was then carried out by comparing the
annotation predicted by each participant to the gold
annotation. For the comparison, equality of anno-
tations is defined as described in Section 4.1. The
evaluation results are reported using the standard
recall/precision/f-score metrics, under different cri-
teria defined through the equalities.
4.1 Equalities and Strict matching
Equality of events is defined as follows:
Event Equality equality holds between any two
events when (1) the event types are the same,
(2) the event triggers are the same, and (3) the
arguments are fully matched.
4
A full matching of arguments between two events
means there is a perfect 1-to-1 mapping between the
two sets of arguments. Equality of individual argu-
ments is defined as follows:
Argument Equality equality holds between any
two arguments when (1) the role types are the
same, and (2-1) both are t-entities and equality
holds between them, or (2-2) both are events
and equality holds between them.
Due to the condition (2-2), event equality is defined
recursively for events referring to events. Equality
of t-entities is defined as follows:
T-entity Equality equality holds between any two
t-entities when (1) the entity types are the same,
and (2) the spans are the same.
Any two text spans (beg1, end1) and (beg2, end2),
are the same iff beg1 = beg2 and end1 = end2.
Note that the event triggers are also t-entities thus
their equality is defined by the t-entity equality.
4.2 Evaluation modes
Various evaluation modes can be defined by varying
equivalence criteria. In the following, we describe
three fundamental variants applied in the evaluation.
Strict matching The strict matching mode requires
exact equality, as defined in section 4.1. As some
of its requirements may be viewed as unnecessarily
precise, practically motivated relaxed variants, de-
scribed in the following, are also applied.
Approximate span matching The approximate
span matching mode is defined by relaxing the
requirement for text span matching for t-entities.
Specifically, a given span is equivalent to a gold
span if it is entirely contained within an extension
of the gold span by one word both to the left and
to the right, that is, beg1 ? ebeg2 and end1 ?
eend2, where (beg1, end1) is the given span and
(ebeg2, eend2) is the extended gold span.
Approximate recursive matching In strict match-
ing, for a regulation event to be correct, the events it
refers to as theme or cause must also be be strictly
correct. The approximate recursive matching mode
is defined by relaxing the requirement for recursive
event matching, so that an event can match even
if the events it refers to are only partially correct.
Event Release date
Announcement Dec 8
Sample data Dec 15
Training data Jan 19 ? 21, Feb 2 (rev1), Feb 10 (rev2)
Devel. data Feb 7
Test data Feb 22 ? Mar 2
Submission Mar 2 ? Mar 9
Table 3: Shared task schedule. The arrows indicate a
change of schedule.
Specifically, for partial matching, only Theme argu-
ments are considered: events can match even if re-
ferred events differ in non-Theme arguments.
5 Schedule
The BioNLP task was held for 12 weeks, from the
sample data release to the final submission. It in-
cluded 5 weeks of system design period with sam-
ple data, 6 weeks of system development period with
training and development data, and a 1 week test pe-
riod. The system development period was originally
planned for 5 weeks but extended by 1 week due to
the delay of the training data release and the revi-
sion. Table 3 shows key dates of the schedule.
6 Supporting Resources
To allow participants to focus development efforts
on novel aspects of event extraction, we prepared
publicly available BioNLP resources readily avail-
able for the shared task. Several fundamental
BioNLP tools were provided through U-Compare
(Kano et al, 2009)2, which included tools for to-
kenization, sentence segmentation, part-of-speech
tagging, chunking and syntactic parsing.
Participants were also provided with the syntactic
analyses created by a selection of parsers. We ap-
plied two mainstream Penn Treebank (PTB) phrase
structure parsers: the Bikel parser3, implementing
Collins? parsing model (Bikel, 2004) and trained
on PTB, and the reranking parser of (Charniak
and Johnson, 2005) with the self-trained biomed-
ical parsing model of (McClosky and Charniak,
2008)4. We also applied the GDep5, native de-
pendency parser trained on the GENIA Treebank
2http://u-compare.org/
3http://www.cis.upenn.edu/?dbikel/software.html
4http://www.cs.brown.edu/?dmcc/biomedical.html
5http://www.cs.cmu.edu/?sagae/parser/gdep/
5
NLP Task
Team Task Org Word Chunking Parsing Trigger Argument Ext. Resources
UTurku 1-- 3C+2BI Porter MC SVM SVM (SVMlight)
JULIELab 1-- 1C+2L+2B OpenNLP OpenNLP GDep Dict+Stat SVM(libSVM) UniProt, Mesh,
Porter ME(Mallet) GOA, UMLS
ConcordU 1-3 3C Stanford Stanford Dict+Stat Rules WordNet, VerbNet,
UMLS
UT+DBCLS 12- 2C Porter MC Dict MLN(thebeast)
CCG
VIBGhent 1-3 2C+1B Porter, Stanford Dict SVM(libSVM)
UTokyo 1-- 3C GTag GDep, Dict ME(liblinear) UIMA
Enju
UNSW 1-- 1C+1B GDep CRF Rules WordNet, MetaMap
UZurich 1-- 3C LingPipe, LTChunk Pro3Gres Dict Rules
Morpha
ASU+HU+BU 123 6C+2BI Porter BioLG, Dict Rules Lucene
Charniak Rules
Cam 1-- 3C Porter RASP Dict Rules
UAntwerp 12- 3C GTag GDep MBL MBL(TiMBL)
Rules
UNIMAN 1-- 4C+2BI Porter GDep Dict, CRF SVM MeSH, GO
GTag Rules
SCAI 1-- 1C Rules
UAveiro 1-- 1C+1L NooJ NooJ Rules BioLexicon
USzeged 1-3 3C+1B GTag Dict, VSM C4.5(WEKA) BioScope
Rules
NICTA 1-3 4C GTag ERG CRF(CRF++) Rules JULIE
CNBMadrid 12- 2C+1B Porter, GTag CBR
GTag Rules
CCP-BTMG 123 7C LingPipe LingPipe OpenDMAP LingPipe, CM Rules GO, SO, MIO,
UIMA
CIPS-ASU 1-- 3C MontyTagger Custom Stanford CRF(ABNER) Rules,
NB(WEKA)
UMich 1-- 2C Stanford MC Dict SVM(SVMlight)
PIKB 1-- 5C+2B MIRA MIRA
KoreaU 1-- 5C GTag GDep Rules, ME ME WSJ
Table 4: Profiles of the participants: GTag=GENIAtagger, MLN=Markov Logic Network, UMLS=UMLS SPE-
CIALIST Lexicon/tools, MC=McClosky-Charniak, GDep=Genia Dependency Parser, Stanford=Stanford Parser,
CBR=Case-Based Reasoning, CM=ConceptMapper.
(Tateisi et al, 2005), and a version of the C&C CCG
deep parser6 adapted to biomedical text (Rimell and
Clark, 2008).
The text of all documents was segmented and to-
kenized using the GENIA Sentence Splitter and the
GENIA Tagger, provided by U-Compare. The same
segmentation was enforced for all parsers, which
were run using default settings. Both the native out-
put of each parser and a representation in the popular
Stanford Dependency (SD) format (de Marneffe et
al., 2006) were provided. The SD representation was
created using the Stanford tools7 to convert from the
PTB scheme, the custom conversion introduced by
(Rimell and Clark, 2008) for the C&C CCG parser,
and a simple format-only conversion for GDep.
7 Results and Discussion
7.1 Participation
In total, 42 teams showed interest in the shared task
and registered for participation, and 24 teams sub-
6http://svn.ask.it.usyd.edu.au/trac/candc/wiki
7http://nlp.stanford.edu/software/lex-parser.shtml
mitted final results. All 24 teams participated in the
obligatory Task 1, six in each of Tasks 2 and 3, and
two teams completed all the three tasks.
Table 4 shows a profile of the 22 final teams,
excepting two who wished to remain anonymous.
A brief examination on the team organization (the
Org column) shows a computer science background
(C) to be most frequent among participants, with
less frequent participation from bioinformaticians
(BI), biologists (B) and liguists (L). This may be
attributed in part to the fact that the event extrac-
tion task required complex computational modeling.
The role of computer scientists may be emphasized
in part due to the fact that the task was novel to most
participants, requiring particular efforts in frame-
work design and implementation and computational
resources. This also suggests there is room for im-
provement from more input from biologists.
7.2 Evaluation results
The final evaluation results of Task 1 are shown in
Table 5. The results on the five event types involv-
6
Team Simple Event Binding Regulation All
UTurku 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95
JULIELab 59.81 / 79.80 / 68.38 49.57 / 35.25 / 41.20 35.03 / 34.18 / 34.60 45.82 / 47.52 / 46.66
ConcordU 49.75 / 81.44 / 61.76 20.46 / 40.57 / 27.20 27.47 / 49.89 / 35.43 34.98 / 61.59 / 44.62
UT+DBCLS 55.75 / 72.74 / 63.12 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35
VIBGhent 54.48 / 79.31 / 64.59 38.04 / 38.60 / 38.32 17.36 / 31.61 / 22.41 33.41 / 51.55 / 40.54
UTokyo 45.69 / 72.19 / 55.96 34.58 / 50.63 / 41.10 14.22 / 34.26 / 20.09 28.13 / 53.56 / 36.88
UNSW 45.85 / 69.94 / 55.39 23.63 / 37.27 / 28.92 16.58 / 28.27 / 20.90 28.22 / 45.78 / 34.92
UZurich 44.92 / 66.62 / 53.66 30.84 / 37.28 / 33.75 14.82 / 30.21 / 19.89 27.75 / 46.60 / 34.78
ASU+HU+BU 45.09 / 76.80 / 56.82 19.88 / 44.52 / 27.49 05.20 / 33.46 / 09.01 21.62 / 62.21 / 32.09
Cam 39.17 / 76.40 / 51.79 12.68 / 31.88 / 18.14 09.98 / 37.76 / 15.79 21.12 / 56.90 / 30.80
UAntwerp 41.29 / 65.68 / 50.70 12.97 / 31.03 / 18.29 11.07 / 29.85 / 16.15 22.50 / 47.70 / 30.58
UNIMAN 50.00 / 63.21 / 55.83 12.68 / 40.37 / 19.30 04.05 / 16.75 / 06.53 22.06 / 48.61 / 30.35
SCAI 43.74 / 70.73 / 54.05 28.82 / 35.21 / 31.70 12.64 / 16.55 / 14.33 25.96 / 36.26 / 30.26
UAveiro 43.57 / 71.63 / 54.18 13.54 / 34.06 / 19.38 06.29 / 21.05 / 09.69 20.93 / 49.30 / 29.38
Team 24 41.29 / 64.72 / 50.41 22.77 / 35.43 / 27.72 09.38 / 19.23 / 12.61 22.69 / 40.55 / 29.10
USzeged 47.63 / 44.44 / 45.98 15.27 / 25.73 / 19.17 04.17 / 18.21 / 06.79 21.53 / 36.99 / 27.21
NICTA 31.13 / 77.31 / 44.39 16.71 / 29.00 / 21.21 07.80 / 18.12 / 10.91 17.44 / 39.99 / 24.29
CNBMadrid 50.25 / 46.59 / 48.35 33.14 / 20.54 / 25.36 12.22 / 07.99 / 09.67 28.63 / 20.88 / 24.15
CCP-BTMG 28.17 / 87.63 / 42.64 12.68 / 40.00 / 19.26 03.09 / 48.11 / 05.80 13.45 / 71.81 / 22.66
CIPS-ASU 39.68 / 38.60 / 39.13 17.29 / 31.58 / 22.35 11.86 / 08.15 / 09.66 22.78 / 19.03 / 20.74
UMich 52.71 / 25.89 / 34.73 31.70 / 12.61 / 18.05 14.22 / 06.56 / 08.98 30.42 / 14.11 / 19.28
PIKB 26.65 / 75.72 / 39.42 07.20 / 39.68 / 12.20 01.09 / 30.51 / 02.10 11.25 / 66.54 / 19.25
Team 09 27.16 / 43.61 / 33.47 03.17 / 09.82 / 04.79 02.42 / 11.90 / 04.02 11.69 / 31.42 / 17.04
KoreaU 20.56 / 66.39 / 31.40 12.97 / 50.00 / 20.59 00.67 / 37.93 / 01.31 09.40 / 61.65 / 16.31
Table 5: Evaluation results of Task 1 (recall / precision / f-score).
Team All Site for Phospho.(56) AtLoc & ToLoc (65) All Second Args.
UT+DBCLS 35.86 / 54.08 / 43.12 71.43 / 71.43 / 71.43 23.08 / 88.24 / 36.59 32.14 / 72.41 / 44.52
UAntwerp 21.52 / 45.77 / 29.27 00.00 / 00.00 / 00.00 01.54 /100.00 / 03.03 06.63 / 52.00 / 11.76
ASU+HU+BU 19.70 / 56.87 / 29.26 00.00 / 00.00 / 00.00 00.00 / 00.00 / 00.00 00.00 / 00.00 / 00.00
Team 24 22.08 / 38.28 / 28.01 55.36 / 93.94 / 69.66 21.54 / 66.67 / 32.56 30.10 / 76.62 / 43.22
CCP-BTMG 13.25 / 70.97 / 22.33 30.36 /100.00 / 46.58 00.00 / 00.00 / 00.00 08.67 /100.00 / 15.96
CNBMadrid 25.02 / 18.32 / 21.15 85.71 / 57.14 / 68.57 32.31 / 47.73 / 38.53 50.00 / 09.71 / 16.27
Table 6: Evaluation results for Task 2.
ing only a single primary theme argument are shown
in one merged class, ?Simple Event?. The broad per-
formance range (31% ? 70%) indicates even the ex-
traction of simple events is not a trivial task. How-
ever, the top-ranked systems show encouraging per-
formance, achieving or approaching 70% f-score.
The performance ranges for Binding (5% ? 44%)
and Regulation (1% ? 40%) events show their ex-
traction to be clearly more challenging. It is in-
teresting that while most systems show better per-
formance for binding over regulation events, the
systems [ConcordU] and [UT+DBCLS] are better
for regulation, showing somewhat reduced perfor-
mance for Binding events. This is in particular con-
trast to the following two systems, [ViBGhent] and
[UTokyo], which show far better performance for
Binding than Regulation events. As one possible
explanation, we find that the latter two differentiate
binding events by their number of themes, while the
former two give no specific treatment to multi-theme
binding events. Such observations and comparisons
are a clear benefit of a community-wide shared task.
Table 6 shows the evaluation results for the teams
who participated in Task 2. The ?All? column shows
the overall performance of the systems for Task 2,
while the ?All Second Args.? column shows the
performance of finding only the secondary argu-
ments. The evaluation results show considerable
differences between the criteria. For example, the
system [Team 24] shows performance comparable
to the top ranked system in finding secondary argu-
ments, although its overall performance for Task 2
is more limited. Table 6 also shows the three sys-
tems, [UT+DBCLS], [Team 24] and [CNBMadrid],
7
Team Negation Speculation
ConcordU 14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27
VIBGhent 10.57 / 45.10 / 17.13 08.65 / 15.79 / 11.18
ASU+HU+BU 03.96 / 27.27 / 06.92 06.25 / 28.26 / 10.24
NICTA 05.29 / 34.48 / 09.17 04.81 / 30.30 / 08.30
USzeged 05.29 / 01.94 / 02.84 12.02 / 03.88 / 05.87
CCP-BTMG 01.76 / 05.26 / 02.64 06.73 / 13.33 / 08.95
Table 7: Evaluation results for Task 3.
 0
 10
 20
 30
 40
 50
 60
02/18 02/21 02/24 02/27 03/02 03/05 03/08
daily average
Figure 4: Scatterplot of the evaluation results on the de-
velopment data during the system development period.
show performance at a practical level in particular in
finding specific sites of phosphorylation.
As shown in Table 7, the performance range for
Task 3 is very low although the representation of the
task is as simple as the simple events. We attribute
the reason to the fact that Task 3 is the only task of
which the annotation is not bound to textual clue,
thus no text-bound annotation was provided.
Figure 4 shows a scatter plot of the performance
of the participating systems during the system devel-
opment period. The performance evaluation comes
from the log of the online evaluation system on the
development data. It shows the best performance
and the average performance of the participating
systems were trending upwards up until the dead-
line of final submission, which indicates there is still
much potential for improvement.
7.3 Ensemble
Table 8 shows experimental results of a system en-
semble using the final submissions. For the ex-
periments, the top 3?10 systems were chosen, and
the output of each system treated as a weighted
vote8. Three weighting schemes were used; ?Equal?
weights each vote equally; ?Averaged? weights each
8We used the ?ensemble? function of U-Compare.
Ensemble Equal Averaged Event Type
Top 3 53.19 53.19 54.08
Top 4 54.34 54.34 55.21
Top 5 54.77 55.03 55.10
Top 6 55.13 55.77 55.96
Top 7 54.33 55.45 55.73
Top 10 52.79 54.63 55.18
Table 8: Experimental results of system ensemble.
vote by the overall f-score of the system; ?Event
Type? weights each vote by the f-score of the sys-
tem for the specific event type. The best score,
55.96%, was obtained by the ?Event Type? weight-
ing scheme, showing a 4% unit improvement over
the best individual system. While using the final
scores for weighting uses data that would not be
available in practice, similar weighting could likely
be obtained e.g. using performance on the devel-
opment data. The experiment demonstrates that an
f-score better than 55% can be achieved simply by
combining the strengths of the systems.
8 Conclusion
Meeting with the community-wide participation, the
BioNLP Shared Task was successful in introducing
fine-grained event extraction to the domain. The
evaluation results of the final submissions from the
participants are both promising and encouraging for
the future of this approach to IE. It has been revealed
that state-of-the-art performance in event extraction
is approaching a practically applicable level for sim-
ple events, and also that there are many remain-
ing challenges in the extraction of complex events.
A brief analysis suggests that the submitted data
together with the system descriptions are rich re-
sources for finding directions for improvements. Fi-
nally, the experience of the shared task participants
provides an invaluable basis for cooperation in fac-
ing further challenges.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Genome Network Project (MEXT, Japan).
8
References
Gary D. Bader, Michael P. Cary, and Chris Sander. 2006.
Pathguide: a Pathway Resource List. Nucleic Acids
Research., 34(suppl 1):D504?506.
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Evelyn Camon, Michele Magrane, Daniel Barrell, Vi-
vian Lee, Emily Dimmer, John Maslen, David Binns,
Nicola Harte, Rodrigo Lopez, and Rolf Apweiler.
2004. The Gene Ontology Annotation (GOA)
Database: sharing knowledge in Uniprot with Gene
Ontology. Nucl. Acids Res., 32(suppl 1):D262?266.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Andrew Chatr-aryamontri, Arnaud Ceol, Luisa Montec-
chi Palazzi, Giuliano Nardelli, Maria Victoria Schnei-
der, Luisa Castagnoli, and Gianni Cesareni. 2007.
MINT: the Molecular INTeraction database. Nucleic
Acids Research, 35(suppl 1):D572?574.
Nancy Chinchor. 1998. Overview of MUC-7/MET-2.
In Message Understanding Conference (MUC-7) Pro-
ceedings.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
William Hersh, Aaron Cohen, Ruslenm Lynn, , and
Phoebe Roberts. 2007. TREC 2007 Genomics track
overview. In Proceeding of the Sixteenth Text RE-
trieval Conference.
Lynette Hirschman, Martin Krallinger, and Alfonso Va-
lencia, editors. 2007. Proceedings of the Second
BioCreative Challenge Evaluation Workshop. CNIO
Centro Nacional de Investigaciones Oncolo?gicas.
Yoshinobu Kano, William Baumgartner, Luke McCro-
hon, Sophia Ananiadou, Kevin Cohen, Larry Hunter,
and Jun?ichi Tsujii. 2009. U-Compare: share and
compare text mining tools with UIMA. Bioinformat-
ics. To appear.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the bio-entity recognition task at JNLPBA. In Pro-
ceedings of the International Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA), pages 70?75.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
Claire Ne?dellec. 2005. Learning Language in Logic -
Genic Interaction Extraction Challenge. In J. Cussens
and C. Ne?dellec, editors, Proceedings of the 4th Learn-
ing Language in Logic Workshop (LLL05), pages 31?
37.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, and
Jun?ichi Tsujii. 2009. Incorporating GENETAG-style
annotation to GENIA corpus. In Proceedings of Nat-
ural Language Processing in Biomedicine (BioNLP)
NAACL 2009 Workshop. To appear.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static Relations: a Piece
in the Biomedical Information Extraction Puzzle.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
Laura Rimell and Stephen Clark. 2008. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, To Appear.
Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi
Song, and Kazuaki Maeda. 2008. Linguistic Re-
sources and Evaluation Techniques for Evaluation of
Cross-Document Automatic Content Extraction. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and John Wilbur. 2005. Genetag: a tagged cor-
pus for gene/protein named entity recognition. BMC
Bioinformatics, 6(Suppl 1):S3.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the IJCNLP 2005,
Companion volume, pages 222?227.
Ellen Voorhees. 2007. Overview of TREC 2007. In
The Sixteenth Text REtrieval Conference (TREC 2007)
Proceedings.
John Wilbur, Lawrence Smith, and Lorraine Tanabe.
2007. BioCreative 2. Gene Mention Task. In
L. Hirschman, M. Krallinger, and A. Valencia, editors,
Proceedings of Second BioCreative Challenge Evalu-
ation Workshop, pages 7?16.
9
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 22?30,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Integrated NLP Evaluation System for Pluggable Evaluation Metrics 
with Extensive Interoperable Toolkit 
 
 
Yoshinobu Kano1   Luke McCrohon1   Sophia Ananiadou2   Jun?ichi Tsujii1,2 
 
1 Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester and National Centre for 
Text Mining, 131 Princess St, M1 7DN, UK 
  
[kano,tsujii]@is.s.u-tokyo.ac.jp 
luke.mccrohon@gmail.com 
sophia.ananiadou@manchester.ac.uk 
 
  
 
Abstract 
To understand the key characteristics of NLP 
tools, evaluation and comparison against dif-
ferent tools is important. And as NLP applica-
tions tend to consist of multiple semi-
independent sub-components, it is not always 
enough to just evaluate complete systems, a 
fine grained evaluation of underlying compo-
nents is also often worthwhile. Standardization 
of NLP components and resources is not only 
significant for reusability, but also in that it al-
lows the comparison of individual components 
in terms of reliability and robustness in a wid-
er range of target domains.  But as many eval-
uation metrics exist in even a single domain, 
any system seeking to aid inter-domain eval-
uation needs not just predefined metrics, but 
must also support pluggable user-defined me-
trics. Such a system would of course need to 
be based on an open standard to allow a large 
number of components to be compared, and 
would ideally include visualization of the dif-
ferences between components. We have de-
veloped a pluggable evaluation system based 
on the UIMA framework, which provides vi-
sualization useful in error analysis. It is a sin-
gle integrated system which includes a large 
ready-to-use, fully interoperable library of 
NLP tools. 
1 Introduction 
When building NLP applications, the same sub-
tasks tend to appear frequently while construct-
ing different systems.  Due to this, the reusability 
of tools designed for such subtasks is a common 
design consideration; fine grained interoperabili-
ty between sub components, not just between 
complete systems. 
In addition to the benefits of reusability, inte-
roperability is also important in evaluation of 
components. Evaluations are normally done by 
comparing two sets of data, a gold standard data 
and test data showing the components perfor-
mance. Naturally this comparison requires the 
two data sets to be in the same data format with 
the same semantics. Comparing of "Apples to 
Apples" provides another reason why standardi-
zation of NLP tools is beneficial. Another advan-
tage of standardization is that the number of gold 
standard data sets that can be compared against is 
also increased, allowing tools to be tested in a 
wider range of domains. 
The ideal is that all components are standar-
dized to conform to an open, widely used intero-
perability framework. One possible such frame-
work is UIMA; Unstructured Information Man-
agement Architecture (Ferrucci et al, 2004), 
which is an open project of OASIS and Apache. 
We have been developing U-Compare (Kano et 
al., 2009)1, an integrated testing an evaluation 
platform based on this framework. 
                                                 
1 Features described in this paper are integrated as U-
Compare system, publicly available from:  
http://u-compare.org/ 
22
Although U-Compare already provided a wide 
range of tools and NLP resources, its inbuilt 
evaluation mechanisms were hard coded into the 
system and were not customizable by end users. 
Furthermore the evaluation metrics used were 
based only on simple strict matchings which se-
verely limited its domains of application. We 
have extended the evaluation mechanism to al-
low users to define their own metrics which can 
be integrated into the range of existing evalua-
tion tools.  
The U-Compare library of interoperable tools 
has also been extended; especially with regard to 
resources related to biomedical named entity ex-
traction. U-Compare is currently providing the 
world largest library of type system compatible 
UIMA components. 
In section 2 of this paper we first look at the 
underlying technologies, UIMA and 
U-Compare. Then we describe the new plugga-
ble evaluation mechanism in section 3 and our 
interoperable toolkit with our type system in sec-
tion 4 and 5. 
 
2 Background 
2.1 UIMA 
UIMA is an open framework specified by OA-
SIS2. Apache UIMA provides a reference im-
plementation as an open source project, with 
both a pure java API and a C++ development kit . 
UIMA itself is intended to be purely a frame-
work, i.e. it does not intend to provide specific 
tools or type system definitions. Users should 
develop such resources themselves. In the fol-
lowing subsections, we briefly describe the basic 
concepts of UIMA, and define keywords used to 
explain our system in later sections. 
2.1.1 CAS and Type System 
The UIMA framework uses the ?stand-off anno-
tation? style (Ferrucci et al, 2006). The underl-
ing raw text of a document is generally kept un-
changed during analysis, and the results of 
processing the text are added as new stand-off 
annotations with references to their positions in 
the raw text. A Common Analysis Structure 
(CAS) holds a set of such annotations. Each of 
which is of a given type as defined in a specified 
                                                 
                                                
2 http://www.oasis-open.org/committees/uima/ 
hierarchical type system. Annotation3 types may 
define features, which are themselves typed. 
Apache UIMA provides definitions of a range of 
built in primitive types, but a more complete type 
system should be specified by developers. The 
top level Apache UIMA type is referred to as 
TOP, other primitive types include. int, String, 
Annotation and FSArray (an array of any annota-
tions). 
2.1.2 Component and Capability 
UIMA components receive and update CAS one 
at a time. Each UIMA component has a capabili-
ty property, which describes what types of anno-
tations it takes as input and what types of anno-
tations it may produce as output.  
UIMA components can be deployed either lo-
cally, or remotely as SOAP web services. Re-
motely deployed web service components and 
locally deployed components can be freely com-
bined in UIMA workflows. 
2.1.3 Aggregate Component and Flow Con-
troller 
UIMA components can be either primitive or 
aggregate. Aggregate components include other 
components as subcomponents. Subcomponents 
may themselves be aggregate. In the case where 
an aggregate has multiple subcomponents these 
are by default processed in linear order. This or-
dering can be customized by implementing a 
custom flow controller. 
2.2 U-Compare 
U-Compare is a joint project of the University of 
Tokyo, the Center for Computational Pharma-
cology at the University of Colorado School of 
Medicine, and the UK National Centre for Text 
Mining. 
U-Compare provides an integrated platform 
for users to construct, edit and compare 
workflows compatible with any UIMA compo-
nent. It also provides a large, ready-to-use toolkit 
of interoperable NLP components for use with 
any UIMA based system. This toolkit is currently 
the world largest repository of type system com-
patible components. These all implement the U-
Compare type system described in section 3. 
 
3  In the UIMA framework, Annotation is a base 
type which has begin and end offset values. In this paper 
we call any objects (any subtype of TOP) as annotations. 
23
2.2.1 Related Works 
There also exist several other public UIMA 
component repositories: CMU UIMA component 
repository, BioNLP UIMA repository (Baum-
gartner et al, 2008), JCoRe (Hahn et al, 2008), 
Tsujii Lab Component Repository at the Univer-
sity of Tokyo (Kano et al, 2008a), etc. Each 
group uses their own type system, and so com-
ponents provided by each group are incompatible. 
Unlike U-Compare these repositories are basical-
ly only collections of UIMA components, U-
Compare goes further by providing a fully inte-
grated set of UIMA tools and utilities. 
2.2.2 Integrated Platform 
U-Compare provides a variety of features as part 
of an integrated platform. The system can be 
launched with a single click in a web browser; all 
required libraries are downloaded and updated 
automatically in background.  
The Workflow Manager GUI helps users to 
create workflows in an easy drag-and-drop fa-
shion. Similarly, import/export of workflows, 
running of workflows and saving results can all 
be handled via a graphical interface.  
U-Compare special parallel aggregate compo-
nents allow combinations of specified compo-
nents to be automatically combined and com-
pared based on their I/O capabilities (Kano et al, 
2008b). When workflows are run, U-Compare 
shows statistics and visualizations of results ap-
propriate to the type of workflow. For example 
when workflows including parallel aggregate 
components are run comparison statistics be-
tween all possible parallel component combina-
tions are given. 
3 Integrated System for Pluggable 
Evaluation Metrics  
While U-Compare already has a mechanism to 
automatically create possible combinations of 
components for comparison from a specified 
workflow, the comparison (evaluation) metric 
itself was hard coded into the system. Only com-
parison based on simple strict matching was 
possible. 
However, many different evaluation metrics 
exist, even for the same type of annotations. For 
example, named entity recognition results are 
often evaluated based on several different anno-
tation intersection criteria: exact match, left/right 
only match, overlap, etc. Evaluation metrics for 
nested components can be even more complex 
(e.g. biomedical relations, deep syntactic struc-
tures). Sometimes new metrics are also required 
for specific tasks. Thus, a mechanism for plugg-
able evaluation metrics in a standardized way is 
seen as desirable. 
3.1 Pluggable Evaluation Component 
Our design goal for the evaluation systems is to 
do as much of the required work as possible and 
to provide utilities to reduce developer?s labor. 
We also want our design to be generic and fix 
within existing UIMA standards. 
The essential process of evaluation can be ge-
neralized and decomposed as follows: 
 
(a) prepare a pair of annotation sets which 
will be used for comparison, 
(b) select annotations which should be in-
cluded in the final evaluation step, 
(c) compare selected annotations against 
each other and mark matched pairs. 
For example, in the case of the Penn Treebank 
style syntactic bracket matching, these steps cor-
respond to (a) prepare two sets of constituents 
and tokens, (b) select only the constituents (re-
moving null elements if required), (c) compare 
constituents between the sets and return any 
matches. 
In our new design, step (a) is performed by the 
system, (b) and (c) are performed by an evalua-
tion component. The evaluation component is 
just a normal UIMA component, pluggable based 
on the UIMA standard. This component is run on 
a CAS which was constructed by the system dur-
ing step (a). This CAS includes an instance of 
ComparisonSet type and its features GoldAnno-
tationGroup and TestAnnotationGroup. Corres-
ponding to step (b), based on this input the com-
parison component should make a selection of 
annotations and store them as FSArray for both 
GoldAnnotations and TestAnnotations. Finally 
for step (c), the component should perform a 
matching and store the results as MatchedPair 
instances in the MatchedAnnotations feature of 
the ComparisonSet. 
Precision, recall, and F1 scores are calculated 
by U-Compare based on the outputted Compari-
sonSet. These calculation can be overridden and 
customized if the developer so desires. 
Implementation of the compare() method of 
the evaluation component is recommended. It is 
used by the system when showing instance based 
evaluations of what feature values are used in 
24
matching, which features are matched, and which 
are not. 
3.2 Combinatorial Evaluation and Er-
ror Analysis 
By default, evaluation statistics are calculated by 
simply counting the numbers of gold, test, 
matched annotations in the returned Compari-
sonSet instance. Then precision, recall, and F1 
scores for each CAS and for the complete set of 
CASes are calculated. Users can specify which 
evaluation metrics are used for each type of an-
notations based on the input specifications they 
set for supplied evaluation components. 
Normally, precision, recall, and F1 scores are 
the only evaluation statistics used in the NLP 
community. It is often the case in many research 
reports that a new tool A performs better than 
another tool B, increasing the F1 score by 1%. In 
such cases it is important to analysis what pro-
portion of annotations are shared between A, B, 
and the gold standard. Is A a strict 1% increase 
over B? Or does it cover 2% of instances B 
doesn?t but miss a different 1%? Our system 
provides these statistics as well. 
Further, our standardized evaluation system 
makes more advanced evaluation available. 
Since the evaluation metrics themselves are more 
or less arbitrary, we should carefully observe the 
results of evaluations. When two or more metrics 
are available for the same type of annotations, 
we can compare the results of each to analyze 
and validate the individual evaluations. 
An immediate application of such comparison 
would be in a voting system, which takes the 
results of several tools as input and selects com-
mon overlapping annotations as output. 
U-Compare also provides visualizations of 
evaluation results allowing instance-based error 
analysis. 
4 U-Compare Type System 
U-Compare currently provides the world largest 
set of type system compatible UIMA compo-
nents. We will describe some of these in section 
5. In creating compatible components in UIMA a 
key task is their type system definitions. 
The U-Compare type system is designed in a 
hierarchical fashion with distinct types to achieve 
a high level of interoperability. It is intended to 
be a shared type system capable of mapping 
types originally defined as part of independent 
type systems (Kano et al, 2008c). In this section 
we describe the U-Compare type system in detail. 
4.1 Basic Types 
While most of the U-Compare types are inherit-
ing a UIMA built-in type, Annotation (Figure 1), 
there are also types directly extending the TOP 
type; let us call these types as metadata types.  
AnnotationMetadata holds a confidence value, 
which is common to all of the U-Compare anno-
tation types as a feature of BaseAnnotation type. 
BaseAnnotation extends DiscontinuousAnnota-
tion, in which fragmental annotations can be 
stored as a FSArray of Annotations, if any.  
ExternalReference is another common meta-
data type where namespace and ID are stored, 
referring to an external ontology entity outside 
UIMA/U-Compare. Because it is not realistic to 
represent everything like such a detailed ontolo-
gy hierarchy in a UIMA type system, this meta-
data is used to recover original information, 
which are not expressed as UIMA types. Refe-
renceAnnotation is another base annotation type, 
which holds an instance of this ExternalRefe-
rence.  
UniqueLabel is a special top level type for ex-
plicitly defined finite label sets, e.g. the Penn 
Treebank tagset. Each label in such a tagset is 
mapped to a single type where UniqueLabel as its 
BaseAnnotation 
<AnnotationMetadata> 
SyntacticAnnotation 
Token
POSToken
<POS> 
RichToken
<String>base
Sentence Dependency 
<DependencyLabel> 
Stanford 
Dependency 
TreeNode 
<TOP>parent 
<FSArray>children
AbstractConstituent
NullElement 
<NullElementLabel>
<Constituent> 
Constituent 
<ConstituentLabel>
FunctionTaggedConstituent 
<FunctionLabel> 
TemplateMappedConstituent 
<Constituent> 
TOP
Coordinations
<FSArray>
Figure 2. Syntactic Types in U-Compare. 
25
ancestor, putting middle level types if possible 
(e.g. Noun type for the Penn Treebank POS tag-
set). These types are omitted in the figure.  
4.2 Syntactic Types 
SyntacticAnnotation is the base type of all syn-
tactic types (Figure 2). POSToken holds a POS 
label, RichToken additionally holds a base form. 
Dependency is used by dependency parsers, 
while TreeNode is for syntactic tree nodes. Con-
stituent, NullElement, FunctionTaggedConsti-
tiuent, TemplateMappedConstituent are designed 
to fully represent all of the Penn Treebank style 
annotations. Coordination is a set of references to 
coordinating nodes (currently used by the Genia 
Treebank). We are planning on extending the set 
of syntactic types to cover the outputs of several 
deep parsers. 
4.3 Semantic Types  
SemanticAnnotation is the base type for semantic 
annotations; it extends ReferenceAnnotation by 
holding the original reference.  
SemanticClassAnnotation is a rather complex 
type designed to be somewhat general. In many 
cases, semantic annotations may reference other 
semantic annotations, e.g. references between 
biological events. Such references are often la-
beled with their roles which we express with the 
ExternalReference type. Such labeled references 
are expressed by LinkingAnnotationSet. As a role 
may refer to more than one annotation, Linkin-
gAnnotationSet has an FSArray of SemanticAn-
notation as a feature. 
There are several biomedical types included in 
Figure 3, e.g. DNA, RNA, Protein, Gene, Cel-
lLine, CellType, etc. It is however difficult to 
decide which ontological entities should be in-
cluded in such a type system. One reason for this 
is that such concepts are not always distinct; dif-
ferent ontologies may give overlapping defini-
tions of these concepts. Further, the number of 
possible substance level entities is infinite; caus-
ing difficult in their expression as individual 
types. The current set of biomedical types in the 
U-Compare type system includes types which are 
frequently used for evaluation in the BioNLP 
research. 
4.4 Document Types  
DocumentAnnotation is the base type for docu-
ment related annotations (Figure 4). It extends 
DocumentClassAnnotation 
<FSArray:DocumentAttribute> 
<FSArray:ReferenceAnnotation> 
DocumentAttribute 
<ExternalReference> 
DocumentAnnotation 
DocumentReferenceAttribute
<ReferenceAnnotation>
DocumentValueAttribute
<String>value 
ReferenceAnnotation TOP
Figure 4. Document types in the U-Compare type system. 
SemanticAnnotation
ReferenceAnnotation
SemanticClassAnnotation 
<FSArray:LinkedAnnotationSet>
NamedEntity EventAnnotation
CellType CellLine GeneOrGeneProductRNADNAProper 
Name
Title 
Place Protein GenePerson 
ProteinRegion
DNARegion
LinkingAnnotationSet 
<ExternalReference> 
<FSArray:SemanticAnnotation>
CoreferenceAnnotation DiscourseEntity Expression
Negation 
TOP 
Speculation
Figure 3. Semantic types in the U-Compare type system. 
26
ReferenceAnnotation to reference the full exter-
nal type in the same way as SemanticAnnotation.  
187 
The document length in bytes is 
output in the first line (end with 
new line),  
DocumentClassAnnotation together with Do-
cumentAttribute are intended to express XML 
style data. XML tags may have fields storing 
their values, and/or idref fields refering to other 
tags. DocumentValueAttiributerepresents simple 
value field, while DocumentReferenceAttribute 
represents idref type fields. A DocumentClas-
sAnnotation corresponds to the tag itself. 
then the raw text follows as is 
(attaching a new line in the end), 
finally annotations follow line by 
line. 
0 187 Document id="u1" 
0 3 POSToken id="u2" pos="DT" 
.... 
Although these types can represent most doc-
ument structures, we still plan to add several 
specific types such as Paragraph, Title, etc. 
Figure 5. An example of the U-Compare simple I/O 
format. 
 
5 Interoperable Components and Utili-
ties 
In this section, we describe our extensive toolkit 
of interoperable components and the set of utili-
ties integrated into the U-Compare system. All of 
the components in our toolkit are compatible 
with the U-Compare type system described in the 
previous section. 
5.1 Corpus Reader Components  
In the UIMA framework, a component which 
generates CASes is called a Collection Reader. 
We have developed several collection readers 
which read annotated corpora and generates an-
notations using the U-Compare type system.  
Because our primary target domain was bio-
medical field, there are corpus readers for the 
biomedical corpora; Aimed corpus (Bunescu et 
al., 2006) reader and BioNLP ?09 shared task 
format reader generate event annotations like 
protein-protein interaction annotations; Readers 
for BIO/IOB format, Bio1 corpus (Tateisi et al, 
2000), BioCreative (Hirschman et al, 2004) task 
1a format, BioIE corpus (Bies et al, 2005), 
NLPBA shared task dataset (Kim et al, 2004), 
Texas Corpus (Bunescu et al, 2005), Yapex 
Corpus (Kristofer Franzen et al, 2002), generate 
biomedical named entities, and Genia Treebank 
corpus (Tateisi et al, 2005) reader generates 
Penn Treebank (Marcus et al, 1993) style brack-
eting and part-of-speech annotations. Format 
readers require users to prepare annotated data, 
while others include corpora themselves, auto-
matically downloaded as an archive on users? 
demand. 
In addition, there is File System Collection 
Reader from Apache UIMA which reads files as 
plain text. We have developed an online interac-
tive text reader, named Input Text Reader. 
5.2 Analysis Engine Components  
There are many tools covering from basic syn-
tactic annotations to the biomedical annotations. 
Some of the tools are running as web services, 
but users can freely mix local services and web 
services. 
For syntactic annotations: sentence detectors 
from GENIA, LingPipe, NaCTeM, OpenNLP 
and Apache UIMA; tokenizers from GENIA tag-
ger (Tsuruoka et al, 2005), OpenNLP, Apache 
UIMA and Penn Bio Tokenizer; POS taggers 
from GENIA tagger, LingPipe, OpenNLP and 
Stepp Tagger; parsers from OpenNLP (CFG), 
Stanford Parser (dependency) (de Marneffe et al, 
2006), Enju (HPSG) (Miyao et al, 2008). 
For semantic annotations: ABNER (Settles, 
2005) for NLPBA/BioCreative trained models, 
GENIA Tagger, NeMine, MedT-NER, LingPipe 
and OpenNLP NER, for named entity recogni-
tions. Akane++ (S?tre et al, 2007) for protein-
protein interaction detections. 
5.3 Components for Developers  
Although Apache UIMA provides APIs in both 
Java and C++ to help users develop UIMA com-
ponents, a level of understanding of the UIMA 
framework is still required. Conversion of exist-
ing tools to the UIMA framework can also be 
difficult, particularly when they are written in 
other programming languages. 
We have designed a simple I/O format to 
make it easy for developers who just want to 
provide a UIMA wrapper for existing tools.  
Input of this format consists of two parts: raw 
text and annotations The first line of the raw text 
section is an integer of byte count of the length 
of the text. The raw text then follows with a new-
line character appended at the end. Annotations 
are then included; one annotation per line, some-
times referring another annotation by assigned 
ids (Figure 5). A line consists of begin position, 
27
end position, type name, unique id, and feature 
values if any. Double newlines indicates an end 
of a CAS. 
Output of the component is lines of annota-
tions if any created by the component. 
U-Compare provides a wrapper component 
which uses this I/O format, communicating with 
wrapped tools via standard I/O streams. 
5.4 Type System Converters 
As U-Compare is a joint project, the U-Compare 
toolkit includes UIMA components originally 
developed using several different type systems. 
In order to integrate these components into the 
U-Compare type system, we have developed 
type system converter components for each ex-
ternal type system. 
The CCP team at the University of Colorado 
made a converter between their CCP type system 
and our type system. We also developed conver-
ters for OpenNLP components and Apache UI-
MA components. These converters remove any 
original annotations not compatible with the U-
Compare type system. This prevents duplicated 
converters from translating external annotation 
multiple times in the same workflow. 
We are providing such non U-Compare com-
ponents by aggregating with type system conver-
ters, so users do not need to aware of the type 
system conversions. 
5.5 Utility Tools  
We have developed and integrated several utility 
tools, especially GUI tools for usability and error 
analysis. 
Figure 6 is showing our workflow manager 
GUI, which provides functions to create a user 
workflow by an easy drag-and-drop way. By 
clicking ?Run Workflow? button in that manager 
window, statistics will be shown (Figure 8).  
Figure 6. A s
There are also a couple of annotation visuali-
zation tools. Figure 7 is showing a viewer for 
tree structures and HPSG feature structures. Fig-
ure 9 is showing a general annotation viewer, 
when annotations have complex inter-
dependencies. 
6 Summary and Future Directions  
We have designed and developed a pluggable 
evaluation system based on the UIMA frame-
work. This evaluation system is integrated with 
the U-Compare combinatorial comparison me-
chanism which makes evaluation of many factors 
available automatically. 
creenshot of Workflow Manager 
GUI and Component Library. 
Since the system behavior is dependent on the 
type system used, we have carefully designed the 
U-Compare type system to cover a broad range 
of concepts used in NLP applications. Based di-
rectly on this type system, or using type system 
converters, we have developed a large toolkit of 
type system compatible interoperable UIMA 
component. All of these features are integrated 
into U-Compare. 
Figure 7. A screenshot of HPSG feature structure 
viewer, showing a skeleton CFG tree, feature values 
and head/semhead links. 
28
In future we are planning to increase the num-
ber of components available, e.g. more syntactic 
parsers, corpus readers, and resources for lan-
guages other than English. This will also re-
quired enhancements to the existing type system 
to support additional components. Finally we 
also hope to add integration with machine learn-
ing tools in the near future. 
 
Acknowledgments 
onal Centre for Text Mining is 
Figure 8. A screenshot of a comparison statistics showing number of instances (gold, test, and
matched), F1, precision, and recall scores of two evaluation metrics on the same data. 
 
We wish to thank Dr. Lawrence Hunter?s text 
mining group at Center for Computational Phar-
macology, University of Colorado School of 
Medicine, for helping build the type system and 
for making their tools available for this research. 
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT, 
Japan). The Nati
funded by JISC. 
W.
ning sys-
tems. J Biomed Discov Collab, 3(1), 1. 
An
ie in the Sky, ACL, Ann Arbor, 
Michigan, USA. 
Ra
tificial Intelligence in Medi-
cine, 33(2), 139-155. 
References  
 A. Baumgartner, Jr., K. B. Cohen, and L. Hunter. 
2008. An open-source framework for large-scale, 
flexible evaluation of biomedical text mi
n  Bies, Seth Kulick, and Mark Mandel. 2005. Pa-
rallel entity and treebank annotation. In Proceed-
ings of the the Workshop on Frontiers in Corpus 
Annotations II: P
zvan  Bunescu, Ruifang Ge, Rohit J. Kate, Edward 
M. Marcotte, Raymond J. Mooney, Arun Kumar 
Ramani, et al 2005. Comparative experiments on 
learning information extractors for proteins and 
their interactions. ArFigure 9. A screenshot of a visualization of com-
plex annotations. 
29
Razvan Bunescu, and Raymond Mooney. 2006. Sub-
sequence Kernels for Relation Extraction. In Y. 
Weiss, B. Scholkopf and J. Platt (Eds.), Advances 
in Neural Information Processing Systems 18 (171-
-178). Cambridge, MA: MIT Press. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. 
In Proceedings of the the 5th International Confe-
rence on Language Resources and Evaluation 
(LREC 2006). 
David Ferrucci, and Adam Lally. 2004. Building an 
example application with the Unstructured Infor-
mation Management Architecture. Ibm Systems 
Journal, 43(3), 455-475. 
David Ferrucci, Adam Lally, Daniel Gruhl, and Ed-
ward Epstein. 2006. Towards an Interoperability 
Standard for Text and Multi-Modal Analytics. 
U. Hahn, E. Buyko, R. Landefeld, M. M?hlhausen, M.  
Poprat, K.  Tomanek, et al 2008, May. An Over-
view of JCoRe, the JULIE Lab UIMA Component 
Repository. In Proceedings of the LREC'08 Work-
shop, Towards Enhanced Interoperability for Large 
HLT Systems: UIMA for NLP, Marrakech, Moroc-
co. 
Lynette Hirschman, Alexander Yeh, Christian 
Blaschke, and Antonio Valencia. 2004. Overview 
of BioCreAtIvE: critical assessment of information 
extraction for biology. BMC Bionformatics, 
6(Suppl 1:S1). 
Yoshinobu Kano, William A Baumgartner, Luke 
McCrohon, Sophia Ananiadou, Kevin B Cohen, 
Lawrence Hunter, et al 2009. U-Compare: share 
and compare text mining tools with UIMA. Bioin-
formatics, accepted. 
Yoshinobu Kano, Ngan Nguyen, Rune S?tre, Keiichi-
ro Fukamachi, Kazuhiro Yoshida, Yusuke Miyao, 
et al 2008c, January. Sharable type system design 
for tool inter-operability and combinatorial com-
parison. In Proceedings of the the First Internation-
al Conference on Global Interoperability for Lan-
guage Resources (ICGL), Hong Kong. 
Yoshinobu Kano, Ngan Nguyen, Rune S?tre, Kazuhi-
ro Yoshida, Keiichiro Fukamachi, Yusuke Miyao, 
et al 2008b, January. Towards Data And Goal 
Oriented Analysis: Tool Inter-Operability And 
Combinatorial Comparison. In Proceedings of the 
3rd International Joint Conference on Natural Lan-
guage Processing (IJCNLP), Hyderabad, India. 
Yoshinobu Kano, Ngan Nguyen, Rune S?tre, Kazuhi-
ro Yoshida, Yusuke Miyao, Yoshimasa Tsuruoka, 
et al 2008a, January. Filling the gaps between 
tools and users: a tool comparator, using protein-
protein interaction as an example. In Proceedings 
of the Pacific Symposium on Biocomputing (PSB), 
Hawaii, USA. 
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, 
Yuka Tateisi, and Nigel Collier. 2004. Introduction 
to the Bio-Entity Recognition Task at JNLPBA. In 
Proceedings of the International Workshop on Nat-
ural Language Processing in Biomedicine and its 
Applications (JNLPBA-04), Geneva, Switzerland. 
Kristofer Franzen, Gunnar Eriksson, Fredrik Olsson, 
Lars Asker, Per Liden, and Joakim Coster. 2002. 
Protein names and how to find them. International 
Journal of Medical Informatics, 67(1-3), 49-61. 
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and 
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the penn treebank. Com-
putational Linguistics, 19(2), 313-330. 
Yusuke Miyao, and Jun'ichi Tsujii. 2008. Feature 
Forest Models for Probabilistic HPSG Parsing. 
Computational Linguistics, 34(1), 35-80. 
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiro Matsubayashi, and To-
moko Ohta. 2007, April. AKANE System: Protein-
Protein Interaction Pairs in BioCreAtIvE2 Chal-
lenge, PPI-IPS subtask. In Proceedings of the 
Second BioCreative Challenge Evaluation Work-
shop. 
Burr Settles. 2005. ABNER: an open source tool for 
automatically tagging genes, proteins and other 
entity names in text. Bioinformatics, 21(14), 3191-
3192. 
Yuka Tateisi, Tomoko Ohta, Nigel Collier, Chikashi 
Nobata, and Jun'ichi Tsujii. 2000, August. Building 
an Annotated Corpus from Biology Research Pa-
pers. In Proceedings of the COLING 2000 Work-
shop on Semantic Annotation and Intelligent Con-
tent, Luxembourg. 
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and 
Jun'ichi Tsujii. 2005, October. Syntax Annotation 
for the GENIA Corpus. In Proceedings of the the 
Second International Joint Conference on Natural 
Language Processing (IJCNLP '05), Companion 
volume, Jeju Island, Korea. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin Dong Kim, 
Tomoko Ohta, J. McNaught, Sophia Ananiadou, et 
al. 2005. Developing a robust part-of-speech tag-
ger for biomedical text. In Advances in Informatics, 
Proceedings (Vol. 3746, 382-392). Berlin: Sprin-
ger-Verlag Berlin. 
 
30
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 49?58,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling Comma Placement in Chinese Text for Better Readability using
Linguistic Features and Gaze Information
Tadayoshi Hara1 Chen Chen2? Yoshinobu Kano3,1 Akiko Aizawa1
1National Institute of Informatics, Japan 2The University of Tokyo, Japan
3PRESTO, Japan Science and Technology Agency
{harasan, kano, aizawa}@nii.ac.jp
Abstract
Comma placements in Chinese text are
relatively arbitrary although there are
some syntactic guidelines for them. In this
research, we attempt to improve the read-
ability of text by optimizing comma place-
ments through integration of linguistic fea-
tures of text and gaze features of readers.
We design a comma predictor for gen-
eral Chinese text based on conditional ran-
dom field models with linguistic features.
After that, we build a rule-based filter for
categorizing commas in text according to
their contribution to readability based on
the analysis of gazes of people reading text
with and without commas.
The experimental results show that our
predictor reproduces the comma distribu-
tion in the Penn Chinese Treebank with
78.41 in F1-score and commas chosen by
our filter smoothen certain gaze behaviors.
1 Introduction
Chinese is an ideographic language, with no natu-
ral apparent word boundaries, little morphology,
and no case markers. Moreover, most Chinese
sentences are quite long. These features make it
especially difficult for Chinese learners to identify
composition of a word or a clause in a sentence.
Punctuation marks, especially commas, are al-
lowed to be placed relatively arbitrarily to serve as
important segmentation cues (Yue, 2006) for pro-
viding syntactic and prosodic boundaries in text;
commas indicate not only phrase or clause bound-
aries but also sentence segmentations, and they
capture some of the major aspects of a writer?s
prosodic intent (Chafe, 1988). The combination
of both aspects promotes cognition when reading
text (Ren and Yang, 2010; Walker et al, 2001).
?The Japan Research Institute, Ltd. (from April, 2013)
Linguistic FeaturesCRF modelCRF model-based Comma Predictor
Gaze FeaturesHuman AnnotationRule-based Comma Filter Text with/without Commas
Parse Tree
Treebank
Comma Distribution for Readability
Comma Distribution in General Text
Input (Comma-less) Text
+
+
Figure 1: Our approach
However, although there are guidelines and re-
search on the syntactic aspects of comma place-
ment, prosodic aspects have not been explored,
since they are more related with cognition. It is
as yet unclear how comma placement should be
optimized for reading, and it has thus far been up
to the writer (Huang and Chen, 2011).
In this research, we attempt to optimize comma
placements by integrating the linguistic features of
text and the gaze features of readers. Figure 1 il-
lustrates our approach. First, we design a comma
predictor for general Chinese text based on con-
ditional random field (CRF) models with various
linguistic features. Second, we build a rule-based
filter for classifying commas in text into ones fa-
cilitating or obstructing readability, by comparing
the gaze features of persons reading text with and
without commas. These two steps are connected
by applying our rule-based filter to commas pre-
dicted by our comma predictor. The experimental
results for each step validate our approach.
Related work is described in Section 2. The
functions of Chinese commas are described in
Section 3. Our CRF model-based comma predic-
tor is examined in Section 4, and our rule-based
comma filter is constructed and examined in Sec-
tion 5 and 6. Section 7 contains a summary and
outlines future directions of this research.
49
[Case 1] When a pause between a subject and a predicate is needed. (? (,) means the original or comparative position of the comma in Chinese text.)
e.g. ????????????????????????(The stars we can see (,)? are mostly fixed stars that are far away from the earth.)
[Case 2] When a pause between an inner predicate and an object of a sentence is needed.
e.g. ?????????????????????(We should see that (,) science needs a person to devote all his/her life to it.)
[Case 3] When a pause after an inner (adverbial, prepositional, etc.) modifier of a sentence is needed.
e.g. ?????????????(He is no stranger (,) to this city.) (The order of the modifier and the main clause is opposite in the English translation.)
[Case 4] When a pause between clauses in a complex sentence is needed, besides the use of semicolon (?).
e.g. ??????????????????????(It is said that there are more than 100 Suzhou traditional gardens, (,) no more than 10 of which I
have been to.)
[Case 5] When a pause between phrases of the same syntactic type is needed.
e.g. ??????????????? (The students prefer young (,) and energetic teachers.)
Table 1: Five main usages of commas in Chinese text
(a) Screenshot of a material
Display PC Monitor SubjectEye TrackerHost PC Monitor
(b) Scene of the experiment (c) Window around a gaze point
Figure 3: Settings for eye-tracking experiments
WS Word surface
POS POS tag
DIP Depth of a word in the parse tree
STAG Syntactic tag
OIC Order of the clause in a sentence that a word belongs to
WL Word length
LOD Length of fragment with specific depth in a parsing tree
Table 2: Features used in our CRF model
2 Related Work
Previous work on Chinese punctuation prediction
mostly focuses on sentence segmentation in au-
tomatic speech recognition (Shriberg et al, 2000;
Huang and Zweig, 2002; Peitz et al, 2011).
Jin et al (2002) classified commas for sentence
segmentation and succeeded in improving pars-
ing performance. Lu and Ng (2010) proposed
an approach built on a dynamic CRF for predict-
ing punctuations, sentence boundaries, and sen-
tence types of speech utterances without prosodic
cues. Zhang et al (2006) suggested that a cascade
CRF-based approach can deal with ancient Chi-
nese prose punctuation better than a single CRF.
Guo et al (2010) implemented a three-tier max-
imum entropy model incorporating linguistically
motivated features for generating commonly used
Chinese punctuation marks in unpunctuated sen-
tences output by a surface realizer.
(a)
WS|POS|STAG|DIP|OIC|WL|LOD|IOB-tag
(b)
Figure 2: Example of a parse tree (a) and its cor-
responding training data (b) with the features
3 Functions of Chinese Commas
There are five main uses of commas in Chinese
text, as shown in Table 1. Cases 1 to 4 are from
ZDIC.NET (2005), and Case 5 obviously exists in
Chinese text. The first three serve the function of
emphasis, while the latter two indicate coordinat-
ing or subordinating clauses or phrases.
In Cases 1 and 2, a comma is inserted as a
kind of pause between a short subject and a long
predicate, or between a short remainder predicate,
such as?? (see/know),??/?? (indicate),?
50
Feature F1 (P/R) A
WS 59.32 (72.67/50.12) 95.45
POS 32.51 (69.06/21.26) 94.08
DIP 34.14 (68.65/22.72) 94.13
STAG 22.44 (64.00/13.60) 93.67
OIC 9.27 (66.56/ 4.98) 93.42
WL 10.70 (75.24/ 5.76) 93.52
LOD 35.32 (59.20/25.17) 93.81
WS+POS 63.75 (79.93/53.01) 96.03
WS +DIP 70.06 (83.27/60.47) 96.61
WS +STAG 57.42 (81.94/44.19) 95.67
WS +OIC 60.35 (77.98/49.22) 95.73
WS +WL 60.90 (76.39/50.63) 95.71
WS +LOD 70.85 (78.87/64.31) 96.53
WS+POS+DIP 73.41 (84.62/64.82) 96.93
WS+POS+DIP+STAG 74.58 (83.66/67.27) 97.01
WS+POS+DIP +OIC 76.87 (84.29/70.65) 97.23
WS+POS+DIP +WL 70.18 (83.33/60.62) 96.63
WS+POS+DIP +LOD 76.61 (82.61/71.43) 97.16
WS+POS+DIP+STAG+OIC 76.62 (84.48/70.09) 97.21
WS+POS+DIP+STAG +WL 74.12 (84.00/66.33) 96.98
WS+POS+DIP+STAG +LOD 77.64 (85.11/71.38) 97.33
WS+POS+DIP +OIC+WL 75.43 (84.76/67.95) 97.11
WS+POS+DIP +OIC +LOD 78.23 (84.23/73.03) 97.36
WS+POS+DIP +WL+LOD 74.01 (85.80/65.06) 97.02
WS+POS+DIP+STAG+OIC+WL 77.25 (83.97/71.53) 97.26
WS+POS+DIP+STAG+OIC +LOD 77.31 (86.36/69.97) 97.33
WS+POS+DIP+STAG +WL+LOD 76.55 (85.24/69.46) 97.23
WS+POS+DIP +OIC+WL+LOD 77.60 (84.30/71.89) 97.30
WS+POS+DIP+STAG+OIC+WL+LOD 78.41 (83.97/73.54) 97.36
F1: F1-Score, P: precision (%), R: recall (%), A: accuracy (%)
Table 3: Performance of the comma predictor
(A) #Characters,
Article (B) #Punctuations, (C) / (A) (C) / (B) Subjects
ID (C) #Commas
6 692 49 28 4.04% 57.14% L, T, C
7 335 30 15 4.48% 50.00% L, T, C
10 346 18 7 2.02% 38.89% L, T, C, Z
12 221 18 7 3.17% 38.89% L, T, C
14 572 33 14 2.45% 42.42% L, T, C
18 471 36 13 2.76% 36.11% C, Z
79 655 53 28 4.27% 52.83% Z
82 471 30 13 2.76% 43.33% Z
121 629 41 19 3.02% 46.34% Z
294 608 50 24 3.95% 48.00% Z
401 567 43 21 3.70% 48.84% L, T, C
406 558 39 18 3.23% 46.15% Z
413 552 52 22 3.99% 42.31% T, C, Z
423 580 49 26 4.48% 53.06% L, C, Z
438 674 46 28 4.15% 60.87% Z
Average 528.73 39.13 18.87 3.57% 48.22% -
Table 4: Materials assigned to each subject
? (find) etc., and following long clause-style ob-
jects. English commas, on the other hand, sel-
dom have such usages (Zeng, 2006). In Cases 3
and 4, commas instead of conjunctions sometimes
connect two clauses in a relation of either coordi-
nation or subordination. English commas, on the
other hand, are only required between independent
clauses connected by conjunctions (Zeng, 2006).
Liu et al (2010) proved that Chinese commas
can change the syntactic structures of sentences
by playing lexical or syntactic roles. Ren and
Yang (2010) claimed that inserting commas as
clause boundaries shortens the fixation time in
post-comma regions. Meanwhile, in computa-
tional linguistics, Xue and Yang (2011) showed
Figure 4: Obtained eye-movement trace map
0
100,000
200,000
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10
ith commas Without commas
Tota
l vie
win
g 
time
 
(sec
.)
0
1 0
2 0
Figure 5: Total viewing time
that Chinese sentence segmentation can be viewed
as detecting loosely coordinated clauses separated
by commas.
4 CRF Model-based Comma Predictor
We first predict comma placements in existing
text. The prediction is formalized as a task to an-
notate each word in a word sequence with an IOB-
style tag such as I-Comma (following a comma),
B-Comma (preceding a comma) or O (neither I-
Comma nor B-Comma). We utilize a CRF model
for this sequential labeling (Lafferty et al, 2001).
4.1 CRF Model for Comma Prediction
A conditional probability assigned to a label se-
quence Y for a particular sequence of words X in
a first-order linear-chain CRF is given by:
P?(Y |X) =
exp(
?n
w
?k
i ?ifi(Yw?1, Yw, X,w))
Z0(X)
where w is a word position in X , fi is a binary
function describing a feature for Yw?1, Yw, X , and
w, ?i is a weight for that feature, and Z0 is a nor-
malization factor over all possible label sequences.
The weight ?i for each fi is learned on training
data. For fi, the linguistic features shown in Ta-
ble 2 are derived from a syntactic parse of a sen-
tence1. The first three were used initially; the rest
were added after we got feedback from construc-
tion of our rule-based filters (see Section 5). Fig-
ure 2 shows an example of a parsing tree and its
corresponding training data.
1Some other features or tag formats which worked well in
the previous research, such as bi-/tri-gram, a preceding word
(L-1) or its POS (POS-1), and IO-style tag (Leaman and Gon-
zalez, 2008) were also examined, but they did not work that
well, probably because of the difference in task settings.
51
0
1,000
2,000
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10Fixa
tion
 
time
 
/
com
ma
 
(sec
.)
0.0
1.0
2.0 ith commas Without commas
Figure 6: Fixation time per comma
0.01.0
2.03.0
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10#reg
res
sion
s /
com
ma 0
ith co mas Without commas
12
3
Figure 7: Number of regressions per comma
4.2 Experimental Settings
The Penn Chinese Treebank (CTB) 7.0 (Nai-
wen Xue and Palmer, 2005) consists of 2,448
articles in five genres. It contains 1,196,329
words, and all sentences are annotated with parse
trees. We selected four genres for written Chi-
nese (newswire, news magazine, broadcast news
and newsgroups/weblogs) from this corpus as our
dataset. These were randomly divided into train-
ing (90%) and test data (10%). We also corrected
errors in tagging and inconsistencies in the dataset,
mainly by solving problems around strange char-
acters tagged as PU (punctuation). The commas
and characters after this preprocessing numbered
63,571 and 1,533,928 in the training data and
4,116 and 111,172 in the test data.
MALLET (McCallum, 2002) and its applica-
tion ABNER (Settles, 2005) were used to train the
CRF model. We evaluated the results in terms
of precision (P = tp/(tp + fp)), recall (R =
tp/(tp+fn)), F1-score (F1 = 2PR/(P+R)), and
accuracy (A = (tp + tn)/(tp + tn + fp + fn)),
where tp, tn, fp and fn are respectively the num-
ber of true positives, true negatives, false positives
and false negatives, based on whether the model
and the corpus provided commas at each location.
4.3 Performance of the CRF Model
Table 3 shows the performance of our CRF
model2. We can see that WS contributed much
more to the performance than other features, prob-
ably because a word surface itself has a lot of
information on both prosodic and syntactic func-
tions. Combining WS with other features greatly
improved performance, and as a result, with all
2Precision, recall, F1-score, and accuracy with WS + POS
+ DIP + L-1 + POS-1 were 82.96%, 65.04%, 72.91 and
96.84%, respectively (lower than those with WS+POS+DIP).
4080
120160
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10Sac
cad
e le
ngth
(1) / 
com
ma
ith co mas Without commas
4080
120160
(pixel)
Figure 8: Saccade length (1) per comma
30
60
90
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Commaith co mas Without commas
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z1030Sac
cad
e le
ngth
(2) / 
com
ma 60
90(pixel)
Figure 9: Saccade length (2) per comma
features (WS + POS + STAG + DIP + OIC + LOD
+ WL), precision, recall, F1-score and accuracy
were 83.97%, 73.54%, 78.41 and 97.36%.
We also found that a large number of false pos-
itives seemed helpful according to native speakers
(see the description of the subjects in Section 5 and
6). Although these commas do not appear in the
CTB text, they might smoothen the reading expe-
rience. We constructed a rule-based filter in order
to pick out such commas.
5 Rule-based Comma Filter
We constructed a rule-based comma filter for clas-
sifying commas in text into ones facilitating (pos-
itive) or obstructing (negative) the reading process
as follows:
[Step 1]: Collect gaze data from persons reading
text with or without commas (Section 5.1).
[Step 2]: Compare gaze features around commas
to find those features that reflect the effect of
comma placement. (Section 5.2).
[Step 3]: Annotate commas with categories based
on the obtained features (Section 5.3), and devise
rules to explain the annotation (Section 5.4).
5.1 Collecting Human Eye-movement Data
Eye-movements during reading contain rich infor-
mation on how the document is being read, what
the reader is interested in, where difficulties hap-
pen, etc. The movements are characterized by fix-
ations (short periods of steadiness), saccades (fast
movements), and regressions (backward saccades)
(Rayner, 1998). In order to analyze the effect of
commas on reading through the features, we col-
lected gaze data from subjects reading text in the
following settings.
[Subjects and Materials] Four native Man-
52
Categories Effect on readability Outward manifestation
Positive (?) Can improve readability. Presence would cause GF+.
Semi-positive (?) Might be necessary for readability, but the importance is not as obvious as a positive comma. Absence might cause GF-.
Semi-negative (2) Might be negative, but its severity is not as obvious as a negative comma. Absence might cause GF+.
Negative (?) Thought to reduce a document?s readability. Presence would cause GF-.
GF+/GF-: values of eye-tracking features that represent good/poor readability
Table 5: Comma categories
Subject Positive (?) Semi-positive (?) Semi-negative (2) Negative (?) Adjustment formula
L ?FT?>800 500<?FT??800 -100<?FT??500 ?FT?<-100 ?FT? = ?FT ? ?RT ? 200
C ?FT?>900 600<?FT??900 -200<?FT??600 ?FT?<-200 ?FT? = ?FT ? ?RT ? 275
T ?FT?>600 300<?FT??600 -300<?FT??300 ?FT?<-300 ?FT? = ?FT ? ?RT ? 250
Z ?FT?>650 350<?FT??650 -250<?FT??350 ?FT?<-250 ?FT? = ?FT ? ?RT ? 250
?FT = [ fixation time (without commas) [ms]]? [ fixation time (with commas) [ms]]
?RT = [ #regressions (without commas) ]? [ #regressions (with commas) ]
Table 6: Estimation formula for judging the contribution of commas to readability
ID ? ? 2 ?
6 13 6 4 5
7 8 6 1 0
10 5 0 1 1
12 1 4 2 0
14 4 4 5 1
18 5 1 4 3
79 11 4 9 4
82 5 6 2 0
ID ? ? 2 ?
121 11 2 6 0
294 9 9 4 1
401 10 7 2 2
406 5 6 5 2
413 8 5 6 3
423 11 4 7 4
438 6 16 6 0
Total 112 80 64 26
Table 7: Categories of annotated commas
darin Chinese speakers (graduate students and re-
searchers) read 15 newswire articles selected from
CTB 7.0 (included in the test data in Section 4.2).
Table 4 and Figure 3(a) show the materials as-
signed to each subject and a screenshot of one ma-
terial. Each article was presented in 12-15 points
of bold-faced Fang-Song font occupying 13?13,
14?15, 15?16 or 16?16 pixels along with a line
spacing of 5-10 pixels3.
[Apparatus] Figure 3(b) shows a scene of the
experiment. An EyeLink 1000 eye tracker (SR
Research Ltd., Toronto, Canada) with a desktop
mount monitored the movements of a right eye at
1,000 Hz. The subject?s head was supported at the
chin and forehead. The distance between the eyes
and the monitor was around 55 cm, and each Chi-
nese character subtended a visual angle 1?. Text
was presented on a 19? monitor at a resolution
of 800?600 pixels, with the brightness adjusted
to a comfortable level. The displayed article was
masked except for the area around a gaze point
(see Figure 3(c)) in order to confirm that the gaze
point was correctly detected and make the subject
concentrate on the area (adjusted for him/her).
[Procedure] Each article was presented twice
(once with/once without commas) to each subject.
3These values, as well as the screen position of the article,
were adjusted for each subject.
The one without commas was presented first4 (not
necessarily in a row). We did not give any compre-
hension test after reading; we just asked the sub-
jects to read carefully and silently at their normal
or lower speed, in order to minimize the effect of
the first reading on the second. The subjects were
informed of the presence or absence of commas
beforehand. The apparatus was calibrated before
the experiment and between trials. The experi-
ment lasted around two hours for each subject.
[Alignment of eye-tracking data to text] Figure 4
shows an example of the obtained eye-movement
trace map, where circles and lines respectively
mean fixation points and saccades, and color depth
shows their duration. The alignment of the data to
the text is a critical task, and although automatic
approaches have been proposed (Mart??nez-Go?mez
et al, 2012a; Mart??nez-Go?mez et al, 2012b), they
do not seem robust enough for our purpose. Ac-
cordingly, we here just compared the entire layout
of the gaze point distribution and that of the actual
text, and adjusted them to have relatively coherent
positions on the x-axis; i.e., the beginning and end
of the gaze point sequence in a line were made as
close as possible to those of the line in the text.
5.2 Analysis of Eye-movement Data
The gaze data were analyzed by focusing on re-
gions around each comma or where each one
should be (three characters left and right to the
comma5).
4If we had used the reversed order, the subject would have
knowledge about original comma distribution, and this would
cause abnormally quick reading of the text without commas.
With the order we set, conflicts between false segmentations
(made in first reading) and correct ones might bother the sub-
ject, which is trade-off (though minor) in the second reading.
5When a comma appeared at the beginning of a line, two
characters to the left and right of the comma and one charac-
53
1. If L Seg and R Seg are both very long, a comma must be put between them.
2. If two ? appear serially, one is necessary whereas the other might be optional or judged negative, but it still depends on the lengths of the siblings.
3. If two neighboring commas appear very close to each other, one of them is judged as negative whereas judgment on the other one is reserved.
4. If several (more than 2) ?s appear continually, one or more ?s might be reserved in consideration of the global condition.
5. A comma is always needed after a long sentence or clause without any syntactically significant punctuation with the function of segmentation.
6. If a ? appears near a ?, it might be judged as negative with a high probability. However, the judgment process is always from the bottom up, which
means ? ? 2? ???. For example, if a 2 appears near a ?, we judge 2 first (to be positive or negative), then judge the ? in the condition
with or without the comma of 2.
Table 8: General rules for reference
Figure 5, 6 and 7 respectively show the total
viewing time, fixation time (duration for all fix-
ations and saccades in a target region) per comma,
and number of regressions per comma6 for each
trial. We can see a general trend wherein the for-
mer two were shorter and the latter was smaller for
the articles with commas than without. The diver-
sity of the subjects was also observed in Figure 6.
Figure 8 and 9 show the saccade length per
comma for different measures. The former (lat-
ter) figure considers a saccade in which at least
one edge (both edges) was in the region. We can-
not see any global trend, probably because of the
difference in global layout of materials brought by
the presence or absence of commas.
5.3 Categorization of Commas
Using the features shown to be effective to repre-
sent the effect of comma placement, we analyzed
the statistics for each comma in order to manu-
ally construct an estimation formula for judging
the contribution of each comma to readability. The
contribution was classified into four categories
(Table 5), and the formula is described in Table 67.
The adjustment formula was based on our obser-
vation that the number of regressions could only
be regarded as an aid. For example, for subject
C, if ?FT=200ms and ?RT =?2, ?FT?=?350,
and therefore, the comma is annotated as negative.
All parameters were decided empirically and man-
ually checked twice (self-judgment and feedback
from the subjects).
On the basis of this estimation formula, all arti-
cles in Table 4 were manually annotated. Table 7
shows the distribution of the assigned categories8.
ter to the left and right of the final character of the last line
were analyzed.
6Calculated by counting the instances where the x-
position of [a fixation / end point of a saccade ] was ahead
of [the former fixation / its start point]. Although the counts
of these two types were almost the same, by counting both of
them, we expected to cover any possible regression.
7One or two features are used to judge the category of a
comma. We will explore more features in the future.
8In the case of severe contradictions, the annotators dis-
cussed them and resolved them by voting.
5.4 Implementation of Rule-based Filter
The annotated commas were classified into Cases
1 to 5 in Table 1, based on the types of left and
right segment conjuncts (L Seg and R Seg, which
were obtained from the parse trees in CTB). For
each of the five cases, the reason for the assign-
ment of a category (?, ?, 2 or ?) to each
comma was explained by a manually constructed
rule which utilized information about L Seg and
R Seg. The rules were constructed so that they
would cover as many instances as possible. Ta-
ble 8 shows the general rules utilized as a refer-
ence, and Table 9 shows the finally obtained rules.
The rightmost column in this table shows the num-
ber of commas matching each rule. These rules
were then implemented as a filter for classifying
commas in a given text.
For several rules (?10, 28, 210, 211 and
212), there were only single instances. In addi-
tion, although our rules were built carefully, a few
exceptions to the detailed threshold were found.
Collecting and investigating more gaze data would
help to make our rules more sophisticated.
6 Performance of the Rule-based Filter
We assumed that our comma predictor provides a
CTB text with the same distribution as the origi-
nal one in CTB (see Figure 1). Accordingly, we
examined the quality of the comma categorization
by our rule-based filter through gaze experiments.
6.1 Experimental Settings
Another five native Mandarin Chinese speakers
were invited as test subjects. The CTB articles as-
signed to the subjects are listed in Table 10. These
articles were selected from the test data in Sec-
tion 4.2 in such a way that 520<#characters<700,
#commas>17, #commas/#punctuations>38%,
and #commas/#characters>3.1%, since we
needed articles of appropriate length with a fair
number of commas. After that, we manually
chose articles that seemed to attract the subjects?
interest from those that satisfied the conditions.
54
Case 1: L Subject + R Predicate #commas
?6 L IP-SBJ + R VP (length both<14 (In Seg Len)) 2
?7 L IP-SBJ/NP-SBJ (Org Len>13, Ttl Len>15) 7
?6 L NP-SBJ/IP-SBJ (<14) + R VP (?25) 2
Case 2: L Predicate + R Object #commas
?9 Long frontings (Modifier/Subject, >7) + short L predicate (VV/VRD/VSB? ? ? , ?3) + Longer R object (IP-OBJ, >28) 6
?8 Short frontings (<5) + short L predicate (<3) + moderate-length R object (IP-SBJ, <20) 4
26 Short frontings (<6) + short L predicate (?3) + long R object (IP-SBJ, >23) 9
Case 3: L Modifier #commas
?3 Short frequently used L modifier (2-3,??,??, etc.) + moderate-length/long R SPO (?w18p10) 13
?7 Short L (PP/LCP)-TMP (5, 6) + long R NP (?10) 4
?10 Long L CP-CND (e.g.,??, >18) + moderate-length R Seg (SPO, IP, etc. <18) 1
?1 Long L modifier (PP(-XXX, P+Long NP/IP), IP-ADV, ?17) 6
?4 Moderate-length/short L modifier (PP(-XXX, P+IP, There is IP inside, >6<15, cf. 26 (NP)) 9
?9 Long L (PP/LCP)-TMP (Ttl Len?10), short R Seg (NP/ADVP, <3) 4
?10 Short L (LCP/PP)-LOC (<8) 2
22 Long L LOC (or there is LCP inside PP, >10) 5
23 Very short frequently used L ADVP/ADV (2) 8
25 Short L (PP/LCP/NP)-TMP (4;5-6, when R Seg is short (<10)) 12
24 Moderate-length PP(-XXX, P+NP, >8 ?13) + R Seg (SPO, IP, VO, MSPO, etc.) 6
28 Short L IP-CND (<8) 1
211 Long L PP-DIR (>20) + short R VO (?10) 1
?2 Very short L (QP/NP/LCP)-TMP (?3) 8
?5 Short frequently used L modifier (as in ?3, ?3) + short/moderate-length R Seg (SPO etc., <c20w9) 1
Case 4: L c + R c #commas
?2 L c & R c are both long (In Seg Len?15; or one>13, the other near 20) 39
?8 L c is the summary of R c 2
?2 Moderate-length L c + R c (both ?10?15; or one?17, the other?12) 25
?3 Moderate-length clause (>10), but connected with familiar CC or ADVP 6
?5 Three or more consecutive moderate-length clauses (all<15, and at least one ?10) 12
?7 Very short L c + R c (both <5), something like slogan) 1
Case 5: L p + R p #commas
?1 Short coordinate modifiers (Both side <5) 4
?4 Short L p+R p (both<c15w5, and at least one <10), but pre-L p (e.g., SBJ) is too long (>18) 2
?5 Between two moderate-length/long phrases (both ?15; or L p?17, R p=10-14; Or L p=10-14, R p>20) 39
?11 Long pre-L p (SBJ /ADV, etc. >16) + short L p (?5) + long R p (?18) 2
(?3 Moderate-length phrase (>10), but connected with familiar CC or ADVP) (6)
?6 Three or more consecutive short/moderate-length phrases (both<15, at least one<8) 5
21 Between short phrases (both ?c13w5), and pre-L p (SBJ/ADV, etc.) is short/moderate-length (<11) 13
27 Coordinate VPs, and L VP is a moderate-length VP (PP-MNR VP) 4
29 Phrasal coordination between a long (?18) and a short (<10) phrase 3
210 Moderate-length coordinate VPs (>10<15), and R VP has the structure like VP (MSP VP) 1
212 Between two short/moderate-length NP phrases (both ?15, e.g., L NP-TPC+R NP-SBJ) 1
?1 Moderate-length/short phrase ((i) c:one>10<18, The other >5?10, w:one?5, the other>5?10; (ii) c:both?10<15, 13
w:both>5?7), and pre-L p (SBJ/ADV, etc.) is short (?5)
? L x/R x: the left/right segment of a target comma which is x.
(x can be ?p? (phrase) / ?c? (clause), syntactic tags (with function tags) such as ?VP? and ?IP-SBJ?, or general functions such as ?Subject? and ?Predicate?.)
? Org Len: the number of characters in a segment (including other commas or punctuation inside).
? In Seg Len/Ttl Len: the number of characters between the comma and nearest punctuation (inside a long/outside a short target segment).
? SPO: subject + predicate + object, belonging to the outermost sentence. The length is defined in the similar way as In Seg Len.
? MSPO: modifier + subject + predicate + object. The length is defined in the similar way as In Seg Len.
? -XX or -XXX: arbitrary type of possible functional tag (or without any functional tag) connected with the former syntactic tag.
? ?ciwj: #characters?i and #words?j.
? In some cases (in Case 3, 4 and 5), the length is calculated after negative (or judged negative) commas are eliminated.
? The rules related with TMP are applied faster than ones related with LCP (in Case 3).
? ?3 appears in both Case 4 (clause) and Case 5 (phrase). The number of commas is given by the sum of those in both cases.
Table 9: Entire classification of rules based on traditional comma categories
(A) #Characters,
Article (B) #Punctuations, (C) / (A) (C) / (B) Subjects
ID (C) #Commas
6 692 49 28 4.04% 57.14% L, S, H
11 672 48 21 3.13% 43.75% L, S, F
15 674 67 26 3.86% 38.81% L, S, H
16 547 43 22 4.02% 51.16% L, S, F
56 524 43 18 3.44% 41.86% L, H, M
73 595 46 28 4.71% 60.87% S, H, F, M
79 655 53 28 4.27% 52.83% H, F, M
99 671 55 24 3.58% 43.64% F, M
Average 628.75 50.50 24.38 3.88% 48.27% -
Table 10: Materials assigned to each subject
Our rule-based filter was applied to the commas
of each article9, and the commas were classified
9Instances of incoherence among the applied rules were
0
40,000
80,000
120,000
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)Positive distribution Negative distribution
040
8012
Tot
al v
iew
ing
tim
e (se
c.)
Figure 10: Total viewing time for two distributions
into two distributions: a positive one (positive +
semi-positive commas) and a negative one (nega-
tive + semi-negative commas). Two types of ma-
terials were thus generated by leaving the commas
in one distribution and removing the others.
manually checked and corrected.
55
20
40
60
80
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
FFT
(10
0) Positive distribution Negative distribution
Figure 11: EMFFT for two distributions
46
810
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
FT
(80
0) Positive distribution Negative distribution
Figure 12: EMFT for two distributions
The apparatus and procedure were almost the
same as those in Section 5.1, whereas, on the ba-
sis of the feedback from the previous experiments,
the font size, number of characters in a line, and
line spacing were fixed to single optimized values,
respectively, 14-point Fang-Song font occupying
15?16 pixels, 33 characters and 7 pixels.
6.2 Evaluation Metrics
We examined whether our positive/negative distri-
butions really facilitated/obstructed the subjects?
reading process by using the following metrics:
TT, EMFFT = FFTFT
10
, EMFT = FTCN?TT
11
,
EMRT = RT2?CN
12
, EMSLO = SLO2?TT ,
where TT, FT, RT and CN are total viewing time,
fixation time, number of regressions, and num-
ber of commas respectively, as described in Sec-
tion 5.2. FFT and SLO are additionally introduced
metrics respectively for the ?total duration for all
first-pass fixations in a target region that exclude
any regressions? and for the ?length of saccades
from inside a target region to the outside?13. All of
the areas around commas appearing in the original
article were considered target areas for the metrics.
The other settings were the same as in Section 5.
6.3 Contribution of Categorized Commas
Figure 10, 11, 12, 13 and 14 respectively show TT,
EMFFT , EMFT , EMRT and EMSLO for two types
of comma distributions in each trial.
10Ratio to the total fixation time in the target areas (FT).
11Normalized by the total viewing time (TT).
12Two types of RT count (see Section 5.2) were averaged.
13Respectively to reflect ?the early-stage processing of the
region? and ?the information processed for a fixation and a
decision of the next fixation point? (Hirotani et al, 2006).
0
5
10
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
RT
(1
0) Positive distribution Nega ive distribution
Figure 13: EMRT for two distributions
0
5
10
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)Positive distribution Negative distribution
EM
SLO
(10
0)
Figure 14: EMSLO for two distributions
For TT, we cannot see any general trend, mainly
because this time, the reading order of the text
was random, which spread out the second reading
effect evenly between the two distributions. For
EMFFT , we cannot reach a conclusion either. In
contrast, in more than half of the trials, EMFFT
was larger for positive distributions, which would
imply that the positive commas helped to prevent
the reader?s gaze from revisiting the target regions.
For most trials, except for subject S whose cal-
ibration was poor and reading process was poor
in M56, EMFT and EMRT decreased and EMSLO
increased for positive distributions, which implies
that the positive commas smoothed the reading
process around the target regions.
7 Conclusion
We proposed an approach for modeling comma
placement in Chinese text for smoothing reading.
In our approach, commas are added to the text on
the basis of a CRF model-based comma predic-
tor trained on the treebank, and a rule-based filter
then classifies the commas into ones facilitating or
obstructing reading. The experimental results on
each part of this approach were encouraging.
In our future work, we would like see how com-
mas affect reading by using much more material,
and thereby refine our framework in order to bring
a better reading experience to readers.
Acknowledgments
This research was partially supported by Kakenhi,
MEXT Japan [23650076] and JST PRESTO.
56
References
Wallace Chafe. 1988. Punctuation and the prosody of
written language. Written Communication, 5:396?
426.
Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2010. A linguistically inspired statistical model
for Chinese punctuation generation. ACM Trans-
actions on Asian Language Information Processing,
9(2):6:1?6:27, June.
Masako Hirotani, Lyn Frazier, and Keith Rayner. 2006.
Punctuation and intonation effects on clause and
sentence wrap-up: Evidence from eye movements.
Journal of Memory and Language, 54(3):425?443.
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Pause and
stop labeling for Chinese sentence boundary detec-
tion. In Proceedings of Recent Advances in Natural
Language Processing, pages 146?153.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proceedings of the International Conference on
Spoken Language Processing, pages 917?920.
Mei xun Jin, Mi-Young Kim, Dongil Kim, and Jong-
Hyeok Lee. 2002. Segmentation of Chinese
long sentences using commas. In Proceedings of
the Third SIGHAN Workshop on Chinese Language
Processing, pages 1?8.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survery of advances in biomed-
ical named entity recognition. In Pacific Symposium
on Biocomputing (PSB?08), pages 652?663.
Baolin Liu, Zhongning Wang, and Zhixing Jin. 2010.
The effects of punctuations in Chinese sentence
comprehension: An erp study. Journal of Neurolin-
guistics, 23(1):66?68.
Wei Lu and Hwee Tou Ng. 2010. Better punctuation
prediction with dynamic conditional random fields.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
?10), pages 177?186.
Pascual Mart??nez-Go?mez, Chen Chen, Tadayoshi Hara,
Yoshinobu Kano, and Akiko Aizawa. 2012a. Image
registration for text-gaze alignment. In Proceedings
of the 2012 ACM international conference on Intel-
ligent User Interfaces (IUI ?12), pages 257?260.
Pascual Mart??nez-Go?mez, Tadayoshi Hara, Chen
Chen, Kyohei Tomita, Yoshinobu Kano, and Akiko
Aizawa. 2012b. Synthesizing image representa-
tions of linguistic and topological features for pre-
dicting areas of attention. In Patricia Anthony, Mit-
suru Ishizuka, and Dickson Lukose, editors, PRICAI
2012: Trends in Artificial Intelligence, pages 312?
323. Springer.
Andrew Kachites McCallum. 2002. MALLET: A ma-
chine learning for language toolkit.
Fu-dong Chiou Naiwen Xue, Fei Xia and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Stephan Peitz, Markus Freitag, Arne Mauser, and Her-
mann Ney. 2011. Modeling punctuation prediction
as machine translation. In Proceedings of Interna-
tional Workshop on Spoken Language Translation,
pages 238?245.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124(3):372?422.
Gui-Qin Ren and Yufang Yang. 2010. Syntac-
tic boundaries and comma placement during silent
reading of Chinese text: evidence from eye move-
ments. Journal of Research in Reading, 33(2):168?
177.
Burr Settles. 2005. ABNER: an open source tool
for automatically tagging genes, proteins, and other
entity names in text. Bioinformatics, 21(14):3191?
3192.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-
Tu?r, and Go?khan Tu?r. 2000. Prosody-based au-
tomatic segmentation of speech into sentences and
topics. Speech Communication, 32(1-2):127?154.
Judy Perkins Walker, Kirk Fongemie, and Tracy
Daigle. 2001. Prosodic facilitation in the resolu-
tion of syntactic ambiguities in subjects with left
and right hemisphere damage. Brain and Language,
78(2):169?196.
Nianwen Xue and Yaqin Yang. 2011. Chinese sen-
tence segmentation as comma classification. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics:shortpapers,
pages 631?635.
Ming Yue. 2006. Discursive usage of six Chinese
punctuation marks. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages
43?48.
ZDIC.NET. 2005. Commonly used Chinese punctua-
tion usage short list. Long Wiki, Retrieved Dec 10,
2012, from http://www.zdic.net/appendix/f3.htm.
(in Chinese).
X. Y. Zeng. 2006. The comparison and the use
of English and Chinese comma. College English,
3(2):62?65. (in Chinese).
57
Kaixu Zhang, Yunqing Xia, and Hang Yu. 2006.
CRF-based approach to sentence segmentation and
punctuation for ancient Chinese prose. Jour-
nal of Tsinghua Univ (Science and Technology),
49(10):1733?1736. (in Chinese).
58
