I n te ract ion  Grammars  
Guy Perrier 
LORIA, Universitd Nancy2 
BP 239 
54506 Vandceuvre-lhs-Nancy Cedex France 
e-marl: perrier@loria.fl' 
Abstract  
Interaction Grammars (IG) are a new linguistic for- 
malism which is based on descriptions of under~ 
specified trees in the fl'amework of intuitionistic lin- 
ear logic (ILL). Syntactic omposition, which is ex- 
pressed by deduction in linear logic, is controlled by 
a system of polarized features. In this way, parsing 
amounts to generating models of tree descriptions 
and it is implemented as a constraint satisfaction 
problem. 
Introduct ion 
IG can be presented as an attempt o bring together 
flmdalnental ideas, some coming froln Tree Adjoin- 
ing Grammars (TAG) and others from Categorial 
GraInmars (CG), in order to overcome the specific 
limitations of each of these formalisms. The compu- 
tational and linguistic relevance of TAG lies in its 
adjunction operation (Joshi et al, 1975; Kroch and 
Joshi, 1985) but the simplicity of its mechanism has 
a counterpart in the inflation of the lexicons that arc 
required for expressing all grammatical phenomena 
of a language. Every time that a word is used in a 
new syntactic ontext, wlfieh can differ only by word 
order for example, a new elementary tree, which en- 
codes this context, must be added to the lexicon in 
a direct manner or by means of a lexical rule. In this 
way, lexicons quickly become colossal, very awkward 
to use and very hard to maintain. 
Recent works aim to solve tiffs problem by factoriz- 
ing linguistic information with the notions of nnder- 
specified trees and tree descriptions. These notions 
were introduced for TAG by Vijay-Shanker with the 
motivation of making adjunetion monotone (Vijay- 
Shanker, 1992). Now, they are exploited fruitfully 
in various directions: for structuring TAG lexicons 
in hierarchical systems of modules (Candito, 1999) 
or for expressing semantic ambiguity (Muskens and 
Krahmer, 1998; Egg et al, 1998) for instance. At the 
same time, these notions are a way of relaxing the 
primitive adjunetion operation in order to capture 
linguistic phenomena: such a possibility is exploited 
by (Rambow et 31., 1995) within D-Tree Grammars 
and by (Kalhneyer, 1999) within Tree Description 
~l'alnlilars. 
Unfortunately, tim counterpart of a more flexible 
framework is often over-generation a d a loss of com- 
putational efficiency in the absence of principles for 
controlling syntactic composition. By looking at 
CG, we can find some answers to this preoccupa- 
tion: a fundamental idea of CG is that grammati- 
cal constituents are viewed as consumable resources 
(Retord, 2000); these resources are divided into pos- 
itive and negative resources which are complemen- 
tary and search to neutralize themselves mutually. 
The core of CG is the Lambek Calculus (Lmnbek, 
1958): by combining resource sensitivity and order 
sensitivity, this logic is a good candidate for repre- 
senting the syntax of natural languages but at the 
same time, this combination entails a rigidity which 
limits its expressive power greatly. An appropriate 
way of relaxing this rigidity constitutes an impor- 
tant research area in CG (Moortgart, 1996). 
The principle of IG is to combine the powerflfl notion 
of under-specified tree description linked to the TAG 
ptfilosophy with control of syntactic omposition by 
a system of polarities in accordance with the CG 
philosot)hy. More precisely, the basic objects of IG 
are syntactic dcscriptions which express dependen- 
cies between syntactic onstituents in the shape of 
under-specified trees. Word order is referred to the 
same level as morphological information, which is 
represented by a system of features which are linked 
to the nodes of syntactic descriptions. Whereas a 
feature is usually a pair (attribute, value), an IG 
feature is a triplet (attribute, polarity, value) where 
polarity can take one of the three values -1, 0 or 
-t-1 and behaves like an electrostatic charge: for in- 
stance, a noun phrase which is waiting to receive 
a syntactic function in a sentence, carries a nega- 
tive feature of type fltnct while a finite verb which 
is searching for its subject, carries a positive t'ea- 
ture funct with value s@j. Attraction between these 
dual features will make possible the fact that the 
verb finds its subject and, simultaneously, the noun 
phrase finds its flmction in the sentence. (Muskens 
and Krahmer, 1998) also recognized the necessity of 
introducing the notion of polarity in tree descrip- 
600 
tions as a mechanism for controlling syntactic om- 
position; the difference with respect to IG lies in 
the granularity of the polarization wlfich is finer in 
li(l: in their proposal, the polarized objects are con- 
stituents, that is description odes, whereas in IG 
one constituent can include several features with op- 
posite polarities. 
Tile frmnework which is chosen tbr tel)resenting syn- 
tactic descriptions in this patmr is that of linear logic 
(Girard, 1987), more precisely a fragment of ILL 
(Lincoln, 1992). The resource sensitivity of linear 
logic allows one to express the fact that 1)olarized 
features behave as consumable resources in \[G: a 
positive feature has to find its dual fea.ture once and 
only once. If we try to use classical or intuitionistic 
logic for modelling IG, the contraction and weaken- 
ing rules, which are inherent in these logics, entail 
a loss of resource-sensitivity: tbr instance, a verb 
could take two subjects by appealing to the con- 
traction rule and some noun phrases wouhl not need 
to find their syntactic role in a sentence by appeal- 
ing to the weakening rule. By discarding these two 
rules, linear logic provides a Kamework that exactly 
corresponds to the "electrostati(-" laws that control 
1)olarized features. 
In this framework, i)arsing takes the shatm of log- 
ical deduction of closed syntactic dcscriptions from 
airy syntactic descriptions: a description is said to 
be. closed when it represents a completely specified 
syntactic tree where all features are neutralized. 
If linear logic provides an elegant Yamework tbr rep- 
resenting IG, it gives no method for parsing effi- 
ciently and avoiding the coufl)inatory explosion that 
may follow from the flexibility of the fi)rmalism. An 
ai~swer to this problem is given by the paradigm of 
constraint solving. Parsing a phrase can be regarded 
as generating models of the partial descrit)tion which 
is provided by a l('xicon for the words of this phrase. 
The process is monotone and can be expressed as 
a constraint satisfactio'n problem. This constraint- 
based approach was inspired by the work of (l)uchier 
and C., 1999; l)uchier and Thater, 1999) on domi- 
nance constraints. (Blache, 1999) shows the advan- 
tages of such an apI/roaeh t)oth from a linguistic and 
emnputational viewpoint with the formalism that lie 
prol)oses mid lie calls Property Grammars. 
1 Syntact i c  descr ip t ions  as l inear  
log ic  fo rmulas  
IG arc formally defined as an ILL theory. Basic 
objects are syntactic dcscriptions which arc repre- 
sented by linear logic formulas in the following form: 
Descr ::= Domin \] Feat \] Descr ? Descr \] 
Descr & Descr 
If a syntactic descrit)tion concerns the dominance 
relation between syntactic constituents, it has the 
type Domin; if it concerlm the features which are 
used for characterizing syntactic or semantic prol)- 
crties of constituents, it has the |yt)e Feat. Finally, 
a description call I)e built recursively fl'om two de- 
scriptions in two ways, which a.re expressed by tile 
two linear logic conjunctions: the multiplicative ten- 
sor (?) and the additive with (&). 
1.1 Multipl ieative and additive conjunction 
of resources in descriptions 
A description D1 ? D2 requires all resources of both 
descriptions D1 and D2 while a description DI&D2 
requires either the resources of DI or the resources 
of D., lint not 1)oth. This use of the two linear logic 
conjunctions is consistent with their left introduc- 
tion rules in the linear sequent calculus: 
F1,F%FPG FI,F t -a  ~ F2 , r t -a  &L2 
F1?lS,Ft -G ?L Flg:F2,Fk'G &el FI~F.2, F\[-G 
In this way, it; is possible to describe all syntactic 
configurations of a word with a single lexical entry 
raider the form of a syntactic description: conunon 
parts of these COtl\[iglll'al;iolls are factorized whereas 
Slmcilic parts are distributed into alternations linked 
together with the comlective with. For instance, a 
possible lexical entry for the tinite verb volt in French 
has the shal)e Dvoit = D1 ? (D2&D3) ? (D4&;DS): 
Dj contains information related to the subject which 
is coiilnloll to all uses of the verb volt; D2 expresses 
the canonical order subject-verb in the sentence that 
is headed by the vert) voit whereas Da expresses the 
reverse order for which the subject must be realized 
under some conditions, such as in the phrase Marie 
que volt ,lean; D4 exl)resses that the verb has an 
exl)licit object whereas D,5 corresl)on(ts to circum- 
stances where this object is not present, such as ill 
the sentence ,Ican volt. 
1.2 Under-specif ication of dominance 
between const ituents 
A description of type Domin has the fl)llowing form: 
Domin ::= Node > \[LNode.sl \[ Node > Node I 
Node >* Node 
LNodes ::= e I Node LNodes 
A predicate N > \[N1, . . . ,Np\]  states that the 
constituent N is decoml)osed into the sub- 
constituents N1 , . . . ,N  v. The order between these 
sub-constituents i  only used tbr identifying each 
one without any linguistic lneaning; word order 
is dealt with at the same level as morphological 
information by means of features. 
A predicate N1 > N2 expresses that N2 is all 
immediate sub-constituent of N1. Such a predicate 
is used when only partial information on tile 
sub-constituents of a phrase is available. 
A predicate N1 >* N2 expresses that N2 is embed- 
ded in N1 at an undetermined del)th. For instance, 
if we continue with description D1 related to the 
verb volt, we can assume that it contains the formula 
601 
(Na > \[N4, N~\]) ? (N4 >* No) which is interpreted 
as follows: the verb phrase Na is constituted of 
tile verb N4 and its object N~; Na represents the 
bare verb whereas N4 represents the verb which 
has been possibly modified by a clitic, a negation 
or an adverb. Under-specification f the dominance 
relation N4 >* No leaves all these modifications 
open. Under-specification of dominmme between 
constituents goes beyond TAG adjunction in that 
the nodes which are in a dominance relation do not 
necessarily have the same grmnmatical category 
and thus linguistic phenomena like wh-extraction 
can be expressed easily in this way. 
1.3 Polarized and under-specified features 
Deserit)tions of tyI)e Fcat, related to features, have 
the following fol'm: 
Feat ::= Node : Attr Pol Val \[ 
Var C Dora \[ Vat  ? Dora 
gol : := <- I  = I + 
Val ::= Coast I Vat  
A feature Node : Attr  Pol Val is a triplet composed 
of an attribute Attr, a polarity Pol and a value Val 
associated with a syntactic node Nodc. Usually, a 
thature is defined as a pair (attribute, vahle). Ill 
IG, we add a polarity to this pair so that features 
behave like electrostatic charges: a positive feature 
Attr ~ Val seeks a negative featm'e Attr +-- Val to 
neutralize it and conversely while a neutral feature 
Attr = Val only acts as a filter through constraints 
on its value Val when it; meets another feature of 
type Attr at; the sanle node. 
Ill all cases, Val is either a constant which is selected 
from an infiifite countable set Coast of feature val- 
ues or a variable which is selected fl'oln an infinite 
countable set Vat of feature variables; then, its def  
inition domain Call be constrained by two lduds of 
predicates: Val E Dora and Val 9( Dora; Dora is a 
finite set of elements taken froln Co7~8t. 
Let us illustrate this presentation with a possible 
lexieal entry for the i)roi)er noun Jean: 
Dj  ..... = (N > \[\])? 
(g :  ~,,,t = ,@ ? (N :  f~ .~t  ~ Vl) 
(N:  ord --+ 1) ? (N:  phon =' Jean')? 
(N : Ocn = m) ? (N : hum = .sg)? 
(N :pets = 3) 
Some features are neutral by nature like agreement 
features: gen=m (gender=male), num=sg (num- 
ber=singular), pets=3 (person=3). Others are po- 
larized by nature too: for instmlce, features of type 
f lmct which express syntactic functions. In the ex- 
ample above, the feature of type funct is negative 
because the noun phrase represented by N is waiting 
to receive a syntactic fllnction (subject, object.. .);  
this flmction is not determined yet and thus it is 
represented by a variable lq. 
The phonological form of a constituent is determilmd 
by a system of two features: phon which gives tile 
effective phonological form of the constituent and 
ord which gives the order in which its immediate 
sub-constituents must be concatened to build this 
phonological form. For instance, we find the tbrnnlla 
(N1 : ord -+ I/2) ? (172 C {12, 21}) in the description 
Dvoit to express that the clause which has the verb 
volt as its head and is represented by node N1 is 
a concatenation subject-verb phrase (14 = 12) or 
verb phrase-subject (172 = 21). When a node has no 
children, two cases occur: the node has an empty 
phonological form and the vahle of the feature ord is 
0 or the node is a lexical anchor and the value of the 
feature ord is 1. In this case, the feature phon is used 
tbr retrieving the effective phonological form, which 
can be verified in the (lescription D.l~an. Polariza- 
tion of phonological tbrms expresses that some con- 
stituents are capable of giving a phonological tbrm 
while others are waiting tbl" one. As the previous 
exalnples hows, this pohu'ity is not carried by the 
tbatul'e phon but by the feature ord. The interest 
of giving privilege to the tbat, ure ord with respect 
to the feature ph, on, is twofold: we can deternline 
its value for a given node without being aware, of 
the phonological form of the children, the effective 
pholmlogical form will be rebuilt step by step from 
the leaves to the root of the final syntactic tree as 
soon as possible; another interest is that features of 
type ord can be dealt with like all other features; in 
particular, we can al)ply to theln the salne type of 
constraints. 
Finally, it is interesting to inention that value shar- 
ing by different features is represented in an easy 
way by using a unique variable for tile vahles of the 
concerned features. 
2 Syntact i c  compos i t ion  as 
deduct ion  in a l inear theory  
By choosing a logical framework tbr a fornml def- 
inition of IG, we find a natural way of expressing 
syntactic eompositiou by means of deduction in lin- 
ear logic according to the 1)aradigm "parsing as de- 
duction" of CG (for a broad survey of CG see (Re- 
tord, 2000)). All interaction granunar is lexiealized 
ill the sense that all linguistic resources are stored 
in a lexicon and these resources will be coinbined 173' 
using inference rules of the ILL deductive system for 
building the acceptable sentences of the correspond- 
ing language. Since syntactic descriptions use only 
a fragment of this logic and if we choose the frame- 
work of the sequent calculus, only seven ILL rules 
are useflll: 
F1, . . . ,Fn I- F I?. . .?Fn id 
P ~- G FI, F2, P I- G 
1, P 1- G 1L Ft @F'2, P l- G @L 
F1, P N G J~'2, P k GG&L2 
FI&F2, P G &L1 /~'I&F2, r I- 
F\[t/X\], P t- G Pl 1- 17 F, I~2 l-- G V1~ cut 
V X ~; P F- Pl, P2 t- G 
602 
With respect o tile usual presentation of the ILL 
sequent cahmlus (Lincolu, 1992), a?iom id is defined 
a bit differently but this definition is equivalent o 
the original one tbr tile logical fragment used by IG. 
Rule gr. is a tirst order rule which is used here for 
instantiating a node variable with a concrete node 
or a feature variable with a concrete feature value. 
Beside these general rules, we need proper axioms 
to express properties related to dominance relations, 
feature polarities, feature values and phonological 
forms. Concerning dominance relal, ions, we have 
the following proper axiom schemes: 
N: > N2,(NI > \[1,,N2,L'\]) }- (N1 > \[L,~2~LI\]y dl 
N >* N F 1 d2 
N1 >* Na, (NI > \[L, N2,1/\]) V N2 >* N3 Q (N1 > \[L, N2,L'\]) d3 
Axiom scheme dl expresses that immediate dom- 
iuallCe is realized t)y a parent-children relation 
whereas axiom schemes d2 and d3 express that 
dominance is realized l)y finite sequences of l:arent- 
children relatkms (L an(1 L '  represent sequences of 
node variables). 
The behaviour of polarities is represented by the 
following proper axiom schemes: 
(N :A I '  V), (N :A=V)  P (N : ,41 '  V) I'1 
(N :A<-V) , (N :A -~V)  ,- (N :A=V)  v~ 
Proi)erties related to feature doinaius and vahles 
rare expressed by tile following axioln schenles: 
< {e} u D"  c '<' 
hi l)oth axiom schelnes, D rel)resellts a couel'el;e 
filtite set of feature values taken from Con.W, and U 
and \ rel)t'esent the usual operations of union aim 
difference of sets. 
Finally, three axiom schemes are used for deducing 
tile effective phouological form of a col:stil;uent from 
the order of the phonological forms of its children: 
N > \[\], (N :  o,'d = 0) ~ N > \[\] V (N :  Vho,, = ,,)Vh.1 
> \[ \], (N :  o,.,~ = 1) F N > \[~ph._, 
,) ph3 (~r > \[N 1 . . . . .  NI,\]) , O, ~'1 I- (N > \ [NI , . . . ,  Np\]) 05) 1 2 
Schemes ph~ and ph2 respectively correspond to 
empty categories and lexical anchors. 
In scheme pha, 0 is an abbreviation for 
(N :o rd  = e(c,)); a is a perlnutation on \[\[1,p~ 
which expresses an order ibr concatenating the 
phonological tbrms v l , . . . ,  vp of the children nodes 
N\ ] , . . . ,  N v of N and c(o-) is a bijective encoding of 
this permutation with an integer. /71 is an abbre- 
viatioll for (NI : phoTt = V l ) , . . .  , (j~Tp : phon = %) 
and P~ an abbreviation for the product 
(N : phon = vo.(1) . . . . .  v,,(l,)) ? (N1 : piton = 
Vl) ?''" ? (Np : ph, o?L = vI, ). 
A particular interaction grmnmar G is defined by 
its vocalmlary \]?occ. and by a lexicon gexc,; the vo- 
cabulary Poco inchldes the words used for tmilding 
the hmguage /--:a generated by this grammar and 
the lexic(m ?c:,:c; associates a syntactic description 
to each word of Foca. Now, we have to (:ombine 
the resources provkled by go:re- by means of the 
inference rules and proper axioms of the linear 
theory T which has .just; been defined to compose 
well-formed and complete syntactic structures of G 
under the shaI)e of closed syntactic descriptions. As 
a preliminary, we have to give a precise definition of 
a closed syntactic description: 
A closed syntactic description is a partic'a- 
lar syntactic description in the shape S ? F 
wh.cre S and F, respectively, represent he 
structural and feature parts of the dcserip- 
tion with the following conditions: 
1. S is a product of predicates in the form 
(,,. >  ,,here ,,., ,,.,, . . . ,  
n v represent eoncrcte syntactic nodes, 
and the structure defined by all these 
parent-children relations is a tree; 
2. F is a product of predicates in the form 
(n :a t t r  = v), where n, attr and v 
represent concrete atoms, and for each 
pair (u, attr) pre.scnt in F, there is cx- 
actly one feature (n : attr  = v) in F. 
3. For every syntactic node 7t in S, there 
is a feature (n : phon. = v) in F. 
Condition 1 guarantees that a closed syntactic 
description rel)resents a COml)letely specitied tree. 
Condition 2 gua.rantees ('oherence and neutrality of 
the feature system which is attached at each syntac- 
tic node. Condition 3 guarani;cos the phonoh)gical 
well-fornmdness of the whole syntactic sl.l'it(:t;::t'e. 
Now, let; us explain how G generates closed syntac- 
tic descriptions from n lexieal entries D , , , , . . . ,  D,~,, 
correspouding to n words Wl , . . . ,  w,, taken fi'om 
Vote;. For this, we need an additional description 
D,.om to represent he root of the final syntactic 
tree which has tile fbrm:(No >* N:) ?. . .  ? (N0 >* 
A:,) ? (No : ord ?- V0). Node No represents the root 
of the syntactic tree and N1, . . . ,  N v are the nodes 
present in descriptions Dwl , . . . ,  D,o,,. Then: 
A closed syntactic dcscr@tion D is said to 
be generated from the words w1, . . . , w,  by 
grammar G if the sequent V N V V (D,.oot? 
Dw: ? ""  ? Dw,) F D is provable in the 
theory 7- (N and {J represent all node vari- 
ables and fi'aturc variables that arc fl'cc in 
Drool, Dw: , . . . ,  Dw.).  
D describes a tree which represents the syntax of a 
phrase given by the feature phon of its root. If we 
add the predicate (No : piton = wl . . .  w, )  to D,.oot, 
we transform the generation of closed syntact;ic de- 
scriptions into parsing of the phrase wl ?. ? w,, 
603 
By continuing with the verb volt, let us give a very 
simple illustration of this mechanism. We assume 
that a lexicon provides us with three descriI)tions 
Dvoit, Dil and D.lcan which respectively eorresi)ond 
to the finite verb volt, tile personal pronoun il and 
tile proper noun Jean. As it was described in sub- 
section 1.1, Dvoit has the shape D1 ? (D2&D3) ? 
(D4&Da and it is schematized by the following di- 
agram: 
i I)1 i . - cal=s (" - - ") ord -> 12121 
, 7 : :7  . i _  , c~t=~p I 
' : -  - - " ord = 1 112/ 
lunct = subj ; ~\[ i 
4 ) cat = v 
? "" ord <- 
cat=v 6 } .... 
)hen = 'voif " o d -> 1 
! ( & 
) ! :  ;,, :, ",/ ", 
: (~)  (3) 
, \ ]  o rd :12  \] ?rd=~'1 " " - "  
( 4 ) i 
1 ,unct bj . . . .  l i ' J '
, ! 3 ) 
i ord = 12 
( 4 ) (. s ) 
cat =np / i i 
erd <- IL 
tunct -> obj 
! 
To remain readable, the diagram includes only the 
most significant features of every node. The nota- 
tion ord -~ 12121 is all abbreviation for ord --+ V 
with 17 G {12, 21} and ord +-- means that the value 
of Lhe feature ord is undetermined. 
Description Dil has a structure that is similar to 
Dvoit : 
11~ 
ord = 12 
cat = nD 
funct = subj 
7 
I 
9 
f ? 
( lo ) 
cat  = s 
\ 
funct  <- subj i 
i .  > , er =21 1 
I ! I~:!  ' - 
cat = vp 
cat = v 
cat =v ( I1 ) 
ord-> 12121 "" \] -" 
cat=ct i t  - . .  ".. "" - -  ~ / 
? . ( 12 ) ~ a+ r cat=v 
pnon = ' . '~  . . . .  - -< I 
ord=l  ~t  K J _ \ord<- / 
\] 1 . . . . .  . . . . . .  
. j -  ,,,\]+ 
typ = decl 
( 11 ') 
x"  
ord = 12 
.-( ) 
typ = "!?r . . . . .  
\[ ( u ' i  
&21 
Tile first additive component of description Dil, 
Dr&D8 represents a choice between tile absence of 
all explicit subject ill the sentence beside tile per- 
sonal pronoun il such as in tile sentence il volt Jean 
and the presence of this subject such as in tile sen- 
tence Jean voit-il ?. The second alternative ntails 
that the sentence is interrogative if we ignore topi- 
calization, which explains description Ds. 
The second additive component of description Dil, 
Dg&Dlo, represents a choice between tile declarative 
type and tile interrogative type of the sentence which 
depends on the relative order between tile verb and 
tile clitic. 
Descrit)tion D Jean is reduced to the following single 
node: 
cat = l lp  
o rd -> 1 
funct <- 
phon = ' Jean' 
From tile description V N V 1~ (D,.oot ~ D,,oit ? 
Da~.,, ? Dil), it is possible to deduce three closed 
syntactic descriptions D.,  Db and D~, which respec- 
tively represent the syntax of the grammatical sen- 
tences :il voit ,lean, voit-il Jean ? and Jean voit-il .~. 
Ill concrete terms, the deduction process that leads 
to these three solutions consists ill plugging nodes 
of the initial descriptions with tile aim of neutraliz- 
ing all polarized features while respecting dominance 
and featm'e constrains. Let us detail the resulting 
description DD by means of the syntactic tree it spec- 
ifies: 
cat = S 
typ  = dec l  II - I -7 ) phon = 'voit il Jean' 
cat  = np 
phon = ' ' 2-8 
funct = subj I 
cat=v 
phon = 'voit il' 
i 
I 
( 3-9 ) 
cat = vp 
phon = 'veil il Jean ' 
J 
I \[ I cat = np 
(4-I0-1i) (5-14) phon = ' Jean '  
I \[ i J funct = obj 
' I \ [ '  
cat=e l i !  ( 12 ) ( 6.13 ) cat=v 
phon =' i r  \] J phon ='vo i t '  
Tile closed syntactic description that specifies tlte 
tree above represents the syntactic structure of the 
sentence voit-il Jean ?. The numbers that label its 
nodes are the traces of the nodes of the descriptions 
that have been plugged in the parsing process. 
3 A constraint-based 
implementation 
From tile viewpoint of a computer scientist, a lin- 
guistic model has to show not only expressive power 
but also computational tractability. In the previous 
section, we have shown that IG computations re- 
duce to ILL proofs. For tile logical fragment that we 
consider here, three logical rules are a source of non- 
604 
determinism in proofsearcll: &L1, &L') and VL. This 
takes the shape of three kinds of choice points in tile 
t)arsing process: selecting the pertinent branch for 
every additive conjunction, identit~ying some node 
variables and instantiating t~ature variables in an 
al)t)ropriate maimer. The NP-conq)letenest of the 
implicative fragment of ILL (Kanovich, 1992) shows 
that it is hopeless to find a general parsing algo- 
r ithm for IG that works in polynomial time in the 
worst cases. Experience has shown that, fortunately, 
these worst cases rarely occur in parsing natural an- 
guages. Nevertheless, the flexibility of IO entails a 
combinatory explosion of the parsing process if we 
use a "generate and test" method and leads us to 
choose a more approt)riate method. The specifica- 
tion of our problem prompts us in a natura l  way to 
a constraint-based al)l)roach as it was suggested by 
st)me proposals for similar prol)h;ms (Duchier and 
C., 1999; Duehier and Thater, 1999). 
The t)rol)lem can be tbrmulated as follows: 
Given a s?jntactic description Do, find all 
closed syntactic descriptions D such that 
VN VV Do t- D is provable in the theory 7- 
(N and l} respectively repro.sent he node 
variables N, , . . . ,  N~ and the. fcaturcs vari- 
ables I~,..., 147~ of Do). 
A flmdame.ntal t)rot)erty of the (teduction process 
that lea(It to a solution is monotonicity to that the 
t)roblem can t)e expressed as a constraint satisfac- 
tion problem (CSP). A CSP is specitied fl:om a set 
of variables to which constraints are apl)lied. Here, 
we consider three sets of variable, s which corretl)on(t 
to tim three kin(Is of choi(:e 1)oints in the parsing pro- 
COTS;  
1. the set {N1, . . . ,N, ,}  of syntacti(" 1,o(le vari- 
a/)les; 
2. the set { l~, . . . ,  I4,, } of t'eature variables; 
3. the set {St , . . . ,Sv}  of sdection variables; ev- 
ery selection variable Si is an integer variable 
which is associated with a connective & of D0 
and which is used for indicating the rank of the 
component of the correspondent additive con- 
junction that is selected in the deduction. 
Selection and feature variables are considered as fi- 
nite domain variables, which imply that all feature 
vahms are encoded as integers (one exeel)tion is that 
features of type phon remain strings). 
Node variables arc' enco(ted indirectly via finite 
set variables by using the metho(t t)roposed in 
(Duchier and C., 1999). Every node variable 
Ni is associated with five finite set w~.riables 
cq(i), up(i), down(i), side(i) and all(i) which are 
used for locating the node i with respect o the oth- 
ers in the sys|;em of dominance relations. Because 
of the presence of additive cm\\]unctions, a node i 
which is present in tile description Do nmy be absent 
from a solution. In this case, eq(i) = {i}, alt(i) = 
~l,n~\{i}, up(i) = down(i) = side(i) = 0; in the 
case that i is present in a solution, alt(i) repre- 
tents the nodes that are not selected in the solution 
whereas tile selected nodes are distributed into the 
four sets cq(i), 'up(i), down(i) and .side(i) according 
to their relative position with respect o i. 
Constraints on the variat)les of the probhnn are di- 
vided into two parts: 
? general constraints guarantee that the solutions 
D are effective closed syntactic descriptions; 
? specific constraints guarantee that the solutions 
D are models of the initial description Do. 
3.1  Genera l  const ra in ts  
Treeness  const ra in ts  For every node i, the parti- 
tion of \[1, n~ between eq(i), up(i), down(i), .side(i) 
and all(i) guarantees that the solution is a directed 
acyclic graph (DAG). 
For expressing that all dominmme relations which 
structure a solution must only be realized by parent- 
ehihtren relations, we must introduce constraints ill 
which variables of type. cq(i) and selection variables 
appear for expressing that every selected node vari- 
able must be identified with a node variable which 
is the parent in a selected parent-children relation. 
In order to express that a solution is more than a 
DAG, that is a tree, we must add the following con- 
straint: for every selected parent-children relation, 
the sets down(j) for the children j present in this 
relation must be disjoint. Such a condition can be 
drol)ped if we want to extend the fbrmalism to take 
into ac(:ount resource thm:ing like coordination tot 
instance; in this ease, syntactic structures are no 
longer trees trot DAGs. 
Neut ra l i ty  const ra in ts  Feature neutrality of a 
solution is guaranteed by constraints which also ap- 
peal to variables of type cq(i) and selectkm vari- 
ables: for each attribute Attr, we consider two sets 
of sets in tile shape cq(i): the first corresponds to 
all selected predicates in the form (Ni : Attr +-- V) 
and the second to all selected predicates in the form 
(Ni : Attr + V). The elements of each of these 
sets must be disjoint sets and every element of the. 
first set; must be identified with one element of the 
second and conversely. 
Other general constraints related to features and 
phonological forms are trivial. 
3.2  Specif ic const ra in ts  
Such constraints are determined by Do. Doini- 
nance constraints are easily iml)lelnented by com-  
b in ing  selection variables and variables of type 
cq(i), 'up(i), down(i), side(i)(Duchier and Thater, 
1999). 
605 
FEaturE constraints concern both feature variables 
and selection variables which are all finite domain 
variables to that their implen:entation appeals to 
classical tools in the domain of constraint program- 
ining. 
3.3 A prototype parser for Ih'ench 
We have implemented a prototype parser for IS"ench. 
It it written in the language Oz (Smolka, 1995) 
which combines various aspects and modules, in- 
cluding constraint prograInming. Though the lin- 
guistic COvErage of tile lexicon is still linfited, we 
have learnt lessons from the first experiments: in 
particular, neutrality constraints play a central role 
for restricting the search space, which confirms the 
inlportancc of polarities for the computationa.1 Gtrl- 
ciency. 
Conc lus ion  
Starting from TAG and CO, we have presented a
linguistic tbrmalism which aims at better cal)turing 
the flexibility of natural language by using two no- 
tions as its basis: underspccifieation and polarities. 
In some SENSE, they correspond to two important 
properties of natural language: ambiguity and re- 
source sensitivity. 
To regard parsing as a constraint satisfaction prob- 
lem fits in with the flexibility of the formalism in 
terms of comi)utational efficiency but, at tile same 
time, it allows to go towards robustness beyond a 
traditional view of parsing in which only grammati- 
cal and completely specified structures are taken into 
a(;count.  
The success of IG does not ette.ntially depend on 
the fbrmal propErtiEs that are usually Exhibited for 
grammatical formalisms: the characterization f tile 
class of languages that are generated by thesE gram- 
mars or the complexity of general parsing algo- 
rithms. Forlnal properties matter but with respect 
to an ESSEntial goal: to Extend the linguistic coverage 
of IG from toy lexicons to massive lexical databases. 
For this, IG have some advantages by making it eas- 
ily to factorize and modularize information: such 
propErtiEs are decisive when one wants to extract 
information from a lexical database fficiently or to 
update data while maintaining the coherence of the 
whole base. 
The success of IG will also depend on their capacity 
to integrate other linguistic lEvEls than the syntactic 
level, the semantic level especially. 
References  
P. Blache. 1999. "Contraintcs ct thdories linguis- 
tiques : des Grammaircs d'Unification aux Gram- 
maires de Propridtds". ThSse d'Habilitation, Uni- 
versit5 Paris 7. 
M.-It Candito. 1999. Oryanisation modulairc ct 
paramdtrablc dc grammaircs dlcctrvniqucs lcxi- 
calisdcs. Application a'a fl'anf.ais et ?~ l'italien,. 
ThSse de Doctorat, Universitd Paris 7. 
D. Duchier and Gardent C. \]999. A constraint- 
based treatment of descriptions. In IWCS-3, Jan 
99, TillbuTy, The Netherlands. 
D. Duchier and S. Thater. 1999. Parsing with tree 
descriptions: a constraint based al)proaeh. In 
NLULP'99,Dcc 99, Las Cruccs, New Mcxico. 
M. Egg, J. Niehren, P. Ruhrberg, and F. Xu. 1998. 
Constraints over lambda structures in semantic 
underspecification. I  COLING/A CL'98, Aug 98, 
Montreal, QuEbec, Canada. 
J.-Y. Girard. 1987. LinEar logic. Th, eorctical Com- 
puter Science, 50(1):1-102. 
A. K. Joshi, L. S. Levy, and M. Takahashi. 1975. 
Tree adjunct grammars. Journal o/Computer and 
System Sciences, 10(1):136-163. 
L. Kalhneyer. 1999. Tree Description Grammars 
and Undcrspccificd Rcprcsentations. Ph.D. thesis, 
UniversitSt Tiibingen. 
M. Kanovich. 1992. Horn programming in linear 
logic is NP-comt/lete. In LICS'92, .\]~tTt 92~ Santa 
Cruz, California, pages 200-210. 
A. Kroch and A. Joshi. 1985. Linguistic relevance of 
tree adjoining grammars. Technical Rel)ort MS- 
CI-85-18, Department of Computer and Informa- 
tion Science, Uifiversity of Pemlsylvania. 
J. Lambek. 1958. The mathematics of sentence 
structure. Amcr. Math. Monthly, 65:154-169. 
P. Lincoln. 1992. Computational aspccts of linear 
logic. Ph.D. thesis, Stanford Uifiversity. 
),4. Moortgart. 1996. Categorial Type Logics. In 
J. van Benthein and A. ter Meulen, editors, Hand- 
book of Logic and Language, chal/ter 2. Elsevier. 
R. Muskens mid E. Krahmer. 1998. Talking about 
trees and truth-conditions. Ill LACL'98. Dec 98, 
Grenoble, France. 
O. Rainbow, K. Vijay-Shanker, and D. Weir. 1995. 
D-tree grammars. In ACL'95, paget 151 ~158. 
C. RetorS. 2000. Systbmes ddductifs et traite- 
Inent des langues:un panorama (let grmnmairEs 
catdgorielles. Research Report RR-3917, INRIA. 
To appear in Technique t Science Iifformatiques. 
Gert Smolka. 1995. The Oz programming model. 
In Jan van Leeuwen, editor, Computer Science 
Today, LEctUrE Notes in ComputEr SciencE, vol. 
1000, pages 324 343. Springer-Verlag, BErlin. 
K. Vijay-Shanker. 1992. Using description of trees 
in a tree adjoining grmnmar. Computational Lin- 
guistics, 18(4):481-517. 
606 
Polarization and abstraction of grammatical formalisms as
methods for lexical disambiguation
Guillaume Bonfante and Bruno Guillaume and Guy Perrier
LORIA - UMR 7503,
Campus Scientifique, B.P. 239,
F-54506 Vand?uvre le`s Nancy CEDEX
{Guillaume.Bonfante, Bruno.Guillaume, Guy.Perrier}@loria.fr
Abstract
In the context of lexicalized grammars, we
propose general methods for lexical disam-
biguation based on polarization and ab-
straction of grammatical formalisms. Polar-
ization makes their resource sensitivity ex-
plicit and abstraction aims at keeping essen-
tially the mechanism of neutralization be-
tween polarities. Parsing with the simpli-
fied grammar in the abstract formalism can
be used efficiently for filtering lexical selec-
tions.
Introduction
There is a complexity issue if one consider ex-
act parsing with large scale lexicalized gram-
mars. Indeed, the number of way of associating
to each word of a sentence a corresponding ele-
mentary structure?a tagging of the sentence?
is the product of the number of lexical entries for
each word. The procedure may have an expo-
nential complexity in the length of the sentence.
In order to filter taggings, we can use proba-
bilistic methods (Joshi and Srinivas, 1994) and
keep only the most probable ones; but if we
want to keep all successful taggings, we must
use exact methods. Among these, one consists
in abstracting information that is relevant for
the filtering process, from the formalism F used
for representing the concerned grammar G. In
this way, we obtain a new formalism Fabs which
is a simplification of F and the grammar G is
translated into a grammar abs(G) in the ab-
stract framework Fabs. From this, disambiguat-
ing with G consists in parsing with abs(G). The
abstraction is relevant if parsing eliminates a
maximum of bad taggings at a minimal cost.
(Boullier, 2003) uses such a method for Lexical-
ized Tree Adjoining Grammars (LTAG) by ab-
stracting a tree adjoining grammar into a con-
text free grammar and further abstracting that
one into a regular grammar. We also propose
to apply abstraction but after a preprocessing
polarization step.
The notion of polarity comes from Categorial
Grammars (Moortgat, 1996) which ground syn-
tactic composition on the resource sensitivity of
natural languages and it is highlighted in Inter-
action Grammars (Perrier, 2003), which result
from refining and making Categorial Grammars
more flexible.
Polarization of a grammatical formalism F
consists in adding polarities to its syntactic
structures to obtain a polarized formalism Fpol
in which neutralization of polarities is used for
controlling syntactic composition. In this way,
the resource sensitivity of syntactic composition
is made explicit. (Kahane, 2004) shows that
many grammatical formalisms can be polarized
by generalizing the system of polarities used in
Interaction Grammars.
To abstract a grammatical formalism, it is in-
teresting to polarize it before because polarities
allow original methods of abstraction.
The validity of our method is based on a con-
cept of morphism (two instances of which being
polarization and abstraction) which character-
izes how one should transport a formalism into
another.
In sections 1 and 2, we present the conceptual
tools of grammatical formalism and morphism
which are used in the following.
In section 3, we define the operation of polar-
izing grammatical formalisms and in section 4,
we describe how polarization is used then for
abstracting these formalisms.
In section 5, we show how abstraction of
grammatical formalisms grounds methods of
lexical disambiguation, which reduce to pars-
ing in simplified formalisms. We illustrate our
purpose with an incremental and a bottom-up
method.
In section 6, we present some experimental
results which illustrate the flexibility of the ap-
proach.
1 Characterization of a grammatical
formalism
Taking a slightly modified characterization of
polarized unification grammars introduced by
(Kahane, 2004) we define a grammatical formal-
ism F (not necessarily polarized) as a quadruple
?StructF ,SatF ,PhonF ,RulesF ?:
1. StructF is a set of syntactic structures
which are graphs1 in which each edge
and vertex may be associated with a la-
bel representing morpho-syntactic informa-
tion; we assume that the set of labels asso-
ciated with F is equipped with subsump-
tion, a partial order denoted v, and with
unification, an operation denoted unionsq, such
that, for any labels l and l?, either l unionsq l? is
not defined, which is denoted l unionsq l? = ?, or
l unionsq l? is the least upper bound of l and l?2;
2. SatF is a subset of StructF , which repre-
sents the saturated syntactic structures of
grammatical sentences;
3. PhonF is a function that projects every
element of SatF in the sentence that has
this element as its syntactic structure.
4. RulesF is a set of composition rules be-
tween syntactic structures. Every element
of RulesF is a specific method for super-
posing parts of syntactic structures; this
method defines the characteristics of the
parts to be superposed and the unification
operation between their labels. Notice that
we do not ask rules to be deterministic.
The composition rules of syntactic structures,
viewed as superposition rules, have the funda-
mental property of monotonicity: they add in-
formation without removing it. Hence, the defi-
nition above applies only to formalisms that can
be expressed as constraint systems in opposition
to transformational systems.
Let us give some examples of grammatical for-
malisms that comply with the definition above
by examining how they do it.
? In LTAG, StructLTAG represents the set
of derived trees, SatLTAG the set of de-
rived trees with a root in the category
sentence and without non terminal leaves.
1Usually trees or directed acyclic graphs.
2The least upper bound of l and l? can exist and, at
the same time, l unionsq l? be not defined; if the operation of
unification is defined everywhere, the set of labels is a
semi-lattice.
The projection PhonLTAG is the canoni-
cal projection of a locally ordered tree on
its leaves. Finally, RulesLTAG is made
up of two rules: substitution and adjunc-
tion. To view adjunction as a superposition
rule, we resort to the monotone presenta-
tion of LTAG with quasi-trees introduced
by (Vijay-Shanker, 1992).
? In Lambek Grammars (LG), StructLG
is the set of partial proofs and these
proofs can be represented in the form
of incomplete Lambek proof nets labelled
with phonological terms (de Groote, 1999).
SatLG represents the set of complete proof
nets with the category sentence as their
conclusion and with syntactic categories
labelled with words as their hypotheses.
The projection PhonLG returns the label
of the conclusion of complete proof nets.
RulesLG is made up of two rules: a binary
rule that consists in identifying two dual
atomic formulas of two partial proof nets
by means of an axiom link and a unary rule
that consists in the same operation but in-
side the same partial proof net.
Now, inside a formalism defined as above, we
can consider particular grammars:
A grammar G of a formalism F is a
subset G ? StructF of its elementary
syntactic structures.
A grammar is lexicalized if every element of G
is anchored by a word in a lexicon. In LTAG, G
is constituted of its initial and auxiliary trees.
In LG, G is constituted of the syntactic trees of
the formulas representing syntactic categories of
words as hypotheses plus a partial proof net an-
chored by the period and including a conclusion
in the category sentence.
From a grammar G defined in a formalism
F , we build the set D(G) of its derived syntac-
tic structures by applying the rules of RulesF
recursively from the elements of G. The lan-
guage generated by the grammar is the projec-
tion L(G) = PhonF (SatF ? D(G)).
2 Morphisms between grammatical
formalisms
Polarization and abstraction can be defined
from a more general notion of morphism be-
tween grammatical formalisms. A morphism
from a grammatical formalism C to a grammat-
ical formalism A is a function f from StructC
to StructA with the following properties3:
(i) f(SatC) ? SatA;
(ii) ?S ? SatC ,PhonA(f(S)) = PhonC(S);
(iii) if S1, . . . , Sn are composed into a struc-
ture S in C by means of rules of RulesC ,
then f(S1), . . . , f(Sn) can be composed
into the structure f(S) by means of rules
of RulesA.
Given such a morphism f and a grammar G
in C, the image of G by f denoted f(G) is
the grammar?in A?induced by the morphism.
The three properties of morphism guarantee
that the language generated by any grammar
G of C is a subset of the language generated by
f(G). In other words, L(G) ? L(f(G)).
We propose to use the notion of morphism in
two ways:
? for polarizing grammatical formalisms and
in this case, morphisms are isomorphisms;
grammars are transposed from a formalism
to another formalism with the same gener-
ative power; in other words, with the pre-
vious notations: L(G) = L(f(G));
? for abstracting grammatical formalisms
and this case, the transposition of gram-
mars by morphisms entails simplification of
grammars and extension of the generated
languages; we have only: L(G) ? L(f(G)).
An example of the use of abstraction for lex-
ical disambiguation may be found in (Boul-
lier, 2003)4. We propose to link polarization
with abstraction because polarities allow origi-
nal methods of abstraction. Polarization is used
as a preprocessing step before the application of
these methods.
3 Polarization of grammatical
formalisms
The goal of polarizing a grammatical formal-
ism is to make explicit the resource sensitiv-
ity that is hidden in syntactic composition, by
adding polarities to the labels of its structures.
When morpho-syntactic labels become polar-
ized in syntactic structures, they get the status
3An elegant definition of morphism could be given
in a category-theoretical framework but we have chosen
here a more elementary definition.
4Our definition of morphism must be slightly ex-
tended for embedding the proposal of (Boullier, 2003).
of consumable resources: a label that is asso-
ciated with the polarity + becomes an avail-
able resource whereas a label that is associated
with the polarity ? becomes an expected re-
source; both combine for producing a saturated
resource associated with the polarity $; labels
associated with the polarity = are neutral in
this process. In a polarized formalism, the sat-
urated structures are those that have all labels
associated with the polarity = or $. We call
them neutral structures. The composition of
structures is guided by a principle of neutraliza-
tion: every positive (negative) label must unify
with a negative (positive) label.
The polarization of a formalism must pre-
serve its generative power: the language that
is generated by a polarized grammar must be
the same as that generated by the initial non-
polarized grammar. This property of (weak and
even strong) equivalence is guaranteed if the
polarized formalism is isomorphic to the non-
polarized formalism from which it stems. For-
mally, given a grammatical formalism F , any
formalism Fpol with a morphism pol : F ? Fpol
is a polarization of F if:
(i) For any structure S ? StructF , pol(S)
results from associating each label of S
with one of the polarities: +, ?, =, $;
in others words, labels of Fpol are pairs
(p, l) with p a polarity and l a label of
F . The set of polarities {+, ?, =, $} is
equipped with the operation of unification
and the subsumption order defined by
Figure 1. The operations of subsumption
and unification on pairs are the pointwise
operations. That is, for any pairs (p, l)
and (p?, l?),
(p, l)v(p?, l?) iff pvp? and lvl?
(p, l)unionsq(p?, l?) = (punionsqp?, lunionsql?)
(ii) SatFpol is constituted of the neutral struc-
tures of StructFpol .
(iii) pol is an isomorphism whose inverse mor-
phism is the function that ignores polar-
ities and keeps invariant the rest of the
structure.
Let us illustrate our purpose by taking again
our two examples of formalisms.
? For LTAG (see figure 2), pol consists in
labelling the root of elementary syntactic
trees with the polarity + and their non ter-
minal leaves (substitution and foot nodes)
? + = $
? $ ?
+ $ +
= ? + = $
$ $
=
? ?
+ ?
? ?
$
Figure 1: unification and subsumption between
polarities
pol destr
N
N*Adj
red
N?
N+
N+
Adj N?
red
 N+, N+, N?, N? 
 red , Adj
Figure 2: Syntactic structures associated
with the adjective red in LTAG, LTAGpol,
(LTAGpol)destr
with the polarity ?. In every pair of quasi-
nodes, the top quasi-node is labelled with
the polarity ? and the bottom quasi-node
is labelled with the polarity +. With re-
spect to the classical presentation of LTAG,
initial trees must be completed by an axiom
with two nodes of the type sentence: a root
with the polarity = and its unique daugh-
ter with the polarity ?. In this way, pol
establishes a perfect bijection between the
saturated structures of LTAG and the neu-
tral structures of LTAGpol. The rules of ad-
junction and substitution of RulesLTAGpol
mimic the corresponding rules in LTAG,
taking into account polarities. We add a
third composition rule, a unary rule which
identifies the two quasi-nodes of a same
pair. It is routine to check that pol is a
polarisation.
? In LG(see figure 3), polarization is already
present explicitly in the formalism: nega-
tive formulas and sub-formulas are input
formulas, hypotheses whereas positive for-
mulas and sub-formulas are output formu-
las, conclusions.
4 Abstraction of polarized
grammatical formalisms
The originality of abstracting polarized for-
malisms is to keep a mechanism of neutraliza-
tion between opposite polarities at the heart of
the abstract formalism. Furthermore, we can
choose different levels of abstraction by keeping
more or less information from the initial formal-
S+ NP?
eats
NP?(NP \ S ) / NP
eats
pol S+, NP?, NP?
eatsdestr
Figure 3: Syntactic structures associated
with the transitive verb eats in LG, LGpol,
(LGpol)destr
ism.
As an example, we propose a high degree ab-
straction, destructuring. Destructuring a polar-
ized formalism consists in ignoring the struc-
ture from the initial syntactic objects to keep
merely the multisets of polarized labels. For-
mally, given a polarized formalism P , we define
the formalism Pdestr as follows:
? Any element M of StructPdestr is a multi-
set of labels. All elements of M are labels
of P , except one exactly, the anchor, which
is a neutral string.
? SatPdestr is made up of multisets containing
only neutral and saturated labels;
? The projection PhonPdestr returns the la-
bel of the anchor.
? RulesPdestr has two neutralization rules. A
binary rule takes two multisets M1 and M2
from StructPdestr as inputs; two unifiable
labels +l1 ? M1(M2) and ?l2 ? M2(M1)
are selected. The rule returns the union of
M1 and M2 in which +l1 and ?l2 are uni-
fied and the two anchors are concatenated.
The only change with the unary rule is that
this operates inside the same multiset.
A morphism destr is associated to Pdestr (see
figure 2 and 3): it takes any structure S from
StructP as input and returns the multiset of its
labels with an additionnal anchor. This anchor
is the neutral string PhonP (S) if this one is
defined.
An important property of Pdestr is that it is
not sensitive to word order: if a sentence is gen-
erated by a particular grammar of Pdestr, by
permuting the words of the sentence, we ob-
tain another sentence generated by the gram-
mar. Destructuring is an abstraction that ap-
plies to any polarized formalism but we can de-
sign abstractions with lower degree which are
specific to particular formalisms (see Section 6).
5 Application to lexical
disambiguation
Abstraction is the basis for a general method
of lexical disambiguation. Given a lexicalized
grammar G in a concrete formalism C, we con-
sider a sentence w1 . . . wn. For each 1 ? i ? n,
let the word wi have the following entries in the
lexicon of G: Si,1, Si,2 . . . Si,mi . A tagging of
the sentence is a sequence S1,k1 , S2,k2 . . . Sn,kn .
We suppose now that we have given an abstrac-
tion morphism abs : C ? Cabs. As L(G) ?
L(abs(G)), any tagging in abs(G) which has no
solutions comes from a bad tagging in G. As
a consequence, the methods we develop try to
eliminate such bad taggings by parsing the sen-
tence w1w2 . . . wn within the grammar abs(G).
We propose two procedures for parsing in the
abstract formalism:
? an incremental procedure which is specific
to the destructuring abstraction,
? a bottom-up procedure which can apply to
various formalisms and abstractions.
5.1 Incremental procedure
We choose polarization followed by destructur-
ing as abstraction. In other words: abs =
destr ?pol. Let us start with the particular case
where unification of labels in C reduces to iden-
tity. In this case, parsing inside the formalism
Cabs is greatly simplified because composition
rules reduce to the neutralization of two labels
+l and ?l. As a consequence, parsing reduces
to a counting of positive and negative polarities
present in the selected tagging for every label
l: every positive label counts for +1 and ev-
ery negative label for ?1, the sum must be 0;
since this counting must be done for every pos-
sible tagging and for every possible label, it is
crucial to factorize counting. For this, we use
automata, which drastically decrease the space
(and also the time) complexity.
For every label l of C that appears with a
polarity + or ? in the possible taggings of the
sentence w1w2 . . . wn, we build the automaton
Al as follows. The set of states of Al is [0..n]?Z.
For any state (i, c), i represents the position at
the beginning of the word wi+1 in the sentence
and c represents a positive or negative count of
labels l. The initial state is (0, 0), and the final
state is (n, 0). Transitions are labeled by lexicon
entries Si,j . Given any Si,j , there is a transition
(i? 1, x)
Si,j
?? (i, y) if y is the sum of x and the
count of labels l in the multi-set destr(Si,j).
Reaching state (i, c) from the initial state
(0, 0) means that
(a) the path taken is of the form
S1,j1 , S2,j2 , . . . , Si,ji , that is a tagging
of the first i words,
(b) c is the count of labels l present
in the union of the multi-sets
abs(S1,j1), abs(S2,j2), . . . , abs(Si,ji).
As a consequence, any path that leads to the fi-
nal state corresponds to a neutral choice of tag-
ging for this label l.
The algorithm is now simply to construct for
each label l the automaton Al and to make the
intersection A =
?
l?LabelsAl of all these au-
tomata. The result of the disambiguation is
the set of paths from the initial state to the fi-
nal state described by this intersection automa-
ton. Notice that at each step of the construction
of the intersection, one should prune automata
from their blind states to ensure the efficiency
of the procedure.
Now, in the general case, unification of labels
in F does not reduce to identification, which in-
troduces nondeterminism in the application of
the neutralization rule. Parsing continues to re-
duce to counting polarities but now the counting
of different labels is nondeterministic and inter-
dependent. For instance, consider the multiset
{+a, +b, ?aunionsq+b} of three different elements.
If we count the number of a, we find 0 if we
consider that +a is neutralized by ?aunionsqb and
+1 otherwise; in the first case, we find +1 for
the count of b and in the second case, we find 0.
Interdependency between the counts of different
labels is very costly to be taken into account and
in the following we ignore this property; there-
fore, in the previous exemple, we consider that
the count of a is 0 or +1 and the count of b is
also 0 or +1 independently from the first one.
For expressing this, given a label l of F and a
positive or negative label l? of Fpol, we define
Pl(l?) as a segment of integers, which represents
the possible counts of l found in l?, as follows:
? if l? is positive, then Pl(l?) =?
?
?
J1, 1K if lvl?
J0, 0K if lunionsql? = ?
J0, 1K otherwise
? if l? is negative, then Pl(l?) =?
?
?
J?1,?1K if lvl?
J0, 0K if lunionsql? = ?
J?1, 0K otherwise
We generalize the function Pl to count the num-
ber ol labels l present in a multi-set abs(S):
Pl(S) = Jinf, supKwith:
inf =
?
l??abs(S) min(Pl(l
?))
sup =
?
l??abs(S) max(Pl(l
?))
The method of disambiguation using au-
tomata presented above is still valid in the gen-
eral case with the following change in the defini-
tion of a transition in the automaton Al: given
any Si,j , there is a transition (i?1, x)
Si,j
?? (i, y)
if y is the sum of x and some element of Pl(Si,j).
With this change, the automaton Al becomes
nondeterministic.
The interest of the incremental procedure is
that it is global to the sentence and that it ig-
nores word order. This feature is interesting for
generation where the question of disambigua-
tion is crucial. This advantage is at the same
time its drawback when we need to take word
order and locality into account. Under this an-
gle, the bottom-up procedure, which will be pre-
sented below, is a good complement to the in-
cremental procedure.
5.2 Bottom-up procedure
We propose here another procedure adapted to
a formalism C with the property of projectiv-
ity. Because of this property, it is possible to
use a CKY-like algorithm in the abstract for-
malism Cabs. To parse a sentence w1w2 ? ? ?wn,
we construct items of the form (i, j, S) with S
an element of StructCabs and i and j such that
wi+1 . . . wj represents the phonological form of
S. We assume that Rules(Cabs) has only unary
and binary rules. Then, three rules are used for
filling the chart:
initialization: the chart is initialized with
items in the form (i, i+ 1, abs(Si+1,k));
reduction: if the chart contains an item
(i, j, S), we add the item (i, j, S?) such that
S? is obtained by application of a unary
composition rule to S;
concatenation: if the chart contains two item
(i, j, S) and (j, k, S?), we add the item
(i, k, S??) such that S?? is obtained by ap-
plication of a binary composition rule to S
and S?.
Parsing succeeds if the chart contains an item
in the form (0, n, S0) such that S0 is an element
of SatCabs . From such an item, we can recover
all taggings that are at its source if, for every
application of a rule, we keep a pointer from the
conclusion to the corresponding premisses. The
other taggings are eliminated.
6 Experiments
In order to validate our methodology, we have
written two toy English grammars for the LG
and the LTAG formalisms. The point of the
tests we have done is to observe the performance
of the lexical disambiguation on highly ambigu-
ous sentences. Hence, we have chosen the three
following sentences which have exactly one cor-
rect reading:
(a) the saw cut the butter.
(b) the butter that the present saw cut
cooked well.
(c) the present saw that the man thinks that
the butter was cut with cut well.
For each test below, we give the execution
time in ms (obtained with a PC Pentium III,
600Mhz) and the performance (number of se-
lected taggings / number of possible taggings).
6.1 Incremental procedure
The incremental procedure (IP) results are
given in Figure 4:
LG LTAG
ms perf. ms perf.
(a) 1 3/36 3 3/96
(b) 42 126/12 960 40 126/48 384
(c) 318 761/248 832 133 104/1 548 288
Figure 4: IP with destr ? pol
One may notice that the number of selected
taggings/total taggings decrease with the length
of the sentence. This is a general phenomenon
explained in (Bonfante et al, 2003).
6.2 Bottom-up procedure
The execution time for the bottom-up proce-
dure (BUP) grows quickly with the ambiguity
of the sentence. So this procedure is not very
relevant if it is used alone. But, if it is used as
a second step after the incremental procedure,
it gives interesting results. In Figure 5, we give
the results obtained with the destr abstraction.
Some other experiments show that we can im-
LG LTAG
ms perf. ms perf.
(a) 2 3/36 9 3/96
(b) 154 104/12 960 339 82/48 384
(c) 2 260 266/248 832 1 821 58/1 548 288
Figure 5: IP + BUP with destr ? pol
prove performance or execution time with spe-
cific methods for each formalism which are less
abstract than destr.
6.2.1 Tailor-made abstraction for LG
For the formalism LG, instead of complete de-
structuring, we keep some partial structural in-
formation to the polarized label. As the for-
malism is projective, we record some constraints
about the continuous segment associated with a
polarity. In this way, some neutralizations pos-
sible in the destr abstraction are not possible
anymore if the two polarities have incompatible
constraints (i.e. lie in different segments). This
new morphism is called proj. The execution
time is problematic but it might be controlled
with a bound on the number of polarities in ev-
ery multiset5 (see Figure 6)
LG
sentence Time(ms) Perf.
(a) 2 1/36
(b) 168 5/12 960
(c) with bound 6 2 364 3/248 832
Figure 6: IP + BUP with proj ? pol
Without bound for sentence (c), the running
time is over 1 min.
6.2.2 Tailor-made abstraction for LTAG
For LTAG: a possible weaker abstraction (called
ltag) consists in keeping, with each polarity,
some information of the LTAG tree it comes
from. Rather than bags where all polarized la-
bels are brought together, we have four kind
of polarized pieces: (1) a positive label coming
from the root of an initial tree, (2) a negative
label coming from a substitution node, (3) a
couple of dual label coming from the root and
the foot of an auxiliary tree or (4) a couple of
dual label coming from the two parts of a quasi-
node. Rules in this formalism reflect the two
operations of LTAG; they do not mix polarities
relative to adjunction with polarities relative to
substitution. Figure 7 shows that the execution
time is improved (wrt. Figure 5).
Conclusion
The examples we have presented above should
not be used for a definitive evaluation of partic-
ular methods, but more as a presentation of the
flexibility of our program: polarizing grammati-
cal formalisms for abstracting them and parsing
5This bound expresses the maximum number of syn-
tactic dependencies between a constituent and the others
in a sentence.
LTAG
ms perf.
(a) 6 3/96
(b) 89 58/48 384
(c) 272 54/1 548 288
Figure 7: IP + BUP with ltag ? pol
in the resulting abstract frameworks for disam-
biguating lexical selections. We have presented
one general tool (the destructuring abstraction)
that may apply to various grammatical frame-
work. But we think that abstractions should be
considered for specific frameworks to be really
efficient. One of our purpose is now to try the
various tools we have developped to some large
covering lexicons.
So far, we have not taken into account the tra-
ditional techniques based on probabilities. Our
point is that these should be seen as an other
way of abstracting grammars. Our hope is that
our program is a good way to mix different
methods, probabilistic or exact.
References
G. Bonfante, B. Guillaume, and G Perrier.
2003. Analyse syntaxique e?lectrostatique.
Traitement Automatique des Langues. To ap-
pear.
P. Boullier. 2003. Supertagging: a Non-
Statistical Parsing-Based Approach. In 8th
International Workshop on Parsing Tech-
nologies (IWPT?03), Nancy, France, 2003,
pages 55?66.
P. de Groote. 1999. An algebraic correct-
ness criterion for intuitionistic multiplica-
tive proofnets. Theoretical Computer Sci-
ence, 224:115?134.
A. Joshi and B. Srinivas. 1994. Disambiguation
of super parts of speech (or supertags) : Al-
most parsing. In COLING?94, Kyoto.
S. Kahane. 2004. Grammaires d?unification po-
larise?es. In TALN?2004, Fe`s, Maroc.
M. Moortgat. 1996. Categorial Type Logics. In
J. van Benthem and A. ter Meulen, editors,
Handbook of Logic and Language, chapter 2.
Elsevier.
G. Perrier. 2003. Les grammaires d?interaction.
Habilitation a` diriger des recherches, Univer-
site? Nancy2.
K. Vijay-Shanker. 1992. Using description of
trees in a tree adjoining grammar. Computa-
tional Linguistics, 18(4):481?517.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 153?156
Manchester, August 2008
A Toolchain for Grammarians
Bruno Guillaume
LORIA
INRIA Nancy Grand-Est
Bruno.Guillaume@loria.fr
Joseph Le Roux
LORIA
Nancy Universit?e
Joseph.Leroux@loria.fr
Jonathan Marchand
LORIA
Nancy Universit?e
Jonathan.Marchand@loria.fr
Guy Perrier
LORIA
Nancy Universit?e
Guy.Perrier@loria.fr
Kar
?
en Fort
LORIA
INRIA Nancy Grand-Est
Karen.Fort@loria.fr
Jennifer Planul
LORIA
Nancy Universit?e
Jennifer.Planul@loria.fr
Abstract
We present a chain of tools used by gram-
marians and computer scientists to develop
grammatical and lexical resources from
linguistic knowledge, for various natural
languages. The developed resources are
intended to be used in Natural Language
Processing (NLP) systems.
1 Introduction
We put ourselves from the point of view of re-
searchers who aim at developing formal grammars
and lexicons for NLP systems, starting from lin-
guistic knowledge. Grammars have to represent all
common linguistic phenomena and lexicons have
to include the most frequent words with their most
frequent uses. As everyone knows, building such
resources is a very complex and time consuming
task.
When one wants to formalize linguistic knowl-
edge, a crucial question arises: which mathemat-
ical framework to choose? Currently, there is no
agreement on the choice of a formalism in the sci-
entific community. Each of the most popular for-
malisms has its own advantages and drawbacks. A
good formalism must have three properties, hard to
conciliate: it must be sufficiently expressive to rep-
resent linguistic generalizations, easily readable by
linguists and computationally tractable. Guided
by those principles, we advocate a recent formal-
ism, Interaction Grammars (IGs) (Perrier, 2003),
the goal of which is to synthesize two key ideas,
expressed in two kinds of formalisms up to now:
using the resource sensitivity of natural languages
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
as a principle of syntactic composition, which is
a characteristic feature of Categorial Grammars
(CG) (Retor?e, 2000), and viewing grammars as
constraint systems, which is a feature of unifica-
tion grammars such as LFG (Bresnan, 2001) or
HPSG (Pollard and Sag, 1994).
Researchers who develop large lexicons and
grammars from linguistic knowledge are con-
fronted to the contradiction between the necessity
to choose a specific grammatical framework and
the cost of developing resources for this frame-
work. One of the most advanced systems de-
voted to such a task is LKB (Copestake, 2001).
LKB allows grammars and lexicons to be devel-
oped for different languages, but only inside the
HPSG framework, or at most a typed feature struc-
ture framework. Therefore, all produced resources
are hardly re-usable for other frameworks. Our
goal is to design a toolchain that is as much as pos-
sible re-usable for other frameworks than IG.
Our toolchain follows the following architecture
(see Figure 1):
? First, for building grammars, we use XMG
(Section 3.1) which translates the source
grammar into an object grammar.
? IGs that we have developed with XMG are
all lexicalized. Therefore, the object grammar
has to be anchored in a lexicon (Section 3.2)
in order to produce the anchored grammar.
? Then, when analyzing a sentence, we start
with a lexical disambiguation module (Sec-
tion 3.3).
? The resulting lexical selections, presented in
the compact form of an automaton, are finally
sent to the LEOPAR parser (Section 3.4).
153
LEOPAR
source grammar
XMG
object grammar lexicons
anchoring
input sentence
lexical disambiguation
output parse trees
anchored grammar
automaton
parsing
Figure 1: Toolchain architecture
2 Interaction Grammars
IGs (Perrier, 2003) are a grammatical formalism
based on the notion of polarity. Polarities express
the resource sensitivity of natural languages by
modeling the distinction between saturated and un-
saturated syntactic structures. Syntactic composi-
tion is represented as a chemical reaction guided
by the saturation of polarities. In a more precise
way, syntactic structures are underspecified trees
equipped with polarities expressing their satura-
tion state. They are superposed under the con-
trol of polarities in order to saturate them. In
CG, Tree Adjoining Grammars (TAGs) and De-
pendency Grammars, syntactic composition can
also be viewed as a mechanism for saturating po-
larities, but this mechanism is less expressive be-
cause node merging is localized at specific places
(root nodes, substitution nodes, foot nodes, ad-
junction nodes . . .). In IGs, tree superposition is
a more flexible way of realizing syntactic compo-
sition. Therefore, it can express sophisticated con-
straints on the environment in which a polarity has
to be saturated. From this angle, IGs are related
to Unification Grammars, such as HPSG, because
tree superposition is a kind of unification, but with
an important difference: polarities play an essen-
tial role in the control of unification.
3 Description of the Toolchain
3.1 The XMG Grammar Compiler
The first piece of software in our toolchain is
XMG
1
(Duchier et al, 2004), a tool used to de-
velop grammars. XMG addresses the issue of de-
signing wide-coverage grammars: it is based on a
distinction between source grammar, written by a
human, and object grammar, used in NLP systems.
XMG provides a high level language for writing
source grammars and a compiler which translates
those grammars into operational object grammars.
XMG is particularly adapted to develop lexical-
ized grammars. In those grammars, parsing a sen-
tence amounts to combining syntactical items at-
tached to words. In order to have an accurate lan-
guage model, it may be necessary to attach a huge
number of syntactical items to some words (verbs
and coordination words, in particular) that describe
the various usages of those words. In this context,
a grammar is a collection of items representing
syntactical behaviors. Those items, although dif-
ferent from each other, often share substructures
(for instance, almost all verbs have a substruc-
ture for subject verb agreement). That is to say,
if a linguist wants to change the way subject-verb
agreement is modeled, (s)he would have to mod-
ify all the items containing that substructure. This
is why designing and maintaining strongly lexical-
ized grammars is a difficult task.
The idea behind the so-called metagrammati-
cal approach is to write only substructures (called
fragments) and then add rules that describe the
combinations (expressed with conjunctions, dis-
junctions and unifications) of those fragments to
obtain complete items.
Fragments may contain syntactic, morpho-
syntactic and semantic pieces of information. An
object grammar is a set of structures containing
syntactic and semantic information, that can be an-
chored using morpho-syntactic information stored
in the interface of the structure (see Section 3.2).
During development and debugging stages, por-
tions of the grammar can be evaluated indepen-
dently. The grammar can be split into various mod-
ules that can be shared amongst grammars. Fi-
nally, graphical tools let the users explore the in-
heritance hierarchy and the partial structures be-
fore complete evaluation.
1
XMG is freely available under the CeCILL license at
http://sourcesup.cru.fr/xmg
154
XMG is also used to develop TAGs (Crabb?e,
2005) and it can be easily extended to other gram-
matical frameworks based on tree representations.
3.2 Anchoring the Object Grammar with a
Lexicon
The tool described in the previous section builds
the set of elementary trees of the grammar. The
toolchain includes a generic anchoring mechanism
which allows to use formalism independent lin-
guistic data for the lexicon part.
Each structure produced by XMG comes with
an interface (a two-level feature structure) which
describes morphological and syntactical con-
straints used to select words from the lexicon. Du-
ally, in the lexicon, each inflected form of the natu-
ral language is described by a set of two-level fea-
ture structures that contain morphological and syn-
tactical information.
If the interface of an unanchored tree unifies
with some feature structure associated with w in
the lexicon, then an anchored tree is produced for
the word w.
The toolchain also contains a modularized lexi-
con manager which aims at easing the integration
of external and formalism independent resources.
The lexicon manager provides several levels of lin-
guistic description to factorize redundant data. It
also contains a flexible compilation mechanism to
improve anchoring efficiency and to ease lexicon
debugging.
3.3 Lexical Disambiguation
Neutralization of polarities is the key mechanism
in the parsing process as it is used to control syn-
tactic composition. This principle can also be used
to filter lexical selections. For a input sentence, a
lexical selection is a choice of an elementary tree
from the anchored grammar for each word of the
sentence.
Indeed, the number of possible lexical selec-
tions may present an exponential complexity in
the length of the sentence. A way of filter-
ing them consists in abstracting some information
from the initial formalism F to a new formalism
F
abs
. Then, parsing in F
abs
allows to eliminate
wrong lexical selections at a minimal cost (Boul-
lier, 2003). (Bonfante et al, 2004) shows that po-
larities allow original methods of abstraction.
Following this idea, the lexical disambiguation
module checks the global neutrality of every lex-
ical selection for each polarized feature: a set of
trees bearing negative and positive polarities can
only be reduced to a neutral tree if the sum of the
negative polarities for each feature equals the sum
of its positive polarities.
Counting the sum of positive and negative fea-
tures can be done in a compact way by using an au-
tomaton. This automaton structure allows to share
all paths that have the same global polarity bal-
ance (Bonfante et al, 2004).
3.4 The LEOPAR Parser
The next piece of software in our toolchain is a
parser based on the IGs formalism
2
. In addition
to a command line interface, the parser provides
an intuitive graphical user interface. Parsing can
be highly customized in both modes. Besides, the
processed data can be viewed at each stage of the
analysis via the interface so one can easily check
the behavior of the grammar and the lexicons in
the parsing process.
The parsing can also be done manually: one first
chooses a lexical selection of the sentence given
by the lexer and then proceeds to the analysis by
neutralizing nodes from the selection. This way,
the syntactic composition can be controlled by the
user.
4 Results
Our toolchain has been used first to produce a large
coverage French IG. Most of the usual syntactical
constructions of French are covered. Some non
trivial constructions covered by the grammar are,
for instance: coordination, negation (in French,
negation is expressed with two words with com-
plex placement rules), long distance dependencies
(with island constraints). The object grammar con-
tains 2,074 syntactic structures which are produced
by 455 classes in the source grammar.
The French grammar has been tested on the
French TSNLP (Test Suite for the Natural Lan-
guage Processing) (Lehmann et al, 1996); this test
suite contains around 1,300 grammatical sentences
and 1,600 ungrammatical ones. The fact that our
grammar is based on linguistic knowledge ensures
a good coverage and greatly limits overgeneration:
88% of the grammatical sentences are correctly
parsed and 85% of the ungrammatical sentences
are rejected by our grammar.
2
LEOPAR is freely available under the CeCILL license at
http://www.loria.fr/equipes/calligramme/
leopar
155
A few months ago, we started to build an En-
glish IG. The modularity of the toolchain was an
advantage to build this grammar by abstracting the
initial grammar and then specifying the abstract
kernel for English. The English TSNLP has been
used to test the new grammar: 85% of the gram-
matical sentences are correctly parsed and 84%
of the ungrammatical sentences are rejected. It is
worth noting that those scores are obtained with a
grammar that is still being developed .
5 Future work
The toolchain we have presented here aims at pro-
ducing grammars and lexicons with large cover-
age from linguistic knowledge. This justifies the
choice of discarding statistical methods in the first
stage of the toolchain development: in the two
steps of lexical disambiguation and parsing, we
want to keep all possible solutions without dis-
carding even the less probable ones. Now, in a
next future, we have the ambition of using the
toolchain for parsing large raw corpora in differ-
ent languages.
For French, we have a large grammar and a large
lexicon, which are essential for such a task. The in-
troduction of statistics in the two modules of lexi-
cal disambiguation and parsing will contribute to
computational efficiency. Moreover, we have to
enrich our parsing strategies with robustness. We
also ambition to integrate semantics into grammars
and lexicons.
Our experience with English is a first step to take
multi-linguality into account. The crucial point
is to make our grammars evolve towards an even
more multi-lingual architecture with an abstract
kernel, common to different languages, and differ-
ent specifications of this kernel for different lan-
guages, thus following the approach of the Gram-
matical Framework (Ranta, 2004).
Finally, to make the toolchain evolve towards
multi-formalism, it is first necessary to extend
XMG for more genericity; there is no fundamental
obstacle to this task. Many widespread formalisms
can then benefit from our original methods of lex-
ical disambiguation and parsing, based on polari-
ties. (Kahane, 2006) presents the polarization of
several formalisms and (Kow, 2007) shows that
this way is promising.
References
Bonfante, G., B. Guillaume, and G. Perrier. 2004. Po-
larization and abstraction of grammatical formalisms
as methods for lexical disambiguation. In CoL-
ing?2004, 2004, pages 303?309, Geneva, Switzer-
land.
Boullier, P. 2003. Supertagging: A non-statistical
parsing-based approach. In IWPT 03, pages 55?65,
Nancy, France.
Bresnan, J. 2001. Lexical-Functional Syntax. Black-
well Publishers, Oxford.
Copestake, A. 2001. Implementing Typed Feature
Structure Grammars. CSLI Publications.
Crabb?e, B. 2005. Repr?esentation informatique de
grammaires fortement lexicalis?ees : application `a la
grammaire d?arbres adjoints. Phd thesis, Universit?e
Nancy 2.
Duchier, D., J. Le Roux, and Y. Parmentier. 2004.
The metagrammar compiler : A NLP Application
with a Multi-paradigm Architecture. In Second
International Mozart/Oz Conference - MOZ 2004,
Charleroi, Belgium.
Kahane, S. 2006. Polarized unification grammars.
In 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 137?144,
Sydney, Australia.
Kow, E. 2007. Surface realisation: ambiguity and de-
terminism. Phd thesis, Universit?e Nancy 2.
Lehmann, S., S. Oepen, S. Regnier-Pros, K. Netter,
V. Lux, J. Klein, K. Falkedal, F. Fouvry, D. Estival,
E. Dauphin, H. Compagnion, J. Baur, L. Balkan, and
D. Arnold. 1996. TSNLP ? Test Suites for Natu-
ral Language Processing. In CoLing 1996, Kopen-
hagen.
Perrier, G. 2003. Les grammaires d?interaction. Ha-
bilitation thesis, Universit?e Nancy 2.
Pollard, C.J. and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Ranta, A. 2004. Grammatical Framework: A Type-
Theoretical Grammar Formalism. Journal of Func-
tional Programming, 14(2):145?189.
Retor?e, C. 2000. The Logic of Categorial Grammars.
ESSLI?2000, Birmingham.
156
Modular Graph Rewriting to Compute Semantics
Guillaume Bonfante
Nancy-Universite? - LORIA
bonfante@loria.fr
Bruno Guillaume
INRIA - LORIA
guillaum@loria.fr
Mathieu Morey
Nancy-Universite? - LORIA
moreymat@loria.fr
Guy Perrier
Nancy-Universite? - LORIA
perrier@loria.fr
Abstract
Taking an asynchronous perspective on the syntax-semantics interface, we propose to use modu-
lar graph rewriting systems as the model of computation. We formally define them and demonstrate
their use with a set of modules which produce underspecified semantic representations from a syn-
tactic dependency graph. We experimentally validate this approach on a set of sentences. The results
open the way for the production of underspecified semantic dependency structures from corpora an-
notated with syntactic dependencies and, more generally, for a broader use of modular rewriting
systems for computational linguistics.
Introduction
The aim of our work is to produce a semantic representation of sentences on a large scale using a formal
and exact approach based on linguistic knowledge. In this perspective, the design of the syntax-semantics
interface is crucial.
Based on the compositionality principle, most models of the syntax-semantics interface use a syn-
chronous approach: the semantic representation of a sentence is built step by step in parallel with its
syntactic structure. According to the choice of the syntactic formalism, this approach is implemented in
different ways: in a Context-Free Grammars (CFG) style framework, every syntactic rule of a grammar
is associated with a semantic composition rule, as in the classical textbook by Heim and Kratzer (1998);
following the principles introduced by Montague, Categorial Grammars use an homomorphism from the
syntax to the semantics (Carpenter (1992)). HPSG integrates the semantic and syntactic representations
in feature structures which combine by unification (Copestake et al (2005)). LFG follows a similar prin-
ciple (Dalrymple (2001)). In a synchronous approach, the syntax-semantics interface closely depends on
the grammatical formalism. Building such an interface can be very costly, especially if we aim at a large
coverage for the grammar.
In our work, we have chosen an asynchronous approach in the sense that we start from a given
syntactic analysis of a sentence to produce a semantic representation. With respect to the synchronous
approach, a drawback is that the reaction of the semantics on the syntax is delayed. On the other hand,
the computation of the semantics is made relatively independent from the syntactic formalism. The only
constraint is the shape of the output of the syntactic analysis.
In the formalisms mentioned above, the syntactic structure most often takes the form of a phrase
structure, but the choice of constituency for the syntax makes the relationship with the semantics more
complicated. We have chosen dependency graphs, because syntactic dependencies are closely related
to predicate-argument relations. Moreover, they can be enriched with relations derived from the syntax,
which are usually ignored, such as the arguments of infinitives or the anaphora determined by the syntax.
One may observe that our syntactic representation of sentences involves plain graphs and not trees.
Indeed, these relations can give rise to multiple governors and dependency cycles. On the semantic side,
65
we have also chosen graphs, which are widely used in different formalisms and theories, such as DMRS
(Copestake (2009)) or MTT (Mel?c?uk (1988)) .
The principles being fixed, our problem was then to choose a model of computation well suited
to transforming syntactic graphs into semantic graphs. The ?-calculus, which is widely used in formal
semantics, is not a good candidate because it is appropriate for computing on trees but not on graphs. Our
choice naturally went to graph rewriting. Graph rewriting is barely used in computational linguistics;
it could be due to the difficulty to manage large sets of rules. Among the pioneers in the use of graph
rewriting, we mention Hyvo?nen (1984); Bohnet and Wanner (2001); Crouch (2005); Jijkoun and de Rijke
(2007); Be?daride and Gardent (2009); Chaumartin and Kahane (2010).
A graph rewriting system is defined as a set of graph rewrite rules and a computation is a sequence
of rewrite rule applications to a given graph. The application of a rule is triggered via a mechanism of
pattern matching, hence a sub-graph is isolated from its context and the result is a local modification of
the input. This allows a linguistic phenomenon to be easily isolated for applying a transformation.
Since each step of computation is fired by some local conditions in the whole graph, it is well known
that one has no grip on the sequence of rewriting steps. The more rules, the more interaction between
rules, and the consistency of the whole rule system becomes difficult to maintain. This bothers our
ambition of a large coverage for the grammar. To solve this problem, we propose to organize rules in
modules. A module is a set of rules that is linguistically consistent and represents a particular step of
the transformation. For instance, in our proposal, there is a module transforming the syntactic arguments
of verbs, predicative nouns and adjectives into their semantic arguments. Another module resolves the
anaphoric links which are internal to the sentence and determined by the syntax.
From a computational point of view, the grouping of a small number of rules inside a module allows
some optimizations in their application, thus leading to efficiency. For instance, the confluence of rewrit-
ing is a critical feature ? one computes only one normal form, not all of them ? for the performance
of the program. Since the underlying relation from syntax to semantics is not functional but relational,
the system cannot be globally confluent. Then, it is particularly interesting to isolate subsets of conflu-
ent rules. Second point, with a small number of rules, one gets much more control on their output. In
particular, it is possible to automatically infer some invariant properties of graphs along the computation
within a particular module. Thus, it simplifies the writing of the rules for the next modules. It is also
possible to plan a strategy in the global evaluation process.
It is well known that syntactic parsers produce outputs in various formats. As a by-product of our
approach, we show that the choice of the input format (that is the syntax) seems to be of low importance
overall. Indeed, as far as two formats contain the same linguistic information with different representa-
tions, a system of rewrite rules can be designed to transform any graph from one format to another as a
preliminary step. The same remark holds for the output formats.
To illustrate our proposal, we have chosen the Paris7 TreeBank (hereafter P7TB) dependency format
defined by Candito et al (2010) as the syntactic input format and the Dependency MRS format (hereafter
DMRS) defined by Copestake (2009) as the semantic output format. We chose those two formats because
the information they represent, if it is not complete, is relatively consensual and because both draw on
large scale experiments: statistical dependency parsing for French1 on the one hand and the DELPH-IN
project2 on the other hand.
Actually, in our experiments, since we do not have an appropriate corpus annotated according to the
P7TB standard, we used our syntactic parser LEOPAR3 whose outputs differ from this standard and we
designed a rewriting system to go from one format to the other.
The paper is organized as follows. In section 1, we define our graph rewriting calculus, the ?-calculus.
In Section 2, we describe the particular rewriting system that is used to transform graphs from the syn-
tactic P7TB format into the DMRS semantic format. In Section 3, we present experimental results on a
test suite of sentences.
1http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html
2http://www.delph-in.net/
3http://leopar.loria.fr
66
1 The ?-calculus, a graph rewriting calculus
Term rewriting and tree rewriting can be defined in a straightforward and canonical way. Graph rewriting
is much more problematic and there is unfortunately no canonical definition of a graph rewriting system.
Graph rewriting can be defined through a categorical approach like SPO or DPO (Rozenberg (1997)).
But, in practice, it is much easier to use a more operational view of rewriting where modification of
the graph (the ?right-hand side? of a rule) is defined by means of a set of commands; the control of the
way rules are applied (the ?left hand-side?) still uses pattern matching as this is done in traditional graph
rewriting.
In this context, a rule is a pair of a pattern and a sequence of commands. We give below the formal
materials about graphs, patterns, matchings and commands. We illustrate the section with examples of
rules and of rewriting.
1.1 Graph definition
In the following, we suppose given a finite set L of edge labels corresponding to the kind of dependencies
used to describe sentences. They may correspond to syntax or to semantics. For instance, we use
L = {SUJ, OBJ, ARG1, ANT, . . .}.
To decorate vertices, we use the standard notion of feature structures. Let N be a finite set of
feature names and A be a finite set of atomic feature values. In our example, N = {cat,mood, . . .} and
A = {passive, v, n, . . .}. A feature is a pair made of a feature name and a set of atomic values. The
feature (cat, {v, aux}) means that the feature name cat is associated to either the value v or aux. In the
sequel, we use the notation cat = v|aux for this feature. Two features f = v and f ? = v? are compatible
whenever f = f ? and v ? v? 6= ?.
A feature structure is a finite set of features such that each feature name occurs at most once. F de-
notes the set of feature structures. Two feature structures are compatible if their respective features with
the same name are pairwise compatible.
A graph G is then defined by a 6-tuple (V, fs, E , lab, ?, ?) with:
? a finite set V of vertices;
? a labelling function fs from V to F ;
? a finite set E of edges;
? a labelling function lab from E to L;
? two functions ? and ? from E to V which give the source and the target of each edge.
Moreover, we require that two edges between the same couple of nodes cannot have the same label.
1.2 Patterns and matchings
Formally, a pattern is a graph and a matching ? of a pattern P = (V ?, fs?, E ?, lab?, ??, ? ?) into a graph
G = (V, fs, E , lab, ?, ?) is an injective graph morphism from P to G. More precisely, ? is a couple of
injective functions: ?V from V ? to V and ?E from E ? to E which:
? respects vertex labelling: fs(?V(v)) and fs?(v) are compatible;
? respects edge labelling: lab(?E(e)) = lab?(e);
? respects edge sources: ?(?E(e)) = ?V(??(e));
? respects edge targets: ?(?E(e)) = ?V(? ?(e)).
67
1.3 Commands
Commands are low-level operations on graphs that are used to describe the rewriting of the graph within
a rule application. In the description below, we suppose to be given a pattern matching ? : P ? G. We
describe here the set of commands which we used in our experiment so far. Naturally, this set could be
extended.
? del edge(?, ?, `) removes the edge labelled ` between ? and ?. More formally, we suppose that
? ? VP , ? ? VP andP contains an edge e from? to ? with label ` ? L. Then, del edge(?, ?, `)(G)
is the graph G without the edge ?(e). In the following, we give only the intuitive definition of the
command: thanks to injectivity of the matching ?, we implicitly forget the distinction between x
and ?(x).
? add edge(?, ?, `) adds an edge labelled ` between ? and ?. Such an edge is supposed not to exist
in G.
? shift edge(?, ?) modifies all edges that are incident to ?: each edge starting from ? is moved to
start from ?; similarly each edge ending on ? is moved to end on ?;
? del node(?) removes the ? node in G. If G contains edges starting from ? or ending on ?, they
are silently removed.
? add node(?) adds a new node with identifier ? (a fresh name).
? add feat(?, f = v) adds the feature f = v to the node ?. If ? already contains a feature name f ,
it is replaced by the new one.
? copy feat(?, ?, f) copies the value of the feature named f from the node ? to the node ?. If ?
does not contain a feature named f , nothing is done. If ? already contains a feature named f , it is
replaced by the new value.
Note that commands define a partial function on graphs: the action add edge(?, ?, `) is undefined
on a graph which already contains an edge labelled ` from ? to ?.
The action of a sequence of commands is the composition of actions of each command. Sequences
of commands are supposed to be consistent with the pattern:
? del edge always refers to an edge described in the pattern and not previously modified by a
del edge or a shift edge command;
? each command refers only to identifiers defined either in the pattern or in a previous add node;
? no command refers to a node previously deleted by a del node command.
Finally, we define a rewrite rule to be a pair of a pattern and a consistent sequence of commands.
A first example of a rule is given below with the pattern on the left and the sequence of commands
on the right. This rule called INIT PASSIVE is used to remove the node corresponding to the auxiliary
of the passive construction and to modify the features accordingly.
INIT PASSIVE
?
cat = v
voice = active
?
cat = v
voice = unk
AUX PASS
c1 = copy feat(?, ?,mood)
c2 = copy feat(?, ?, tense)
c3 = add feat(?, voice = passive)
c4 = del edge(?, ?, AUX PASS)
c5 = shift edge(?, ?)
c6 = del node(?)
Our second example (PASSIVE ATS) illustrates the add node command. It is used in a passive
construction where the semantic subject of the verb is not realized syntactically.
68
PASSIVE ATS
?
cat = v
voice = passive
? ?
SUJ ATS c1 = del edge(?, ?, SUJ)
c2 = add edge(?, ?, OBJ)
c3 = del edge(?, ?, ATS)
c4 = add edge(?, ?, ATO)
c5 = add feat(?, voice = active)
c6 = add node(?)
c7 = add edge(?, SUJ, ?)
1.4 Rewriting
We consider a graph G and a rewrite rule r = (P, [c1, . . . , ck]). We say that G? is obtained from G by a
rewrite step with the r rule (written G ??r G?) if there is a matching morphism ? : P ? G and G? is
obtained from G by applying the composition of commands ck ? . . . ? c1.
Let us now illustrate two rewrite steps with the rules above. Consider the first graph below which is
a syntactic dependency structure for the French sentence ?Marie est conside?re?e comme brillante? [Mary
is considered as bright]. The second graph is obtained by application of the INIT PASSIVE rewrite rule
and the last one with the PASSIVE ATS rewrite rule.
Marie
cat = np
lemma = MARIE
est
cat = v
lemma = E?TRE
voice = active
tense = present
conside?re?e
cat = v
lemma = CONSIDE?RER
voice = unk
comme
cat = prep
lemma = COMME
brillante
cat = adj
lemma = BRILLANT
SUJ
AUX PASS ATS OBJ
Marie
cat = np
lemma = MARIE
est conside?re?e
cat = v
lemma = CONSIDE?RER
voice = passive
tense = present
comme
cat = prep
lemma = COMME
brillante
cat = adj
lemma = BRILLANT
SUJ ATS OBJ
 Marie
cat = np
lemma = MARIE
est conside?re?e
cat = v
lemma = CONSIDE?RER
voice = active
tense = present
comme
cat = prep
lemma = COMME
brillante
cat = adj
lemma = BRILLANT
SUJ
OBJ ATO OBJ
1.5 Modules and normal forms
A module contains a set of rewrite rules but, in order to have a finer control on the output of these
modules, it is useful to declare some forbidden patterns. Hence a module is defined by a set R of rules
and a set P of forbidden patterns.
For a given module M = (R,P), we say that G? is an M-normal form of the graph G if there is a
sequence of rewriting steps with rules of R from G to G?: G ??r1 G1 ??r2 G2 . . . ??rk G?, if no rule
of R can be applied to G? and no pattern of P matches in G?.
In our experiment, forbidden patterns are often used to control the subset of edges allowed in normal
forms. For instance, the NORMAL module contains the forbidden pattern: AUX PASS . Hence, we
can then safely suppose that no graph contains any AUX PASS edge afterward.
2 From syntactic dependency graphs to semantic graphs
Linguistic theories diverge on many issues including the exact definition of the linguistic levels and
the relationships between them. Our aim here is not to commit to any linguistic theory but rather to
69
demonstrate that graph rewriting is an adequate and realistic computational framework for the syntax-
semantics interface. Consequently, our approach is bound to neither the (syntactic and semantic) formats
we have chosen nor the transformation modules we have designed; both are mainly meant to exemplify
our proposal.
2.1 Representational formats
Our syntactic and semantic formats both rely on the notion of linguistic dependency. The syntactic
format is an enrichment of the one which was designed to annotate the French Treebank (Abeille? and
Barrier (2004)) with surface syntactic dependencies (Candito et al (2010)). The enrichment is twofold:
? if they are present in the sentence, the deep arguments of infinitives and participles (from participial
subordinate clauses) are marked with the usual labels of syntactic functions,
? the anaphora relations that are predictable from the syntax (i.e. the antecedents of relative, reflexive
and repeated pronouns) are marked with a special label ANT.
This additional information can already be provided by many syntactic parsers and is particularly inter-
esting to compute semantics.
The semantic format is DependencyMinimal Recursion Semantics (DMRS) which was introduced by
Copestake (2009) as a compact and easily readable equivalent to Robust Minimal Recursion Semantics
(RMRS), which was defined by Copestake (2007). This underspecified semantic formalism was designed
for large scale experiments without committing to fine-grained semantic choices. DMRS graphs contain
the predicate-argument relations, the restriction of generalized quantifiers and the mode of combination
between predicates. Predicate-argument relations are labelled ARGi, where i is an integer following a
fixed order of obliqueness SUJ, OBJ, ATS, ATO, A-OBJ, DE-OBJ. . . . Naturally, the lexicon must be consistent
with this ordering. The restrictions of generalized quantifiers are labelled RSTR ; their bodies are not
overtly expressed but can be retrieved from the graph. There are three ways of combining predicates:
? EQ when two predicates are elements of a same conjunction;
? H when a predicate is in the scope of another predicate; it is not necessarily one of its arguments
because quantifiers may occur between them;
? NEQ for all other cases.
2.2 Modular rewriting system
Graph rewriting allows to proceed step by step to the transformation of a syntactic graph into a semantic
one, by associating a rewrite rule to each linguistic rule. While the effect of every rule is local, grouping
rules in modules allows a better control on the global effect of all rules.
We do not have the space here to propose a system of rules that covers the whole French grammar.
We however propose six modules which cover a significative part of this grammar (cleft clauses, coor-
dination, enumeration, comparatives and ellipses are left aside but they can be handled by other rewrite
modules):
? NORMAL handles the regular syntactic transformations involving predicates: it computes tense
and transforms all redistributions of arguments (passive and middle voices, impersonal construc-
tions and the combination of them) to the active canonical form. This reduces the number of rules
required to produce the predicate-argument relations in the ARG module below.
? PREP removes affixes, prepositions and complementizers.
? ARG transforms the verbal, nominal and adjectival predicative phrases into predicate-argument
relations.
70
? DET translates the determiner dependencies (denoted DET) to generalized quantifiers.
? MOD interprets the various modifier dependencies (denoted MOD), according to their specificity:
adjectives, adverbs, adjunct prepositional phrases, participial clauses, relative clauses, adjunct
clauses.
? ANA interprets all anaphoric relations that are determined by the syntax (denoted ANT).
Modules provide an easy way to control the order in which rules are fired. In order to properly set up the
rules in modules, we first have to fix the global ordering of the modules. Some ordering constraints are
evident: for instance, NORMAL must precede PREP, which must precede ARG. The rules we present in
the following are based on the order NORMAL, PREP, ARG, DET, MOD, ANA.
2.2.1 Normalization of syntactic dependencies
The NORMAL module has two effects: it merges tense and voice auxiliaries with their past participle
and brings all the argument redistributions back to the canonical active form. This module accounts
for the passive and middle voices and the impersonal construction for verbs that are not essentially
impersonal. The combination of the two voices with the impersonal construction is naturally expressed
by the composition of the corresponding rewrite rules. The two rules given in section 1.4 are part of this
module. The first rule (INIT PASSIVE) merges the past participle of the verb with its passive auxiliary.
The auxiliary brings its mood and tense to the verb, which is marked as being passive. The second rule
(PASSIVE ATS) transforms a passive verb with a subject and an attribute of the subject into its active
equivalent with a semantically undetermined subject, an object (which corresponds to the subject of the
passive form) and an attribute of the object (which corresponds to the attribute of the subject of the
passive form).
2.2.2 Erasure of affixes, prepositions and complementizers
The PREP module removes affixes, prepositions and complementizers. For example, the rule given here
merges prepositions with the attribute of the object that they introduce. The value of the preposition is
kept to compute the semantics.
PREP ATO
?
voice = active
?
cat = prep
prep = ?
?
ATO OBJ c1 = copy feat(?, ?, prep)
c2 = del edge(?, ?, OBJ)
c3 = shift edge(?, ?)
c4 = del node(?)
2.2.3 From lexical predicative phrases to semantic predicates
The ARG module transforms the syntactic arguments of a predicative word (a verb, a common noun or
an adjective) into its semantic arguments. Following DMRS, the predicate-argument relations are not
labelled with thematic roles but only numbered. The numbering reflects the syntactic obliqueness.
ARG OBJ
? ?
cat = n|np|pro
OBJ
c1 = del edge(?, ?, OBJ)
c2 = add edge(?, ?, ARG2)
c3 = add edge(?, ?, NEQ)
2.2.4 From determiners to generalized quantifiers
DET reverts the determiner dependencies (labelled DET) from common nouns to determiners into depen-
dencies of type RSTR from the corresponding generalized quantifier to the nominal predicate which is
the core of their restriction.
71
DET
?
cat = det
?
cat = n
DET
c1 = del edge(?, ?, DET)
c2 = add edge(?, ?, RSTR)
c3 = add edge(?, ?, H)
2.2.5 Interpretation of different kinds of modification
MOD deals with the modifier dependencies (labelled MOD, MOD REL and MOD LOC), providing rules
for the different kinds of modifiers. Adjectives and adverbs are translated as predicates whose first
argument is the modified entity. The modifier and modified entities are in a conjunction (EQ), except
for scopal adverbs which take scope (H) over the modified predicate. Because only lexical information
enables to differentiate scopal from non-scopal adverbs, we consider all adverbs to be systematically
ambiguous at the moment. Adjunct prepositional phrases (resp. clauses) have a similar rule except that
their corresponding predicate is the translation of the preposition (resp. complementizer), which has
two arguments: the modified entity and the noun (resp. verb) which heads the phrase (resp. clause).
Participial and relative clauses exhibit a relation labelled EQ or NEQ between the head of the clause and
the antecedent, depending on the restrictive or appositive type of the clause.
2.2.6 Resolution of syntactic anaphora
ANA deals with dependencies of type ANT and merges their source and their target. We apply them to
reflexive, relative and repeated pronouns.
3 Experiments
For the experimentation, we are interested in a test suite which is at the same time small enough to be
manually validated and large enough to cover a rich variety of linguistic phenomena. As said earlier, we
use the P7 surface dependency format as input, so the first attempt at building a test suite is to consider
examples in the guide which describes the format. By nature, an annotation guide tries to cover a large
range of phenomena with a small set of examples.
The latest version4 of this guide (Candito et al (2010)) contains 186 linguistic examples. In our cur-
rent implementation of the semantic constructions, we leave out clefts, coordinations and comparatives.
We also leave out a small set of exotic sentences for which we are not able to give a sensible syntactic
structure. Finally, our experiment runs on 116 French sentences. Syntactic structures following P7 spec-
ifications are obtained with some graph rewriting on the output of our parser. Each syntactic structure
was manually checked and corrected when needed. Then, graph rewriting with the modules described in
the previous section is performed.
For all of these sentences, we produce at least one normal form. Even if DMRS is underspecified, our
system can output several semantic representations for one syntactic structure (for instance, for appositive
and restrictive relative clauses). We sometimes overgenerate because we do not use lexical information
like the difference between scopal and non-scopal adverbs.
The result for three sentences is given below and the full set is available on a web page 5.
4version 1.1, january 2010
5http://leopar.loria.fr/doku.php?id=iwcs2011
72
[012] ?Le franc?ais se parle de moins en moins dans les confe?rences.? [The French language is less and
less spoken in conferences.]
le
cat=det
fran?ais
cat=n
se
cat=pro
parle
cat=v
mood=ind
tense=pres
voice=unk
de moins en moins
cat=adv
dans
cat=prep
prep=loc
les
cat=det
conf?rences
cat=n
DET AFF_MOYEN MOD DET
SUJ MOD_LOC OBJ
/la/ct=dea=
/s?tritnR/ct=dr
S THPT
/pt?la/ct=do?vvednre=arRadp?aRovncadtc=noa
mTEQ NAG
//
mTE1 NAG
/eaf?vnrRfarf?vnrR/ct=dteo
mTE1 AG
/etrR/ct=dp?app?apdlvc
AG mTE1
/cvrs2?arcaR/ct=dr
NAG mTEQ
/laR/ct=dea=
S THPT
[057] ?J?encourage Marie a` venir.? [I invite Mary to come.]
je
cat=pro
encourage
cat=v
mood=ind
tense=pres
voice=unk
Marie
cat=np
?
cat=prep
prep=?
venir
cat=v
mood=inf
voice=unk
SUJ OBJ OBJ
A-OBJ
SUJ
/je/cat=pro
/encourage/cat=vmood=indtense=presvoice=active
ARG1 NEQ
/Marie/cat=np
ARG2 NEQ
/venir/cat=vmood=infprep=?voice=active
ARG3 EQ
ARG1 NEQ
[106] ?La se?rie dont Pierre conna??t la fin? [The story Peter knows the end of]
la
cat=det
s?rie
cat=n
dont
cat=pro
Pierre
cat=np
conna?t
cat=v
mood=ind
tense=pres
voice=unk
la
cat=det
fin
cat=n
DET ANT SUJ DET
OBJMOD_REL
DE-OBJ
/la/cat=det
/s?rie/cat=n
RSTR H
/Pierre/cat=np
/conna?t/cat=vmood=indtense=presvoice=active
EQ
NEQ ARG1
/fin/cat=n
NEQ ARG2
/la/cat=det
RSTR H
ARG1 NEQ
73
Conclusion
In this paper, we have shown the relevance of modular graph rewriting to compute semantic representa-
tions from graph-shaped syntactic structures. The positive results of our experiments on a test suite of
varied sentences make us confident that the method can apply to large corpora.
The particular modular graph rewriting system presented in the paper was merely here to illustrate
the method, which can be used for other input and output formats. There is another aspect to the flexi-
bility of the method: we may start from the same system of rules and enrich it with new rules to get a
finer semantic analysis ? if DMRS is considered as providing a minimal analysis ? or integrate lexi-
cal information. The method allows the semantic ambiguity to remain unsolved within underspecified
representations or to be solved with a rule system aiming at computing models of underspecified rep-
resentations. Moreover, we believe that its flexibility makes graph rewriting a convenient framework to
deal with idiomatic expressions.
References
Abeille?, A. and N. Barrier (2004). Enriching a french treebank. In Proceedings of LREC.
Be?daride, P. and C. Gardent (2009). Semantic Normalisation : a Framework and an Experiment. In
Proceedings of IWCS, Tilburg Netherlands.
Bohnet, B. and L. Wanner (2001). On using a parallel graph rewriting formalism in generation. In
Proceedings of EWNLG ?01, pp. 1?11. Association for Computational Linguistics.
Candito, M., B. Crabbe?, and P. Denis (2010). Statistical french dependency parsing: Treebank conversion
and first results. Proceedings of LREC2010.
Candito, M., B. Crabbe?, and M. Falco (2010). De?pendances syntaxiques de surface pour le fran?cais.
Carpenter, B. (1992). The logic of typed feature structures. Cambridge: Cambridge University Press.
Chaumartin, F.-R. and S. Kahane (2010). Une approche paresseuse de l?analyse se?mantique ou comment
construire une interface syntaxe-se?mantique a` partir d?exemples. In TALN 2010, Montreal, Canada.
Copestake, A. (2007). Semantic composition with (robust) minimal recursion semantics. In Proceedings
of the Workshop on Deep Linguistic Processing, pp. 73?80. Association for Computational Linguistics.
Copestake, A. (2009). Invited Talk: Slacker semantics: Why superficiality, dependency and avoidance
of commitment can be the right way to go. In Proceedings of EACL 2009, Athens, Greece, pp. 1?9.
Copestake, A., D. Flickinger, C. Pollard, and I. Sag (2005). Minimal Recursion Semantics - an Introduc-
tion. Research on Language and Computation 3, 281?332.
Crouch, D. (2005). Packed Rewriting for Mapping Semantics to KR. In Proceedings of IWCS.
Dalrymple, M. (2001). Lexical Functional Grammar. New York: Academic Press.
Heim, I. and A. Kratzer (1998). Semantics in generative grammar. Wiley-Blackwell.
Hyvo?nen, E. (1984). Semantic Parsing as Graph Language Transformation - a Multidimensional Ap-
proach to Parsing Highly Inflectional Languages. In COLING, pp. 517?520.
Jijkoun, V. and M. de Rijke (2007). Learning to transform linguistic graphs. In Second Workshop on
TextGraphs: Graph-Based Algorithms for Natural Language Processing, Rochester, NY, USA.
Mel?c?uk, I. (1988). Dependency Syntax: Theory and Practice. Albany: State Univ. of New York Press.
Rozenberg, G. (Ed.) (1997). Handbook of Graph Grammars and Computing by Graph Transformations,
Volume 1: Foundations. World Scientific.
74
