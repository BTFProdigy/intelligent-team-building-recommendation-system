Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 29?37,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Agile Corpus Annotation in Practice:
An Overview of Manual and Automatic Annotation of CVs
Bea Alex Claire Grover Rongzhou Shen
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
Mijail Kabadjov
Joint Research Centre
European Commission
Via E. Fermi 2749, Ispra (VA), Italy
Contact: balex@staffmail.ed.ac.uk
Abstract
This paper describes work testing agile
data annotation by moving away from the
traditional, linear phases of corpus cre-
ation towards iterative ones and by recog-
nizing the potential for sources of error oc-
curring throughout the annotation process.
1 Introduction
Annotated data sets are an important resources for
various research fields, including natural language
processing (NLP) and text mining (TM). While the
detection of annotation inconsistencies in different
data sets has been investigated (e.g. Nova?k and
Raz??mova?, 2009) and their effect on NLP perfor-
mance has been studied (e.g. Alex et al 2006), very
little work has been done on deriving better methods
of annotation as a whole process in order to maxi-
mize both the quality and quantity of annotated data.
This paper describes our annotation project in which
we tested the relatively new approach of agile cor-
pus annotation (Voormann and Gut, 2008) of mov-
ing away from the traditional, linear phases of cor-
pus creation towards iterative ones and of recogniz-
ing the fact that sources of error can occur through-
out the annotation process.
We explain agile annotation and discuss related
work in Section 2. Section 3 describes the en-
tire annotation process and all its aspects. We pro-
vide details on the data collection and preparation,
the annotation tool, the annotators and the annota-
tion phases. Section 4 describes the final annota-
tion scheme and Section 5 presents inter-annotator-
agreement (IAA) figures measured throughout the
annotation. In Section 6, we summarize the per-
formance of the machine-learning (ML)-based TM
components which were trained and evaluated on the
annotated data. We discuss our findings and con-
clude in Section 7.
2 Background and Related Work
The manual and automatic annotation work de-
scribed in this paper was conducted as part of the
TXV project. The technology used was based
on TM components that were originally developed
for the biomedical domain during its predecessor
project (Alex et al, 2008b). In TXV we adapted
the tools to the recruitment domain in a short time
frame. The aim was to extract key information from
curricula vitae (CVs) for matching applicants to job
adverts and to each other. The TM output is visu-
alized in a web application with search navigation
that captures relationships between candidates, their
skills and organizations etc. This web interface al-
lows recruiters to find hidden information in large
volumes of unstructured text.
Both projects were managed using agile, test-
driven software development, i.e. solutions were
created based on the principles of rapid-prototyping
and iterative development cycles of deliverable ver-
sions of the TM system and the web application.1
The same principles were also applied to other
project work, including the manual annotation. The
aim of this annotation was to produce annotated data
for training ML-based TM technology as well as
evaluating system components.
Collecting data, drawing up annotation guidelines
and getting annotators to annotate this data in se-
quential steps is similar to the waterfall model in
software engineering (Royce, 1970). This approach
can be inefficient and costly if annotators unknow-
ingly carried out work that could have been avoided
and it can lead to difficulties if at the end of the pro-
cess the requirements no longer match the annota-
tions. Instead we applied agile software engineering
methods to the process of creating annotated data.
This is a relatively recent philosophy in software
1The agile software development principles are explained in
the Agile Manifesto: http://agilemanifesto.org/
29
Figure 1: The phases of traditional corpus creation (a) and the cyclic approach in agile corpus creation (b).
Reproduction of Figure 2 in Voormann and Gut (2008).
development which was inspired to overcome the
drawbacks of the waterfall model. The idea of ap-
plying agile methods to corpus creation and annota-
tion was first inspired by Voormann and Gut (2008)
but was not tested empirically. Cyclic annotation
was already proposed by Atkins et al (1992) and
Biber (1993) with a focus on data creation rather
than data annotation. In this paper, we describe a
way of testing this agile annotation in practice.
The idea behind an agile annotation process is
to produce useable manually annotated data fast as
well as discover and correct flaws in either the an-
notation guidelines or the annotation setup early on.
Voormann and Gut (2008) propose query-driven an-
notation, a cyclic corpus creation and annotation
process that begins with formulating a query. The
main advantages of this approach are:
? The annotation scheme evolves over time
which ensures that annotations are consistent
and remain focussed on the research that is
carried out. An iterative annotation process
therefore improves the annotation guidelines
but keeps the annotations suitable to the rele-
vant research questions.
? Problems with the annotation guidelines, er-
rors in the annotation and issues with the setup
become apparent immediately and can be cor-
rected early on. This can avoid difficulties later
on and will save time and cost.
? Some annotation data is available early on.
Voormann and Gut compare the cyclical approach
in agile annotation to traditional linear-phrase cor-
pus creation depicted in Figure 1. In the following
section we describe the annotation process in our
project which followed the principles of agile cor-
pus creation.
3 Annotation Process
This section provides an overview of all aspect in-
volved in the annotation of a data set of CVs for
various types of semantic information useful to re-
cruiters when analysing CVs and placing candidates
with particular jobs or organizations. We provide
information on the data collection, the document
preparation, the annotation tool and the annotation
process following agile methods.
3.1 Data Collection
We automatically collected a set of CVs of soft-
ware engineers and programmers which are publicly
available online. This data set was created by firstly
querying Google using the Google API2 for word
documents containing either the terms ?CV?, ?re-
sume? or ?curriculum vitae? as well as the terms
?developer?, ?programmer? or ?software? but ex-
cluding documents containing the word ?template?
or ?sample?. Furthermore, the query was restricted
to a 3-month period from 30/03 to 30/06/2008.3
We automatically downloaded the Word docu-
ments returned by this query, resulting in a pool of
1,000 candidate CVs available for annotation. We
split these documents randomly into a TRAIN, a DE-
VTEST and a TEST set in a ratio of approximately
64:16:20. We used the annotated TRAIN data for
training ML-based models and deriving rules and
the DEVTEST data for system development and op-
timization. We set aside the blind TEST set for
evaluating the final performance of our named en-
tity recognition (NER) and relation extraction (RE)
2http://code.google.com/apis/ajaxsearch
3The exact Google query is: ?(CV OR resume OR ?cur-
riculum vitae?) AND (developer OR programmer OR soft-
ware) AND filetype:doc AND -template AND -sample AND
daterange:2454466-2454647?.
30
CV data set
Set TRAIN DEVTEST TEST ALL
Files 253 72 78 403
Annotations 279 84 91 454
Table 1: Number of files and annotated files in each
section of the CV data set.
components (see Section 6).
The final manually annotated data set contains
403 files, of which 352 are singly and 51 doubly an-
notated, resulting in an overall total of 454 annota-
tions (see Table 1). This does not include the files
used during the pilot annotation. The doubly an-
notated CVs were used to determine inter-annotator
agreement (IAA) in regular intervals (see Section 5).
Some of the documents in the pool were not gen-
uine CVs but either job adverts or CV writing ad-
vice. We let the annotators carry out the filtering
process of only choosing genuine CVs of software
developers and programmers for annotation and re-
ject but record any documents that did not fit this cat-
egory. The annotators rejected 99 files as being ei-
ther not CVs at all (49) or being out-of-domain CVs
from other types of professionals (50). Therefore,
just over 50% of the documents in the pool were
used up during the annotation process.
3.2 Document Preparation
Before annotation, all candidate CVs were then au-
tomatically converted from Word DOC format to
OpenOffice ODT as well as to Acrobat PDF format
in a batch process using OpenOffice macros. The
resulting contents.xml files for each ODT version of
the documents contain the textual information of the
original CVs. An XSLT stylesheet was used to sim-
plify this format to a simpler in-house XML format,
as the input into our pre-processing pipeline. We re-
tained all formatting and style information in span
elements for potential later use.
The pre-processing includes tokenization, sen-
tence boundary detection, part-of-speech tagging,
lemmatization, chunking, abbreviation detection
and rule-based NER for person, location names and
dates. This information extraction system is a mod-
ular pipeline built around the LT-XML24 and LT-
TTT25 toolsets. The NER output is stored as stand-
4http://www.ltg.ed.ac.uk/software/ltxml2
5http://www.ltg.ed.ac.uk/software/lt-ttt2
off annotations in the XML. These pre-processed
files were used as the basis for annotation.
3.3 Annotation Tool
For annotating the text of the CVs we chose
MMAX2, the Java-based open source tool (Mu?ller
and Strube, 2006).6 MMAX2 supports multiple lev-
els of annotation by way of stand-off annotation.
As a result MMAX2 creates one separate file for
each level of annotation for each given base data file.
Only the annotation level files get edited during the
annotation phase. The base data files which con-
tain the textual information of the documents do not
change. In our project, we were interested in three
levels of annotation, one for named entities (NEs),
one for zones and one for relations between NEs.
The MMAX2 GUI allows annotators to mark up
nested structures as well as intra- and inter-sentential
relations. Both of these functionalities were crucial
to our annotation effort.
As the files used for annotation already con-
tained some NEs which were recognized automat-
ically using the rule-based NER system and stored
in standoff XML, the conversion into and out of the
MMAX2 format was relatively straightforward. For
each file to be annotated, we created one base file
containing the tokenized text and one entity file con-
taining the rule-based NEs.7
3.4 Annotation Phases
We employed 3 annotators with various degrees of
experience in annotation and computer science and
therefore familiar with software engineering skills
and terminology. The lead researcher of the project,
the first author of this paper, managed the annotators
and organized regular meetings with them.
We followed the agile corpus creation approach
and carried out cycles of annotations, starting with
a simple paper-based pilot annotation. This first an-
notation of 10 documents enabled us to get a first
impression of the type of information contained in
CVs of software engineers and programmers as well
as the type of information we wanted to capture in
the manual and automatic annotation. We drew up a
first set of potential types of zones that occur within
6http://mmax2.sourceforge.net
7For more information on how this is done see Mu?ller and
Strube (2006).
31
CVs and the types of NEs that can be found within
each zone (e.g. an EDUCATION zone containing NEs
of type LOC, ORG and QUAL).
Using this set of potential markables, we decided
on a subset of NEs and zones to be annotated in fu-
ture rounds. Regarding the zones, we settled on an-
notating zone titles in a similar way as NEs. Our
assumption was that recognizing the beginning of a
zone can sufficiently identify zone boundaries. We
did not include relations between NEs at this stages,
as we wanted to get a clearer idea of the definitions
of relevant NEs first before proceeding to relations.
We then carried out a second pilot annotation us-
ing 10 more CVs selected from the candidate pool.
We used the revised annotation scheme and this
time the annotation was done electronically using
MMAX2. The annotators also had access to the
PDF and DOC versions of each file in case crucial
structural or formatting information was lost in the
conversion. Files were annotated for NEs and zone
titles. We also asked the annotators to answer the
following questions:
? Does it make sense to annotate the proposed
markables and what are the difficulties in doing
so?
? Are there any interesting markables missing
from the list?
? Are there are any issues with using the annota-
tion tool?
Half way through the second pilot we scheduled a
further meeting to discuss their answers, addressed
any question, comments or issues with regard to the
annotation and adjusted the annotation guidelines
accordingly. At this point, as we felt that the defini-
tions of NEs were sufficiently clear and added guide-
lines for annotating various types of binary relations
between NEs, for example a LOC-ORG relation re-
ferring to a particular organization situated at a par-
ticular location, e.g. Google - Dublin. We list the
final set of markables as defined at the end of the
annotation process in Tables 2 and 3.
During the second half of the second pilot we
asked the annotators to time their annotation and es-
tablished that it can take between 30 minutes and 1.5
hours to annotate a CV. We then calculated pairwise
IAA for two doubly annotated files which allowed
us to get some evidence for which definition of NEs,
zone titles and relations were still ambiguous or not
actually relevant.
In parallel with both pilots, we also liaised closely
with a local recruitment company to gain a first-
hand understanding of what information recruiters
are interested in when matching candidates to em-
ployments or employers. This consultation as well
as the conclusions made after the second pilot led
to further adaptions of the annotation scheme before
the main annotation phase began.
Based on the feedback from the second pilot an-
notation, we also made some changes to the data
conversion and the annotation tool setup to reduce
the amount of work for annotators but without re-
stricting the set of markables. In the case of some
nested NEs, we propagated relations between em-
bedded NEs that could be referred from the relations
of the containing NEs. For example, two DATE enti-
ties nested within a DATERANGE entity, the latter of
which the annotator related to an ORG entity, were
related to the same ORG entity automatically. We
also introduced a general GROUP entity which could
be used by the annotators to mark up lists of NEs,
for example, if they were all related to a different
NE mention of type X. In that case, the annotators
only had to mark up a relation between the GROUP
and X. All implicit relations between the NEs nested
in the GROUP and X were propagated during the con-
version from the MMAX2 format back into the in-
house XML format. This proved particularly useful
for annotating relations between SKILL entities and
other types of NEs.
Once those changes had been made, the main an-
notation phase began. Each in-domain CV that was
loaded into the annotation tool already contained
some NEs pre-annotated by the rule-based NER sys-
tem (see Section 3.2). The annotators had to correct
the annotations in case they were erroneous. Over-
all, the annotators reported this pre-annotation to be
useful rather than hindering as they did not have to
do too many corrections. At the end of each day, the
annotators checked in their work into the project?s
subversion (SVN) repository. This provided us with
additional control and backup in case we needed to
go back to previous versions at later stages.
The annotation guidelines still evolved during the
main annotation. Regular annotation meetings were
32
held in case the annotators had questions on the
guidelines or if they wanted to discuss specific ex-
amples. If a change was made to the annotation
guidelines, all annotators were informed and asked
to update their annotations accordingly. Moreover,
IAA was calculated regularly on sub-sections of the
doubly annotated data. This provided more empiri-
cal evidence for the types of markables the annota-
tors found difficult to mark up and where clarifica-
tions where necessary. The reasons for this were that
their definitions were ambiguous or underspecified.
We deliberately kept the initial annotation scheme
simple. The idea was for the annotators to shape the
annotation scheme based on evidence in the actual
data. We believe that this approach made the data
set more useful for its final use to train and evaluate
TM components. As a result of this agile annotation
approach, we became aware of any issues very early
on and were able to correct them accordingly.
4 Annotation Scheme
In this section, we provide a summary of the final
annotation scheme as an overview of all the mark-
ables present in the annotated data set.
4.1 Named Entities
In general, we asked the annotators to mark up ev-
ery mention of all NE types throughout the entire
CV, even if they did not refer to the CV owner. With
some exceptions (DATE in DATERANGE and LOC or
ORG in ADDRESS), annotators were asked to avoid
nested NEs and aim for a flat annotation. Discontin-
uous NEs in coordinated structures had to be marked
as such, i.e. the NE should only contain strings that
refer to it. Finally, abbreviations and their defini-
tions had to be annotated as two separate NEs. The
NE types in the final annotation guidelines are listed
in Table 2. While carrying out the NE annotation,
the annotators were also asked to set the NE at-
tribute of type CANDIDATE (by default set to true)
to false if a certain NE was not an attribute of the
CV owner (e.g. the ADDRESS of a referee).
4.2 Zone Titles
Regarding the zone titles, we provided a list of syn-
onyms for each type as context (see Table 2). The
annotators were asked only to annotate main zone
titles, ignoring sub-zones. They were also asked to
Entity Type Description
ADDRESS Addresses with streets or postcodes.
DATE Absolute (e.g. 10/04/2010), underspec-
ified (e.g. April 2010) or relative dates
(e.g. to date) including DATE entities
within DATERANGE entities.
DATERANGE Date ranges with a specific start and end
date including ones with either point not
explicitly stated (e.g. since 2008).
DOB Dates of birth.
EMAIL Email addresses.
JOB Job titles and roles referring to the of-
ficial name a post (e.g. software devel-
oper) but not a skill (e.g. software de-
velopment).
LOC Geo-political place names.
ORG Names of companies, institutions and
organizations.
PER Person names excluding titles.
PHONE Telephone and fax numbers.
POSTCODE Post codes.
QUAL Qualifications achieved or working to-
wards.
SKILL Skills and areas of expertise incl. hard
skills (e.g. Java, C++, French) or gen-
eral areas of expertise (e.g. software de-
velopment) but not soft or interpersonal
skills (e.g. networking, team work).
TIMESPAN Durations of time (e.g. 7 years, 2
months, over 2 years).
URL URLs
GROUP Dummy NE to group several NEs for
annotating multiple relations at once.
The individual NEs contained within
the group still have to be annotated.
Zone Title Type Synonyms
EDUCATION Education, Qualifications, Training,
Certifications, Courses
SKILLS Skills, Qualifications, Experience,
Competencies
SUMMARY Summary, Profile
PERSONAL Personal Information, Personal Data
EMPLOYMENT Employment, Employment History,
Work History, Career, Career Record
REFERENCES References, Referees
OTHER Other zone titles not covered by this list,
e.g. Publications, Patents, Grants, As-
sociations, Interests, Additional.
Table 2: The types of NEs and zone titles annotated.
mark up only the relevant sub-string of the text re-
ferring to the zone title and not the entire title if it
contained irrelevant information.
4.3 Relations
The binary relations that were annotated (see Table
3) always link two different types of NE mentions.
Annotators were asked to mark up relations within
the same zone but not across zones.
33
Relation Type Description
TEMP-SKILL A skill related to a temporal expression
(e.g. Java - 7 years). TEMP includes any
temporal NE types (DATE, DATERANGE
and TIMESPAN).
TEMP-LOC A location related to a temporal expres-
sion (e.g. Dublin - summer 2004).
TEMP-ORG An organization related to a temporal
expression (e.g. Google - 2001-2004).
TEMP-JOB A job title related to a temporal ex-
pression (e.g. Software Engineer -
Sep. 2001 to Jun. 2004).
TEMP-QUAL A qualification related to a temporal ex-
pression (e.g. PhD - June 2004).
LOC-ORG An organization related to a location
(e.g. Google - Dublin).
LOC-JOB A job title related to a location
(e.g. Software Engineer - Dublin).
LOC-QUAL A qualification related to a location
(e.g. PhD - Dublin).
ORG-JOB A job title related to an organization
(e.g. Software Engineer - Google).
ORG-QUAL A qualification related to an organiza-
tion (e.g. PhD - University of Toronto).
GROUP-X A relation that can be assigned in case
a group of NEs all relate to another NE
X. GROUP-X can be any of the relation
pairs mentioned in this list.
Table 3: The types of relations annotated.
5 Inter-Annotator Agreement
We first calculated pairwise IAA for all markables
at the end of the 2nd pilot and continued doing so
throughout the main annotation phase. For each pair
of annotations on the same document, IAA was cal-
culated by scoring one annotator against another us-
ing precision (P), recall (R) and F1.8 An overall IAA
was calculated by micro-averaging across all anno-
tated document pairs.9 We used F1 rather than the
Kappa score (Cohen, 1960) to measure IAA as the
latter requires comparison with a random baseline,
which does not make sense for tasks such as NER.
Table 4 compares the IAA figures we obtained for
2 doubly annotated documents during the 2nd pilot
phase, i.e. the first time we measured IAA, to those
we obtained on 9 different files once the main an-
notation was completed. For NEs and zone titles,
IAA was calculated using P, R and F1, defining two
mentions as equal if they had the same left and right
8P, R and F1 are calculated in standard fashion from the
number of true positives, false positives and false negatives.
9Micro-averaging was chosen over macro-averaging, since
we felt that the latter would give undue weight to documents
with fewer markables.
boundaries and the same type. Although this com-
parison is done over different sub-sets of the corpus,
it is still possible to conclude that the NE IAA im-
proved considerably over the course of the annota-
tion process.
The IAA scores for the majority of NEs were in-
creased considerably at the end, with the exception
of SKILL for which the IAA ended up being slightly
lower as well as DOB and PER of which there are
not sufficient examples in either sets to obtain re-
liably results.10 There are very large increases in
IAA for JOB and ORG entities, as we discovered dur-
ing the pilot annotation that the guidelines for those
markables were not concrete enough regarding their
boundaries and definitions. Their final IAA figures
show that both of these types of NEs were still most
difficult to annotate at the end. However, a final total
IAA of 84.8 F1 for all NEs is a relatively high score.
In comparison, the final IAA score of 97.1 F1 for the
zone titles shows that recognizing zone titles is an
even easier task for humans to perform compared to
recognizing NEs.
When calculating IAA for relations, only those re-
lations for which both annotators agreed on the NEs
were included. This is done to get an idea of the
difficulty of the RE task independently of NER. Re-
lation IAA was also measured using F1, where rela-
tions are counted as equal if they connect exactly the
same NE pair. The IAA for relations between NEs
within CVs is relatively high both during the pilot
annotation and at the end of the main annotation and
only increased slightly over this time. These figures
show that this task is much easier than annotating re-
lations in other domains, e.g. in biomedical research
papers (Alex et al, 2008a).
The IAA figures show that even with cyclic anno-
tation, evolving guidelines and continuous updating,
human annotators can find it challenging to annotate
some markables consistently. This has an effect on
the results of the automatic annotation where the an-
notated data is used to train ML-based models and
to evaluate their performance.
10The reason why there are no figures for POSTCODE and
TIMESPAN entities for the pilot annotation is that none appeared
in those documents.
34
(1) 2nd Pilot Annotation (2) End of Main Annotation (3) Automatic Annotation
Type P R F1 TPs P R F1 TPs P R F1 TPs
Named Entities
ADDRESS 100.0 100.0 100.0 1 100.0 100.0 100.0 10 13.8 16.0 14.8 8
DATE 62.5 92.6 74.6 25 98.5 98.5 98.5 191 94.1 95.7 94.9 1,850
DATERANGE 91.3 95.5 93.3 21 98.6 97.3 97.9 71 91.4 87.0 89.2 637
DOB 100.0 100.0 100.0 1 75.0 100.0 85.7 3 70.8 70.8 70.8 17
EMAIL 100.0 100.0 100.0 2 100.0 100.0 100.0 8 95.9 100.0 97.9 93
JOB 39.1 52.9 45.0 9 72.5 69.9 71.2 95 70.5 61.4 65.6 742
LOC 88.9 100.0 94.1 16 100.0 95.8 97.9 137 83.2 87.3 85.2 1,259
ORG 68.0 81.0 73.9 17 93.4 86.4 89.8 171 57.1 44.7 50.2 749
PER 100.0 100.0 100.0 2 100.0 95.0 97.4 19 69.8 40.5 51.2 196
PHONE 100.0 100.0 100.0 4 100.0 100.0 100.0 16 90.9 85.7 88.2 90
POSTCODE - - - - 90.9 90.9 90.9 10 98.3 71.3 82.6 57
QUAL 9.1 7.7 8.3 1 68.4 81.3 74.3 13 53.9 27.2 36.1 56
SKILL 76.6 86.8 81.4 210 79.3 79.0 79.2 863 67.9 66.5 67.2 5,645
TIMESPAN - - - - 91.7 91.7 91.7 33 74.0 76.8 75.4 179
URL 100.0 100.0 100.0 2 100.0 100.0 100.0 43 97.2 90.5 93.7 209
All 73.0 84.1 78.1 311 85.4 84.2 84.8 1,683 73.5 69.4 71.4 11,787
Zone Titles
EDUCATION 100.0 100.0 100.0 3 100.0 100.0 100.0 9 86.3 75.0 80.3 63
EMPLOYMENT 100.0 100.0 100.0 1 100.0 88.9 94.1 8 83.1 69.7 75.8 69
OTHER 100.0 100.0 100.0 1 - - - - 39.3 28.2 32.8 22
PERSONAL 25.0 25.0 25.0 1 100.0 100.0 100.0 4 65.4 53.1 58.6 17
REFERENCES 100.0 100.0 100.0 1 100.0 100.0 100.0 3 94.4 89.5 91.9 17
SKILLS 33.3 40.0 36.4 2 100.0 100.0 100.0 7 63.8 38.9 48.4 44
SUMMARY - - - - 75.0 100.0 85.7 3 82.2 64.9 72.6 37
All 56.3 60.0 58.1 9 97.1 97.1 97.1 34 72.7 55.8 63.2 269
Relations
DATE-JOB - - - - 100.0 83.3 90.9 10 28.1 44.7 34.5 110
DATE-LOC - - - - 88.9 72.7 80.0 8 71.3 52.7 60.6 223
DATE-ORG - - - - 100.0 88.2 93.8 15 53.0 51.5 52.3 218
DATE-QUAL - - - - 100.0 100.0 100.0 6 60.6 73.1 66.3 57
DATERANGE-JOB 77.8 100.0 87.5 7 91.7 100.0 95.7 66 80.4 72.5 76.2 663
DATERANGE-LOC 91.7 100.0 95.7 11 85.4 79.6 82.4 70 82.0 82.7 82.4 735
DATERANGE-ORG 93.8 100.0 96.8 15 80.2 76.2 78.2 77 72.2 76.4 74.2 644
DATERANGE-QUAL 100.0 100.0 100.0 1 100.0 100.0 100.0 21 71.1 62.1 66.3 59
DATERANGE-SKILL 89.0 98.1 93.3 105 82.2 100.0 90.5 352 61.1 33.7 43.4 1,574
DATE-SKILL 100.0 9.1 16.7 1 95.0 67.1 78.6 57 23.6 54.5 33.0 368
JOB-LOC NaN 0.0 NaN 0 91.8 65.6 76.5 78 77.0 69.1 72.8 932
JOB-ORG 87.5 100.0 93.3 7 86.8 73.3 79.5 99 64.6 50.7 56.8 758
JOB-TIMESPAN - - - - 85.7 54.6 66.7 6 56.0 61.8 58.8 47
LOC-ORG NaN 0.0 NaN 0 89.6 71.4 79.5 120 79.7 78.9 79.3 1,044
LOC-QUAL NaN 0.0 NaN 0 100.0 100.0 100.0 19 75.6 78.7 77.1 133
LOC-TIMESPAN - - - - 100.0 75.0 85.7 3 48.2 36.1 41.3 13
ORG-QUAL NaN 0.0 NaN 0 95.2 95.2 95.2 20 77.8 71.4 74.5 140
ORG-TIMESPAN - - - - 83.3 55.6 66.7 5 55.9 33.3 41.8 19
SKILL-TIMESPAN - - - - 86.1 74.0 79.6 37 59.5 52.6 55.8 280
All 85.5 83.1 84.2 147 86.8 82.6 84.6 1,069 63.1 55.3 59.0 8,017
Table 4: IAA for NEs, zone titles and relations in precision (P), recall (R) and F1 at two stages in the
annotation process: (1) at the end of the second pilot annotation and (2) at the end of the main annotation
phase; as well as automatic annotation scores (3) on the blind TEST set. The total number of true positives
(TPs) is shown to provide an idea of the quantities of markables in each set.
35
6 Automatic Annotation
Table 4 also lists the final scores of the automatic
ML-based NER and RE components (Alex et al,
2008b) which were adapted to the recruitment do-
main during the TXV project. Following agile
methods, we trained and evaluated models very
early into the annotation process. During the sys-
tem optimization, learning curves helped to investi-
gate for which markables having more training data
available would improve performance.
The NER component recognizes NEs and zone
titles simultaneously with an overall F1 of 71.4
(84.2% of IAA) and 63.2 (65.0% of IAA), respec-
tively. Extremely high or higher than average scores
were obtained for DATE, DATERANGE, EMAIL, LOC,
PHONE, POSTCODE, TIMESPAN and URL entities.
Mid-range to lower scores were obtained for AD-
DRESS, DOB, JOB, ORG, PER, QUAL and SKILL enti-
ties. One reason is the similarity between NE types,
e.g. DOB is difficult to differentiate from DATE. The
layout of CVs and the lack of full sentences also
pose a challenge as the NER component is trained
using contextual features surrounding NEs that are
often not present in CV data. Finally, the strict eval-
uation counts numerous boundary errors for NEs
which can be considered correct, e.g. the system
often recognizes organization names like ?Sun Mi-
crosystems, Inc? whereas the annotator included the
full stop at the end (?Sun Microsystems, Inc.?).
The RE component (Haddow, 2008) performs
with an overall F1 of 59.0 on the CV TEST set
(69.7% of IAA). It yields high or above aver-
age scores for 10 relation types (DATE-LOC, DATE-
QUAL, DATERANGE-JOB, DATERANGE-LOC, DAT-
ERANGE-ORG, DATERANGE-QUAL, JOB-LOC, LOC-
ORG, LOC-QUAL, ORG-QUAL). It yields mid-range
to low scores for the other relation types (DATE-
JOB, DATE-ORG, DATERANGE-SKILL, DATE-SKILL,
JOB-ORG, JOB-TIMESPAN, LOC-TIMESPAN, ORG-
TIMESPAN, SKILL-TIMESPAN). The most frequent
type is DATERANGE-SKILL, a skill obtained during a
particular time period. Its entities tend to be found in
the same zone but not always in immediate context.
Such relations are inter-sentential, i.e. their entities
are in different sentences or what is perceived as sen-
tences by the system. Due to nature of the data, there
are few intra-sentential relations, relations between
NEs in the same sentence. The further apart two re-
lated NEs are, the more difficult it is to recognize
them. Similarly to NER, one challenge for RE from
CVs is their diverse structure and formatting.
7 Discussion and Conclusion
The increase in the IAA figures for the markables
over time show that agile corpus annotation resulted
in more qualitative annotations. It is difficult to
prove that the final annotation quality is higher than
it would have been had we followed the traditional
way of annotation. Comparing two such methods in
parallel is very difficult to achieve as the main aim
of annotation is usually to create a corpus and not to
investigate the best and most efficient method.
However, using the agile approach we identified
problems early on and made improvements to the
annotation scheme and the setup during the process
rather than at the end. Given a fixed annotation time
frame and the proportion of time we spent on cor-
recting errors throughout the annotation process, one
might conclude that we annotated less data than we
may have done, had we not followed the agile ap-
proach. However, Voormann and Hut (2008) argue
that agile annotation actually results in more useable
data at the end and in less data being thrown away.
Had we followed the traditional approach, we
would unlikely have planned a correction phase at
the end. The two main reason for that are cost
and the general belief that the more annotated data
the better. A final major correction phase is usu-
ally viewed as too expensive during an annotation
project. In order to avoid this cost, the traditional ap-
proach taken tends to be to create a set of annotation
guidelines when starting out and hold off the main
annotation until the guidelines are finalized and con-
sidered sufficiently defined. This approach does not
lend itself well to changes and adjustments later on
which are inevitable when dealing with natural lan-
guage. As a result the final less accurate annotated
corpus tends to be accepted as the ground truth or
gold standard and may not be as suitable and useful
for a given purpose as it could have been follow-
ing the agile annotation approach. Besides changing
the way in which annotators work, we recognize the
need for more flexible annotation tools that allow an-
notators to implement changes more rapidly.
36
References
Beatrice Alex, Malvina Nissim, and Claire Grover. 2006.
The impact of annotation on the performance of pro-
tein tagging in biomedical text. In Proceedings of the
5th International Conference on Language Resources
and Evaluation (LREC 2006), Genoa, Italy.
Bea Alex, Claire Grover, Barry Haddow, Mijail Kabad-
jov, Ewan Klein, Michael Matthews, Richard Tobin,
and Xinglong Wang. 2008a. The ITI TXM corpora:
Tissue expressions and protein-protein interactions. In
Proceedings of the Workshop on Building and Evalu-
ating Resources for Biomedical Text Mining at LREC
2008, Marrakech, Morocco.
Beatrice Alex, Claire Grover, Barry Haddow, Mijail
Kabadjov, Ewan Klein, Michael Matthews, Richard
Tobin, and Xinglong Wang. 2008b. Automating cu-
ration using a natural language processing pipeline.
Genome Biology, 9(Suppl 2):S10.
Sue Atkins, Jeremy Clear, and Nicholas Ostler. 1992.
Corpus design criteria. Literary and Linguistic Com-
puting, 7(1):1?16.
Douglas Biber. 1993. Representativeness in corpus de-
sign. Literary and Linguistic Computing, 8(4):243?
257.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37?46.
Barry Haddow. 2008. Using automated feature optimisa-
tion to create an adaptable relation extraction system.
In Proceedings of BioNLP 2008, Columbus, Ohio.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy. New Re-
sources, New Tools, New Methods., pages 197?214.
Peter Lang, Frankfurt. (English Corpus Linguistics,
Vol.3).
Va?clav Nova?k and Magda Raz??mova?. 2009. Unsu-
pervised detection of annotation inconsistencies using
apriori algorithm. In Proceedings of the Third Linguis-
tic Annotation Workshop (LAW III), pages 138?141,
Suntec, Singapore.
Winston Royce. 1970. Managing the development
of large software systems. In Proceedings of IEEE
WESCON, pages 1?9.
Holger Voormann and Ulrike Gut. 2008. Agile corpus
creation. Corpus Linguistics and Linguistic Theory,
4(2):235?251.
37
Space Characters in Chinese Semi-structured Texts
Rongzhou Shen
School of Informatics
University of Edinburgh
rshen@inf.ed.ac.uk
Claire Grover
School of Informatics
University of Edinburgh
grover@inf.ed.ac.uk
Ewan Klein
School of Informatics
University of Edinburgh
ewan@inf.ed.ac.uk
Abstract
Space characters can have an important role
in disambiguating text. However, few, if
any, Chinese information extraction systems
make full use of space characters. However,
it seems that treatment of space characters
is necessary, especially in cases of extract-
ing information from semi-structured docu-
ments. This investigation aims to address
the importance of space characters in Chi-
nese information extraction by parsing some
semi-structured documents with two simi-
lar grammars - one with treatment for space
characters, the other ignoring it. This paper
also introduces two post processing filters
to further improve treatment of space char-
acters. Results show that the grammar that
takes account of spaces clearly out-performs
the one that ignores them, and so concludes
that space characters can play a useful role in
information extraction.
1 Introduction
It is well known that a snippet of text in Chi-
nese (or some other oriental languages) consists of
a span of continuous characters without delimiting
white spaces to identify words. Therefore, most pars-
ing systems do not make full use of space charac-
ters when parsing. Furthermore, even though Latin-
based languages such as English have delimiting
white spaces between words, most systems treat them
as no more than delimiting characters. Therefore,
space characters are usually stripped out of the text
before processing.
However, this is intuitively wrong. (Rus and
Summers, 1994) stated that ?the non-textual con-
tent of documents complement the textual content and
should play an equal role?. This paper shows that
space character plays an equal role as the textual con-
tent, where it can be used not only to construct a cer-
tain layout, but also to signal a certain syntactic struc-
ture. Some researchers have been seen to make use of
space characters, but they mainly use spaces to cre-
ate or recognise certain special layouts. For example,
(Rus and Summers, 1994) used white spaces to refor-
mat documents into somewhat structured styles; (Ng
et al, 1999) and (Hurst and Nasukawa, 2000) used
spaces to recognise tables in free text. Wrapper gen-
eration is more related to our research since it uses
layout to extract structured content from documents
(Irmak and Suel, 2006; Chen et al, 2003). How-
ever, wrapper generation is too high level, this paper
is aimed at exploring the effects of space characters
at a lower level.
In this paper, we focus on semi-structured doc-
uments (in our case, real-world Chinese Curricula
Vitae), since these types of documents tend to con-
tain more space layout information. This paper is
intended to address the importance of space charac-
ters not only in layout extraction, but also in infor-
mation extraction. To do so, Daxtra Technologies?
grammar formalism and their additional elements for
basic space character treatment is introduced 1. Then
an improved treatment plan is given for further dis-
ambiguation. Finally, we perform evaluation of the
tools on a set of real-world CVs and give proposals
for future work.
2 Space Characters
A space character, when considered as punctu-
ation, is a blank area devoid of content, serving to
separate words, letters, numbers and other punctu-
ation. (Jones, 1994) found broadly three types of
punctuation marks: delimiting, separating and dis-
ambiguating. Similarly, space characters have three
different functionalities: delimiting, structuring and
disambiguating.
Space characters are natural delimiters in some
languages. In English and many other Latin-based
languages for example, spaces are used for separat-
ing words and certain punctuation marks (e.g. period
and colon). However, in formal Chinese typesetting,
spaces are not used to delimit words or characters.
Hence the need for automatic word segmentation sys-
tems (Zhang et al, 2003). The current segmenta-
tion systems mainly focus on resolving ambiguities
and detecting new words in segmenting text with no
spaces (Gao et al, 2005). However, ambiguities can
be caused not only by characters themselves, but also
the spaces and layout around them. The paper will
later demonstrate this in terms of recognising entities,
but the same should apply to segmentation.
Therefore, Chinese documents can have white
spaces, it is up to the author of the document to de-
cide when to use spaces, which makes dealing with
people?s spacing habits one of the reasons to include
treatment of space characters in linguistic systems.
Structuring refers to space characters being used
for layout purposes. For example, spaces and tabs
can be used to create tables, putting spaces in front
1Daxtra Technologies provides software for re-
sume/CV parsing and extraction for candidate acquisition:
http://www.daxtra.com
of a piece of text means to start a new paragraph
etc. In some cases, such structuring space characters
represent a relation between the elements that the
spaces are delimiting. For the following example,
each line contains a label and a value separated using
spaces to create a table.
??(Name) ???
??(Age) 25?
Email li25@gmail.com
??(Place of Birth) ??
Disambiguating spaces occur where an unintentional
ambiguity could result if the spaces were not there.
Two types of ambiguities are usually caused by ig-
noring the effect of white space:
Overlapping Ambiguity, where a set of tokens can
either be appended to the previous set of tokens
to form an entity, or precede the next set of to-
kens to form a different entity. For example, in a
Chinese CV?s job history section, the following
two situations could occur:
1999?10?1 ??????
1999.10.1 A Japanese Company Ac-
countant
1999?10?1? ?????
1999.10.1 Accountant in this com-
pany
In the above example, two spans of text use ex-
actly the same set of characters, but since the
space is not in the same place, they have differ-
ent meanings. Thus ignoring white space in this
case could result in an overlapping ambiguity.
Combinatorial Ambiguity, where two sets of to-
kens can either be joined to form a single entity,
or be separated to form two different entities.
For example, ??? ??? could mean Man-
ager Assistant when joined together, or since
there are spaces in between the two words ??
?? and ????, they could also mean Manager
and Assistant.
3 Basic Space Character Treatment
Daxtra Technologies? parsing system is a gram-
mar formalism used to develop grammatical rules for
recognising Named Entities and Relations. The sys-
tem is based on context free grammar, but includes
additional elements for integrating linguistic infor-
mation (e.g. grammar and lexicon) and layout infor-
mation (e.g. space characters) to parse structured and
unstructured text. Along with parsing the text, the
parser also labels the matched text with XML tags.
A typical Daxtra grammar rule looks like the
following:
: person =
person-firstname
+ person-lastname !ATTACHED_L
: person =
person-firstname
+ person-midname !ATTACHED_L
+ person-lastname !ATTACHED_L
As the above example illustrates, a rule begins
with a colon and the rule?s name. For example, con-
sider the following two person names:
Rongzhou Shen
Andrew Peter Baker
assuming that ?Rongzhou Shen? matches the first
rule and ?Andrew Peter Baker? matches the second
rule, then both will be surrouned by <person>
XML tags.
Contents following the equal sign are a combi-
nation of other defined grammar rule names or lex-
icon names to build up the body of the person
rules. Thus, for the first person rule to match a
piece of text, the sub contents of the text must match
person-firstname and person-lastname
in the order given. Any other contents between a right
hand side rule name and its XML tag replacement
(i.e. the square bracketed contents) are attributes at-
tached to the rule. These attributes include layout in-
formation.
For describing layout information, the Dax-
tra grammar formalism offers three types of
space grammar rule: ATTACHED (ATTACHED L,
ATTACHED R), TABULATION (TABULATION L,
TABULATION M, TABULATION R) and
LINEBREAK (LINEBREAK L, LINEBREAK M,
LINEBREAK R).
ATTACHED This attribute checks the matching con-
tents of the attached rule for surrounding
spaces. Accordingly, ATTACHED L detects
spaces on the left of the matching contents, and
ATTACHED R detects spaces on the right of the
matching contents.
TABULATION Similar to ATTACHED, this checks
for tabulation characters in the matching con-
tents. TABULATION L, TABULATION M and
TABULATION R checks for tabulations before,
inside or after the matching text respectively. A
tabulation is either a tab character or a span of
more than three continuous white spaces.
LINEBREAK As the name suggests, this at-
tribute checks for line breaks in the match-
ing text. LINEBREAK L, LINEBREAK M and
LINEBREAK R checks for line breaks before,
inside or after the matching text respectively.
4 Improved Algorithm
Although the Daxtra grammar formalism offers
a full range of space layout descriptors, questions still
arise. Consider the job history examples in Table 1.
The first one would parse correctly with some sim-
ple grammar such as the following (assuming that we
have all the needed lexicons):
: history = date-range !ATTACHED_R
+ company !ATTACHED_R
+ occupation !ATTACHED_R
+ occupation
However, the same rule would become ambigu-
ous for the second example, where there is a space
Original 1999 - 2000 3CR Health Beauty International Ltd. ???? ????
Translation 1999 - 2000 3CR Health Beauty International Ltd. Assis. Manager Assis. Accountant
Original 1995 - 1997 EMaiMai.com Hong Kong Ltd. ?? ?? ??
Translation 1995 - 1997 EMaiMai.com Hong Kong Ltd. Manager Assistant Accountant
Table 1: An example job history section in a CV file
between ???? (Manager) and ???? (Assistant).
In such a case, two matches are found, as shown in
Table 2.
We may notice that the word ???? (Assistant)
is closer to the word ???? (Manager) than the word
???? (Accountant), hence the correct entities be-
ing ?????? (Manager Assistant) and ????
(Accountant). If on the other hand, there were more
spaces between ???? (Manager) and ???? (As-
sistant) than ???? (Assistant) and ???? (Accoun-
tant), we may infer that the entities would be ????
(Manager) and ?????? (Assistant Accountant).
Therefore, more control is needed for incorpo-
rating space layout information. For example, the
problem in Table 2 can be resolved by comparing
the number of spaces between the words. To do so,
we replaced the spaces with XML tags with an at-
tribute indicating the number of spaces replaced. For
example, a span of four spaces will become: <w
spaces=?4? />. Based on such a transformation,
we came up with the following post-processing filters
for resolving ambiguities and other errors caused by
space characters:
Filter least-space For different matches of the same
rule, always choose the match that has the least
number of spaces inside the entities.
For example, consider the two cases in Table 3.
They both have exactly the same set of charac-
ters, but are in fact two different combinations,
as indicated by the translations in the table.
Assuming that a simple rule like the following is
used to match both the job histories:
: history = date !ATTACHED_R
+ company
+ occupation
Then for (1) in Table 3, the two possible matches
are shown in Table 4.
Therefore, the first match yields a total of one
space inside the entities (between ??? and
???), while the second match yields three
spaces (between ??? and ??????). Thus
the first match is chosen.
Similarly for (2) in Table 3, there are two pos-
sible matches (see Table 5), in which the first
has four spaces inside the entities and the sec-
ond has two spaces, so the system chooses the
second match.
Filter equal-space For a parsing with only one pos-
sible match, check whether the entity contains
an unequal number of spaces between charac-
ters.
For example, ??????? ?????
(Chinese Accountant Regulations Listed Com-
pany) can be recognised by the system as a
company entity, but it is in fact not. Thus in
this case, the filter equal-space will reject it -
there are no spaces between the first six charac-
ters, but two spaces appear after them, so the two
spaces are not considered as part of an entity.
5 Evaluation
The evaluation data is a set of entities extracted
from 314 real world CVs. The original CVs were
all MS Word files, then converted to plain text using
Original 1995 - 1997 EMaiMai.com Hong Kong Ltd. ?? ?? ??
Translation 1995 - 1997 EMaiMai.com Hong Kong Ltd. Manager Assistant Accountant
Match 1 1995 - 1997 EMaiMai.com Hong Kong Ltd. ?? ????
Translation 1995 - 1997 EMaiMai.com Hong Kong Ltd. Manager Assistant Accountant
Match 2 1995 - 1997 EMaiMai.com Hong Kong Ltd. ???? ??
Translation 1995 - 1997 EMaiMai.com Hong Kong Ltd. Manager Assistant Accountant
Table 2: The second example?s two matching variants
Original (1) 2002? ? ???????????
Translated 2002 Winter P&G Global Accountant and Finance
Original (2) 2002? ? ? ? ?????????
Translated 2002 Dong Bao Jie Company Global Accountant and Finance
Table 3: Examples showing two different combinations using the same set of characters. (Note: ?DongBao-
Jie? and ?Winter P&G? have the same characters in Chinese.
Match 1 2002? ? ???? ???????
Translation 2002 Winter P&G Company Global Accountant and Finance
Match 2 2002? ? ???? ???????
Translation 2002 Dong BaoJie Company Global Accountant and Finance
Table 4: Two possible matches of (1) in Table 3
Match 1 2002? ? ? ??? ???????
Translation 2002 Winter P &G Company Global Accountant and Finance
Match 2 2002? ? ? ??? ???????
Translation 2002 Dong Bao Jie Company Global Accountant and Finance
Table 5: Two possible matches of (2) in Table 3
wvWare 2. The converted files were all encoded us-
ing UTF-8. To demonstrate generality of the rules
and filters, the selected CVs included differents kinds
of layout, among which plain paragraphs, tables and
lists are the most common. Table 6 shows the types
of entities extracted.
To evaluate the effect of the different treatments
of space characters, four sets of data were prepared,
Table 7 shows the list of data.
For annotating the gold set, we performed
named entity recognition using the latest grammar
2wvWare is an opensource project for accessing and
converting MS Word files: http://wvware.sourceforge.net/
rules, then hand corrected the mistakes to produce a
gold data set. For evaluation method, we used the
standard Precision/Recall/F-score measures. To com-
pute the standard measures, the XML output from the
original parsed texts are converted to a CoNLL style
format. For the example in Table 8, the converted
CoNLL format looks like Figure 1.
5.1 The Results
A total of 24,434 entities were annotated in the
gold set, Table 9 shows the distribution of the entity
types among the whole set of entities.
After running each version of the grammar (i.e.
Baseline, Version 1, Version 2) on the whole set of
Number of cor-
rectly labeled
characters
Number of gold
annotated charac-
ters
Number of sys-
tem annotated
characters
Precision
(%)
Recall
(%)
F1 (%)
Baseline 272901 302491 321059 85.00 90.22 87.53
Version 1 285736 302491 305339 93.58 94.46 94.02
Version 2 287365 302491 303948 94.54 95.00 94.77
Table 10: Results of each version computed against the gold data set
Entity Type Examples
date 1990?10?1?, 1990.10.01
date-range 1998/10/1 - 1999/10/1
company ??????????(Zhongxin
Industrial Bank Shanghai Branch)
occupation ? ?(Accountant), ? ? ?
?(Manager Assistant)
person ???(Shen Rongzhou)
educational ?????(University of Edin-
burgh),????(University of Na-
tional Defenses)
degree ??(Bachelor), ????(Masters
Degree)
subject ??(Physics), ????(Physical
Chemistry)
Table 6: Types of named entities extracted from CVs.
Data name Description
Gold Human annotated data
Baseline Daxtra grammar without space at-
tributes
Version 1 Daxtra grammar with space at-
tributes
Version 2 Daxtra grammar with space at-
tributes and least-space filter and
equal-space filter.
Table 7: The four sets of data prepared
CVs and converting the XML output into CoNLL for-
mat, there were a total of four sets of result files (in-
cluding Gold annotated data set) and 1256 result files
in total (one result file per CV). We then performed
Text ? ? ??? ???????
Translation Dong Bao Jie Company Global
Accountant and Finance
Rule : history = company + occupation
Table 8: A sample text and its matching rule
? B-company
#space I-company
? I-company
#space I-company
? I-company
? I-company
? I-company
#space O
? B-occupation
? I-occupation
? I-occupation
? I-occupation
? I-occupation
? I-occupation
? I-occupation
Figure 1: The converted CoNLL style format for Ta-
ble 8
pair-wise comparisons of the result files from each
version with the result files in the gold data set. Table
10 shows the final results.
As can be seen from Table 10, Version 1 is a
great improvement over the Baseline in that both F1
score and precision increased by over 6%, while re-
call rose by 4.24%. This strongly indicates that the
importance of space layout information is not to be
Entity Type Total Number
date 10006
date-range 166
company 5456
occupation 3993
person 783
educational 1686
degree 1039
subject 1305
Total 24434
Table 9: Distribution of entity types in the CVs
neglected in named entity recognition tasks. A much
lower number of system annotated characters for Ver-
sion 1 shows that the layout information is disam-
biguating multiple matches, thus rejecting many pre-
dictions.
Although not as significant, Version 2 has still
gained an improvement on performance over Version
1 by 0.75% in F1 score. A lower number of pre-
dicted annotations and a higher number of correctly
predicted annotations both indicate more ambiguities
have been resolved as a result.
Further investigations into the errors made by
the Baseline showed that most ambiguities were over-
lapping ambiguities (over 90%). A possible reason
for the smaller number of combinatorial ambiguities
could be that people tend to be careful in writing
their CVs, and tend to disambiguate entities by them-
selves. For example, instead of writing ??? ?
??, separating the two words using a space, people
will use punctuation marks to divide them. Further-
more, the case where people put spaces between each
character wasn?t so often seen: there were 16 CVs in
total where such a case was found. Thus filter equal-
space did not disambiguate many.
Further dividing the results down into smaller
parts, we found that most of the ambiguities in the
Baseline came from company, educational, occupa-
tion and subject names. This has two main causes:
(1) These entities? grammar contain many generative
rules, so ambiguities can not be avoided; (2) The con-
text around these entities contain the most layout in-
formation (e.g. job history, educational history). Date
and date-range entities were not affected so much by
the layout information since they are straightforward
to recognise. However, there was one case where the
Baseline predicted a date wrongly:
1995?1?1??1997?1?1 ???
?????????
1995.1.1 - 1997.1.1 Japan Sakura
Bank Shanghai Branch
The Baseline version predicted ?1997?1?1
?? as a single entity of type date. This is obviously
a human typing error, where the author missed out
??? on the end of the date. This error was later fixed
by Version 1.
From to the above discussion, we may know that
least-space is mainly targeted at resolving overlap-
ping ambiguities (which account for more than 90%
of the ambiguities found), thus making it the more
significant filter of the two.
Although Version 1 and Version 2 both had im-
provements over the Baseline, many errors still occur
and they are categorized as follows:
? Rejections caused by filter equal-space were in
fact real entities, uneven spaces in the entities
were mostly human typing error;
? Choices made by filter least-space were occa-
sionally wrong. This happens most often when
two matches have a very small difference in the
number of spaces inside entities;
? Grammars either overgenerate (cause plain to-
kens to be predicted as entities) or undergenerate
(cause entities to be not detected);
? Lack of lexicon.
6 Conclusion
This paper has attempted to address the impor-
tance of space characters in Chinese linguistic pars-
ing or information extraction in semi-structured doc-
uments. Essentially, space characters can contribute
to the syntactic structure of texts and should not be
only treated as delimiters or be stripped out of the
document. This is especially true for semi-structured
documents such as CVs.
As our results indicate, integrating simple layout
information with linguistic grammars can greatly im-
prove the performance of information extraction. A
further improvement can be achieved using the two
filters introduced in the fourth section.
Although Daxtra?s grammar formalism is cho-
sen as the tool for information extraction, since it al-
ready includes treatment of space characters, other
tools are also available to carry out the same job.
For example, Edinburgh University Language Tech-
nology Group?s LT-TTT2 (Grover and Tobin, 2006)
3.
Our paper focuses mainly on Chinese CVs, but
space layout information can be used widely in other
languages and documents. In English for exam-
ple, although words are separated by a single space,
spaces are not always used as delimiters (e.g. con-
structing tables, columns), thus providing the need
for integrating space layout. In terms of document
types, plain paragraph based text (e.g. articles, blogs
etc.) may not be affected too much by space char-
acters, but integrating space layout information in
parsing these documents should not decrease perfor-
mance either. Furthermore, semi-structured docu-
ments may not be just limited to CVs: people?s online
portfolios, advertisements etc. all have space layout
information attached. Therefore, much investigation
still needs to be done on the effect of space characters
in different types of documents.
References
Chen, Liangyou, Hasan M. Jamil, and Nan Wang.
2003. Automatic wrapper generation for semi-
structured biological data based on table struc-
ture identification. Database and Expert Sys-
3http://www.ltg.ed.ac.uk/software/lt-ttt2
tems Applications, International Workshop on,
0:55.
Gao, Jiangfeng, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese Word Segmentation and
Named Entity Recognition: A Pragmatic Ap-
proach. Computational Linguistics, 31(4):531
? 574.
Grover, Claire and Richard Tobin. 2006. Rule-based
chunking and reusability. In Proceedings of the
Fifth International Conference on Language Re-
sources and Evaluation (LREC 2006), Genoa,
Italy.
Hurst, Matthew and Tetsuya Nasukawa. 2000. Lay-
out and Language: Integrating Spatial and Lin-
guistic Knowledge for Layout Understanding
Tasks. In Proceedings of COLING, pages 334
? 340.
Irmak, Utku and Torsten Suel. 2006. Interactive
Wrapper Generation with Minimal User Effort.
In Proceedings of the 15th International Confer-
ence on World Wide Web, pages 553 ? 563.
Jones, Bernard. 1994. Exploring The Role of Punc-
tuation in Parsing Natural Text. In Proceedings
of 15th Conference on Computational Linguis-
tics, pages 421 ? 425.
Ng, Hwee Tou, Chung Yong Lim, and Jessica Li Teng
Koo. 1999. Learning to Recognize Tables in
Free Text. In Proceedings of the 37th annual
meeting of the Association for Computational
Linguistics on Computational Linguistics, pages
443 ? 450.
Rus, Daniela and Kristen Summers. 1994. Using
White Space for Automated Document Structur-
ing. Technical Report TR94-1452, Cornell Uni-
versity, Department of Computer Science, July.
Zhang, Hua-Ping, Hong-Kui Yu, De-Yi Xiong, and
Qun Liu. 2003. HMM-based Chinese Lexical
Analyzer ICTCLAS. In Proceedings of the Sec-
ond SIGHAN Workshop on Chinese Language
Processing, pages 184 ? 187.
