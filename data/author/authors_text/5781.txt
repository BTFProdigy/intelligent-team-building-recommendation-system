Integrating Cross-Lingually Relevant News Articles and
Monolingual Web Documents in Bilingual Lexicon Acquisition
Takehito Utsuro? and Kohei Hino? and Mitsuhiro Kida?
Seiichi Nakagawa? and Satoshi Sato?
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606-8501, Japan
?Department of Information and Computer Sciences, Toyohashi University of Technology
Tenpaku-cho, Toyohashi, 441?8580, Japan
Abstract
In the framework of bilingual lexicon acquisition
from cross-lingually relevant news articles on the
Web, it is relatively harder to reliably estimate bilin-
gual term correspondences for low frequency terms.
Considering such a situation, this paper proposes to
complementarily use much larger monolingual Web
documents collected by search engines, as a resource
for reliably re-estimating bilingual term correspon-
dences. We experimentally show that, using a suf-
ficient number of monolingual Web documents, it
is quite possible to have reliable estimate of bilin-
gual term correspondences for those low frequency
terms.
1 Introduction
Translation knowledge acquisition from paral-
lel/comparative corpora is one of the most im-
portant research topics of corpus-based MT.
This is because it is necessary for an MT sys-
tem to (semi-)automatically increase its trans-
lation knowledge in order for it to be used in
the real world situation. One limitation of
the corpus-based translation knowledge acquisi-
tion approach is that the techniques of transla-
tion knowledge acquisition heavily rely on avail-
ability of parallel/comparative corpora. How-
ever, the sizes as well as the domain of existing
parallel/comparative corpora are limited, while
it is very expensive to manually collect paral-
lel/comparative corpora. Therefore, it is quite
important to overcome this resource scarcity
bottleneck in corpus-based translation knowl-
edge acquisition research.
In order to solve this problem, we proposed
an approach of taking bilingual news articles
on Web news sites as a source for translation
knowledge acquisition (Utsuro et al, 2003). In
the case of Web news sites in Japan, Japanese
as well as English news articles are updated ev-
eryday. Although most of those bilingual news
articles are not parallel even if they are from
the same site, certain portion of those bilingual
news articles share their contents or at least re-
port quite relevant topics. This characteristic
is quite important for the purpose of transla-
tion knowledge acquisition. Utsuro et al (2003)
showed that it is possible to acquire translation
knowledge of domain specific named entities,
event expressions, and collocational expressions
from the collection of bilingual news articles on
Web news sites.
Based on the results of our previous study,
this paper further examines the correlation of
term frequency and the reliability of bilingual
term correspondences estimated from bilingual
news articles. We show that, for high frequency
terms, it is relatively easier to reliably estimate
bilingual term correspondences. However, for
low frequency terms, it is relatively harder to re-
liably estimate bilingual term correspondences.
Low frequency problem of this type often hap-
pens when a sufficient number of bilingual news
articles are not available at hand.
Considering such a situation, this paper then
proposes to complementarily use much larger
monolingual Web documents collected by search
engines, as a resource for reliably re-estimating
bilingual term correspondences. Those col-
lected monolingual Web documents are re-
garded as comparable corpora. Here, a stan-
dard technique of estimating bilingual term cor-
respondences from comparable corpora is em-
ployed. In the evaluation, we show that, using
a sufficient number of monolingual Web docu-
ments, it is relatively easier to have reliable esti-
mate of bilingual term correspondences. As one
of the most remarkable experimental evalua-
tion results, we further show that, for the terms
which appear infrequently in news articles, the
accuracy of re-estimating bilingual term corre-
spondences does actually improve.
Figure 1: Translation Knowledge Acquisition
from Web News Sites: Overview
2 Estimating Bilingual Term
Correspondences from
Cross-Lingually Relevant News
Articles
2.1 Overview
Figure 1 illustrates the overview of our frame-
work of translation knowledge acquisition from
Web news sites. First, pairs of Japanese and
English news articles which report identical con-
tents or at least closely related contents are re-
trieved. In this cross-lingual retrieval process,
translation knowledge such as a bilingual dic-
tionary and an MT software is used for mea-
suring similarity of Japanese and English arti-
cles across languages. Then, by applying pre-
viously studied techniques of translation knowl-
edge acquisition from parallel/comparative cor-
pora, translation knowledge such as bilingual
term correspondences are acquired.
2.2 Cross-Language Retrieval of Rel-
evant News Articles
This section gives the overview of our frame-
work of cross-language retrieval of relevant news
articles from Web news sites (Utsuro et al,
2003). First, from Web news sites, both
Japanese and English news articles within cer-
tain range of dates are retrieved. Let dJ and
dE denote one of the retrieved Japanese and
English articles, respectively. Then, each En-
glish article dE is translated into a Japanese
document dMTJ by some commercial MT soft-
ware1. Each Japanese article dJ as well as the
Japanese translation dMTJ of each English ar-
ticle are next segmented into word sequences,
and word frequency vectors v(dJ ) and v(dMTJ )
are generated. Then, cosine similarities between
v(dJ ) and v(dMTJ ) are calculated
2 and pairs of
articles dJ and dE which satisfy certain criterion
are considered as candidates for cross-lingually
relevant article pairs.
As we describe in section 4.1, on Web news
sites in Japan, the number of articles up-
dated per day is far greater (about 4 times)
in Japanese than in English. Thus, it is
much easier to find cross-lingually relevant ar-
ticles for each English query article than for
each Japanese query article. Considering this
fact, we estimate bilingual term correspon-
dences from the results of cross-lingually re-
trieving relevant Japanese articles with English
query articles. For each English query article
diE and its Japanese translation d
MTi
J , the set
DiJ of Japanese articles that are within certain
range of dates and are with cosine similarities
higher than or equal to a certain lower bound
Ld is constructed:
DiJ =
{
dJ | cos(v(dMTiJ ), v(dJ )) ? Ld
}
(1)
2.3 Estimating Bilingual Term Cor-
respondences with Pseudo-
Parallel Corpus
This section describes the technique we apply to
the task of estimating bilingual term correspon-
dences from cross-lingually relevant news texts.
Here, we regard cross-lingually relevant news
texts as a pseudo-parallel corpus, to which stan-
dard techniques of estimating bilingual term
correspondences from parallel corpora can be
applied3.
1In this query translation process, we compared an
MT software with a bilingual lexicon. CLIR with query
translation by an MT software performed much better
than that by a bilingual lexicon. In the case of news
articles on Web news sites, it is relatively easier to find
articles in the other language which report closely related
contents, with just a few days difference of report dates.
In such a case, exact query translation by an MT soft-
ware is suitable, because exact translation is expected to
easily match the closely related articles in the other lan-
guage. As we mention in section 3.3, this is opposite to
the situation of monolingual Web documents, where it is
much less expected to find closely related documents in
the other language.
2It is also quite possible to employ weights other than
word frequencies such as tf ?idf and similarity measures
other than cosine measure such as dice or Jaccard coef-
ficients.
3We also applied another technique based on con-
textual vector similarities (Utsuro et al, 2003), which
First, we concatenate constituent Japanese
articles of DiJ into one article D
?i
J , and regard
the article pair diE and D
?i
J as a pseudo-parallel
sentence pair. Next, we collect such pseudo-
parallel sentence pairs and construct a pseudo-
parallel corpus PPCEJ of English and Japanese
articles:
PPCEJ =
{
?diE , D
?i
J ? | D
i
J = ?
}
Then, we apply standard techniques of es-
timating bilingual term correspondences from
parallel corpora (Matsumoto and Utsuro, 2000)
to this pseudo-parallel corpus PPCEJ . First,
from a pseudo-parallel sentence pair diE and D
?i
J ,
we extract monolingual (possibly compound4)
term pair tE and tJ :
r?tE , tJ ? s.t. ?diE?dJ , tE in d
i
E , tJ in dJ , (2)
cos(v(dMTiJ ), v(dJ )) ? Ld
Then, based on the contingency table of co-
occurrence document frequencies of tE and tJ
below, we estimate bilingual term correspon-
dences according to the statistical measures
such as the mutual information, the ?2 statistic,
the dice coefficient, and the log-likelihood ratio.
tJ ?tJ
tE df(tE , tJ ) = a df(tE ,?tJ ) = b
?tE df(?tE , tJ) = c df(?tE ,?tJ) = d
We compare the performance of those four
measures, where the ?2 statistic and the log-
likelihood ratio perform best, the dice coefficient
the second best, and the mutual information the
worst. In section 4.3, we show results with the
?2 statistic as the bilingual term correspondence
corrEJ(tE , tJ):
?2(tE , tJ) =
(ad ? bc)2
(a + b)(a + c)(b + d)(c + d)
3 Re-estimating Bilingual Term
Correspondences using
Monolingual Web Documents
3.1 Overview
This section illustrates the overview of the pro-
cess of re-estimating bilingual term correspon-
dences using monolingual Web documents col-
lected by search engines. Figure 2 gives its
rough idea.
has been well studied in the context of bilingual lexicon
acquisition from comparable corpora. In this method,
we regard cross-lingually relevant texts as a compara-
ble corpus, where bilingual term correspondences are es-
timated in terms of contextual similarities across lan-
guages. This technique is less effective than the one we
describe here (Utsuro et al, 2003).
4In the evaluation of this paper, we restrict English
and Japanese terms t
E
and t
J
to be up to five words
long.
Figure 2: Re-estimating Bilingual Term Corre-
spondences using Monolingual Web Documents:
Overview
Suppose that we have an English term, and
that the problem to solve here is to find its
Japanese translation. As we described in the
previous section and in Figure 1, with a cross-
lingually relevant Japanese and English news
articles database, we can have a certain num-
ber of Japanese translation candidates for the
target English term. Here, for high frequency
terms, it is relatively easier to have reliable
ranking of those Japanese translation candi-
dates. However, for low frequency terms, hav-
ing reliable ranking of those Japanese transla-
tion candidates is difficult. Especially, low fre-
quency problem of this type often happens when
we do not have large enough language resources
(in this case, cross-lingually relevant news arti-
cles).
Considering such a situation, re-estimation of
bilingual term correspondences proceeds as fol-
lows, using much larger monolingual Web doc-
uments sets that are easily accessible through
search engines. First, English pages which
contain the target English term are collected
through an English search engine. In the simi-
lar way, for each Japanese term in the Japanese
translation candidates, Japanese pages which
contain the Japanese term are collected through
a Japanese search engine. Then, texts con-
tained in those English and Japanese pages are
extracted and are regarded as comparable cor-
pora. Here, a standard technique of estimat-
ing bilingual term correspondences from com-
parable corpora (e.g., Fung and Yee (1998) and
Rapp (1999)) is employed. Contextual sim-
ilarity between the target English term and
the Japanese translation candidate is measured
across languages, and all the Japanese transla-
tion candidates are re-ranked according to the
contextual similarities.
3.2 Filtering by Hits of Search En-
gines
Before re-estimating bilingual term correspon-
dences using monolingual Web documents, we
assume there exists certain correlation between
hits of the English term tE and the Japanese
term tJ returned by search engines. Depending
on the hits h(tE) of tE , we restrict the hits h(tJ )
of tJ to be within the range of a lower bound
hL and an upper bound hU :
hL < h(tJ ) ? hU
As search engines, we used AltaVista
(http://www. altavista.com/ for En-
glish, and goo (http://www.goo.ne.jp/) for
Japanese. With a development data set con-
sisting of translation pairs of an English term
and a Japanese term, we manually constructed
the following rules for determining the lower
bound hL and the upper bound hU :
1. 0 < h(tE) ? 100
hL = 0, hU = 10, 000 ? h(tE)
2. 100 < h(tE) ? 20, 000
hL = 0.05 ? h(tE), hU = 1, 000, 000
3. 20, 000 < h(tE)
hL = 1, 000, hU = 50 ? h(tE)
In the experimental evaluation of Section 4.4,
the initial set of Japanese translation candi-
dates consists of 50 terms for each English term,
which are then reduced to on the average 24.8
terms with this filtering.
3.3 Re-estimating Bilingual Term
Correspondences based on Con-
textual Similarity
This section describes how to re-estimate bilin-
gual term correspondences using monolingual
Web documents collected by search engines.
For an English term tE and a Japanese term
tJ , let D(tE) and D(tJ) be the sets of docu-
ments returned by search engines with queries
tE and tJ , respectively. Then, for the English
term tE, translated contextual vector cvtrJ (tE)
is constructed as below: each English sen-
tence sE which contains tE is translated into
Japanese sentence strJ , then the term frequency
vectors5 v(strJ ) of Japanese translation s
tr
J are
5In the term frequency vectores, compound terms are
restricted to be up to five words long both for English
and Japanese.
Table 1: Statistics of # of Days, Articles, and
Article Sizes
total total average # average
# of # of of articles article
days articles per day size (bytes)
Eng 935 23064 24.7 3228.9
Jap 941 96688 102.8 837.7
summed up into the translated contextual vec-
tor cvtrJ(tE):
cvtrJ (tE) =
?
?s
E
in D(t
E
) s.t. t
E
in s
E
v(strJ )
The contextual vector cv(tJ ) for the Japanese
term tJ is also constructed by summing up the
term frequency vectors v(sJ) of each Japanese
sentence sJ which contains tJ :
cv(tJ ) =
?
?s
J
in D(t
J
) s.t. t
J
in s
J
v(sJ)
In the translation of English sentences into
Japanese, we evaluated an MT software and a
bilingual lexicon in terms of the performance of
re-estimation of bilingual term correspondences.
Unlike the situation of cross-lingually relevant
news articles mentioned in Section 2.2, trans-
lation by a bilingual lexicon is more effective
for monolingual Web documents. In the case of
monolingual Web documents, it is much less ex-
pected to find closely related documents in the
other language. In such cases, multiple trans-
lation rather than exact translation by an MT
software is suitable. In Section 4.4, we show
evaluation results with translation by a bilin-
gual lexicon6.
Finally, bilingual term correspondence
corrEJ(tE , tJ) is estimated in terms of co-
sine measure cos(cvtrJ (tE), cv(tJ )) between
contextual vectors cvtrJ (tE) and cv(tJ ).
4 Experimental Evaluation
4.1 Japanese-English Relevant News
Articles on Web News Sites
We collected Japanese and English news articles
from a Web news site. Table 1 shows the total
number of collected articles and the range of
dates of those articles represented as the num-
ber of days. Table 1 also shows the number of
articles updated in one day, and the average ar-
ticle size. The number of Japanese articles up-
dated in one day are far greater (about 4 times)
than that of English articles.
6Eijiro Ver.37, 850,000 entries, http://homepage3.
nifty.com/edp/.
Table 2: # of Japanese/English Articles Pairs with Similarity Values above Lower Bounds
Lower Bound Ld of Articles? Sim w/o 0.3 0.4 0.5
Difference of Dates (days) CLIR ? 2
# of English Articles 23064 6073 2392 701
# of Japanese Articles 96688 12367 3444 882
# of English-Japanese Article Pairs ? 16507 3840 918
Next, for several lower bounds Ld of the
similarity between English and Japanese arti-
cles, Table 2 shows the numbers of English and
Japanese articles as well as article pairs which
satisfy the similarity lower bound. Here, the
difference of dates of English and Japanese arti-
cles is within two days, with which it is guaran-
teed that, if exist, closely related articles in the
other language can be discovered (see Utsuro et
al. (2003) for details). Note that it can happen
that one article has similarity values above the
lower bound against more than one articles in
the other language.
According to our previous study (Utsuro et
al., 2003), cross-lingually relevant news arti-
cles are available in the direction of English-
to-Japanese retrieval for more than half of the
retrieval query English articles. Furthermore,
with the similarity lower bound Ld = 0.3, pre-
cision and recall of cross-language retrieval are
around 30% and 60%, respectively. Therefore,
with the similarity lower bound Ld = 0.3, at
least 1,800 (? 6, 073?0.5?0.6) English articles
have relevant Japanese articles in the results of
cross-language retrieval. Based on this analysis,
the next section gives evaluation results with
the similarity lower bound Ld = 0.3.
4.2 English Term List for Evaluation
For the evaluation of this paper, we first man-
ually select target English terms and their
reference Japanese translation, and examine
whether reference bilingual term correspon-
dences can be estimated by the methods pre-
sented in Sections 2 and 3. Target English terms
are selected by the following procedure.
First, from the whole English articles of Ta-
ble 1, any sequence of more than one words
whose frequency is more than or equal to 10 is
enumerated. This enumeration is easily imple-
mented and efficiently computed by employing
the technique of PrefixSpan (Pei et al, 2001).
Here, certain portion of those word sequences
are appropriate as compound terms, while the
rest are some fragments of a compound term,
or concatenation of those fragments. In or-
der to automatically select candidates for cor-
rect compound terms, we parse those word se-
Figure 3: Accuracy of Estimating Bilingual
Term Correspondences with News Articles
quences by Charniak parser7, and collect noun
phrases which consist of adjectives, nouns, and
present/past participles. For each of those word
sequences, the ?2 statistic against Japanese
translation candidates is calculated, then those
word sequences are sorted in descending order of
their ?2 statistic. Finally, among top 3,000 can-
didates for compound terms, 100 English com-
pound terms are randomly selected for the eval-
uation of this paper. Selected 100 terms satisfy
the following condition: those English terms can
be correctly translated neither by the MT soft-
ware used in Section 2.2, nor by the bilingual
lexicon used in Section 3.3.
4.3 Estimating Bilingual Term Cor-
respondences with News Articles
For the 100 English terms selected in the pre-
vious section, Japanese translation candidates
which satisfy the condition of the formula (2) in
Section 2.3 are collected, and are ranked accord-
ing to the ?2 statistic. Figure 3 plots the rate
of reference Japanese translation being within
top n candidates. In the figure, the plot labeled
as ?full? is the result with the whole articles in
Table 1. In this case, the accuracy of the top
ranked Japanese translation candidate is about
40%, and the rate of reference Japanese trans-
lation within top five candidates is about 75%.
7http://www.cs.brown.edu/people/ec/
Table 3: Statistics of Average Document Frequencies and Number of Days
Document Frequencies of target English Term # of Days
Data Set df(tE) df(tE, tJ) Eng Jap
freq=10, 13.6 days 14.9 9.1 13.6 21.9
freq=10, 20 days 14.9 9.1 21.0 78.7
freq=10, 200 days 14.9 9.1 200 581
freq=70, 600 days 37.4 24.9 600 872
full 53.9 35.6 935 941
On the other hand, other plots labeled as
?Freq=x, y days? are the results when the num-
ber of the news articles is reduced, which are
simulations for estimating bilingual term cor-
respondences for low frequency terms. Here,
the label ?Freq=x, y days? indicates that news
articles used for ?2 statistic estimation is re-
stricted to certain portion of the whole news
articles so that the following condition be satis-
fied: i) co-occurrence document frequency of a
target English term and its reference Japanese
translation is fixed to be x,8 ii) the number of
days be greater than or equal to y. For each
news articles data set, Table 3 shows document
frequencies df(tE) of a target English term tE ,
co-occurrence document frequencies df(tE, tJ )
of tE and its reference Japanese translation tJ ,
and the numbers of days for English as well as
Japanese articles. Those numbers are all aver-
aged over the 100 English terms. The number of
days for Japanese articles could be at maximum
five times larger than that for English articles,
because relevant Japanese articles are retrieved
against a query English article from the dates of
differences within two days (details are in Sec-
tions 2.2 and 4.1).
As can be seen from the plots of Figure 3,
the smaller the news articles data set, the lower
the plot is. Especially, in the case of the small-
est news articles data set, it is clear that re-
liable ranking of Japanese translation candi-
dates is difficult. This is because it is not easy
to discriminate the reference Japanese transla-
tion and the other candidates with statistics ob-
tained from such a small news articles data set.
4.4 Re-estimating Bilingual Term
Correspondences with Monolin-
gual Web Documents
For the 100 target English terms evaluated in
the previous section, this section describes the
result of applying the technique presented in
Section 3.3, i.e., re-estimating bilingual term
8When the co-occurrence document frequency of t
E
and t
J
in the whole news articles is less than x, all the
co-occurring dates are included.
Figure 4: Accuracy of Re-estimating Bilingual
Term Correspondences with Monolingual Web
Documents
correspondences with monolingual Web docu-
ments. For each of the 100 target English
terms, bilingual term correspondences are re-
estimated against candidates of Japanese trans-
lation ranked within top 50 according to the
?2 statistic. Here, as a simulation for terms
that are infrequent in news articles, 50 can-
didate terms for Japanese translation are col-
lected from the smallest data set labeled as
?Freq=10, 13.6 days?. As mentioned in Sec-
tion 3.2, those 50 candidates are reduced to on
the average 24.8 terms with the filtering by hits
of search engines. For each of an English term
tE and a Japanese term tJ , 100 monolingual
documents are collected by search engines9 10.
Figure 4 compares the plots of re-estimation
with monolingual Web documents and estima-
tion by news articles (data set ?Freq=10, 13.6
9In the result of our preliminary evaluation, accuracy
of re-estimating bilingual term correspondences did not
improve even if more than 100 documents were used.
10Alternatively, as the monolingual documents from
which contextual vectors are constructed, we evaluated
each of the short passages listed in the summary pages
returned by search engines, instead of the whole docu-
ments of the URLs listed in the summary pages. The
difference of the performance of bilingual term corre-
spondence estimation is little, while the computational
cost can reduced to almost 5%.
days?). It is clear from this result that mono-
lingual Web documents contribute to improving
the accuracy of estimating bilingual term corre-
spondences for low frequency terms.
One of the major reasons for this improve-
ment is that topics of monolingual Web doc-
uments collected through search engines are
much more diverse than those of news articles.
Such diverse topics help discriminate correct
and incorrect Japanese translation candidates.
For example, suppose that the target English
term tE is ?special anti-terrorism law? and its
reference Japanese translation is ???????
????. In the news articles we used for evalua-
tion, most articles in which tE or tJ appear have
?dispatch of Self-Defense Force for reconstruc-
tion of Iraq? as their topics. Here, Japanese
translation candidates other than ??????
????? that are highly ranked according to
the ?2 statistic are: e.g., ????? (dissolution
of the House of Representatives)? and ?????
??? (assistance for reconstruction of Iraq)?,
which frequently appear in the topic of ?dis-
patch of Self-Defense Force for reconstruction
of Iraq?.
On the other hand, in the case of monolin-
gual Web documents collected through search
engines, it can be expected that topics of docu-
ments may vary according to the query terms.
In the case of the example above, the major
topic is ?dispatch of Self-Defense Force for re-
construction of Iraq? for both of reference terms
tE and tJ , while major topics for other Japanese
translation candidates are: ?issues on Japanese
Diet? for ????? (dissolution of the House
of Representatives)? and ?issues on reconstruc-
tion of Iraq, not only in Japan, but all over the
world? for ???????? (assistance for re-
construction of Iraq)?. Those topics of incor-
rect Japanese translation candidates are differ-
ent from that of the target English term tE, and
their contextual vector similarities against the
target English term tE are relatively low com-
pared with the reference Japanese translation
tJ . Consequently, the reference Japanese trans-
lation tJ is re-ranked higher compared with the
ranking based on news articles.
5 Related Works
In large scale experimental evaluation of bilin-
gual term correspondence estimation from com-
parable corpora, it is difficult to estimate bilin-
gual term correspondences against every possi-
ble pair of terms due to its computational com-
plexity. Previous works on bilingual term cor-
respondence estimation from comparable cor-
pora controlled experimental evaluation in var-
ious ways in order to reduce this computational
complexity. For example, Rapp (1999) filtered
out bilingual term pairs with low monolingual
frequencies (those below 100 times), while Fung
and Yee (1998) restricted candidate bilingual
term pairs to be pairs of the most frequent 118
unknown words. Cao and Li (2002) restricted
candidate bilingual compound term pairs by
consulting a seed bilingual lexicon and requir-
ing their constituent words to be translation
of each other across languages. On the other
hand, in the framework of bilingual term corre-
spondences estimation of this paper, the compu-
tational complexity of enumerating translation
candidates can be easily avoided with the help of
cross-language retrieval of relevant news texts.
Furthermore, unlike Cao and Li (2002), bilin-
gual term correspondences for compound terms
are not restricted to compositional translation.
6 Conclusion
In the framework of bilingual lexicon acquisition
from cross-lingually relevant news articles on
the Web, it has been relatively harder to reliably
estimate bilingual term correspondences for low
frequency terms. This paper proposed to com-
plementarily use much larger monolingual Web
documents collected by search engines, as a re-
source for reliably re-estimating bilingual term
correspondences. We showed that, for the terms
which appear infrequently in news articles, the
accuracy of re-estimating bilingual term corre-
spondences actually improved.
References
Y. Cao and H. Li. 2002. Base noun phrase translation
using Web data and the EM algorithm. In Proc. 19th
COLING, pages 127?133.
P. Fung and L. Y. Yee. 1998. An IR approach for trans-
lating new words from nonparallel, comparable texts.
In Proc. 17th COLING and 36th ACL, pages 414?420.
Y. Matsumoto and T. Utsuro. 2000. Lexical knowledge
acquisition. In R. Dale, H. Moisl, and H. Somers,
editors, Handbook of Natural Language Processing,
chapter 24, pages 563?610. Marcel Dekker Inc.
J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto. 2001.
Prefixspan: Mining sequential patterns efficiently by
prefix-projected pattern growth. In Proc. Inter. Conf.
Data Mining, pages 215?224.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proc. 37th ACL, pages 519?526.
T. Utsuro, T. Horiuchi, T. Hamamoto, K. Hino, and
T. Nakayama. 2003. Effect of cross-language IR in
bilingual lexicon acquisition from comparable cor-
pora. In Proc. 10th EACL, pages 355?362.
An Empirical Study on
Multiple LVCSR Model Combination by Machine Learning
Takehito Utsuro? Yasuhiro Kodama? Tomohiro Watanabe??
Hiromitsu Nishizaki?? Seiichi Nakagawa??
?Graduate School of Informatics, Kyoto University, Kyoto, 606-8501, Japan
?Sony Corporation ??Toyohashi University of Technology ??University of Yamanashi
Abstract
This paper proposes to apply machine learn-
ing techniques to the task of combining out-
puts of multiple LVCSR models. The proposed
technique has advantages over that by voting
schemes such as ROVER, especially when the
majority of participating models are not reli-
able. In this machine learning framework, as
features of machine learning, information such
as the model IDs which output the hypothe-
sized word are useful for improving the word
recognition rate. Experimental results show
that the combination results achieve a relative
word error reduction of up to 39 % against the
best performing single model and that of up to
23 % against ROVER. We further empirically
show that it performs better when LVCSR mod-
els to be combined are chosen so as to cover as
many correctly recognized words as possible,
rather than choosing models in descending or-
der of their word correct rates.
1 Introduction
Since current speech recognizers? outputs are far from
perfect and always include a certain amount of recogni-
tion errors, it is quite desirable to have an estimate of con-
fidence for each hypothesized word. This is especially
true for many practical applications of speech recogni-
tion systems such as automatic weighting of additional,
non-speech knowledge sources, keyword based speech
understanding, and recognition error rejection ? confir-
mation in spoken dialogue systems. Most of previous
works on confidence measures (e.g., (Kemp and Schaaf,
1997) ) are based on features available in a single LVCSR
model. However, it is well known that a voting scheme
such as ROVER (Recognizer output voting error reduc-
tion) for combining multiple speech recognizers? outputs
can achieve word error reduction (Fiscus, 1997; Ever-
mann and Woodland, 2000). Considering the success of
a simple voting scheme such as ROVER, it also seems
quite possible to improve reliability of previously stud-
ied features for confidence measures by simply exploit-
ing more than one speech recognizers? outputs. From this
observation, we experimentally evaluated the agreement
among the outputs of multiple Japanese LVCSR models,
with respect to whether it is effective as an estimate of
confidence for each hypothesized word.
Our previous study reported that the agreement be-
tween the outputs with two different acoustic models can
achieve quite reliable confidence, and also showed that
the proposed measure of confidence outperforms previ-
ously studied features for confidence measures such as
the acoustic stability and the hypothesis density (Kemp
and Schaaf, 1997). We also reported evaluation results
with 26 distinct acoustic models and identified the fea-
tures of acoustic models most effective in achieving high
confidence (Utsuro et al, 2002). The most remarkable
results are as follows: for the newspaper sentence ut-
terances, nearly 99% precision is achieved by decreas-
ing 94% word correct rate of the best performing single
model by only 7%. For the broadcast news speech, nearly
95% precision is achieved by decreasing 72% word cor-
rect rate of the best performing single model by only 8%.
Based on those results of our previous studies, this pa-
per proposes to apply machine learning techniques to the
task of combining outputs of multiple LVCSR models.
As a machine learning technique, the Support Vector Ma-
chine (SVM) (Vapnik, 1995) learning technique is em-
ployed. A Support Vector Machine is trained for choos-
ing the most confident one among several hypothesized
words, where, as features of SVM learning, information
such as the model IDs which output the hypothesized
word, its part-of-speech, and the number of syllables are
useful for improving the word recognition rate.
Model combination by high performance machine
learning techniques such as SVM learning has advantages
over that by voting schemes such as ROVER and oth-
ers (Fiscus, 1997; Evermann and Woodland, 2000), espe-
cially when the majority of participating models are not
reliable. In the model combination techniques based on
voting schemes, outputs of multiple LVCSR models are
combined according to simple majority vote or weighted
majority vote based on confidence of each hypothesized
word such as its likelihood. The results of model com-
bination by those voting techniques can be harmed when
the majority of participating models have quite low per-
formance and output word recognition errors with high
confidence. On the other hand, in the model combination
by high performance machine learning techniques such
as SVM learning, among those participating models, re-
liable ones and unreliable ones are easily discriminated
through the training process of machine learning frame-
work. Furthermore, depending on the features of hypoth-
esized words such as its part-of-speech and the number
of syllables, outputs of multiple models are combined in
an optimal fashion so as to minimize word recognition
errors in the combination results.
Experimental results show that model combination by
SVM achieves the followings: i.e., for the newspaper sen-
tence utterances, a relative word error reduction of 39 %
against the best performing single model and that of 23
% against ROVER; for the broadcast news speech, a rel-
ative word error reduction of 13 % against the best per-
forming single model and that of 8 % against ROVER.
We further empirically show that it performs better when
LVCSR models to be combined are chosen so as to cover
as many correctly recognized words as possible, rather
than choosing models in descending order of their word
correct rates1.
2 Specification of Japanese LVCSR
Systems
2.1 Decoders
As decoders of Japanese LVCSR systems, we use the one
named Julius, which is provided by IPA Japanese dicta-
tion free software project (Kawahara and others, 1998),
as well as the one named SPOJUS (Kai et al, 1998),
which has been developed in Nakagawa lab., Toyohashi
Univ. of Tech., Japan. Both decoders are composed of
two decoding passes, where the first pass uses the word
bigram, and the second pass uses the word trigram.
2.2 Acoustic Models
The acoustic models of Japanese LVCSR systems are
based on Gaussian mixture HMM. We evaluate phoneme-
based HMMs as well as syllable-based HMMs.
2.2.1 Acoustic Models with the Decoder JULIUS
As the acoustic models used with the decoder Julius,
we evaluate phoneme-based HMMs as well as syllable-
based HMMs. The following four types of HMMs are
evaluated: i) triphone model, ii) phonetic tied mixture
1Compared with our previous report (Utsuro et al, 2003),
the major achievement of the paper is this empirical result.
Utsuro et al (2003) examined the correlation between each
word?s confidence and the word?s features, and then introduced
the framework of combining outputs of multiple LVCSR mod-
els by SVM learning.
(PTM) triphone model, iii) monophone model, and iv)
syllable model. Every HMM phoneme model is gender-
dependent (male). For each of the four models above,
we evaluate both HMMs with and without the short pause
state, which amount to 8 acoustic models in total.
2.2.2 Acoustic Models with the Decoder SPOJUS
The acoustic models used with the decoder SPOJUS are
based on syllable HMMs, which have been developed
in Nakagawa laboratory, Toyohashi University of Tech-
nology, Japan (Nakagawa and Yamamoto, 1996). The
acoustic models are gender-dependent (male) syllable
unit HMMs. Among various combinations of features of
acoustic models2, we carefully choose 9 acoustic models
so that they include the best performing ones as well as
a sufficient number of minimal pairs which have differ-
ence in only one feature. Then, for each of the 9 models,
we evaluate both HMMs with and without the short pause
states, which amount to 18 acoustic models in total.
2.3 Language Models
As the language models, the following two types of word
bigram / trigram language models for 20k vocabulary
size are evaluated: 1) the one trained using 45 months
Mainichi newspaper articles, 2) the one trained using 5
years Japanese NHK (Japan Broadcasting Corporation)
broadcast news scripts (about 120,000 sentences).
2.4 Evaluation Data Sets
The evaluation data sets consist of newspaper sentence
utterances, which are relatively easier for speech recog-
nizers, and rather harder broadcast news speech: 1) 100
newspaper sentence utterances from 10 male speakers
consisting of 1,565 words, selected by IPA Japanese dic-
tation free software project (Kawahara and others, 1998)
from the JNAS (Japanese Newspaper Article Sentences)
speech data (Itou and others, 1998), 2) 175 Japanese
NHK broadcast news (June 1st, 1996) speech sentences
consisting of 6,813 words, uttered by 14 male speakers
(six announcers and eight reporters).
2.5 Word Recognition Rates
Word correct and accuracy rates of the individual LVCSR
models for the above two evaluation data sets are mea-
sured, where for the recognition of the newspaper sen-
tence utterances, the language model used is the one
trained using newspaper articles, and for the recognition
of the broadcast news speech, the language model used
is the one trained using broadcast news scripts. Word
recognition rates for the above two evaluation data sets
are summarized as below:
2Sampling frequencies, frame shift lengths, feature param-
eters, covariance matrices, and self loop transition / duration
control.
(a) Newspaper Sentence
(b) Broadcast News
Figure 1: Comparison among Combination by SVM /
(Weighted) Majority Votes / Individual Models
newspaper sentence utterances
decoder word correct (%) word accuracy (%)
Julius 93.0(max) to 72.7(min) 90.4(max) to 69.4(min)
SPOJUS 90.2(max) to 78.1(min) 85.3(max) to 51.0(min)
broadcast news speech
decoder word correct (%) word accuracy (%)
Julius 71.7(max) to 49.0(min) 68.8(max) to 39.7(min)
SPOJUS 70.7(max) to 55.4(min) 62.8(max) to 36.2(min)
3 Combining Outputs of Multiple LVCSR
Models by SVM
This section describes the results of applying SVM learn-
ing technique to the task of combining outputs of multiple
LVCSR models considering the confidence of each word.
We divide each of the data sets described in Section 2.4
into two halves3, where one half is used for training and
the other half for testing. A Support Vector Machine
is trained for choosing the most confident one among
several hypothesized words from the outputs of the 26
LVCSR models4. As features of the SVM learning, we
use the model IDs which output the word, the part-of-
speech of the word, and the number of syllables 5. As
3It is guaranteed that the two halves do not share speakers.
4We used SVM light (http://svmlight.joachims.
org/) as a tool for SVM learning. We compared linear and
quadratic kernels and the linear kernel performs better.
5Contribution of the parts-of-speech and the numbers of syl-
lables was slight. We also evaluated the effect of acoustic and
(a) Newspaper Sentence
(b) Broadcast News
Figure 2: Comparing Methods for Combining Outputs of
n (3 ? n ? 26) Models
classes of the SVM learning, we use whether each hy-
pothesized word is correct or incorrect. Since Support
Vector Machines are binary classifiers, we regard the dis-
tance from the separating hyperplane to each hypothe-
sized word as the word?s confidence. The outputs of the
26 LVCSR models are aligned by Dynamic Time Warp-
ing, and the most confident one among those competing
hypothesized words is chosen as the result of model com-
bination. We also require the confidence of hypothesized
words to be higher than a certain threshold, and choose
the ones with the confidence above this threshold as the
result of model combination.
The results of the performance evaluation against the
test data are shown in Figure 1. All the results in Fig-
ure 1 are the best performing ones among those for com-
bining outputs of n (3 ? n ? 26) models. The results
of model combination by SVM are indicated as ?SVM?.
As a baseline performance, that of the best performing
single model with respect to word correct rate (?Individ-
ual Model with Max Cor?) is shown. (Note that their
word recognition rates are those for the half of the whole
data set, and thus different from those in Section 2.5.)
For both speech data, model combination by SVM sig-
language scores of each hypothesized word as features of SVM,
where their contribution to improving the overall performance
was very little.
(a) Newspaper Sentence
(b) Broadcast News
Figure 3: Comparison between Maximizing Recall of
Union / Descending Order of Word Correct Rates
nificantly outperforms the best performing single model.
In terms of word accuracy rate, relative word error re-
duction are 39 % for the newspaper sentence utterances
and 13 % for the broadcast news speech. Figure 1 also
shows the performance of ROVER (Fiscus, 1997) as an-
other baseline, where ?Majority Vote? shows the perfor-
mance of the strategy of outputting no word at a tie, while
?Weighted Majority Vote? shows the performance when,
for each individual model, word correct rate for each sen-
tence is estimated and used as the weight of hypothesized
words. Model combination by SVM mostly outperforms
ROVER for both speech data. In terms of word accuracy
rate, relative word error rate reduction are 23 % for the
newspaper sentence utterances and 8 % for the broadcast
news speech6.
Figure 2 plots the changes of word accuracy rates
against the increasing number of models which partici-
pate in LVCSR model combination. Here, LVCSR mod-
els to be combined are chosen so as to cover as many cor-
rectly recognized words as possible, rather than choosing
models in descending order of their word correct rates.
(As we show later, the former outperforms the latter.) It
6Remarkable improvements are achieved especially in word
accuracy rates. This is due to the strategy of requiring the confi-
dence of hypothesized words to be higher than a certain thresh-
old, where insertion error words tend to be discarded.
is quite clear from this result that the difference of model
combination by SVM and (weighted) majority votes be-
comes much larger as more and more models participate
in model combination. This is because the majority of
participating models become unreliable in the second half
of the curves in Figure 2.
Figure 3 compares the model selection procedures, i.e.,
choosing models so as to cover as many correctly recog-
nized words as possible (indicated as ?Maximizing Recall
of Union?), and choosing models in descending order of
their word correct rates (indicated as ?Descending Order
of Word Correct Rates?). The former performs better in
the first half of the curves. This result indicates that, even
if recognition error words increase in the outputs of mod-
els participating in LVCSR model combination, it is bet-
ter to cover as many correctly recognized words as pos-
sible. This is because, in the model combination by high
performance machine learning techniques such as SVM
learning, reliable and unreliable hypothesized words are
easily discriminated through the training process.
4 Concluding Remarks
This paper proposed to apply the SVM learning technique
to the task of combining outputs of multiple LVCSR
models. The proposed technique has advantages over that
by voting schemes such as ROVER, especially when the
majority of participating models are not reliable.
References
G. Evermann and P. Woodland. 2000. Posterior probability
decoding, confidence estimation and system combination. In
Proc. NIST Speech Transcription Workshop.
J. G. Fiscus. 1997. A post-processing system to yield reduced
word error rates: Recognizer output voting error reduction
(ROVER). In Proc. ASRU, pages 347?354.
K. Itou et al 1998. The design of the newspaper-based
Japanese large vocabulary continuous speech recognition
corpus. In Proc. 5th ICSLP, pages 3261?3264.
A. Kai, Y. Hirose, and S. Nakagawa. 1998. Dealing with out-
of-vocabulary words and speech disfluencies in an n-gram
based speech understanding system. In Proc. 5th ICSLP,
pages 2427?2430.
T. Kawahara et al 1998. Sharable software repository for
Japanese large vocabulary continuous speech recognition. In
Proc. 5th ICSLP, pages 3257?3260.
T. Kemp and T. Schaaf. 1997. Estimating confidence using
word lattices. In Proc. 5th Eurospeech, pages 827?830.
S. Nakagawa and K. Yamamoto. 1996. Evaluation of segmen-
tal unit input HMM. In Proc. 21st ICASSP, pages 439?442.
T. Utsuro, T. Harada, H. Nishizaki, and S. Nakagawa. 2002.
A confidence measure based on agreement among multiple
LVCSR models ? correlation between pair of acoustic mod-
els and confidence ?. In Proc. 7th ICSLP, pages 701?704.
T. Utsuro, Y. Kodama, T. Watanabe, H. Nishizaki, and S. Nak-
agawa. 2003. Confidence of agreement among multiple
LVCSR models and model combination by SVM. In Proc.
28th ICASSP, volume I, pages 16?19.
V. N. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer-Verlag.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 197?200,
Prague, June 2007. c?2007 Association for Computational Linguistics
Expanding Indonesian-Japanese Small Translation Dictionary
Using a Pivot Language
Masatoshi Tsuchiya? Ayu Purwarianti? Toshiyuki Wakita? Seiichi Nakagawa?
?Information and Media Center / ?Department of Information and Computer Sciences,
Toyohashi University of Technology
tsuchiya@imc.tut.ac.jp, {wakita,ayu,nakagawa}@slp.ics.tut.ac.jp
Abstract
We propose a novel method to expand a
small existing translation dictionary to a
large translation dictionary using a pivot lan-
guage. Our method depends on the assump-
tion that it is possible to find a pivot lan-
guage for a given language pair on con-
dition that there are both a large transla-
tion dictionary from the source language
to the pivot language, and a large transla-
tion dictionary from the pivot language to
the destination language. Experiments that
expands the Indonesian-Japanese dictionary
using the English language as a pivot lan-
guage shows that the proposed method can
improve performance of a real CLIR system.
1 Introduction
Rich cross lingual resources including large trans-
lation dictionaries are necessary in order to realize
working cross-lingual NLP applications. However,
it is infeasible to build such resources for all lan-
guage pairs, because there are many languages in the
world. Actually, while rich resources are available
for several popular language pairs like the English
language and the Japanese language, poor resources
are only available for rest unfamiliar language pairs.
In order to resolve this situation, automatic con-
struction of translation dictionary is effective, but it
is quite difficult as widely known. We, therefore,
concentrate on the task of expanding a small existing
translation dictionary instead of it. Let us consider
three dictionaries: a small seed dictionary which
consists of headwords in the source language and
their translations in the destination language, a large
source-pivot dictionarywhich consists of headwords
in the source language and their translations in the
pivot language, and a large pivot-destination dictio-
nary which consists of headwords in the pivot lan-
guage and their translations in the destination lan-
guage. When these three dictionaries are given, ex-
panding the seed dictionary is to translate words in
the source language that meets two conditions: (1)
they are not contained in the seed dictionary, and (2)
they can be translated to the destination language
transitively referring both the source-pivot dictio-
nary and the pivot-destination dictionary.
Obviously, this task depends on two assumptions:
(a) the existence of the small seed dictionary, and
(b) the existence of the pivot language which meets
the condition that there are both a large source-
pivot dictionary and a large pivot-destination dic-
tionary. Because of the first assumption, it is true
that this task cannot be applied to a brand-new lan-
guage pair. However, the number of such brand-
new language pairs are decreasing while machine-
readable language resources are increasing. More-
over, The second assumption is valid for many lan-
guage pairs, when supposing the English language
as a pivot. From these point of view, we think that
the expansion task is more promising, although it de-
pends more assumptions than the construction task.
There are two different points among the expan-
sion task and the construction task. Previous re-
searches of the construction task can be classified
into two groups. The first group consists of re-
searches to construct a new translation dictionary for
a fresh language pair from existing translation dic-
tionaries or other language resources (Tanaka and
Umemura, 1994). In the first group, information of
the seed dictionary are not counted in them unlike
the expansion task, because it is assumed that there
is no seed dictionary for such fresh language pairs.
The second group consists of researches to translate
197
xs
v(xs) vt(xs)
ys zs u(zs)
Corpus in
the source
Source-Pivot
Dictionary
Pivot-
Destination
Dictionary
Corpus in
the destination
Seed
Dictionary
Select
output
words
Figure 1: Translation Procedure
novel words using both a large existing translation
dictionary and other linguistic resources like huge
parallel corpora (Tonoike et al, 2005). Because al-
most of novel words are nouns, these researches fo-
cus into the task of translating nouns. In the expan-
sion task, however, it is necessary to translate verbs
and adjectives as well as nouns, because a seed dic-
tionary will be so small that only basic words will be
contained in it if the target language pair is unfamil-
iar. We will discuss about this topic in Section 3.2.
The remainder of this paper is organised as fol-
lows: Section 2 describes the method to expand a
small seed dictionary. The experiments presented in
Section 3 shows that the proposed method can im-
prove performance of a real CLIR system. This pa-
per ends with concluding remarks in Section 4.
2 Method of Expanding Seed Dictionary
The proposed method roughly consists of two steps
shown in Figure 1. The first step is to generate a co-
occurrence vector on the destination language cor-
responding to an input word, using both the seed
dictionary and a monolingual corpus in the source
language. The second step is to list translation can-
didates up, referring both the source-pivot dictionary
and the pivot-destination dictionary, and to calculate
their co-occurrence vectors based on a monolingual
corpus in the destination.
The seed dictionary is used to convert a co-
occurrence vector in the source language into a
vector in the destination language. In this paper,
f(wi, wj) represents a co-occurrence frequency of
a word wi and a word wj for all languages. A co-
occurrence vector v(xs) of a word xs in the source
is:
v(xs) = (f(xs, x1), . . . , f(xs, xn)), (1)
where xi(i = 1, 2, . . . , n) is a headword of the
seed dictionary D. A co-occurrence vector v(xs),
whose each element is corresponding to a word in
the source, is converted into a vector vt(xs), whose
each element is corresponding to a word in the des-
tination, referring the dictionary D:
vt(xs) = (ft(xs, z1), . . . , ft(xs, zm)), (2)
where zj(j = 1, 2, . . . ,m) is a translation word
which appears in the dictionary D. The function
ft(xs, zk), which assigns a co-occurrence degree be-
tween a word xs and a word zj in the destination
based on a co-occurrence vector of a word xs in the
source, is defined as follows:
ft(xs, zj) =
n
?
i=1
f(xs, xi) ? ?(xi, zj). (3)
where ?(xi, zj) is equal to one when a word zj is in-
cluded in a translation word set D(xi), which con-
sists of translation words of a word xi, and zero oth-
erwise.
A set of description sentences Ys in the pivot
are obtained referring the source-pivot dictionary
for a word xs. After that, a description sentence
ys ? Ys in the pivot is converted to a set of de-
scription sentences Zs in the destination referring
the pivot-destination dictionary. A co-occurrence
vector against a candidate description sentence zs =
z1sz2s ? ? ? zls, which is an instance of Zs, is calculated
by this equation:
u(zs) =
( l
?
k=1
f(zks , z1) , . . . ,
l
?
k=1
f(zks , zm)
)
(4)
Finally, the candidate zs which meets a certain
condition is selected as an output. Two conditions
are examined in this paper: (1) selecting top-n can-
didates from sorted ones according to each similarity
score, and (2) selecting candidates whose similarity
scores are greater than a certain threshold. In this pa-
per, cosine distance s(vt(xs),u(zs)) between a vec-
tor based on an input word xs and a vector based on
198
a candidate zs is used as the similarity score between
them.
3 Experiments
In this section, we present the experiments of the
proposed method that the Indonesian language, the
English language and the Japanese language are
adopted as the source language, the pivot language
and the destination language respectively.
3.1 Experimental Data
The proposed method depends on three translation
dictionaries and two monolingual corpora as de-
scribed in Section 2.
Mainichi Newspaper Corpus (1993?1995), which
contains 3.5M sentences consist of 140M words, is
used as the Japanese corpus. When measuring simi-
larity between words using co-occurrence vectors, it
is common that a corpus in the source language for
the similar domain to one of the corpus in the source
language is more suitable than one for a different do-
main. Unfortunately, because we could not find such
corpus, the articles which were downloaded from
the Indonesian Newspaper WEB sites1 are used as
the Indonesian corpus. It contains 1.3M sentences,
which are tokenized into 10M words.
An online Indonesian-Japanese dictionary2 con-
tains 10,172 headwords, however, only 6,577 head-
words of them appear in the Indonesian corpus. We
divide them into two sets: the first set which con-
sists of 6,077 entries is used as the seed dictionary,
and the second set which consists of 500 entries is
used to evaluate translation performance. Moreover,
an online Indonesian-English dictionary3, and an
English-Japanese dictionary(Michibata, 2002) are
also used as the source-pivot dictionary and the
pivot-destination dictionary.
3.2 Evaluation of Translation Performance
As described in Section 2, two conditions of select-
ing output words among candidates are examined.
Table 1 shows their performances and the baseline,
1http://www.kompas.com/,
http://www.tempointeraktif.com/
2http://m1.ryu.titech.ac.jp/?indonesia/
todai/dokumen/kamusjpina.pdf
3http://nlp.aia.bppt.go.id/kebi
that is the translation performance when all candi-
dates are selected as output words. It is revealed that
the condition of selecting top-n candidates outper-
forms the another condition and the baseline. The
maximum F?=1 value of 52.5% is achieved when
selecting top-3 candidates as output words.
Table 2 shows that the lexical distribution of head-
words contained in the seed dictionary are quite sim-
ilar to the lexical distribution of headwords con-
tained in the source-pivot dictionary. This obser-
vation means that it is necessary to translate verbs
and adjectives as well as nouns, when expanding this
seed dictionary. Table 3 shows translation perfor-
mances against nouns, verbs and adjectives, when
selecting top-3 candidates as output words. The pro-
posed method can be regarded likely because it is
effective to verbs and adjectives as well as to nouns,
whereas the baseline precision of verbs is consider-
ably lower than the others.
3.3 CLIR Performance Improved by
Expanded Dictionary
In this section, performance impact is presented
when the dictionary expanded by the proposed
method is adopted to the real CLIR system proposed
in (Purwarianti et al, 2007).
NTCIR3 Web Retrieval Task(Eguchi et al, 2003)
provides the evaluation dataset and defines the eval-
uation metric. The evaluation metric consists of four
MAP values: PC, PL, RC and RL. They are cor-
responding to assessment types respectively. The
dataset consists 100GB Japanese WEB documents
and 47 queries of Japanese topics. The Indonesian
queries, which are manually translated from them,
are used as inputs of the experiment systems. The
number of unique words which occur in the queries
is 301, and the number of unique words which are
not contained in the Indonesian-Japanese dictionary
is 106 (35%). It is reduced to 78 (26%), while the
existing dictionary that contains 10,172 entries is ex-
panded to the dictionary containing 20,457 entries
with the proposed method.
Table 4 shows the MAP values achieved by both
the baseline systems using the existing dictionary
and ones using the expanded dictionary. The for-
mer three systems use existing dictionaries, and the
latter three systems use the expanded one. The 3rd
system translates keywords transitively using both
199
Table 1: Comparison between Conditions of Selecting Output Words
Selecting top-n candidates Selecting plausible candidates Baseline
n = 1 n = 2 n = 3 n = 5 n = 10 x = 0.1 x = 0.16 x = 0.2 x = 0.3
Prec. 55.4% 49.9% 46.2% 40.0% 32.2% 20.8% 23.6% 25.8% 33.0% 18.9%
Rec. 40.9% 52.6% 60.7% 67.4% 74.8% 65.3% 50.1% 40.0% 16.9% 82.5%
F?=1 47.1% 51.2% 52.5% 50.2% 45.0% 31.6% 32.1% 31.4% 22.4% 30.8%
Table 2: Lexical Classification of Headwords
Indonesian- Indonesian-
Japanese English
# of nouns 4085 (57.4%) 15718 (53.5%)
# of verbs 1910 (26.8%) 9600 (32.7%)
# of adjectives 795 (11.2%) 3390 (11.5%)
# of other words 330 (4.6%) 682 (2.3%)
Total 7120 (100%) 29390 (100%)
Table 3: Performance for Nouns, Verbs and Adjectives
Noun Verb Adjective
n = 3 Baseline n = 3 Baseline n = 3 Baseline
Prec. 49.1% 21.8% 41.0% 14.7% 46.9% 26.7%
Rec. 65.6% 80.6% 52.3% 84.1% 59.4% 88.4%
F?=1 56.2% 34.3% 46.0% 25.0% 52.4% 41.0%
Table 4: CLIR Performance
PC PL RC RL
(1) Existing Indonesian-Japanese dictionary 0.044 0.044 0.037 0.037
(2) Existing Indonesian-Japanese dictionary and Japanese proper name dictionary 0.054 0.052 0.047 0.045
(3) Indonesian-English-Japanese transitive translation with statistic filtering 0.078 0.072 0.055 0.053
(4) Expanded Indonesian-Japanese dictionary 0.061 0.059 0.046 0.046
(5) Expanded Indonesian-Japanese dictionary with Japanese proper name dictionary 0.066 0.063 0.049 0.049
(6) Expanded Indonesian-Japanese dictionary with Japanese proper name dictionary and
statistic filtering
0.074 0.072 0.059 0.058
the source-pivot dictionary and the pivot-destination
dictionary, and the others translate keywords using
either the existing source-destination dictionary or
the expanded one. The 3rd system and the 6th sys-
tem try to eliminate unnecessary translations based
statistic measures calculated from retrieved docu-
ments. These measures are effective as shown in
(Purwarianti et al, 2007), but, consume a high run-
time computational cost to reduce enormous transla-
tion candidates statistically. It is revealed that CLIR
systems using the expanded dictionary outperform
ones using the existing dictionary without statistic
filtering. And more, it shows that ones using the ex-
panded dictionary without statistic filtering achieve
near performance to the 3rd system without paying
a high run-time computational cost. Once it is paid,
the 6th system achieves almost same score of the 3rd
system. These observation leads that we can con-
clude that our proposed method to expand dictionary
is valuable to a real CLIR system.
4 Concluding Remarks
In this paper, a novel method of expanding a small
existing translation dictionary to a large translation
dictionary using a pivot language is proposed. Our
method uses information obtained from a small ex-
isting translation dictionary from the source lan-
guage to the destination language effectively. Exper-
iments that expands the Indonesian-Japanese dictio-
nary using the English language as a pivot language
shows that the proposed method can improve perfor-
mance of a real CLIR system.
References
Koji Eguchi, Keizo Oyama, Emi Ishida, Noriko Kando, , and
Kazuko Kuriyama. 2003. Overview of the web retrieval task
at the third NTCIR workshop. In Proceedings of the Third
NTCIR Workshop on research in Information Retrieval, Au-
tomatic Text Summarization and Question Answering.
Hideki Michibata, editor. 2002. Eijiro. ALC, 3. (in Japanese).
Ayu Purwarianti, Masatoshi Tsuchiya, and Seiichi Nakagawa.
2007. Indonesian-Japanese transitive translation using En-
glish for CLIR. Journal of Natural Language Processing,
14(2), Apr.
Kumiko Tanaka and Kyoji Umemura. 1994. Construction of
a bilingual dictionary intermediated by a third language. In
Proceedings of the 15th International Conference on Com-
putational Linguistics.
Masatugu Tonoike, Mitsuhiro Kida, Toshihiro Takagi, Yasuhiro
Sasaki, Takehito Utsuro, and Satoshi Sato. 2005. Trans-
lation estimation for technical terms using corpus collected
from the web. In Proceedings of the Pacific Association for
Computational Linguistics, pages 325?331, August.
200
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 125?128,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Robust Extraction of Named Entity Including Unfamiliar Word
Masatoshi Tsuchiya? Shinya Hida? Seiichi Nakagawa?
?Information and Media Center / ?Department of Information and Computer Sciences,
Toyohashi University of Technology
tsuchiya@imc.tut.ac.jp, {hida,nakagawa}@slp.ics.tut.ac.jp
Abstract
This paper proposes a novel method to extract
named entities including unfamiliar words
which do not occur or occur few times in a
training corpus using a large unannotated cor-
pus. The proposed method consists of two
steps. The first step is to assign the most simi-
lar and familiar word to each unfamiliar word
based on their context vectors calculated from
a large unannotated corpus. After that, tra-
ditional machine learning approaches are em-
ployed as the second step. The experiments of
extracting Japanese named entities from IREX
corpus and NHK corpus show the effective-
ness of the proposed method.
1 Introduction
It is widely agreed that extraction of named entity
(henceforth, denoted as NE) is an important sub-
task for various NLP applications. Various ma-
chine learning approaches such as maximum en-
tropy(Uchimoto et al, 2000), decision list(Sassano
and Utsuro, 2000; Isozaki, 2001), and Support
Vector Machine(Yamada et al, 2002; Isozaki and
Kazawa, 2002) were investigated for extracting NEs.
All of them require a corpus whose NEs are an-
notated properly as training data. However, it is dif-
ficult to obtain an enough corpus in the real world,
because there are increasing the number of NEs like
personal names and company names. For example,
a large database of organization names(Nichigai As-
sociates, 2007) already contains 171,708 entries and
is still increasing. Therefore, a robust method to ex-
tract NEs including unfamiliar words which do not
occur or occur few times in a training corpus is nec-
essary.
This paper proposes a novel method of extract-
ing NEs which contain unfamiliar morphemes us-
ing a large unannotated corpus, in order to resolve
the above problem. The proposed method consists
Table 1: Statistics of NE Types of IREX Corpus
NE Type Frequency (%)
ARTIFACT 747 (4.0)
DATE 3567 (19.1)
LOCATION 5463 (29.2)
MONEY 390 (2.1)
ORGANIZATION 3676 (19.7)
PERCENT 492 (2.6)
PERSON 3840 (20.6)
TIME 502 (2.7)
Total 18677
of two steps. The first step is to assign the most
similar and familiar morpheme to each unfamiliar
morpheme based on their context vectors calculated
from a large unannotated corpus. The second step is
to employ traditional machine learning approaches
using both features of original morphemes and fea-
tures of similar morphemes. The experiments of
extracting Japanese NEs from IREX corpus and
NHK corpus show the effectiveness of the proposed
method.
2 Extraction of Japanese Named Entity
2.1 Task of the IREX Workshop
The task of NE extraction of the IREX workshop
(Sekine and Eriguchi, 2000) is to recognize eight
NE types in Table 1. The organizer of the IREX
workshop provided a training corpus, which consists
of 1,174 newspaper articles published from January
1st 1995 to 10th which include 18,677 NEs. In the
Japanese language, no other corpus whose NEs are
annotated is publicly available as far as we know.1
2.2 Chunking of Named Entities
It is quite common that the task of extracting
Japanese NEs from a sentence is formalized as
a chunking problem against a sequence of mor-
1The organizer of the IREX workshop also provides the test-
ing data to its participants, however, we cannot see it because
we did not join it.
125
phemes. For representing proper chunks, we em-
ploy IOB2 representation, one of those which have
been studied well in various chunking tasks of
NLP (Tjong Kim Sang, 1999). This representation
uses the following three labels.
B Current token is the beginning of a chunk.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
Actually, we prepare the 16 derived labels from the
label B and the label I for eight NE types, in order
to distinguish them.
When the task of extracting Japanese NEs from
a sentence is formalized as a chunking problem of a
sequence of morphemes, the segmentation boundary
problem arises as widely known. For example, the
NE definition of IREX tells that a Chinese character
? (bei)? must be extracted as an NE means Amer-
ica from a morpheme ? (hou-bei)? which means
visiting America. A naive chunker using a mor-
pheme as a chunking unit cannot extract such kind of
NEs. In order to cope this problem, (Uchimoto et al,
2000) proposed employing translation rules to mod-
ify problematic morphemes, and (Asahara and Mat-
sumoto, 2003; Nakano and Hirai, 2004) formalized
the task of extracting NEs as a chunking problem
of a sequence of characters instead of a sequence of
morphemes. In this paper, we keep the naive formal-
ization, because it is still enough to compare perfor-
mances of proposed methods and baseline methods.
3 Robust Extraction of Named Entities
Including Unfamiliar Words
The proposed method of extracting NEs consists
of two steps. Its first step is to assign the most
similar and familiar morpheme to each unfamiliar
morpheme based on their context vectors calculated
from a large unannotated corpus. The second step is
to employ traditional machine learning approaches
using both features of original morphemes and fea-
tures of similar morphemes. The following sub-
sections describe these steps respectively.
3.1 Assignment of Similar Morpheme
A context vector Vm of a morpheme m is a vector
consisting of frequencies of all possible unigrams
and bigrams,
Vm =
?
?
?
?
?
f(m,m0), ? ? ? f(m,mN ),
f(m,m0,m0), ? ? ? f(m,mN ,mN ),
f(m0,m), ? ? ? f(mN ,m),
f(m0,m0,m), ? ? ? f(mN ,mN ,m)
?
?
?
?
?
,
where M ? {m0,m1, . . . ,mN} is a set of all mor-
phemes of the unannotated corpus, f(mi,mj) is a
frequency that a sequence of a morpheme mi and
a morpheme mj occurs in the unannotated corpus,
and f(mi,mj ,mk) is a frequency that a sequence
of morphemes mi,mj and mk occurs in the unan-
notated corpus.
Suppose an unfamiliar morpheme mu ? M?MF ,
where MF is a set of familiar morphemes that occur
frequently in the annotated corpus. The most sim-
ilar morpheme m?u to the morpheme mu measured
with their context vectors is given by the following
equation,
m?u = argmax
m?MF
sim(Vmu , Vm), (1)
where sim(Vi, Vj) is a similarity function between
context vectors. In this paper, the cosine function is
employed as it.
3.2 Features
The feature set Fi at i-th position is defined as a tuple
of the morpheme feature MF (mi) of the i-th mor-
pheme mi, the similar morpheme feature SF (mi),
and the character type feature CF (mi).
Fi = ? MF (mi), SF (mi), CF (mi) ?
The morpheme feature MF (mi) is a pair of the sur-
face string and the part-of-speech of mi. The similar
morpheme feature SF (mi) is defined as
SF (mi) =
{
MF (m?i) if mi ? M ? MF
MF (mi) otherwise
,
where m?i is the most similar and familiar morpheme
to mi given by Equation (1). The character type fea-
ture CF (mi) is a set of four binary flags to indi-
cate that the surface string of mi contains a Chinese
character, a hiragana character, a katakana charac-
ter, and an English alphabet respectively.
When we identify the chunk label ci for the i-
th morpheme mi, the surrounding five feature sets
Fi?2, Fi?1, Fi, Fi+1, Fi+2 and the preceding two
chunk labels ci?2, ci?1 are refered.
126
Morpheme Feature Similar Morpheme Feature Character
(English POS (English POS Type Chunk Label
translation) translation) Feature
(kyou) (today) Noun?Adverbial (kyou) (today) Noun?Adverbial ?1, 0, 0, 0? O
(no) gen Particle (no) gen Particle ?0, 1, 0, 0? O
(Ishikari) (Ishikari) Noun?Proper (Kantou) (Kantou) Noun?Proper ?1, 0, 0, 0? B-LOCATION
(heiya) (plain) Noun?Generic (heiya) (plain) Noun?Generic ?1, 0, 0, 0? I-LOCATION
(no) gen Particle (no) gen Particle ?0, 1, 0, 0? O
(tenki) (weather) Noun?Generic (tenki) (weather) Noun?Generic ?1, 0, 0, 0? O
(ha) top Particle (ha) top Particle ?0, 1, 0, 0? O
(hare) (fine) Noun?Generic (hare) (fine) Noun?Generic ?1, 1, 0, 0? O
Figure 1: Example of Training Instance for Proposed Method
?? Parsing Direction ??
Feature set Fi?2 Fi?1 Fi Fi+1 Fi+2
Chunk label ci?2 ci?1 ci
Figure 1 shows an example of training instance of
the proposed method for the sentence ? (kyou)
(no) (Ishikari) (heiya) (no)
(tenki) (ha) (hare)? which means ?It is fine at
Ishikari-plain, today?. ? (Kantou)? is assigned
as the most similar and familiar morpheme to ?
(Ishikari)? which is unfamiliar in the training corpus.
4 Experimental Evaluation
4.1 Experimental Setup
IREX Corpus is used as the annotated corpus to train
statistical NE chunkers, and MF is defined experi-
mentally as a set of all morphemes which occur five
or more times in IREX corpus. Mainichi News-
paper Corpus (1993?1995), which contains 3.5M
sentences consisting of 140M words, is used as
the unannotated corpus to calculate context vectors.
MeCab2(Kudo et al, 2004) is used as a preprocess-
ing morphological analyzer through experiments.
In this paper, either Conditional Random
Fields(CRF)3(Lafferty et al, 2001) or Support Vec-
tor Machine(SVM)4(Cristianini and Shawe-Taylor,
2000) is employed to train a statistical NE chunker.
4.2 Experiment of IREX Corpus
Table 2 shows the results of extracting NEs of IREX
corpus, which are measured with F-measure through
5-fold cross validation. The columns of ?Proposed?
show the results with SF , and the ones of ?Base-
line? show the results without SF . The column of
?NExT? shows the result of using NExT(Masui et
2http://mecab.sourceforge.net/
3http://chasen.org/?taku/software/CRF++/
4http://chasen.org/?taku/software/
yamcha/
Table 2: NE Extraction Performance of IREX Corpus
Proposed Baseline NExT
CRF SVM CRF SVM
ARTIFACT 0.487 0.518 0.458 0.457 -
DATE 0.921 0.909 0.916 0.916 0.682
LOCATION 0.866 0.863 0.847 0.846 0.696
MONEY 0.951 0.610 0.937 0.937 0.895
ORGANIZATION 0.774 0.766 0.744 0.742 0.506
PERCENT 0.936 0.863 0.928 0.928 0.821
PERSON 0.825 0.842 0.788 0.787 0.672
TIME 0.901 0.903 0.902 0.901 0.800
Total 0.842 0.834 0.821 0.820 0.732
Table 3: Statistics of NE Types of NHK Corpus
NE Type Frequency (%)
DATE 755 (19%)
LOCATION 1465 (36%)
MONEY 124 (3%)
ORGANIZATION 1056 (26%)
PERCENT 55 (1%)
PERSON 516 (13%)
TIME 101 (2%)
Total 4072
al., 2002), an NE chunker based on hand-crafted
rules, without 5-fold cross validation.
As shown in Table 2, machine learning ap-
proaches with SF outperform ones without SF .
Please note that the result of SVM without SF and
the result of (Yamada et al, 2002) are comparable,
because our using feature set without SF is quite
similar to their feature set. This fact suggests that
SF is effective to achieve better performances than
the previous research. CRF with SF achieves better
performance than SVM with SF , although CRF and
SVM are comparable in the case without SF . NExT
achieves poorer performance than CRF and SVM.
4.3 Experiment of NHK Corpus
Nippon Housou Kyoukai (NHK) corpus is a set of
transcriptions of 30 broadcast news programs which
were broadcasted from June 1st 1996 to 12th. Ta-
ble 3 shows the statistics of NEs of NHK corpus
which were annotated by a graduate student except
127
Table 4: NE Extraction Performance of NHK Corpus
Proposed Baseline NExT
CRF SVM CRF SVM
DATE 0.630 0.595 0.571 0.569 0.523
LOCATION 0.837 0.825 0.797 0.811 0.741
MONEY 0.988 0.660 0.971 0.623 0.996
ORGANIZATION 0.662 0.636 0.601 0.598 0.612
PERCENT 0.538 0.430 0.539 0.435 0.254
PERSON 0.794 0.813 0.752 0.787 0.622
TIME 0.250 0.224 0.200 0.247 0.260
Total 0.746 0.719 0.702 0.697 0.615
Table 5: Extraction of Familiar/Unfamiliar NEs
Familiar Unfamiliar Other
CRF (Proposed) 0.789 0.654 0.621
CRF (Baseline) 0.757 0.556 0.614
for ARTIFACT in accordance with the NE definition
of IREX. Because all articles of IREX corpus had
been published earlier than broadcasting programs
of NHK corpus, we can suppose that NHK corpus
contains unfamiliar NEs like real input texts.
Table 4 shows the results of chunkers trained from
whole IREX corpus against NHK corpus. The meth-
ods with SF outperform the ones without SF . Fur-
thermore, performance improvements between the
ones with SF and the ones without SF are greater
than Table 2.
The performance of CRF with SF and one of
CRF without SF are compared in Table 5. The col-
umn ?Familiar? shows the results of extracting NEs
which consist of familiar morphemes, as well as the
column ?Unfamiliar? shows the results of extracting
NEs which consist of unfamiliar morphemes. The
column ?Other? shows the results of extracting NEs
which contain both familiar morpheme and unfa-
miliar one. These results indicate that SF is espe-
cially effective to extract NEs consisting of unfamil-
iar morphemes.
5 Concluding Remarks
This paper proposes a novel method to extract NEs
including unfamiliar morphemes which do not occur
or occur few times in a training corpus using a large
unannotated corpus. The experimental results show
that SF is effective for robust extracting NEs which
consist of unfamiliar morphemes. There are other
effective features of extracting NEs like N -best mor-
pheme sequences described in (Asahara and Mat-
sumoto, 2003) and features of surrounding phrases
described in (Nakano and Hirai, 2004). We will in-
vestigate incorporating SF and these features in the
near future.
References
Masayuki Asahara and Yuji Matsumoto. 2003. Japanese
named entity extraction with redundant morphological
analysis. In Proc. of HLT?NAACL ?03, pages 8?15.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient sup-
port vector classifiers for named entity recognition. In
Proc. of the 19th COLING, pages 1?7.
Hideki Isozaki. 2001. Japanese named entity recogni-
tion based on a simple rule generator and decision tree
learning. In Proc. of ACL ?01, pages 314?321.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to japanese
morphological analysis. In Proc. of EMNLP2004,
pages 230?237.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of ICML, pages 282?289.
Fumito Masui, Shinya Suzuki, and Junichi Fukumoto.
2002. Development of named entity extraction tool
NExT for text processing. In Proceedings of the 8th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 176?179. (in Japanese).
Keigo Nakano and Yuzo Hirai. 2004. Japanese named
entity extraction with bunsetsu features. Transac-
tions of Information Processing Society of Japan,
45(3):934?941, Mar. (in Japanese).
Nichigai Associates, editor. 2007. DCS Kikan-mei Jisho.
Nichigai Associates. (in Japanese).
Manabu Sassano and Takehito Utsuro. 2000. Named
entity chunking techniques in supervised learning for
japanese named entity recognition. In Proc. of the 18th
COLING, pages 705?711.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation: analysis of results.
In Proc. of the 18th COLING, pages 1106?1110.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. of the 9th EACL, pages 173?179.
Kiyotaka Uchimoto, Ma Qing, Masaki Murata, Hiromi
Ozaku, Masao Utiyama, and Hitoshi Isahara. 2000.
Named entity extraction based on a maximum entropy
model and transformation rules. Journal of Natural
Language Processing, 7(2):63?90, Apr. (in Japanese).
Hiroyasu Yamada, Taku Kudo, and Yuji Matsumoto.
2002. Japanese named entity extraction using support
vector machine. Transactions of Information Process-
ing Society of Japan, 43(1):44?53, Jan. (in Japanese).
128
Interpreter for Highly Portable Spoken Dialogue System
Masamitsu Umeda
Department of Information and
Computer Science,
Toyohashi University of Technology
Aichi, JAPAN, 441-8580
umeda@slp.ics.tut.ac.jp
Satoru Kogure
Faculty of Education,
Aichi University of Education
Aichi, JAPAN, 448-8542
skogure@auecc.aichi-edu.ac.jp
Seiichi Nakagawa
Department of Information and
Computer Science,
Toyohashi University of Technology
Aichi, JAPAN, 441-8580
nakagawa@slp.ics.tut.ac.jp
Abstract
Recently the technology for speech recog-
nition and language processing for spoken
dialogue systems has been improved, and
speech recognition systems and dialogue
systems have been developed to the ex-
tent of practical usage. In order to become
more practical, not only those fundamen-
tal techniques but also the techniques of
portability and expansibility should be de-
veloped. In our previous research, we
demonstrated the portability of the speech
recognition module to a developed portal
spoken dialogue system. And we con-
structed a dialogue strategy design tool
of dialogue script for controlling the dia-
logue strategy.
In this paper, we report a highly portable
interpreter using a commercial electronic
dictionary. We apply this to three do-
mains/tasks and confirm the validity of the
interpreter for each domain/task.
Keywords: spoken dialogue system, robust inter-
preter, portability, dialogue script
1 Introduction
Recently, much research has been done on the ro-
bustness and reliability of spoken dialogue sys-
tems. We developed a ?Mt.Fuji sightseeing guid-
ance? system which uses touch screen input, speech
input/output and graphical output, and have im-
proved the sub-modules of a speech recognizer, nat-
ural language interpreter, response generator and
multi-modal interface (Kai and Nakagawa, 1995;
Itoh et al, 1995; Denda et al, 1996; Kogure et al,
1999; Nakagawa et al, 2000). General speaking, all
of these modules except for the speech recognition
module depended on a given task or domain.
As speech recognition systems are increasingly
being used in practical applications, spoken dialogue
systems will also become more widespread. How-
ever, the cost of developing a new spoken dialogue
system is enormous. The systems that have been de-
veloped so far can not be transferred to other do-
mains easily, and a highly-portable system that can
be easily adapted to another domain or task urgently
should be developed. There are several examples of
researches that focused on high portability and ex-
pansibility (Kaspar and Hoffmannn, 1998; Brond-
sted et al, 1998; Sutton et al, 1998; Sasajima et al,
1999; Abella and Gorin, 1999; Levin et al, 2000).
In (Kaspar and Hoffmannn, 1998), a prototype
could be simply constructed even in a complicated
speech dialogue system using the PIA system, which
was implemented using Visual Basic. This sys-
tem placed priority on achieving high robustness of
speech recognition and high naturalness of gener-
ated dialogue. However, the system limited the task
to the domain of knowledge search. E. Levin et
al. reported the design and implementation of the
AT&T Communicator mixed-initiative spoken dia-
logue system (Levin et al, 2000). The communica-
tor project sponsored by DARPA launched in 1999.
On the other hand, we have also considered the
portability of spoken dialogue system (Kogure and
Nakagawa, 2000). We showed from experience that
Figure 1: System overview.
it was difficult to transfer from the Mt. Fuji sight-
seeing guidance existing system to the East Mikawa
sightseeing guidance.
Therefore?we built portable spoken dialogue sys-
tem developing tools based on GUI (Kogure and
Nakagawa, 2000). Especially, we focused on the
developing tools of spoken dialogue system for
database retrieval.
Some spoken language systems focused on ro-
bust matching to handle ungrammatical utterances
and illegal sentences. The Template Matcher (TM)
at the Stanford Research Institute (Moore and Pod-
lozny, 1991) instantiates competing templates, each
of which seeks to fill its slots with appropriate words
and phrases from the utterance. The template with
the highest score yields the semantic representation.
R.Kuhn and R.De Mori suggest a method where the
system?s rules for semantic interpretation are learnt
automatically from training data (Kuhn and Mori,
1995). The rules are encoded in forest of special-
ized decision trees called as Semantic Classifica-
tion Trees (SCTs). The learned rules are robust to
ungrammatical sentences and misrecognition in the
input. Minker compared a rule-based case frame
grammar analysis and a stochastics based case frame
grammar analysis for understanding(Minker, 1998).
Globally speaking, the performances of the stochas-
tic and rule-based parsers were comparable for ATIS
(Air Travel Information Service). Y. Wang sug-
gests a robust chart parser which is the major Spo-
ken Language Understanding (SLU) engine compo-
nent behind MiPad(Wang, 2001). The robustness to
ungrammaticality and noise can be attributed to its
ability of skipping minimum unparsable segments in
the input. The robust parsing algorithm is an exten-
sion of the bottom-up chart-parsing algorithm.
In (Nakagawa et al, 2000), the dialogue system
understands spontaneous speech which has many
ambiguous phenomena such as interjections, el-
lipses, inversions, repairs, unknown words and so
on, and responds to the user?s utterance. But the sys-
tem fails to analyze some utterances. ?Incomplete-
ness? with the interpreter mainly causes the analy-
sis failure, and leads to domain/task dependent de-
velopment. Therefore, in this paper we focused on
improving the interpreter in order to make it a ro-
bust/portable one.
For this purpose, we used the EDR electronic dic-
tionary as a semantic dictionary and improved the
interpreter so that it had some new functions. Fi-
nally, we applied the system to three domains/tasks;
hotel retrieval, Mt.Fuji sightseeing guidance and lit-
erature retrieval.
2 Highly-portable System for Information
Retrieval from Database
The proposed system in the dialogue processing
parts consists of semantics interpreter, retrieval
module, response generator and dialogue man-
ager which integrates three modules. The system
overview is shown in Figure 1.
Each module has the following roles:
? Speech Recognizer: This module recognizes
a user utterance via speech input and generates
a recognized sentence. We used SPOJUS (Kai
and Nakagawa, 1995) for this.
? Semantic Interpreter: This module under-
stands a user utterance via recognized sentence
and generates semantic representation.
? Retrieval Module: This module that extracts
the word as retrieval key word/phrase from the
semantic representation.
? Response Generator: This module is acti-
vated by a dialogue manager, selects a kind
of response strategies and generates a response
sentence.
? Text-to-speech Module: This module gener-
ates a response sentence, synthesizes speech
signals and playbacks this audio files to user.
In this paper, we focused on three modules: Se-
mantic Interpreter, Retrieval Module and Re-
sponse Generator. We believe that the Speech
Recognizer except for the language models and the
Text-to-speech Module are domain and task inde-
pendent. A domain/task adaptation technique of the
language models for the speech recognizer was pre-
liminarily evaluated (Kogure and Nakagawa, 2000).
3 Improving the Semantic Interpreter
3.1 Adding New Functionality
Our purpose is to improve our previous inter-
preter (Kogure and Nakagawa, 2000) to make it
more robust and portable. In this study, we imple-
mented new functions into the interpreter as follows:
? Processing of Correction Utterance: The sys-
tem omits the word sequence as restatement
Figure 2: An example of processing of correction
utterance.
Figure 3: An example of removing recognition error
caused by logging error.
and word preceding this sequence. The omitted
word sequence is the word or word sequence
which the user uses when he/she tells the sys-
tem to repair the wrong word or word sequence.
Figure 2 shows this process.
? Removing Recognition Error by Logging
Error: The system ignores non-semantic
words at the head or the tail of the sentence
caused by detection errors of beginning/ending
speech and adopts the grammaticaly longest
candidate from plural candidates as the valid
Figure 4: An example of improving keyword re-
trieval.
Figure 5: An example of concept classification.
sentence. Speech segmentation errors in back-
ground noise environments yield non-semantic
words. Figure 3 shows this process.
? Improving Keyword Retrieval: If an input
sentence is inadequate for parsing, the system
creates a retrieval condition from the semantic
labels which are attached to the word used as
the keyword in a sentence. Figure 4 shows this
process.
3.2 EDR Electronic Dictionary for Semantic
Interpreter
In the study, we clearly separated domain/task in-
dependent parts and dependent parts. When an ap-
plication developer wants to change a domain or
task, he/she modifies or prepares only domain/task
dependent parts, such as semantic features, proper
nouns, and so on. In this section, we describe the do-
main/task dependent information obtained from the
EDR electronic dictionary, which is used to analyze
the utterance in the interpreter.
The EDR electronic dictionary1 has widely been
used for research and application development of
1http://www.jsa.co.jp/EDR/
? ?
search[3cec34] :::
agent 30f6b0;30f746 =GA,
object 3f97b1 =WO
? ?
Figure 6: An example of case frame.
Japanese language processing. We developed a high
portable semantic dictionary for the interpreter using
the EDR electronic dictionary.
3.2.1 Concept Dictionary
The EDR electronic dictionary is constructed by a
hierarchical tree structure with classification records
which are composed of 410,000 concepts.
Figure 5 shows a part of concept classification
from ?location.? ?30f751? denotes the concept clas-
sification for ?location.?
We assign the concept for domain/task dependent
words to the existing hierarchical structure. In Fig-
ure 5, ?G-PLACE? is a keyword which is used for
referencing to a place. Since the morphological
analysis assigns the concept to the word, the inter-
preter can use the concept information.
3.2.2 Dictionary of Selectional Restriction for
Japanese Verbs
The interpreter analyzes an input sentence us-
ing the dictionary of selectional restrictions or case
frames for Japanese verbs, which are attached as a
co-occurrence dictionary for the EDR electronic dic-
tionary. This dictionary contains information related
to the surface case and the deep case about main
verbs. The interpreter converts the input sentence
to the case frame format information using this dic-
tionary, which is formalized from the information on
a deep case and the conceptual classification infor-
mation for every verb using the technique of a selec-
tional restriction.
Figure 6 shows the case frame of the verb
?search?. It shows that ?3cec34? is the concept
classification of ?search.? When the deep case is
?agent? and the surface case is ?GA?, ?30f6b0? or
?30f746? is selected, and when the deep case is ?ob-
ject? and the surface case is ?WO?, ?3f97b1? is se-
lected. Where ?GA? and ?WO? are particle peculiars
(postpositions) in Japanese.
Figure 7: An example of semantic interpreter.
3.3 An Example
Figure 7 shows an example of the interpreter flow.
In Figure 7 , the user inputs a Japanese utterance
that means ?What hotels are there along Yamanote
Line?? in English. The interpreter executes the pro-
cesses of a morphological analysis and parsing, and
generates a parsing result that is represented by a
tree-structure in accordance to the prepared gram-
mar. This sentence?s verb is ?ARU?(There are, in
English), and the concept classification of ?ARU? is
?0e5a74?. The interpreter generates a semantic anal-
ysis result from the parsing result using this concept
classification information, that is, the case frame of
?ARU?. The system transfers the semantic analysis
result into a retrieval condition. This condition is
used to generate SQL language for the retrieval.
4 Domain/Task Independency and
Dependency
4.1 Domain/Task Independency/Dependency
In this section, we describe how to realize the high
portability which has the benefit of efficiency when
application developers design a new dialogue sys-
tem. That is, while the system performance is kept at
a certain level, we will shorten the period of develop-
ing the dialogue system as possible. Therefore, we
define the domain/task independency/dependency.
Domain independency means that application devel-
opers can use common data for all domains 2. Do-
main dependency means that applications are not do-
main independent. Task independency means that
application developers can use these common data
for all tasks 3 in a certain domain. Task dependency
means that is not task independent. Hereafter, four
data types are abbreviated as DI, DD, TI, TD, respec-
tively.
We clearly separated semantics, retrieval and re-
sponse modules into DI/DI, DD/TI, TI/TD, and TI/TI
parts, respectively. The system core was built whilst
keeping it completely domain and task independent
(DI/TI). We can divide data sets three types4 as
shown in Table 1; DI/TI, DD/TI and DD/TD. DI/TI
data sets served as information for a morphological
analysis except for nouns or verbs with respect to
the domain/task and so on. DD/TI data sets are used
as information for morphological analysis of nouns
or verbs with respect to the domain/task, informa-
tion for semantics analysis and so on. DD/TD data
sets are used as information for format that the sys-
tem responds the retrieval results and so on. In the
2Domain is a field or area of dialogue object.
3Task is the problem or process that the user wants to realize
in a particular domain.
4Since we define a domain and a task as the above, all do-
main independent data sets are surely task independent data
sets, therefore DI/TD data sets are none.
Table 1: Separation of domain/task dependent data sets and domain/task independent data sets.
types Data
DI/TI
syllable HMMs, morphological dictionary except
for noun and verb, syntactic grammar, noun and verb
semantic dictionary for dialogue processing ,seman-
tic dictionary, case frame
DD/TI morphological dictionary for noun and verb field in-formation of database, database
DD/TD convert rule from semantic representation to re-
trieval pattern, display format of retrieval result
DI: domain independent DD: domain dependent
TI: task independent TD: task dependent
Figure 8: Task adaptation.
next section, we describe how application develop-
ers prepare DD/TI and DD/TD data sets.
4.2 System Core
We used Chasen 5 as Japanese morphological analy-
sis and PostgreSQL 6 as a database retrieval man-
agement system. We have constructed Semantics
Modules based on the Mt. Fuji sightseeing guidance
system (Nakagawa et al, 2000) with separating task
dependent and independent parts. All parts of the
system core are clearly DI/TI.
5http://chasen.aist-nara.ac.jp/
6http://www.pgsql.com/
4.3 Data Sets
Data sets consist of DI/TI, DD/TI and DD/TD data
sets. The separated results are shown in Table 1.
5 Task Adaptation -Hotel Retrieval
System-
Figure 8 shows the flow chart of the task adaptation.
We consider what kind of data may be prepared as
task-dependent knowledge when a new task is ap-
plied. In the proposed framework, the application
developer prepares the following:
? A generally usable database (machine read-
able)
? The format information of each field of the
database
A generally usable database
------------------------------------------------------------
<name>GrandCentralHotel</name>
<address>2-2 Kandatsukasa-machi Chiyoda Tokyo</address>
<access>On foot-from JR line and subway line Kanda station 3 minutes</access>
<land>Tokyo Tower,TOKYO DOME</land>
<single>8900,7120</single>
:
------------------------------------------------------------
The format information of each field of the database
Database format information
------------------------------------------------------------
S|<name>|</name>|name
S|<address>|</address>|address
R|<access>|:|,|</access>|access
R|<land>|:|,|</land>|land
R|<single>|:|,|</single>|single
:
------------------------------------------------------------
Transfer format information
------------------------------------------------------------
DATABASE:::
hotel:$hotel_id{int},$name{varchar(200)},$address{varchar(200)},
$checkin{varchar(10)},$checkout{varchar(10)},$scale{varchar(20)},
$room{varchar(20)},$smoking{varchar(20)}
acc:$acc_id{int},$hotel_id{int},$access[]{varchar(200)}
land:$land_id{int},$hotel_id{int},$land[]{varchar(200)}
single:$single_id{int},$hotel_id{int},$single[]{varchar(200)}
double:$double_id{int},$hotel_id{int},$double[]{varchar(200)}
twin:$twin_id{int},$hotel_id{int},$twin[]{varchar(200)}
inside_hotel:$inside_hotel_id{int},$hotel_id{int},$inside_hotel[]{varchar(200)}
room:$room_id{int},$hotel_id{int},$room[]{varchar(200)}
service:$service_id{int},$hotel_id{int},$service[]{varchar(200)}
card:$card_id{int},$hotel_id{int},$card[]{varchar(200)}
DATABASE:::
Figure 9: An example of hotel information and the definition given by a developer.
? A corpus of user utterances (dialogue exam-
ples)
The database information retrieval system first re-
quires a database (generally, one that is open to the
public) as a retrieval target. The format information
of each field in the database should be given in order
to access the database. In addition, since the dictio-
nary and language model are adapted to the database
information retrieval system in the task, a corpus of
the user utterances is required.
For examples, Figure 9 shows a generally usable
database and the format information of each field
of the database in a hotel retrieval system.
We used the hotel data of Hornet7 of the hotel ref-
erence site in Japan. The web page of this site can
be automatically translated into a database. The data
obtained from the HTML source is translated into a
data format like the generally usable database in
Figure 9.
This is translated according to the rules of Fig-
ure 9. In Figure 9, hotel information consists of
7http://www.inn-info.co.jp/
the name of the hotel, address of the hotel, access
method to the hotel, and so on. Transfer format in-
formation describes how each item of hotel informa-
tion is processed.
In Figure 9, the landmarks, ?Tokyo Tower? and
?Tokyo Dome? are substituted into an array variable
?landmark? which represents a landmark.
From the description of this file, the construction
of a database and creation of the table for every cat-
egory can be performed automatically. And word
registration to an analysis dictionary can be auto-
matically performed using a morphological-analysis
dictionary tool and a semantic dictionary registra-
tion tool. In this way, the hotel retrieval system was
constructed in about 10 hours.
The sample of a dialogue is shown in Figure 10.
6 Other Task Adaptation
6.1 Application to Mt. Fuji Sightseeing
Guidance System
We applied the portable system to Mt. Fuji sight-
seeing guidance system as well as the hotel retrieval
===== Speech Input =====
Input :What hotels are there around Kanda Station?
<=== input utterance
:What hotels are there near in Kanda Station?
<=== recognized sentence
===== System Output =====
5 facilities were found.
===== Retrieval Result =====
No.1 Olympic in Kanda, single room:11000yen
No.2 Grand Central Hotel,single room:8900yen,double room:12200yen, twin room:142000yen
No.3 Sun Hotel Kanda,single room:8400yen,twin room:13600yen
No.4 Central Hotel,single room:6500yen,double room:9800yen,twin room:9300yen
No.5 New Central Hotel,single room:7000yen,double room:7500yen,twin room:9600yen
===== System Output =====
The retrieval result is the above.
===== Speech Input =====
Input :Please display the data of No.2.
<=== input utterance
:Please display the data of No.2.
<=== recognized sentence
===== System Output =====
The details of the data of No.2 are displayed.
===== Retrieval Result =====
*** Grand Central Hotel ***
2-2 Kandatsukasa-machi Chiyoda Tokyo
On foot-from JR line and subway line Kanda station 3 minutes
Landmark:
Room charge :: single:8900yen double:12200yen twin:14200yen
check in time is 15:00,check out time is 11:00
11 stories,157 rooms
No smoking room is Nothing
Inside-a-hotel equipment:conference room,vending
machine,laundromat,restaurant,banquet hall
indoor equipment:bath,toilet,air conditioning,lighting
desk,refrigerator,Japanese tea,drier,amenity
set,television,video broadcast,satellite broadcasting,telephone
service:Room service,copy,facsimile,word processor loan,iron
loan,trouser press loan,parcel delivery service
receptionist,cleaning receptionist,massage,mail servic
Usable credit card:DC,VISA,JCB,Master,UC,AMEX,Million,Dainers,NICOS
Figure 10: Dialogue example (hotel retrieval system).
===== Speech Input =====
Input :What is there in Kawaguchi Lake? <=== input utterance-1
===== System Output =====
29 facilities were found.
The reference conditions which can be used by addition are shown below.
action : reading(9) lodging(7) appreciation(2) resting(2) camping(2)
kind : hotel(4) museum(3) concert hall(2) pension(2) campsite(2)
===== Speech Input =====
Input :Campsite. <=== input utterance-2
===== System Output =====
(Facility-kind campsite) <=== retrieval condition
2 facilities were found.
===== Retrieval Result =====
Kitagishi no Myoukosan(campsite), Kawaguchi Lake, 0yen
Tozawa center(campsite), Kawaguchi Lake, 4000yen
===== Speech Input =====
The retrieval result is the above.
Figure 11: Dialogue example (Mt. Fuji sightseeing guidance system).
===== System Output =====
This is a document retrieval system. Please input retrieval conditions.
===== Speech Input =====
input: Is there a paper on the multi modal? <=== input utterance
Yet uh uh.. is there a paper on the multi modal such?
<=== recognized sentence
===== System Output =====
23 papers were found.
Please input additional retrieval conditions.
===== Speech Input =====
input: This is related to Internet. <=== input utterance
This is related to Internet. <=== recognized sentence
===== System Output =====
3 paper were found.
===== Retrieval Result =====
No.1 A. Nakashima, et al:"Intelligent network for personal move
communication",Institute of Electronics, Information and
Communication Engineers Journal of Japan, 1995)
No.2 K. Ono, et al:"Development of new generation communication
network",Institute of Electronics, Information and Communication
Engineers Journal of Japan, 1995)
No.3 H. Aiso, et al:"Future prospects of information highway",
Institute of Electronics, Information and Communication Engineers
Journal of Japan, 1995)
===== System Output =====
The retrieval result is the above.
Figure 12: Dialogue example (literature retrieval system).
system. An example of using the system is shown in
Figure 11.
The dialogue manager uses the dialogue script in
order to decide a dialogue strategy. This scripts have
the functions that if retrieval results are too numer-
ous, the system does not display all retrieval results
but inquires the user for additional retrieval condi-
tions. Of course, application developers can easily
change this dialogue strategy with the GUI tools.
For example, since input utterance-2 in
Figure 11 is not a sentence, the interpreter may fail
at the step of the semantic analysis. The interpreter
adapts Improved Keyword Retrieval described in
Section 3. The concept classification of campsite
is ?G-KIND?, so the interpreter directly changes
this word (see retrieval condition) to ?G-
KIND.?
6.2 Application to Literature Retrieval System
We also applied the system to a literature retrieval
system as well as the hotel retrieval system.
Figure 12 shows an example of using the system.
In this example the recognized sentence has
misrecognition and interjections. So the CFG gram-
mar cannot accept this sentence. Therefore, Re-
moving Recognition Error by Lodging Error
in Section 3 is used for this sentence. So yet,
uh, uh and such are omitted in the recognized
utterance.
If there are some related papers catalogues as
retrieved results and the user inputs an utterance
?Please display detailed information of the second
paper.?, the system displays detailed information
like the abstract of the second paper in the paper cat-
alogue.
6.3 Evaluation of Portability
We evaluated the portability of our dialogue sys-
tem and robustness of the interpreter through the
Mt.Fuji sightseeing system and literature retrieval
system, and we confirmed that the developing period
was shortened to the time of the hotel?s one, which
means a reduction to 10 hours from 15 hours in the
previous system (Kogure and Nakagawa, 2000).
7 Summary
We improved the portable semantic interpreter for
spoken dialogue systems. The highly portable se-
mantic interpreter was constructed using the EDR
electronic dictionary. By the example of the hotel
retrieval system, we explained the structure of a high
portable system.
We also constructed the Mt.Fuji sightseeing sys-
tem and literature retrieval system, and we con-
firmed that the developing effort/period was reduced
to the same as the hotel?s one, which means a re-
duction to 10 hours from 15 hours in the previous
system.
In the near future, we will construct a system that
can change a task during a dialogue.
References
A. Abella and A. L. Gorin. 1999. Construct algebra:
Analytical dialog management. In Proc. of ACL ?99,
pages 191?199.
T. Brondsted, B. Bai, and J. Olsen. 1998. The reward
service creation environment. an overview. In Proc. of
ICSLP ?98, pages 1175?1178.
A. Denda, T. Itoh, and S. Nakagawa. 1996. A robust
dialogue system with spontaneous speech and touch
screen. In Proc. of ICMI ?96, pages 144?151.
T. Itoh, M. Hidano, M. Yamamoto, and S. Nakagawa.
1995. Spontaneous speech understanding for a robust
dialogue system. In Proc. of NLPRS ?95, volume 2,
pages 538?543.
A. Kai and S. Nakagawa. 1995. Investigation on un-
known word processing and strategies for spontaneous
speech understanding. In Proc. of EUROSPEECH ?95,
pages 2095?2098.
S. Kaspar and A. Hoffmannn. 1998. Semi-automated
incremental prototyping of spoken dialog systems. In
Proc. of ICSLP ?98, pages 859?862.
S. Kogure and S. Nakagawa. 2000. A portable devel-
opment tool for spoken dialogue systems. In Proc. of
ICSLP ?2000, pages 238?241.
S. Kogure, T. Itoh, and S. Nakagawa. 1999. A seman-
tic interpreter for a robust spoken dialogue system. In
Proc. ICMI ?99, pages II 61?66.
R. Kuhn and R. De Mori. 1995. The application of se-
mantic classification trees to natural language under-
standing. In IEEE Trans. Pattern Analysis and Ma-
chine Intelligence, volume 17, pages 449?460.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, E. Bia-
tov, E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.
2000. The at&t-darpa communicator mixed-initiative
spoken dialog system. In Proc. of ICSLP ?2000, vol-
ume 2, pages 122?125.
W. Minker. 1998. Stochastic versus rule-based speech
understanding for information retrieval. In Speech
Communication, pages 223?147.
R. Moore and A. Podlozny. 1991. A template matcher
for robust nl interpretation. In Proc. Speech and
Natural Language Workshop, Morgan Kaufmann Inc.,
pages 190?194.
S. Nakagawa, S. Kogure, and T. Itoh. 2000. A semantic
interpreter and a cooperative response generator for a
robust spoken dialogue system. IJPRAI, 14(5):553?
569.
M. Sasajima, T. Yano, and Y. Kono. 1999. Europa:
generic framework for developing spoken dialogue
systes. In Proc. of EUROSPEECH ?99, pages 1163?
1166.
S. Sutton, R. Cole, J. de Villiers, J. Schalkwyk, P. Ver-
meulen, M. Macon, Y.Yan, E. Kaiser, B. Rundle,
K. Shobaki, P. Hosom, A. Kain, J. Wouters, D. Mas-
saro, and M. Cohen. 1998. Universal speech tools:the
cslu toolkit. In Proc. of ICSLP ?98, pages 3221?3224.
Y. Wang. 2001. Robust language understanding in mi-
pad. In Proc. of EUROSPEECH ?2001, pages 1555?
1558.
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Indonesian-Japanese CLIR Using Only Limited Resource 
Ayu Purwarianti Masatoshi Tsuchiya Seiichi Nakagawa 
Department of Information and Computer Science, Toyohashi University of Technology 
ayu@slp.ics.tut.ac.jp tsuchiya@imc.tut.ac.jp nakagawa@slp.ics.tut.ac.jp
 
Abstract 
Our research aim here is to build a CLIR 
system that works for a language pair 
with poor resources where the source 
language (e.g. Indonesian) has limited 
language resources. Our Indonesian-
Japanese CLIR system employs the 
existing Japanese IR system, and we 
focus our research on the Indonesian-
Japanese query translation. There are two 
problems in our limited resource query 
translation: the OOV problem and the 
translation ambiguity. The OOV problem 
is handled using target language?s 
resources (English-Japanese dictionary 
and Japanese proper name dictionary). 
The translation ambiguity is handled 
using a Japanese monolingual corpus in 
our translation filtering. We select the 
final translation set using the mutual 
information score and the TF?IDF score. 
The result on NTCIR 3 (NII-NACSIS 
Test Collection for IR Systems) Web 
Retrieval Task shows that the translation 
method achieved a higher IR score than 
the transitive machine translation (using 
Kataku (Indonesian-English) and 
Babelfish/ Excite (English-Japanese) 
engine) result. The best result achieved 
about 49% of the monolingual retrieval. 
1 Introductions 
Due to the various languages used by different 
nations in the world, the CLIR has been an 
interesting research topic. For language pair with 
a rich language resource, the translation in the 
CLIR can be done with a bilingual dictionary - 
based direct translation, machine translation - or 
a parallel corpus - based translation. For a rare 
language pair, there is an attempt to use a pivot 
language (usually English), known as transitive 
translation, because there is no ample bilingual 
dictionary or machine translation system 
available. Some studies have been done in the 
field of transitive translation using bilingual 
dictionaries in the CLIR system such as 
[Ballesteros 2000; Gollins and Sanderson 2001]. 
Ballesteros [2000] translated Spanish queries 
into French with English as the interlingua. 
Ballesteros used Collins Spanish-English and 
English-French dictionaries. Gollins and 
Sanderson [2001] translated German queries into 
English using two pivot languages (Spanish and 
Dutch). Gollins used the Euro Wordnet as a data 
resource. To our knowledge, no CLIR is 
available with transitive translation for a source 
language with poor data resources such as 
Indonesian. 
Translation using a bilingual dictionary 
usually provides many translation alternatives 
only a few of which are appropriate. A transitive 
translation gives more translation alternatives 
than a direct translation. In order to select the 
most appropriate translation, a monolingual 
corpus can be used to select the best translation. 
Ballesteros and Croft [1998] used an English 
corpus to select some English translation based 
on Spanish-English translation and analyzed the 
co-occurrence frequencies to disambiguate 
phrase translations. The occurrence score is 
called the em score. Each set is ranked by em 
score, and the highest ranking set is taken as the 
final translation. Gao et al [2001] used a Chinese 
corpus to select the best English-Chinese 
translation set. It modified the EMMI weighting 
measure to calculate the term coherence score. 
Qu et al [2002] selected the best Spanish-
English and Chinese-English translation using an 
English corpus. The coherence score calculation 
was based on 1) web page count; 2) retrieval 
score; and 3) mutual information score. Mirna 
[2001] translated Indonesian into English and 
used an English monolingual corpus to select the 
best translation, employing a term similarity 
score based on the Dice similarity coefficient. 
Federico and Bertoldi [2002] combined the N-
best translation based on an HMM model of a 
query translation pair and relevant document 
probability of the input word to rank Italian 
documents retrieved by English query. Kishida 
and Kando [2004], used all terms to retrieve a 
document in order to obtain the best term 
combination and chose the most frequent term in 
1
each term translation set that appears in the top 
ranked document.  
In our poor resource language ? Japanese 
CLIR where we select Indonesian as the source 
language with limited resource, we calculate the 
mutual information score for each Japanese 
translation combination, using a Japanese 
monolingual corpus. After that, we select one 
translation combination with the highest TF?IDF 
score obtained from the Japanese IR engine. 
By our experiments on Indonesian-Japanese 
CLIR, we would like to show how easy it is to 
build a CLIR for a restricted language resource. 
By using only an Indonesian (as the source 
language) ? English dictionary we are able to 
retrieve Japanese documents with 41% of the 
performance achieved by the monolingual 
Japanese IR system.  
The rest of the paper is organized as follows: 
Section 2 presents an overview of an Indonesian 
query sentence; Section 3 discusses the method 
used for our Indonesian-Japanese CLIR; Section 
4 describes the comparison methods, and Section 
5 presents our experimental data and the results. 
2 Indonesian Query Sentence 
Indonesian is the official language in Indonesia. 
The language is understood by people in 
Indonesia, Malaysia, and Brunei. The Indonesian 
language family is Malayo-Polynesian 
(Austronesian), which extends across the islands 
of Southeast Asia and the Pacific [Wikipedia]. 
Indonesian is not related to either English or 
Japanese.  
Unlike other languages used in Indonesia 
such as Javanese, Sundanese and Balinese that 
use their own scripts, Indonesian uses the 
familiar Roman script. It uses only 26 letters as 
in the English alphabet. A transliteration module 
is not needed to translate an Indonesian sentence.  
Indonesian language does not have 
declensions or conjugations. The basic sentence 
order is Subject-Verb-Object. Verbs are not 
inflected for person or number. There are no 
tenses. Tense is denoted by the time adverb or 
some other tense indicators. The time adverb can 
be placed at the front or end of the sentence.  
A rather complex characteristic of the 
Indonesian language is that it is an agglutinave 
language. Words in Indonesian, usually verbs, 
can be attached by many prefixes or suffixes. 
Affixes used in the Indonesian language include 
[Kosasih 2003] me(n)-, ber-, di-, ter-, pe(n)-, per-, 
se-, ke-, -el-, -em-, -er-, -kan, -i, -nya, -an, me(n)-
kan, di-kan, memper-i, diper-i, ke-an, pe(n)-an, 
per-an, ber-an, ber-kan, se-nya. Words with 
different affixes might have uniform or different 
translation. Examples of different word 
translation are ?membaca? and ?pembaca?, 
which are translated into ?read? and ?reader?, 
respectively. Examples of same word translation 
are the words ?baca? and ?bacakan?, which are 
both translated into ?read? in English. Other 
examples are the words ?membaca? and ?dibaca?, 
which are translated into ?read? and ?being read?, 
respectively. By using a stop word elimination, 
the translation result of ?membaca? and ?dibaca? 
will give the same English translation, ?read?.  
An Indonesian dictionary usually contains 
words with affixes (that have different 
translations) and base words. For example, ?se-
nya? affix declares a ?most possible? pattern, 
such as ?sebanyak-banyaknya? (as much as 
possible), ?sesedikit-sedikitnya? (less possible), 
?sehitam-sehitamnya? (as black as possible). 
This affix can be attached to many adjectives 
with the same meaning pattern. Therefore, words 
with ?se-nya? affix are usually not included in an 
Indonesian dictionary.  
Query 1 
Saya ingin mengetahui siapa yang telah menjadi peraih 
Academy Awards beberapa generasi secara berturut-turut 
(I want to know who have been the recipients of successive 
generations of Academy Awards) 
Query 2 
Temukan buku-buku yang mengulas tentang novel yang 
ditulis oleh Miyabe Miyuki 
(Find book reviews of novels written by Miyabe Miyuki) 
Figure 1. Indonesian Query Examples 
Indonesian sentences usually consist of 
native (Indonesian) words and borrowed words. 
The two query examples in Figure 1 contain 
borrowed words. The first query contains 
?Academy Awards?, which is borrowed from the 
English language. The second query contains 
?Miyabe Miyuki?, which is transliterated from 
Japanese. To obtain a good translation, the query 
translation in our system must be able to translate 
those words, the Indonesian (native) words and 
the borrowed words. Problems that occur in a 
query translation here include OOV words and 
translation ambiguity. 
3 Indonesian - Japanese Query 
Translation System 
Indonesian-Japanese query translation is a 
component of the Indonesian-Japanese CLIR. 
The query translation system aims to translate an 
2
Indonesian sentence query 
Indonesian sentence query 
Indonesian query sentence(s) into a Japanese 
keyword list. The Japanese keyword list is then 
executed in the Japanese IR system to retrieve 
the relevant document. The schema of the 
Indonesian-Japanese query translation system 
can be seen in Figure 2.  
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Indonesian-Japanese Query 
Translation Schema 
The query translation system consists of 2 
subsystems: the keyword translation and 
translation candidate filtering. The keyword 
translation system seeks to obtain Japanese 
translation candidates for an Indonesian query 
sentence. The translation candidate filtering aims 
to select the most appropriate translation among 
all Japanese translation alternatives. The filtering 
result is used as the input for the Japanese IR 
system. The keyword translation and translation 
filtering process is described in the next section.  
3.1 Indonesian ? Japanese Key Word 
Translation Process  
The keyword translation system is a process used 
to translate Indonesian keywords into Japanese 
keywords. In this research, we do transitive 
translation using bilingual dictionaries as the 
proposed method. Other approaches such as 
direct translation or machine translation are 
employed for the comparison method. The 
schema of our keyword transitive translation 
using bilingual dictionaries is shown in Figure 3.  
The keyword translation process consists of 
native (Indonesian) word translation and 
borrowed word translation. The native words are 
translated using Indonesian-English and English-
Japanese dictionaries. Because the Indonesian 
tag parser is not available, we do the translation 
on a single word and consecutive pair of words 
that exist as a single term in the Indonesian-
English dictionary. As mentioned in the previous 
section dealing with affix combination in 
Indonesian language, not all words with the affix 
combination are recorded in an Indonesian 
dictionary. Therefore, if a search does not reveal 
the exact word, it will search for other words that 
are the basic term of the query word or have the 
same basic term. For example, the Indonesian 
word, ?munculnya? (come out), has a basic term 
?muncul? with the postfix ?-nya?.  Here, the term 
?munculnya? is not available in the dictionary. 
Therefore, the searching will take ?muncul? as 
the matching word with ?munculnya? and give 
the English translation for ?muncul? such as 
?come out? as its translation result. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Indonesian-Japanese Keyword 
Translation Schema 
In Indonesian, a noun phrase has the reverse 
word position of that in English. For example, 
?ozone hole? is translated as ?lubang ozon? 
(ozone=ozon, hole=lubang) in Indonesian. 
Therefore, in English translation, besides word-
by-word translation, we also search for the 
reversed English word pair as a single term in an 
English-Japanese dictionary. This strategy 
reduces the number of translation alternatives.  
The borrowed words are translated using an 
English-Japanese dictionary. The English-
Japanese dictionary is used because most of the 
borrowed words in our query translation system 
come from English. Examples of borrowed 
words in our query are ?Academy Awards?, 
?Aurora?, ?Tang?, ?baseball?, ?Plum?, ?taping?, 
and ?Kubrick?.  
Even though using an English-Japanese 
dictionary may help with accurate translation of 
words, but there are some proper names which 
can not be translated by this dictionary, such as 
?Miyabe Miyuki?, ?Miyazaki Hayao?, ?Honjo 
Manami?, etc. These proper names come from 
Japanese words which are romanized. In the 
Japanese language, these proper names might be 
written in one of the following scripts: kanji 
(Chinese character), hiragana, katakana and 
romaji (roman alphabet). One alphabet word can 
Indonesian ? Japanese 
Keyword Translation 
Candidates for Japanese Translation 
Translation Candidate Filtering 
Japanese Translation 
Indonesian ? English 
Bilingual Dictionary 
Japanese Keyword List 
English ? Japanese 
Bilingual Dictionary 
Translation
Candidates for Japanese Translation 
Japanese Morphological Analyzer (Chasen) 
Japanese Stop Word Elimination 
Indonesian words borrowed words 
? English ? Japanese Bilingual 
Dictionary Translation  
? Japanese Proper Name 
Dictionary Translation 
? Hiragana/Katakana 
Transliteration 
3
be transliterated into more than one Japanese 
words. For example, ?Miyabe? can be 
transliterated into ??, ??, ??? or ???. 
?? and ?? are written in kanji, ??? is 
written in hiragana, and ???  is written in 
katakana. For hiragana and katakana script, the 
borrowed word is translated by using a pair list 
between hiragana or katakana and its roman 
alphabet. These systems have a one-to-one 
correspondence for pronunciation (syllables or 
phonemes), something that can not be done for 
kanji. Therefore, to find the Japanese word in 
kanji corresponding to borrowed words, we use a 
Japanese proper name dictionary. Each term in 
the original proper name dictionary usually 
consists of two words, the first and last names. 
For a wider selection of translation candidates, 
we separate each term with two words into two 
terms. Even though the input word can not be 
found in the original proper name dictionary 
(family name and first name), a match may still 
be possible with the new proper name dictionary. 
Each of the above translation processes also 
involves the stop word elimination process, 
which aims to delete stop words or words that do 
not have significant meaning in the documents 
retrieved. The stop word elimination is done at 
every language step. First, Indonesian stop word 
elimination is applied to a Indonesian query 
sentence to obtain Indonesian keywords. Second, 
English stop word elimination is applied before 
English keywords are translated into Japanese 
keywords. Finally, Japanese stop word 
elimination is done after the Japanese keywords 
are morphologically analyzed by Chasen 
(http://chasen.naist.jp/hiki/ChaSen). 
The keyword transitive translation is used in 
2 systems: 1) transitive translation to translate all 
words in the query, and 2) transitive translation 
to translate OOV (Indonesian) words from direct 
translation using an Indonesian-Japanese 
dictionary. We label the first method as the 
transitive translation using bilingual dictionary 
and the second method as the combined 
translation (direct-transitive).  
3.2 Candidate Filtering Process 
The keyword transitive translation results in 
many more translation candidates than the direct 
translation result. The candidates have a 
translation ambiguity problem which will be 
handled by our Japanese translation candidate 
filtering process, which seeks to select the most 
appropriate translation among the Japanese 
translation candidates. In order to select the best 
Japanese translation, rather than choosing only 
the highest TF?IDF score or only the highest 
mutual information score among all sets, we 
combine both scores. The procedure is as 
follows: 
1. Calculate the mutual information score for 
all term sets. To avoid calculation of all term 
sets, we calculate the mutual information 
score iteratively. First we calculate it for 2 
translation candidate sets. Then we select 
100 sets with the highest mutual information 
score. These sets are joined with the 3rd 
translation candidate sets and the mutual 
information score is recalculated. This step is 
repeated until all translation candidate sets 
are covered.  
For a word set, the mutual information score 
is shown in Equation 1.  
I(t1?tn) =???
= +=
1
1 1
n
i
n
ij
I(ti;tj) 
=???
= +=
1
1 1 )(tlog).(tlog
)t,(tlogn
i
n
ij ji
ji
PP
P
 
 
 
 
 
(1)
I(t1?tn) means the mutual information for a 
set of words t1, t2,?tn. I(ti,tj) means the 
mutual information between two words (ti,tj). 
Here, for a zero frequency word, it will have 
no impact on the mutual information score of 
a word set.  
2. Select 5 sets with highest mutual information 
score and execute them into the IR engine in 
order to obtain the TF?IDF scores. The TF
?IDF score used here is the relevance score 
between the document and the query 
(Equation (2) from Fujii and Ishikawa 
[2003]). 
?
t  
??
??
?
?
??
??
?
?
+ tt,ii
t,i
DF
N.log
TF
avglen
DL
TF
 
 
 
(2)
 
TFt,i denotes the frequency of term t 
appearing in document i. DFt denotes the 
number of documents containing term t. N 
indicates the total number of documents in 
the collection. DLi denotes the length of 
document i (i.e., the number of characters 
contained in i), and avglen the average 
length of documents in the collection. 
3. Select the term set with the highest mutual 
information score among 3 top TF? IDF 
scores 
Figure 4 shows an example of the keyword 
selection process after completion of the 
4
keyword translation process. The translation 
combination and set rankings are for all words (4 
translation sets) in the query. Actually, the 
translation combinations and sets for the query 
example are also ranked for 2 and 3 translation 
sets. All resulting sets (ranked by its mutual 
information score) are executed in the IR system 
in order to obtain the TF?IDF score. The final 
query chosen is the one with the highest TF?
IDF score. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Illustration of Translation Filtering 
Method 
4 Compared Methods 
In the experiment, we compare our proposed 
method with other translation methods. Methods 
for comparing Indonesian-Japanese query 
translation include transitive translation using 
MT (machine translation), direct translation 
using existing Indonesian-Japanese dictionary, 
direct translation using a built-in Indonesian-
Japanese dictionary, transitive translation with 
English keyword selection based on mutual 
information taken from English corpus, and 
transitive translation with Japanese keyword 
selection based on mutual information only.  
4.1 Transitive Translation using Machine 
Translation 
The first method compared is a transitive 
translation using MT (machine translation). The 
Indonesian- Japanese transitive translation using 
MT has a schema similar to Indonesian-Japanese 
transitive translation using a bilingual dictionary.  
However, machine transitive translation does not 
use Indonesian-English and English-Japanese 
dictionaries. Indonesian queries are translated 
into English queries using an online Indonesian-
English MT (Kataku engine, 
http://www.toggletext.com). The English 
translation results are then translated into 
Japanese using 2 online MTs (Babelfish engine, 
http://www.altavista.com/babelfish and Excite 
engine, http://www.excite.co.jp/world). 
4.2 Direct Translation using Existing 
Indonesian-Japanese Bilingual 
Dictionary 
The second method compared is a direct 
translation using an Indonesian-Japanese 
dictionary. This direct translation also has a 
schema similar to the transitive translation using 
bilingual dictionary (Figure 2). The difference is 
that in translation of an Indonesian keyword, 
only 1 dictionary is used, rather than using 2 
dictionaries; in this case, an Indonesian-Japanese 
bilingual dictionary with a fewer words than the 
Indonesian-English and English-Japanese 
dictionaries.  
4.3 Direct Translation using Built-in 
Indonesian-Japanese Dictionary 
We also compare the transitive translation results 
with the direct translation using a built-in 
Indonesian-Japanese dictionary. The Indonesian-
Japanese dictionary is built from Indonesian-
English, English-Japanese and Japanese-English 
dictionaries using ?one-time inverse 
consultation? such as in Tanaka and Umemura 
[1998]. The matching process is similar with that 
in query translation. A Japanese translation is 
searched for an English translation (from every 
Indonesian term in Indonesian-English 
dictionary) as a term in the Japanese-English 
dictionary. If no match can be found, the English 
terms will be normalized by eliminating certain 
stop words (?to?, ?a?, ?an?, ?the?, ?to be?, ?kind 
of?). These normalized English terms will be 
checked again in the Japanese-English dictionary. 
For every Japanese translation, a ?one-time 
inverse consultation? is calculated. If the score is 
Query: 
Saya ingin mengetahui metode untuk belajar 
bagaimana menari salsa (= I wanted to know the 
method of studying how to dance the salsa)  
 
Keyword Selection: 
Metode (method), belajar (to learn, to study, to take 
up), menari (dance), salsa 
 
Japanese Keyword: 
Metode: ????,??,??,?? 
Belajar: ???,??,??,??,??,??,??,
??,???,??,???,??,??,?????
Menari: ??,???,?????,???,??,?
?,?? 
Salsa: ??? 
 
Translation Combination: 
(????,???,??,???) 
(??,???,??,???) 
(??,???,??,???), etc 
 
Rank sets based on Mutual Information Score: 
1. (??, ??,   ??,   ???) 
2. (??, ??,   ??,   ???) 
3. (??, ???, ???, ???) 
4. (??, ???, ???, ???) 
5. (??, ???, ??,   ???) 
 
Select query with highest TF?IDF score 
??, ???, ???, ???
5
more than one (for more than one English term), 
then it is accepted as an Indonesian-Japanese pair. 
If not, the WordNet is used to find its synonym 
and recalculate the ?one-time inverse 
consultation? score so as to compensate for the 
poor quality of Indonesian-English dictionary 
(29054 words). 
5 Experiments 
5.1 Experimental Data  
We measure our query translation performance 
by the IR score achieved by a CLIR system 
because CLIR is a real application and includes 
the performance of key word expansion. For this, 
we do not use word translation accuracy, as for 
the CLIR, since a one-to-one translation rate is 
not suitable, given there are so many 
semantically equivalent words.  
Our CLIR experiments are conducted on 
NTCIR-3 Web Retrieval Task data (100 Gb 
Japanese documents), in which the Japanese 
queries and translated English queries were 
prepared. The Indonesian queries (47 queries) 
are manually translated from English queries. 
The 47 queries contain 528 Indonesian words 
(225 are not stop words), 35 English borrowed 
words, and 16 transliterated Japanese words 
(proper nouns). The IR system (Fujii and 
Ishikawa [2003]) is borrowed from Atsushi Fujii 
(Tsukuba University). External resources used in 
the query translation are listed in Table 1.  
Table 1. External Resource List 
Resource Description 
KEBI Indonesian-English 
dictionary, 29,054 words  
Eijirou English-Japanese dictionary, 
556,237 words 
Kmsmini2000 Indonesian-Japanese 
dictionary, 14,823 words 
ToggleText Kataku Indonesian-English machine 
translation 
Excite  English-Japanese machine 
translation 
Babelfish English-Japanese machine 
translation 
[Fox, 1989] and 
[Zu et al, 2004] 
English stop words (are also 
translated into Indonesian 
stop words) 
Chasen Japanese morphological 
analyzer 
Jinmei Jisho Japanese proper name 
dictionary, 61,629 words 
Mainichi Shinbun 
& Online Yomiuri 
Shinbun 
Japanese newspaper corpus 
5.2 Experimental Result 
In the experiments, we compare the IR score of 
each translation method. The IR scores shown in 
this section are in Mean Average Precision 
(MAP) scores. The evaluation metrics is referred 
to [Fujii and Ishikawa 2003b]. Each query group 
has 4 MAP scores: RL (highly relevant 
document as correct answer with hyperlink 
information used), RC (highly relevant document 
as correct answer), PL (partially relevant 
document as correct answer with hyperlink 
information used), and PC (partially relevant 
document as correct answer). The documents 
hyperlinked from retrieved documents are used 
for relevance assessment. 
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
jp iej-mx iej-mb ijn ij iej ij-iej
PC PL RC RL
Figure 5. Baseline Indonesian-Japanese CLIR 
Figure 5 shows the IR scores of queries 
translated using basic translation methods such 
as the bilingual dictionary or machine translation, 
without any enhanced process. The labels used in 
Figure 5 are:  
? jp (monolingual translation), where ?jp? 
denotes Japanese query 
? iej (transitive translation using bilingual 
dictionary), where ?i?, ?e?, ?j? denote 
Indonesian, English and Japanese, respectively,  
? iej-mx (transitive machine translation using 
Kataku and Excite engines), where ?m? 
denotes machine translation,  
? iej-mb (transitive machine translation using 
Kataku and Babelfish engines), 
? ijn (direct translation using the built in 
Indonesian-Japanese dictionary), 
? ij (direct translation using Indonesian-Japanese 
dictionary), 
? ij-iej (combination of direct (ij) and transitive 
(iej) translation using bilingual dictionary). 
The highest CLIR score in the baseline 
translation (without the enhancement process) 
achieves 30% of the performance achieved by 
the monolingual IR (jp).  
IR results in Figure 6 shows that OOV 
translation does improve the retrieval result. 
Here, our proposed methods (iej and ij-iej) 
achieve lower score than the comparison 
methods. 
6
0
0.01
0.02
0.03
0.04
0.05
iej-mx iej-mb ijn ij iej ij-iej
PC PL RC RL
 
Figure 6. Indonesian-Japanese CLIR with OOV 
Translation 
0 0.02 0.04 0.06 0.08
ij-iej-IR-60
ij-iej-IR-50
ij-iej-IR-40
ij-iej-IR-30
ij-iej-IR-20
ij-iej-IR-10
ij-iej-IR-5
ij-iej-I-10
ij-iej-I-5
ij-iej-I-3
ij-iej-I5
ij-iej-I4
ij-iej-I3
ij-iej-I2
ij-iej-I1
iej-IR-30
iej-IR-20
iej-IR-10
iej-IR-5
iej-I-10
iej-I-5
iej-I-3
iej-I5
iej-I4
iej-I3
iej-I2
iej-I1
ij-IR-30
ij-IR-20
ij-IR-10
ij-IR-5
ij-I-10
ij-I-5
ij-I-3
ij-I5
ij-I4
ij-I3
ij-I2
ij-I1
ijn-IR
ijn-I-10
ijn-I-5
ijn-I-3
ijn-I5
ijn-I4
ijn-I3
ijn-I2
ijn-I1
iej-mx-IR
iej-mx-I-
iej-mx-I-5
iej-mx-I-3
iej-mx-I5
iej-mx-I4
iej-mx-I3
iej-mx-I2
iej-mx-I1
iej-mb-IR
iej-mb-I-
iej-mb-I-5
iej-mb-I-3
iej-mb-I5
iej-mb-I4
iej-mb-I3
iej-mb-I2
iej-mb-I1
RL
RC
PL
PC
 
Figure 7. Indonesian-Japanese CLIR with OOV 
Translation and Keyword Filtering 
Figure 7 shows the MAP score on the 
proposed Indonesian-Japanese CLIR. The 
keyword selection description of each query 
label follows: 
? In (n = 1 .. 5): one query candidate based on 
mutual information score; example: I2 means 
the 2nd ranked query by its mutual information 
score. 
? I-n (n = 3,5,10): combination of the n-best 
query candidates based on mutual information 
score; example: iej-3 (disjuncture of the 3-best 
mutual information score candidates).  
? IR: the 1-best query candidate based on 
combination of mutual information score and 
TF? IDF engine score. X in IR-X shows 
number of combinations. For example, IR-5 
means the highest TF? IDF score among 5 
highest mutual information score sets.  
Figure 7 shows that the proposed filtering 
method yields higher IR score on the transitive 
translation. We achieve 41% of the performance 
achieved by the monolingual IR. The proposed 
transitive translation (iej-IR-10) improves the IR 
score of the baseline method of transitive 
translation (iej) from 0.0156 to 0.0512. The t-test 
shows that iej-IR-10 significantly increases the 
baseline method (iej) with a 97% confidence 
level, T(68) = 1.91, p<0.03. t-test also shows that, 
compared to other baseline systems, the 
proposed transitive translation (iej-IR-10) can 
significantly increase the IR score at 85% (T(84) 
= 1.04, p<0.15), 69% (T(86) = 0.49, p<0.31), 
91% (T(83) = 1.35, p<0.09), and 93% (T(70) = 
1.49, p<0.07) confidence level for iej-mb, iej-mx, 
ij and ij-iej, respectively. Another proposed 
method, a combination of direct and transitive 
translation (ij-iej), achieved the best IR score 
among all the translation methods. The proposed 
combination translation method (ijiej-IR-30) 
improves the  IR score of the baseline 
combination translation (ij-iej) from 0.025 to 
0.0629. The t-test showed that the proposed 
combination translation improves IR score of the 
baseline ij-iej with a 98% confidence level, T(69) 
= 2.09, p<0.02. Compared to other baseline 
systems, t-test shows that the proposed 
combination translation method (ijiej-IR-30) 
improves the IR score at 95% (T(83) = 1.66, 
p<0.05), 86% (T(85) = 1.087, p<0.14), 97%, 
(T(82) = 1.91, p<0.03) and 99% (T(67) = 2.38, 
p<0.005) confidence level for iej-mb, iej-mx, ij 
and iej, respectively. 
6 Conclusions 
We present a translation method on CLIR that 
is suitable for language pair with poor resources, 
where the source language has a limited data 
resource. Compared to other translation methods 
7
such as transitive translation using machine 
translation and direct translation using bilingual 
dictionary (the source-target dictionary is a poor 
bilingual dictionary), our transitive translation 
and the combined translation (direct translation 
and transitive translation) achieve higher IR 
scores. The transitive translation achieves a 41% 
performance of the monolingual IR and the 
combined translation achieves a 49% 
performance of the monolingual IR.  
The two important methods in our transitive 
translation are the borrowed word translation and 
the keyword selection method. The borrowed 
word approach can reduce the number of OOV 
from 50 words to 5 words using a pivot-target 
(English-Japanese) bilingual dictionary and 
target (Japanese) proper name dictionary. The 
keyword selection using the combination of 
mutual information score and TF?IDF score has 
improved the baseline transitive translation. The 
other important method, the combination method 
between transitive and direct translation using 
bilingual dictionaries also improves the CLIR 
performance.  
Acknowledgements 
We would like to give our appreciation to Dr. 
Atsushi Fujii (Tsukuba University) to allow us to 
use the IR Engine in our research. This work was 
partially supported by The 21st Century COE 
Program ?Intelligent Human Sensing? 
References 
Adriani, Mirna. 2000. Using statistical term similarity 
for sense disambiguation in cross language 
information retrieval. Information Retrieval: 67-78. 
Agency for The Assessment and Application of 
Technology: KEBI (Kamus Elektronik Bahasa 
Indonesia). http://nlp.aia.bppt.go.id/kebi/. Last 
access: February 2004. 
Babelfish English-Japanese Online Machine 
Translation. http://www.altavista.com/babelfish/. 
Last access:  April 2004. 
Ballesteros, Lisa A. and W. Bruce Croft. 1998. 
Resolving ambiguity for cross-language retrieval. 
ACM Sigir. 
Ballesteros, Lisa A. 2000. Cross Language Retrieval 
via Transitive Translation. Advances in 
Information Retrieval: 203-230. Kluwer Academic 
Publisher. 
Chasen. http://chasen.naist.jp/hiki/ChaSen/. Last 
access: February 2004. 
Chen, Kuang-hua, et,al. 2003. Overview of CLIR Task 
at the Third NTCIR Workshop. Proceedings of the 
Third NTCIR Workshop. 
Excite English-Japanese Online Machine Translation. 
http://www.excite.co.jp/world/. Last access: April 
2004. 
Federico, M. and N. Bertoldi. 2002. Statistical cross 
language information retrieval using n-best query 
translations. Proc. Of 25th International ACM 
Sigir. 
Fox, Christopher. 1989. A stop list for general text. 
ACM Sigir, Vol 24:19-21, Issue 2 Fall 89/Winter 
90. 
Fujii, Atsushi and Tetsuya Ishikawa. 2003. NTCIR-3 
cross-language IR experiments at ULIS. Proc. Of 
the Third NTCIR Workshop. 
Fujii, Atsushi and Katunobu Itou. 2003. Building a 
test collection for speech driven web retrieval. 
Proceedings of the 8th European Conference on 
Speech Communication and Technology. 
Gao, Jianfeng,  et, al. 2001. Improving query 
translation for cross-language information 
retrieval using statistical model. Proc. Sigir.  
Gollins, Tim and Mark Sanderson. 2001. Improving 
cross language information retrieval with 
triangulated translation. Proc. Sigir.  
ToggleText, Kataku Automatic Translation System. 
http://www.toggletext.com/kataku_trial.php. Last 
access: May 2004. 
Information Retrieval Resources for Bahasia 
Indonesia. Informatics Institute, University of 
Amsterdam. http://ilps.science.uva.nl/Resources/. 
Last access: Jan 2005. 
Kishida, Kazuaki and Noriko Kando. 2004. Two-stage 
refinement of query translation in a pivot language 
approach to cross-lingual information retrieval: 
An experiment at CLEF 2003. CLEF 2003, LNCS 
3237: 253-262.  
Kosasih, E. 2003. Kompetensi Ketatabahasaan dan 
Kesusastraan, Cermat Berbahasa Indonesia. 
Yrama Widya. 
Mainichi Shinbun CD-Rom data sets 1993-1995, 
Nichigai Associates Co., 1994-1996. 
Michibata, H., ed.: Eijirou, Alc. Last access:2002. 
Qu, Yan and G. Grefenstette, D. A. Evans. 2002. 
Resolving translation ambiguity using monolingual 
corpora. Advanced in Cross-Language Information 
Retrieval, vol. 2785 of LNCS: 223-241. Springer 
Verlag. 
Sanggar Bahasa Indonesia Proyek: Kmsmini2000. 
http://ml.ryu.titech.ac.jp/~indonesia/tokodai/dokum
en/ kamusjpina.pdf. Last access: May 2004. 
Tanaka, Kumiko and Kyoji Umemura. Construction 
of a bilingual dictionary intermediated by a third 
language. COLING 1994, pages 297-303, Kyoto. 
Wikipedia on Indonesian Language. 
http://en.wikipedia.org/wiki/ Indonesian_language. 
Last access: May 2005. 
WordNet. http://wordnet.princeton.edu/. Last access: 
February 2004. 
Zu, Guowei, et, al. 2004. Automatic Text 
Classification Techniques. IEEJ Trans EIS, Vol. 
124, No. 3. 
8
Chunking Japanese Compound Functional Expressions
by Machine Learning
Masatoshi Tsuchiya? and Takao Shime? and Toshihiro Takagi?
Takehito Utsuro?? and Kiyotaka Uchimoto?? and Suguru Matsuyoshi?
Satoshi Sato?? and Seiichi Nakagawa??
?Computer Center / ??Department of Information and Computer Sciences,
Toyohashi University of Technology, Tenpaku-cho, Toyohashi, 441?8580, JAPAN
?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606?8501, JAPAN
??Graduate School of Systems and Information Engineering, University of Tsukuba,
1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN
??National Institute of Information and Communications Technology,
3?5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619?0289 JAPAN
??Graduate School of Engineering, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPAN
Abstract
The Japanese language has various types
of compound functional expressions,
which are very important for recogniz-
ing the syntactic structures of Japanese
sentences and for understanding their
semantic contents. In this paper, we
formalize the task of identifying Japanese
compound functional expressions in a
text as a chunking problem. We apply a
machine learning technique to this task,
where we employ that of Support Vector
Machines (SVMs). We show that the pro-
posed method significantly outperforms
existing Japanese text processing tools.
1 Introduction
As in the case of other languages, the Japanese
language has various types of functional words
such as post-positional particles and auxiliary
verbs. In addition to those functional words,
the Japanese language has much more compound
functional expressions which consist of more than
one words including both content words and func-
tional words. Those single functional words as
well as compound functional expressions are very
important for recognizing the syntactic structures
of Japanese sentences and for understanding their
semantic contents. Recognition and understanding
of them are also very important for various kinds
of NLP applications such as dialogue systems, ma-
chine translation, and question answering. How-
ever, recognition and semantic interpretation of
compound functional expressions are especially
difficult because it often happens that one com-
pound expression may have both a literal (in other
words, compositional) content word usage and
a non-literal (in other words, non-compositional)
functional usage.
For example, Table 1 shows two example sen-
tences of a compound expression ?? (ni)???
(tsuite)?, which consists of a post-positional par-
ticle ?? (ni)?, and a conjugated form ????
(tsuite)? of a verb ??? (tsuku)?. In the sentence
(A), the compound expression functions as a case-
marking particle and has a non-compositional
functional meaning ?about?. On the other hand,
in the sentence (B), the expression simply corre-
sponds to a literal concatenation of the usages of
the constituents: the post-positional particle ??
(ni)? and the verb ???? (tsuite)?, and has a
content word meaning ?follow?. Therefore, when
considering machine translation of those Japanese
sentences into English, it is necessary to precisely
judge the usage of the compound expression ??
(ni)??? (tsuite)?, as shown in the English trans-
lation of the two sentences in Table 1.
There exist widely-used Japanese text process-
ing tools, i.e., pairs of a morphological analysis
tool and a subsequent parsing tool, such as JU-
MAN1+ KNP2 and ChaSen3+ CaboCha4. How-
ever, they process those compound expressions
only partially, in that their morphological analy-
sis dictionaries list only limited number of com-
pound expressions. Furthermore, even if certain
expressions are listed in a morphological analysis
1http://www.kc.t.u-tokyo.ac.jp/
nl-resource/juman-e.html
2http://www.kc.t.u-tokyo.ac.jp/
nl-resource/knp-e.html
3http://chasen.naist.jp/hiki/ChaSen/
4http://chasen.org/?taku/software/
cabocha/
25
Table 1: Translation Selection of a Japanese Compound Expression ?? (ni)??? (tsuite)?
? (watashi) ? (ha) ? (kare) ? (ni)??? (tsuite) ??? (hanashita)
(A) (I) (TOP) (he) (about) (talked)
(I talked about him.)
? (watashi) ? (ha) ? (kare) ? (ni) ??? (tsuite) ??? (hashitta)
(B) (I) (TOP) (he) (ACC) (follow) (ran)
(I ran following him.)
Table 2: Classification of Functional Expressions based on Grammatical Function
# of major # of
Grammatical Function Type expressions variants Example
subsequent to predicate 36 67 ????
post-positional / modifying predicate (to-naru-to)
particle subsequent to nominal 45 121 ?????
type / modifying predicate (ni-kakete-ha)
subsequent to predicate, nominal 2 3 ???
/ modifying nominal (to-iu)
auxiliary verb type 42 146 ??? (te-ii)
total 125 337 ?
dictionary, those existing tools often fail in resolv-
ing the ambiguities of their usages, such as those
in Table 1. This is mainly because the frame-
work of those existing tools is not designed so as
to resolve such ambiguities of compound (possi-
bly functional) expressions by carefully consider-
ing the context of those expressions.
Considering such a situation, it is necessary
to develop a tool which properly recognizes and
semantically interprets Japanese compound func-
tional expressions. In this paper, we apply a ma-
chine learning technique to the task of identify-
ing Japanese compound functional expressions in
a text. We formalize this identification task as a
chunking problem. We employ the technique of
Support Vector Machines (SVMs) (Vapnik, 1998)
as the machine learning technique, which has been
successfully applied to various natural language
processing tasks including chunking tasks such
as phrase chunking (Kudo and Matsumoto, 2001)
and named entity chunking (Mayfield et al, 2003).
In the preliminary experimental evaluation, we fo-
cus on 52 expressions that have balanced distribu-
tion of their usages in the newspaper text corpus
and are among the most difficult ones in terms of
their identification in a text. We show that the pro-
posed method significantly outperforms existing
Japanese text processing tools as well as another
tool based on hand-crafted rules. We further show
that, in the proposed SVMs based framework, it is
sufficient to collect and manually annotate about
50 training examples per expression.
2 Japanese Compound Functional
Expressions and their Example
Database
2.1 Japanese Compound Functional
Expressions
There exist several collections which list Japanese
functional expressions and examine their usages.
For example, (Morita and Matsuki, 1989) examine
450 functional expressions and (Group Jamashii,
1998) also lists 965 expressions and their exam-
ple sentences. Compared with those two collec-
tions, Gendaigo Hukugouji Youreishu (National
Language Research Institute, 2001) (henceforth,
denoted as GHY) concentrates on 125 major func-
tional expressions which have non-compositional
usages, as well as their variants5 (337 expressions
in total), and collects example sentences of those
expressions. As a first step of developing a tool for
identifying Japanese compound functional expres-
sions, we start with those 125 major functional ex-
pressions and their variants. In this paper, we take
an approach of regarding each of those variants as
a fixed expression, rather than a semi-fixed expres-
sion or a syntactically-flexible expression (Sag et
al., 2002). Then, we focus on evaluating the ef-
fectiveness of straightforwardly applying a stan-
5For each of those 125 major expressions, the differences
between it and its variants are summarized as below: i) in-
sertion/deletion/alternation of certain particles, ii) alternation
of synonymous words, iii) normal/honorific/conversational
forms, iv) base/adnominal/negative forms.
26
Table 3: Examples of Classifying Functional/Content Usages
Expression Example sentence (English translation) Usage
(1) ???? ????????????? ????
??????
functional
(to-naru-to) (The situation is serious if it is not effec-
tive against this disease.)
(???? (to-naru-to) = if)
(2) ???? ???????????????
???? ????????
content
(to-naru-to) (They think that it will become a require-
ment for him to be the president.)
(????? (to-naru-to)
= that (something) becomes ?)
(3) ????? ???????? ????? ????
??????????
functional
(ni-kakete-ha) (He has a great talent for earning money.) (?????? (ni-kakete-ha)
= for ?)
(4) ????? ???? ????? ???? content
(ni-kakete-ha) (I do not worry about it.)
( (??)??????
((?)-wo-ki-ni-kakete-ha)
= worry about ?)
(5) ??? ??????? ??? ??????
??
functional
(to-iu) (I heard that he is alive.) (???? (to-iu) = that ?)
(6) ??? ????????????? ????? content
(to-iu) (Somebody says ?Please visit us.?.) (???? (to-iu)
= say (that) ?)
(7) ??? ???????????? ??? ? functional
(te-ii) (You may have a break after we finish this
discussion.)
(???? (te-ii) = may ?)
(8) ??? ????????? ??? ? content
(te-ii) (This bag is nice because it is big.) (???? (te-ii)
= nice because ?)
dard chunking technique to the task of identifying
Japanese compound functional expressions.
As in Table 2, according to their grammat-
ical functions, those 337 expressions in total
are roughly classified into post-positional particle
type, and auxiliary verb type. Functional expres-
sions of post-positional particle type are further
classified into three subtypes: i) those subsequent
to a predicate and modifying a predicate, which
mainly function as conjunctive particles and are
used for constructing subordinate clauses, ii) those
subsequent to a nominal, and modifying a predi-
cate, which mainly function as case-marking parti-
cles, iii) those subsequent to a nominal, and modi-
fying a nominal, which mainly function as adnom-
inal particles and are used for constructing adnom-
inal clauses. For each of those types, Table 2 also
shows the number of major expressions as well as
that of their variants listed in GHY, and an exam-
ple expression. Furthermore, Table 3 gives exam-
ple sentences of those example expressions as well
as the description of their usages.
2.2 Issues on Identifying Compound
Functional Expressions in a Text
The task of identifying Japanese compound func-
tional expressions roughly consists of detecting
candidates of compound functional expressions in
a text and of judging the usages of those can-
didate expressions. The class of Japanese com-
pound functional expressions can be regarded as
closed and their number is at most a few thousand.
27
Table 4: Examples of Detecting more than one Candidate Expression
Expression Example sentence (English translation) Usage
(9) ??? ????? ??? ???????? functional
(to-iu) (That?s why a match is not so easy.) (NP1??? (to-iu)NP2
= NP
2
called as NP
1
)
(10) ?????? ??? ?????? ???????? functional
(to-iu-mono-no) (Although he won, the score is bad.)
(???????
(to-iu-mono-no)
= although ?)
Therefore, it is easy to enumerate all the com-
pound functional expressions and their morpheme
sequences. Then, in the process of detecting can-
didates of compound functional expressions in a
text, the text are matched against the morpheme
sequences of the compound functional expressions
considered.
Here, most of the 125 major functional expres-
sions we consider in this paper are compound ex-
pressions which consist of one or more content
words as well as functional words. As we intro-
duced with the examples of Table 1, it is often
the case that they have both a compositional con-
tent word usage as well as a non-compositional
functional usage. For example, in Table 3, the
expression ????? (to-naru-to)? in the sen-
tence (2) has the meaning ? that (something) be-
comes ??, which corresponds to a literal concate-
nation of the usages of the constituents: the post-
positional particle ???, the verb ????, and the
post-positional particle ???, and can be regarded
as a content word usage. On the other hand, in
the case of the sentence (1), the expression ???
?? (to-naru-to)? has a non-compositional func-
tional meaning ?if?. Based on this discussion, we
classify the usages of those expressions into two
classes: functional and content. Here, functional
usages include both non-compositional and com-
positional functional usages, although most of the
functional usages of those 125 major expressions
can be regarded as non-compositional. On the
other hand, content usages include compositional
content word usages only.
More practically, in the process of detecting
candidates of compound functional expressions in
a text, it can happen that more than one can-
didate expression is detected. For example, in
Table 4, both of the candidate compound func-
tional expressions ???? (to-iu)? and ????
??? (to-iu-mono-no)? are detected in the sen-
tence (9). This is because the sequence of the two
morphemes ?? (to)? and ??? (iu)? constituting
the candidate expression ???? (to-iu)? is a sub-
sequence of the four morphemes constituting the
candidate expression ??????? (to-iu-mono-
no)? as below:
Morpheme sequence
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression??? (to-iu)
? (to) ?? (iu) ?? (mono) ? (no)
Candidate expression?????? (to-iu-mono-no)
? (to) ?? (iu) ?? (mono) ? (no)
This is also the case with the sentence (10).
Here, however, as indicated in Table 4, the sen-
tence (9) is an example of the functional usage of
the compound functional expression ???? (to-
iu)?, where the sequence of the two morphemes ?
? (to)? and ??? (iu)? should be identified and
chunked into a compound functional expression.
On the other hand, the sentence (10) is an ex-
ample of the functional usage of the compound
functional expression ??????? (to-iu-mono-
no)?, where the sequence of the four morphemes ?
? (to)?, ??? (iu)?, ??? (mono)?, and ?? (no)?
should be identified and chunked into a compound
functional expression. Actually, in the result of
our preliminary corpus study, at least in about 20%
of the occurrences of Japanese compound func-
tional expressions, more than one candidate ex-
pression can be detected. This result indicates that
it is necessary to consider more than one candidate
expression in the task of identifying a Japanese
compound functional expression, and also in the
task of classifying the functional/content usage of
a candidate expression. Thus, in this paper, based
on this observation, we formalize the task of iden-
tifying Japanese compound functional expressions
as a chunking problem, rather than a classification
problem.
28
Table 5: Number of Sentences collected from
1995 Mainichi Newspaper Texts (for 337 Expres-
sions)
# of expressions
50 ? # of sentences 187 (55%)
0 < # of sentences < 50 117 (35%)
# of sentences = 0 33 (10%)
2.3 Developing an Example Database
We developed an example database of Japanese
compound functional expressions, which is used
for training/testing a chunker of Japanese com-
pound functional expressions (Tsuchiya et al,
2005). The corpus from which we collect example
sentences is 1995 Mainichi newspaper text corpus
(1,294,794 sentences, 47,355,330 bytes). For each
of the 337 expressions, 50 sentences are collected
and chunk labels are annotated according to the
following procedure.
1. The expression is morphologically analyzed
by ChaSen, and its morpheme sequence6 is
obtained.
2. The corpus is morphologically analyzed by
ChaSen, and 50 sentences which include the
morpheme sequence of the expression are
collected.
3. For each sentence, every occurrence of the
337 expressions is annotated with one of the
usages functional/content by an annotator7.
Table 5 classifies the 337 expressions accord-
ing to the number of sentences collected from the
1995 Mainichi newspaper text corpus. For more
than half of the 337 expressions, more than 50 sen-
tences are collected, although about 10% of the
377 expressions do not appear in the whole cor-
pus. Out of those 187 expressions with more than
50 sentences, 52 are those with balanced distribu-
tion of the functional/content usages in the news-
paper text corpus. Those 52 expressions can be re-
garded as among the most difficult ones in the task
of identifying and classifying functional/content
6For those expressions whose constituent has conjugation
and the conjugated form also has the same usage as the ex-
pression with the original form, the morpheme sequence is
expanded so that the expanded morpheme sequences include
those with conjugated forms.
7For the most frequent 184 expressions, on the average,
the agreement rate between two human annotators is 0.93 and
the Kappa value is 0.73, which means allowing tentative con-
clusions to be drawn (Carletta, 1996; Ng et al, 1999). For
65% of the 184 expressions, the Kappa value is above 0.8,
which means good reliability.
usages. Thus, this paper focuses on those 52 ex-
pressions in the training/testing of chunking com-
pound functional expressions. We extract 2,600
sentences (= 52 expressions ? 50 sentences) from
the whole example database and use them for
training/testing the chunker. The number of the
morphemes for the 2,600 sentences is 92,899. We
ignore the chunk labels for the expressions other
than the 52 expressions, resulting in 2,482/701
chunk labels for the functional/content usages, re-
spectively.
3 Chunking Japanese Compound
Functional Expressions with SVMs
3.1 Support Vector Machines
The principle idea of SVMs is to find a separate
hyperplane that maximizes the margin between
two classes (Vapnik, 1998). If the classes are not
separated by a hyperplane in the original input
space, the samples are transformed in a higher di-
mensional features space.
Giving x is the context (a set of features) of
an input example; xi and yi(i = 1, ..., l, xi ?
Rn, yi?{1,?1}) indicate the context of the train-
ing data and its category, respectively; The deci-
sion function f in SVM framework is defined as:
f(x) = sgn
( l
?
i=1
?iyiK(xi,x) + b
)
(1)
where K is a kernel function, b ? R is a thresh-
old, and ?i are weights. Besides, the weights ?i
satisfy the following constraints:
0 ? ?i ? C (i = 1, ..., l) (2)
?l
i=1 ?iyi = 0 (3)
where C is a misclassification cost. The xi with
non-zero ?i are called support vectors. To train
an SVM is to find the ?i and the b by solving the
optimization problem; maximizing the following
under the constraints of (2) and (3):
L(?) =
l
?
i=1
?i?
1
2
l
?
i,j=1
?i?jyiyjK(x
i
,x
j
) (4)
The kernel function K is used to transform the
samples in a higher dimensional features space.
Among many kinds of kernel functions available,
we focus on the d-th polynomial kernel:
K(x,y) = (x ? y + 1)d (5)
29
Through experimental evaluation on chunking
Japanese compound functional expressions, we
compared polynomial kernels with d = 1, 2, and
3. Kernels with d = 2 and 3 perform best, while
the kernel with d = 3 requires much more compu-
tational cost than that with d = 2. Thus, through-
out the paper, we show results with the quadratic
kernel (d = 2).
3.2 Chunking with SVMs
This section describes details of formalizing the
chunking task using SVMs. In this paper, we use
an SVMs-based chunking tool YamCha8 (Kudo
and Matsumoto, 2001). In the SVMs-based
chunking framework, SVMs are used as classi-
fiers for assigning labels for representing chunks
to each token. In our task of chunking Japanese
compound functional expressions, each sentence
is represented as a sequence of morphemes, where
a morpheme is regarded as a token.
3.2.1 Chunk Representation
For representing proper chunks, we employ
IOB2 representation, one of those which have
been studied well in various chunking tasks of nat-
ural language processing (Tjong Kim Sang, 1999;
Kudo and Matsumoto, 2001). This method uses
the following set of three labels for representing
proper chunks.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
B Current token is the beginning of a chunk.
As we described in section 2.2, given a candi-
date expression, we classify the usages of the ex-
pression into two classes: functional and content.
Accordingly, we distinguish the chunks of the two
types: the functional type chunk and the content
type chunk. In total, we have the following five la-
bels for representing those chunks: B-functional,
I-functional, B-content, I-content, and O. Ta-
ble 6 gives examples of those chunk labels rep-
resenting chunks.
Finally, as for exending SVMs to multi-class
classifiers, we experimentally compare the pair-
wise method and the one vs. rest method, where
the pairwise method slightly outperformed the one
vs. rest method. Throughout the paper, we show
results with the pairwise method.
8http://chasen.org/?taku/software/
yamcha/
3.2.2 Features
For the feature sets for training/testing of
SVMs, we use the information available in the sur-
rounding context, such as the morphemes, their
parts-of-speech tags, as well as the chunk labels.
More precisely, suppose that we identify the chunk
label ci for the i-th morpheme:
?? Parsing Direction ??
Morpheme m
i?2
m
i?1
m
i
m
i+1
m
i+2
Feature set F
i?2
F
i?1
F
i
F
i+1
F
i+2
at a position
Chunk label c
i?2
c
i?1
c
i
Here, mi is the morpheme appearing at i-th po-
sition, Fi is the feature set at i-th position, and ci
is the chunk label for i-th morpheme. Roughly
speaking, when identifying the chunk label ci for
the i-th morpheme, we use the feature sets Fi?2,
Fi?1, Fi, Fi+1, Fi+2 at the positions i ? 2, i ? 1,
i, i + 1, i + 2, as well as the preceding two chunk
labels ci?2 and ci?1.
The detailed definition of the feature set Fi at i-
th position is given below. The feature set Fi is de-
fined as a tuple of the morpheme feature MF (mi)
of the i-th morpheme mi, the chunk candidate fea-
ture CF (i) at i-th position, and the chunk context
feature OF (i) at i-th position.
Fi = ? MF (mi), CF (i), OF (i) ?
The morpheme feature MF (mi) consists of the
lexical form, part-of-speech, conjugation type and
form, base form, and pronunciation of mi.
The chunk candidate feature CF (i) and the
chunk context feature OF (i) are defined consid-
ering the candidate compound functional expres-
sion, which is a sequence of morphemes includ-
ing the morpheme mi at the current position i. As
we described in section 2, the class of Japanese
compound functional expressions can be regarded
as closed and their number is at most a few thou-
sand. Therefore, it is easy to enumerate all the
compound functional expressions and their mor-
pheme sequences. Chunk labels other than O
should be assigned to a morpheme only when it
constitutes at least one of those enumerated com-
pound functional expressions. Suppose that a se-
quence of morphemes mj . . . mi . . . mk including
mi at the current position i constitutes a candidate
functional expression E as below:
m
j?2
m
j?1
m
j
. . . m
i
. . . m
k
m
k+1
m
k+2
candidate E of
a compound
functional expression
where the morphemes mj?2, mj?1, mk+1, and
mk+2 are at immediate left/right contexts of E.
Then, the chunk candidate feature CF (i) at i-th
position is defined as a tuple of the number of mor-
phemes constituting E and the position of mi in
E. The chunk context feature OF (i) at i-th posi-
tion is defined as a tuple of the morpheme features
30
Table 6: Examples of Chunk Representation and Chunk Candidate/Context Features
(a) Sentence (7) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
?? (giron) (discussion) O ? ?
? (ga) (NOM) O ? ?
???(owatt) (finish) O ? ?
?? (tara) (after) O ? ?
?? (kyuukei) (break) O ? ?
? (shi) (have) O ? ?
? (te) (may) B-functional ?2, 1? ? MF (?? (kyuukei)), ?, MF (? (shi)), ?,
?? (ii) I-functional ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
(b) Sentence (8) of Table 3
(English Chunk candidate Chunk context
Morpheme translation) Chunk label feature feature
?? (kono) (this) O ? ?
??? (bag) (discussion) O ? ?
? (ha) (TOP) O ? ?
??? (ookiku) (big) O ? ?
? (te) (because) B-content ?2, 1? ? MF (? (ha)), ?, MF (??? (ookiku)), ?,
?? (ii) (nice) I-content ?2, 2? MF (?(period)), ?, ?, ? ?
?(period) (period) O ? ?
as well as the chunk candidate features at immedi-
ate left/right contexts of E.
CF (i) = ? length of E, position of m
i
in E ?
OF (i) = ? MF (m
j?2
), CF (j ? 2),
MF (m
j?1
), CF (j ? 1),
MF (m
k+1
), CF (k + 1),
MF (m
k+2
), CF (k + 2) ?
Table 6 gives examples of chunk candidate fea-
tures and chunk context features
It can happen that the morpheme at the cur-
rent position i constitutes more than one candidate
compound functional expression. For example,
in the example below, the morpheme sequences
mi?1mimi+1, mi?1mi, and mimi+1mi+2 consti-
tute candidate expressions E
1
, E
2
, and E
3
, respec-
tively.
Morpheme sequence m
i?1
m
i
m
i+1
m
i+2
Candidate E
1
m
i?1
m
i
m
i+1
Candidate E
2
m
i?1
m
i
Candidate E
3
m
i
m
i+1
m
i+2
In such cases, we prefer the one starting with the
leftmost morpheme. If more than one candidate
expression starts with the leftmost morpheme, we
prefer the longest one. In the example above, we
prefer the candidate E
1
and construct the chunk
candidate features and chunk context features con-
sidering E
1
only.
4 Experimental Evaluation
The detail of the data set we use in the experimen-
tal evaluation was presented in section 2.3. As we
show in Table 7, performance of our SVMs-based
chunkers as well as several baselines including ex-
isting Japanese text processing tools is evaluated
in terms of precision/recall/F?=1 of identifying
functional chunks. Performance is evaluated also
in terms of accuracy of classifying detected can-
didate expressions into functional/content chunks.
Among those baselines, ?majority ( = functional)?
always assigns functional usage to the detected
candidate expressions. ?Hand-crafted rules? are
manually created 145 rules each of which has con-
ditions on morphemes constituting a compound
functional expression as well as those at immedi-
ate left/right contexts. Performance of our SVMs-
based chunkers is measured through 10-fold cross
validation.
As shown in Table 7, our SVMs-based chunkers
significantly outperform those baselines both in
F?=1 and classification accuracy9. We also evalu-
ate the effectiveness of each feature set, i.e., the
morpheme feature, the chunk candidate feature,
and the chunk context feature. The results in the
table show that the chunker with the chunk candi-
date feature performs almost best even without the
chunk context feature10.
9Recall of existing Japanese text processing tools is low,
because those tools can process only 50?60% of the whole
52 compound functional expressions, and for the remaining
40?50% expressions, they fail in identifying all of the occur-
rences of functional usages.
10It is also worthwhile to note that training the SVMs-
based chunker with the full set of features requires computa-
tional cost three times as much as training without the chunk
31
Table 7: Evaluation Results (%)
Identifying Acc. of classifying
functional chunks functional/content
Prec. Rec. F?=1 chunks
majority ( = functional) 78.0 100 87.6 78.0
Baselines Juman/KNP 89.2 49.3 63.5 55.8
ChaSen/CaboCha 89.0 45.6 60.3 53.2
hand-crafted rules 90.7 81.6 85.9 79.1
SVM morpheme 88.0 91.0 89.4 86.5
(feature morpheme + chunk-candidate 91.0 93.2 92.1 89.0
set) morpheme + chunk-candidate/context 91.1 93.6 92.3 89.2
Figure 1: Change of F?=1 with Different Number
of Training Instances
For the SVMs-based chunker with the chunk
candidate feature with/without the chunk context
feature, Figure 1 plots the change of F?=1 when
training with different number of labeled chunks
as training instances. With this result, the increase
in F?=1 seems to stop with the maximum num-
ber of training instances, which supports the claim
that it is sufficient to collect and manually annotate
about 50 training examples per expression.
5 Concluding Remarks
The Japanese language has various types of com-
pound functional expressions, which are very im-
portant for recognizing the syntactic structures of
Japanese sentences and for understanding their se-
mantic contents. In this paper, we formalized
the task of identifying Japanese compound func-
tional expressions in a text as a chunking prob-
lem. We applied a machine learning technique
to this task, where we employed that of Sup-
port Vector Machines (SVMs). We showed that
the proposed method significantly outperforms ex-
isting Japanese text processing tools. The pro-
context feature.
posed framework has advantages over an approach
based on manually created rules such as the one in
(Shudo et al, 2004), in that it requires human cost
to manually create and maintain those rules. On
the other hand, in our framework based on the ma-
chine learning technique, it is sufficient to collect
and manually annotate about 50 training examples
per expression.
References
J. Carletta. 1996. Assessing agreement on classification
tasks: the Kappa statistic. Computational Linguistics,
22(2):249?254.
Group Jamashii, editor. 1998. Nihongo Bunkei Jiten.
Kuroshio Publisher. (in Japanese).
T. Kudo and Y. Matsumoto. 2001. Chunking with support
vector machines. In Proc. 2nd NAACL, pages 192?199.
J. Mayfield, P. McNamee, and C. Piatko. 2003. Named entity
recognition using hundreds of thousands of features. In
Proc. 7th CoNLL, pages 184?187.
Y. Morita and M. Matsuki. 1989. Nihongo Hyougen Bunkei,
volume 5 of NAFL Sensho. ALC. (in Japanese).
National Language Research Institute. 2001. Gendaigo
Hukugouji Youreishu. (in Japanese).
H. T. Ng, C. Y. Lim, and S. K. Foo. 1999. A case study on
inter-annotator agreement for word sense disambiguation.
In Proc. ACL SIGLEXWorkshop on Standardizing Lexical
Resources, pages 9?13.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In Proc. 3rd CICLING, pages 1?15.
K. Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura. 2004.
MWEs as non-propositional content indicators. In Proc.
2nd ACL Workshop on Multiword Expressions: Integrat-
ing Processing, pages 32?39.
E. Tjong Kim Sang. 1999. Representing text chunks. In
Proc. 9th EACL, pages 173?179.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2005. A corpus for classifying usages of Japanese
compound functional expressions. In Proc. PACLING,
pages 345?350.
V. N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
32
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 161?167,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Analysis and Robust Extraction of Changing Named Entities
Masatoshi Tsuchiya? Shoko Endo? Seiichi Nakagawa?
?Information and Media Center / ?Department of Information and Computer Sciences,
Toyohashi University of Technology
tsuchiya@imc.tut.ac.jp, {shoko,nakagawa}@slp.ics.tut.ac.jp
Abstract
This paper focuses on the change of named
entities over time and its influence on the
performance of the named entity tagger.
First, we analyze Japanese named enti-
ties which appear in Mainichi Newspaper
articles published in 1995, 1996, 1997,
1998 and 2005. This analysis reveals that
the number of named entity types and
the number of named entity tokens are
almost steady over time and that 70 ?
80% of named entity types in a certain
year occur in the articles published either
in its succeeding year or in its preceding
year. These facts lead that 20 ? 30%
of named entity types are replaced with
new ones every year. The experiment
against these texts shows that our propos-
ing semi-supervised method which com-
bines a small annotated corpus and a large
unannotated corpus for training works ro-
bustly although the traditional supervised
method is fragile against the change of
name entity distribution.
1 Introduction
It is widely agreed that extraction of named entity
(henceforth, denoted as NE) is an important sub-
task for various NLP applications, such as infor-
mation retrieval, machine translation, information
extraction and natural language understanding.
Several conferences like Message Understanding
Conference(Grishman and Sundheim, 1996) and
the IREX workshop (Sekine and Eriguchi, 2000)
were conducted to encourage researchers of NE
extraction and to provide its common evaluation
basis.
In Japanese NE extraction, it is quite common
to apply morphological analysis as preprocessing
stage which segments a sentence into a sequence
of morphemes. After that, either a pattern matcher
based on hand-crafted rules or a statistical chun-
ker is employed to extract NEs from a sequence of
morphemes. Various machine learning approaches
such as maximum entropy(Uchimoto et al, 2000),
decision list(Sassano and Utsuro, 2000; Isozaki,
2001), and Support Vector Machine(Yamada et
al., 2002; Isozaki and Kazawa, 2002) were in-
vestigated for extracting NEs. These researches
show that machine learning approaches are more
promising than approaches based on hand-crafted
rules if a large corpus whose NEs are properly an-
notated is available as training data.
However, it is difficult to obtain an enough cor-
pus in the real world because of the increasing
number of NE types and the increasing time gap
between the training corpus and the test corpus.
There is the increasing number of NE types like
personal names and company names in the real
world. For example, a large database of organi-
zation names(Nichigai Associates, 2007) already
contains 171,708 types and is still increasing. Be-
cause annotation work is quite expensive, the an-
notated corpus may become obsolete in a short
period of time. Both of two factors expands the
difference of NE distribution between the training
corpus and the test corpus, and it may decrease the
performance of the NE tagger as shown in (Mota
and Grishman, 2008). Therefore, a robust method
to extract NEs which do not occur or occur few
times in a training corpus is necessary.
This paper focuses on the change of NEs over
time and its influence on the performance of the
NE tagger. First, we annotate NEs in Mainichi
Newspaper articles published in 1996, 1997, 1998
and 2005, and analyze NEs which appear in
these texts and an existing corpus. It consists of
Mainichi Newspaper articles published in 1995,
thus, we get an annotated corpus that spans 10
years. This analysis reveals that the number of
NE types and the number of NE tokens are almost
161
Table 1: Statistics of NE categories of IREX cor-
pus
NE Categories Frequency (%)
ARTIFACT 747 (4.0)
DATE 3567 (19.1)
LOCATION 5463 (29.2)
MONEY 390 (2.1)
ORGANIZATION 3676 (19.7)
PERCENT 492 (2.6)
PERSON 3840 (20.6)
TIME 502 (2.7)
Total 18677
steady over time and that that 70 ? 80% of NE
types in a certain year occur in the articles pub-
lished either in its succeeding year or in its preced-
ing year. These facts lead that 20 ? 30% of named
entity types are replaced with new ones every year.
The experiment against these corpora shows that
the traditional supervised method is fragile against
the change of NE types and that our proposing
semi-supervised method which combines a small
annotated corpus and a large unannotated corpus
for training is robust against the change of NE
types.
2 Analysis of Changing Named Entities
2.1 Task of the IREX Workshop
The task of NE extraction of the IREX work-
shop (Sekine and Eriguchi, 2000) is to recognize
eight NE categories in Table 1. The organizer
of the IREX workshop provided a training corpus
(henceforth, denoted as IREX corpus), which con-
sists of 1,174 Mainichi Newspaper articles pub-
lished from January 1st 1995 to 10th which in-
clude 18,677 NEs. In the Japanese language, no
other corpora whose NEs are annotated are pub-
licly available as far as we know.1 Thus, IREX
corpus is referred as a golden sample of NE distri-
bution in this paper.
2.2 Data Description
The most homogeneous texts which are written in
different days are desirable, to explore the influ-
ence of the text time frame on NE distribution. Be-
cause IREX corpus is referred as a golden sample
1The organizer of the IREX workshop also provides the
testing data to its participants, however, we cannot see it be-
cause we did not join it.
??
???
???
???
???
???
???
???
???
???
????
?????
?????
???
?????
?????
????
?????
?????
????
?????
?????
???
?????
?????
???
?????
?????
????
?????
?????
????
?????
?????
????
??????????????????????????????????????????????????????
Figure 1: Distribution of NE categories
??
???
???
???
???
????
?? ?? ?? ? ? ? ? ????????????????????????????????????????????????????????
?????
????
????
?
????????????????
Figure 2: Overlap ratio of NEs over years
in this paper, Mainichi Newspaper articles writ-
ten in different years than IREX corpus is suit-
able. Thus, ordinal days of June and October in
1996, 1997, 1998 and 2005 are randomly selected
as sampling days.
Because annotating work is too expensive for
us to annotate all articles published in sampling
days, thirty percent of them are only annotated.
Each article of Mainichi Newspaper belongs into
16 categories like front page articles, international
stories, economical stories, political stories, edito-
rial columns, and human interest stories. Because
these categories may influence to NE distribution,
it is important to keep the proportion of categories
in the sampled texts to the proportion in the whole
newspaper, in order to investigate NE distribution
over the whole newspaper. Therefore, thirty per-
cent articles of each category published at sam-
pling days are randomly selected and annotated in
accordance with the IREX regulation.
2.3 Analysis of Annotated Samples
Table 2 shows the statistics of our annotated cor-
pus. The leftmost column of Table 2 (whose pub-
162
Table 2: Statistics of sampling texts
Published date 1995 1996 1997 1998 2005Jan. 1?10 Jun. 5 Oct. 15 Jun. 10 Oct. 7 Jun. 8 Oct. 21 Jun. 23 Oct. 12
# of articles 1174 120 133 106 117 96 126 90 99
# of characters 407881 60790 53625 46653 50362 51006 67744 49038 44344
# of NE types 6979 1446 1656 1276 1350 1190 1226 1230 1113
# of NE tokens 18677 2519 2652 2145 2403 2126 2052 1902 2007
# of NE types / # of characters 0.0171 0.0238 0.0309 0.0274 0.0268 0.0233 0.0181 0.0251 0.0251
# of NE tokens / # of characters 0.0458 0.0414 0.0495 0.0460 0.0477 0.0417 0.0303 0.0388 0.0453
Table 3: Overlap of NE types between texts published in different years
Published date of Published year of unannotated corpus U
annotated corpus A 1993 1994 1995 1996 1997 1998 1999
Jan. 1?10 (1995) 73.2% 78.6% ? 74.4% 65.0% 64.4% 63.3%
Jun. 6, Oct. 15 (1996) 67.2% 71.7% 72.2% ? 77.3% 76.0% 75.1%
Jun. 6, Oct. 7 (1997) 71.2% 73.4% 74.4% 78.6% ? 80.8% 78.6%
Jun. 8, Oct. 21 (1998) 72.5% 74.6% 76.2% 79.7% 82.7% ? 84.0%
Jun. 23, Oct. 12 (2005) 62.3% 64.1% 66.8% 68.7% 71.2% 72.9% 73.8%
lish date is January 1st to 10th in 1995) is corre-
sponding to IREX corpus, and other columns are
corresponding to articles annotated by ourselves.
Table 2 illustrates that the normalized number of
NE types and the normalized number of NE tokens
are almost steady over time. Figure 1 shows the
distributions of NE categories for sampling texts
and that there is no significant difference between
them.
We also investigate the relation of the time gap
between texts and NE types which appear in these
texts. The overlap ratio of NE types between the
annotated corpus A published in the year YA and
the annotated corpus B published in the year YB
was defined in (Mota and Grishman, 2008) as fol-
lows
type overlap(A,B) = |TA ? TB|
|TA|+ |TB| ? |TA ? TB|
,
where TA and TB are lists of NE types which ap-
pear in A and B respectively. However, it is im-
possible to compute reliable type overlap in our
research because enough annotated texts are un-
available. As an alternative of type overlap, the
overlap ratio of NE types between the annotated
corpus A and the unannotated corpus U published
in the year YU is defined as follows
string overlap(A,U) =
?
s?TA ?(s, U)
|TA|
,
where ?(s, U) is the binary function to indicate
whether the string s occurs in the string U or not.
Table 3 shows string ratio values of annotated
texts. It shows that 70 ? 80% of TA appear in the
preceding year of YA, and that 70 ? 80% of TA
appear in the succeeding year of YA.
Figure 2 shows the relation between the time
gap YU ? YA and string ratio(A,U). Sup-
pose that all NEs are independent and equiv-
alent on their occurrence probability and that
string ratio(A,U) is equal to 0.8 when the time
gap YU ? YA is equal to one. When the time gap
YU ? ? YA is equal to two years, although this as-
sumption leads that string ratio(A,U ?) will be
equal to 0.64, string ratio(A,U ?) in Figure 2 is
greater than 0.7. This suggests that NEs are not
equivalent on their occurrence probability. And
more, Table 4 shows that the longer time span
of the annotated text increases the number of NE
types. These facts lead that some NEs are short-
lived and superseded by other new NEs.
3 Robust Extraction of Changing Named
Entities
It is infeasible to prepare a large annotated cor-
pus which covers all increasing NEs. A semi-
supervised learning approach which combines a
small annotated corpus and a large unannotated
corpus for training is promising to cope this prob-
lem. (Miller et al, 2004) proposed the method
using classes which are assigned to words based
on the class language model built from a large
unannotated corpus. (Ando and Zhang, 2005) pro-
163
Table 4: Number of NE types and Time Span of Annotated Text
1995 1995?1996 1995?1997 1995?1998 1995?2005
ARTIFACT 541 (1.00) 743 (1.37) 862 (1.59) 1025 (1.89) 1169 (2.16)
DATE 950 (1.00) 1147 (1.21) 1326 (1.40) 1461 (1.54) 1583 (1.67)
LOCATION 1403 (1.00) 1914 (1.36) 2214 (1.58) 2495 (1.78) 2692 (1.92)
MONEY 301 (1.00) 492 (1.63) 570 (1.89) 656 (2.18) 749 (2.49)
ORGANIZATION 1487 (1.00) 1890 (1.27) 2280 (1.53) 2566 (1.73) 2893 (1.95)
PERCENT 249 (1.00) 319 (1.28) 353 (1.42) 401 (1.61) 443 (1.78)
PERSON 1842 (1.00) 2540 (1.38) 3175 (1.72) 3683 (2.00) 4243 (2.30)
TIME 206 (1.00) 257 (1.25) 291 (1.41) 314 (1.52) 332 (1.61)
Total 6979 (1.00) 9302 (1.33) 11071 (1.59) 12601 (1.81) 14104 (2.02)
(Values in brackets are rates of increase comparing to 1995.)
Morpheme Feature Similar Morpheme Feature Character
(English POS (English POS Type Chunk Label
translation) translation) Feature
?? (kyou) (today) Noun?Adverbial ?? (kyou) (today) Noun?Adverbial ?1, 0, 0, 0, 0, 0? O
? (no) gen Particle ? (no) gen Particle ?0, 1, 0, 0, 0, 0? O
?? (Ishikari) (Ishikari) Noun?Proper ?? (Kantou) (Kantou) Noun?Proper ?1, 0, 0, 0, 0, 0? B-LOCATION
?? (heiya) (plain) Noun?Generic ?? (heiya) (plain) Noun?Generic ?1, 0, 0, 0, 0, 0? I-LOCATION
? (no) gen Particle ? (no) gen Particle ?0, 1, 0, 0, 0, 0? O
?? (tenki) (weather) Noun?Generic ?? (tenki) (weather) Noun?Generic ?1, 0, 0, 0, 0, 0? O
? (ha) top Particle ? (ha) top Particle ?0, 1, 0, 0, 0, 0? O
?? (hare) (fine) Noun?Generic ?? (hare) (fine) Noun?Generic ?1, 1, 0, 0, 0, 0? O
Figure 3: Example of Training Instance for Proposed Method
posed the method using thousands of automati-
cally generated auxiliary classification problems
on an unannotated corpus. (?) proposed the semi-
supervised discriminative model whose potential
function can treat both an annotated corpus and an
unannotated corpus.
In this paper, the method proposed by (Tsuchiya
et al, 2008) is employed, because its implementa-
tion is quite easy. It consists of two steps. The
first step is to assign the most similar and famil-
iar morpheme to each unfamiliar morpheme based
on their context vectors calculated from a large
unannotated corpus. The second step is to employ
Conditional Random Fields(CRF)2(Lafferty et al,
2001) using both features of original morphemes
and features of similar morphemes.
This section gives the detail of this method.
3.1 Chunking of Named Entities
It is quite common that the task of extracting
Japanese NEs from a sentence is formalized as
a chunking problem against a sequence of mor-
phemes. For representing proper chunks, we em-
ploy IOB2 representation, one of representations
which have been studied well in various chunking
2http://chasen.org/?taku/software/CRF+
+/
tasks of NLP (Tjong Kim Sang, 1999). This rep-
resentation uses the following three labels.
B Current token is the beginning of a chunk.
I Current token is a middle or the end of a
chunk consisting of more than one token.
O Current token is outside of any chunk.
Actually, we prepare the 16 derived labels from
the label B and the label I for eight NE categories,
in order to distinguish them.
When the task of extracting Japanese NEs from
a sentence is formalized as a chunking problem
of a sequence of morphemes, the segmentation
boundary problem arises as widely known. For
example, the NE definition of IREX tells that a
Chinese character ?? (bei)? must be extracted as
an NE means America from a morpheme ???
(hou-bei)? which means visiting America. A naive
chunker using a morpheme as a chunking unit can-
not extract such a kind of NEs. In order to cope
this problem, (Uchimoto et al, 2000) proposed
employing translation rules to modify problematic
morphemes, and (Asahara and Matsumoto, 2003;
Nakano and Hirai, 2004) formalized the task of ex-
tracting NEs as a chunking problem of a sequence
of characters instead of a sequence of morphemes.
In this paper, we keep the naive formalization, be-
cause it is still enough to analyze the influence of
164
the text time frame.
3.2 Assignment of Similar Morpheme
A context vector Vm of a morpheme m is a vector
consisting of frequencies of all possible unigrams
and bigrams,
Vm =
?
?
?
?
?
f(m,m0), ? ? ? f(m,mN ),
f(m,m0,m0), ? ? ? f(m,mN ,mN ),
f(m0,m), ? ? ? f(mN ,m),
f(m0,m0,m), ? ? ? f(mN ,mN ,m)
?
?
?
?
?
,
where M ? {m0,m1, . . . ,mN} is a set of all
morphemes of the unannotated corpus, f(mi,mj)
is a frequency that a sequence of a morpheme mi
and a morpheme mj occurs in the unannotated
corpus, and f(mi,mj ,mk) is a frequency that a
sequence of morphemes mi,mj and mk occurs in
the unannotated corpus.
Suppose an unfamiliar morpheme mu ? M ?
MF , where MF is a set of familiar morphemes
that occur frequently in the annotated corpus. The
most similar morpheme m?u to the morpheme mu
measured with their context vectors is given by the
following equation,
m?u = argmax
m?MF
sim(Vmu , Vm), (1)
where sim(Vi, Vj) is a similarity function between
context vectors. In this paper, the cosine function
is employed as it.
3.3 Features
The feature set Fi at i-th position is defined as
a tuple of the morpheme feature MF (mi) of the
i-th morpheme mi, the similar morpheme feature
SF (mi), and the character type feature CF (mi).
Fi = ? MF (mi), SF (mi), CF (mi) ?
The morpheme feature MF (mi) is a pair of the
surface string and the part-of-speech of mi. The
similar morpheme feature SF (mi) is defined as
SF (mi) =
{
MF (m?i) if mi ? M ?MF
MF (mi) otherwise
,
where m?i is the most similar and familiar mor-
pheme to mi given by Eqn. 1. The character type
feature CF (mi) is a set of six binary flags to in-
dicate that the surface string of mi contains a Chi-
nese character, a hiragana character, a katakana
?? Chunking Direction ??
Feature set Fi?2 Fi?1 Fi Fi+1 Fi+2
Chunk label ci?2 ci?1 ci
Figure 4: Chunking Direction
character, an English alphabet, a number and an
other character respectively.
When we identify the chunk label ci for the i-
th morpheme mi, the surrounding five feature sets
Fi?2, Fi?1, Fi, Fi+1, Fi+2 and the preceding two
chunk labels ci?2, ci?1 are referred as shown in
Figure 4.
Figure 3 shows an example of training instance
of the proposed method for the sentence ???
(kyou)? (no)?? (Ishikari)?? (heiya)? (no)
?? (tenki)? (ha)?? (hare)? which means ?It
is fine at Ishikari-plain, today?. ??? (Kantou)?
is assigned as the most similar and familiar mor-
pheme to ??? (Ishikari)? which is unfamiliar in
the training corpus.
3.4 Experimental Result
Figure 5 compares performances of the proposed
method and the baseline method over the test texts
which were published in 1996, 1997, 1998 and
2005. The proposed method combines a small an-
notated corpus and a large unannotated corpus as
already described. This experiment refers IREX
corpus as a small annotated corpus, and refers
Mainichi Newspaper articles published from 1993
to the preceding year of the test text published
year as a large unannotated corpus. For example,
when the test text was published in 1998, Mainichi
Newspaper articles published from 1993 to 1997
are used. The baseline method is trained from
IREX corpus with CRF. But, it uses only MF and
CF as features, and does not use SF . Figure 5 il-
lustrates two points: (1) the proposed method out-
performs the baseline method consistently, (2) the
baseline method is fragile to changing of test texts.
Figure 6 shows the relation between the per-
formance of the proposed method and the size of
unannotated corpus against the test corpus pub-
lished in 2005. It reveals that that increasing unan-
notated corpus size improves the performance of
the proposed method.
4 Conclusion
In this paper, we explored the change of NE dis-
tribution over time and its influence on the per-
165
??????????
??????????
??????????
??????????
??????????
??????????
???? ???? ???? ????????????????????????????????
???
????
??
????????????????
Figure 5: Comparison between proposed method
and baseline method
?????
????
?????
????
?????
????
?????
????? ????
?????
????
?????
????
?????
????
?????
????
?????
????
?????
????
??????????????????????????????????
???
???
???
Figure 6: Relation of performance and unanno-
tated corpus size
formance of the NE tagger. First, we annotated
Mainichi Newspaper articles published in 1996,
1997, 1998 and 2005, and analyzed NEs which
appear in these texts and IREX corpus which con-
sists of Mainichi Newspaper articles published in
1995. This analysis illustrated that the number of
NE types and the number of NE tokens are al-
most steady over time, and that 70 ? 80% of NE
types seen in a certain year occur in the texts pub-
lished either in its succeeding year or in its pre-
ceding year. The experiment against these texts
showed that our proposing semi-supervised NE
tagger works robustly although the traditional su-
pervised NE tagger is fragile against the change of
NE types. Based on the results described in this
paper, we will investigate the relation between the
performance of NE tagger and the similarity of its
training corpus and its test corpus.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proc. of ACL ?05, pages 1?9, June.
Masayuki Asahara and Yuji Matsumoto. 2003.
Japanese named entity extraction with redundant
morphological analysis. In Proc. of HLT?NAACL
?03, pages 8?15.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference-6: a brief history. In
Proc. of the 16th COLING, pages 466?471.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. of the 19th COLING, pages 1?7.
Hideki Isozaki. 2001. Japanese named entity recogni-
tion based on a simple rule generator and decision
tree learning. In Proc. of ACL ?01, pages 314?321.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282?
289.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proc. of HLT-NAACL 2004,
pages 337?342, May.
Cristina Mota and Ralph Grishman. 2008. Is this NE
tagger getting old? In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), May.
Keigo Nakano and Yuzo Hirai. 2004. Japanese named
entity extraction with bunsetsu features. Transac-
tions of Information Processing Society of Japan,
45(3):934?941, Mar. (in Japanese).
Nichigai Associates, editor. 2007. DCS Kikan-mei
Jisho. Nichigai Associates. (in Japanese).
Manabu Sassano and Takehito Utsuro. 2000. Named
entity chunking techniques in supervised learning
for japanese named entity recognition. In Proc. of
the 18th COLING, pages 705?711.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation: analysis of re-
sults. In Proc. of the 18th COLING, pages 1106?
1110.
E. Tjong Kim Sang. 1999. Representing text chunks.
In Proc. of the 9th EACL, pages 173?179.
Masatoshi Tsuchiya, Shinya Hida, and Seiichi Naka-
gawa. 2008. Robust extraction of named entity in-
cluding unfamiliar word. In Proceedings of ACL-
08: HLT, Short Papers, pages 125?128, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
166
Kiyotaka Uchimoto, Ma Qing, Masaki Murata, Hiromi
Ozaku, Masao Utiyama, and Hitoshi Isahara. 2000.
Named entity extraction based on a maximum en-
tropy model and transformation rules. Journal of
Natural Language Processing, 7(2):63?90, Apr. (in
Japanese).
Hiroyasu Yamada, Taku Kudo, and Yuji Matsumoto.
2002. Japanese named entity extraction using sup-
port vector machine. Transactions of Information
Processing Society of Japan, 43(1):44?53, Jan. (in
Japanese).
167
