Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1573?1582,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automated planning for situated natural language generation
Konstantina Garoufi and Alexander Koller
Cluster of Excellence ?Multimodal Computing and Interaction?
Saarland University, Saarbru?cken, Germany
{garoufi,koller}@mmci.uni-saarland.de
Abstract
We present a natural language genera-
tion approach which models, exploits, and
manipulates the non-linguistic context in
situated communication, using techniques
from AI planning. We show how to gen-
erate instructions which deliberately guide
the hearer to a location that is convenient
for the generation of simple referring ex-
pressions, and how to generate referring
expressions with context-dependent adjec-
tives. We implement and evaluate our
approach in the framework of the Chal-
lenge on Generating Instructions in Vir-
tual Environments, finding that it performs
well even under the constraints of real-
time generation.
1 Introduction
The problem of situated natural language gen-
eration (NLG)?i.e., of generating natural lan-
guage in the context of a physical (or virtual)
environment?has received increasing attention in
the past few years. On the one hand, this is be-
cause it is the foundation of various emerging ap-
plications, including human-robot interaction and
mobile navigation systems, and is the focus of a
current evaluation effort, the Challenges on Gener-
ating Instructions in Virtual Environments (GIVE;
(Koller et al, 2010b)). On the other hand, situated
generation comes with interesting theoretical chal-
lenges: Compared to the generation of pure text,
the interpretation of expressions in situated com-
munication is sensitive to the non-linguistic con-
text, and this context can change as easily as the
user can move around in the environment.
One interesting aspect of situated communica-
tion from an NLG perspective is that this non-
linguistic context can be manipulated by the
speaker. Consider the following segment of dis-
course between an instruction giver (IG) and an
instruction follower (IF), which is adapted from
the SCARE corpus (Stoia et al, 2008):
(1) IG: Walk forward and then turn right.
IF: (walks and turns)
IG: OK. Now hit the button in the middle.
In this example, the IG plans to refer to an ob-
ject (here, a button); and in order to do so, gives a
navigation instruction to guide the IF to a conve-
nient location at which she can then use a simple
referring expression (RE). That is, there is an inter-
action between navigation instructions (intended
to manipulate the non-linguistic context in a cer-
tain way) and referring expressions (which exploit
the non-linguistic context). Although such subdi-
alogues are common in SCARE, we are not aware
of any previous research that can generate them in
a computationally feasible manner.
This paper presents an approach to generation
which is able to model the effect of an utter-
ance on the non-linguistic context, and to inten-
tionally generate utterances such as the above as
part of a process of referring to objects. Our ap-
proach builds upon the CRISP generation system
(Koller and Stone, 2007), which translates gener-
ation problems into planning problems and solves
these with an AI planner. We extend the CRISP
planning operators with the perlocutionary effects
that uttering a particular word has on the physi-
cal environment if it is understood correctly; more
specifically, on the position and orientation of the
hearer. This allows the planner to predict the non-
linguistic context in which a later part of the ut-
terance will be interpreted, and therefore to search
for contexts that allow the use of simple REs. As a
result, the work of referring to an object gets dis-
tributed over multiple utterances of low cognitive
load rather than a single complex noun phrase.
A second contribution of our paper is the gen-
eration of REs involving context-dependent adjec-
tives: A button can be described as ?the left blue
1573
button? even if there is a red button to its left. We
model adjectives whose interpretation depends on
the nominal phrases they modify, as well as on the
non-linguistic context, by keeping track of the dis-
tractors that remain after uttering a series of mod-
ifiers. Thus, unlike most other RE generation ap-
proaches, we are not restricted to building an RE
by simply intersecting lexically specified sets rep-
resenting the extensions of different attributes, but
can correctly generate expressions whose mean-
ing depends on the context in a number of ways.
In this way we are able to refer to objects earlier
and more flexibly.
We implement and evaluate our approach in
the context of a GIVE NLG system, by using
the GIVE-1 software infrastructure and a GIVE-1
evaluation world. This shows that our system gen-
erates an instruction-giving discourse as in (1) in
about a second. It outperforms a mostly non-
situated baseline significantly, and compares well
against a second baseline based on one of the
top-performing systems of the GIVE-1 Challenge.
Next to the practical usefulness this evaluation es-
tablishes, we argue that our approach to jointly
modeling the grammatical and physical effects of
a communicative action can also inform new mod-
els of the pragmatics of speech acts.
Plan of the paper. We discuss related work in
Section 2, and review the CRISP system, on which
our work is based, in Section 3. We then show
in Section 4 how we extend CRISP to generate
navigation-and-reference discourses as in (1), and
add context-dependent adjectives in Section 5. We
evaluate our system in Section 6; Section 7 con-
cludes and points to future work.
2 Related work
The research reported here can be seen in the
wider context of approaches to generating refer-
ring expressions. Since the foundational work of
Dale and Reiter (1995), there has been a consider-
able amount of literature on this topic. Our work
departs from the mainstream in two ways. First, it
exploits the situated communicative setting to de-
liberately modify the context in which an RE is
generated. Second, unlike most other RE genera-
tion systems, we allow the contribution of a modi-
fier to an RE to depend both on the context and on
the rest of the RE.
We are aware of only one earlier study on gen-
eration of REs with focus on interleaving naviga-
tion and referring (Stoia et al, 2006). In this ma-
chine learning approach, Stoia et al train classi-
fiers that signal when the context conditions (e.g.
visibility of target and distractors) are appropriate
for the generation of an RE. This method can be
then used as part of a content selection component
of an NLG system. Such a component, however,
can only inform a system on whether to choose
navigation over RE generation at a given point of
the discourse, and is not able to help it decide
what kind of navigational instructions to generate
so that subsequent REs become simple.
To our knowledge, the only previous research
on generating REs with context-dependent modi-
fiers is van Deemter?s (2006) algorithm for gener-
ating vague adjectives. Unlike van Deemter, we
integrate the RE generation process tightly with
the syntactic realization, which allows us to gen-
erate REs with more than one context-dependent
modifier and model the effect of their linear or-
der on the meaning of the phrase. In modeling
the context, we focus on the non-linguistic con-
text and the influence of each of the RE?s words;
this is in contrast to previous research on context-
sensitive generation of REs, which mainly focused
on the discourse context (Krahmer and Theune,
2002). Our interpretation of context-dependent
modifiers picks up ideas by Kamp and Partee
(1995) and implements them in a practical system,
while our method of ordering modifiers is linguis-
tically informed by the class-based paradigm (e.g.,
Mitchell (2009)).
On the other hand, our work also stands in a tra-
dition of NLG research that is based on AI plan-
ning. Early approaches (Perrault and Allen, 1980;
Appelt, 1985) provided compelling intuitions for
this connection, but were not computationally vi-
able. The research we report here can be seen
as combining Appelt?s idea of using planning for
sentence-level NLG with a computationally be-
nign variant of Perrault et al?s approach of model-
ing the intended perlocutionary effects of a speech
act as the effects of a planning operator. Our work
is linked to a growing body of very recent work
that applies modern planning research to various
problems in NLG (Steedman and Petrick, 2007;
Brenner and Kruijff-Korbayova?, 2008; Benotti,
2009). It is directly based on Koller and Stone?s
(2007) reimplementation of the SPUD generator
(Stone et al, 2003) with planning. As far as we
know, ours is the first system in the SPUD tradi-
1574
S:self
NP:subj ? 
VP:self
V:self
pushes
NP:obj ? 
semcontent: {push(self,subj,obj)}
John
NP:self
semcontent: {John(self)}
NP:self
the
N:self
button
semcontent: {button(self)}
N:self
red N * 
semcontent: {red(self)}
(a)
S:e
NP:j ? 
VP:e
V:e
pushes
NP:b
1
 ? 
(b)
John
NP:j
NP:b
1
the
N:b
1
button
N:b
1
red N * 
Figure 1: (a) An example grammar; (b) a derivation of ?John pushes the red button? using (a).
tion that explicitly models the context change ef-
fects of an utterance.
While nothing in our work directly hinges on
this, we implemented our approach in the context
of an NLG system for the GIVE Challenge (Koller
et al, 2010b), that is, as an instruction giving sys-
tem for virtual worlds. This makes our system
comparable with other approaches to instruction
giving implemented in the GIVE framework.
3 Sentence generation as planning
Our work is based on the CRISP system (Koller
and Stone, 2007), which encodes sentence gener-
ation with tree-adjoining grammars (TAG; (Joshi
and Schabes, 1997)) as an AI planning problem
and solves that using efficient planners. It then
decodes the resulting plan into a TAG derivation,
from which it can read off a sentence. In this sec-
tion, we briefly recall how this works. For space
reasons, we will present primarily examples in-
stead of definitions.
3.1 TAG sentence generation
The CRISP generation problem (like that of SPUD
(Stone et al, 2003)) assumes a lexicon of entries
consisting of a TAG elementary tree annotated
with semantic and pragmatic information. An ex-
ample is shown in Fig. 1a. In addition to the el-
ementary tree, each lexicon entry specifies its se-
mantic content and possibly a semantic require-
ment, which can express certain presuppositions
triggered by this entry. The nodes in the tree may
be labeled with argument names such as semantic
roles, which specify the participants in the rela-
tion expressed by the lexicon entry; in the exam-
ple, every entry uses the semantic role self repre-
senting the event or individual itself, and the en-
try for ?pushes? furthermore uses subj and obj for
the subject and object argument, respectively. We
combine here for simplicity the entries for ?the?
and ?button? into ?the button?.
For generation, we assume as input a knowl-
edge base and a communicative goal in addition to
the grammar. The goal is to compute a derivation
that expresses the communicative goal in a sen-
tence that is grammatically correct and complete;
whose meaning is justified by the knowledge base;
and in which all REs can be resolved to unique
individuals in the world by the hearer. Let?s say,
for example, that we have a knowledge base
{push(e, j, b1), John(j), button(b1), button(b2),
red(b1)}. Then we can combine instances of the
trees for ?John?, ?pushes?, and ?the button? into
a grammatically complete derivation. However,
because both b1 and b2 satisfy the semantic
content of ?the button?, we must adjoin ?red? into
the derivation to make the RE refer uniquely to
b1. The complete derivation is shown in Fig. 1b;
we can read off the output sentence ?John pushes
the red button? from the leaves of the derived tree
we build in this way.
3.2 TAG generation as planning
In the CRISP system, Koller and Stone (2007)
show how this generation problem can be solved
by converting it into a planning problem (Nau et
al., 2004). The basic idea is to encode the partial
derivation in the planning state, and to encode the
action of adding each elementary tree in the plan-
ning operators. The encoding of our example as a
planning problem is shown in Fig. 2.
In the example, we start with an initial state
which contains the entire knowledge base, plus
atoms subst(S, root) and ref(root, e) expressing
that we want to generate a sentence about the event
e. We can then apply the (instantiated) action
pushes(root, n1, n2, n3, e, j, b1), which models the
act of substituting the elementary tree for ?pushes?
1575
pushes(u, u1, u2, un, x, x1, x2):
Precond: subst(S, u), ref(u, x), push(x, x1, x2),
current(u1), next(u1, u2), next(u2, un)
Effect: ?subst(S, u), subst(NP, u1), subst(NP, u2),
ref(u1, x1), ref(u2, x2), ?y.distractor(u1, y),
?y.distractor(u2, y)
John(u, x):
Precond: subst(NP, u), ref(u, x), John(x)
Effect: ?subst(NP, u), ?y.?John(y) ? ?distractor(u, y)
the-button(u, x):
Precond: subst(NP, u), ref(u, x), button(x)
Effect: ?subst(NP, u), canadjoin(N, u),
?y.?button(y) ? ?distractor(u, y)
red(u, x):
Precond: canadjoin(N, u), ref(u, x), red(x)
Effect: ?y.?red(y) ? ?distractor(u, y)
Figure 2: CRISP planning operators for the ele-
mentary trees in Fig. 1.
into the substitution node root: It can only be
applied because root is an unfilled substitution
node (precondition subst(S, root)), and its effect
is to remove subst(S, root) from the planning state
while adding two new atoms subst(NP, n1) and
subst(NP, n2) for the substitution nodes of the
?pushes? tree. The planning state maintains in-
formation about which individual each node refers
to in the ref atoms. The current and next atoms
are needed to select unused names for newly in-
troduced syntax nodes.1 Finally, the action in-
troduces a number of distractor atoms including
distractor(n2, e) and distractor(n2, b2), express-
ing that the RE at n2 can still be misunderstood
by the hearer as e or b2.
In this new state, all subst and distractor
atoms for n1 can be eliminated with the ac-
tion John(n1, j). We can also apply the action
the-button(n2, b1) to eliminate subst(NP, n2)
and distractor(n2, e), since e is not a button.
However distractor(n2, b2) remains. Now be-
cause the action the-button also introduced the
atom canadjoin(N, n2), we can remove the fi-
nal distractor atom by applying red(n2, b1).
This brings us into a goal state, and we
are done. Goal states in CRISP planning
problems are characterized by axioms such as
?A?u.?subst(A, u) (encoding grammatical com-
pleteness) and ?u?x.?distractor(u, x) (requiring
unique reference).
1This is a different solution to the name-selection problem
than in Koller and Stone (2007). It is simpler and improves
computational efficiency.
1
2
3
4
1 2 3 4
b
1
b
2
b
3f
1
north
Figure 3: An example map for instruction giving.
3.3 Decoding the plan
An AI planner such as FF (Hoffmann and Nebel,
2001) can compute a plan for a planning problem
that consists of the planning operators in Fig. 2
and a specification of the initial state and the goal.
We can then decode this plan into the TAG deriva-
tion shown in Fig. 1b. The basic idea of this
decoding step is that an action with a precondi-
tion subst(A, u) fills the substitution node u, while
an action with a precondition canadjoin(A, u) ad-
joins into a node of category A in the elementary
tree that was substituted into u. CRISP allows
multiple trees to adjoin into the same node. In this
case, the decoder executes the adjunctions in the
order in which they occur in the plan.
4 Context manipulation
We are now ready to describe our NLG ap-
proach, SCRISP (?Situated CRISP?), which ex-
tends CRISP to take the non-linguistic context of
the generated utterance into account, and deliber-
ately manipulate it to simplify RE generation.
As a simplified version of our introductory in-
struction giving example (1), consider the map in
Fig. 3. The instruction follower (IF), who is lo-
cated on the map at position pos3,2 facing north,
sees the scene from the first-person perspective as
in Fig. 7. Now an instruction giver (IG) could in-
struct the IF to press the button b1 in this scene by
saying ?push the button on the wall to your left?.
Interpreting this instruction is difficult for the IF
because it requires her to either memorize the RE
until she has turned to see the button, or to per-
form a mental rotation task to visualize b1 inter-
nally. Alternatively, the IG can first instruct the
IF to ?turn left?; once the IF has done this, the IG
can then simply say ?now push the button in front
1576
S:self
V:self
push
NP:obj ? 
semreq: visible(p, o, obj)
nonlingcon: player?pos(p),
player?ori(o)
impeff: push(obj)
S:self
V:self
turn
Adv
left
nonlingcon: player?ori(o1),
next?ori?left(o1, o2)
nonlingeff: ?player?ori(o1),
player?ori(o2)
impeff: turnleft
S:self
S:self *
S:other ? 
and
Figure 4: An example SCRISP lexicon.
of you?. This lowers the cognitive load on the IF,
and presumably improves the rate of correctly in-
terpreted REs.
SCRISP is capable of deliberately generat-
ing such context-changing navigation instructions.
The key idea of our approach is to extend the
CRISP planning operators with preconditions and
effects that describe the (simulated) physical envi-
ronment: A ?turn left? action, for example, mod-
ifies the IF?s orientation in space and changes the
set of visible objects; a ?push? operator can then
pick up this changed set and restrict the distractors
of the forthcoming RE it introduces (i.e. ?the but-
ton?) to only objects that are visible in the changed
context. We also extend CRISP to generate imper-
ative rather than declarative sentences.
4.1 Situated CRISP
We define a lexicon for SCRISP to be a CRISP
lexicon in which every lexicon entry may also de-
scribe non-linguistic conditions, non-linguistic ef-
fects and imperative effects. Each of these is a
set of atoms over constants, semantic roles, and
possibly some free variables. Non-linguistic con-
ditions specify what must be true in the world
so a particular instance of a lexicon entry can be
uttered felicitously; non-linguistic effects specify
what changes uttering the word brings about in the
world; and imperative effects contribute to the IF?s
?to-do list? (Portner, 2007) by adding the proper-
ties they denote.
A small lexicon for our example is shown in
Fig. 4. This lexicon specifies that saying ?push
X? puts pushing X on the IF?s to-do list, and car-
ries the presupposition that X must be visible from
the location where ?push X? is uttered; this re-
flects our simplifying assumption that the IG can
turnleft(u, x, o1, o2):
Precond: subst(S, u), ref(u, x), player?ori(o1),
next?ori?left(o1, o2), . . .
Effect: ?subst(S, u),?player?ori(o1), player?ori(o2),
to?do(turnleft), . . .
push(u, u1, un, x, x1, p, o):
Precond: subst(S, u), ref(u, x), player?pos(p),
player?ori(o), visible(p, o, x1), . . .
Effect: ?subst(S, u), subst(NP, u1), ref(u1, x1),
?y.(y 6= x1 ? visible(p, o, y) ? distractor(u1, y)),
to?do(push(x1)), canadjoin(S, u), . . .
and(u, u1, un, e1, e2):
Precond: canadjoin(S, u), ref(u, e1), . . .
Effect: subst(S, u1), ref(u1, e2), . . .
Figure 5: SCRISP planning operators for the lexi-
con in Fig. 4.
only refer to objects that are currently visible.
Similarly, ?turn left? puts turning left on the IF?s
agenda. In addition, the lexicon entry for ?turn
left? specifies that, under the assumption that the
IF understands and follows the instruction, they
will turn 90 degrees to the left after hearing it. The
planning operators are written in a way that as-
sumes that the intended (perlocutionary) effects of
an utterance actually come true. This assumption
is crucial in connecting the non-linguistic effects
of one SCRISP action to the non-linguistic pre-
conditions of another, and generalizes to a scalable
model of planning perlocutionary acts. We discuss
this in more detail in Koller et al (2010a).
We then translate a SCRISP generation prob-
lem into a planning problem. In addition to what
CRISP does, we translate all non-linguistic condi-
tions into preconditions and all non-linguistic ef-
fects into effects of the planning operator, adding
any free variables to the operator?s parameters.
An imperative effect P is translated into an ef-
fect to?do(P ). The operators for the example lex-
icon of Fig. 4 are shown in Fig. 5. Finally, we
add information about the situated environment to
the initial state, and specify the planning goal by
adding to?do(P ) atoms for each atom P that is to
be placed on the IF?s agenda.
4.2 An example
Now let?s look at how this generates the appropri-
ate instructions for our example scene of Fig. 3.
We encode the state of the world as depicted
in the map in an initial state which contains,
among others, the atoms player?pos(pos3,2),
player?ori(north), next?ori?left(north,west),
1577
visible(pos3,2,west, b1), etc.
2 We want the IF to
press b1, so we add to?do(push(b1)) to the goal.
We can start by applying the action
turnleft(root, e, north,west) to the initial
state. Next to the ordinary grammatical effects
from CRISP, this action makes player?ori(west)
true. The new state does not contain any subst
atoms, but we can continue the sentence by
adjoining ?and?, i.e. by applying the action
and(root, n1, n2, e, e1). This produces a new
atom subst(S, e1), which satisfies one precon-
dition of push(n1, n2, n3, e1, b1, pos3,2,west).
Because turnleft changed the player orientation,
the visible precondition of push is now satisfied
too (unlike in the initial state, in which b1 was not
visible). Applying the action push now introduces
the need to substitute a noun phrase for the object,
which we can eliminate with an application of
the-button(n2, b1) as in Subsection 3.2.
Since there are no other visible buttons from
pos3,2 facing west, there are no remaining
distractor atoms at this point, and a goal state
has been reached. Together, this four-step plan
decodes into the sentence ?turn left and push
the button?. The final state contains the atoms
to?do(push(b1)) and to?do(turnleft), indicating
that an IF that understands and accepts this in-
struction also accepts these two commitments into
their to-do list.
5 Generating context-dependent
adjectives
Now consider if we wanted to instruct the IF to
press b2 in Fig. 3 instead of b1, say with the
instruction ?push the left button?. This is still
challenging, because (like most other approaches
to RE generation) CRISP interprets adjectives by
simply intersecting all their extensions. In the case
of ?left?, the most reasonable way to do this would
be to interpret it as ?leftmost among all visible ob-
jects?; but this is f1 in the example, and so there is
no distinguishing RE for b2.
In truth, spatial adjectives like ?left? and ?up-
per? depend on the context in two different ways.
On the one hand, they are interpreted with respect
to the current spatio-visual context, in that what is
on the left depends on the current position and ori-
entation of the hearer. On the other hand, they also
2In a more complex situation, it may be infeasible to ex-
haustively model visibility in this way. This could be fixed by
connecting the planner to an external spatial reasoner (Dorn-
hege et al, 2009).
left(u, x):
Precond: ?y.?(distractor(u, y) ? left?of(y, x)),
canadjoin(N, u), ref(u, x)
Effect: ?y.(left?of(x, y) ? ?distractor(u, y)),
premod?index(u, 2), . . .
red(u, x):
Precond: red(x), canadjoin(N, u), ref(u, x),
?premod?index(u, 2)
Effect: ?y.(?red(y) ? ?distractor(u, y)),
premod?index(u, 1), . . .
Figure 6: SCRISP operators for context-
dependent and context-independent adjectives.
depend on the meaning of the phrase they modify:
?the left button? is not necessarily both a button
and further to the left than all other objects, it is
only the leftmost object among the buttons.
We will now show how to extend SCRISP so it
can generate REs that use such context-dependent
adjectives.
5.1 Context-dependence of adjectives in
SCRISP
As a planning-based approach to NLG, SCRISP
is not limited to simply intersecting sets of po-
tential referents that only depend on the attributes
that contribute to an RE: Distractors are removed
by applying operators which may have context-
sensitive conditions depending on the referent and
the distractors that are still left.
Our encoding of context-dependent adjectives
as planning operators is shown in Fig. 6. We only
show the operators here for lack of space; they can
of course be computed automatically from lexicon
entries. In addition to the ordinary CRISP precon-
ditions, the left operator has a precondition requir-
ing that no current distractor for the RE u is to the
left of x, capturing a presupposition of the adjec-
tive. Its effect is that everything that is to the right
of x is no longer a distractor for u. Notice that we
allow that there may still be distractors after left
has been applied (above or below x); we only re-
quire unique reference in the goal state. (Ignore
the premod?index part of the effect for now; we
will get to that in a moment.)
Let?s say that we are computing a plan for re-
ferring to b2 in the example map of Fig. 3, starting
with push(root, n1, n2, e, b2, pos3,1, north) and
the-button(n1, b2). The state after these two ac-
tions is not a goal state, because it still contains
the atom distractor(n1, b3) (the plant f1 was re-
moved as a distractor by the action the-button).
1578
Now assume that we have modeled the spatial
relations between all objects in the initial state
in left?of and above atoms; in particular, we
have left?of(b2, b3). Then the action instance
left(n1, b2) is applicable in this state, as there is
no other object that is still a distractor in this state
and that is to the left of b2. Applying left removes
distractor(n1, b3) from the state. Thus we have
reached a goal state; the complete plan decodes to
the sentence ?push the left button?.
This system is sensitive to the order in which
operators for context-dependent adjectives are ap-
plied. To generate the RE ?the upper left but-
ton?, for instance, we first apply the left action and
then the upper action, and therefore upper only
needs to remove distractors in the leftmost posi-
tion. On the other hand, the RE ?the left upper
button? corresponds to first applying upper and
then left. These action sequences succeed in re-
moving all distractors for different context states,
which is consistent with the difference in meaning
between the two REs.
Furthermore, notice that the adjective operators
themselves do not interact directly with the en-
coding of the context in atoms like visible and
player?pos, just like the noun operators in Sec-
tion 4 didn?t. The REs to which the adjectives and
nouns contribute are introduced by verb operators;
it is these verb operators that inspect the current
context and initialize the distractor set for the new
RE appropriately. This makes the correctness of
the generated sentence independent of the order in
which noun and adjective operators occur in the
plan. We only need to ensure that the verbs are
ordered correctly, and the workload of modeling
interactions with the non-linguistic context is lim-
ited to a single place in the encoding.
5.2 Adjective word order
One final challenge that arises in our system is to
generate the adjectives in the correct order, which
on top of semantically valid must be linguisti-
cally acceptable. In particular, it is known that
some types of adjectives are limited with respect
to the word order in which they can occur in a
noun phrase. For instance, ?large foreign finan-
cial firms? sounds perfectly acceptable, but ?? for-
eign large financial firms? sounds odd (Shaw and
Hatzivassiloglou, 1999). In our setting, some ad-
jective orders are forbidden because only one or-
der produces a correct and distinguishing descrip-
Figure 7: The IF?s view of the scene in Fig. 3, as
rendered by the GIVE client.
tion of the target referent (cf. ?upper left? vs. ?left
upper? example above). However, there are also
other constraints at work: ?? the red left button? is
rather odd even when it is a semantically correct
description, whereas ?the left red button? is fine.
To ensure that SCRISP chooses to generate
these adjectives correctly, we follow a class-based
approach to the premodifier ordering problem
(Mitchell, 2009). In our lexicon we assign adjec-
tives denoting spatial relations (?left?) to one class
and adjectives denoting color (?red?) to another;
then we require that spatial adjectives must always
precede color adjectives. We enforce this by keep-
ing track of the current premodifier index of the RE
in atoms of the form premod?index. Any newly
generated RE node starts off with a premodifier
index of zero; adjoining an adjective of a certain
class then raises this number to the index for that
class. As the operators in Fig. 6 illustrate, color
adjectives such as ?red? have index one and can
only be used while the index is not higher; once
an adjective from a higher class (such as ?left?, of
a class with index two) is used, the premod?index
precondition of the ?red? operator will fail. For
this reason, we can generate a plan for ?the left
red button?, but not for ?? the red left button?, as
desired.
6 Evaluation
To establish the quality of the generated instruc-
tions, we implemented SCRISP as part of a gener-
ation system in the GIVE-1 framework, and eval-
uated it against two baselines. GIVE-1 was the
First Challenge on Generating Instructions in Vir-
tual Environments, which was completed in 2009
1579
SCRISP
1. Turn right and move one step.
2. Push the right red button.
Baseline A
1. Press the right red button on the
wall to your right.
Baseline B
1. Turn right.
2. Walk forward 3 steps.
3. Turn right.
4. Walk forward 1 step.
5. Turn left.
6. Good! Now press the left button.
Table 1: Example system instructions generated in
the same scene. REs for the target are typeset in
boldface.
(Koller et al, 2010b). In this challenge, sys-
tems must generate real-time instructions that help
users perform a task in a treasure-hunt virtual en-
vironment such as the one shown in Fig. 7.
We conducted our evaluation in World 2 from
GIVE-1, which was deliberately designed to be
challenging for RE generation. The world con-
sists of one room filled with several objects and
buttons, most of which cannot be distinguished by
simple descriptions. Moreover, some of those may
activate an alarm and cause the player to lose the
game. The player?s moves and turns are discrete
and the NLG system has complete and accurate
real-time information about the state of the world.
Instructions that each of the three systems under
comparison generated in an example scene of the
evaluation world are presented in Table 1.
The evaluation took place online via the Ama-
zon Mechanical Turk, where we collected 25
games for each system. We focus on four mea-
sures of evaluation: success rates for solving the
task and resolving the generated REs, average
task completion time (in seconds) for successful
games, and average distance (in steps) between the
IF and the referent at the time when the RE was
generated. As in the challenge, the task is consid-
ered as solved if the player has correctly been led
through manipulating all target objects required to
discover and collect the treasure; in World 2, the
minimum number of such targets is eight. An RE
is successfully resolved if it results in the manipu-
lation of the referent, whereas manipulation of an
alarm-triggering distractor ends the game unsuc-
cessfully.
6.1 The SCRISP system
Our system receives as input a plan for what the
IF should do to solve the task, and successively
takes object-manipulating actions as the commu-
success RE
rate time success distance
SCRISP 69% 306 71% 2.49
Baseline A 16%** 230 49%** 1.97*
Baseline B 84% 288 81%* 2.00*
Table 2: Evaluation results. Differences to
SCRISP are significant at *p < .05, **p < .005
(Pearson?s chi-square test for system success rates;
unpaired two-sample t-test for the rest).
nicative goals for SCRISP. Then, for each of the
communicative goals, it generates instructions us-
ing SCRISP, segments them into navigation and
action parts, and presents these to the user as sep-
arate instructions sequentially (see Table 1).
For each instruction, SCRISP thus draws from
a knowledge base of about 1500 facts and a gram-
mar of about 30 lexicon entries. We use the
FF planner (Hoffmann and Nebel, 2001; Koller
and Hoffmann, 2010) to solve the planning prob-
lems. The maximum planning time for any in-
struction is 1.03 seconds on a 3.06 GHz Intel Core
2 Duo CPU. So although our planning-based sys-
tem tackles a very difficult search problem, FF is
very good at solving it?fast enough to generate
instructions in real time.
6.2 Comparison with Baseline A
Baseline A is a very basic system designed to sim-
ulate the performance of a classical RE genera-
tion module which does not attempt to manipu-
late the visual context. We hand-coded a correct
distinguishing RE for each target button in the
world; the only way in which Baseline A reacts
to changes of the context is to describe on which
wall the button is with respect to the user?s current
orientation (e.g. ?Press the right red button on the
wall to your right?).
As Table 2 shows, our system guided 69% of
users to complete the task successfully, compared
to only 16% for Baseline A (difference is statis-
tically significant at p < .005; Pearson?s chi-
square test). This is primarily because only 49%
of the REs generated by Baseline A were success-
ful. This comparison illustrates the importance of
REs that minimize the cognitive load on the IF to
avoid misunderstandings.
6.3 Comparison with Baseline B
Baseline B is a corrected and improved version
of the ?Austin? system (Chen and Karpov, 2009),
1580
one of the best-performing systems of the GIVE-1
Challenge. Baseline B, like the original ?Austin?
system, issues navigation instructions by precom-
puting the shortest path from the IF?s current lo-
cation to the target, and generates REs using the
description logic based algorithm of Areces et al
(2008). Unlike the original system, which inflex-
ibly navigates the user all the way to the target,
Baseline B starts off with navigation, and oppor-
tunistically instructs the IF to push a button once it
has become visible and can be described by a dis-
tinguishing RE. We fixed bugs in the original im-
plementation of the RE generation module, so that
Baseline B generates only unambiguous REs. The
module nonetheless naively treats all adjectives as
intersective and is not sensitive to the context of
their comparison set. Specifically, a button can-
not be referred to as ?the right red button? if it is
not the rightmost of all visible objects?which ex-
plains the long chain of navigational instructions
the system produced in Table 1.
We did not find any significant differences in
the success rates or task completion times between
this system and SCRISP, but the former achieved
a higher RE success rate (see Table 2). However,
a closer analysis shows that SCRISP was able to
generate REs from significantly further away. This
means that SCRISP?s RE generator solves a harder
problem, as it typically has to deal with more vis-
ible distractors. Furthermore, because of the in-
creased distance, the system?s execution monitor-
ing strategies (e.g. for detection and repair of mis-
understandings) become increasingly important,
and this was not a focus of this work. In summary,
then, we take the results to mean that SCRISP per-
forms quite capably in comparison to a top-ranked
GIVE-1 system.
7 Conclusion
In this paper, we have shown how situated instruc-
tions can be generated using AI planning. We ex-
ploited the planner?s ability to model the perlocu-
tionary effects of communicative actions for effi-
cient generation. We showed how this made it pos-
sible to generate instructions that manipulate the
non-linguistic context in convenient ways, and to
generate correct REs with context-dependent ad-
jectives.
We believe that this illustrates the power of
a planning-based approach to NLG to flexibly
model very different phenomena. An interesting
topic for future work, for instance, is to expand our
notion of context by taking visual and discourse
salience into account when generating REs. In ad-
dition, we plan to experiment with assigning costs
to planning operators in a metric planning problem
(Hoffmann, 2002) in order to model the cognitive
cost of an RE (Krahmer et al, 2003) and compute
minimal-cost instruction sequences.
On a more theoretical level, the SCRISP actions
model the physical effects of a correctly under-
stood and grounded instruction directly as effects
of the planning operator. This is computationally
much less complex than classical speech act plan-
ning (Perrault and Allen, 1980), in which the in-
tended physical effect comes at the end of a long
chain of inferences. But our approach is also very
optimistic in estimating the perlocutionary effects
of an instruction, and must be complemented by an
appropriate model of execution monitoring. What
this means for a novel scalable approach to the
pragmatics of speech acts (Koller et al, 2010a)
is, we believe, an interesting avenue for future re-
search.
Acknowledgments. We are grateful to Jo?rg
Hoffmann for improving the efficiency of FF in the
SCRISP domain at a crucial time, and to Margaret
Mitchell, Matthew Stone and Kees van Deemter
for helping us expand our view of the context-
dependent adjective generation problem. We also
thank Ines Rehbein and Josef Ruppenhofer for
testing early implementations of our system, and
Andrew Gargett as well as the reviewers for their
helpful comments.
References
Douglas E. Appelt. 1985. Planning English sentences.
Cambridge University Press, Cambridge, England.
Carlos Areces, Alexander Koller, and Kristina Strieg-
nitz. 2008. Referring expressions as formulas of
description logic. In Proceedings of the 5th Inter-
national Natural Language Generation Conference,
pages 42?49, Salt Fork, Ohio, USA.
Luciana Benotti. 2009. Clarification potential of in-
structions. In Proceedings of the SIGDIAL 2009
Conference, pages 196?205, London, UK.
Michael Brenner and Ivana Kruijff-Korbayova?. 2008.
A continual multiagent planning approach to situ-
ated dialogue. In Proceedings of the 12th Workshop
on the Semantics and Pragmatics of Dialogue, Lon-
don, UK.
1581
David Chen and Igor Karpov. 2009. The
GIVE-1 Austin system. In The First
GIVE Challenge: System descriptions.
http://www.give-challenge.org/
research/files/GIVE-09-Austin.pdf.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the genera-
tion of referring expressions. Cognitive Science, 19.
Christian Dornhege, Patrick Eyerich, Thomas Keller,
Sebastian Tru?g, Michael Brenner, and Bernhard
Nebel. 2009. Semantic attachments for domain-
independent planning systems. In Proceedings of
the 19th International Conference on Automated
Planning and Scheduling, pages 114?121.
Jo?rg Hoffmann and Bernhard Nebel. 2001. The
FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence
Research, 14:253?302.
Jo?rg Hoffmann. 2002. Extending FF to numerical state
variables. In Proceedings of the 15th European Con-
ference on Artificial Intelligence, Lyon, France.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?123. Springer-Verlag, Berlin, Ger-
many.
Hans Kamp and Barbara Partee. 1995. Prototype the-
ory and compositionality. Cognition, 57(2):129 ?
191.
Alexander Koller and Jo?rg Hoffmann. 2010. Waking
up a sleeping rabbit: On natural-language sentence
generation with FF. In Proceedings of the 20th In-
ternational Conference on Automated Planning and
Scheduling, Toronto, Canada.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, Prague, Czech Republic.
Alexander Koller, Andrew Gargett, and Konstantina
Garoufi. 2010a. A scalable model of planning per-
locutionary acts. In Proceedings of the 14th Work-
shop on the Semantics and Pragmatics of Dialogue,
Poznan, Poland.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010b. The First Challenge on
Generating Instructions in Virtual Environments.
In M. Theune and E. Krahmer, editors, Empir-
ical Methods in Natural Language Generation,
volume 5790 of LNCS, pages 337?361. Springer,
Berlin/Heidelberg. To appear.
Emiel Krahmer and Mariet Theune. 2002. Effi-
cient context-sensitive generation of referring ex-
pressions. In Kees van Deemter and Rodger Kibble,
editors, Information Sharing: Reference and Pre-
supposition in Language Generation and Interpre-
tation, pages 223?264. CSLI Publications.
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Margaret Mitchell. 2009. Class-based ordering of
prenominal modifiers. In Proceedings of the 12th
European Workshop on Natural Language Genera-
tion, pages 50?57, Athens, Greece.
Dana Nau, Malik Ghallab, and Paolo Traverso. 2004.
Automated Planning: Theory and Practice. Morgan
Kaufmann.
C. Raymond Perrault and James F. Allen. 1980. A
plan-based analysis of indirect speech acts. Amer-
ican Journal of Computational Linguistics, 6(3?
4):167?182.
Paul Portner. 2007. Imperatives and modals. Natural
Language Semantics, 15(4):351?383.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proceedings of the
37th Annual Meeting of the Association for Compu-
tational Linguistics, pages 135?143, College Park,
Maryland, USA.
Mark Steedman and Ronald P. A. Petrick. 2007. Plan-
ning dialog actions. In Proceedings of the 8th SIG-
dial Workshop on Discourse and Dialogue, pages
265?272, Antwerp, Belgium.
Laura Stoia, Donna K. Byron, Darla Magdalene Shock-
ley, and Eric Fosler-Lussier. 2006. Sentence
planning for realtime navigational instructions. In
NAACL ?06: Proceedings of the Human Language
Technology Conference of the NAACL, pages 157?
160, Morristown, NJ, USA.
Laura Stoia, Darla M. Shockley, Donna K. Byron,
and Eric Fosler-Lussier. 2008. SCARE: A sit-
uated corpus with annotated referring expressions.
In Proceedings of the 6th International Conference
on Language Resources and Evaluation, Marrakech,
Morocco.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with communicative intentions: The SPUD
system. Computational Intelligence, 19(4):311?
381.
Kees van Deemter. 2006. Generating referring ex-
pressions that involve gradable properties. Compu-
tational Linguistics, 32(2).
1582
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 30?39,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Enhancing Referential Success by Tracking Hearer Gaze
Alexander Koller
University of Potsdam
koller@ling.uni-potsdam.de
Konstantina Garoufi
University of Potsdam
garoufi@uni-potsdam.de
Maria Staudte
Saarland University
masta@coli.uni-saarland.de
Matthew Crocker
Saarland University
crocker@coli.uni-saarland.de
Abstract
The ability to monitor the communicative suc-
cess of its utterances and, if necessary, provide
feedback and repair is useful for a dialog sys-
tem. We show that in situated communication,
eyetracking can be used to reliably and effi-
ciently monitor the hearer?s reference resolu-
tion process. An interactive system that draws
on hearer gaze to provide positive or nega-
tive feedback after referring to objects outper-
forms baseline systems on metrics of referen-
tial success and user confusion.
1 Introduction
Because dialog is interactive, interlocutors are con-
stantly engaged in a process of predicting and mon-
itoring the effects of their utterances. Typically, a
speaker produces an utterance with a specific com-
municative goal in mind?e.g., that the hearer will
perform an action or adopt a certain belief?, and
chooses one particular utterance because they pre-
dict that it will achieve this communicative goal.
They will then monitor the hearer?s reactions and
infer from their observations whether the prediction
actually came true. If they recognize that the hearer
misunderstood the utterance, they may repair the
problem by diagnosing what caused the misunder-
standing and giving the hearer feedback. In a task-
oriented dialog in which the hearer must perform a
part of the task, feedback is especially important to
inform the hearer when they made a mistake in the
task. Ideally, the speaker should even detect when
the hearer is about to make a mistake, and use feed-
back to keep them from making the mistake at all.
Many implemented dialog systems include a com-
ponent for monitoring and repair. For instance,
Traum (1994) presents a model for monitoring the
grounding status of utterances in the TRAINS sys-
tem; Young et al (1994) show how the student?s
utterances in a dialog system can be used to un-
cover mistaken assumptions about their mental state;
and Paek and Horvitz (1999) discuss an automated
helpdesk system that can track grounding under un-
certainty. However, most of these systems rely on
the user?s verbal utterances as their primary source
of information; monitoring thus presupposes an
(error-prone) language understanding module.
In the context of situated communication, where
the speaker and hearer share a physical (or virtual)
environment, one type of observation that can poten-
tially give us a very direct handle on the hearer?s un-
derstanding of an utterance is eye gaze. Eyetracking
studies in psycholinguistics have shown that when
listeners hear a referring expression, they tend to
rapidly attend to the object in a scene to which they
resolve this expression (Tanenhaus et al, 1995; Al-
lopenna et al, 1998). For utterances that involve ref-
erences to objects in the current environment, one
can therefore ask whether eyetracking can be used
to reliably judge the communicative success of the
utterance. This would be of practical interest for
implemented dialog systems once eyetracking be-
comes a mainstream technology; and even today, a
system that reliably monitors communicative suc-
cess using eyetracking could serve as a testbed for
exploring monitoring and repair strategies.
In this paper, we present an interactive natural-
language generation (NLG) system that uses eye-
30
tracking to monitor communicative success. Our
system gives real-time instructions that are designed
to help the user perform a treasure-hunt task in the
virtual 3D environments of the recent Challenges
on Generating Instructions in Virtual Environments
(GIVE; Koller et al (2010)). It monitors how the
user resolves referring expressions (REs) by map-
ping the user?s gaze to objects in the virtual environ-
ment. The system takes gaze to the intended referent
as evidence of successful understanding, and gives
the user positive feedback; by contrast, gaze to other
objects triggers negative feedback. Crucially, this
feedback comes before the user interacts with the
object in the virtual environment, keeping the user
from making mistakes before they happen.
We evaluate our system against one baseline that
gives no feedback, and another that bases its feed-
back on monitoring the user?s movements and their
field of view. We find that the eyetracking-based
system outperforms both on referential success, and
that users interacting with it show significantly fewer
signs of confusion about how to complete their task.
This demonstrates that eyetracking can serve as a
reliable source of evidence in monitoring commu-
nicative success. The system is, to our knowledge,
the first dialog or NLG system that uses the hearer?s
gaze to monitor understanding of REs.
Plan of the paper. The paper is structured as fol-
lows. We first discuss related work in Section 2. We
then describe our approach as well as the baselines
in Section 3, set up the evaluation in Section 4 and
present the results in Section 5. In Sections 6 and 7
we discuss our findings and conclude.
2 Related work
Dialog systems model a process of grounding, in
which they decide to what extent the user has under-
stood the utterance and the communicative goal has
been reached. Observing the user behavior to moni-
tor the state of understanding is a key component in
this process. A full solution may require plan recog-
nition or abductive or epistemic reasoning (see e.g.
Young et al (1994), Hirst et al (1994)); in practice,
many systems use more streamlined (Traum, 1994)
or statistical methods (Paek and Horvitz, 1999).
Most dialog systems focus on the verbal interaction
of the system and user, and the user?s utterances are
therefore the primary source of evidence in the mon-
itoring process. Some incremental dialog systems
can monitor the user?s verbal reactions to the sys-
tem?s utterances in real time, and continuously up-
date the grounding state while the system utterance
is still in progress (Skantze and Schlangen, 2009;
Buss and Schlangen, 2010).
In this paper, we focus on the generation side of a
dialog system?the user is the hearer?and on mon-
itoring the user?s extralinguistic reactions, in par-
ticular their gaze. Tanenhaus et al (1995) and Al-
lopenna et al (1998) showed that subjects in psy-
cholinguistic experiments who hear an RE visually
attend to the object to which they resolve the RE.
The ?visual world? experimental paradigm exploits
this by presenting objects on a computer screen and
using an eyetracker to monitor the subject?s gaze.
This research uses gaze only as an experimental tool
and not as part of an interactive dialog system, and
the visual worlds are usually limited to static 2D
scenes. Also, such setups cannot account for the re-
ciprocal nature of dialog and the consequences that
hearer gaze has for the speaker?s monitoring process.
In the context of situated dialog systems, previ-
ous studies have employed robots and virtual agents
as speakers to explore how and when speaker gaze
helps human hearers to ground referring expressions
(Foster, 2007). For instance, Staudte and Crocker
(2011) show that an agent can make it easier for the
(human) hearer to resolve a system-generated RE by
looking at the intended referent, using head and eye
movements. Conversely, the performance of a sys-
tem for resolving human-produced REs can be im-
proved by taking the (human) speaker?s gaze into ac-
count (Iida et al, 2011). Gaze has also been used to
track the general dynamics of a dialog, such as turn
taking (Jokinen et al, in press).
Here we are interested in monitoring the hearer?s
gaze in order to determine whether they have under-
stood an RE. To our knowledge, there has been no
research on this; in particular, not in dynamic 3D
environments. The closest earlier work of which we
are aware comes from the context of the GIVE Chal-
lenge, a shared task for interactive, situated natural
language generation systems. These systems typi-
cally approximate hearer gaze as visibility of objects
on the screen and monitor grounding based on this
(Denis, 2010; Racca et al, 2011).
31
Figure 1: A first-person view of a virtual 3D environment.
3 Interactive natural-language generation
in virtual environments
In this paper, we consider the communicative situ-
ation of the GIVE Challenge (Koller et al, 2010;
Striegnitz et al, 2011). In this task, a human user can
move about freely in a virtual indoor environment
featuring several interconnected rooms and corri-
dors. A 3D view of the environment is displayed on
a computer screen as in Fig. 1, and the user can walk
forward/backward and turn left/right, using the cur-
sor keys. They can also press buttons attached to the
walls, by clicking on them with the mouse once they
are close enough. The small and big white circles in
Fig. 1, which represent eyetracking information, are
not actually visible to the user.
The user interacts with a real-time NLG system in
the context of a treasure-hunt game, where their task
is to find a trophy hidden in a wall safe. They must
press certain buttons in the correct sequence in or-
der to open the safe; however, they do not have prior
knowledge of which buttons to press, so they rely
on instructions and REs generated by the system. A
room may contain several buttons other than the tar-
get, which is the button that the user must press next.
These other buttons are called distractors. Next to
buttons, rooms also contain a number of landmark
objects, such as chairs and plants, which cannot di-
rectly be interacted with, but may be used in REs
to nearby targets. Fig. 2 shows a top-down map of
the virtual environment in which the scene of Fig. 1
arose. We call an entire game up to the successful
discovery of the trophy, an interaction of the system
and the user.
Figure 2: A map of the environment in Fig. 1; note the
user in the upper right room.
3.1 Monitoring communicative success
NLG systems in the GIVE setting are in an interac-
tive communicative situation. This situation repre-
sents one complete half of a dialog situation: Only
the system gets to use language, but the user moves
and acts in response to the system?s utterances. As a
result, the system should continuously monitor and
react to what the user does, in real time. This is
most tangible in the system?s use of REs. When a
user misinterprets (or simply does not understand)
a system-generated RE, there is a high chance that
they will end up pressing the wrong button. This
will hinder the completion of the task. A system
that predicts how the user resolves the RE by mon-
itoring their movements and actions, and that can
proactively give the user feedback to keep them from
making a mistake, will therefore perform better than
one which cannot do this. Furthermore, if the sys-
tem can give positive feedback when it detects that
the user is about to do the right thing, this may in-
crease the user?s confidence.
Monitoring communicative success in GIVE in-
teractions and providing the right feedback can be
challenging. For example, in the original interaction
from which we took the screenshot of Fig. 1, the sys-
tem instructed the user to ?push the right button to
the right of the green button?, referring to the right-
most blue button in the scene. In response, the user
first walked hesitantly towards the far pair of buttons
(green and blue), and then turned to face the other
pair, as seen in Fig. 3. A typical NLG system used
32
Figure 3: The scene of Fig. 1, after the user moved and
turned in response to a referring expression.
in the GIVE Challenge (e.g., Dionne et al (2009),
Denis (2010), Racca et al (2011)) may try to predict
how the user might resolve the RE based on the vis-
ibility of objects, timing data, or distances. Relying
only on such data, however, even a human observer
could have difficulties in interpreting the user?s reac-
tion; the user in Fig. 3 ended up closer to the green
and blue buttons, but the other buttons (the two blue
ones) are, to similar degrees, visually in focus.
The contribution of this paper is to present a
method for monitoring the communicative success
of an RE based on eyetracking. We start from the
hypothesis that when the user resolves an RE to a
certain object, they will tend to gaze at this object.
In the scene of Fig. 3, the user was indeed looking
at the system?s intended referent, which they later
pressed; the small white circles indicate a trace of re-
cent fixations on the screen, and the big white circle
marks the object in the virtual environment to which
the system resolved these screen positions. Our sys-
tem takes this gaze information, which is available in
real time, as evidence for how the user has resolved
its RE, and generates positive or negative feedback
based on this.
3.2 NLG systems
To demonstrate the usefulness of the eyetracking-
based approach, we implemented and compared
three different NLG systems. All of these use
an identical module for generating navigation in-
structions, which guides the user to a specific lo-
cation, as well as object manipulation instructions
such as ?push the blue button?; ?the blue button?
is an RE that describes an object to the user. The
systems generate REs that are optimized for being
easy for the hearer to understand, according to a
corpus-based model of understandability (Garoufi
and Koller, 2011). The model was trained on human
instructions produced in a subset of the virtual envi-
ronments we use in this work. The resulting system
computes referring expressions that are correct and
uniquely describe the referent as seen by the hearer
at the moment in which generation starts.
Unlike in the original GIVE Challenge, the gen-
erated instructions are converted to speech by the
Mary text-to-speech system (Schro?der and Trouvain,
2003) and presented via loudspeaker. At any point,
the user may press the ?H? key on their keyboard to
indicate that they are confused and request a clari-
fication. This will cause the system to generate an
instruction newly; if it contains an RE, this RE may
or may not be the same as the one used in the origi-
nal utterance.
The difference between the three systems is in the
way they monitor communicative success and deter-
mine when to give feedback to the user.
The no-feedback system. As a baseline system,
we used a system which does not monitor success
at all, and therefore never gives feedback on its own
initiative. Notice that the system still re-generates an
RE when the user presses the ?H? key.
Movement-based monitoring. As a second base-
line, we implemented a system that attempts to mon-
itor whether a user understood an RE based on their
movements. This system is intended to represent
the user monitoring that can be implemented, with
a reasonable amount of effort, on the basis of imme-
diately available information in the GIVE setting.
The movement-based system gives no feedback
until only a single button in the current room is vis-
ible to the user, since it can be hard to make a re-
liable prediction if the user sees several buttons on
their screen. Then it tracks the user?s distance from
this button, where ?distance? is a weighted sum of
walking distance to the button and the angle the user
must turn to face the button. If, after hearing the RE,
the user has decreased the distance by more than a
given threshold, the system concludes that the hearer
has resolved the RE as this button. If that is the but-
ton the system intended to refer to, the system utters
33
the positive feedback ?yes, that one?. For incorrect
buttons, it utters the negative feedback ?no, not that
one?. Although the negative feedback is relatively
vague, it has the advantage of limiting the variability
of the system?s outputs, which facilitates evaluation.
Eyetracking-based monitoring. Finally, the
eyetracking-based system attempts to predict
whether the user will press the correct button
or not by monitoring their gaze. At intervals of
approximately 15 ms, the system determines the
(x,y) position on the screen that the user is looking
at. It then identifies the object in the environment
that corresponds to this position by casting a ray
from the (virtual) camera through the screen plane,
and picking the closest object lying within a small
range of this ray (Fig. 1; see Staudte et al (2012) for
details). If the user continously looks at the same
object for more than a certain amount of time, the
system counts this as an inspection of the object; for
our experiments, we chose a threshold of 300 ms.
Once the system detects an inspection to a button in
the room, it generates positive or negative feedback
utterances in exactly the same way as the movement
system does.
Both the movement-based and the eyetracking-
based model withhold their feedback until a first
full description of the referent (a first-mention RE)
has been spoken. Additionally, they only provide
feedback once for every newly approached or in-
spected button and will not repeat this feedback un-
less the user has approached or inspected another
button in the meantime. Example interactions of a
user with each of the three systems are presented in
Appendix A.
4 Evaluation
We set up a human evaluation study in order to as-
sess the performance of the eyetracking system as
compared against the two baselines on the situated
instruction giving task. For this, we record partic-
ipant interactions with the three systems employed
in three different virtual environments. These en-
vironments were taken from Gargett et al (2010);
they vary as to the visual and spatial properties of
the objects they contain. One of these environments
is shown in Fig. 2. Overall, 31 participants (12 fe-
males) were tested. All reported their English skills
as fluent, and all were capable of completing the
tasks. Their mean age was 27.6 years.
4.1 Task and procedure
A faceLAB eyetracking system (http://www.
seeingmachines.com/product/facelab)
remotely monitored participants? eye movements on
a 24-inch monitor, as in Fig. 4 and 5 of Appendix B.
Before the experiment, participants received written
instructions that described the task and explained
that they would be given instructions by an NLG
system. They were encouraged to request additional
help any time they felt that the instructions were not
sufficient (by pressing the ?H? key).
The eyetracker was calibrated using a nine-point
fixation stimulus. We disguised the importance of
gaze from the participants by telling them that we
videotaped them and that the camera needed calibra-
tion. Each participant started with a short practice
session to familiarize themselves with the interface
and to clarify remaining questions. We then col-
lected three complete interactions, each with a dif-
ferent virtual environment and NLG system (alter-
nated according to a Latin square design). Finally,
each participant received a questionnaire which was
aimed to reveal whether they noticed that they were
eyetracked and that one of the generation systems
made use of that, and how satisfied they were with
this interaction. The entire experiment lasted ap-
proximately 30 minutes.
4.2 Analysis
For the assessment of communicative success in
these interactions, we considered as referential
scenes the parts of the interaction between the onset
of a first-mention RE to a given referent and the par-
ticipant?s reaction (pressing a button or navigating
away to another room). To control for external fac-
tors that could have an impact on this, we discarded
individual scenes in which the systems rephrased
their first-mention REs (e.g. by adding further at-
tributes), as well as a few scenes which the partic-
ipants had to go through a second time due to tech-
nical glitches. To remove errors in eyetracker cali-
bration, we included interactions with the eyetrack-
ing NLG system in the analysis only when we were
able to record inspections (to the referent or any dis-
tractor) in at least 80% of all referential scenes. This
34
success success w/out confusion #scenes
system all easy hard all easy hard all easy hard
eyetracking 93.4 100.0 90.4 91.9 100.0 88.2 198 62 136
with feedback 94.3 100.0 91.7 92.8 100.0 89.4 194 62 132
without feedback 50.0 - 50.0 50.0 - 50.0 4 0 4
no-feedback 86.6* 100.0? 80.6* 83.5** 98.9? 76.5** 284 88 196
movement 89.8? 100.0? 85.2? 87.5? 97.8? 82.8? 295 92 203
with feedback 93.9 100.0 90.6 91.9 97.7 88.7 247 88 159
without feedback 68.8 100.0 65.9 64.6 100.0 61.4 48 4 44
Table 1: Mean referential success rate (%) and number of scenes for the systems, broken down by scene complexity
and presence of feedback. Differences of overall system performances to the eyetracking system are: significant at
** p < 0.01, * p < 0.05; ? not significant.
filtered out 9 interactions out of the 93 we collected.
Inferential statistics on this data were carried out
using mixed-effect models from the lme4 package
in R (Baayen et al, 2008). Specifically, we used
logistic regression for modeling binary data, Poisson
regression for count variables and linear regression
for continuous data.
5 Results
On evaluating the post-task questionnaires, we did
not find any significant preferences for a particular
NLG system. Roughly the same number of them
chose each of the systems on questions such as
?which system did you prefer??. When asked for
differences between the systems in free-form ques-
tions, no participant mentioned the system?s reaction
to their eye gaze?though some noticed the (lack of)
feedback. We take this to mean that the participants
did not realize they were being eyetracked.
Below, we report results on objective metrics that
do not depend on participants? judgments.
5.1 Confusion
A key goal of any RE generation system is that
the user understands the REs easily. One measure
of the ease of understanding is the frequency with
which participants pressed the ?H? key to indicate
their confusion and ask for help. The overall average
of ?H? keystrokes per interaction was 1.14 for the
eyetracking-based system, 1.77 for the movement-
based system, and 2.26 for the no-feedback system.
A model fitted to the keystroke distribution per sys-
tem shows significant differences both between the
eyetracking and the no-feedback system (Coeff. =
0.703, SE = 0.233, Wald?s Z = 3.012, p < .01) and
between the eyetracking and the movement-based
system (Coeff. = 0.475, SE = 0.241, Wald?s Z =
1.967, p < .05). In other words, the feedback
given by the eyetracking-based system significantly
reduces user confusion.
5.2 Referential success
An even more direct way to measure the interac-
tion quality is the ratio of generated REs that the
participants were able to resolve correctly. In our
evaluation, we looked at two different definitions
of success. First, an RE can count as success-
ful if the first button that the user pressed after
hearing the RE was the system?s intended referent.
The results of this evaluation are shown in the left-
most part of Table 1, under ?success?. A logis-
tic mixed-effects model fitted to the referential suc-
cess data revealed a marginal main effect of sys-
tem (?2(2) = 5.55, p = .062). Pairwise com-
parisons further show that the eyetracking system
performs significantly better than the no-feedback
system (Coeff. = ?0.765, SE = 0.342, Wald?s Z =
?2.24, p < .05); no significant difference was found
between the eyetracking-based and the movement-
based system.
Second, we can additionally require that an RE
only counts as successful if the user did not press
the ?H? key between hearing the first-mention RE
and pressing the correct button. This is a stricter
version of referential success, which requires that
the system recognized cases of potential confusion
35
and did not force the user to take the initiative in
case of difficulties. It is in line with Dethlefs et al?s
(2010) findings that metrics that penalize difficul-
ties the user encountered before successfully com-
pleting the task are better predictors of user satisfac-
tion than ones that only consider the eventual task
completion. Our results on this metric are shown
in the middle part of Table 1, under ?success with-
out confusion?. We observe again a main effect of
system (?2(2) = 7.78, p < .05); furthermore, the
eyetracking system elicited again more correct but-
tons than the no-feedback system (Coeff. = ?0.813,
SE = 0.306, Wald?s Z = ?2.66, p < 0.01).
To obtain a more detailed view of when and to
what extent the systems? behavior differed, we dis-
tinguished scenes according to their complexity. A
scene was classified as easy if a) there were no dis-
tractors in it, or b) all distractors had different colors
from the target, while the system included the color
attribute in its RE. All other scenes were considered
hard. Note that ?easy? and ?hard? are properties of
the scene and not of the system, because every sys-
tem generated the same REs in each scene.
In the experiments, we found essentially no differ-
ence between the success rates of different systems
on easy scenes (see the ?easy? columns of Table 1):
All systems were almost always successful. The
differences came almost exclusively from the hard
scenes, where the eyetracking system performed sig-
nificantly better than the no-feedback system (suc-
cess: Coeff. = ?0.793, SE = 0.348, Wald?s Z =
?2.28, p < 0.05; success without confusion: Coeff.
= ?0.833, SE = 0.315, Wald?s Z = ?2.64, p < 0.01)
and, at least numerically, also much better than the
movement system.
There was a particularly interesting difference in
the feedback behavior of the eyetracking and move-
ment systems on hard scenes (see the rightmost part
of Table 1, labeled ?#scenes?). In easy scenes,
both systems almost always gave feedback (62/62
= 100.0%; 88/92 = 95.6%); but for hard scenes,
the ratio of scenes in which the movement system
gave feedback at all dropped to 159/203 = 78.3%,
whereas the ratio for the eyetracking system re-
mained high. This may have contributed to the over-
all performance difference between the two systems.
#actions distance duration idle
system (norm.) (norm.) (norm.) (sec)
eyetracking 1.06 1.22 1.49 256.6
no-feedback 1.22* 1.27 1.59 272.5
movement 1.16 1.26 1.56 274.4
Table 2: Mean values of additional metrics. Differences
to the eyetracking system are significant at * p < 0.05.
5.3 Further performance metrics
Finally, we measured a number of other objective
metrics, including the number of actions (i.e., but-
ton presses), the distance the user traveled, the to-
tal duration of the interaction, and the mean time
a participant spent idle. Even though these mea-
sures only partly provide statistically significant re-
sults, they help to draw a clearer picture of how the
eyetracking-based feedback affects performance.
Because the three virtual environments were of
different complexity, we normalized the number of
actions, distance, and duration by dividing the value
for a given interaction by the minimum value for all
interactions of the same virtual environment. The re-
sulting measures are shown in Table 2. Participants
performed significantly fewer actions in the eye-
tracking system than in the no-feedback system (Co-
eff. = 0.174, SE = 0.067, t = 2.57, p(mcmc) < .05);
there were also trends that users of the eyetracking-
based system traveled the shortest distance, needed
the least overall time, and spent the least time idle.
The only measure deviating from this trend is
movement speed, i.e., the speed at which users re-
acted to the systems? instructions to press certain
buttons. For all successful scenes (without confu-
sion), we computed the speed by dividing the GIVE
distance (including turning distance) between the
target referent and the user?s location at the time of
the instruction containing the first-mention RE by
the time (in seconds) between hearing the instruc-
tion and pressing the target. The mean movement
speed is 0.518 for the no-feedback system, 0.493 for
the movement system, and 0.472 for the eyetracking
system. A marginal main effect of movement speed
confirms this trend (?2(2) = 5.58, p = .061) and
shows that participants moved more slowly when
getting eyetracking-based feedback than when get-
ting no feedback at all (Coeff. = 0.0352, SE =
36
0.0166, t = ?4.97, p(mcmc) < .05).
6 Discussion
The results in Section 5 demonstrate the usefulness
of eyetracking as a foundation for monitoring and
feedback. Compared to the no-feedback system, the
eyetracking-based system achieved a significantly
lower confusion rate and a significantly higher RE
success rate, especially on hard instances. The dif-
ference increases further if we discount scenes in
which the user had to ask for help, thus forcing the
system to give feedback anyway. In other words,
eyetracking provides reliable and direct access to the
hearer?s reference resolution process. Real-time di-
alog systems can use gaze information to monitor
the success of REs and generate feedback before the
user actually makes a mistake.
Monitoring and feedback could also be achieved
without using eyetracking. To explore this alterna-
tive, we compared eyetracking against a movement-
based system. We found that the former outper-
formed the latter on hearer confusion and (at least
numerically) on referential success, while not per-
forming worse on other measures. This means that
the improvement comes not merely from the fact
that feedback was given; it is also important when
and where feedback is given. The crucial weakness
of the movement-based system is that it gave feed-
back for hard instances much more rarely than the
eyetracking system. Increasing recall by lowering
the system?s confidence threshold would introduce
fresh errors. Further improvements must therefore
come at the cost of a more complex monitoring sys-
tem, both conceptually and in terms of implementa-
tion effort. From this perspective, eyetracking offers
good performance at low implementation cost.
One result that seems to go against the trend is that
users of the eyetracking system moved significantly
more slowly on their way to a target. We see two
possible explanations for this. First, it may be that
users needed some time to listen to the feedback, or
were encouraged by it to look at more objects. A
second explanation is that this is not really a differ-
ence in the quality of the systems? behavior, but a
difference in the populations over which the mean
speed was computed: The speed was only averaged
over scenes in which the users resolved the RE cor-
rectly, and the eyetracking system achieved commu-
nicative success in many cases in which the others
did not?presumably complex scenes in which the
user had to work harder to find the correct button.
This issue bears more careful analysis.
Finally, the eyetracking-based system could be
improved further in many ways. On the one hand,
it suffers from the fact that all objects in the 3D en-
vironment shift on the screen when the user turns
or moves. The user?s eyes will typically follow the
object they are currently inspecting, but lag behind
until the screen comes to a stop again. One topic
for future work would be to remove noise of this
kind from the eyetracker signal. On the other hand,
the negative feedback our system gave (?no, not that
one?) was quite unspecific. More specific feedback
(?no, the BLUE button?) might further improve the
system?s performance.
7 Conclusion
We described an interactive NLG system that uses
eyetracking to monitor the communicative success
of the REs it generates. The communication is sit-
uated in a virtual 3D environment in which the user
can move freely, and our system automatically maps
eyetracking screen coordinates to objects in the en-
vironment. A task-based evaluation found that the
eyetracking-based system outperforms both a no-
feedback system and a system whose feedback is
based on the user?s movements in the virtual envi-
ronment, along with their field of view.
Eyetracking is currently widely available in re-
search institutions, which should make our system
easy to reimplement in other situated domains. We
anticipate that eyetracking may become mainstream
technology in the not-too-distant future. But even
in a purely research context, we believe that the di-
rectness with which eyetracking allows us to observe
the hearer?s interpretation process may be useful as
a testbed for efficient theories of grounding.
Acknowledgments. This research was partly sup-
ported by the Cluster of Excellence ?Multimodal
Computing and Interaction? at Saarland University.
We are grateful to Irena Dotcheva for help with
data collection as well as to Alexandre Denis and
Christoph Clodo for software support, and to Kristi-
ina Jokinen for helpful comments.
37
Figure 4: A screenshot from the faceLAB software, including visualization of eye-gaze position in 3D space.
References
Paul Allopenna, James Magnuson, and Michael Tanen-
haus. 1998. Tracking the Time Course of Spoken
Word Recognition Using Eye Movements: Evidence
for Continuous Mapping Models. Journal of Memory
and Language, 38:419?439.
R.H. Baayen, D.J. Davidson, and D.M. Bates. 2008.
Mixed-effects modeling with crossed random effects
for subjects and items. Journal of Memory and Lan-
guage, 59:390?412.
Okko Buss and David Schlangen. 2010. Modelling
sub-utterance phenomena in spoken dialogue systems.
In Aspects of Semantics and Pragmatics of Dialogue.
SemDial 2010, 14th Workshop on the Semantics and
Pragmatics of Dialogue, pages 33?41.
Alexandre Denis. 2010. Generating referring expres-
sions with reference domain theory. In Proceedings
of the 6th International Natural Language Generation
Conference.
Nina Dethlefs, Heriberto Cuayahuitl, Kai-Florian
Richter, Elena Andonova, and John Bateman. 2010.
Evaluating task success in a dialogue system for
indoor navigation. In Aspects of Semantics and Prag-
matics of Dialogue. SemDial 2010, 14th Workshop
on the Semantics and Pragmatics of Dialogue, pages
143?146.
Daniel Dionne, Salvador de la Puente, Carlos Leo?n,
Pablo Gerva?s, and Raquel Herva?s. 2009. A model
for human readable instruction generation using level-
based discourse planning and dynamic inference of at-
tributes. In Proceedings of the 12th European Work-
shop on Natural Language Generation.
Mary Ellen Foster. 2007. Enhancing human-computer
interaction with embodied conversational agents. In
Proceedings of HCI International 2007.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 Corpus of
Giving Instructions in Virtual Environments. In Pro-
ceedings of the 7th Conference on International Lan-
guage Resources and Evaluation.
Konstantina Garoufi and Alexander Koller. 2011. The
Potsdam NLG systems at the GIVE-2.5 Challenge. In
Proceedings of the Generation Challenges Session at
the 13th European Workshop on Natural Language
Generation.
Graeme Hirst, Susan McRoy, Peter Heeman, Philip Ed-
monds, and Diane Horton. 1994. Repairing conver-
sational misunderstandings and non-understandings.
Speech Communications, 15:213?229.
Ryu Iida, Masaaki Yasuhara, and Takenobu Tokunaga.
2011. Multi-modal reference resolution in situated
dialogue by integrating linguistic and extra-linguistic
clues. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing.
K. Jokinen, H. Furukawa, M. Nishida, and S. Yamamoto.
in press. Gaze and turn-taking behaviour in casual
conversational interactions. ACM Trans. Interactive
Intelligent Systems. Special Issue on Eye Gaze in In-
telligent Human-Machine Interaction.
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
38
Oberlander. 2010. The First Challenge on Generating
Instructions in Virtual Environments. In Emiel Krah-
mer and Mariet Theune, editors, Empirical Methods in
Natural Language Generation, number 5790 in LNCS,
pages 337?361. Springer.
Tim Paek and Eric Horvitz. 1999. Uncertainty, utility,
and misunderstanding: A decision-theoretic perspec-
tive on grounding in conversational systems. In AAAI
Fall Symposium on Psychological Models of Commu-
nication in Collaborative Systems.
David Nicola?s Racca, Luciana Benotti, and Pablo
Duboue. 2011. The GIVE-2.5 C Generation System.
In Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Language
Generation.
Marc Schro?der and J. Trouvain. 2003. The German
Text-to-Speech Synthesis System MARY: A Tool for
Research, Development and Teaching. International
Journal of Speech Technology, 6:365?377.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Maria Staudte and Matthew W. Crocker. 2011. Inves-
tigating joint attention mechanisms through human-
robot interaction. Cognition, 120(2):268?291.
Maria Staudte, Alexander Koller, Konstantina Garoufi,
and Matthew W. Crocker. 2012. Using listener gaze
to augment speech generation in a virtual 3D environ-
ment. In Proceedings of the 34th Annual Conference
of the Cognitive Science Society. To appear.
Kristina Striegnitz, Alexandre Denis, Andrew Gargett,
Konstantina Garoufi, Alexander Koller, and Mariet
Theune. 2011. Report on the Second Second Chal-
lenge on Generating Instructions in Virtual Environ-
ments (GIVE-2.5). In Proceedings of the Generation
Challenges Session at the 13th European Workshop on
Natural Language Generation.
Michael K. Tanenhaus, Michael J. Spivey-Knowlton,
Kathleen M. Eberhard, and Julie C. Sedivy. 1995. In-
tegration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
David Traum. 1994. A computational theory of ground-
ing in natural language conversation. Ph.D. thesis,
University of Rochester.
Michael Young, Johanna Moore, and Martha Pollack.
1994. Towards a principled representation for dis-
course plans. In Proceedings of the Sixteenth Annual
Meeting of the Cognitive Science Society.
A Example interactions
The following interactions between a user (U) and
each of the three systems (S) were recorded during
the systems? attempts to instruct the user to press the
rightmost blue button shown in Fig. 1.
A.1 Eyetracking system
(1) S: Push the right button to the right of the green
button.
U: (approaches the pair of blue and green but-
ton and inspects one of them)
S: No, not that one!
. . . (U inspects other buttons in the scene, while
S provides appropriate feedback)
U: (inspects the correct target)
S: Yes, that one!
U: (presses the correct button)
A.2 Movement system
(2) S: Push the right button to the right of the green
button.
U: (approaches the pair of blue and green but-
tons; once the user is very close to the blue but-
ton, it happens to become the only button visi-
ble on screen)
U: (continues moving closer to the blue button)
S: No, not that one!
U: (has no time to react to the system?s feed-
back and presses the wrong blue button)
A.3 No-feedback system
(3) S: Push the right button to the right of the green
button.
U: (presses the wrong blue button)
B The experimental setup
Figure 5: A faceLAB eyetracking system monitored par-
ticipants? eye movements during the interactions.
39
