Adaptivity in Question Answering
with User Modelling and a Dialogue Interface
Silvia Quarteroni and Suresh Manandhar
Department of Computer Science
University of York
York YO10 5DD
UK
{silvia,suresh}@cs.york.ac.uk
Abstract
Most question answering (QA) and infor-
mation retrieval (IR) systems are insensi-
tive to different users? needs and prefer-
ences, and also to the existence of multi-
ple, complex or controversial answers. We
introduce adaptivity in QA and IR by cre-
ating a hybrid system based on a dialogue
interface and a user model. Keywords:
question answering, information retrieval,
user modelling, dialogue interfaces.
1 Introduction
While standard information retrieval (IR) systems
present the results of a query in the form of a
ranked list of relevant documents, question an-
swering (QA) systems attempt to return them in
the form of sentences (or paragraphs, or phrases),
responding more precisely to the user?s request.
However, in most state-of-the-art QA systems
the output remains independent of the questioner?s
characteristics, goals and needs. In other words,
there is a lack of user modelling: a 10-year-old and
a University History student would get the same
answer to the question: ?When did the Middle
Ages begin??. Secondly, most of the effort of cur-
rent QA is on factoid questions, i.e. questions con-
cerning people, dates, etc., which can generally be
answered by a short sentence or phrase (Kwok et
al., 2001). The main QA evaluation campaign,
TREC-QA 1, has long focused on this type of
questions, for which the simplifying assumption is
that there exists only one correct answer. Even re-
cent TREC campaigns (Voorhees, 2003; Voorhees,
2004) do not move sufficiently beyond the factoid
approach. They account for two types of non-
factoid questions ?list and definitional? but not for
non-factoid answers. In fact, a) TREC defines list
questions as questions requiring multiple factoid
1http://trec.nist.gov
answers, b) it is clear that a definition question
may be answered by spotting definitional passages
(what is not clear is how to spot them). However,
accounting for the fact that some simple questions
may have complex or controversial answers (e.g.
?What were the causes of World War II??) remains
an unsolved problem. We argue that in such situa-
tions returning a short paragraph or text snippet is
more appropriate than exact answer spotting. Fi-
nally, QA systems rarely interact with the user:
the typical session involves the user submitting a
query and the system returning a result; the session
is then concluded.
To respond to these deficiencies of existing QA
systems, we propose an adaptive system where a
QA module interacts with a user model and a di-
alogue interface (see Figure 1). The dialogue in-
terface provides the query terms to the QA mod-
ule, and the user model (UM) provides criteria
to adapt query results to the user?s needs. Given
such information, the goal of the QA module is to
be able to discriminate between simple/factoid an-
swers and more complex answers, presenting them
in a TREC-style manner in the first case and more
appropriately in the second.
DIALOGUE
INTERFACE
QUESTION
PROCESSING
DOCUMENT
RETRIEVAL
ANSWER
EXTRACTION
USER
MODEL
Question
Answer
QA MODULE
Figure 1: High level system architecture
Related work To our knowledge, our system is
among the first to address the need for a different
approach to non-factoid (complex/controversial)
199
answers. Although the three-tiered structure of
our QA module reflects that of a typical web-
based QA system, e.g. MULDER (Kwok et al,
2001), a significant aspect of novelty in our archi-
tecture is that the QA component is supported by
the user model. Additionally, we drastically re-
duce the amount of linguistic processing applied
during question processing and answer generation,
while giving more relief to the post-retrieval phase
and to the role of the UM.
2 User model
Depending on the application of interest, the UM
can be designed to suit the information needs of
the QA module in different ways. As our current
application, YourQA2, is a learning-oriented, web-
based system, our UM consists of the user?s:
1) age range, a ? {7 ? 11, 11 ? 16, adult};
2) reading level, r ? {poor,medium, good};
3) webpages of interest/bookmarks, w.
Analogies can be found with the SeAn (Ardissono
et al, 2001) and SiteIF (Magnini and Strapparava,
2001) news recommender systems where age and
browsing history, respectively, are part of the UM.
In this paper we focus on how to filter and adapt
search results using the reading level parameter.
3 Dialogue interface
The dialogue component will interact with both
the UM and the QA module. From a UM point of
view, the dialogue history will store previous con-
versations useful to construct and update a model
of the user?s interests, goals and level of under-
standing. From a QA point of view, the main goal
of the dialogue component is to provide users with
a friendly interface to build their requests. A typi-
cal scenario would start this way:
? System: Hi, how can I help you?
? User: I would like to know what books Roald Dahl wrote.
The query sentence ?what books Roald Dahl wrote?, is
thus extracted and handed to the QA module. In a
second phase, the dialogue module is responsible
for providing the answer to the user once the QA
module has generated it. The dialogue manager
consults the UM to decide on the most suitable
formulation of the answer (e.g. short sentences)
and produce the final answer accordingly, e.g.:
? System: Roald Dahl wrote many books for kids and adults,
including: ?The Witches?, ?Charlie and the Chocolate Fac-
tory?, and ?James and the Giant Peach".
2http://www.cs.york.ac.uk/aig/aqua
4 Question Answering Module
The flow between the three QA phases ? question
processing, document retrieval and answer gener-
ation ? is described below (see Fig. 2).
4.1 Question processing
We perform query expansion, which consists in
creating additional queries using question word
synonyms in the purpose of increasing the recall
of the search engine. Synonyms are obtained via
the WordNet 2.0 3 lexical database.
Question QUERY
EXPANSION
DOCUMENT
RETRIEVAL
KEYPHRASE
EXTRACTION
ESTIMATION
OF READING
LEVELS
CLUSTERING
Language
Models
UM-BASED
FILTERING
SEMANTIC
SIMILARITY
RANKING
User Model
Reading
Level
Ranked
Answer
Candidates
Figure 2: Diagram of the QA module
4.2 Retrieval
Document retrieval We retrieve the top 20 doc-
uments returned by Google4 for each query pro-
duced via query expansion. These are processed
in the following steps, which progressively narrow
the part of the text containing relevant informa-
tion.
Keyphrase extraction Once the documents are
retrieved, we perform keyphrase extraction to de-
termine their three most relevant topics using Kea
(Witten et al, 1999), an extractor based on Na?ve
Bayes classification.
Estimation of reading levels To adapt the read-
ability of the results to the user, we estimate
the reading difficulty of the retrieved documents
using the Smoothed Unigram Model (Collins-
Thompson and Callan, 2004), which proceeds in
3http://wordnet.princeton.edu
4http://www.google.com
200
two phases. 1) In the training phase, sets of repre-
sentative documents are collected for a given num-
ber of reading levels. Then, a unigram language
model is created for each set, i.e. a list of (word
stem, probability) entries for the words appearing
in its documents. Our models account for the fol-
lowing reading levels: poor (suitable for ages 7?
11), medium (ages 11?16) and good (adults). 2)
In the test phase, given an unclassified document
D, its estimated reading level is the model lmi
maximizing the likelihood that D ? lmi5.
Clustering We use the extracted topics and es-
timated reading levels as features to apply hierar-
chical clustering on the documents. We use the
WEKA (Witten and Frank, 2000) implementation
of the Cobweb algorithm. This produces a tree
where each leaf corresponds to one document, and
sibling leaves denote documents with similar top-
ics and reading difficulty.
4.3 Answer extraction
In this phase, the clustered documents are filtered
based on the user model and answer sentences are
located and formatted for presentation.
UM-based filtering The documents in the clus-
ter tree are filtered according to their reading diffi-
culty: only those compatible with the UM?s read-
ing level are retained for further analysis6.
Semantic similarity Within each of the retained
documents, we seek the sentences which are se-
mantically most relevant to the query by applying
the metric in (Alfonseca et al, 2001): we rep-
resent each document sentence p and the query
q as word sets P = {pw1, . . . , pwm} and Q =
{qw1, . . . , qwn}. The distance from p to q is then
distq(p) =
?
1?i?m minj [d(pwi, qwj)], where
d(pwi, qwj) is the word-level distance between
pwi and qwj based on (Jiang and Conrath, 1997).
Ranking Given the query q, we thus locate
in each document D the sentence p? such that
p? = argminp?D[distq(p)]; then, distq(p?) be-
comes the document score. Moreover, each clus-
5The likelihood is estimated using the formula:
Li,D =
?
w?D C(w, D) ? log(P (w|lmi)), where w is a
word in the document, C(w, d) is the number of occurrences
of w in D and P (w|lmi) is the probability with which w
occurs in lmi
6However, if their number does not exceed a given thresh-
old, we accept in our candidate set part of the documents hav-
ing the next lowest readability ? or a medium readability if the
user?s reading level is low
ter is assigned a score consisting in the maximal
score of the documents composing it. This allows
to rank not only documents, but also clusters, and
present results grouped by cluster in decreasing or-
der of document score.
Answer presentation We present our answers
in an HTML page, where results are listed follow-
ing the ranking described above. Each result con-
sists of the title and clickable URL of the originat-
ing document, and the passage where the sentence
which best answers the query is located and high-
lighted. Question keywords and potentially useful
information such as named entities are in colour.
5 Sample result
We have been running our system on a range
of queries, including factoid/simple, complex and
controversial ones. As an example of the latter, we
report the query ?Who wrote the Iliad??, which is
a subject of debate. These are some top results:
? UMgood: ?Most Classicists would agree that, whether
there was ever such a composer as "Homer" or not, the
Homeric poems are the product of an oral tradition [. . . ]
Could the Iliad and Odyssey have been oral-formulaic po-
ems, composed on the spot by the poet using a collection of
memorized traditional verses and phases??
? UMmed: ?No reliable ancient evidence for Homer ?
[. . . ] General ancient assumption that same poet wrote Il-
iad and Odyssey (and possibly other poems) questioned by
many modern scholars: differences explained biographi-
cally in ancient world (e g wrote Od. in old age); but simi-
larities could be due to imitation.?
? UMpoor: ?Homer wrote The Iliad and The Odyssey
(at least, supposedly a blind bard named "Homer" did).?
In the three results, the problem of attribution of
the Iliad is made clearly visible: document pas-
sages provide a context which helps to explain the
controversy at different levels of difficulty.
6 Evaluation
Since YourQA does not single out one correct an-
swer phrase, TREC evaluation metrics are not suit-
able for it. A user-centred methodology to assess
how individual information needs are met is more
appropriate. We base our evaluation on (Su, 2003),
which proposes a comprehensive search engine
evaluation model, defining the following metrics:
1. Relevance: we define strict precision (P1) as
the ratio between the number of results rated as
relevant and all the returned results, and loose pre-
201
cision (P2) as the ratio between the number of re-
sults rated as relevant or partially relevant and all
the returned results.
2. User satisfaction: a 7-point Likert scale7 is used
to assess the user?s satisfaction with loose preci-
sion of results (S1) and query success (S2).
3. Reading level accuracy: given the set R of re-
sults returned for a reading level r, Ar is the ratio
between the number of results ? R rated by the
users as suitable for r and |R|.
4. Overall utility (U ): the search session as a
whole is assessed via a 7-point Likert scale.
We performed our evaluation by running 24
queries (some of which in Tab. 2) on Google and
YourQA and submitting the results ?i.e. Google
result page snippets and YourQA passages? of
both to 20 evaluators, along with a questionnaire.
The relevance results (P1 and P2) in Tab. 1 show a
P1 P2 S1 S2 U
Google 0,39 0,63 4,70 4,61 4,59
YourQA 0,51 0,79 5,39 5,39 5,57
Table 1: Evaluation results
10-15% difference in favour of YourQA for both
strict and loose precision. The coarse seman-
tic processing applied and context visualisation
thus contribute to creating more relevant passages.
Both user satisfaction results (S1 and S2) in Tab.
1 also denote a higher level of satisfaction tributed
to YourQA. Tab. 2 shows that evaluators found our
Query Ag Am Ap
When did the Middle Ages begin? 0,91 0,82 0,68
Who painted the Sistine Chapel? 0,85 0,72 0,79
When did the Romans invade Britain? 0,87 0,74 0,82
Who was a famous cubist? 0,90 0,75 0,85
Who was the first American in space? 0,94 0,80 0,72
Definition of metaphor 0,95 0,81 0,38
average 0,94 0,85 0,72
Table 2: Sample queries and accuracy values
results appropriate for the reading levels to which
they were assigned. The accuracy tended to de-
crease (from 94% to 72%) with the level: it is
indeed more constraining to conform to a lower
reading level than to a higher one. Finally, the
7This measure ? ranging from 1= ?extremely unsatisfac-
tory? to 7=?extremely satisfactory? ? is particularly suitable
to assess how well a system meets user?s search needs.
general satisfaction values for U in Tab. 1 show
an improved preference for YourQA.
7 Conclusion
A user-tailored QA system is proposed where a
user model contributes to adapting answers to the
user?s needs and presenting them appropriately.
A preliminary evaluation of our core QA module
shows a positive feedback from human assessors.
Our short term goals involve performing a more
extensive evaluation and implementing a dialogue
interface to improve the system?s interactivity.
References
E. Alfonseca, M. DeBoni, J.-L. Jara-Valencia, and
S. Manandhar. 2001. A prototype question answer-
ing system using syntactic and semantic information
for answer retrieval. In Text REtrieval Conference.
L. Ardissono, L. Console, and I. Torre. 2001. An adap-
tive system for the personalized access to news. AI
Commun., 14(3):129?147.
K. Collins-Thompson and J. P. Callan. 2004. A lan-
guage modeling approach to predicting reading dif-
ficulty. In Proceedings of HLT/NAACL.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference Re-
search on Computational Linguistics (ROCLING X).
C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In World Wide
Web, pages 150?161.
Bernardo Magnini and Carlo Strapparava. 2001. Im-
proving user modelling with content-based tech-
niques. In UM: Proceedings of the 8th Int. Confer-
ence, volume 2109 of LNCS. Springer.
L. T. Su. 2003. A comprehensive and systematic
model of user evaluation of web search engines: Ii.
an evaluation by undergraduates. J. Am. Soc. Inf.
Sci. Technol., 54(13):1193?1223.
E. M. Voorhees. 2003. Overview of the TREC 2003
question answering track. In Text REtrieval Confer-
ence.
E. M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Text REtrieval Confer-
ence.
H. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools and Techniques with Java
Implementation. Morgan Kaufmann.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. 1999. KEA: Practical au-
tomatic keyphrase extraction. In ACM DL, pages
254?255.
202
An Analysis of Clarification Dialogue for Question Answering
Marco De Boni

School of Computing
Leeds Metropolitan University
Leeds LS6 3QS, UK
Department of Computer Science
University of York
York Y010 5DD, UK
mdeboni@cs.york.ac.uk
Suresh Manandhar

 Department of Computer Science
University of York
York Y010 5DD, UK
suresh@cs.york.ac.uk



Abstract
We examine clarification dialogue, a
mechanism for refining user questions with
follow-up questions, in the context of open
domain Question Answering systems. We
develop an algorithm for clarification dialogue
recognition through the analysis of collected
data on clarification dialogues and examine
the importance of clarification dialogue
recognition for question answering. The
algorithm is evaluated and shown to
successfully recognize the occurrence of
clarification dialogue in the majority of cases
and to simplify the task of answer retrieval.
1 Clarification dialogues in Question
Answering
Question Answering Systems aim to determine an
answer to a question by searching for a response in a
collection of documents (see Voorhees 2002 for an
overview of current systems). In order to achieve this
(see for example Harabagiu et al 2002), systems narrow
down the search by using information retrieval
techniques to select a subset of documents, or
paragraphs within documents, containing keywords
from the question and a concept which corresponds to
the correct question type (e.g. a question starting with
the word ?Who?? would require an answer containing a
person). The exact answer sentence is then sought by
either attempting to unify the answer semantically with
the question, through some kind of logical
transformation (e.g. Moldovan and Rus 2001) or by
some form of pattern matching (e.g. Soubbotin 2002;
Harabagiu et al 1999).
Often, though, a single question is not enough to meet
user?s goals and an elaboration or clarification dialogue
is required, i.e. a dialogue with the user which would
enable the answering system to refine its understanding
of the questioner's needs (for reasons of space we shall
not investigate here the difference between elaboration
dialogues, clarification dialogues and coherent topical
subdialogues and we shall hence refer to this type of
dialogue simply as ?clarification dialogue?, noting that
this may not be entirely satisfactory from a theoretical
linguistic point of view). While a number of researchers
have looked at clarification dialogue from a theoretical
point of view (e.g. Ginzburg 1998; Ginzburg and Sag
2000; van Beek at al. 1993), or from the point of view
of task oriented dialogue within a narrow domain (e.g.
Ardissono and Sestero 1996), we are not aware of any
work on clarification dialogue for open domain question
answering systems such as the ones presented at the
TREC workshops, apart from the experiments carried
out for the (subsequently abandoned) ?context? task in
the TREC-10 QA workshop (Voorhees 2002; Harabagiu
et al 2002). Here we seek to partially address this
problem by looking at some particular aspect of
clarification dialogues in the context of open domain
question answering. In particular, we examine the
problem of recognizing that a clarification dialogue is
occurring, i.e. how to recognize that the current question
under consideration is part of a previous series (i.e.
clarifying previous questions) or the start of a new
series; we then show how the recognition that a
clarification dialogue is occurring can simplify the
problem of answer retrieval.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 48-55
                                                         Proceedings of HLT-NAACL 2003
2 The TREC Context Experiments
The TREC-2001 QA track included a "context" task
which aimed at testing systems' ability to track context
through a series of questions (Voorhees 2002). In other
words, systems were required to respond correctly to a
kind of clarification dialogue in which a full
understanding of questions depended on an
understanding of previous questions. In order to test the
ability to answer such questions correctly, a total of 42
questions were prepared by NIST staff, divided into 10
series of related question sentences which therefore
constituted a type of clarification dialogue; the
sentences varied in length between 3 and 8 questions,
with an average of 4 questions per dialogue. These
clarification dialogues were however presented to the
question answering systems already classified and hence
systems did not need to recognize that clarification was
actually taking place.  Consequently systems that simply
looked for an answer in the subset of documents
retrieved for the first question in a series performed well
without any understanding of the fact that the questions
constituted a coherent series.
In a more realistic approach, systems would not be
informed in advance of the start and end of a series of
clarification questions and would not be able to use this
information to limit the subset of documents in which
an answer is to be sought.
3 Analysis of the TREC context questions
We manually analysed the TREC context question
collection in order to determine what features could be
used to determine the start and end of a question series,
with the following conclusions:
? Pronouns and possessive adjectives: questions such
as ?When was it born??, which followed ?What was
the first transgenic mammal??, were referring to
some previously mentioned object through a
pronoun (?it?). The use of personal pronouns (?he?,
?it?, ?) and possessive adjectives (?his?, ?her?,?)
which did not have any referent in the question
under consideration was therefore considered an
indication of a clarification question..
? Absence of verbs: questions such as ?On what body
of water?? clearly referred to some previous
question or answer.
? Repetition of proper nouns: the question series
starting with ?What type of vessel was the modern
Varyag?? had a follow-up question ?How long was
the Varyag??, where the repetition of the proper
noun indicates that the same subject matter is under
investigation.
? Importance of semantic relations: the first question
series started with the question ?Which museum in
Florence was damaged by a major bomb
explosion??; follow-up questions included ?How
many people were killed?? and ?How much
explosive was used??, where there is a clear
semantic relation between the ?explosion? of the
initial question and the ?killing? and ?explosive? of
the following questions. Questions belonging to a
series were ?about? the same subject, and this
aboutness could be seen in the use of semantically
related words.
4 Experiments in Clarification Dialogue
Recognition
It was therefore speculated that an algorithm which
made use of these features would successfully recognize
the occurrence of clarification dialogue. Given that the
only available data was the collection of ?context?
questions used in TREC-10, it was felt necessary to
collect further data in order to test our algorithm
rigorously. This was necessary both because of the
small number of questions in the TREC data and the
fact that there was no guarantee that an algorithm built
for this dataset would perform well on ?real? user
questions. A collection of 253 questions was therefore
put together by asking potential users to seek
information on a particular topic by asking a prototype
question answering system a series of questions, with
?cue? questions derived from the TREC question
collection given as starting points for the dialogues.
These questions made up 24 clarification dialogues,
varying in length from 3 questions to 23, with an
average length of 12 questions (the data is available
from the main author upon request).
The differences between the TREC ?context?
collection and the new collection are summarized in the
following table:

 Groups Qs Av. len Max Min
TREC 10 41 4 8 4
New 24 253 12 23 3

The questions were recorded and manually tagged to
recognize the occurrence of clarification dialogue.
The questions thus collected were then fed into a
system implementing the algorithm, with no indication
as to where a clarification dialogue occurred. The
system then attempted to recognize the occurrence of a
clarification dialogue. Finally the results given by the
system were compared to the manually recognized
clarification dialogue tags. In particular the algorithm
was evaluated for its capacity to:
? recognize a new series of questions (i.e. to tell that
the current question is not a clarification of any
previous question) (indicated by New in the results
table)
? recognize that the current question is clarifying a
previous question (indicated by Clarification in the
table)
5 Clarification Recognition Algorithm
Our approach to clarification dialogue recognition
looks at certain features of the question currently under
consideration (e.g. pronouns and proper nouns) and
compares the meaning of the current question with the
meanings of previous questions to determine whether
they are ?about? the same matter.
Given a question q0  and n  previously asked
questions q
-1..q-n  we have a function
Clarification_Question which is true if a question is
considered a clarification of a previously asked
question. In the light of empirical work such as
(Ginzburg 1998), which indicates that questioners do
not usually refer back to questions which are very
distant, we only considered the set of the previously
mentioned 10 questions.
A question is deemed to be a clarification of a
previous question if:
1. There are direct references to nouns mentioned in
the previous n  questions through  the use of
pronouns (he, she, it, ?) or possessive adjectives
(his, her, its?) which have no references in the
current question.
2. The question does not contain any verbs
3. There are explicit references to proper and common
nouns mentioned in the previous n  questions, i.e.
repetitions which refer to an identical object; or
there is a strong sentence similarity between the
current question and the previously asked
questions.
In other words:

Clarification_Question
   (qn,q-1..q-n)
is true if
1. q0  has pronoun and
possessive adjective
references to q
-1..q-n 
2. q0  does not contain any
verbs
3. q0 has repetition of
common or proper nouns
in q
-1..q-n or q0  has a
strong semantic
similarity to some q ?
q
-1..q-n 
6 Sentence Similarity Metric
A major part of our clarification dialogue recognition
algorithm is the sentence similarity metric which looks
at the similarity in meaning between the current
question and previous questions. WordNet (Miller 1999;
Fellbaum 1998), a lexical database which organizes
words into synsets, sets of synonymous words, and
specifies a number of relationships such as hypernym,
synonym, meronym which can exist between the synsets
in the lexicon, has been shown to be fruitful in the
calculation of semantic similarity. One approach has
been to determine similarity by calculating the length of
the path or relations connecting the words which
constitute sentences (see for example Green 1997 and
Hirst and St-Onge 1998); different approaches have
been proposed (for an evaluation see (Budanitsky and
Hirst 2001)), either using all WordNet relations
(Budanitsky and Hirst 2001) or only is-a relations
(Resnik 1995; Jiang and Conrath 1997; Mihalcea and
Moldvoan 1999). Miller (1999), Harabagiu et al (2002)
and De Boni and Manandhar (2002) found WordNet
glosses, considered as micro-contexts, to be useful in
determining conceptual similarity. (Lee et al 2002)
have applied conceptual similarity to the Question
Answering task, giving an answer A  a score dependent
on the number of matching terms in A  and the question.
Our sentence similarity measure followed on these
ideas, adding to the use of WordNet relations, part-of-
speech information, compound noun and word
frequency information.
In particular, sentence similarity was considered as a
function which took as arguments a sentence s1  and a
second sentence s2 and returned a value representing the
semantic relevance of s1  in respect of s2  in the context of
knowledge B, i.e.

semantic-relevance( s1, s2, B  ) = n ?
 


semantic-relevance(s1,s,B) < semantic-
relevance(s2,s, B) represents the fact that sentence s1  is
less relevant than s2  in respect to the sentence s and the
context B. In our experiments, B  was taken to be the set
of semantic relations given by WordNet. Clearly, the
use of a different knowledge base would give different
results, depending on its completeness and correctness.
In order to calculate the semantic similarity between
a sentence s1  and another sentence s2, s1  and s2  were
considered as sets P  and Q  of word stems. The
similarity between each word in the question and each
word in the answer was then calculated and the sum of
the closest matches gave the overall similarity. In other
words, given two sets Q  and P, where
Q={qw1,qw2,?,qwn} and P={pw1,pw2,?,pwm}, the
similarity between Q  and P  is given by
1<p<n Argmaxm similarity( qwp, pwm)

The function similarity( w1, w2) maps the stems of
the two words w1 and w2  to a similarity measure m
representing how semantically related the two words
are; similarity( wi, wj)< similarity( wi, wk) represents the
fact that the word wj is less semantically related than wk
in respect to the word wi. In particular similarity=0 if
two words are not at all semantically related and
similarity=1 if the words are the same.

similarity( w1, w2) = h  ?
 


where 0 ?  h  ?  1. In particular, similarity( w1, w2) = 0 if
w1?ST ? w2?ST, where ST is a set containing a number
of stop-words (e.g. ?the?, ?a?, ?to?) which are too
common to be able to be usefully employed to estimate
semantic similarity. In all other cases,  h  is calculated as
follows: the words w1 and w2  are compared using all the
available WordNet relationships (is-a, satellite, similar,
pertains, meronym, entails, etc.), with the additional
relationship, ?same-as?, which indicated that two words
were identical. Each relationship is given a weighting
indicating how related two words are, with a ?same as?
relationship indicating the closest relationship, followed
by synonym relationships, hypernym, hyponym, then
satellite, meronym, pertains, entails.
So, for example, given the question ?Who went to
the mountains yesterday?? and the second question ?Did
Fred walk to the big mountain and then to mount
Pleasant??, Q  would be the set {who, go, to, the,
mountain, yesterday} and P  would be the set {Did,
Fred, walk, to, the, big, mountain, and, then, to, mount,
Pleasant}.
In order to calculate similarity the algorithm would
consider each word in turn. ?Who? would be ignored as
it is a common word and hence part of the list of stop-
words. ?Go? would be related to ?walk? in a is-a
relationship and receive a score h1. ?To? and ?the?
would be found in the list of stop-words and ignored.
?Mountain? would be considered most similar to
?mountain? (same-as relationship) and receive a score
h2: ?mount? would be in a synonym relationship with
?mountain? and give a lower score, so it is ignored.
?Yesterday? would receive a score of 0 as there are no
semantically related words in Q. The similarity measure
of Q  in respect to P  would therefore be given by h1  + h2.
In order to improve performance of the similarity
measure, additional information was considered in
addition to simple word matching (see De Boni and
Manandhar 2003 for a complete discussion):
? Compound noun information.  The motivation
behind is similar to the reason for using chunking
information, i.e. the fact that the word ?United? in
?United States?  should not be considered similar to
?United? as in ?Manchester United?. As opposed to
when using chunking information, however, when
using noun compound information, the compound
is considered a single word, as opposed to a group
of words: chunking and compound noun
information may therefore be combined as in ?[the
[United States] official team]?.
? Proper noun information. The intuition behind this
is that titles (of books, films, etc.) should not be
confused with the ?normal? use of the same words:
?blue lagoon? as in the sentence ?the film Blue
Lagoon was rather strange? should not be
considered as similar to the same words in the
sentence ?they swan in the blue lagoon? as they are
to the sentence ?I enjoyed Blue Lagoon when I was
younger?.
? Word frequency information.  This is a step beyond
the use of stop-words, following the intuition that
the more a word is common the less it is useful in
determining similarity between sentence. So, given
the sentences ?metatheoretical reasoning is
common in philosophy? and ?metatheoretical
arguments are common in philosophy?, the word
?metatheoretical? should be considered more
important in determining relevance than the words
?common?, ?philosophy? and ?is? as it is much
more rare and therefore less probably found in
irrelevant sentences. Word frequency data was
taken from the Given that the questions examined
were generic queries which did not necessarily refer
to a specific set of documents, the word frequency
for individual words was taken to be the word
frequency given in the British National Corpus (see
BNCFreq 2003). The top 100 words, making up
43% of the English Language, were then used as
stop-words and were not used in calculating
semantic similarity.

7 Results
An implementation of the algorithm was evaluated
on the TREC context questions used to develop the
algorithm and then on the collection of 500 new
clarification dialogue questions. The results on the
TREC data, which was used to develop the algorithm,
were as follows (see below for discussion and an
explanation of each method):

TREC Meth.0 Meth.1 Meth.2 Meth.3aMeth.3b
New  90 90 90 60 80
Clarif. 47 53 59 78 72
Where ?New? indicates the ability to recognize
whether the current question is the first in a new series
of clarification questions and ?Clarif.? (for
?Clarification?) indicates the ability to recognize
whether the current question is a clarification question.
The results for the same experiments conducted on
the collected data were as follows:

Collected Meth.0 Meth.1 Meth.2 Meth.3a Meth.3b
New  100 100 100 67 83
Clarif. 64 62 66 91 89

Method 0. This method did not use any linguistic
information and simply took a question to be a
clarification question if it had any words in common
with the previous n questions, else took the question to
be the beginning of a new series. 64% of questions in
the new collection could be recognized with this simple
algorithm, which did not misclassify any "new"
questions.
Method 1. This method employed point 1 of the
algorithm described in section 5: 62% of questions in
the new collection could be recognized as clarification
questions simply by looking for "reference" keywords
such as he, she, this, so, etc. which clearly referred to
previous questions. Interestingly this did not misclassify
any "new" questions.
Method 2. This method employed points 1 and 2 of
the algorithm described in section 5: 5% of questions in
the new collection could be recognized simply by
looking for the absence of verbs, which, combined with
keyword lookup (Method 1), improved performance to
66%. Again this did not misclassify any "new"
questions.
Method 3a. This method employed the full
algorithm described in section 5 (point 3 is the
similarity measure algorithm described in section 6):
clarification recognition rose to 91% of the new
collection by looking at the similarity between nouns in
the current question and nouns in the previous
questions, in addition to reference words and the
absence of verbs. Misclassification was a serious
problem, however with correctly classified "new"
questions falling to 67%.
Method 3b. This was the same as method 3a, but
specified a similarity threshold when employing the
similarity measure described in section 6: this required
the nouns in the current question to be similar to nouns
in the previous question beyond a specified similarity
threshold. This brought clarification question
recognition down to 89% of the new collection, but
misclassification of "new" questions was reduced
significantly, with "new" questions being correctly
classified 83% of the time.
Problems noted were:
? False positives: questions following a similar but
unrelated question series. E.g. "Are they all Muslim
countries?" (talking about religion, but in the
context of a general conversation about Saudi
Arabia) followed by "What is the chief religion in
Peru?" (also about religion, but in a totally
unrelated context).
? Questions referring to answers, not previous
questions (e.g. clarifying the meaning of a word
contained in the answer, or building upon a concept
defined in the answer: e.g. "What did Antonio
Carlos Tobim play?" following "Which famous
musicians did he play with?" in the context of a
series of questions about Fank Sinatra: Antonio
Carlos Tobim was referred to in the answer to the
previous question, and nowhere else in the
exchange. These made up 3% of the missed
clarifications.
? Absence of relationships in WordNet, e.g. between
"NASDAQ" and "index" (as in share index).
Absence of verb-noun relationships in WordNet,
e.g. between to die and death, between "battle" and
"win" (i.e. after a battle one side generally wins and
another side loses), "airport" and "visit" (i.e. people
who are visiting another country use an airport to
get there)

As can be seen from the tables above, the same
experiments conducted on the TREC context questions
yielded worse results; it was difficult to say, however,
whether this was due to the small size of the TREC data
or the nature of the data itself, which perhaps did not
fully reflect ?real? dialogues.
 As regards the recognition of question in a series
(the recognition that a clarification I taking place), the
number of sentences recognized by keyword alone was
smaller in the TREC data (53% compared to 62%),
while the number of questions not containing verbs was
roughly similar (about 6%). The improvement given by
computing noun similarity between successive
questions gave worse results on the TREC data: using
method 3a resulted in an improvement to the overall
correctness of 19 percentage points, or a 32% increase
(compared to an improvement of 25 percentage points,
or a 38% increase on the collected data); using method
3b resulted in an improvement of 13 percentage points,
or a 22% increase (compared to an improvement of 23
percentage points or a 35% increase on the collected
data), perhaps indicating that in "real" conversation
speakers tend to use simpler semantic relationships than
what was observed in the TREC data.
8 Usefulness of Clarification Dialogue
Recognition
Recognizing that a clarification dialogue is occurring
only makes sense if this information can then be used to
improve answer retrieval performance.
We therefore hypothesized that noting that a
questioner is trying to clarify previously asked questions
is important in order to determine the context in which
an answer is to be sought: in other words, the answers to
certain questions are constrained by the context in
which they have been uttered. The question ?What does
attenuate mean??, for example, may require a generic
answer outlining all the possible meanings of
?attenuate? if asked in isolation, or a particular meaning
if asked after the word has been seen in an answer (i.e.
in a definite context which constrains its meaning).  In
other cases, questions do not make sense at all out of a
context. For example, no answer could be given to the
question ?where?? asked on its own, while following a
question such as ?Does Sean have a house anywhere
apart from Scotland?? it becomes an easily intelligible
query.
The usual way in which Question Answering
systems constrain possible answers is by restricting the
number of documents in which an answer is sought by
filtering the total number of available documents
through the use of an information retrieval engine. The
information retrieval engine selects a subset of the
available documents based on a number of keywords
derived from the question at hand. In the simplest case,
it is necessary to note that some words in the current
question refer to words in previous questions or answers
and hence use these other words when formulating the
IR query. For example, the question ?Is he married??
cannot be used as is  in order to select documents, as the
only word passed to the IR engine would be ?married?
(possibly the root version ?marry?) which would return
too many documents to be of any use. Noting that the
?he? refers to a previously mentioned person (e.g. ?Sean
Connery?) would enable the answerer to seek an answer
in a smaller number of documents. Moreover, given that
the current question is asked in the context of a previous
question, the documents retrieved for the previous
related question could provide a context in which to
initially seek an answer.
In order to verify the usefulness of constraining the
set of documents from in which to seek an answer, a
subset made of 15 clarification dialogues (about 100
questions) from the given question data was analyzed by
taking the initial question for a series, submitting it to
the Google Internet Search Engine and then manually
checking to see how many of the questions in the series
could be answered simply by using the first 20
documents retrieved for the first question in a series.
The results are summarized in the following diagram
(Fig. 1):

Fig. 1: Search technique used for Question
First Q in series
Words in Q
Coreference
Mini-clarification
Other

? 69% of clarification questions could be answered
by looking within the documents used for the
previous question in the series, thus indicating the
usefulness of noting the occurrence of clarification
dialogue.
? The remaining 31% could not be answered by
making reference to the previously retrieved
documents, and to find an answer a different
approach had to be taken. In particular:
? 6% could be answered after retrieving documents
simply by using the words in the question as search
terms (e.g. ?What caused the boxer uprising??);
? 14% required some form of coreference resolution
and could be answered only by combining the
words in the question with the words to which the
relative pronouns in the question referred (e.g.
?What film is he working on at the moment?, with
the reference to ?he? resolved, which gets passed to
the search engine as ?What film is Sean Connery
working on at the moment??);
? 7% required more than 20 documents to be
retrieved by the search engine or other, more
complex techniques. An example is a question such
as ?Where exactly?? which requires both an
understanding of the context in which the question
is asked (?Where?? makes no sense on its own) and
the previously given answer (which was probably a
place, but not restrictive enough for the questioner).
? 4% constituted mini-clarification dialogues within a
larger clarification dialogue (a slight deviation from
the main topic which was being investigated by the
questioner) and could be answered by looking at
the documents retrieved for the first question in the
mini-series.

Recognizing that a clarification dialogue is
occurring therefore can simplify the task of retrieving an
answer by specifying that an answer must be in the set
of documents used the previous questions. This is
consistent with the results found in the TREC context
task (Voorhees 2002), which indicated that systems
were capable of finding most answers to questions in a
context dialogue simply by looking at the documents
retrieved for the initial question in a series. As in the
case of clarification dialogue recognition, therefore,
simple techniques can resolve the majority of cases;
nevertheless, a full solution to the problem requires
more complex methods. The last case indicates that it is
not enough simply to look at the documents provided by
the first question in a series in order to seek an answer:
it is necessary to use the documents found for a
previously asked question which is related to the current
question (i.e. the questioner could "jump" between
topics). For example, given the following series of
questions starting with Q1:

Q1: When was the Hellenistic Age?
[?]
Q5: How did Alexander the great become ruler?
Q6: Did he conquer anywhere else?
Q7: What was the Greek religion in the Hellenistic Age?

where Q6  should be related to Q5  but Q7  should be
related to Q1, and not Q6. In this case, given that the
subject matter of Q1  is more immediately related to the
subject matter of Q7  than Q6  (although the subject
matter of Q6  is still broadly related, it is more of a
specialized subtopic), the documents retrieved for Q1
will probably be more relevant to Q7  than the
documents retrieved for Q6 (which would probably be
the same documents retrieved for Q5)

9 Conclusion
It has been shown that recognizing that a clarification
dialogue is occurring can simplify the task of retrieving
an answer by constraining the subset of documents in
which an answer is to be found. An algorithm was
presented to recognize the occurrence of clarification
dialogue and is shown to have a good performance. The
major limitation of our algorithm is the fact that it only
considers series of questions, not series of answers. As
noted above, it is often necessary to look at an answer to
a question to determine whether the current question is a
clarification question or not. Our sentence similarity
algorithm was limited by the number of semantic
relationships in WordNet: for example, a big
improvement would come from the use of noun-verb
relationships. Future work will be directed on extending
WordNet in this direction and in providing other useful
semantic relationships. Work also needs to be done on
using information given by answers, not just questions
in recognizing clarification dialogue and on coping with
the cases in which clarification dialogue recognition is
not enough to retrieve an answer and where other, more
complex, techniques need to be used. It would also be
beneficial to examine the use of a similarity function in
which similarity decayed in function of the distance in
time between the current question and the past
questions.
References
Ardissono, L. and Sestero, D. 1996. "Using dynamic
user models in the recognition of the plans of the
user". User Modeling and User-Adapted Interaction,
5(2):157-190.
BNCFreq. 2003. English Word Frequency List.
http://www.eecs.umich.edu/~qstout/586/bncfreq.html
(last accessed March 2003).
Budanitsky, A., and Hirst, G. 2001. ?Semantic distance
in WordNet: and experimental, application-oriented
evaluation of five measures?, in Proceedings of the
NAACL 2001 Workshop on WordNet and other
lexical resources, Pittsburgh.
De Boni, M. and Manandhar, S. 2003. ?The Use of
Sentence Similarity as a Semantic Relevance Metric
for Question Answering?. Proceedings of the AAAI
Symposium on New Directions in Question
Answering, Stanford.
De Boni, M. and Manandhar, S. 2002. ?Automated
Discovery of Telic Relations for WordNet?.
Proceedings of the First International WordNet
Conference, India.
Fellbaum, C. 1998. WordNet, An electronic Lexical
Database, MIT Press.
Ginzburg , J. 1998. "Clarifying Utterances" In: J.
Hulstijn and A. Nijholt (eds.) Proceedings of the 2nd
Workshop on the Formal Semantics and Pragmatics
of Dialogue, Twente.
Ginzburg and Sag, 2000. Interrogative Investigations,
CSLI.
Green, S. J. 1997. Automatically generating hypertext
by computing semantic similarity, Technical Report
n. 366, University of Toronto.
Harabagiu, S., Miller, A. G., Moldovan, D. 1999.
?WordNet2 - a morphologically and semantically
enhanced resource?, In Proceedings of SIGLEX-99,
University of Maryland.
Harabagiu, S., et al 2002. ?Answering Complex, List
and Context Questions with LCC?s Question-
Answering Server?, Proceedings of TREC-10, NIST.
Hirst, G., and St-Onge, D. 1998. ?Lexical chains as
representations of context for the detection and
correction of malapropisms?, in Fellbaum (ed.),
WordNet: and electronic lexical database, MIT
Press.
Jiang, J. J., and Conrath, D. W. 1997. ?Semantic
similarity based on corpus statistics and lexical
taxonomy?, in Proceedings of ICRCL, Taiwan.
Lee, G. G., et al 2002. ?SiteQ: Engineering High
Performance QA System Using Lexico-Semantic
Pattern Matching and Shallow NLP?, Proceedings of
TREC-10, NIST.
Lin, D. 1998. ?An information-theoretic definition of
similarity?, in Proceedings of the 15th International
Conference on Machine Learning, Madison.
Mihalcea, R. and Moldovan, D. 1999. ?A Method for
Word Sense Disambiguation of Unrestricted Text?,
in Proceedings of ACL ?99, Maryland, NY.
Miller, G. A. 1999. ?WordNet: A Lexical Database?,
Communications of the ACM, 38 (11).
Moldovan, D. and Rus, V. 2001. ?Logic Form
Transformation of WordNet and its Applicability to
Question Answering?, Proceedings of the 39th
conference of ACL, Toulouse.
Resnik, P. 1995. ?Using information content to evaluate
semantic similarity?, in Proceedings of the 14th
IJCAI, Montreal.
Soubbotin, M. M. 2002. :?Patterns of Potential Answer
Expressions as Clues to the Right Answers?,
Proceedings of TREC-10, NIST.
van Beek, P., Cohen, R. and Schmidt, K., 1993. ?From
plan critiquing to clarification dialogue for
cooperative response generation?, Computational
Intelligence  9:132-154.
Voorhees, E. 2002. ?Overview of the TREC 2001
Question Answering Track?, Proceedings of TREC-
10, NIST.
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776?783,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Exploiting Syntactic and Shallow Semantic Kernels
for Question/Answer Classification
Alessandro Moschitti
University of Trento
38050 Povo di Trento
Italy
moschitti@dit.unitn.it
Silvia Quarteroni
The University of York
York YO10 5DD
United Kingdom
silvia@cs.york.ac.uk
Roberto Basili
?Tor Vergata? University
Via del Politecnico 1
00133 Rome, Italy
basili@info.uniroma2.it
Suresh Manandhar
The University of York
York YO10 5DD
United Kingdom
suresh@cs.york.ac.uk
Abstract
We study the impact of syntactic and shallow
semantic information in automatic classifi-
cation of questions and answers and answer
re-ranking. We define (a) new tree struc-
tures based on shallow semantics encoded
in Predicate Argument Structures (PASs)
and (b) new kernel functions to exploit the
representational power of such structures
with Support Vector Machines. Our ex-
periments suggest that syntactic information
helps tasks such as question/answer classifi-
cation and that shallow semantics gives re-
markable contribution when a reliable set of
PASs can be extracted, e.g. from answers.
1 Introduction
Question answering (QA) is as a form of informa-
tion retrieval where one or more answers are re-
turned to a question in natural language in the form
of sentences or phrases. The typical QA system ar-
chitecture consists of three phases: question pro-
cessing, document retrieval and answer extraction
(Kwok et al, 2001).
Question processing is often centered on question
classification, which selects one of k expected an-
swer classes. Most accurate models apply super-
vised machine learning techniques, e.g. SNoW (Li
and Roth, 2005), where questions are encoded us-
ing various lexical, syntactic and semantic features.
The retrieval and answer extraction phases consist in
retrieving relevant documents (Collins-Thompson et
al., 2004) and selecting candidate answer passages
from them. A further answer re-ranking phase is op-
tionally applied. Here, too, the syntactic structure
of a sentence appears to provide more useful infor-
mation than a bag of words (Chen et al, 2006), al-
though the correct way to exploit it is still an open
problem.
An effective way to integrate syntactic structures
in machine learning algorithms is the use of tree ker-
nel (TK) functions (Collins and Duffy, 2002), which
have been successfully applied to question classifi-
cation (Zhang and Lee, 2003; Moschitti, 2006) and
other tasks, e.g. relation extraction (Zelenko et al,
2003; Moschitti, 2006). In more complex tasks such
as computing the relatedness between questions and
answers in answer re-ranking, to our knowledge no
study uses kernel functions to encode syntactic in-
formation. Moreover, the study of shallow semantic
information such as predicate argument structures
annotated in the PropBank (PB) project (Kingsbury
and Palmer, 2002) (www.cis.upenn.edu/?ace) is a
promising research direction. We argue that seman-
tic structures can be used to characterize the relation
between a question and a candidate answer.
In this paper, we extensively study new structural
representations, encoding parse trees, bag-of-words,
POS tags and predicate argument structures (PASs)
for question classification and answer re-ranking.
We define new tree representations for both simple
and nested PASs, i.e. PASs whose arguments are
other predicates (Section 2). Moreover, we define
new kernel functions to exploit PASs, which we au-
tomatically derive with our SRL system (Moschitti
et al, 2005) (Section 3).
Our experiments using SVMs and the above ker-
776
nels and data (Section 4) shows the following: (a)
our approach reaches state-of-the-art accuracy on
question classification. (b) PB predicative structures
are not effective for question classification but show
promising results for answer classification on a cor-
pus of answers to TREC-QA 2001 description ques-
tions. We created such dataset by using YourQA
(Quarteroni and Manandhar, 2006), our basic Web-
based QA system1. (c) The answer classifier in-
creases the ranking accuracy of our QA system by
about 25%.
Our results show that PAS and syntactic parsing
are promising methods to address tasks affected by
data sparseness like question/answer categorization.
2 Encoding Shallow Semantic Structures
Traditionally, information retrieval techniques are
based on the bag-of-words (BOW) approach aug-
mented by language modeling (Allan et al, 2002).
When the task requires the use of more complex se-
mantics, the above approaches are often inadequate
to perform fine-level textual analysis.
An improvement on BOW is given by the use of
syntactic parse trees, e.g. for question classification
(Zhang and Lee, 2003), but these, too are inadequate
when dealing with definitional answers expressed by
long and articulated sentences or even paragraphs.
On the contrary, shallow semantic representations,
bearing a more ?compact? information, could pre-
vent the sparseness of deep structural approaches
and the weakness of BOW models.
Initiatives such as PropBank (PB) (Kingsbury
and Palmer, 2002) have made possible the design of
accurate automatic Semantic Role Labeling (SRL)
systems (Carreras and Ma`rquez, 2005). Attempting
an application of SRL to QA hence seems natural,
as pinpointing the answer to a question relies on a
deep understanding of the semantics of both.
Let us consider the PB annotation: [ARG1
Antigens] were [AM?TMP originally] [rel
defined] [ARG2 as non-self molecules].
Such annotation can be used to design a shallow
semantic representation that can be matched against
other semantically similar sentences, e.g. [ARG0
Researchers] [rel describe] [ARG1 antigens]
[ARG2 as foreign molecules] [ARGM?LOC in
1Demo at: http://cs.york.ac.uk/aig/aqua.
PAS
rel
define
ARG1
antigens
ARG2
molecules
ARGM-TMP
originally
PAS
rel
describe
ARG0
researchers
ARG1
antigens
ARG2
molecules
ARGM-LOC
body
Figure 1: Compact predicate argument structures of
two different sentences.
the body].
For this purpose, we can represent the above anno-
tated sentences using the tree structures described in
Figure 1. In this compact representation, hereafter
Predicate-Argument Structures (PAS), arguments
are replaced with their most important word ? often
referred to as the semantic head. This reduces
data sparseness with respect to a typical BOW
representation.
However, sentences rarely contain a single pred-
icate; it happens more generally that propositions
contain one or more subordinate clauses. For
instance let us consider a slight modification of the
first sentence: ?Antigens were originally defined
as non-self molecules which bound specifically to
antibodies2 .? Here, the main predicate is ?defined?,
followed by a subordinate predicate ?bound?. Our
SRL system outputs the following two annotations:
(1) [ARG1 Antigens] were [ARGM?TMP
originally] [rel defined] [ARG2 as non-self
molecules which bound specifically to
antibodies].
(2) Antigens were originally defined as
[ARG1 non-self molecules] [R?A1 which] [rel
bound] [ARGM?MNR specifically] [ARG2 to
antibodies].
giving the PASs in Figure 2.(a) resp. 2.(b).
As visible in Figure 2.(a), when an argument node
corresponds to an entire subordinate clause, we label
its leaf with PAS, e.g. the leaf of ARG2. Such PAS
node is actually the root of the subordinate clause
in Figure 2.(b). Taken as standalone, such PASs do
not express the whole meaning of the sentence; it
is more accurate to define a single structure encod-
ing the dependency between the two predicates as in
2This is an actual answer to ?What are antibodies?? from
our question answering system, YourQA.
777
PAS
rel
define
ARG1
antigens
ARG2
PAS
AM-TMP
originally
(a)
PAS
rel
bound
ARG1
molecules
R-ARG1
which
AM-ADV
specifically
ARG2
antibodies
(b)
PAS
rel
define
ARG1
antigens
ARG2
PAS
rel
bound
ARG1
molecules
R-ARG1
which
AM-ADV
specifically
ARG2
antibodies
AM-TMP
originally
(c)
Figure 2: Two PASs composing a PASN
Figure 2.(c). We refer to nested PASs as PASNs.
It is worth to note that semantically equivalent
sentences syntactically expressed in different ways
share the same PB arguments and the same PASs,
whereas semantically different sentences result in
different PASs. For example, the sentence: ?Anti-
gens were originally defined as antibodies which
bound specifically to non-self molecules?, uses the
same words as (2) but has different meaning. Its PB
annotation:
(3) Antigens were originally defined
as [ARG1 antibodies] [R?A1 which] [rel
bound] [ARGM?MNR specifically] [ARG2 to
non-self molecules],
clearly differs from (2), as ARG2 is now non-
self molecules; consequently, the PASs are also
different.
Once we have assumed that parse trees and PASs
can improve on the simple BOW representation, we
face the problem of representing tree structures in
learning machines. Section 3 introduces a viable ap-
proach based on tree kernels.
3 Syntactic and Semantic Kernels for Text
As mentioned above, encoding syntactic/semantic
information represented by means of tree structures
in the learning algorithm is problematic. A first so-
lution is to use all its possible substructures as fea-
tures. Given the combinatorial explosion of consid-
ering subparts, the resulting feature space is usually
very large. A tree kernel (TK) function which com-
putes the number of common subtrees between two
syntactic parse trees has been given in (Collins and
Duffy, 2002). Unfortunately, such subtrees are sub-
ject to the constraint that their nodes are taken with
all or none of the children they have in the original
tree. This makes the TK function not well suited for
the PAS trees defined above. For instance, although
the two PASs of Figure 1 share most of the subtrees
rooted in the PAS node, Collins and Duffy?s kernel
would compute no match.
In the next section we describe a new kernel de-
rived from the above tree kernel, able to evaluate the
meaningful substructures for PAS trees. Moreover,
as a single PAS may not be sufficient for text rep-
resentation, we propose a new kernel that combines
the contributions of different PASs.
3.1 Tree kernels
Given two trees T1 and T2, let {f1, f2, ..} = F be
the set of substructures (fragments) and Ii(n) be
equal to 1 if fi is rooted at node n, 0 otherwise.
Collins and Duffy?s kernel is defined as
TK(T1, T2) =
?
n1?NT1
?
n2?NT2 ?(n1, n2), (1)
where NT1 and NT2 are the sets of nodes
in T1 and T2, respectively and ?(n1, n2) =
?|F|
i=1 Ii(n1)Ii(n2). The latter is equal to the number
of common fragments rooted in nodes n1 and n2. ?
can be computed as follows:
(1) if the productions (i.e. the nodes with their
direct children) at n1 and n2 are different then
?(n1, n2) = 0;
(2) if the productions at n1 and n2 are the same, and
n1 and n2 only have leaf children (i.e. they are pre-
terminal symbols) then ?(n1, n2) = 1;
(3) if the productions at n1 and n2 are the same, and
n1 and n2 are not pre-terminals then ?(n1, n2) =
?nc(n1)
j=1 (1+?(cjn1 , cjn2)), where nc(n1) is the num-
ber of children of n1 and cjn is the j-th child of n.
Such tree kernel can be normalized and a ? factor
can be added to reduce the weight of large structures
(refer to (Collins and Duffy, 2002) for a complete
description). The critical aspect of steps (1), (2) and
(3) is that the productions of two evaluated nodes
have to be identical to allow the match of further de-
scendants. This means that common substructures
cannot be composed by a node with only some of its
778
PAS
SLOT
rel
define
SLOT
ARG1
antigens
*
SLOT
ARG2
PAS
*
SLOT
ARGM-TMP
originally
*
(a)
PAS
SLOT
rel
define
SLOT
ARG1
antigens
*
SLOT
null
SLOT
null
(b)
PAS
SLOT
rel
define
SLOT
null
SLOT
ARG2
PAS
*
SLOT
null
(c)
Figure 3: A PAS with some of its fragments.
children as an effective PAS representation would
require. We solve this problem by designing the
Shallow Semantic Tree Kernel (SSTK) which allows
to match portions of a PAS.
3.2 The Shallow Semantic Tree Kernel (SSTK)
The SSTK is based on two ideas: first, we change
the PAS, as shown in Figure 3.(a) by adding SLOT
nodes. These accommodate argument labels in a
specific order, i.e. we provide a fixed number of
slots, possibly filled with null arguments, that en-
code all possible predicate arguments. For simplic-
ity, the figure shows a structure of just 4 arguments,
but more can be added to accommodate the max-
imum number of arguments a predicate can have.
Leaf nodes are filled with the wildcard character *
but they may alternatively accommodate additional
information.
The slot nodes are used in such a way that the
adopted TK function can generate fragments con-
taining one or more children like for example those
shown in frames (b) and (c) of Figure 3. As pre-
viously pointed out, if the arguments were directly
attached to the root node, the kernel function would
only generate the structure with all children (or the
structure with no children, i.e. empty).
Second, as the original tree kernel would generate
many matches with slots filled with the null label,
we have set a new step 0:
(0) if n1 (or n2) is a pre-terminal node and its child
label is null, ?(n1, n2) = 0;
and subtract one unit to ?(n1, n2), in step 3:
(3) ?(n1, n2) =
?nc(n1)
j=1 (1 + ?(cjn1 , cjn2))? 1,
The above changes generate a new ? which,
when substituted (in place of the original ?) in Eq.
1, gives the new Shallow Semantic Tree Kernel. To
show that SSTK is effective in counting the number
of relations shared by two PASs, we propose the fol-
lowing:
Proposition 1 The new ? function applied to the
modified PAS counts the number of all possible k-
ary relations derivable from a set of k arguments,
i.e.
?k
i=1
(k
i
)
relations of arity from 1 to k (the pred-
icate being considered as a special argument).
Proof We observe that a kernel applied to a tree and
itself computes all its substructures, thus if we eval-
uate SSTK between a PAS and itself we must obtain
the number of generated k-ary relations. We prove
by induction the above claim.
For the base case (k = 0): we use a PAS with no
arguments, i.e. all its slots are filled with null la-
bels. Let r be the PAS root; since r is not a pre-
terminal, step 3 is selected and ? is recursively ap-
plied to all r?s children, i.e. the slot nodes. For the
latter, step 0 assigns ?(cjr, cjr) = 0. As a result,
?(r, r) = ?nc(r)j=1 (1 + 0)? 1 = 0 and the base case
holds.
For the general case, r is the root of a PAS with k+1
arguments. ?(r, r) = ?nc(r)j=1 (1 + ?(cjr, cjr)) ? 1
=
?k
j=1(1+?(cjr , cjr))?(1+?(ck+1r , ck+1r ))?1. For
k arguments, we assume by induction that?kj=1(1+
?(cjr, cjr))? 1 =
?k
i=1
(k
i
)
, i.e. the number of k-ary
relations. Moreover, (1 + ?(ck+1r , ck+1r )) = 2, thus
?(r, r) = ?ki=1
(k
i
)
? 2 = 2k ? 2 = 2k+1 = ?k+1i=1
(k+1
i
)
, i.e. all the relations until arity k + 1 2
TK functions can be applied to sentence parse
trees, therefore their usefulness for text processing
applications, e.g. question classification, is evident.
On the contrary, the SSTK applied to one PAS ex-
tracted from a text fragment may not be meaningful
since its representation needs to take into account all
the PASs that it contains. We address such problem
779
by defining a kernel on multiple PASs.
Let Pt and Pt? be the sets of PASs extracted from
the text fragment t and t?. We define:
Kall(Pt, Pt?) =
?
p?Pt
?
p??Pt?
SSTK(p, p?), (2)
While during the experiments (Sect. 4) the Kall
kernel is used to handle predicate argument struc-
tures, TK (Eq. 1) is used to process parse trees and
the linear kernel to handle POS and BOW features.
4 Experiments
The purpose of our experiments is to study the im-
pact of the new representations introduced earlier for
QA tasks. In particular, we focus on question clas-
sification and answer re-ranking for Web-based QA
systems.
In the question classification task, we extend pre-
vious studies, e.g. (Zhang and Lee, 2003; Moschitti,
2006), by testing a set of previously designed ker-
nels and their combination with our new Shallow Se-
mantic Tree Kernel. In the answer re-ranking task,
we approach the problem of detecting description
answers, among the most complex in the literature
(Cui et al, 2005; Kazawa et al, 2001).
The representations that we adopt are: bag-of-
words (BOW), bag-of-POS tags (POS), parse tree
(PT), predicate argument structure (PAS) and nested
PAS (PASN). BOW and POS are processed by
means of a linear kernel, PT is processed with TK,
PAS and PASN are processed by SSTK. We imple-
mented the proposed kernels in the SVM-light-TK
software available at ai-nlp.info.uniroma2.it/
moschitti/ which encodes tree kernel functions in
SVM-light (Joachims, 1999).
4.1 Question classification
As a first experiment, we focus on question classi-
fication, for which benchmarks and baseline results
are available (Zhang and Lee, 2003; Li and Roth,
2005). We design a question multi-classifier by
combining n binary SVMs3 according to the ONE-
vs-ALL scheme, where the final output class is the
one associated with the most probable prediction.
The PASs were automatically derived by our SRL
3We adopted the default regularization parameter (i.e., the
average of 1/||~x||) and tried a few cost-factor values to adjust
the rate between Precision and Recall on the development set.
system which achieves a 76% F1-measure (Mos-
chitti et al, 2005).
As benchmark data, we use the question train-
ing and test set available at: l2r.cs.uiuc.edu/
?cogcomp/Data/QA/QC/, where the test set are the
500 TREC 2001 test questions (Voorhees, 2001).
We refer to this split as UIUC. The performance of
the multi-classifier and the individual binary classi-
fiers is measured with accuracy resp. F1-measure.
To collect statistically significant information, we
run 10-fold cross validation on the 6,000 questions.
Features Accuracy (UIUC) Accuracy (c.v.)
PT 90.4 84.8?1.2
BOW 90.6 84.7?1.2
PAS 34.2 43.0?1.9
POS 26.4 32.4?2.1
PT+BOW 91.8 86.1?1.1
PT+BOW+POS 91.8 84.7?1.5
PAS+BOW 90.0 82.1?1.3
PAS+BOW+POS 88.8 81.0?1.5
Table 1: Accuracy of the question classifier with dif-
ferent feature combinations
Question classification results Table 1 shows the
accuracy of different question representations on the
UIUC split (Column 1) and the average accuracy ?
the corresponding confidence limit (at 90% signifi-
cance) on the cross validation splits (Column 2).(i)
The TK on PT and the linear kernel on BOW pro-
duce a very high result, i.e. about 90.5%. This is
higher than the best outcome derived in (Zhang and
Lee, 2003), i.e. 90%, obtained with a kernel combin-
ing BOW and PT on the same data. Combined with
PT, BOW reaches 91.8%, very close to the 92.5%
accuracy reached in (Li and Roth, 2005) using com-
plex semantic information from external resources.
(ii) The PAS feature provides no improvement. This
is mainly because at least half of the training and
test questions only contain the predicate ?to be?, for
which a PAS cannot be derived by a PB-based shal-
low semantic parser.
(iii) The 10-fold cross-validation experiments con-
firm the trends observed in the UIUC split. The
best model (according to statistical significance) is
PT+BOW, achieving an 86.1% average accuracy4.
4This value is lower than the UIUC split one as the UIUC
test set is not consistent with the training set (it contains the
780
4.2 Answer classification
Question classification does not allow to fully ex-
ploit the PAS potential since questions tend to be
short and with few verbal predicates (i.e. the only
ones that our SRL system can extract). A differ-
ent scenario is answer classification, i.e. deciding
if a passage/sentence correctly answers a question.
Here, the semantics to be generated by the classi-
fier are not constrained to a small taxonomy and an-
swer length may make the PT-based representation
too sparse.
We learn answer classification with a binary SVM
which determines if an answer is correct for the tar-
get question: here, the classification instances are
?question, answer? pairs. Each pair component can
be encoded with PT, BOW, PAS and PASN repre-
sentations (processed by previous kernels).
As test data, we collected the 138 TREC 2001 test
questions labeled as ?description? and for each, we
obtained a list of answer paragraphs extracted from
Web documents using YourQA. Each paragraph sen-
tence was manually evaluated based on whether it
contained an answer to the corresponding question.
Moreover, to simplify the classification problem, we
isolated for each paragraph the sentence which ob-
tained the maximal judgment (in case more than one
sentence in the paragraph had the same judgment,
we chose the first one). We collected a corpus con-
taining 1309 sentences, 416 of which ? labeled ?+1?
? answered the question either concisely or with
noise; the rest ? labeled ?-1?? were either irrele-
vant to the question or contained hints relating to the
question but could not be judged as valid answers5.
Answer classification results To test the impact
of our models on answer classification, we ran 5-fold
cross-validation, with the constraint that two pairs
?q, a1? and ?q, a2? associated with the same ques-
tion q could not be split between training and test-
ing. Hence, each reported value is the average over 5
different outcomes. The standard deviations ranged
TREC 2001 questions) and includes a larger percentage of eas-
ily classified question types, e.g. the numeric (22.6%) and de-
scription classes (27.6%) whose percentage in training is 16.4%
resp. 16.2%.
5For instance, given the question ?What are invertebrates??,
the sentence ?At least 99% of all animal species are inverte-
brates, comprising . . . ? was labeled ?-1? , while ?Invertebrates
are animals without backbones.? was labeled ?+1?.
  
  
   
   
    
    
   
   
  
  
      	  	                 

         







Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 65?68,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Detecting Compositionality in Multi-Word Expressions
Ioannis Korkontzelos
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
johnkork@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
suresh@cs.york.ac.uk
Abstract
Identifying whether a multi-word expres-
sion (MWE) is compositional or not is im-
portant for numerous NLP applications.
Sense induction can partition the context
of MWEs into semantic uses and there-
fore aid in deciding compositionality. We
propose an unsupervised system to ex-
plore this hypothesis on compound nom-
inals, proper names and adjective-noun
constructions, and evaluate the contribu-
tion of sense induction. The evaluation
set is derived from WordNet in a semi-
supervised way. Graph connectivity mea-
sures are employed for unsupervised pa-
rameter tuning.
1 Introduction and related work
Multi-word expressions (MWEs) are sequences of
words that tend to cooccur more frequently than
chance and are either idiosyncratic or decompos-
able into multiple simple words (Baldwin, 2006).
Deciding idiomaticity of MWEs is highly impor-
tant for machine translation, information retrieval,
question answering, lexical acquisition, parsing
and language generation.
Compositionality refers to the degree to which
the meaning of a MWE can be predicted by com-
bining the meanings of its components. Unlike
syntactic compositionality (e.g. by and large), se-
mantic compositionality is continuous (Baldwin,
2006).
In this paper, we propose a novel unsupervised
approach that compares the major senses of a
MWE and its semantic head using distributional
similarity measures to test the compositionality of
the MWE. These senses are induced by a graph
based sense induction system, whose parameters
are estimated in an unsupervised manner exploit-
ing a number of graph connectivity measures (Ko-
rkontzelos et al, 2009). Our method partitions the
context space and only uses the major senses, fil-
tering out minor senses. In our approach the only
language dependent components are a PoS tagger
and a parser.
There are several studies relevant to detecting
compositionality of noun-noun MWEs (Baldwin et
al., 2003) verb-particle constructions (Bannard et
al., 2003; McCarthy et al, 2003) and verb-noun
pairs (Katz and Giesbrecht, 2006). Datasets with
human compositionality judgements are available
for these MWE categories (Cook et al, 2008).
Here, we focus on compound nominals, proper
names and adjective-noun constructions.
Our contributions are three-fold: firstly, we ex-
perimentally show that sense induction can as-
sist in identifying compositional MWEs. Sec-
ondly, we show that unsupervised parameter tun-
ing (Korkontzelos et al, 2009) results in accuracy
that is comparable to the best manually selected
combination of parameters. Thirdly, we propose
a semi-supervised approach for extracting non-
compositional MWEs from WordNet, to decrease
annotation cost.
2 Proposed approach
Let us consider the non-compositional MWE ?red
carpet?. It mainly refers to a strip of red carpeting
laid down for dignitaries to walk on. However, it
is possible to encounter instances of ?red carpet?
referring to any carpet of red colour. Our method
first applies sense induction to identify the major
semantic uses (senses) of a MWE (?red carpet?)
and its semantic head (?carpet?). Then, it com-
pares these uses to decide MWE compositionality.
The more diverse these uses are, the more possi-
bly the MWE is non-compositional. Our algorithm
consists of 4 steps:
A. Corpora collection and preprocessing. Our
approach receives as input a MWE (e.g. ?red car-
pet?). The dependency output of Stanford Parser
(Klein and Manning, 2003) is used to locate the
65
Figure 1: ?red carpet?, sense induction example
MWE semantic head. Two different corpora are
collected (for the MWE and its semantic head).
Each consists of webtext snippets of length 15 to
200 tokens in which the MWE/semantic head ap-
pears. Given a MWE, a set of queries is created:
All synonyms of the MWE extracted from Word-
Net are collected
1
. The MWE is paired with each
synonym to create a set of queries. For each query,
snippets are collected by parsing the web-pages re-
turned by Yahoo!. The union of all snippets pro-
duces the MWE corpus. The corpus for a semantic
head is created equivalently.
To keep the computational time reasonable,
only the longest 3, 000 snippets are kept from each
corpus. Both corpora are PoS tagged (GENIA tag-
ger). In common with Agirre et al (2006), only
nouns are kept and lemmatized, since they are
more discriminative than other PoS.
B. Sense Induction methods can be broadly di-
vided into vector-space models and graph based
models. Sense induction methods are evaluated
under the SemEval-2007 framework (Agirre and
Soroa, 2007). We employ the collocational graph-
based sense induction of Klapaftis and Manand-
har (2008) in this work (henceforth referred to as
KM). The method consists of 3 stages:
Corpus preprocessing aims to capture nouns
that are contextually related to the target
MWE/head. Log-likelihood ratio (G
2
) (Dunning,
1993) with respect to a large reference corpus, Web
1T 5-gram Corpus (Brants and Franz, 2006), is
used to capture the contextually relevant nouns.
P
1
is the G
2
threshold below which nouns are re-
moved from corpora.
Graph creation. A collocation is defined as a
pair of nouns cooccuring within a snippet. Each
1
Thus, for ?red carpet?, corpora will be collected for ?red
carpet? and ?carpet?. The synonyms of ?red carpet? are
?rug?, ?carpet? and ?carpeting?
noun within a snippet is combined with every
other, generating
(
n
2
)
collocations. Each collo-
cation is represented as a weighted vertex. P
2
thresholds collocation frequencies and P
3
colloca-
tion weights. Weighted edges are drawn based on
cooccurrence of the corresponding vertices in one
or more snippets (e.g. w
8
and w
7,9
, fig. 1). In con-
trast to KM, frequencies for weighting vertices and
edges are obtained from Yahoo! web-page counts
to deal with data sparsity.
Graph clustering uses Chinese Whispers
2
(Bie-
mann, 2006) to cluster the graph. Each cluster now
represents a sense of the target word.
KM produces larger number of clusters (uses)
than expected. To reduce it we exploit the one
sense per collocation property (Yarowsky, 1995).
Given a cluster l
i
, we compute the set S
i
of snip-
pets that contain at least one collocation of l
i
. Any
clusters l
a
and l
b
are merged if S
a
? S
b
.
C. Comparing the induced senses. We used
two techniques to measure the distributional simi-
larity of major uses of the MWE and its semantic
head, both based on Jaccard coefficient (J). ?Ma-
jor use? denotes the cluster of collocations which
tags the most snippets. Lee (1999) shows that
J performs better than other symmetric similarity
measures such as cosine, Jensen-Shannon diver-
gence, etc. The first is J
c
= J(A,B) =
|A?B|
|A?B|
,
where A, B are sets of collocations. The second,
J
sn
, is based on the snippets that are tagged by
the induced uses. Let K
i
be the set of snippets in
which at least one collocation of the use i occurs.
J
sn
= J(K
j
,K
k
), where j, k are the major uses
of the MWE and its semantic head, respectively.
D. Determining compositionality. Given the
major uses of a MWE and its semantic head,
the MWE is considered as compositional, when
the corresponding distributional similarity mea-
sure (J
c
or J
sn
) value is above a parameter thresh-
old, sim. Otherwise, it is considered as non-
compositional.
3 Test set of MWEs
To the best of our knowledge there are no noun
compound datasets accompanied with composi-
tionality judgements available. Thus, we devel-
oped an algorithm to aid human annotation. For
each of the 52, 217 MWEs of WordNet 3.0 (Miller,
1995) we collected:
2
Chinese Whispers is not guaranteed to converge, thus
200 was adopted as the maximum number of iterations.
66
Non-compositional MWEs
agony aunt, black maria, dead end, dutch oven,
fish finger, fool?s paradise, goat?s rue, green light,
high jump, joint chiefs, lip service, living rock,
monkey puzzle, motor pool, prince Albert,
stocking stuffer, sweet bay, teddy boy, think tank
Compositional MWEs
box white oak, cartridge brass, common iguana,
closed chain, eastern pipistrel, field mushroom,
hard candy, king snake, labor camp, lemon tree,
life form, parenthesis-free notation, parking brake,
petit juror, relational adjective, taxonomic category,
telephone service, tea table, upland cotton
Table 1: Test set with compositionality annotation.
MWEs whose compositionality was successfully
detected by: (a) 1c1word baseline are in bold font,
(b) manual parameter selection are underlined and
(c) average cluster coefficient are in italics.
1. all synonyms of the MWE
2. all hypernyms of the MWE
3. sister-synsets of the MWE, within distance
3
3
4. synsets that are in holonymy or meronymy re-
lation to the MWE, within distance 3
If the semantic head of the MWE is also in the
above collection then the MWE is likely to be com-
positional, otherwise it is likely that the MWE is
non-compositional.
6, 287 MWEs were judged as potentially non-
compositional. We randomly chose 19 and
checked them manually. Those that were compo-
sitional were replaced by other randomly chosen
ones. The process was repeated until we ended up
with 19 non-compositional examples. Similarly,
19 negative examples that were judged as compo-
sitional were collected (Table 1).
4 Evaluation setting and results
The sense induction component of our algorithm
depends upon 3 parameters: P
1
is the G
2
threshold
below which noun are removed from corpora. P
2
thresholds collocation frequencies and P
3
colloca-
tion weights. We chose P
1
? {5, 10, 15}, P
2
?
{10
2
, 10
3
, 10
4
, 10
5
} and P
3
? {0.2, 0.3, 0.4}. For
reference, P
1
values of 3.84, 6.63, 10.83 and
15.13 correspond to G
2
values for confidence lev-
els of 95%, 99%, 99.9% and 99.99%, respectively.
To assess the performance of the proposed al-
gorithm we compute accuracy, the percentage of
MWEs whose compositionality was correctly de-
termined against the gold standard.
3
Locating sister synsets at distance D implies ascending
D steps and then descending D steps.
Figure 2: Proposed system and 1c1word accuracy.
Figure 3: Unweighted graph con/vity measures.
We compared the system?s performance against
a baseline, 1c1word, that assigns the whole graph
to a single cluster and no graph clustering is
performed. 1c1word corresponds to a relevant
SemEval-2007 baseline (Agirre and Soroa, 2007)
and helps in showing whether sense induction can
assist determining compositionality.
Our method was evaluated for each ?P
1
, P
2
, P
3
?
combination and similarity measures J
c
and J
sn
,
separately. We used our development set to deter-
mine if there are parameter values that verify our
hypothesis. Given a sim value (see section 2, last
paragraph), we chose the best performing parame-
ter combination manually.
The best results for manual parameter selection
were obtained for sim = 95% giving an accu-
racy of 68.42% for detecting non-compositional
MWEs. In all experiments, J
sn
outperforms J
c
.
With manually selected parameters, our system?s
accuracy is higher than 1c1word for all sim values
(5% points) (fig. 2, table 1). The initial hypothesis
holds; sense induction improves MWE composi-
tionality detection.
5 Unsupervised parameter tuning
We followed Korkontzelos et al (2009) to select
the ?best? parameters ?P
1
, P
2
, P
3
? for the collo-
cational graph of each MWE or head word. We
applied 8 graph connectivity measures (weighted
and unweighted versions of average degree, clus-
ter coefficient, graph entropy and edge density)
separately on each of the clusters (resulting from
the application of the chinese whispers algorithm).
Each graph connectivity measure assigns a
score to each cluster. We averaged the scores over
67
Figure 4: Weighted graph connectivity measures.
the clusters from the same graph. For each con-
nectivity measure, we chose the parameter combi-
nation ?P
1
, P
2
, P
3
? that gave the highest score.
While manual parameter tuning chooses a sin-
gle globally best set of parameters (see section 4),
the graph connectivity measures generate different
values of ?P
1
, P
2
, P
3
? for each graph.
5.1 Evaluation results
The best performing distributional similarity mea-
sure is J
sn
. Unweighted versions of graph con-
nectivity measures perform better than weighted
ones. Figures 3 and 4 present a comparison be-
tween the unweighted and weighted versions of
all graph connectivity measures, respectively, for
all sim values. Average cluster coefficient per-
forms better or equally well to the other graph
connectivity measures for all sim values (except
for sim ? [90%, 100%]). The accuracy of aver-
age cluster coefficient is equal (68.42%) to that
of manual parameter selection (section 4, table
1). The second best performing unweighted graph
connectivity measures is average graph entropy.
For weighted graph connectivity measures, aver-
age graph entropy performs best, followed by av-
erage weighted clustering coefficient.
6 Conclusion and Future Work
We hypothesized that sense induction can assist in
identifying compositional MWEs. We introduced
an unsupervised system to experimentally explore
the hypothesis, and showed that it holds. We
proposed a semi-supervised way to extract non-
compositional MWEs from WordNet. We showed
that graph connectivity measures can be success-
fully employed to perform unsupervised parame-
ter tuning of our system. It would be interesting
to explore ways to substitute querying Yahoo! so
as to make the system quicker. Experimentation
with more sophisticated graph connectivity mea-
sures could possibly improve accuracy.
References
E. Agirre and A. Soroa. 2007. Semeval-2007, task
02: Evaluating WSI and discrimination systems. In
proceedings of SemEval-2007. ACL.
E. Agirre, D. Mart??nez, O. de Lacalle, and A. Soroa.
2006. Two graph-based algorithms for state-of-the-
art WSD. In proceedings of EMNLP-2006. ACL.
T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows.
2003. An empirical model of MWE decomposabil-
ity. In proceedings of the MWE workshop. ACL.
T. Baldwin. 2006. Compositionality and MWEs: Six
of one, half a dozen of the other? In proceedings of
the MWE workshop. ACL.
C. Bannard, T. Baldwin, and A. Lascarides. 2003.
A statistical approach to the semantics of verb-
particles. In proceedings of the MWE workshop.
ACL.
C. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to
NLP problems. In proceedings of TextGraphs. ACL.
T. Brants and A. Franz. 2006. Web 1t 5-gram corpus,
version 1. Technical report, Google Research.
P. Cook, A. Fazly, and S. Stevenson. 2008. The VNC-
Tokens Dataset. In proceedings of the MWE work-
shop. ACL.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
G. Katz and E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional MWEs using latent se-
mantic analysis. In proceedings of the MWE work-
shop. ACL.
I. P. Klapaftis and S. Manandhar. 2008. WSI using
graphs of collocations. In proceedings of ECAI-
2008.
D. Klein and C. Manning. 2003. Fast exact inference
with a factored model for natural language parsing.
In proceedings of NIPS 15. MIT Press.
I. Korkontzelos, I. Klapaftis, and S. Manandhar. 2009.
Graph connectivity measures for unsupervised pa-
rameter tuning of graph-based sense induction sys-
tems. In proceedings of the UMSLLS Workshop,
NAACL HLT 2009.
L. Lee. 1999. Measures of distributional similarity. In
proceedings of ACL.
D. McCarthy, B. Keller, and J. Carroll. 2003. De-
tecting a continuum of compositionality in phrasal
verbs. In proceedings of the MWE workshop. ACL.
G. A. Miller. 1995. WordNet: a lexical database for
English. ACM, 38(11):39?41.
D. Yarowsky. 1995. Unsupervised WSD rivaling su-
pervised methods. In proceedings of ACL.
68
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 93?96,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Automatic Generation of Information-seeking Questions Using Concept
Clusters
Shuguang Li
Department of Computer Science
University of York, YO10 5DD, UK
sgli@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
University of York, YO10 5DD, UK
suresh@cs.york.ac.uk
Abstract
One of the basic problems of efficiently
generating information-seeking dialogue
in interactive question answering is to find
the topic of an information-seeking ques-
tion with respect to the answer documents.
In this paper we propose an approach to
solving this problem using concept clus-
ters. Our empirical results on TREC col-
lections and our ambiguous question col-
lection shows that this approach can be
successfully employed to handle ambigu-
ous and list questions.
1 Introduction
Question Answering systems have received a lot
of interest from NLP researchers during the past
years. But it is often the case that traditional QA
systems cannot satisfy the information needs of
the users as the question processing part may fail
to properly classify the question or the informa-
tion needed for extracting and generating the an-
swer is either implicit or not present in the ques-
tion. In such cases, interactive dialogue is needed
to clarify the information needs and reformulate
the question in a way that will help the system to
find the correct answer.
Due to the fact that casual users often ask ques-
tions with ambiguity and vagueness, and most of
the questions have multiple answers, current QA
systems return a list of answers for most questions.
The answers for one question usually belong to
different topics. In order to satisfy the information
needs of the user, information-seeking dialogue
should take advantage of the inherent grouping of
the answers.
Several methods have been investigated for gen-
erating topics for questions in information-seeking
dialogue. Hori et al (2003) proposed a method
for generating the topics for disambiguation ques-
tions. The scores are computed purely based on
the syntactic ambiguity present in the question.
Phrases that are not modified by other phrases are
considered to be highly ambiguous while phrases
that are modified are considered less ambiguous.
Small et al (2004) utilizes clarification dialogue
to reduce the misunderstanding of the questions
between the HITIQA system and the user. The
topics for such clarification questions are based
on manually constructed topic frames. Similarly
in (Hickl et al, 2006), suggestions are made to
users in the form of predictive question and answer
pairs (known as QUABs) which are either gener-
ated automatically from the set of documents re-
turned for a query (using techniques first described
in (Harabagiu et al, 2005), or are selected from a
large database of questions-answer pairs created
offline (prior to a dialogue) by human annotators.
In Curtis et al (2005), query expansion of the
question based on Cyc Knowledge is used to gen-
erate topics for clarification questions. In Duan et
al. (2008), the tree-cutting model is used to select
topics from a set of relevant questions from Yahoo
Answers.
None of the above methods consider the con-
texts of the list of answers in the documents re-
turned by QA systems. The topic of a good
information-seeking question should not only be
relevant to the original question but also should be
able to distinguish each answer from the others so
that the new information can reduce the ambiguity
and vagueness in the original question. Instead of
using traditional clustering methods on categoriza-
tion of web results, we present a new topic gener-
ation approach using concept clusters and a sepa-
rability scoring mechanism for ranking the topics.
2 Topic Generation Based on Concept
Clustering
Text categorization and clustering especially hier-
archical clustering are predominant approaches to
organizing large amounts of information into top-
93
ics or categories. But the main issue of catego-
rization is that it is still difficult to automatically
construct a good category structure, and manu-
ally formed hierarchies are usually small. And the
main challenge of clustering algorithms is that the
automatically formed cluster hierarchy may be un-
readable or meaningless for human users. In order
to overcome the limits of the above methods, we
propose a concept clusters method and choose the
labels of the clusters as topics.
Recent research on automatically extracting
concepts and clusters of words from large database
makes it feasible to grow a big set of concept clus-
ters. Clustering by Committee (CBC) in Pantel
et al (2002) made use of the fact that words in
the same cluster tend to appear in similar con-
texts. Pasca et al (2008) utilized Google logs and
lexico-syntactic patterns to get clusters with labels
simultaneously. Google also released Google Sets
which can be used to grow concept clusters with
different sizes.
Currently our clusters are the union of the sets
generated by the above three approaches, and
we label them using the method described in
Pasca et al (2008). We define the concept
clusters in our collection as {C
1
, C
2
, ..., C
n
}.
C
i
={e
i1
, e
i2
, ..., e
im
}, e
ij
is j
th
subtopic of clus-
ter C
i
and m is the size of C
i
.
We designed our system to take a question
and its corresponding list of answers as input
and then retrieve Google snippet documents for
each of the answers with respect to the ques-
tion. In a vectorspace model, a document is
represented by a vector of keywords extracted
from the document, with associated weights rep-
resenting the importance of the keywords in the
document and within the whole document col-
lection. A document D
j
in the collection is
represented as {W
0j
,W
1j
, ...,W
nj
}, and W
ij
is
the weight of word i in document j. Here we
use our concept clusters to create concept clus-
ter vectors. A document D
j
now is represented
as <WC
1j
,WC
2j
, ...,WC
nj
>, and WC
ij
is the
score vector of document D
j
for concept cluster
C
i
:
WC
ij
= <Score
j
(e
i1
), Score
j
(e
i2
), ...Score
j
(e
im
)>
Score
j
(e
ip
) is the weight of subtopic e
ip
of cluster C
i
in
document D
j
.
Currently we use tf-idf scheme (Yang et al, 1999)
to calculate the weight of subtopics.
3 Concept Cluster Separability Measure
We view different concept clusters from the con-
texts of the answers as different groups of fea-
tures that can be used to classify the answers docu-
ments. We rank different context features by their
separability on the answers. Currently our system
retrieves the answers from Google search snippets,
and each snippet is quite short. So we combine the
top 50 snippets for one answer into one document.
One answer is associated with one such big doc-
ument. We propose the following interclass mea-
sure to compare the separability of different clus-
ters:
Score(C
i
) =
D
N
N
?
p<q
Dis(D
p
, D
q
),
D is the Dimension Penalty score, D =
1
M
,
M is the size of cluster C
i
,
N is the combined total number of classes from all the answers
Dis(D
p
, D
q
) =
?
n
?
m=0
(Score
p
(e
im
)? Score
q
(e
im
))
2
We introduce D, the ?Dimension Penalty? score
which gives higher penalty to bigger clusters. Cur-
rently we use the reciprocal of the size of the clus-
ter. The second part is the average pairwise dis-
tance between answers. N is the total number of
classes of the answers. Next we describe in detail
how to use the concept cluster vectors and separa-
bility measure to rank clusters.
4 Cluster Ranking Algorithm
Input:
Answer set A = {A
1
, A
2
, ..., A
p
};
Documents set D = {D
1
, D
2
, ..., D
p
} associated with answer set A;
Concept cluster set CS = {C
i
| some of the subtopics from C
i
occurs in D};
Threshold ?
1
, ?
2
; The question Q;
Concept cluster set QS = {C
i
| some of the subtopics from C
i
occurs in Q}
Output:
T = {< C
i
, Score >}, a set of pairs of a concept cluster and its ranking
score;
QS;
Variables: X , Y ;
Steps:
1. CS = CS ?QS
2. For each cluster C
i
in CS
3. X = No. of answers in which context subtopics from C
i
are present;
4. Y = No. of subtopics from C
i
that occurs in the answers? contexts;
5. If X < ?
1
or Y < ?
2
6. delete C
i
from CS
7. continue
8. Represent every document as a concept cluster vector on C
i
(see
section 2)
9. Calculate the Score(C
i
) using our separability measure
10. Store < C
i
, Score > in T
11. return T the medoid.
Figure 1: Concept Cluster Ranking Algorithm
Figure 1 describes the algorithm for rank-
ing concept clusters based on their separabil-
ity score. This algorithm starts by deleting all
94
the clusters which are in QS from CS so that
we only focus on the context clusters whose
subtopics are present in the answers. However
in some cases this assumption is incorrect
1
. Tak-
ing the question shown in Table 2 for example,
there are 6 answers for question LQ1, and in
Step 1 CS = {C
41
American State, C
1522
Times,
C
414
Tournament, C
10004
Y ear, ...} and QS =
{C
4545
Event}. Using cluster C
414
(see Table 2),
D = {D
1
{Daytona 500, 24 Hours of Daytona,
24 Hours of Le Mans, ...}, D
2
{3M Performance
400, Cummins 200, ...}, D
3
{Indy 500, Truck se-
ries, ...}, ...}, and hence the vector representa-
tion for a given document D
j
using C
414
will
be <Score
j
(indy 500), Score
j
(Cummins 200),
Score
j
(daytona 500), ...>.
In Step 2 through 11 from Figure 1, for each
context cluster C
i
in CS we calculate X (the num-
ber of answers in which context subtopics from C
i
are present), and Y (the number of subtopics from
C
i
that occurs in the answers? contexts). We would
like the clusters to hold two characteristics: (a) at
least occur in ?
1
answers as we want to have a
cluster whose subtopics are widely distributed in
the answers. Currently we set ?
1
as half the num-
ber of the answers; (b) at least have ?
2
subtopics
occurring in the answers? documents. We set ?
2
as the number of the answers. For example, for
cluster C
414
, X = 6, Y = 10, ?
1
= 3 and ?
2
=
6, so this cluster has the above two characteris-
tics. If a cluster has the above two characteris-
tics, we use our separability measure described in
section 3 to calculate a score for this cluster. The
size of C
414
is 11, so Score(C
414
) =
1
11?6
?
N
p<q
Dis(D
p
, D
q
). Ranking the clusters based on this
separability score means we will select a clus-
ter which has several subtopics occurring in the
answers and the answers are distinguished from
each other because they belong to these different
subtopics. The top three clusters for question LQ1
is shown in Table 2.
5 Experiment
5.1 Data Set and Baseline Method
To the best of our knowledge, the only available
test data of multiple answer questions are list ques-
tions from TREC 2004-2007 Data. For our first
1
For the question ?In which movies did Christopher
Reeve acted??, cluster Actor{Christopher Reeve, michael
caine, anthony hopkins, ...} is quite useful. While for ?Which
country won the football world cup?? cluster Sports{football,
hockey, ...} is useless.
list question collection we randomly selected 200
questions which have at least 3 answers. We
changed the list questions to factoid ones with
additional words from their context questions to
eliminate ellipsis and reference. For the ambigu-
ous questions, we manually choose 200 questions
from TREC 1999-2007 data and some questions
discussed as examples in Hori et al (2003) and
Burger et al (2001).
We compare our approach with a baseline
method. Our baseline system does not rank the
clusters by the above separability score instead it
prefers the cluster which occurs in more answers
and have more subtopics distributed in the answer
documents. If we still use X to represent the num-
ber of answers in which context subtopics from
one cluster are present and Y to represent the num-
ber of subtopics from this cluster that occurs in the
answers? contexts, for the baseline system, we will
use X ? Y to rank all the concept clusters found
in the contexts.
5.2 Results and Error Analysis
We applied our algorithm on the two collections
of questions. Two assessors were involved in the
manual judgments with an inter-rater agreement
of 97%. For each approach, we obtained the top
20 clusters based on their scores. Given a clus-
ter with its subtopics in the contexts of the an-
swers, an assessor manually labeled each cluster
?good? or ?bad?. If it is labeled ?good?, the cluster
is deemed relevant to the question and the clus-
ter?s label could be used as dialogue seeking ques-
tion?s topic to distinguish one answer from the oth-
ers. Otherwise, the assessor will label a cluster as
?bad?. We use the above two ranking approaches
to rank the clusters for each question. Table 1 pro-
vides the statistics of the performance on the the
two question collection. List B means the base-
line method on the list question set while Am-
biguous S means our separability method on the
ambiguous questions. The ?MAP? column is the
mean of average precisions over the set of clusters.
The ?P@1? column is the precision of the top one
cluster while the ?P@3? column is the precision
of the top three clusters
2
. The ?Err@3? column is
the percentage of questions whose top three clus-
ters are all labeled ?bad?. One example associated
with the manually constructed desirable questions
2
?P@3? is the number of ?good? clusters out of the top
three clusters
95
Table 1: Experiment results
Methods MAP P@1 P@3 Err@3
List B 41.3% 42.1% 27.7% 33.0%
List S 60.3% 90.0% 81.3% 11.0%
Ambiguous B 31.1% 33.2% 21.8% 47.1%
Ambiguous S 53.6% 71.1% 64.2% 29.7%
Table 2: TREC Question Examples
LQ1: Who is the winners of the NASCAR races?
1
st
C
414
(Tournament):{indy 500, Cummins 200, day-
tona 500, ...}
Q1 Which Tournament are you interested in?
2
nd
C
41
(American State):{houston, baltimore, los an-
geles, ...}
Q2 Which American State were the races held?
3
rd
C
1522
(Times):{once, twice, three times, ...}
Q3 How many times did the winner win?
is shown in Table 2.
From Table 1, we can see that our approach
outperforms the baseline approach in terms of all
the measures. We can see that 11% of the ques-
tions have no ?good? clusters. Further analysis
of the answer documents shows that the ?bad?
clusters fall into four categories. First, there are
noisy subtopics in some clusters. Second, some
questions? clusters are all labeled ?bad? because
the contexts for different answers are too simi-
lar. Third, unstructured web document soften con-
tain multiple subtopics. This means that different
subtopics are in the context of the same answer.
Currently we only look for context words while
not using any scheme to specify whether there is a
relationship between the answer and the subtopics.
Finally, for other ?bad? cases and the questions
with no good clusters all of the separability scores
are quite low. This is because the answers fall
into different topics which do not share a common
topic in our cluster collection.
6 Conclusion and Discussion
This paper proposes a new approach to solve
the problem of generating an information-seeking
question?s topic using concept clusters that can be
used in a clarification dialogue to handle ambigu-
ous questions. Our empirical results show that this
approach leads to good performance on TREC col-
lections and our ambiguous question collections.
The contribution of this paper are: (1) a new con-
cept cluster method that maps a document into a
vector of subtopics; (2) a new ranking scheme to
rank the context clusters according to their sepa-
rability. The labels of the chosen clusters can be
used as topics in an information-seeking question.
Finally our approach shows significant improve-
ment (nearly 48% points) over comparable base-
line system.
But currently we only consider the context clus-
ters while ignoring the clusters associated with the
questions. In the future, we will further investigate
the relationships between the concept clusters in
the question and the answers.
References
Tiphaine Dalmas, Bonnie L. Webber: Answer com-
parison in automated question answering. J. Applied
Logic (JAPLL) 5(1):104-120, (2007).
Chiori Hori, Sadaoki Furui: A new approach to auto-
matic speech summarization. IEEE Transactions on
Multimedia (TMM) 5(3):368-378, (2003).
Sharon Small and Tomek Strzalkowski, HITIQA:
A Data Driven Approach to Interactive Analyti-
cal Question Answering, in Proceedings of HLT-
NAACL 2004: Short Papers, (2004).
Andrew Hickl, Patrick Wang, John Lehmann, Sanda
M. Harabagiu: FERRET: Interactive Question-
Answering for Real-World Environments. ACL,
(2006).
Sanda M. Harabagiu, Andrew Hickl, John Lehmann,
Dan I. Moldovan: Experiments with Interactive
Question-Answering. ACL, (2005).
John Burger et al: Issues, Tasks and Program Struc-
tures to Roadmap Research in Question and An-
swering (Q&A),DARPA/NSF committee publica-
tion, (2001).
Patrick Pantel, Dekang Lin: Document clustering with
committees. SIGIR 2002:199-206, (2002).
Marius Pasca and Benjamin Van Durme: Weakly-
Supervised Acquisition of Open-Domain Classes
and Class Attributes from Web Documents and
Query Logs. ACL, (2008).
Sanda M. Harabagiu, Andrew Hickl, V. Finley La-
catusu: Satisfying information needs with multi-
document summaries. Inf. Process. Manage. (IPM)
43(6):1619-1642, (2007).
Huizhong Duan, Yunbo Cao, Chin-Yew Lin and Yong
Yu: Searching Questions by Identifying Question
Topic and Question Focus. ACL, (2008).
Jon Curtis, G. Matthews and D. Baxter: On the Effec-
tive Use of Cyc in a Question Answering System.
IJCAI Workshop on Knowledge and Reasoning for
Answering Questions, Edinburgh, (2005).
96
A Psychologically Plausible and Computationally Effective Approach to
Learning Syntax
Stephen Watkinson and Suresh Manandhar,
Department of Computer Science,
University of York,
York YO10 5DD,
UK.
Abstract
Computational learning of natural lan-
guage is often attempted without using
the knowledge available from other re-
search areas such as psychology and
linguistics. This can lead to systems
that solve problems that are neither
theoretically or practically useful. In
this paper we present a system CLL
which aims to learn natural language
syntax in a way that is both compu-
tationally effective and psychologically
plausible. This theoretically plausible
system can also perform the practically
useful task of unsupervised learning of
syntax. CLL has then been applied to
a corpus of declarative sentences from
the Penn Treebank (Marcus et al, 1993;
Marcus et al, 1994) on which it has
been shown to perform comparatively
well with respect to much less psycho-
logically plausible systems, which are
significantly more supervised and are
applied to somewhat simpler problems.
1 Introduction
Computational learning of natural language can
be considered from two common perspectives.
Firstly, there is the psychological perspective,
which leads to the investigation of learning prob-
lems similar to those faced by people and the
building of systems that seek to model human lan-
guage learning faculties. Secondly, there is the
computational perspective, which seeks to build
systems that effectively solve language learning
problems that are of practical importance.
In principle, there is significant overlap be-
tween these two perspectives. The most common
language learning problems that we wish to solve
computationally are frequently those that humans
have to solve. For example when humans learn
language, especially syntax, it seems to be in a
mostly unsupervised setting i.e. there is no an-
notation of training examples. From a computa-
tional perspective, while there are some annotated
resources available, in general we have very large
amounts of unannotated text available from which
we desire to be able to extract grammars, mean-
ing etc. Given this overlap, it seems wise to in-
vestigate what we know of the human approach,
as humans are good at solving these problems.
In this work we present a system for learning
syntax that seeks to maintain both the psycho-
logical and computational perspectives. We also
show that this is an effective way to build natural
language learning systems. We represent the syn-
tactic knowledge using the Categorial Grammar
(CG) formalism, so in Section 2 we introduce CG.
In Section 3 we aim to define the problem that is
to be solved in a way that is psychologically plau-
sible. This is followed in Section 4 by the descrip-
tion of CLL a computational effective solution to
the problem, which we maintain is also reason-
ably psychologically plausible. Related work is
discussed in Section 5. The results of experiments
using CLL on examples from the Penn Treebank
are presented in Section 6 and we draw some con-
clusions from this work in Section 7.
2 Categorial Grammar
Categorial Grammar (CG) (Steedman, 1993;
Wood, 1993) provides a functional approach to
lexicalised grammar, and so, can be thought of as
defining a syntactic calculus. Below we describe
the basic (AB) CG.
There is a set of atomic categories in CG, which
are usually nouns (n), noun phrases (np) and sen-
tences (s). It is then possible to build up complex
categories using the two slash operators ?/? and
?
 
?. If A and B are categories then A/B is a cate-
gory and A
 
B is a category. With basic CG there
are just two rules for combining categories: the
forward (FA) and backward (BA) functional ap-
plication rules.
   
	

 
   

In Figure 1 the parse derivation for ?John ate the
apple? is presented, which shows examples of the
types of categories that words can take and also
how those categories are combined using the ap-
plication rules.
ate the apple
np (s\np)/np np/n n
np
s\np
s
FA
FA
BA
John
Figure 1: A Example Parse in Pure CG
Categorial grammar does not handle compound
noun phrases very well, so we have added some
simple combination rules that allow the possibil-
ity of joining adjacent nouns and noun phrases.
Perhaps the main advantage of using a lexi-
calised formalism such as CG for this task is that
the learning of the grammar and the learning of
the lexicon is one task. CG will also easily al-
low extensions such that new categories could be
generated or that category schema could be used.
3 A Plausible Problem
The desire in this work, is to show that a computa-
tionally effective system, in this case CLL, can be
built in such a way that both the problem it solves
and the way it is implemented are psychologically
plausible. We would also suggest that defining the
problem in this way leads to a practically useful
problem being attempted.
Initially we seek to define the problem in a
psychologically plausible way. The aim is to in-
duce a broad coverage grammar for English from
a set of appropriate examples. Beyond this, the
problem can to some extent be defined by the
knowledge the learner already has; the informa-
tion that is available in the environment and the
knowledge which is to be learned. Psychology
and psycholinguistics provide us with a signifi-
cant amount of data from which we may derive
a fairly good picture of how the problem is de-
fined for humans. In particular, we will concen-
trate on a child?s acquisition of their first language
and how this relates to a computational model, as
this seems to be the point at which human lan-
guage acquisition is at its most efficient.
3.1 The Environment
With respect to the environment in which a child
learns, we will concentrate on two questions.
1. What examples of language are children ex-
posed to?
2. What kind of language teaching do children
experience?
It is clear that children experience positive ex-
amples of syntax i.e. all the language utterances
they hear, although these may be somewhat noisy
(people make lots of mistakes). Children do not,
however, experience negative examples, as peo-
ple do not (at least in any consistent way) present
ungrammatical examples and mark them as incor-
rect.
?From a syntactic perspective, examples ap-
pear to have little discernible annotation. Pinker
(Pinker, 1990) summarises what seems to be the
only evidence that children receive structural in-
formation. It is suggested that structural informa-
tion may be obtained by the infant from the ex-
aggerated intonation which adults use when talk-
ing to children. While there may be a link, it is
not clear what it is and it is certain that complete
structures for sentences cannot be considered to
be available, as there is not enough information in
intonation alone.
Hence, we have defined a learning setting that
is both positive examples only and unsupervised.
However, there has been some suggestion that
negative evidence may be available in the form
of parental correction. This leads to issues of lan-
guage teaching.
It is suggested that the language presented
to children is in fact very detailed and struc-
tured. The motherese hypothesis or child directed
speech (Harley, 1995; Pinker, 1990; Atkinson,
1996), proposes that, starting with very simple
language, adults gradually increase the complex-
ity of the language they use with children, such
that they actually provide children with a struc-
tured set of language lessons. The theory is based
upon research that shows that adults use a differ-
ent style of speech with infants than with other
adults (Snow and Ferguson, 1977).
However, Pinker (Pinker, 1990) provides argu-
ments against the acceptance of the Motherese
hypothesis. Firstly, although it may appear that
the language is simplified, in fact the language
used is syntactically complex ? for example it
contains a lot of questions. Secondly, there exist
societies where children are not considered worth
talking to until they can talk. Hence, there is
no motherese and only adult-to-adult speech ex-
amples which infants hear and from which they
have to acquire their language. These children do
not learn language any slower than the children
who are exposed to motherese. Atkinson (Atkin-
son, 1996) provides further arguments against the
motherese hypothesis, suggesting that making the
input simpler would make learning more difficult.
For the simpler the input is, the less information
is contained within it and so there is less informa-
tion from which to learn.
An alternative suggestion for the provision
of teaching is that negative evidence is actually
available to the child in the form of feedback or
correction from parents. This model was tested by
Brown and Hanlon (Brown and Hanlon, 1979) by
studying transcripts of parent-child conversations.
They studied adults responses to childrens? gram-
matical and ungrammatical sentences and could
find no correlation between children?s grammati-
cal sentences and parent?s encouragement. They
even found that parents do not understand chil-
dren?s well-formed questions much better than
their ill-formed questions. Pinker (Pinker, 1990)
reports that these results have been replicated.
This can only lead to the conclusion that there is
no significant negative evidence available to the
infant attempting to learn syntax.
Hence, we have a learner that is unsupervised,
positive only and does not have a teacher. In prac-
tice this means that we build a system that learns
from an unannotated corpus of examples of a lan-
guage (in this case we use unannotated examples
from the Penn Treebank) and there is no oracle or
teacher involved.
3.2 The Learner?s Knowledge
A child can be considered to have two types of
knowledge to bring to the problem. Firstly there
may be some innate knowledge that is built into
the human brain, which is used in determining
the language learning process. Secondly, there is
knowledge that the child has already acquired.
The issue of a child?s innate knowledge has
been the subject of a significant debate, which we
do not have the space to do justice to here. Instead
we will present the approach that we will take and
the reasons for following it, while accepting that
there will be those who will disagree.
The poverty of stimulus argument (Chomsky,
1980; Carroll, 1994) suggests that the environ-
ment simply does not provide enough informa-
tion for a learner to be able to select between
possible grammars. Hence, it seems that there
needs to be some internal bias. Further evidence
for this is the strong similarity between natu-
ral languages with respect to syntax, which has
led Chomsky to hypothesise that all humans are
born with a Universal Grammar (Chomsky, 1965;
Chomsky, 1972; Chomsky, 1986) which deter-
mines the search space of possible grammars for
languages. This is supported further by the Lan-
guage Bioprogram Hypothesis (LBH) of Bicker-
ton (Bickerton, 1984), who analysed creoles, the
languages that develop in communities where dif-
ferent nationalities with different languages work
alongside each other. Initially, in such contexts, a
pigeon develops, which is a very limited language
that combines elements of both languages found
in the community. The pigeon has very limited
syntactic structures. The next generation devel-
ops the pigeon into a full language ? the creole.
Bickerton (Bickerton, 1984) found that the cre-
oles, developing from syntactically impoverished
language examples as they do, actually contain
syntactic structures not available to the learners
from their pigeon environment. These structures
show a strong similarity to the syntactic structures
of other natural languages. Bickerton (Bickerton,
1984) states:
?the most cogent explanation of this
similarity is that it derives from the
structure of a species-specific program
for language, genetically coded and ex-
pressed, in ways still largely mysteri-
ous, in the structures and modes of op-
eration of the human brain.?
Practically, there are a variety of options for
providing a suitable level of innate knowledge.
By choosing a lexicalised grammar (see Sec-
tion 2) we have allowed the system to have a few
basic rules for word combination and a set of pos-
sible categories for words. Currently, the use of
a complete set of possible lexical categories is
perhaps too strong a bias to be psychologically
plausible. In future we will look at either gener-
ating categories, or using category schemas, both
of which might be more plausible.
The second type of knowledge available to the
learner is that which has already been learned. We
can, to some extent, determine this from develop-
mental psychology. Before the stage of learning
syntax children have already learned a wide vari-
ety of words with some notion of their meaning
(Carroll, 1994). They then seem to be beginning
to use single words to communicate more than
just the meaning of the word (Rodgon, 1976; Car-
roll, 1994) and then they begin to acquire syntax.
In terms of a learning system this would sug-
gest the availability of some initial lexical infor-
mation like word groupings or some bootstrap-
ping lexicon. Here we present results using a sys-
tem that has a small initial lexicon that it is as-
sumed that the child has learned. We are also in-
vestigating using word grouping information.
3.3 What is to be learned?
Given the knowledge that is available to the
learner and the environment from which the
learner receives examples of the language, the
learner is left with the task of learning a complex,
i.e. lexicalised, lexicon.
Using CG means that we are aiming to build a
lexicon that contains the required CG category or
categories for each word, which defines the syn-
tactic role or roles of that word. In future, we may
look at extending the grammar to include more
detail, so that the syntactic roles of words are de-
fined more accurately.
Interestingly, this leads us to a practically inter-
esting problem. Given the amount of unannotated
text available for a variety of different languages
and for a variety of different domains, it would
be very useful to have a system that could extract
grammars from selections of such text.
4 A Computationally Effective Solution
The system we have developed is shown diagram-
matically in Figure 2. In the following sections
we explain the learning setting and the learning
procedure respectively.
4.1 The Learning Setting
The input to the learning setting has five parts,
which are discussed below.
The Corpus The corpus is a set of positive ex-
amples represented in Prolog as facts containing
a list of words e.g.
ex([mary, loved, a, computer]).
The Lexicon The lexicon is initially empty,
apart from a small set of closed-class words used
to bootstrap the process, as this is what the learner
induces. It is stored by the learner as a set of Pro-
log facts of the form:
lex(Word, Category, Frequency).
Where Word is a word, Category is a Prolog
representation of the CG category assigned to that
word and Frequency is the number of times
this category has been assigned to this word up to
the current point in the learning process, or in the
case of the initial closed-class words a probability
distribution is predefined..
The Rules The CG functional application rules
and compound noun phrase rules (see Section 2)
are supplied to the learner. Extra rules may be
added in future for fuller grammatical coverage.
The Categories The learner has a complete set
of the categories that can be assigned to a word in
the lexicon.
Parsed
N most probable parses
Rules
&
Categories
Parser
Probabilistic
Examples
Lexicon
Current
Example
Corpus
Parse
Selector
Lexicon
Modifier
Figure 2: A Diagram of the Structure of the Learner
The Parser The system employs a  -best
probabilistic chart parser, developed from a
standard stochastic CKY algorithm taken from
Collins (Collins, 1999). The probability of a word
being assigned a category is based on the relative
frequency, which is calculated from the current
lexicon. Simple smoothing is used to allow for
unseen lexical entries. The probabilities of the en-
tries in the initial lexicon are predefined.
Each non-lexical edge in the chart has a proba-
bility calculated by multiplying the probabilities
of the two edges that are combined to form it.
Edges between two vertices are not added if there
are  edges labelled with the same category and
a higher probability, between the same two ver-
tices (if one has a lower probability it is replaced).
Also, for efficiency, edges are not added between
vertices if there is an edge already in place with
a much higher probability. The chart in Figure 3
shows examples of edges that would not be added.
The top half of the chart shows one parse and the
bottom half another. If  was set to  then the
dashed edge spanning all the vertices would not
be added, as it has a lower probability than the
other s edge covering the same vertices. Simi-
larly, the dashed edge between the first and third
vertices would not be added, as the probability of
the n is so much lower than the probability of the
np.
np - 0.1
s - 0.512
s\np - 0.8
np - 0.64
n - 0.8
ran
np - 0.1
s - 0.0009
s\np - 0.009
n - 0.0008
n/n - 0.001
np/n - 0.8
(s\np)/np - 0.09
manthe
Figure 3: Example chart showing edge pruning
4.2 The Learning Procedure
Having described the various components with
which the learner is provided, we now describe
how they are used in the learning procedure.
Parsing the Examples Examples are taken
from the corpus one at a time and parsed. Each
example is stored with the group of parses gener-
ated for it, so they can be efficiently accessed in
future. The parse that is selected (see below) as
the current best parse is maintained at the head of
this group. The head parse contributes informa-
tion to the lexicon and annotates the corpus. The
parses are also used extensively for the efficiency
of the parse selection module, as will be described
below. When the parser fails to find an analysis of
an example, either because it is ungrammatical, or
because of the incompleteness of the coverage of
the grammar, the system skips to the next exam-
ple.
The Parse Selector Each of the  -best parses is
considered in turn to determine which can be used
to make the most compressive lexicon (by a given
measure), following the compression as learning
approach of, for example, Li and Vita?nyi (Li and
Vita?nyi, 1993) and Wolff (Wolff, 1987), who used
it with respect to language learning. The current
size measure for the lexicon is the sum of the sizes
of the categories for each lexical entry. The size
of a category is the number of atomic categories
within it. It is not enough to look at what a parse
would add to the lexicon. The effect on previ-
ous parses of the changes in lexicon frequencies
must also be propagated by reparsing examples
that may be affected.
This may appear an expensive way of deter-
mining which parse to select, but it enables the
system to calculate the most compressive lexicon
and up-to-date annotation for the corpus. We can
also use previous parses to reduce some of the
parsing workload.
Lexicon Modification The final stage takes the
current lexicon and replaces it with the lexicon
built with the selected parse.
The whole process is repeated until all the ex-
amples have been parsed. The final lexicon is left
after the final example has been processed. The
most probable annotation of the corpus is the set
of top-most parses after the final parse selection.
5 Related Work
Wolff (Wolff, 1987) using a similar (if rather more
empiricist) setting also uses syntactic analysis and
compression to build grammars. However, this
syntactic analysis would appear to be very expen-
sive and the system has not been applied to large
scale problems. The compression metric is ap-
plied with respect to the compression of the cor-
pus, rather than the compression of syntactic in-
formation extracted from the corpus, as in CLL.
It seems unlikely that this simple induction al-
gorithm would generate linguistically plausible
grammars when presented with complex naturally
occurring data.
Joshi and Srinivas (Joshi and Srinivas, 1994)
have developed a method called supertagging that
similarly attaches complex syntactic tags (su-
pertags) to words. The most effective learning
model appears to have been a combination of
symbolic and stochastic techniques, like the ap-
proach presented here. However, a full lexicon is
supplied to the learner, so that the problem is re-
duced to one of disambiguating between the pos-
sible supertags. The learning appears to be super-
vised and occurs over parts-of-speech rather than
over the actual words. However, some notion of
label accuracy is supplied and this can be com-
pared with the accuracy of our system.
Osborne and Briscoe (Osborne and Briscoe,
1997) present a fairly supervised system for learn-
ing unusual stochastic CGs (the atomic categories
a far more varied than standard CG) again using
part-of-speech strings rather than words. While
the problem solved is much simpler, this system
provides a suitable comparison for learning ap-
propriate lexicons for parsing.
Neither Joshi and Srinivas (Joshi and Srini-
vas, 1994) nor Osborne and Briscoe (Osborne and
Briscoe, 1997) can be considered psychologically
plausible, but they are computationally effective
and they do provide results for comparison.
Two other approaches to learning CGs are
presented by Adriaans (Adriaans, 1992) and
Solomon (Solomon, 1991). Adriaans, describes
a purely symbolic method that uses the context of
words to define their category. An oracle is re-
quired for the learner to test its hypotheses, thus
providing negative evidence. This would seem to
be awkward from a engineering view point i.e.
how one could provide an oracle to achieve this,
and implausible from a psychological point of
view, as humans do not seem to receive such ev-
idence (Pinker, 1990). Unfortunately, no results
on natural language corpora seem to be available.
Solomon?s approach (Solomon, 1991) uses
unannotated corpora, to build lexicons for simple
CG. He uses a simple corpora of sentences from
children?s books, with a slightly ad hoc and non-
incremental, heuristic approach to developing cat-
egories for words. The results show that a wide
range of categories can be learned, but the cur-
rent algorithm, as the author admits, is probably
too naive to scale up to working on full corpora.
No results on the coverage of the CGs learned are
provided.
6 Results
Early results on small simple corpora with a
simpler version of the learner were presented
in (Watkinson and Manandhar, 1999; Watkinson
and Manandhar, 2000). Here, we present experi-
ments performed using two complex corpora, C1
and C2, extracted from the Penn Treebank (Mar-
cus et al, 1993; Marcus et al, 1994). These cor-
pora did not contain sentences with null elements
(i.e. movement). C1 contains 5000 sentences of
15 words or less. C2 contains 1000 sentences of
15 words or less. Lexicons were induced from
C1 and then used with the parser to parse C2.
Experiments were performed with a closed-class
word initial lexicon of 348 entries (LIL) and a
smaller closed-class word initial lexicon of 31 en-
tries (SIL) to determine the bootstrapping effect
of this initial lexicon.
The resulting lexicons are described in Table 1.
These can be compared with a gold standard CG
annotated corpus which has been built (Watkin-
son and Manandhar, 2001), which has a size of
15,136 lexical entries and an average ambiguity
of 1.25 categories per word. This corpus is only
loosely a gold standard, as it has been automat-
ically constructed. However, it gives an indica-
tion of the effectiveness of the lexical labelling
and is currently the best CG tagged resource avail-
able to us. The accuracy of the parsed examples
both from the training and test corpora are also
described in Table 1. Two measures are used to
evaluate the parses: lexical accuracy, which is the
percentage of correctly tagged words compared
to the extracted gold standard corpus (Watkin-
son and Manandhar, 2001) and average crossing
bracket rate (CBR) (Goodman, 1996).
In general the system performs better with the
larger initial lexicon to bootstrap it. The size
and ambiguity of the lexicon are close to that of
the gold standard, indicating that the right level
of compression has occurred. The best crossing
bracket rate of 4.70 compares favourably with Os-
borne and Briscoe (Osborne and Briscoe, 1997)
who give crossing bracket rates of around 3 for
a variety of systems. Considering that they are
solving a much simpler problem, our average
crossing bracket rates seem reasonable.
The lexical accuracy value is fairly low. Joshi
and Srinivas (Joshi and Srinivas, 1994) achieve
a best of 77.26% accuracy. Two factors explain
this. Firstly their system is simply disambiguat-
ing which tag to use in a context again using a
corpus of tag sequences ? a much simpler prob-
lem. Secondly, it would appear that the gold stan-
dard corpus they use is much more accurate than
ours. Despite this, a system that assigned the tags
randomly for our problem, would achieve an ac-
curacy of 3.33%, so over 50% is a reasonable
achievement.
7 Conclusions
There is further work to be completed in extend-
ing the system to allow it to deal with movement
and thus the whole of the Penn Treebank. Further
investigation of parameters of CLL should also
be completed. Further work needs to be done in
building an accurate gold standard corpus. There
is also a possibility of performing experiments on
sequences of parts-of-speech, as Joshi and Srini-
vas (Joshi and Srinivas, 1994) and Osborne and
Briscoe (Osborne and Briscoe, 1997) did. This
would reduce the effects of the sparse data prob-
lem.
However, we have presented a system that is
psychologically plausible and whose results show
that, given the complexity of the problem at-
tempted, it is computationally effective. The re-
sults compare reasonably with systems attempt-
ing much simpler and psychologically less plau-
sible problems.
References
Pieter Willem Adriaans. 1992. Language Learning
from a Categorial Perspective. Ph.D. thesis, Uni-
versiteit van Amsterdam.
Martin Atkinson. 1996. Syntax and learnability.
In Martin Atkinson, Stefano Bertolo, Robin Clark,
Jonathan Kaye, and Ian Roberts, editors, Learnabil-
ity and Language Acquisition: a self contained Tu-
torial for Linguists, pages 33 ? 53. LAGB.
Derek Bickerton. 1984. The language bioprogram hy-
pothesis. The Behavioral and Brain Sciences, 7:173
? 221.
Roger Brown and Camille Hanlon. 1979. Deriva-
tional complexity and order of acquistion in child
speech. In John R. Hayes, editor, Cognition and
Initial Lexicon Size Average Ambiguity Lexical Accuracy Average CBR
C1 C2 C1 C2
SIL 12,706 1.21 44.76 47.53 5.43 4.70
LIL 13,851 1.24 49.54 51.89 5.61 4.86
Table 1: Summary of the Lexicons and Parses built by CLL
Development of Language, pages 11?53. John Wi-
ley and Sons Inc.
David W. Carroll. 1994. Psychology of Language.
Brooks/Cole Publishing Company, second edition
edition.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. The MIT Press.
Noam Chomsky. 1972. Language and Mind. Har-
court Brace Jovanovich.
Noam Chomsky. 1980. Rules and Representations.
Basil Blackwell.
Noam Chomsky. 1986. Knowledge of Language: Its
Nature, Origin and Use. Praeger.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of the 34th Annual Meeting of
the ACL, pages 35 ? 64. Association for Computa-
tional Linguistics.
Trevor A. Harley. 1995. The Psychology of Lan-
guage: From Data to Theory. Erlbaum (UK) Taylor
& Francis.
Aravind K. Joshi and B. Srinivas. 1994. Disambigua-
tion of super parts of speech (or supertags): Al-
most parsing. In Proceedings of the 15th Confer-
ence on Computational Linguistics (COLING?94),
pages 154?160.
M. Li and P.M.B. Vita?nyi. 1993. Theories of learning.
In Proceedings of the International Conference of
Young Computer Scientists.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In The ARPA Human
Language Technology Workshop.
Miles Osborne and Ted Briscoe. 1997. Learning
stochastic categorial grammars. In Computational
Natural Language Learning Workshop CoNLL?97,
pages 80?87.
Steven Pinker. 1990. Language acquisition. In
Daniel N. Oshershon and Howard Lasnik, editors,
An Invitation to Cognitive Science: Language, vol-
ume 1, pages 199?241. The MIT Press.
Maris Monitz Rodgon. 1976. Single-word usage, cog-
nitive development, and the beginnings of combina-
torial speech: A study of ten English- speaking chil-
dren. Cambridge University Press.
Catherine E. Snow and Charles A. Ferguson, editors.
1977. Talking to children: Language input and ac-
quistion. Cambridge University Press.
W. Daniel Solomon. 1991. Learning a grammar.
Technical Report UMCS-AI-91-2-1, Department of
Computer Science, Artificial Intelligence Group,
University of Manchester.
Mark Steedman. 1993. Categorial grammar. Lingua,
90:221 ? 258.
Stephen Watkinson and Suresh Manandhar. 1999.
Unsupervised lexical learning with categorial gram-
mars. In Andrew Kehler and Andreas Stolcke, edi-
tors, Proceedings of the Workshop on Unsupervised
Learning in Natural Language Processing, pages
59?66.
Stephen Watkinson and Suresh Manandhar. 2000.
Unsupervised lexical learning with categorial gram-
mars using the LLL corpus. In James Cussens
and Sas?o Dz?eroski, editors, Learning Language in
Logic, volume 1925 of Lecture Notes in Artificial
Intelligence. Springer.
Stephen Watkinson and Suresh Manandhar. 2001.
Translating treebank annotation for evaluation.
In Proceedings of the Workshop on Evaluation
Methodologies for Language and Dialogue Sys-
tems, ACL/EACL 2001. To Appear.
J.G. Wolff. 1987. Cognitive development as optimi-
sation. In L. Bolc, editor, Computational Models of
Learning. Springer Verlag.
Mary McGee Wood. 1993. Categorial Grammars.
Linguistic Theory Guides. Routledge. General Ed-
itor Richard Hudson.
Translating Treebank Annotation for Evaluation
Stephen Watkinson and Suresh Manandhar
Department of Computer Science,
University of York,
York YO10 5DD,
UK.
stephen.watkinson@cs.york.ac.uk
suresh.manandhar@cs.york.ac.uk
Abstract
In this paper we discuss the need for
corpora with a variety of annotations
to provide suitable resources to evalu-
ate different Natural Language Process-
ing systems and to compare them. A
supervised machine learning technique
is presented for translating corpora be-
tween syntactic formalisms and is ap-
plied to the task of translating the Penn
Treebank annotation into a Categorial
Grammar annotation. It is compared
with a current alternative approach and
results indicate annotation of broader
coverage using a more compact gram-
mar.
1 Introduction
Annotated corpora have become a vital tool for
Natural Language Processing (NLP) systems, as
they provide both a standard against which results
can be evaluated and a resource from which to ex-
tract linguistic information e.g. lexicons. This is
especially true in any NLP task that requires the
annotation of examples, e.g. part-of-speech tag-
ging, parsing and semantic annotation, where it
is vital to have a correct standard against which
to compare the results of systems attempting to
solve the task. Similarly, it is crucial in a lan-
guage learning context, where what is learned can
be used to annotate examples e.g. syntax learning,
lexical learning. In this case the learned artefact
is used to annotate the examples, which can then
be compared against the correctly annotated ver-
sion. Hence, correctly annotated corpora are vital
for the evaluation of a very large number of NLP
tasks.
Unfortunately, there are often no suitably an-
notated corpora for a given task. For example, the
Penn Treebank (Marcus et al, 1993; Marcus et
al., 1994; Bies et al, 1994) provides a large cor-
pus of syntactically annotated examples mostly
from the Wall Street Journal. It is an excellent
resource for tasks dealing with the syntax of writ-
ten English. However, if the annotation formal-
ism (a phrase-structure grammar with some sim-
ple features) does not match that of one?s NLP
system, it is of very little use. For example, sup-
pose a parser using Categorial Grammar (Wood,
1993; Steedman, 1993) is developed and applied
to the examples in the corpus. While the bracket-
ing of the examples will bear a strong relationship
to the bracketing of the treebank, the labelling of
the lexical items and the inner nodes of the tree
will be entirely different and no labelling evalua-
tion will be possible.
However, intuitively, plenty of syntactic infor-
mation is available. In fact, for most evaluation,
all the syntactic information required is available,
but in the wrong form. It seems obvious that a
system for translating the syntactic information
between formalisms would be a useful tool.
Here, we present a system that translates the
annotation of the Penn Treebank from the stan-
dard phrase structure annotation to a Categorial
Grammar (CG) annotation and in the process in-
duces large scale CG lexicons. It is a data-driven
multi-pass system that uses both predefined rules
and machine learning techniques to translate the
trees and in the process induce a large scale CG
lexicon. The system was designed to produce the
lexical annotations for the sentences without null
elements (i.e. without movement) from the Penn
Treebank, so that these could be used to evalu-
ate the results produced by an unsupervised CG
lexicon learner (Watkinson and Manandhar, 2000;
Watkinson and Manandhar, 2001).
The system has four major features. Firstly,
there is significant control over how the treebank
is annotated. This is vital if the results are to be
used for evaluation. Secondly, the system pre-
vents propagation of translation errors throughout
the trees by being data-driven. Thirdly, the system
deals elegantly with erroneous annotation, even
providing a degree of self-correction. Finally, the
approach is general enough to apply to other sim-
ilar problems.
The system is compared with a top-down alter-
native based on the algorithm of Hockenmaier et
al (Hockenmaier et al, 2000), which is currently
the system which has been applied to the most
similar task, although it is really for CG lexicon
extraction. The comparison suggests that the al-
gorithm presented here gives more compact and
linguistically elegant solutions. Investigation also
indicates that the corpus produced is effectively
translated for its purpose.
In Section 2 other work in the area is briefly re-
viewed. In Section 3 the precise translation task
is described. This is followed in Section 4 with
a detailed description of the algorithms used for
this task and some discussion as to their appropri-
ateness. The results from the experiments are in
Section 5. Finally, in Section 6 the results are dis-
cussed along with the contributions of the work
and some suggestions for future work.
2 Previous Work
The most appropriate work to consider within this
context is the grammar extraction literature. Per-
haps the earliest example is the approach of Char-
niak (Charniak, 1996), who simply extracted a
context-free grammar by reading off the produc-
tion rules implied by the trees in the Penn Tree-
bank. While not translating the formalism of the
treebank, this has led to work extracting gram-
mars of different formalisms.
The majority of work is based on the most obvi-
ous extension of the Charniak approach, which is
to extract subtree-based grammars e.g. the Data-
Oriented Parsing (DOP) approach (Bod, 1995),
or extracting Lexicalised Tree Adjoining Gram-
mars (LTAGs), or more generally Lexicalised
Tree Grammars (LTGs) (Neumann, 1998; Xia,
1999; Chen and Vijay-Shanker, 2000). Each ap-
proach involves a process that splits up the anno-
tated trees in the treebank into a set of subtrees
that define the grammar. These approaches still
continue to work with the syntactic data in the
same form as it is found in the corpora.
A slightly different approach has been followed
by Krotov et al(Krotov et al, 1998), where they
extract the grammar from the Penn Treebank like
Charniak, but then compact it. This provides a
smaller grammar of similar quality to a grammar
that has not been compacted, when a linguisti-
cally motivated compaction is used. However, the
formalism remains unchanged. Similarly, John-
son (Johnson, 1998) modifies the labelling of the
Penn Treebank, but remains within a CFG frame-
work.
Hockenmaier et al(Hockenmaier et al, 2000),
although to some extent following the approach
of Xia (Xia, 1999) where LTAGs are extracted,
have pursued an alternative by extracting Com-
binatory Categorial Grammar (CCG) (Steedman,
1993; Wood, 1993) lexicons from the Penn Tree-
bank. In this case the data in the treebank is
truly translated into another formalism providing
an entire CCG annotation for the corpus based
on a top-down algorithm. The lexicon is built by
reading off the lexical assignments made for each
tree. This is the most closely related work to this
research, especially as it translates into a formal-
ism very closely related to CG.
The algorithm presented by Hockenmaier et al
(Hockenmaier et al, 2000) has been used to build
a top-down system against which to compare our
data-driven system. The algorithms are both de-
scribed in detail in Section 4.
3 The Task
Given a subset of the examples from the Penn
Treebank annotated with syntactic and part-of-
speech information (slightly modified), the sys-
tem should return the examples annotated with
the correct CG categories attached to the words
of the sentence and the lexicons these imply.
The context of the task explains some parts of
its definition. The translated corpus is to be used
as a standard against which to compare the lex-
ical annotation (i.e. the categories assigned to
the words) of the output of an unsupervised CG
learner that annotated the words of the examples
with CG categories and then extracts a proba-
bilistic lexicon (see Watkinson and Manandhar
(Watkinson and Manandhar, 2001) for details).
Hence, there is no need for specific tree annota-
tion. The learner currently uses a slightly mod-
ified subset of the treebank, which is described
below.
3.1 The Corpus
The systems are applied to examples from the
Penn Treebank (Marcus et al, 1993; Marcus et
al., 1994; Bies et al, 1994) a corpus of over
4.5 million words of American English annotated
with both part-of-speech and syntactic tree infor-
mation.
To be exact, we are using the Treebank II ver-
sion (Bies et al, 1994; Marcus et al, 1994),
which attempts to address the problem of com-
plement/adjunct distinction, which previous ver-
sions had ignored. While the documentation is
clear that the complement/adjunct structure is not
explicitly marked (Marcus et al, 1994), the anno-
tation includes a set of labels that relate to the role
of a particular constituent in the sentence. These
labels are attached to the standard constituent la-
bel and it is possible to use heuristics to determine
the probable complement/adjunct structure in the
trees (Collins, 1999; Xia, 1999), which is obvi-
ously useful in translating the annotation.
The full Penn Treebank is not being used. As
mentioned already, the current research only uses
sentences without null elements (i.e. without
movement) from the treebank and does not in-
clude any of the sentence fragments. However,
as Categorial Grammar formalisms do not usually
change the lexical entries of words to deal with
movement, but use further rules (Wood, 1993;
Steedman, 1993; Hockenmaier et al, 2000), the
lexicons learned here will be valid over corpora
with movement. The extracted corpus, C1, in fact
contains 5000 of the declarative sentences of fif-
teen words or less (although the sentence length
makes little difference to either of the translation
procedures described) from the Wall Street Jour-
nal section of the treebank. To give an indication
of the complexity of the corpus, the number of
tokens, i.e. the total number of words including
repetitions of the same word, is 47,782. The total
number of unique words, i.e. not including repe-
titions of the same word, is 12,277. We also ex-
tracted C2, a 1000 example corpus (also of declar-
ative sentences from the Wall Street Journal sec-
tion) with 9467 tokens and 3731 words, which is
used in the evaluation process.
The corpora also have some small modifica-
tions, which mean that adjacent nominals in the
same subtree are combined to form a single nom-
inal and the punctuation is removed. These mod-
ifications are made for use with the unsuper-
vised learner (Watkinson and Manandhar, 2000;
Watkinson and Manandhar, 2001) to simplify the
learning process. They may also slightly simplify
the translation process, but it is necessary for the
corpus annotation that we want.
3.2 Categorial Grammar
Categorial Grammar (CG) (Wood, 1993; Steed-
man, 1993) provides a functional approach to lex-
icalised grammar, and so can be thought of as
defining a syntactic calculus. Below we describe
the basic (AB) CG. The current work uses this
simple form of the grammar, which suffices for
the syntactic annotation of the corpora currently
being used.
There is a set of atomic categories in CG, which
are usually nouns (n), noun phrases (np) sen-
tences (s) and sometimes prepositional phrases
(pp), although this can be consider shorthand for
the full category (Wood, 1993). It is then possible
to build up complex categories using the two slash
operators ?/? and ?n?. If A and B are categories
then A/B and AnB are categories, where (follow-
ing Steedman?s notation (Steedman, 1993)) A is
the resulting category when B, the argument cate-
gory, is found. The direction of the ?slash? func-
tors indicates the position of the argument in the
sentence i.e. a ?/? indicates that a word or phrase
with the category of the argument should imme-
diately follow in the sentence. With the ?n? the
word or phrase with the argument category should
immediately precede the word or phrase with this
category. This is most easily seen with examples.
Suppose we consider an intransitive verb like
?run?. The category that is required to complete
the sentence is a subject noun phrase. Hence, the
category of ?run? is a sentence that is missing a
preceding noun phrase i.e. snnp. Similarly, with
a transitive verb like ?ate?, the verb requires a
subject noun phrase. However, it also requires an
object noun phrase, which is attached first. The
category for ?ate? is therefore (snnp)/np.
With basic CG there are just two rules for com-
bining categories: the forward (FA) and back-
ward (BA) functional application rules. Follow-
ing Steedman?s notation (Steedman, 1993) these
are:
X=Y Y ) X (FA)
Y XnY ) X (BA)
where X and Y are CG categories. In Figure 1
the parse derivation for ?John ate the apple? is
presented, showing examples of how these rules
are applied to categories.
ate the apple
np (s\np)/np np/n n
np
s\np
s
FA
FA
BA
John
Figure 1: An Example Parse in Basic CG
The CG formalism described above has been
shown to be weakly equivalent to context-free
phrase structure grammars (Bar-Hillel et al,
1964). While such expressive power covers a
large amount of natural language structure, it
has been suggested that a more flexible and ex-
pressive formalism may capture natural language
more accurately (Wood, 1993; Steedman, 1993).
In future we may consider applying the principle
developed here to perform translations to these
more complex formalisms, although many of the
changes will not actually change the lexical en-
tries, just the way they can be combined.
4 Alternative Approaches
This section presents the two approaches to trans-
lation that are being compared. Firstly, there is
also
H(RB)
A(ADVP)
declined
H(VBD)
H(VP)
the dollar
A(DT) H(NN)
C(NP-SBJ)
H(VP)
H(S)
Figure 2: A tree with constituents marked
the top-down method, which is a version of the
algorithm described by Hockenmaier et al(Hock-
enmaier et al, 2000), but used for translating into
simple (AB) CG rather than the Steedman?s Com-
binatory Categorial Grammar (CCG) (Steedman,
1993). The algorithm here does not need to deal
with movement, as the corpus does not contain
any. The atomic pp category is included in the CG
with this approach, but not with our approach, as
it is a convenient shorthand for the prepositional
phrase category.
The second approach is a multiple-pass data-
driven system. Rules for translating the trees are
applied in order of complexity starting with sim-
ple part-of-speech translation and finishing with a
category generation stage.
4.1 Top-Down Category Generation
The algorithm has two stages.
Mark constituents All the nodes of all trees
are marked with their roles i.e. as heads, com-
plements or adjuncts. While Hockenmaier et
al (Hockenmaier et al, 2000) are unclear, it is
assumed that this is achieved using heuristics.
Collins (Collins, 1999) describes such a set of
heuristics, which are used with some minor mod-
ifications for CG and the changed Penn Treebank
annotation. Figure 2 shows an example of an an-
notated tree.
Assign categories This is a recursive top-down
process, where the top category in the tree is an s.
The category of the complements is determined
by a mapping between Treebank labels and cate-
gories e.g. NP in the treebank becomes np. Hock-
enmaier et al(Hockenmaier et al, 2000) do not
provide the mapping, so it was built specially for
this system. This mapping led to the inclusion
the
s
also
(s\np)/(s\np)
declined
np
np/np np
dollar
s\np
s\np
(s\np)/(s\np) s\np
Figure 3: An example with categories assigned
of the pp category as shorthand for prepositional
complements. It should make no difference to the
annotation process, but could lead to the genera-
tion of a few more categories. The head child of a
subtree is given the category of the parent plus the
complements required, which are found by look-
ing first to the left of the head and then to the
right, and adding them in the order they should
processed in. Finally, adjuncts are assigned the
generic X=X or XnX where X is the head cate-
gory with the complements removed which have
been dealt with before the adjunct is processed.
Figure 3 shows an example of a tree with the cat-
egories assigned to it.
This algorithm has several advantages. It is
simple and robust and has been shown by Hock-
enmaier et al(Hockenmaier et al, 2000) to pro-
vide good lexical annotation leading to useful
CCG lexicons.
However, it has two main disadvantages.
Firstly, there is no control over category gener-
ation other than the rather weak constraints of the
formalism and the heuristic syntactic roles. This
is likely to lead to some linguistically implausible
annotation. Secondly, the top-down nature of the
algorithm is likely to lead to any translation errors
being propagated down the tree, which will lead
to some unusual and large categories, as Hocken-
maier et al(Hockenmaier et al, 2000) report.
4.2 Bottom-Up Sequential
Our system uses a four stage process, where the
type of translation changes at each stage.
4.2.1 Stage 1: Parts-of-Speech
This is the simplest level of translation. The
mapping between the Penn Treebank part-of-
speech annotation and the CG category annota-
tion is many-to-many, but some parts-of-speech
the dollar also declined
S
VP
NP-SBJ
NNnp/n
ADVP
RB
VP
VBD
Figure 4: Example of the output of Stage 1
can be translated directly into categories using
simple rules e.g. the following rule states that
words with the determiner part of speech (DT)
can be translated into the CG category np/n.
DT! np=n
The system passes through the full set of ex-
amples and translates the appropriate parts-of-
speech. See Figure 4 for an example of the output
of this stage.
4.2.2 Stage 2: Subtrees
The next pass through the data allows more
complex rules to be used. Consider the part-of-
speech label NNS, used in the Penn Treebank an-
notation scheme to indicate a plural noun. Its syn-
tactic role can be that of a simple noun (n) or a
noun phrase (np), so we need a mechanism for
choosing between these two possibilities.
The most obvious mechanism is to use the sur-
rounding subtree to provide the context to select
the correct rule. If the NNS tag is part of a noun
phrase which begins with something fulfilling the
determiner role, then the tag should be translated
to the CG category n, otherwise it should be trans-
lated as an np.
The algorithm for applying the set of context-
based rules is a simple matching process through-
out the treebank. Figure 5 shows the output from
this stage on an example.
4.2.3 Stage 3: Structural Heuristic
In this stage, the system uses further knowl-
edge to attempt to inform the translation process.
Where words have not been translated, the system
annotates the subtree with the head, complements
and adjuncts using a modified version of Collins?
heuristics (Collins, 1999).
the dollar also declined
S
VP
NP-SBJ
np/n
ADVP
RB
VP
VBDn
Figure 5: Example of the output of Stage 2
the also declined
S
VP
NP-SBJ
np/n
ADVP
RB
VP
s\npn
dollar
Figure 6: Example of the output of Stage 3
Further categories can now be obtained. For
example, if the head of the subtree requires an np
category to its right as its first complement and
there is a word marked as a complement in this
position, then it can be translated as an np. Alter-
natively, if the head category is unknown, but it is
verbal according to the Penn Treebank label then
looking at the categories of the complements can
determine the type of verb it is e.g. no comple-
ments following a verb indicates a CG category
snnp. Figure 6 shows the effects of this stage on
the example.
4.2.4 Stage 4: Category Generation
In the final stage each lexical category that has
not been annotated is given a variable for a cat-
egory. The tree is then traversed bottom-up in-
stantiating these categories by using head, com-
plement and adjunct annotation and the already
annotated categories. The building of head and
adjunct categories follows the same process de-
scribed for the top-down algorithm. Comple-
ments either gain their categories through this
process or have already had them assigned. Fig-
ure 7 shows the final output.
This approach has two main advantages.
Firstly, the user has control over the type of CG
to which the treebank is translated, due to the
the also declined
s
s\np
np/n
s\np
s\npn
dollar
np
(s\np)/(s\np)
(s\np)/(s\np)
Figure 7: Example of the output of Stage 4
use of predefined categories for predefined con-
texts. Secondly, the bottom-up approach ensures
that translation errors are not propagated seriously
through the tree.
A further advantage exists that has not, as yet,
been fully investigated. The system, due to its
multi-pass nature, has the potential for transla-
tions to clash. Experience has shown that this oc-
curs when there is an annotation error, so the sys-
tem can be used to highlight these and can also
provide some level of self-correction. This has
not been investigated in detail, but the current ap-
proach, which gives satisfactory results, is to as-
sume the head category is correct and adjust com-
plements and adjuncts accordingly. In future, a
simple correction scheme could easily be added
to produce a self-correcting translator.
The main weakness of the system is the re-
liance upon the head/complement/adjunct anno-
tating heuristics, which were not designed to be
used with a CG.
The system also returns some categories with
variables. This is due in part to the heuristics and
in part to the small number of rules currently used
in the early stages of the translation process. Most
of the problem categories could be dealt with by
the addition of a few more rules in stages 2 and 3.
5 Results
Here we provide similar evaluation of the systems
as others (Hockenmaier et al, 2000; Xia, 1999)
for easy comparison. Both systems were used
translate C1 and C2. C2 is used for determin-
ing the coverage of the grammar used by the two
systems. Both systems, at times, failed to trans-
late examples (frequently due to annotation error
in the original treebank). The top-down system
failed on 60 and 15 examples from C1 and C2
Top-down Bottom-up
No. of cats 167 106
Lexicon size 15887 15136
Ave. cats/word 1.31 1.25
Ave. cat size 8.02 5.12
Table 1: Table of category and lexicon informa-
tion on the translated corpora
Freq. Range Number of Categories
Top-down Bottom-up
1f1 42 29
2f10 61 34
11f20 14 9
21f100 24 11
101f1000 17 13
1001f5000 7 7
5001f10000 1 2
10001f12000 0 1
12001f15000 1 0
Table 2: Table of the category frequencies for
both approaches
respectively. The bottom-up system failed on 66
and 15 examples from C1 and C2 respectively.
Table 1 describes the type of categories used
to translate C1 and the size of the lexicons gen-
erated. Categories with variables in were ig-
nored, as they could usually be unified with an
already existing category. With this in mind, the
bottom-up algorithm extracted a more compact
lexicon. The average category sizes (the number
of slash operators in categories) are interesting,
as they indicate the profligacy of the top-down al-
gorithm in creating unwieldy categories, whereas
the bottom-up approach uses smaller and, on in-
spection, more plausible categories. These results
seem, in part, to vindicate the choice of a con-
trolled bottom-up approach.
Tables 2 and 3 present the results for both sys-
tems for the frequency distribution of categories
(i.e. the number of categories that appeared with
a particular frequency) and the frequency distri-
bution of the number of categories for a word (i.e.
the number of words that had a particular num-
ber of categories). The trends for both systems
are similar. There are a large number of cate-
gories that appear very infrequently, these tend
Freq. Range Word frequency
Top-down Bottom-up
f=1 10486 10377
f=2 1263 1264
f=3 264 264
f=4 86 86
5  f  9 100 100
10  f  14 20 20
15  f  24 10 10
25  f  30 2 2
Table 3: Frequencies of words appearing in a fre-
quency range of number of categories
to be the larger, generated categories and often
fit unusual circumstances e.g. misannotation of
the treebank, or mistakes in the use of the heuris-
tics. The bottom-up approach has many fewer of
these categories, indicating the problem of propa-
gating of errors down the tree with the top-down
approach. There are also a few exceptionally fre-
quent categories, these are noun phrases, nouns,
and some of the common verbs.
The number of categories per word is simi-
lar, suggesting the approaches are similar in their
ability to produce the variety of categories re-
quired for words.
While these figures give some indication of the
quality and compactness of the translation, it is
useful to determine the coverage of the lexicon
extracted from C1 by comparing it with a lexicon
extracted from C2 and so determine the quality
and generality of the lexicon that has been pro-
duced in the translation. Table 4 shows the com-
parison. Here entry means the C1 lexicon con-
tains an entry the same as the C2 entry. kwkc
means that the entry from C2 is not in C1, but
both the word and the category are known. kwuc
means the word is in the C1 lexicon, but the cat-
egory is not. Finally, uw indicates that the word
is in C1. Despite a smaller lexicon and a smaller
number of categories, the bottom-up system gives
better coverage. Note especially that there are no
unknown categories with with the bottom-up ap-
proach and that the percentage of exact entries is
much higher.
Top-down Bottom-up
Categories 98 65
New categories 4 0
entry % 37.29 48.31
kwkc % 10.55 11.09
kwuc % 11.46 0
uw % 40.70 40.60
Table 4: Table comparing the coverage of the two
approaches
6 Conclusions
The system presented provides a useful and accu-
rate method for translating the annotation of the
Penn Treebank into a CG annotation. Compar-
isons with an alternative approach suggest that the
increase of control provided by the system lead to
a more accurate and compact translation, which is
more linguistically plausible. Most importantly,
the system is flexible enough to allow the user to
annotate corpora with the kind of CG they are in-
terested in, which is vital when it is to be used for
evaluation.
It would be useful to expand the systems to
work on the full treebank i.e. including sentences
with movement (see Hockenmaier et al(Hocken-
maier et al, 2000) for discussion of a possible
method). The correcting of the annotation of the
treebank during translation should also be inves-
tigated further.
References
Y. Bar-Hillel, C. Gaifman, and E. Shamir. 1964. On
categorial and phrase structure grammars. In Lan-
guage and Information, pages 99 ? 115. Addison-
Wesley. First appeared in The Bulletin of the Re-
search Council of Israel, vol. 9F, pp. 1-16, 1960.
Ann Bies, Mark Ferguson, Karen Katz, and Robert
MacIntyre, 1994. Bracketing Guidlines for Tree-
bank II Style Penn Treebank Project.
Rens Bod. 1995. Enriching Linguistics with Statis-
tics: Performance Models of Natural Language.
Ph.D. thesis, Department of Computational Lin-
guistics, Universiteit van Amsterdam.
Eugene Charniak. 1996. Treebank grammars. In Pro-
ceedings of AAAI/IAAI.
John Chen and K. Vijay-Shanker. 2000. Automated
extraction of tags from the Penn Treebank. In Pro-
ceedings of the 6th International Workshop on Pars-
ing Technologies.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Julia Hockenmaier, Gann Bierner, and Jason
Baldridge. 2000. Providing robustness for a
ccg system. In Proceedings of the Workshop on
Linguistic Theory and Grammar Implementation,
ESSLLI 2000.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Alexander Krotov, Mark Hepple, Robert Gaizauskas,
and Yorick Wilks. 1998. Compacting the
Penn Treebank grammar. In Proceedings of
COLING?98?ACL?98.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In The ARPA Human
Language Technology Workshop.
Gu?nter Neumann. 1998. Automatic extraction of
stochastic lexicalized tree grammars from tree-
banks. In Proceedings of the 4th Workshop on tree-
adjoining grammars and related frameworks.
Mark Steedman. 1993. Categorial grammar. Lingua,
90:221 ? 258.
Stephen Watkinson and Suresh Manandhar. 2000.
Unsupervised lexical learning with categorial gram-
mars using the LLL corpus. In James Cussens
and Sas?o Dz?eroski, editors, Learning Language in
Logic, volume 1925 of Lecture Notes in Artificial
Intelligence. Springer.
Stephen Watkinson and Suresh Manandhar. 2001. A
psychologically plausible and computationally ef-
fective approach to learning syntax. To appear at
CoNLL?01.
Mary McGee Wood. 1993. Categorial Grammars.
Linguistic Theory Guides. Routledge. General Ed-
itor Richard Hudson.
F. Xia. 1999. Extracting tree adjoining grammars
from bracketed corpora. In Proceedings of the 5th
Natural Language Processing Pacific Rim Sympo-
sium (NLPRS-99).
Incorporating User Models in Question Answering to Improve Readability
Silvia Quarteroni and Suresh Manandhar
Department of Computer Science
University of York
York YO10 5DD
United Kingdom
{silvia,suresh}@cs.york.ac.uk
Abstract
Most question answering and information
retrieval systems are insensitive to differ-
ent users? needs and preferences, as well
as their reading level. In (Quarteroni and
Manandhar, 2006), we introduce a hybrid
QA-IR system based on a a user model.
In this paper we focus on how the system
filters and re-ranks the search engine re-
sults for a query according to their reading
difficulty, providing user-tailored answers.
Keywords: question answering, informa-
tion retrieval, user modelling, readability.
1 Introduction
Question answering (QA) systems are information
retrieval systems accepting queries in natural lan-
guage and returning the results in the form of sen-
tences (or paragraphs, or phrases). They move
beyond standard information retrieval (IR) where
results are presented in the form of a ranked list
of query-relevant documents. Such a finer answer
presentation is possible thanks to the application
of computational linguistics techniques in order
to filter irrelevant documents, and of a consistent
amount of question pre-processing and result post-
processing.
However, in most state-of-the-art QA systems
the output remains independent of the questioner?s
characteristics, goals and needs; in other words,
there is a lack of user modelling. For instance, an
elementary school child and a University history
student would get the same answer to the question:
?When did the Middle Ages begin??.
Secondly, most QA systems focus on factoid
questions, i.e. questions concerning people, dates,
numerical quantities etc., which can generally be
answered by a short sentence or phrase (Kwok et
al., 2001). The mainstream approach to QA evalu-
ation, represented by TREC-QA campaigns1, has
long fostered the criterion that a ?good? system
is one that returns the ?correct? answer in the
shortest possible formulation. Although recent ef-
forts in TREC 2003 and 2004 (Voorhees, 2003;
Voorhees, 2004) denoted an interest towards list
questions and definitional (or ?other?) questions,
we believe that there has not been enough inter-
est towards non-factoid answers. The real issue is
?realizing? that the answer to a question is some-
times too complex to be formulated and evaluated
as a factoid: some queries have multiple, com-
plex or controversial answers (take e.g. ?What
were the causes of World War II??). In such sit-
uations, returning a short paragraph or text snip-
pet is more appropriate than exact answer spot-
ting. For instance, the answer to ?What is a
metaphor?? may be better understood with the in-
clusion of examples. This viewpoint is supported
by recent user behaviour studies which showed
that even in the case of factoid-based QA systems,
the most eligible result format consisted in a para-
graph where the sentence containing the answer
was highlighted (Lin et al, 2003).
The issue of non-factoids is related to the user
modelling problem: while factoid answers do not
necessarily require to be contextualized within the
user?s knowledge and viewpoint, the need is much
stronger in the case of definitions, explanations
and descriptions. This is mentioned in the TREC
2003 report (Voorhees, 2003) when discussing the
evaluation of definitional questions: however, the
issue is expeditiously solved by assuming a fixed
user profile (the ?average news reader?).
We are currently developing an adaptive sys-
tem which adjusts its output with respect to a user
model. The system can be seen as an enhanced IR
system which adapts both the content and presen-
tation of the final results, improving their quality.
1http://trec.nist.gov
50 KRAQ06
In this paper, we show that QA systems can benefit
from the contribution of user models, and explain
how these can be used to filter the information pre-
sented as an answer based on readability. Eventu-
ally, we describe preliminary results obtained via
an evaluation framework inspired by user-centered
search engine evaluation.
2 System Architecture
The high-level architecture as represented in Fig-
ure 1 shows the basic components of the system,
the QA module and the user model.
QUESTION 
PROCESSING
DOCUMENT
RETRIEVAL
ANSWER 
EXTRACTION
Question
Answer
QA MODULE
USER MODEL
Webpage
Webpages
Age Range
Reading 
Level
Figure 1: High level system architecture
The QA module, described in the following sec-
tion, is organized according to the three-tier parti-
tion underlying most state-of-the-art systems: 1)
question processing, 2) document retrieval, 3) an-
swer generation. The module makes use of a web
search engine for document retrieval and consults
the user model to obtain the criteria to filter and
re-rank the search engine results and to eventually
present them appropriately to the user.
2.1 User model
Depending on the application of interest, the user
model (UM) can be designed to suit the informa-
tion needs of the QA module in different ways.
Our current application, YourQA2, is a learning-
oriented system to help students find information
on the Web for their assignments. Our UM con-
sists of the user?s:
? age range, a ? {7? 11, 11? 16, adult}
? reading level, r ? {poor,medium, good}
? webpages of interest/bookmarks, w
The age range parameter has been chosen to
match the partition between primary school, con-
temporary school and higher education age in
2http://www.cs.york.ac.uk/aig/aqua
Britain; our reading level parameter takes three
values which ideally (but not necessarily) corre-
spond to the three age ranges and may be further
refined in the future for more fine-grained mod-
elling.
Analogies can be found with the SeAn (Ardis-
sono et al, 2001), and SiteIF (Magnini and Strap-
parava, 2001) news recommender systems, where
information such as age and browsing history,
resp. are part of the UM. More generally, our
approach is similar to that of personalized search
systems (Teevan et al, 2005; Pitkow et al, 2002),
which construct UMs based on the user?s docu-
ments and webpages.
In our system, UM information is explicitly col-
lected from the user; while age and reading level
are self-assessed, the user?s interests are extracted
from the document set w using a keyphrase ex-
tractor (see further for details). Eventually, a di-
alogue framework with a history component will
contribute to the construction and update of the
user model in a less intruding and thus more user-
friendly way. In this paper we focus on how to
adapt search result presentation using the reading
level parameter: age and webpages will not be dis-
cussed.
2.2 Related work
Non-factoids and user modelling As men-
tioned above, the TREC-QA evaluation campaign,
to which the vast majority of current QA systems
abide, mainly approaches factoid-based answers.
To our knowledge, our system is among the first to
address the need for a different approach to non-
factoid answers. The structure of our QA compo-
nent reflects the typical structure of a web-based
QA system in its three-tier composition. Analo-
gies in this can be found for instance in MUL-
DER (Kwok et al, 2001), which is organized ac-
cording to a question processing/answer extrac-
tion/passage ranking pipeline. However, a signifi-
cant aspect of novelty in our architecture is that the
QA component is supported by the user model.
Additionally, we have changed the relative im-
portance of the different tiers: while we drastically
reduce linguistic processing during question pro-
cessing and answer generation, we give more re-
lief to the post-retrieval phase and to the role of
the UM. Having removed the need for fine-grained
answer spotting, the emphasis is shifted towards
finding closely connected sentences that are highly
51 KRAQ06
relevant to answer the query.
Readability Within computational linguistics,
several applications have been designed to address
the needs of users with low reading skills. The
computational approach to textual adaptation is
commonly based on natural language generation:
the process ?translate? a difficult text into a syntac-
tically and lexically simpler version. In the case of
PSET (Carroll et al, 1999) for instance, a tagger, a
morphological analyzer and generator and a parser
are used to reformulate newspaper text for users
affected by aphasia. Another interesting research
is Inui et al?s lexical and syntactical paraphrasing
system for deaf students (Inui et al, 2003). In this
system, the judgment of experts (teachers) is used
to learn selection rules for paraphrases acquired
using various methods (statistical, manual, etc.).
In the SKILLSUM project (Williams and Reiter,
2005), used to generate literacy test reports, a set
of choices regarding output (cue phrases, order-
ing and punctuation) are taken by a micro-planner
based on a set of rules.
Our approach is conceptually different from the
above: exploiting the wealth of information avail-
able in the context of a Web-based QA system, we
can afford to choose among the documents avail-
able on a given subject those which best suit our
readability requirements. This is possible thanks
to the versatility of language modelling, which al-
lows us to tailor the readability estimation of doc-
uments to any kind of user profile in a dynamic
manner, as explained in section 3.2.3.
3 QA Module
In this section we discuss the information flow
among the subcomponents of the QA module (see
Figure 2 for a representative diagram) and focus
on reading level estimation and document filter-
ing. For further details on the implementation of
the QA module, see (Quarteroni and Manandhar,
2006).
3.1 Question Processing
The first step performed by YourQA is query ex-
pansion: additional queries are created replacing
question terms with synonyms using WordNet3.
3http://wordnet.princeton.edu
Question
QUERY 
EXPANSION
DOCUMENT
RETRIEVAL
KEYPHRASE
EXTRACTION
ESTIMATION
OF READING
LEVELS
CLUSTERING
Language
Models
UM-BASED
FILTERING
SEMANTIC 
SIMILARITY
RANKING
User Model
Reading 
Level
Ranked
Answer
Candidates
Figure 2: Diagram of the QA module
3.2 Retrieval and Result Processing
3.2.1 Document retrieval
We use Google4 to retrieve the top 20 docu-
ments returned for each of the queries issued from
the query expansion phase. The subsequent steps
will progressively narrow the parts of these docu-
ments where relevant information is located.
3.2.2 Keyphrase extraction
Keyphrase extraction is useful in two ways:
first, it produces features to group the retrieved
documents thematically during the clustering
phase, and thus enables to present results by
groups. Secondly, when the document parame-
ter (w) of the UM is active, matches are sought
between the keyphrases extracted from the docu-
ments and those extracted from the user?s set of
interesting documents; thus it is possible to pri-
oritize results which are more compatible with
his/her interests.
Hence, once the documents are retrieved, we
extract their keyphrases using Kea (Witten et al,
1999), an extractor based on Na?ve Bayes classifi-
cation. Kea first splits each document into phrases
and then takes short subsequences of these initial
phrases as candidate keyphrases. Two attributes
are used to classify a phrase p as a keyphrase or
a non-keyphrase: its TF? IDF score within the
set of retrieved documents and the index of p?s
first appearance in the document. Kea outputs a
ranked list of phrases, among which we select the
top three as keyphrases for each of our documents.
4http://www.google.com
52 KRAQ06
3.2.3 Estimation of reading levels
In order to adjust search result presentation to
the user?s reading ability, we estimate the read-
ing difficulty of each retrieved document using the
Smoothed Unigram Model, a variation of a Multi-
nomial Bayes classifier (Collins-Thompson and
Callan, 2004). Whereas other popular approaches
such as Flesch-Kincaid (Kincaid et al, 1975) are
based on sentence length, the language modelling
approach accounts especially for lexical informa-
tion. The latter has been found to be more effective
as the former when approaching the reading level
of subjects in primary and secondary school age
(Collins-Thompson and Callan, 2004). Moreover,
it is more applicable than length-based approach
for Web documents, where sentences are typically
short regardless of the complexity of the text.
The language modelling approach proceeds in
two phases: in the training phase, given a range of
reading levels, a set of representative documents
is collected for each reading level. A unigram lan-
guage model lms is then built for each set s; the
model consists of a list of the word stems appear-
ing in the training documents with their individual
probabilities. Textual readability is not modelled
at a conceptual level: thus complex concepts ex-
plained in simple words might be classified as suit-
able even for a poor reading level; However we
have observed that in most Web documents lexi-
cal, syntactic and conceptual complexity are usu-
ally consistent within documents, hence it makes
sense to apply a reasoning-free technique with-
out impairing readability estimation. Our unigram
language models account for the following read-
ing levels:
1) poor, i.e. suitable for ages 7 ? 11;
2) medium, suitable for ages 11?16;
3) good, suitable for adults.
This partition in three groups has been chosen to
suit the training data available for our school appli-
cation, which consists of about 180 HTML pages
(mostly from the ?BBC schools?5, ?Think En-
ergy?6, ?Cassini Huygens resource for schools?7
and ?Magic Keys storybooks?8 websites), explic-
itly annotated by the publishers according to the
reading levels above.
In the test phase, given an unclassified docu-
5http://bbc.co.uk/schools
6http://www.think-energy.com
7http://www.pparc.ac.uk/Ed/ch/Home.htm
8http://www.magickeys.com/books/
ment D, the estimated reading level of D is the
language model lmi maximizing the likelihood
L(lmi|D) that D has been generated by lmi. Such
likelihood is estimated using the formula:
L(lmi|D) =
?
w?D
C(w,D) ? log(P (w|lmi))
where w is a word in the document, C(w, d) rep-
resents the number of occurrences of w in D and
P (w|lmi) is the probability that w occurs in lmi
(approached by its frequency).
An advantage of language modelling is its
portability, since it is quite quick to create word
stem/frequency histograms on the fly. This implies
that models can be produced to represent more
fine-grained reading levels as well as the specific
requirements of a single user: the only necessary
information are sets of training documents repre-
senting each level to be modelled.
3.2.4 Clustering
As an indicator of inter-document relatedness,
we use document clustering (Steinbach et al,
2000) to group them using both their estimated
reading difficulty and their topic (i.e. their
keyphrases). In particular we use a hierarchi-
cal algorithm, Cobweb (implemented using the
WEKA suite of tools (Witten and Frank, 2000) as
it produces a cluster tree which is visually sim-
ple to analyse: each leaf corresponds to one doc-
ument, and sibling leaves denote documents that
are strongly related both in topic and in reading
difficulty. Figure 3 illustrates an example clus-
ter tree for the the query: ?Who painted the Sis-
tine chapel??. Leaf labels represent document
keyphrases extracted by Kea for the corresponding
documents and ovals represent non-terminal nodes
in the cluster tree (these are labelled using the most
common keyphrases in their underlying leaves).
3.3 Answer Extraction
The purpose of answer extraction is to present the
most interesting excerpts of the retrieved docu-
ments according to both the user?s query topics
and reading level. This process, presented in sec-
tions 3.3.1 ? 3.3.4, follows the diagram in Figure
2: we use the UM to filter the clustered documents,
then compute the similarity between the question
and the filtered document passages in order to re-
turn the best ones in a ranked list.
53 KRAQ06
0 chapel
1 ceiling 4 michelangelo art
7 chapel
 ceiling 
paint 
pope 2
 painted 
ceiling 
frescoes 3
 art 
michelangelo 
paint 5
 art 
michelangelo 
download 6
8 chapel michelangelo
11 chapel
 michelangelo 
paintings 
chapel 9
 chapel 
michelangelo 
christ 10
12 chapel
 red_ball 
chapel 
vaticano 15
 chapel 
sistine_chapel 
walls 13
 fresco 
cappella_sistina 
chapel 14
Figure 3: Cluster tree for ?Who painted the Sistine
chapel??. Leaf 3 and the leaves grouped under
nodes 8 and 12 represent documents with an esti-
mated good reading level; leaf 15 and the leaves
underlying node 4 have a medium reading level;
leaf 2 represents a poor reading level document.
3.3.1 UM-based filtering
The documents in the cluster tree are filtered ac-
cording to the UM reading level, r: only those
compatible with the user?s reading ability are re-
tained for further analysis. However, if the num-
ber of retained documents does not exceed a given
threshold, we accept in our candidate set part of
the documents having the next lowest readability
in case r ? {good,medium} or a medium read-
ability in case r = poor.
3.3.2 Semantic similarity
Within each of the documents retained, we seek
for the sentences which are semantically most rel-
evant to the query. Given a sentence p and the
query q, we represent them as two sets of words
P = {pw1, . . . , pwm} and Q = {qw1, . . . , qwn}.
The semantic distance from p to q is then:
distq(p) =
?
1?i?m minj [d(pwi, qwj)]
where d(pwi, qwj) represents the Jiang-Conrath
word-level distance between pwi and qwj (Jiang
and Conrath, 1997), based on WordNet 2.0. The
intuition is that for each question word, we find the
word in the candidate answer sentence which min-
imizes the word-level distance and then we com-
pute the sum of such minima.
3.3.3 Passage and cluster ranking
For a given document, we can thus isolate a sen-
tence s minimizing the distance to the query. The
passage P , i.e. a window of up to 5 sentences cen-
tered on s, will be a candidate result. We assign
to such passage a score equal to the similarity of s
to the query; in turn, the score of P is used as the
score of the document containing it. We also de-
fine a ranking function for clusters, which allows
to order them according to the maximal score of
their component documents. Passages from the
highest ranking cluster will be presented first to
the user, in decreasing order of score, followed by
the passages from lower ranking clusters.
3.3.4 Answer presentation
To present our answers, we fix a threshold for
the number of results to be returned following the
ranking exposed above. Each result consists of
a title and document passage where the sentence
which best answers the query is highlighted; the
URL of the original document is also available for
loading if the user finds the passage interesting and
wants to read more.
4 Results
We report the results of running our system on
a range of queries, which include factoid/simple,
complex and controversial questions9.
4.1 Simple answer
As an example of a simple query, we present the
results for: ?Who painted the Sistine Chapel??,
the system returned the following passages:
?UMgood: ?Sistine Chapel (sis-teen). A chapel adjoin-
ing Saint Peter?s Basilica, noted for the frescoes of biblical
subject painted by Michelangelo on its walls and ceilings.?
?UMmed: ?In all Michelangelo painted more than 300
different figures on the Sistine Chapel ceiling.?
?UMpoor: ?My name is Jacopo L?Indaco and I was
an assistant to Michelangelo when he painted the Sistine
Chapel.?
To obtain the above answers the system was run 3
times with different values for the reading level pa-
rameter in the UM, as defined in 3.2.3. As we can
see, in all cases the correct information is present
although not always explicitly, as in the first two
cases. This is because our current semantic sim-
ilarity metric only operates at word level. In this
example, all sentences containing ?painted?, ?Sis-
tine? and ?Chapel? obtain a distance of 0 to the
query, regardless of their formulation. Also no-
tice how the difference in language complexity is
clearly discernible in the different answers.
4.2 Complex answer
We illustrate the results of our system with the
query ?Definition of metaphor?, which relates to
a difficult concept. Our top results, highlighted in
9Notice that this partition is not to be interpreted as a
methodological division, as we currently approach complex
and controversial answers the same way.
54 KRAQ06
boldface within their containing passages, are:
?UMgood: ?Definition of Metaphor. Metaphor is a pro-
cess by which we conceive "one thing in terms of another,
and its primary function is understanding" (Lakoff and
Johnson, Metaphors We Live By 36). That is, we often do
(and must) talk about things that do not physically exist, that
we cannot reach out and touch or take apart with our hands
to see what is inside and how it works: love, time, anger,
life. [. . . ] We make them tangible by, in a process of thought,
applying models of physical things to them: love is falling;
argument is war. ?
?UMmed: ?Definition: A metaphor is a figure of speech
that compares unlike things by saying that one thing is the
other. Often metaphors are simple comparisons, but they can
be extended so that different aspects of the things compared
are treated separately. ?
?UMpoor: we did not find any document exactly match-
ing the reading level, we thus returned the results forUMmed
(see above).
As before, there is a clear difference in the lan-
guage complexity of the two answers.
4.3 Controversial answer
We illustrate this case using the query ?Who wrote
the Iliad??, which is a subject of debate: the exis-
tence of a single author, as well as the historicity
of the poet Homer are under discussion. These are
among our top results:
?UMgood: ?Most Classicists would agree that, whether
there was ever such a composer as "Homer" or not, the
Homeric poems are the product of an oral tradition, a
generations-old technique that was the collective inheritance
of many singer-poets, aoidoi. [. . . ] Could the Iliad and
Odyssey have been oral-formulaic poems, composed on the
spot by the poet using a collection of memorized traditional
verses and phases??
?UMmed: ?No reliable ancient evidence for Homer ?
earliest traditions involve conjecture (e.g. conflicting claims
to be his place of origin) and legend (e.g. Homer as son
of river-god). General ancient assumption that same poet
wrote Iliad and Odyssey (and possibly other poems) ques-
tioned by many modern scholars: differences explained bi-
ographically in ancient world (e g wrote Od. in old age);
but similarities could be due to imitation.?
?UMpoor: ?Homer wrote The Iliad and The Odyssey (at
least, supposedly a blind bard named "Homer" did).?
In this case we can see how the problem of attri-
bution of the Iliad is made clearly visible: in the
three results, document passages provide a context
which helps to explain such controversy at differ-
ent levels of difficulty.
5 Evaluation
5.1 Methodology
Our system is not a QA system in the strict
sense, as it does not single out one correct an-
swer phrase. The key objective is an improved sat-
isfaction of the user towards its adaptive results,
which are hopefully more suitable to his read-
ing level. A user-centred evaluation methodology
that assesses how the system meets individual in-
formation needs is therefore more appropriate for
YourQA than TREC-QA metrics.
We draw our evaluation guidelines from (Su,
2003), which proposes a comprehensive search
engine evaluation model. We define the following
metrics (see Table 1):
1. Relevance:
? strict precision (P1): the ratio between
the number of results rated as relevant
and all the returned results,
? loose precision (P2): the ratio between
the number of results rated as relevant
or partially relevant and all the returned
results.
2. User satisfaction: a 7-point Likert scale10 is
used to assess satisfaction with:
? loose precision of results (S1),
? query success (S2).
3. Reading level accuracy (Ar). This metric
was not present in (Su, 2003) and has been
introduced to assess the reading level estima-
tion. Given the set R of results returned by
the system for a reading level r, it is the ratio
between the number of documents ? R rated
by the users as suitable for r and |R|. We
compute Ar for each reading level.
4. Overall utility (U ): the search session as a
whole is assessed via a 7-point Likert scale.
We have discarded some of the metrics proposed
by (Su, 2003) when they appeared as linked to
technical aspects of search engines (e.g. connec-
tivity), and when response time was concerned as
at the present stage this has not been considered
10This measure ? ranging from 1= ?extremely unsatisfac-
tory? to 7=?extremely satisfactory? ? is particularly suitable
to assess the degree to which the system meets the user?s
search needs. It was reported in (Su, 1991) as the best sin-
gle measure for information retrieval among 20 tested.
55 KRAQ06
an issue. Also, we exclude metrics relating to the
user interface which are not relevant for this study.
Metric field description
Relevance P1 strict precision
P2 loose precision
Satisfaction S1 with loose precision
S2 with query success
Accuracy Ag good reading level
Am medium reading level
Ap poor reading level
Utility U overall session
Table 1: Summary of evaluation metrics
5.2 Evaluation results
We performed our evaluation by running 24
queries (partly reported in Table 3) on both Google
and YourQA11. The results ? i.e. snippets from
the Google result page and passages returned by
YourQA ? were given to 20 evaluators. These
were aged between 16 and 52, all having a self-
assessed good or medium English reading level.
They came from various backgrounds (University
students/graduates, professionals, high school)
and mother-tongues. Evaluators filled in a ques-
tionnaire assessing the relevance of each passage,
the success and result readability of the single
queries, and the overall utility of the system; val-
ues were thus computed for the metrics in Table 1.
P1 P2 S1 S2 U
Google 0,39 0,63 4,70 4,61 4,59
YourQA 0,51 0,79 5,39 5,39 5,57
Table 2: Evaluation results
5.2.1 Relevance
The precision results (see Table 2) for the whole
search session were computed by averaging the
values obtained for the 20 queries. Although quite
close, they show a 10-15% difference in favour of
the YourQA system for both strict precision (P1)
and loose precision (P2). This suggests that the
coarse semantic processing applied and the visu-
alisation of the context contribute to the creation
of more relevant passages.
11To make the two systems more comparable, we turned
off query expansion and only submitted the original question
sentence
5.2.2 User satisfaction
After each query, we asked evaluators the fol-
lowing questions: ?How would you rate the ratio
of relevant/partly relevant results returned?? (as-
sessing S1) and ?How would you rate the success
of this search?? (assessing S2). Table 2 denotes a
higher level of satisfaction tributed to the YourQA
system in both cases.
5.2.3 Reading level accuracy
Adaptivity to the users? reading level is the dis-
tinguishing feature of the YourQA system: we
were thus particularly interested in its perfor-
mance in this respect. Table 3 shows that alto-
gether, evaluators found our results appropriate for
the reading levels to which they were assigned.
The accuracy tended to decrease (from 94% to
72%) with the level: this was predictable as it is
more constraining to conform to a lower reading
level than to a higher one. However this also sug-
gests that our estimation of document difficulty
was perhaps too ?optimisitic?: we are currently
working with better quality training data which al-
lows to obtain more accurate language models.
Query Ag Am Ap
Who painted the Sistine Chapel? 0,85 0,72 0,79
Who was the first American in space? 0,94 0,80 0,72
Who was Achilles? best friend? 1,00 0,98 0,79
When did the Romans invade Britain? 0,87 0,74 0,82
Definition of metaphor 0,95 0,81 0,38
What is chickenpox? 1,00 0,97 0,68
Define german measles 1,00 0,87 0,80
Types of rhyme 1,00 1,00 0,79
Who was a famous cubist? 0,90 0,75 0,85
When did the Middle Ages begin? 0,91 0,82 0,68
Was there a Trojan war? 0,97 1,00 0,83
Shakespeare?s most famous play? 0,90 0,97 0,83
average 0,94 0,85 0,72
Table 3: Queries and reading level accuracy
5.2.4 Overall utility
At the end of the whole search session, users
answered the question: ?Overall, how was this
search session?? relating to their search experi-
ence with Google and the YourQA system. The
values obtained for U in Table 2 show a clear pref-
erence (a difference of ' 1 on the 7-point scale) of
the users for YourQA, which is very positive con-
56 KRAQ06
sidering that it represents their general judgement
on the system.
5.3 Future work
We plan to run a larger evaluation by including
more metrics, such as user vs system ranking of
results and the contribution of cluster by cluster
presentation. We intend to conduct an evaluation
also involving users with a poor reading level, so
that each evaluator will only examine answers tar-
geted to his/her reading level. We will analyse our
results with respect to the individual reading levels
and the different types of questions proposed.
6 Conclusion
A user-tailored open domain QA system is out-
lined where a user model contributes to elaborat-
ing answers corresponding to the user?s needs and
presenting them efficiently. In this paper we have
focused on how the user?s reading level (a param-
eter in the UM) can be used to filter and re-order
the candidate answer passages. Our preliminary
results show a positive feedback from human as-
sessors on the utility of the system in an informa-
tion seeking domain. Our short term goals involve
performing a more extensive evaluation, exploit-
ing more UM parameters in answer selection and
implementing a dialogue interface to improve the
system?s interactivity.
References
L. Ardissono, L. Console, and I. Torre. 2001. An adap-
tive system for the personalized access to news. AI
Commun., 14(3):129?147.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings of
EACL?99, pages 269?270.
K. Collins-Thompson and J. P. Callan. 2004. A lan-
guage modeling approach to predicting reading dif-
ficulty. In Proceedings of HLT/NAACL.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In ACL Workshop on Paraphrasing:
Paraphrase Acquisition and Applications, pages 9?
16.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference Re-
search on Computational Linguistics (ROCLING X).
J. Kincaid, R. Fishburne, R. Rodgers, and B. Chissom.
1975. Derivation of new readability formulas for
navy enlisted personnel. Technical Report Branch
Report 8-75, Chief of Naval Training.
C. C. T. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In World Wide
Web, pages 150?161.
J. Lin, D. Quan, V. Sinha, and K Bakshi. 2003. What
makes a good answer? the role of context in question
answering. In Proceedings of INTERACT 2003.
Bernardo Magnini and Carlo Strapparava. 2001. Im-
proving user modelling with content-based tech-
niques. In UM: Proceedings of the 8th Int. Confer-
ence, volume 2109 of LNCS. Springer.
James Pitkow, Hinrich Schuetze, Todd Cass, Rob Coo-
ley, Don Turnbull, Andy Edmonds, Eytan Adar, and
Thomas Breuel. 2002. Personalized search. Com-
mun. ACM, 45(9):50?55.
S. Quarteroni and S. Manandhar. 2006. User mod-
elling for adaptive question answering and informa-
tion retrieval. In Proceedings of FLAIRS?06.
M. Steinbach, G. Karypid, and V. Kumar. 2000. A
comparison of document clustering techniques.
L. T. Su. 1991. An investigation to find appropriate
measures for evaluating interactive information re-
trieval. Ph.D. thesis, New Brunswick, NJ, USA.
L. T. Su. 2003. A comprehensive and systematic
model of user evaluation of web search engines: Ii.
an evaluation by undergraduates. J. Am. Soc. Inf.
Sci. Technol., 54(13):1193?1223.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz.
2005. Personalizing search via automated analysis
of interests and activities. In Proceedings of SIGIR
?05, pages 449?456, New York, NY, USA. ACM
Press.
E. M. Voorhees. 2003. Overview of the TREC 2003
question answering track. In Text REtrieval Confer-
ence.
E. M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Text REtrieval Confer-
ence.
S. Williams and E. Reiter. 2005. Generating readable
texts for readers with low basic skills. In Proceed-
ings of ENLG-2005, pages 140?147.
H. Witten and E. Frank. 2000. Data Mining: Practical
Machine Learning Tools and Techniques with Java
Implementation. Morgan Kaufmann.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. 1999. KEA: Practical au-
tomatic keyphrase extraction. In ACM DL, pages
254?255.
57 KRAQ06
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 414?417,
Prague, June 2007. c?2007 Association for Computational Linguistics
UOY: A Hypergraph Model For Word Sense Induction & Disambiguation
Ioannis P. Klapaftis
University of York
Department of Computer Science
giannis@cs.york.ac.uk
Suresh Manandhar
University of York
Department of Computer Science
suresh@cs.york.ac.uk
Abstract
This paper is an outcome of ongoing re-
search and presents an unsupervised method
for automatic word sense induction (WSI)
and disambiguation (WSD). The induction
algorithm is based on modeling the co-
occurrences of two or more words using
hypergraphs. WSI takes place by detect-
ing high-density components in the co-
occurrence hypergraphs. WSD assigns to
each induced cluster a score equal to the sum
of weights of its hyperedges found in the lo-
cal context of the target word. Our system
participates in SemEval-2007 word sense in-
duction and discrimination task.
1 Introduction
The majority of both supervised and unsupervised
approaches to WSD is based on the ?fixed-list? of
senses paradigm where the senses of a target word
is a closed list of definitions coming from a stan-
dard dictionary (Agirre et al, 2006). Lexicographers
have long warned about the problems of such an ap-
proach, since dictionaries are not suited to this task;
they often contain general definitions, they suffer
from the lack of explicit semantic and topical rela-
tions or interconnections, and they often do not re-
flect the exact content of the context, in which the
target word appears (Veronis, 2004).
To overcome this limitation, unsupervised WSD
has moved towards inducing the senses of a target
word directly from a corpus, and then disambiguat-
ing each instance of it. Most of the work in WSI
is based on the vector space model, where the con-
text of each instance of a target word is represented
as a vector of features (e.g second-order word co-
occurrences) (Schutze, 1998; Purandare and Peder-
sen, 2004). These vectors are clustered and the re-
sulting clusters represent the induced senses. How-
ever, as shown experimentally in (Veronis, 2004),
vector-based techniques are unable to detect low-
frequency senses of a target word.
Recently, graph-based methods were employed in
WSI to isolate highly infrequent senses of a target
word. HyperLex (Veronis, 2004) and the adaptation
of PageRank (Brin and Page, 1998) in (Agirre et al,
2006) have been shown to outperform the most fre-
quent sense (MFS) baseline in terms of supervised
recall, but they still fall short of supervised WSD
systems.
Graph-based approaches operate on a 2-
dimensional space, assuming a one-to-one relation-
ship between co-occurring words. However, this
assumption is insufficient, taking into account the
fact that two or more words are usually combined
to form a relationship of concepts in the context.
Additionally, graph-based approaches fail to model
and exploit the existence of collocations or terms
consisting of more than two words.
This paper proposes a method for WSI, which
is based on a hypergraph model operating on
a n-dimensional space. In such a model, co-
occurrences of two or more words are represented
using weighted hyperedges. A hyperedge is a more
expressive representation than a simple edge, be-
cause it is able to capture the information shared
by two or more words. Our system participates in
414
SemEval-2007 word sense induction and discrimi-
nation task (SWSID) (Agirre and Soroa, 2007).
2 Sense Induction & Disambiguation
This section presents the induction and disambigua-
tion algorithms.
2.1 Sense Induction
2.1.1 The Hypergraph Model
A hypergraph H = (V, F ) is a generalization of
a graph, which consists of a set of vertices V and a
set of hyperedges F ; each hyperedge is a subset of
vertices. While an edge relates 2 vertices, a hyper-
edge relates n vertices (where n ? 1). In our prob-
lem, we represent each word by a vertex and any
set of co-occurring related words by a hyperedge.
In our approach, we restrict hyperedges to 2, 3 or
4 words. Figure 1 shows an example of an abstract
hypergraph model 1.
Figure 1: An example of a Hypergraph
The degree of a vertex is the number of hyper-
edges it belongs to, and the degree of a hyperedge is
the number of vertices it contains. A path in the hy-
pergraph model is a sequence of vertices and hyper-
edges such as v1, f1, ..., vi?1, fi?1, vi, where vk are
vertices, fk are hyperedges, each hyperedge fk con-
tains vertices to its left and right in the path and no
hyperedge or vertex is repeated. The length of a path
is the number of hyperedges it contains, the distance
between two vertices is the shortest path between
them and the distance between two hyperedges is the
minimum distance of all the pairs of their vertices.
2.1.2 Building The Hypergraph
Let bp be the base corpus from which we induce
the senses of a target word tw. Our bp consists of
BNC and all the SWSID paragraphs containing the
1Image was taken from Wikipedia (Rocchini, 2006)
target word. The total size of bp is 2000 paragraphs.
Note that if SWSID paragraphs of tw are more than
2000, BNC is not used.
In order to build the hypergraph, tw is removed
from bp and each paragraph pi is POS-tagged. Fol-
lowing the example in (Agirre et al, 2006), only
nouns are kept and lemmatised. We apply two fil-
tering heuristics. The first one is the minimum fre-
quency of nouns (parameter p1), and the second one
is the minimum size of a paragraph (parameter p2).
A key problem at this stage is the determination of
related vertices (nouns), which can be grouped into
hyperedges and the weighting of each such hyper-
edge. We deal with this problem by using associa-
tion rules (Agrawal and Srikant, 1994). Frequent hy-
peredges are detected by calculating support, which
should exceed a user-defined threshold (parameter
p3).
Let f be a candidate hyperedge and a, b, c its ver-
tices. Then freq(a, b, c) is the number of para-
graphs in bp, which contain all the vertices of f , and
n is the total size of bp. Support of f is shown in
Equation 1.
support(f) =
freq(a, b, c)
n
(1)
The weight assigned to each collected hyperedge,
f , is the average of m calculated confidences, where
m is the size of f . Let f be a hyperedge containing
the vertices a, b, c. The confidence for the rule r0 =
{a, b} => {c} is defined in Equation 2.
confidence(r0) =
freq(a, b, c)
freq(a, b)
(2)
Since there is a three-way relationship among a, b
and c, we have two more rules r1 = {a, c} => {b}
and r2 = {b, c} => {a}. Hence, the weighting of
f is the average of the 3 calculated confidences. We
apply a filtering heuristic (parameter p4) to remove
hyperedges with low weights from the hypergraph.
At the end of this stage, the constructed hypergraph
is reduced, so that our hypergraph model agrees with
the one described in subsection 2.1.1.
2.1.3 Extracting Senses
Preliminary experiments on 10 nouns of
SensEval-3 English lexical-sample task (Mihalcea
et al, 2004) (S3LS), suggested that our hypergraphs
415
are small-world networks, since they exhibited
a high clustering coefficient and a small average
path length. Furthermore, the frequency of vertices
with a given degree plotted against the degree
showed that our hypergraphs satisfy a power-law
distribution P (d) = c ? d??, where d is the vertex
degree, P (d) is the frequency of vertices with
degree d. Figure 2 shows the log-log plot for the
noun difference of S3LS.
Figure 2: Log-log plot for the noun difference.
In order to extract the senses of the target word,
we modify the HyperLex algorithm (Veronis, 2004)
for selecting the root hubs of the hypergraph as fol-
lows. At each step, the algorithm finds the vertex vi
with the highest degree, which is selected as a root
hub, according to two criteria.
The first one is the minimum number of hyper-
edges it belongs to (parameter p5), and the second is
the average weight of the first p5 hyperedges (para-
meter p6) 2. If these criteria are satisfied, then hyper-
edges containing vi are grouped to a single cluster cj
(new sense) with a 0 distance from vi, and removed
from the hypergraph. The process stops, when there
is no vertex eligible to be a root hub.
Each remaining hyperedge, fk, is assigned to the
cluster, cj , closest to it, by calculating the minimum
distance between fk and each hyperedge of cj as de-
fined in subsection 2.1.1. The weight assigned to fk
is inversely proportional to its distance from cj .
2.2 Word Sense Disambiguation
Given an instance of the target word, tw, paragraph
pi containing tw is POS-tagged, nouns are kept and
2Hyperedges are sorted in decreasing order of weight
lemmatised. Next, each induced cluster cj is as-
signed a score equal to the sum of weights of its
hyperedges found in pi.
3 Evaluation
3.1 Preliminary Experiments
This method is an outcome of ongoing research.
Due to time restrictions we were able to test and
tune (Table 1), but not optimize, our system only on
a very small set of nouns of S3LS targeting at a high
supervised recall. Our supervised recall on the 10
first nouns of S3LS was 66.8%, 9.8% points above
the MFS baseline.
Parameter Value
p1:Minimum frequency of a noun 8
p2:Minimum size of a paragraph 4
p3:Support threshold 0.002
p4:Average confidence threshold 0.2
p5:Minimum number of hyperedges 6
p6:Minimum average weight of hyperedges 0.25
Table 1: Chosen parameters for our system
3.2 SemEval-2007 Results
Tables 2 and 3 show the average supervised recall,
FScore, entropy and purity of our system on nouns
and verbs of the test data respectively. The submit-
ted answer consisted only of the winning cluster per
instance of a target word, in effect assigning it with
weight 1 (default).
Entropy measures how well the various gold stan-
dard senses are distributed within each cluster, while
purity measures how pure a cluster is, containing ob-
jects from primarily one class. In general, the lower
the entropy and the larger the purity values, the bet-
ter the clustering algorithm performs.
Measure Proposed methodology MFS
Entropy 25.5 46.3
Purity 89.8 82.4
FScore 65.8 80.7
Sup. Recall 81.6 80.9
Table 2: System performance for nouns.
For nouns our system achieves a low entropy and
a high purity outperforming the MFS baseline, but a
lower FScore. This can be explained by the fact that
the average number of clusters we produce for nouns
is 11, while the gold standard average of senses is
around 2.8. For verbs the performance of our system
416
is worse than for nouns, although entropy and purity
still outperform the MFS baseline. FScore is very
low, despite that the average number of clusters we
produce for verbs (around 8) is less than the number
of clusters we produce for nouns. This means that
for verbs the senses of gold standard are much more
spread among induced clusters than for nouns, caus-
ing a low unsupervised recall. Overall, FScore re-
sults are in accordance with the idea of microsenses
mentioned in (Agirre et al, 2006). FScore is biased
towards clusters similar to the gold standard senses
and cannot capture that theory.
Measure Proposed methodology MFS
Entropy 28.9 44.4
Purity 82.0 77
F-score 45.1 76.8
Sup. Recall 73.3 76.2
Table 3: System performance for verbs.
Our supervised recall for verbs is 73.3%, and be-
low the MFS baseline (76.2%), which no system
managed to outperform. For nouns our supervised
recall is 81.6%, which is around 0.7% above the
MFS baseline. In order to fully examine the perfor-
mance of our system we applied a second evaluation
of our methodology using the SWSID official soft-
ware.
The solution per target word instance included the
entire set of clusters with their associated weights
(Table 4). Results show that the submitted answer
(instance - winning cluster), was degrading seri-
ously our performance both for verbs and nouns due
to the loss of information in the mapping step.
POS Proposed Methodology MFS
Nouns 84.3 80.9
Verbs 75.6 76.2
Total 80.2 78.7
Table 4: Supervised recall in second evaluation.
Our supervised recall for nouns has outperformed
the MFS baseline by 3.4% with the best system
achieving 86.8%. Performance for verbs is 75.6%,
0.6% below the best system and MFS.
4 Conclusion
We have presented a hypergraph model for word
sense induction and disambiguation. Preliminary
experiments suggested that our reduced hypergraphs
are small-world networks. WSI identifies the highly
connected components (hubs) in the hypergraph,
while WSD assigns to each cluster a score equal to
the sum of weights of its hyperedges found in the
local context of a target word.
Results show that our system achieves high en-
tropy and purity performance outperforming the
MFS baseline. Our methodology achieves a low
FScore producing clusters that are dissimilar to the
gold standard senses. Our supervised recall for
nouns is 3.4% above the MFS baseline. For verbs,
our supervised recall is below the MFS baseline,
which no system managed to outperform.
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
2: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007. ACL.
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art wsd. In Proceedings of the EMNLP
Conference, pages 585?593. ACL.
Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast
algorithms for mining association rules in large data-
bases. In VLDB ?94: Proceedings of the 20th Inter-
national Conference on Very Large DataBases, pages
487?499, USA. Morgan Kaufmann Publishers Inc.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The senseval-3 english lexical sample task.
In R. Mihaleca and P. Edmonds, editors, SensEval-3
Proceedings, pages 25?28, Spain, July. ACL.
Amruta Purandare and Ted Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of CoNLL-2004,
pages 41?48. ACL.
Claudio Rocchini. 2006. Hypergraph sample image.
Wikipedia.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Jean Veronis. 2004. Hyperlex:lexical cartography for
information retrieval. Computer Speech & Language,
18(3).
417
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 138?147,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Argumentative Human Computer Dialogue for Automated Persuasion
Pierre Andrews* and Suresh Manandhar* and Marco De Boni**
* Department of Computer Science
University of York
York YO10 5DD
UK
{pandrews,suresh}@cs.york.ac.uk
** Unilever Corporate Research
Bedford MK44 1LQ
UK
Marco.De-Boni@unilever.com
Abstract
Argumentation is an emerging topic in the
field of human computer dialogue. In this
paper we describe a novel approach to dia-
logue management that has been developed to
achieve persuasion using a textual argumen-
tation dialogue system. The paper introduces
a layered management architecture that mixes
task-oriented dialogue techniques with chat-
bot techniques to achieve better persuasive-
ness in the dialogue.
1 Introduction
Human computer dialogue is a wide research area
in Artificial Intelligence. Computer dialogue is
now used at production stage for applications such
as tutorial dialogue ? that helps teaching students
(Freedman, 2000) ? task-oriented dialogue ? that
achieves a particular, limited task, such as book-
ing a trip (Allen et al, 2000) ? and chatbot dialogue
(Levy et al, 1997) ? that is used within entertain-
ment and help systems.
None of these approaches use persuasion as a
mechanism to achieve dialogue goals. However,
research towards the use of persuasion in Hu-
man Computer Interactions has spawned around the
field of natural argumentation (Norman and Reed,
2003). Similarly research on Embodied Con-
versational Agents (ECA) (Bickmore and Picard,
2005) is also attempting to improve the persuasive-
ness of agents with persuasion techniques; how-
ever, it concentrates on the visual representation
of the interlocutor rather than the dialogue man-
agement. Previous research on human computer
dialogue has rarely focused on persuasive tech-
niques (Guerini, Stock, and Zancanaro, 2004, initi-
ated some research in that field). Our dialogue man-
agement system applies a novel method, taking ad-
vantage of persuasive and argumentation techniques
to achieve persuasive dialogue.
According to the cognitive dissonance theory
(Festinger, 1957), people will try to minimise the
discrepancy between their behaviour and their be-
liefs by integrating new beliefs or distorting existing
ones. In this paper, we approach persuasion as a pro-
cess shaping user?s beliefs to eventually change their
behaviour.
The presented dialogue management system has
been developed to work on known limitations of cur-
rent dialogue systems:
The impression of lack of control is an issue when
the user is interacting with a purely task-oriented di-
alogue system (Farzanfar et al, 2005). The system
follows a plan to achieve the particular task, and the
user?s dialogue moves are dictated by the planner
and the plan operators.
The lack of empathy of computers is also a
problem in human-computer interaction for applica-
tions such as health-care, where persuasive dialogue
could be applied (Bickmore and Giorgino, 2004).
The system does not respond to the user?s personal
and emotional state, which sometimes lowers the
user?s implication in the dialogue. However, exist-
ing research (Klein, Moon, and Picard, 1999) shows
that a system that gives appropriate response to the
user?s emotion can lower frustration.
In human-human communication, these lim-
itations reduce the effectiveness of persuasion
138
(Stiff and Mongeau, 2002). Even if the response to-
wards the computer is not always identical to the
one to humans, it seems sensible to think that per-
suasive dialogue systems can be improved by apply-
ing known findings from human-human communi-
cation.
The dialogue management architecture described
in this paper (see Figure 1) addresses these dialogue
management issues by using a novel layered ap-
proach to dialogue management, allowing the mix-
ing of techniques from task-oriented dialogue man-
agement and chatbot techniques (see Section 4).
Figure 1: Layered Management Architecture
The use of a planner guarantees the consistency
of the dialogue and the achievement of persuasive
goals (see Section 4.2). Argumentative dialogue can
be seen as a form of task-oriented dialogue where
the system?s task is to persuade the user by present-
ing the arguments. Thus, the dialogue manager first
uses a task-oriented dialogue methodology to cre-
ate a dialogue plan that will determine the content
of the dialogue. The planning component?s role is
to guarantee the consistency of the dialogue and the
achievement of the persuasive goals.
In state-of-the-art task-oriented dialogue manage-
ment systems, the planner provides instructions for
a surface realizer (Green and Lehman, 2002), re-
sponsible of generating the utterance corresponding
to the plan step. Our approach is different to al-
low more reactivity to the user and give a feeling
of control over the dialogue. In this layered ap-
proach, the reactive component provides a direct re-
action to the user input, generating one or more ut-
terances for a given plan step, allowing for reactions
to user?s counter arguments as well as backchannel
and chitchat phases without cluttering the plan.
Experimental results show that this layered ap-
proach allows the user to feel more comfortable in
the dialogue while preserving the dialogue consis-
tency provided by the planner. Eventually, this trans-
lates into a more persuasive dialogue (see Section 6).
2 Related Work
Persuasion through dialogue is a novel
field of Human Computer Interaction.
Reiter, Robertson, and Osman (2003),Reed (1998)
and Carenini and Moore (2000) apply persuasive
communication principles to natural language
generation, but only focus on monologue.
The 3-tier planner for tutoring dialogue by
Zinn, Moore, and Core (2002) provides a di-
alogue management technique close to our
approach: a top-tier generates a dialogue plan,
the middle-tier generates refinements to the
plan and the bottom-tier generates utterances.
Mazzotta, de Rosis, and Carofiglio (2007) also
propose a planning framework for user-adapted
persuasion where the plan operators are mapped
to natural language (or ECA) generation. How-
ever, these planning approaches do not include a
mechanism to react to user?s counter arguments
that are difficult to plan beforehand. This paper
propose a novel approach that could improve
the user?s comfort in the dialogue as well as its
persuasiveness.
3 Case Study
Part of the problem in evaluating persuasive dia-
logue is using an effective evaluation framework.
Moon (1998) uses the Desert Survival Scenario to
evaluate the difference of persuasion and trust in
interaction between humans when face-to-face or
when mediated by a computer system (via an instant
messaging platform).
The Desert Survival Scenario
(Lafferty, Eady, and Elmers, 1974) is a negoti-
ation scenario used in team training. The team is
put in a scenario where they are stranded in the
desert after a plane crash. They have to negotiate a
ranking of the most eligible items (knife, compass,
map, etc.) that they should keep for their survival.
For the evaluation of the dialogue system, a simi-
lar scenario is presented to the participants. The user
has to choose an initial preferred ranking of items
139
and then engages in a discussion with the dialogue
system that tries to persuade the user to change the
ranking. At the end of the dialogue, the user has the
opportunity to either change or keep the ranking.
The architecture of the dialogue system is de-
scribed throughout this paper using examples from
the Desert Scenario. The full evaluation protocol is
described in Section 5 and 6.
4 Dialogue Management Architecture
The following sections provide a description of
the dialogue management architecture introduced in
Figure 1.
4.1 Argumentation Model
The Argumentation model represents the different
arguments (conclusions and premises) that can be
proposed by the user or by the system. Figure 2
gives a simplified example of the Desert Scenario
model.
Figure 2: Argumentation Model Sample
This model shows the different facts that are
known by the system and the relations be-
tween them. Arrows represent the support re-
lation between two facts. For example, res-
cue knows where you are is a support to the fact
goal(signal) (the user goal is to signal presence to
the rescue) as well as a support to goal(stay put) (the
user goal is to stay close to the wreckage). This
relational model is comparable to the argumenta-
tion framework proposed by Dung (1995), but stores
more information about each argument for reason-
ing within the planning and reactive component (see
Section 4.2).
Each fact in this model represents a belief to be
introduced to the user. For example, when the dia-
logue tries to achieve the goal reorder(flashlight >
air map): the system wants the user to believe that
the ?flashlight? item should be ranked higher than
the ?air map? item. The argumentation model de-
scribes the argumentation process that is required
to introduce this new belief: the system first has to
make sure the user believes in rate lower(air map)
and rate higher(flashlight).
Lower level facts (see Figure 2) are the goal facts
of the dialogue, the ones the system chooses as di-
alogue goals, according to known user beliefs and
the system?s goal beliefs (e.g. according to the rank-
ing the system is trying to defend). The facts in the
middle of the hierarchy are intermediate facts that
need to be asserted during the dialogue. The top-
level facts are world knowledge: facts that require
minimum defense and can be easily grounded in the
dialogue.
4.2 Planning Component
The planning component?s task is to find a plan us-
ing the argumentation model to introduce the re-
quired facts in the user?s belief to support the per-
suasive goals. The plan is describes a path in the ar-
gumentation model beliefs hierarchy that translates
to argumentation segments in the dialogue.
In our current evaluation method, the goal of the
dialogue is to change the user?s beliefs about the
items so that the user eventually changes the rank-
ing. At the beginning of the dialogue, the ranking of
the system is chosen and persuasive goals are com-
puted for the dialogue. These persuasive goals cor-
respond to the lower level facts in the argumentation
model ? like ?reorder(flashlight > air map)? in our
previous example. The available planning operators
are:
use world(fact) describes a step in the dialogue
that introduces a simple fact to the user.
ground(fact) describes a step in the dialogue that
grounds a fact in the user beliefs. Grounding a fact
is a different task from the use world operator as it
will need more support during the dialogue.
do support([fact0, fact1, . . . ], fact2) describes a
complex support operation. The system will initiate
a dialogue segment supporting fact2 with the facts
fact1 and fact0, etc. that have previously been intro-
duced in the user beliefs.
The planning component can also use two
non-argumentative operators, do greetings and
140
do farewells, that are placed respectively at the be-
ginning and the end of the dialogue plan to open and
close the session.
Here is an example plan using the two argu-
ments described in Figure 2 to support the goal re-
order(flashlight > air map):
Step 1 do greetings
Step 2 use world(goal(be found))
ground(rescue knows where you are)
ground(can(helpatnight,
item(flashlight)))
Step 3 do support([can(helpatnight,
item(flashlight))],
rate higher(item(flashlight)))
do support(
[rescue knows where you are,
goal(be found)],
goal(stay put))
Step 4 do support([goal(stay put)],
rate lower(item(air map)))
Step 5 do support(...,
reorder(item(flashlight),
item(air map)))
Step 6 do farewells
The plan is then interpreted by the reactive com-
ponent that is responsible for realizing each step in
a dialogue segment.
4.3 The Reactive Component
The reactive component?s first task is to realize the
operators chosen by the planning component into di-
alogue utterance(s). However, it should not be mis-
taken for a surface language realizer. The reactive
component?s task, when realizing the operator, is to
decide how to present the particular argumentation
operator and its parameters to the user according to
the dialogue context and the user?s reaction to the
argument. This reactive process is described in the
following sections.
4.3.1 Realization and Reaction Strategies
Each step of the plan describes the general topic
of a dialogue segment1. A dialogue segment is
a set of utterances from the system and from
1i.e. it is not directly interpreted as an instruction to generate
one unique utterance.
the user that are related to a particular argument.
For example, in the Desert Scenario, the operator
ground(can(helpatnight, item(flashlight))) may re-
sult in the following set of utterances:
S(ystem) I think the flashlight could
be useful as it could help us at
night,
U(ser) How is that? We are not going
to move during the night.
S well, if we want to collect water,
it will be best to do things at
night and not under the burning
sun.
U I see. It could be useful then.
In this example, the ground operator has been re-
alized by the reactive component in two different ut-
terances to react to the user?s interaction.
The goal of the reactive component is to make the
user feel that the system understands what has been
said. It is also important to avoid replanning as it
tries to defend the arguments chosen in the plan.
As described in Section 4.2, the planner relies on
the argumentation model to create a dialogue plan.
Encoding all possible defenses and reactions to the
user directly in this model will explode the search
space of the planner and require careful authoring
to avoid planning inconsistencies2 . In addition, pre-
dicting at the planning level what counter arguments
a user is likely to make requires a prior knowledge
of the user?s beliefs. At the beginning of a one-off
dialogue, it is not possible to make prior assump-
tions on the user?s beliefs; the system has a shal-
low knowledge of the user?s beliefs and will discover
them as the dialogue goes.
Hence, it is more natural to author a reactive di-
alogue that will respond to the user?s counter ar-
guments as they come and extends the user beliefs
model as it goes. In our architecture if the user is
disagreeing with an argument, the plan is not revised
directly; if possible, the reactive component selects
new, contextually appropriate, supporting facts for
the current plan operator. It can do this multiple
consecutive local repairs if the user needs more con-
vincing and the domain model provides enough de-
fenses. This allows for a simpler planning frame-
work.
2a new plan could go against the previously used arguments.
141
In addition, when available, and even if the user
agrees with the current argument, the reactive com-
ponent can also choose from a set of ?dialogue
smoothing? or backchannel utterances to make the
dialogue feel more natural. Here is an example from
the Desert Scenario:
S We don?t have much water, we need to
be rescued as soon as possible.
(from plan step: user world( goal(be found)))
U right
S I am glad we agree.(backchannel)
S There is a good chance that the
rescue team already knows our
whereabouts. We should be
optimistic and plan accordingly,
don?t you think?
(from plan step:
use world( rescue knows where you are))
4.3.2 Detecting user reactions
The reactive component needs to detect if the user
is agreeing to its current argument or resisting the
new fact that is presented. Because the dialogue
management system was developed from the per-
spective of a system that could be easily ported to
different domains, choice was made to use a domain
independent and robust agreement/disagreement de-
tection.
The agreement/disagreement detection is based
on an utterance classifier. The classifier is a cas-
cade of binary Support Vector Machines (SVM)
(Vapnik, 2000) trained on the ICSI Meeting cor-
pus (Janin et al, 2003). The corpus contains 8135
spurts3 annotated with agreement/disagreement in-
formation Hillard, Ostendorf, and Shriberg (2003).
A multi-class SVM classifier is trained on local
features of the spurts such as a) the length of the
spurt, b) the first word of the spurt, c) the bigrams of
the spurts, and d) part of speech tags. The classifica-
tion achieves an accuracy of 83.17% with an N-Fold
4 ways split cross validation. Additional results and
comparison with state-of-the-art are available in Ap-
pendix A.
During the dialogue, the classifier is applied on
each of the user?s utterances, trying to determine if
the user is agreeing or disagreeing with the system.
3speech utterances that have no pauses longer than .5 sec-
onds.
According to this labelling, the strategies described
in section 4.3.1 and 4.3.3 are applied.
4.3.3 Revising the plan
The reactive component will attempt local repairs
to the plan by defending the argumentation move
chosen by the planning component. However, there
are cases when the user will still not accept an ar-
gument. In these cases, imposing the belief to the
user is counter-productive and the current goal be-
lief should be dropped from the plan.
For each utterance chosen by the reactive com-
ponent, the belief model of the user is updated to
represent the system knowledge of the user?s be-
liefs. Every time the user agrees to an utterance
from the system, the belief model is extended with
a new belief; in the previous example, when the
user says ?I see, it could be useful then.?, the sys-
tem detects an agreement (see the Section 4.3.2)
and extends the user?s beliefs model with the be-
lief: can(helpatnight, item(flashlight)). The agree-
ment is then followed by a local repair, since the
user doesn?t disagree with the statement made, the
system also extends the belief model with beliefs rel-
evant to the content of the local repair, thus learning
more about the user?s belief model.
As a result of this process, when the system de-
cides to revise the plan, the planning component
does not start from the same beliefs state as previ-
ously. In effect, the system is able to learn user?s be-
liefs based on the agreement/disagreement with the
user, it can therefore make a more effective use of
the argumentation hierarchy to find a better plan to
achieve the persuasive goals.
Still, there are some cases when the planning
component will be unable to find a new plan from
the current belief state to the goal belief state ? this
can happen when the planner has exhausted all its
argumentative moves for a particular sub-goal. In
these cases, the system has to make concessions and
drop the persuasive goals that it cannot fulfil. By
dropping goals, the system will lower the final per-
suasiveness, but guarantees not coercing the user.
4.3.4 Generation
Utterance generation is made at the reactive com-
ponent level. In the current version of the dia-
logue management system, the utterance generation
142
is based on an extended version of Alicebot AIML
4
.
AIML is an XML language that provides a pat-
tern/template generation model mainly used for
chatbot systems. An AIML bot defines a set of
categories that associate a topic, the context of the
previous bot utterance (called that in the AIML ter-
minology), a matching pattern that will match the
last user utterance and a generation template. The
topic, matching and that field define matching pat-
terns that can contain * wildcards accepting any to-
ken(s) of the user utterance (e.g. HELLO * would
match any utterance starting by ?Hello?). They are
linked to a generation template that can reuse the to-
kens matched by the patterns wildcards to generate
an utterance tailored to the user input and the dia-
logue context.
For the purpose of layered dialogue management,
the AIML language has been extended to include
more features: 1) A new pattern slot has been in-
troduced to link a set of categories to a particular ar-
gumentation operator; 2) Utterances generations are
linked to the belief they are trying to introduce to
the user and if an agreement is detected, this belief
is added to the user belief model.
For example, a set of matching categories for the
Desert Scenario could be:
Plan operator: use world(goal(survive))
Category 1 :
Pattern *
Template Surviving is our
priority, do you want
to hear about my desert
survival insights?
Category 2 :
Pattern * insights
That * survival insights
Template I mean, I had a few
ideas ...common knowledge I
suppose.
Category 3 :
Pattern *
That * survival insights
Template Well, we are in this
together. Let me tell you
of what I think of desert
survival, ok?
4http://www.alicebot.org/
These three categories can be used to match
the user reaction during the dialogue seg-
ment corresponding to the plan operator:
use world(goal(survive)). Category 1 is used
as the initiative taking generation. It will be the
first one to be used when the system comes from
a previously finished step. Categories 2-3 are all
?defenses? that support Category 1. They will be
used to react to the user if no agreement is detected
from the last utterances. For example, if the user
says ?what kind of survival insights??? as a reply
to the generation from Category 1, a disagreement
is detected and the reactive component will have a
contextualised answer as given by category 2 whose
that pattern matches the last utterance from the
system, the pattern pattern matches the user
utterance.
The dialogue management system uses 187 cate-
gories tailored to the Desert Scenario as well as 3737
general categories coming from the Alice chatbot
and used to generate the dialogue smoothing utter-
ances. Developing domain specific reactions is a te-
dious and slow process that was iteratively achieved
with Wizard of OZ experiments with real users. In
these experiments, users were told they were going
to have a dialogue with another human in the Desert
Scenario context. The dialogue system manages the
whole dialogue, except for the generation phase that
is mediated by an expert that can either choose the
reaction of the system from an existing set of utter-
ances, or type a new one.
5 Persuasiveness Metric
Evaluating a behavior change would require a long-
term observation of the behavior that would be de-
pendent to external elements (Bickmore and Picard,
2005). To evaluate our system, an evaluation proto-
col measuring the change in the beliefs underlying
the behavior was chosen. As explained in Section 3,
the Desert Scenario is used as a base for the evalu-
ation. Each participant is told that he is stranded in
the desert. The user gives a preferred initial rank-
ing Ri of the items (knife, compass, map, etc.). The
user then engages in a dialogue with the system. The
system then attempts to change the user?s ranking to
a different ranking Rs through persuasive dialogue.
At the end of the dialogue, the user can change this
143
choice to arrive at a final ranking Rf .
The persuasiveness of the dialogue can be mea-
sured as the evolution of the distance between
the user ranking (Ri, Rf ) and the system ranking
(Rs). The Kendall ? distance (Kendall, 1938) is
used to compute the pairwise disagreement between
two rankings. The change of the Kendall ? dis-
tance during the dialogue gives an evaluation of the
persuasiveness of the dialogue: Persuasiveness =
K?(Ri, Rs) ? K?(Rf , Rs). In the current evalu-
ation protocol, the Rs is always the reverse of the
Ri, so K?(Ri, Rs) is always the maximum distance
possible: n?(n?1)2 where n is the number of items to
rank. The minimum Kendall tau distance is 0. If the
system was persuasive enough to make the user in-
vert the initial ranking, Persuasiveness of the system
is maximum and equal to: n?(n?1)2 . If the system
does not succeed in changing the user ranking, then
Persuasiveness is zero.
6 Evaluation Results and Discussion
16 participants have been recruited from a variety of
ages (from 20 to 59) and background. They were
all told to use a web application that describes the
Desert Scenario (see Section 3) and proposes to un-
dertake two instant messaging chats with two human
users5. However, both discussions are managed by
different versions of the dialogue system, following
a similar protocol:
? one version of the dialogue is managed by a
limited version of the dialogue system, with no
reactive component. This version is similar to
a purely task-oriented system, planning and re-
vising the plan directly on dialogue failures,
? the second version is the full dialogue system
as described in this paper.
Each participant went through one dialogue with
each system, in a random order. This comparison
shows that the dialogue flexibility provided by the
reactive component allows a more persuasive dia-
logue. In addition, when faced with the second dia-
logue, the participant has formed more beliefs about
the scenario and is more able to counter argue.
5The evaluation is available Online at
http://www.cs.york.ac.uk/aig/eden
Figure 3: Comparative Results. interpret, not coercive,
perceived persuasion are on a scale of [0 ? 4] (see Ap-
pendix B). Persuasiveness is on a scale of [?10, 10].
Figure 3 reports the independent Persuasiveness
metric results as well as interesting answers to a
questionnaire that the participants filled after each
dialogue (see the Appendix B for detailed results
and questionnaire).
Over all the dialogues, the full system is 18%
more persuasive than the limited system. This is
measured by the Persuasiveness metric introduced in
Section 5. With the full system, the participants did
an average of 1.33 swaps of items towards the sys-
tem?s ranking. With the limited system, the partic-
ipants did an average of 0.47 swaps of items away
from the system?s ranking. However, the answers
to the self evaluated perceived persuasion question
show that the participants did not see any significant
difference in the ability to persuade of the limited
and the full systems.
According to the question interpret, the partici-
pants found that the limited system understood bet-
ter what they said. This last result might be ex-
plained by the behavior of the systems: the limited
system drops an argument at every user disagree-
ment, making the user believe that the disagreement
was understood. The full system tries to defend the
argument; if possible with a contextually tailored
support, however, if this is not available, it may use a
generic support, making the user believe he was not
fully understood.
Our interpretation of the fact that the discrepancy
between user self evaluation of the interaction with
the system and the measured persuasion is that, even
if the full system is more argumentative, the user
144
didn?t feel coerced6. These results show that a more
persuasive dialogue can be achieved without deteri-
orating the user perception of the interaction.
7 Conclusion
Our dialogue management system introduces a
novel approach to dialogue management by using
a layered model mixing the advantages of state-of-
the-art dialogue management approaches. A plan-
ning component tailored to the task of argumenta-
tion and persuasion searches the ideal path in an ar-
gumentation model to persuade the user. To give a
reactive and natural feel to the dialogue, this task-
oriented layer is extended by a reactive component
inspired from the chatbot dialogue management ap-
proach. The Desert Scenario evaluation, providing
a simple and independent metric for the persuasive-
ness of the dialogue system provided a good proto-
col for the evaluation of the dialogue system. This
one showed to be 18% more persuasive than a purely
task-oriented system that was not able to react to the
user interaction as smoothly.
Our current research on the dialogue management
system consists in developing another evaluation do-
main where a more complex utterance generation
can be used. This will allow going further than the
simple template based system, offering more diverse
answers to the user and avoiding repetitions; it will
also allow us to experiment textual persuasion tai-
lored to other parameters of the user representation,
such as the user personality.
References
Allen, J. F., G. Ferguson, B. W. Miller, E. K. Ringger, and
T. Sikorski. 2000. Dialogue Systems: From Theory to
Practice in TRAINS-96, chapter 14.
Bickmore, T. and T. Giorgino. 2004. Some novel aspects
of health communication from a dialogue systems per-
spective. In AAAI Fall Symposium.
Bickmore, T. W. and R. W. Picard. 2005. Establishing and
maintaining long-term human-computer relationships.
ACM Trans. Comput.-Hum. Interact., 12(2):293?327.
Carenini, G. and J. Moore. 2000. A strategy for generat-
ing evaluative arguments. In International Conference
on Natural Language Generation.
Dung, P. M. 1995. On the acceptability of arguments
and its fundamental role in nonmonotonic reasoning,
6The answers to the not coercive question do not show any
significant difference in the perception of coercion of the two
system.
logic programming and n-person games. Artif. Intell.,
77(2):321?357.
Farzanfar, R., S. Frishkopf, J. Migneault, and R. Fried-
man. 2005. Telephone-linked care for physical ac-
tivity: a qualitative evaluation of the use patterns of
an information technology program for patients. J. of
Biomedical Informatics, 38(3):220?228.
Festinger, Leon. 1957. A Theory of Cognitive Disso-
nance. Stanford University Press.
Freedman, R. 2000. Plan-based dialogue management in
a physics tutor. In Proceedings of ANLP ?00.
Galley, M., K. Mckeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: use of bayesian networks to model
pragmatic dependencies. In Proceedings of ACL?04.
Green, N. and J. F. Lehman. 2002. An integrated dis-
course recipe-based model for task-oriented dialogue.
Discourse Processes, 33(2):133?158.
Guerini, M., O. Stock, and M. Zancanaro. 2004. Per-
suasive strategies and rhetorical relation selection. In
Proceedings of ECAI-CMNA.
Hillard, D., M. Ostendorf, and E. Shriberg. 2003. Detec-
tion of agreement vs. disagreement in meetings: train-
ing with unlabeled data. In Proceedings of NAACL?03.
Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proceedings of ICASSP?03.
Kendall, M. G. 1938. A new measure of rank correlation.
Biometrika, 30(1/2):81?93.
Klein, J., Y. Moon, and R. W. Picard. 1999. This com-
puter responds to user frustration. In CHI?99.
Lafferty, J. C., Eady, and J. Elmers. 1974. The desert
survival problem.
Levy, D., R. Catizone, B. Battacharia, A. Krotov, and
Y. Wilks. 1997. Converse:a conversational compan-
ion. In Proceedings of 1st International Workshop on
Human-Computer Conversation.
Mazzotta, I., F. de Rosis, and V. Carofiglio. 2007. Por-
tia: A user-adapted persuasion system in the healthy-
eating domain. Intelligent Systems, IEEE, 22(6).
Moon, Y. 1998. The effects of distance in local versus
remote human-computer interaction. In Proceedings
of SIGCHI?98.
Norman, Timothy J. and Chris Reed. 2003. Argumenta-
tion Machines : New Frontiers in Argument and Com-
putation (Argumentation Library). Springer.
Reed, C. 1998. Generating Arguments in Natural Lan-
guage. Ph.D. thesis, University College London.
Reiter, E., R. Robertson, and L. M. Osman. 2003.
Lessons from a failure: generating tailored smoking
cessation letters. Artif. Intell., 144(1-2):41?58.
Stiff, J. B. and P. A. Mongeau. 2002. Persuasive Commu-
nication, second edition.
Vapnik, V. N. 2000. The Nature of Statistical Learning
Theory.
Zinn, C., J. D. Moore, and M. G. Core. 2002. A 3-tier
planning architecture for managing tutorial dialogue.
In Proceedings of ITS ?02.
145
A Agreement/Disagreement Classification
Setup 1 Setup 2
Galley et al, global features 86.92% 84.07%
Galley et al, local features 85.62% 83.11%
Hillard et al 82% NA
SVM 86.47% 83.17%
Table 1: Accuracy of different agreement/disagreement
classification approaches.
The accuracy of state-of-the-art techniques
(Hillard, Ostendorf, and Shriberg (2003) and
Galley et al (2004)) are reported in Table 1 and
compared to our SVM classifier. Two experimental
setups were used:
Setup 1 reproduces Hillard, Ostendorf, and Shriberg
(2003) training/testing split between meetings;
Setup 2 reproduces the N-Fold, 4 ways split used by
Galley et al (2004).
The SVM results are arguably lower than Galley et al
system with labeled dependencies. However, this is be-
cause our system only relies on local features of each
utterance, while Galley et al (2004) use global features
(i.e. features describing relations between consecutive ut-
terances) suggest that adding global features would also
improve the SVM classifier.
B Evaluation Questionnaire
In the evaluation described in section 6, the participants
were asked to give their level of agreement with each
statement on the scale: Strongly disagree (0), Disagree
(1), Neither agree nor disagree (2), Agree (3), Strongly
Agree(4). Table 2 provides a list of questions with the
average agreement level and the result of a paired t-test
between the two system results.
146
label question full system limited system ttest
interpret ?In the conversation, the other user inter-
preted correctly what you said?
1.73 2.13 0.06
perceived persuasion ?In the conversation, the other user was
persuasive?
2.47 2.53 0.44
not coercive ?The other user was not forceful in
changing your opinion?
2.4 2.73 0.15
sluggish ?The other user was sluggish and slow to
reply to you in this conversation?
1.27 1.27 0.5
understand ?The other user was easy to understand
in the conversation?
3.2 3.13 0.4
pace ?The pace of interaction with the other
user was appropriate in this conversa-
tion?
2.73 3.07 0.1
friendliness ?The other user was friendly? 2.93 2.87 0.4
length length of the dialogue 12min 19s 08min 33s 0.07
persuasiveness Persuasiveness 1.33 -0.47 0.05
Table 2: Results from the evaluation questionnaire.
147
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 10?17,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Utilizing Contextually Relevant Terms in Bilingual Lexicon Extraction
Azniah Ismail
Department of Computer Science
University of York
York YO10 5DD UK
azniah@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
University of York
York YO10 5DD UK
suresh@cs.york.ac.uk
Abstract
This paper demonstrates one efficient tech-
nique in extracting bilingual word pairs from
non-parallel but comparable corpora. Instead
of using the common approach of taking high
frequency words to build up the initial bilin-
gual lexicon, we show contextually relevant
terms that co-occur with cognate pairs can be
efficiently utilized to build a bilingual dictio-
nary. The result shows that our models using
this technique have significant improvement
over baseline models especially when highest-
ranked translation candidate per word is con-
sidered.
1 Introduction
Bilingual lexicons or dictionaries are invaluable
knowledge resources for language processing tasks.
The compilation of such bilingual lexicons remains
as a substantial issue to linguistic fields. In gen-
eral practice, many linguists and translators spend
huge amounts of money and effort to compile this
type of knowledge resources either manually, semi-
automatically or automatically. Thus, obtaining the
data is expensive.
In this paper, we demonstrate a technique that uti-
lizes contextually relevant terms that co-occur with
cognate pairs to expand an initial bilingual lexi-
con. We use unannotated resources that are freely
available such as English-Spanish Europarl corpus
(Koehn, 2005) and another different set of cognate
pairs as seed words.
We show that this technique is able to achieve
high precision score for bilingual lexicon extracted
from non-parallel but comparable corpora. Our
model using this technique with spelling similarity
approach obtains 85.4 percent precision at 50.0 per-
cent recall. Precision of 79.0 percent at 50.0 percent
recall is recorded when using this technique with
context similarity approach. Furthermore, by using
a string edit-distance vs. precision curve, we also
reveal that the latter model is able to capture words
efficiently compared to a baseline model.
Section 2 is dedicated to mention some of the re-
lated works. In Section 3, the technique that we used
is explained. Section 4 describes our experimental
setup followed by the evaluation results in Section
5. Discussion and conclusion are in Section 6 and 7
respectively.
2 Related Work
Koehn and Knight (2002) describe few potential
clues that may help in extracting bilingual lexi-
con from two monolingual corpora such as identi-
cal words, similar spelling, and similar context fea-
tures. In reporting our work, we treat both identical
word pairs and similar spelling word pairs as cog-
nate pairs.
Koehn and Knight (2002) map 976 identical
word pairs that are found in their two monolin-
gual German-English corpora and report that 88.0
percent of them are correct. They propose to re-
strict the word length, at least of length 6, to in-
crease the accuracy of the collected word pairs.
Koehn and Knight (2002) mention few related works
that use different measurement to compute the sim-
ilarity, such as longest common subsequence ratio
(Melamed, 1995) and string edit distance (Mann
10
and Yarowski, 2001). However, Koehn and Knight
(2002) point out that majority of their word pairs
do not show much resemblance at all since they
use German-English language pair. Haghighi et al
(2008) mention one disadvantage of using edit dis-
tance, that is, precision quickly degrades with higher
recall. Instead, they propose assigning a feature to
each substring of length of three or less for each
word.
For approaches based on contextual features or
context similarity, we assume that for a word that
occurs in a certain context, its translation equivalent
also occurs in equivalent contexts. Contextual fea-
tures are the frequency counts of context words oc-
curring in the surrounding of target word W. A con-
text vector for each W is then constructed, with only
context words found in the seed lexicon. The context
vectors are then translated into the target language
before their similarity is measured.
Fung and Yee (1998) point out that not only the
number of common words in context gives some
similarity clue to a word and its translation, but the
actual ranking of the context word frequencies also
provides important clue to the similarity between a
bilingual word pair. This fact has motivated Fung
and Yee (1998) to use tfidf weighting to compute the
vectors. This idea is similar to Rapp (1999) who
proposed to transform all co-occurrence vectors us-
ing log likelihood ratio instead of just using the
frequency counts of the co-occurrences. These val-
ues are used to define whether the context words are
highly associated with the W or not.
Earlier work relies on a large bilingual dictionary
as their seed lexicon (Rapp, 1999; Fung and Yee,
1998; among others). Koehn and Knight (2002)
present one interesting idea of using extracted cog-
nate pairs from corpus as the seed words in order
to alleviate the need of huge, initial bilingual lex-
icon. Haghighi et al (2008), amongst a few oth-
ers, propose using canonical correlation analysis to
reduce the dimension. Haghighi et al(2008) only
use a small-sized bilingual lexicon containing 100
word pairs as seed lexicon. They obtain 89.0 percent
precision at 33.0 percent recall for their English-
Spanish induction with best feature set, using top-
ically similar but non-parallel corpora.
3 The Utilizing Technique
Most works in bilingual lexicon extraction use lists
of high frequency words that are obtained from
source and target language corpus to be their source
and target word lists respectively. In our work, we
aim to extract a high precision bilingual lexicon us-
ing different approach. Instead, we use list of con-
textually relevant terms that co-occur with cognate
pairs.
Figure 1: Cognate pair extraction
These cognate pairs can be derived automatically
by mapping or finding identical words occur in two
high frequency list of two monolingual corpora (see
Figure 1). They are used to acquire list of source
word Ws and target word Wt. Ws and Wt are contex-
tually relevant terms that highly co-occur with the
cognate pairs in the same context. Thus, log likeli-
hood measure can be used to identify them.
Next, bilingual word pairs are extracted among
words in these Ws and Wt list using either context
similarity or spelling similarity. Figure 2 shows
some examples of potential bilingual word pairs,
of Ws and Wt, co-occurring with identical cognate
pairs of word ?civil?.
As we are working on English-Spanish language
pair, we extract bilingual lexicon using string edit
distance to identify spelling similarity between Ws
11
and Wt. Figure 3 outlines the algorithm using
spelling similarity in more detail.
Using the same Ws and Wt lists, we extract bilin-
gual lexicon by computing the context similarity be-
tween each {Ws,Wt} pair. To identify the context
similarity, the relation between each {Ws, Wt} pair
can be detected automatically using a vector similar-
ity measure such as cosine measure as in (1). The A
and B are the elements in the context vectors, con-
taining either zero or non-zero seed word values for
Ws and Wt, respectively.
Cosine similarity = cos(?) = A?B||A|| ? ||B|| (1)
The cosine measure favors {Ws,Wt} pairs that
share the most number of non-zero seed word val-
ues. However, one disadvantage of this measure is
that the cosine value directly proportional to the ac-
tual Ws and Wt values. Even though Ws and Wt
might not closely correlated with the same set of
seed words, the matching score could be high if Ws
or Wt has high seed word values everywhere. Thus,
we transform the context vectors from real value
into binary vectors before the similarity is computed.
Figure 4 outlines the algorithm using context simi-
larity in more detail.
In the algorithm, after the Ws and Wt lists are ob-
tained, each Ws and Wt units is represented by their
context vector containing log likelihood (LL) values
of contextually relevant words, occurring in the seed
lexicon, that highly co-occur with the Ws and Wt re-
spectively. To get this context vector, for each Ws
and Wt, all sentences in the English or Spanish cor-
pora containing the respective word are extracted to
form a particular sub corpus, e.g. sub corpus soci-
ety is a collection of sentences containing the source
word society.
Using window size of a sentence, the LL value
of term occurring with the word Ws or Wt in their
respective sub corpora is computed. Term that is
highly associated with the Ws or Wt is called con-
textually relevant term. However, we consider each
term with LL value higher than certain threshold
(e.g. threshold ? 15.0) to be contextually relevant.
Contextually relevant terms occurring in the seed
lexicon are used to build the context vector for the
Figure 2: Bilingual word pairs are found within context
of cognate word civil
Figure 3: Utilizing technique with spelling similarity
12
Figure 4: Utilizing technique with context similarity
Ws or Wt respectively. For example, word participa-
tion and education occurring in the seed lexicon are
contextually relevant terms for source word society.
Thus, they become elements of the context vector.
Then, we transform the context vectors, from real
value into binary, before we compute the similarity
with cosine measure.
4 Experimental Setup
4.1 Data
For source and target monolingual corpus, we de-
rive English and Spanish sentences from parallel Eu-
roparl corpora (Koehn, 2005).
? We split each of them into three parts; year
1996 - 1999, year 2000 - 2003 and year 2004
- 2006.
? We only take the first part, about 400k sen-
tences of Europarl Spanish (year 1996 - 1999)
and 2nd part, also about 400k from Europarl
English (year 2000 - 2003). We refer the partic-
ular part taken from the source language corpus
as S and the other part of the target language
corpus as T.
This approach is quite common in order to ob-
tain non-parallel but comparable (or same domain)
corpus. Examples can be found in Fung and Che-
ung (2004), followed by Haghighi et al (2008).
For corpus pre-processing, we only use sentence
boundary detection and tokenization on raw text.
We decided that large quantities of raw text requir-
ing minimum processing could also be considered as
minimal since they are inexpensive and not limited.
These should contribute to low or medium density
languages for which annotated resources are limited.
We also clean all tags and filter out stop words from
the corpus.
4.2 Evaluation
We extracted our evaluation lexicon from Word Ref-
erence? free online dictionary . For this work, the
word types are not restricted but mostly are con-
tent words. We have two sets of evaluation. In one,
we take high ranked candidate pairs where Ws could
have multiple translations. In the other, we only con-
sider highest-ranked Wt for each Ws. For evalua-
tion purposes, we take only the top 2000 candidate
ranked-pairs from the output. From that list, only
candidate pairs with words found in the evaluation
lexicon are proposed. We use F1-measure to evalu-
ate proposed lexicon against the evaluation lexicon.
The recall is defined as the proportion of the high
ranked candidate pairs. The precision is given as the
number of correct candidate pairs divided by the to-
tal number of proposed candidate pairs.
4.3 Other Setups
The following were also setup and used:
? List of cognate pairs
We obtained 79 identical cognate pairs from the
?from website http://www.wordreference.com
13
top 2000 high frequency lists of our S and T but
we chose 55 of these that have at least 100 con-
textually relevant terms that are highly associ-
ated with each of them.
? Seed lexicon
We also take a set of cognate pairs to be our
seed lexicon. We defined the size of a small
seed lexicon ranges between 100 to 1k word
pairs. Hence, our seed lexicon containing 700
cognate pairs are still considered as a small-
sized seed lexicon. However, instead of acquir-
ing this set of cognate pairs automatically, we
compiled the cognate pairs from a few Learn-
ing Spanish Cognates websites ?. This ap-
proach is a simple alternative to replace the
10-20k general dictionaries (Rapp, 1999; Fung
and McKeown, 2004) or automatic seed words
(Koehn and Knight, 2002; Haghighi et al,
2008). However, this approach can only be
used if the source and target language are fairly
related and both share lexically similar words
that most likely have same meaning. Other-
wise, we have to rely on general bilingual dic-
tionaries.
? Stop list
Previously (Rapp, 1999; Koehn and Knight,
2002; among others) suggested filtering out
commonly occurring words that do not help
in processing natural language data. This idea
sometimes seem as a negative approach to the
natural articles of language, however various
studies have proven that it is sensible to do so.
? Baseline system
We build baseline systems using basic context
similarity and spelling similarity features.
5 Evaluation Results
For the first evaluation, candidate pairs are ranked
after being measured either with cosine for context
similarity or edit distance for spelling similarity. In
this evaluation, we take the first 2000 of {Ws, Wt}
candidate pairs from the proposed lexicon where Ws
may have multiple translations or multiple Wt. See
Table 1.
?such as http://www.colorincolorado.org and
http://www.language-learning-advisor.com
Setting P0.1 P0.25 P0.33 P0.5 Best-F1
ContextSim (CS) 42.9 69.6 60.7 58.7 49.6
SpellingSim (SS) 90.5 74.2 69.9 64.6 50.9
(a) from baseline models
Setting P0.1 P0.25 P0.33 P0.5 Best-F1
E-ContextSim (ECS) 78.3 73.5 71.8 64.0 51.2
E-SpellingSim (ESS) 95.8 75.6 71.8 63.4 51.5
(b) from our proposed models
Table 1: Performance of baseline and our model for top
2000 candidates below certain threshold and ranked
Setting P0.1 P0.25 P0.33 P0.5 Best-F1
ContextSim-Top1 (CST) 58.3 61.2 64.8 55.2 52.6
SpellingSim-Top1 (SST) 84.9 66.4 52.7 34.5 37.0
(a) from baseline models
Setting P0.1 P0.25 P0.33 P0.5 Best-F1
E-ContextSim-Top1 (ECST) 85.0 81.1 79.7 79.0 57.1
E-SpellingSim-Top1 (ESST) 100.0 93.6 91.6 85.4 59.0
(b) from our proposed models
Table 2: Performance of baseline and our model for top
2000 candidates of top 1
Using either context or spelling similarity ap-
proach on S and T (labeled ECS and ESS respec-
tively), our models achieved about 51.2 percent of
best F1 measure. Those are not a significant im-
provement with only 1.0 to 2.0 percent error reduc-
tion over the baseline models (labeled CS and SS).
For the second evaluation, we take the first 2000
of {Ws, Wt} pairs where Ws may only have the high-
est ranked Wt as translation candidates (See Table
2). This time, both of our models (with context
similarity and spelling similarity, labeled ECST and
ESST respectively) yielded almost 60.0 percent of
best F1 measure. It is noted that using ESST alone
recorded a significant improvement of 20.0 percent
in the F1 score compared to SST baseline model.
ESST obtained 85.4 percent precision at 50.0 per-
cent recall. Precision of 79.0 percent at 50.0 percent
recall is recorded when using ECST. However, the
ECST has not recorded a significant difference over
CST baseline model (57.1 and 52.6 percent respec-
tively) in the second evaluation. The overall perfor-
mances, represented by precision scores for different
14
Figure 5: String Edit Distance vs. Precision curve
range of recalls, for these four models are illustrated
in Appendix A.
It is important to see the inner performance of the
ECST model with further analysis. We present a
string edit distance value (EDv) vs. precision curve
for ECST and CST in Figure 5 to measure the per-
formance of the ECST model in capturing bilingual
pairs with less similar orthographic features, those
that may not be captured using spelling similarity.
The graph in Figure 5 shows that even though
CST has higher precision score than ECST at EDv
of 2, it is not significant (the difference is less than
5.0 percent) and the spelling is still similar. On the
other hand, precision for proposed lexicon with EDv
above 3 (where the Ws and the proposed translation
equivalent Wt spelling becoming more dissimilar)
using ECST is higher than CST. The most significant
difference of the precision is almost 35.0 percent,
where ECST achieved almost 75.0 percent precision
compared to CST with 40.0 percent precision at EDv
of 4. It is followed by ECST with almost 50.0 per-
cent precision compared to CST with precision less
than 35.0 percent, offering about 15.0 percent preci-
sion improvement at EDv of 5.
6 Discussion
As we are working on English-Spanish language
pair, we could have focused on spelling similar-
ity feature only. Performance of the model using
this feature usually record higher accuracy other-
wise they may not be commonly occurring in a cor-
pus. Our models with this particular feature have
recorded higher F1 scores especially when consid-
ering only the highest-ranked candidates.
We also experiment with context similarity ap-
proach. We would like to see how far this approach
helps to add to the candidate scores from our corpus
S and T. The other reason is sometimes a correct tar-
get is not always a cognate even though a cognate
for it is available. Our ECST model has not recorded
significant improvement over CST baseline model in
the F1-measure. However, we were able to show that
by utilizing contextually relevant terms, ECST gath-
ers more correct candidate pairs especially when it
comes to words with dissimilar spelling. This means
that ECST is able to add more to the candidate scores
compared to CST. Thus, more correct translation
pairs can be expected with a good combination of
ECST and ESST.
The following are the advantages of our utilizing
technique:
? Reduced errors, hence able to improve preci-
sion scores.
? Extraction is more efficient in the contextual
boundaries (see Appendix B for examples).
? Context similarity approach within our tech-
nique has a potential to add more to the can-
didate scores.
Yet, our attempt using cognate pairs as seed words is
more appropriate for language pairs that share large
number of cognates or similar spelling words with
same meaning. Otherwise, one may have to rely on
bilingual dictionaries.
There may be some possible supporting strate-
gies, which we could use to help improve further
the precision score within the utilizing technique.
For example, dimension reduction using canonical
correlation analysis (CCA), resemblance detection,
measure of dispersion, reference corpus and further
noise reduction. However, we do not include a re-
ranking method, as we are using collection of cog-
nate pairs instead of a general bilingual dictionary.
Since our corpus S and T is in similar domain, we
might still not have seen the potential of this tech-
nique in its entirety. One may want to test the tech-
nique with different type of corpora for future works.
15
Nevertheless, we are still concerned that many
spurious translation equivalents were proposed be-
cause the words actually have higher correlation
with the input source word compared to the real
target word. Otherwise, the translation equivalents
may not be in the boundaries or in the corpus from
which translation equivalents are to be extracted.
Haghighi et al(2008) have reported that the most
common errors detected in their analysis on top 100
errors were from semantically related words, which
had strong context feature correlations. Thus, the
issue remains. We leave all these for further discus-
sion in future works.
7 Conclusion
We present a bilingual lexicon extraction technique
that utilizes contextually relevant terms that co-
occur with cognate pairs to expand an initial bilin-
gual lexicon. We show that this utilizing technique
is able to achieve high precision score for bilingual
lexicon extracted from non-parallel but comparable
corpora. We demonstrate this technique using unan-
notated resources that are freely available.
Our model using this technique with spelling sim-
ilarity obtains 85.4 percent precision at 50.0 percent
recall. Precision of 79.0 percent at 50.0 percent re-
call is recorded when using this technique with con-
text similarity approach. We also reveal that the
latter model with context similarity is able to cap-
ture words efficiently compared to a baseline model.
Thus, we show contextually relevant terms that co-
occur with cognate pairs can be efficiently utilized
to build a bilingual dictionary.
References
Cranias, L., Papageorgiou, H, and Piperidis, S. 1994.
A matching technique in Example-Based Machine
Translation. In International Conference On Compu-
tational Linguistics Proceedings, 15th conference on
Computational linguistics, Kyoto, Japan.
Diab, M., and Finch, S. 2000. A statistical word-level
translation model for comparable corpora. In Proceed-
ings of the Conference on Content-based multimedia
information access (RIAO).
Fung, P., and Cheung, P. 2004. Mining very non-parallel
corpora: Parallel sentence and lexicon extraction via
bootstrapping and EM. In Proceedings of the 2004
Conference on Empirical Method in Natural Language
Processing (EMNLP), Barcelona, Spain.
Fung, P., and Yee, L.Y. 1998. An IR Approach for
Translating New Words from Nonparallel, Compara-
ble Texts. In Proceedings of COLING-ACL98, Mon-
treal, Canada, 1998.
Fung, P., and McKeown, K. 1997. Finding Terminology
Translations from Non-parallel Corpora. In The 5th
Annual Workshop on Very Large Corpora, Hong Kong,
Aug 1997.
Haghighi, A., Liang, P., Berg-Krikpatrick, T., and Klein,
D. 2008. Learning bilingual lexicons from monolin-
gual corpora. In Proceedings of The ACL 2008, June
15 -20 2008, Columbus, Ohio
Koehn, P. 2005. Europarl: a parallel corpus for statistical
machine translation. In MT Summit
Koehn, P., and Knight , K. 2001. Knowledge sources
for word-level translation models. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Koehn, P., and Knight , K. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings of
ACL 2002, July 2002, Philadelphia, USA, pp. 9-16.
Rapp, R. 1995. Identifying word translations in non-
parallel texts. In Proceedings of ACL 33, pages 320-
322.
Rapp, R. 1999. Automatic identification of word transla-
tions from unrelated English and German corpora. In
Proceedings of ACL 37, pages 519-526.
16
Appendix A. Precision scores with different recalls
Appendix B. Some examples of effective extraction via utilizing technique
17
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 36?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Graph Connectivity Measures for Unsupervised Parameter Tuning
of Graph-Based Sense Induction Systems
Ioannis Korkontzelos, Ioannis Klapaftis and Suresh Manandhar
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
{johnkork, giannis, suresh}@cs.york.ac.uk
Abstract
Word Sense Induction (WSI) is the task of
identifying the different senses (uses) of a tar-
get word in a given text. This paper focuses
on the unsupervised estimation of the free pa-
rameters of a graph-based WSI method, and
explores the use of eight Graph Connectiv-
ity Measures (GCM) that assess the degree of
connectivity in a graph. Given a target word
and a set of parameters, GCM evaluate the
connectivity of the produced clusters, which
correspond to subgraphs of the initial (unclus-
tered) graph. Each parameter setting is as-
signed a score according to one of the GCM
and the highest scoring setting is then selected.
Our evaluation on the nouns of SemEval-2007
WSI task (SWSI) shows that: (1) all GCM es-
timate a set of parameters which significantly
outperform the worst performing parameter
setting in both SWSI evaluation schemes, (2)
all GCM estimate a set of parameters which
outperform the Most Frequent Sense (MFS)
baseline by a statistically significant amount
in the supervised evaluation scheme, and (3)
two of the measures estimate a set of parame-
ters that performs closely to a set of parame-
ters estimated in supervised manner.
1 Introduction
Using word senses instead of word forms is essential
in many applications such as information retrieval
(IR) and machine translation (MT) (Pantel and Lin,
2002). Word senses are a prerequisite for word sense
disambiguation (WSD) algorithms. However, they
are usually represented as a fixed-list of definitions
of a manually constructed lexical database. The
fixed-list of senses paradigm has several disadvan-
tages. Firstly, lexical databases often contain general
definitions and miss many domain specific senses
(Agirre et al, 2001). Secondly, they suffer from the
lack of explicit semantic and topical relations be-
tween concepts (Agirre et al, 2001). Thirdly, they
often do not reflect the exact content of the context
in which the target word appears (Veronis, 2004).
WSI aims to overcome these limitations of hand-
constructed lexicons.
Most WSI systems are based on the vector-space
model that represents each context of a target word
as a vector of features (e.g. frequency of cooccur-
ring words). Vectors are clustered and the resulting
clusters are taken to represent the induced senses.
Recently, graph-based methods have been employed
to WSI (Dorow and Widdows, 2003; Veronis, 2004;
Agirre and Soroa, 2007b).
Typically, graph-based approaches represent each
word co-occurring with the target word, within a
pre-specified window, as a vertex. Two vertices
are connected via an edge if they co-occur in one
or more contexts of the target word. This co-
occurrence graph is then clustered employing differ-
ent graph clustering algorithms to induce the senses.
Each cluster (induced sense) consists of words ex-
pected to be topically related to the particular sense.
As a result, graph-based approaches assume that
each context word is related to one and only one
sense of the target one.
Recently, Klapaftis and Manandhar (2008) argued
that this assumption might not be always valid, since
a context word may be related to more than one
senses of the target one. As a result, they pro-
36
posed the use of a graph-based model for WSI, in
which each vertex of the graph corresponds to a
collocation (word-pair) that co-occurs with the tar-
get word, while edges are drawn based on the co-
occurrence frequency of their associated colloca-
tions. Clustering of this collocational graph would
produce clusters, which consist of a set of collo-
cations. The intuition is that the produced clusters
will be less sense-conflating than those produced
by other graph-based approaches, since collocations
provide strong and consistent clues to the senses of
a target word (Yarowsky, 1995).
The collocational graph-based approach as well
as the majority of state-of-the-art WSI systems es-
timate their parameters either empirically or by em-
ploying supervised techniques. The SemEval-2007
WSI task (SWSI) participating systems UOY and
UBC-AS used labeled data for parameter estimation
(Agirre and Soroa, 2007a), while the authors of I2R,
UPV SI and UMND2 have empirically chosen val-
ues for their parameters. This issue imposes limits
on the unsupervised nature of these algorithms, as
well as on their performance on different datasets.
More specifically, when applying an unsupervised
WSI system on different datasets, one cannot be sure
that the same set of parameters is appropriate for all
datasets (Karakos et al, 2007). In most cases, a new
parameter tuning might be necessary. Unsupervised
estimation of free parameters may enhance the unsu-
pervised nature of systems, making them applicable
to any dataset, even if there are no tagged data avail-
able.
In this paper, we focus on estimating the free
parameters of the collocational graph-based WSI
method (Klapaftis and Manandhar, 2008) using
eight graph connectivity measures (GCM). Given a
parameter setting and the associated induced cluster-
ing solution, each induced cluster corresponds to a
subgraph of the original unclustered graph. A graph
connectivity measure GCMi scores each cluster by
evaluating the degree of connectivity of its corre-
sponding subgraph. Each clustering solution is then
assigned the average of the scores of its clusters. Fi-
nally, the highest scoring solution is selected.
Our evaluation on the nouns of SWSI shows
that GCM improve the worst performing parame-
ter setting by large margins in both SWSI evaluation
schemes, although they are below the best perform-
ing parameter setting. Moreover, the evaluation in
a WSD setting shows that all GCM estimate a set
of parameters which are above the Most Frequent
Sense (MFS) baseline by a statistically significant
amount. Finally our results show that two of the
measures, i.e. average degree and weighted average
degree, estimate a set of parameters that performs
closely to a set of parameters estimated in a super-
vised manner. All of these findings, suggest that
GCM are able to identify useful differences regard-
ing the quality of the induced clusters for different
parameter combinations, in effect being useful for
unsupervised parameter estimation.
2 Collocational graphs for WSI
Let bc, be the base corpus, which consists of para-
graphs containing the target word tw. The aim is
to induce the senses of tw given bc as the only in-
put. Let rc be a large reference corpus. In Klapaftis
and Manandhar (2008) the British National Corpus1
is used as a reference corpus. The WSI algorithm
consists of the following stages.
Corpus pre-processing The target of this stage is
to filter the paragraphs of the base corpus, in order to
keep the words which are topically (and possibly se-
mantically) related to the target one. Initially, tw is
removed from bc and both bc and rc are PoS-tagged.
In the next step, only nouns are kept in the para-
graphs of bc, since they are characterised by higher
discriminative ability than verbs, adverbs or adjec-
tives which may appear in a variety of different con-
texts. At the end of this pre-processing step, each
paragraph of bc and rc is a list of lemmatized nouns
(Klapaftis and Manandhar, 2008).
In the next step, the paragraphs of bc are fil-
tered by removing common nouns which are noisy;
contextually not related to tw. Given a contex-
tual word cw that occurs in the paragraphs of bc, a
log-likelihood ratio (G2) test is employed (Dunning,
1993), which checks if the distribution of cw in bc
is similar to the distribution of cw in rc; p(cw|bc) =
p(cw|rc) (null hypothesis). If this is true, G2 has a
small value. If this value is less than a pre-specified
threshold (parameter p1) the noun is removed from
bc.
1The British National Corpus (BNC) (2001, version 2). Dis-
tributed by Oxford University Computing Services.
37
Target: cnn nbc Target: nbc news
nbc tv nbc tv
cnn tv soap opera
cnn radio nbc show
news newscast news newscast
radio television nbc newshour
cnn headline cnn headline
nbc politics radio tv
breaking news breaking news
Table 1: Collocations connected to cnn nbc and nbc news
This process identifies nouns that are more indica-
tive in bc than in rc and vice versa. However, in this
setting we are not interested in nouns which have
a distinctive frequency in rc. As a result, each cw
which has a relative frequency in bc less than in rc
is filtered out. At the end of this stage, each para-
graph of bc is a list of nouns which are assumed to
be contextually related to the target word tw.
Creating the initial collocational graph The tar-
get of this stage is to determine the related nouns,
which will form the collocations, and the weight of
each collocation. Klapaftis and Manandhar (2008)
consider collocations of size 2, i.e. pairs of nouns.
For each paragraph of bc of size n, collocations
are identified by generating all the possible (cn2
)
combinations. The frequency of a collocation c is
the number of paragraphs in the whole SWSI corpus
(27132 paragraphs), in which c occurs.
Each collocation is assigned a weight, measuring
the relative frequency of two nouns co-occurring.
Let freqij denote the number of paragraphs in
which nouns i and j cooccur, and freqj denote the
number of paragraphs, where noun j occurs. The
conditional probability p(i|j) is defined in equation
1, and p(j|i) is computed in a similar way. The
weight of collocation cij is the average of these con-
ditional probabilities wcij = p(i|j) + p(j|i).
p(i|j) = freqijfreqj (1)
Finally, Klapaftis and Manandhar (2008) only ex-
tract collocations which have frequency (parame-
ter p2) and weight (parameter p3) higher than pre-
specified thresholds. This filtering appears to com-
pensate for inaccuracies in G2, as well as for low-
frequency distant collocations that are ambiguous.
Each weighted collocation is represented as a ver-
tex. Two vertices share an edge, if they co-occur in
one or more paragraphs of bc.
Populating and weighing the collocational graph
The constructed graph, G, is sparse, since the pre-
vious stage attempted to identify rare events, i.e.
co-occurring collocations. To address this problem,
Klapaftis and Manandhar (2008) apply a smooth-
ing technique, similar to the one in Cimiano et
al. (2005), extending the principle that a word is
characterised by the company it keeps (Firth, 1957)
to collocations. The target is to discover new edges
between vertices and to assign weights to all edges.
Each vertex i (collocation ci) is associated to
a vector V Ci containing its neighbouring vertices
(collocations). Table 1 shows an example of two
vertices, cnn nbc and nbc news, which are discon-
nected in G of the target word network. The example
was taken from Klapaftis and Manandhar (2008).
In the next step, the similarity between all vertex
vectors V Ci and V Cj is calculated using the Jaccard
coefficient, i.e. JC(V Ci, V Cj) = |V Ci?V Cj ||V Ci?V Cj | . Twocollocations ci and cj are mutually similar if ci is the
most similar collocation to cj and vice versa.
Given that collocations ci and cj are mutually
similar, an occurrence of a collocation ck with one
of ci, cj is also counted as an occurrence with the
other collocation. For example in Table 1, if cnn nbc
and nbc news are mutually similar, then the zero-
frequency event between nbc news and cnn tv is
set equal to the joint frequency between cnn nbc
and cnn tv. Marginal frequencies of collocations
are updated and the overall result is consequently a
smoothing of relative frequencies.
The weight applied to each edge connecting ver-
tices i and j (collocations ci and cj ) is the maximum
of their conditional probabilities: p(i|j) = freqijfreqj ,where freqi is the number of paragraphs collocation
ci occurs. p(j|i) is defined similarly.
Inducing senses and tagging In this final stage,
the collocational graph is clustered to produced the
senses (clusters) of the target word. The clustering
method employed is Chinese Whispers (CW) (Bie-
mann, 2006). CW is linear to the number of graph
edges, while it offers the advantage that it does not
require any input parameters, producing the clusters
of a graph automatically.
38
Figure 1: An example undirected weighted graph.
Initially, CW assigns all vertices to different
classes. Each vertex i is processed for a number of
iterations and inherits the strongest class in its lo-
cal neighbourhood (LN) in an update step. LN is
defined as the set of vertices which share an edge
with i. In each iteration for vertex i: each class, cl,
receives a score equal to the sum of the weights of
edges (i, j), where j has been assigned to class cl.
The maximum score determines the strongest class.
In case of multiple strongest classes, one is chosen
randomly. Classes are updated immediately, mean-
ing that a vertex can inherit from its LN classes that
were introduced in the same iteration.
Once CW has produced the clusters of a target
word, each of the instances of tw is tagged with
one of the induced clusters. This process is simi-
lar to Word Sense Disambiguation (WSD) with the
difference that the sense repository has been auto-
matically produced. Particularly, given an instance
of tw in paragraph pi: each induced cluster cl is as-
signed a score equal to the number of its collocations
(i.e. pairs of words) occurring in pi. We observe that
the tagging method exploits the one sense per collo-
cation property (Yarowsky, 1995), which means that
WSD based on collocations is probably finer than
WSD based on simple words, since ambiguity is re-
duced (Klapaftis and Manandhar, 2008).
3 Unsupervised parameter tuning
In this section we investigate unsupervised ways to
address the issue of choosing parameter values. To
this end, we employ a variety of GCM, which mea-
sure the relative importance of each vertex and as-
sess the overall connectivity of the corresponding
graph. These measures are average degree, cluster
coefficient, graph entropy and edge density (Navigli
and Lapata, 2007; Zesch and Gurevych, 2007).
GCM quantify the degree of connectivity of the
produced clusters (subgraphs), which represent the
senses (uses) of the target word for a given cluster-
ing solution (parameter setting). Higher values of
GCM indicate subgraphs (clusters) of higher con-
nectivity. Given a parameter setting, the induced
clustering solution and a graph connectivity measure
GCMi, each induced cluster is assigned the result-
ing score of applying GCMi on the corresponding
subgraph of the initial unclustered graph. Each clus-
tering solution is assigned the average of the scores
of its clusters (table 6), and the highest scoring one
is selected.
For each measure, we have developed two ver-
sions, i.e. one which considers the edge weights in
the subgraph, and a second which does not. In the
following description the terms graph and subgraph
are interchangeable.
Let G = (V,E) be an undirected graph (in-
duced sense), where V is a set of vertices and E =
{(u, v) : u, v ? V } a set of edges connecting vertex
pairs. Each edge is weighted by a positive weight,
W : wuv ? [0,?). Figure 1 shows a small example
to explain the computation of GCM. The graph con-
sists of 8 vertices, |V | = 8, and 10 edges, |E| = 10.
Edge weights appear on edges, e.g. wab = 14 .
Average Degree The degree (deg) of a vertex u is
the number of edges connected to u:
deg(u) = |{(u, v) ? E : v ? V }| (2)
The average degree (AvgDeg) of a graph can be
computed as:
AvgDeg(G(V,E)) = 1|V |
?
u?V
deg(u) (3)
The first row of table 2 shows the vertex degrees
of the example graph (figure 1) and AvgDeg(G) =
20
8 = 2.5.Edge weights can be integrated into the degree
computation. Let mew be the maximum edge
weight in the graph:
mew = max
(u,v)?E
wuv (4)
Average Weighted Degree The weighted de-
gree(w deg) of a vertex is defined as:
w deg(u) = 1|V |
?
(u,v)?E
wuv
mew (5)
39
a b c d e f g h
deg(u) 2 2 3 4 3 3 2 1
wdeg(u) 54 1 52 94 74 32 32 14
Tu 1 1 1 1 1 2 1 0
cc(u) 1 1 13 16 13 23 1 0
WTu 34 1 14 14 12 32 14 0
wcc(u) 34 1 112 124 16 12 14 0
p(u) 110 110 320 15 320 320 110 120
en(u) ? 100 33 33 41 46 41 41 33 22
wp(u) 116 120 18 980 780 340 340 180
we(u) ? 100 25 22 38 35 31 28 28 8
Table 2: Computations of graph connectivity measures
and relevant quantities on the example graph (figure 1).
Average weighted degree (AvgWDeg), similarly to
AvgDeg, is averaged over all vertices of the graph.
In the graph of figure 1, mew = 1. The second row
of table 2 shows the weighted degrees of all vertices.
AvgWDeg(G) = 4836 ' 1.33.
Average Cluster Coefficient The cluster coeffi-
cient (cc) of a vertex, u, is defined as:
cc(u) = Tu2?1ku(ku ? 1) (6)
Tu =
?
(u,v)?E
?
(v,x)?E
x 6=u
1 (7)
Tu is the number of edges between the ku neigh-
bours of u. Obviously ku = deg(u). 2?1ku(ku? 1)
would be the number of edges between the neigh-
bours of u if the graph they define was fully con-
nected. Average cluster coefficient (AvgCC) is aver-
aged over all vertices of the graph.
The computations of Tu and cc(u) on the example
graph are shown in the third and fourth rows of table
2. Consequently, AvgCC(G) = 916 = 0.5625.
Average Weighted Cluster Coefficient Let WTu
be the sum of edge weights between the neighbours
of u over mew. Weighted cluster coefficient (wcc)
can be computed as:
wcc(u) = WTu2?1ku(ku ? 1) (8)
WTu = 1mew
?
(u,v)?E
?
(v,x)?E
x 6=u
wvx (9)
Average weighted cluster coefficient (AvgWCC) is
averaged over all vertices of the graph. The com-
putations of WTu and wcc(u) on the example graph
(figure 1) are shown in the fifth and sixth rows of
table 2 and AvgWCC(G) = 678?24 ' 0.349.
Graph Entropy Entropy measures the amount of
information (alternatively the uncertainty) in a ran-
dom variable. For a graph, high entropy indicates
that many vertices are equally important and low en-
tropy that only few vertices are relevant (Navigli and
Lapata, 2007). The entropy (en) of a vertex u can be
defined as:
en(u) = ?p(u) log2 p(u) (10)
The probability of a vertex, p(u), is determined by
the degree distribution:
p(u) =
{deg(u)
2|E|
}
u?V
(11)
Graph entropy (GE) is computed by summing all
vertex entropies and normalising by log2 |V |. The
seventh and eighth row of table 2 show the compu-
tations of p(u) and en(u) on the example graph, re-
spectively. Thus, GE ' 0.97.
Weighted Graph Entropy Similarly to previous
graph connectivity measures, the weighted entropy
(wen) of a vertex u is defined as:
we(u) = ?wp(u) log2 wp(u) (12)
where: wp(u) =
{ w deg(u)
2 ?mew ? |E|
}
u?V
Weighted graph entropy (GE) is computed by sum-
ming all vertex weighted entropies and normalising
by log2 |V |. The last two rows of table 2 show the
computations of wp(u) and we(u) on the example
graph. Consequently, WGE ' 0.73.
Edge Density and Weighted Edge Density Edge
density (ed) quantifies how many edges the graph
has, as a ratio over the number of edges of a fully
connected graph of the same size:
A(V ) = 2
(|V |
2
)
(13)
40
Edge density (ed) is a global graph connectivity
measure; it refers to the whole graph and not a spe-
cific vertex. Edge density (ed) and weighted edge
density (wed) can be defined as follows:
ed(G(V,E)) = |E|A(V ) (14)
wed(G(V,E)) = 1A(V )
?
(u,v)?E
wu,v
mew (15)
In the graph of figure 1: A(V ) = 2(82
) = 28,
ed(G) = 1028 ' 0.357,
? wu,v
mew = 6 and wed(G) =6
28 ' 0.214.The use of the aforementioned GCM allows the
estimation of a different parameter setting for each
target word. Table 3 shows the parameters of the col-
locational graph-based WSI system (Klapaftis and
Manandhar, 2008). These parameters affect how the
collocational graph is constructed, and in effect the
quality of the induced clusters.
4 Evaluation
4.1 Experimental setting
The collocational WSI approach was evaluated un-
der the framework and corpus of SemEval-2007
WSI task (Agirre and Soroa, 2007a). The corpus
consists of text of the Wall Street Journal corpus,
and is hand-tagged with OntoNotes senses (Hovy et
al., 2006). The evaluation focuses on all 35 nouns of
SWSI. SWSI task employs two evaluation schemes.
In unsupervised evaluation, the results are treated as
clusters of contexts and gold standard (GS) senses
as classes. In a perfect clustering solution, each in-
duced cluster contains the same contexts as one of
the classes (Homogeneity), and each class contains
the same contexts as one of the clusters (Complete-
ness). F-Score is used to assess the overall quality of
clustering. Entropy and purity are also used, com-
plementarily. F-Score is a better measure than en-
tropy or purity, since F-Score measures both homo-
geneity and completeness, while entropy and purity
measure only the former. In the second scheme, su-
pervised evaluation, the training corpus is used to
map the induced clusters to GS senses. The testing
corpus is then used to measure WSD performance
(Table 4, Sup. Recall).
The graph-based collocational WSI method is re-
ferred as Col-Sm (where ?Col? stands for the ?col-
Parameter Range Value
G2 threshold 5, 10, 15 p1 = 5
Collocation frequency 4, 6, 8, 10 p2 = 8
Collocation weight 0.2, 0.3, 0.4 p3 = 0.2
Table 3: Parameters ranges and values in Klapaftis and
Manandhar (2008)
locational WSI? approach and ?Sm? for its ver-
sion using ?smoothing?). Col-Bl (where ?Bl? stands
for ?baseline?) refers to the same system without
smoothing. The parameters of Col-Sm were origi-
nally estimated by cross-validation on the training
set of SWSI. Out of 72 parameter combinations, the
setting with the highest F-Score was chosen and ap-
plied to all 35 nouns of the test set. This is referred
as Col-Sm-org (where ?org? stands for ?original?) in
Table 4. Table 3 shows all values for each parameter,
and the chosen values, under supervised parameter
estimation2. Col-Bl-org (Table 4) induces senses as
Col-Sm-org does, but without smoothing.
In table 4, Col-Sm-w (respectively Col-Bl-w)
refers to the evaluation of Col-Sm (Col-Bl), follow-
ing the same technique for parameter estimation as
in Klapaftis and Manandhar (2008) for each target
word separately (?w? stands for ?word?). Given that
GCM are applied for each target word separately,
these baselines will allow to see the performance of
GCM compared to a supervised setting.
The 1c1inst baseline assigns each instance to a
distinct cluster, while the 1c1w baseline groups all
instances of a target word into a single cluster. 1c1w
is equivalent to MFS in this setting. The fifth column
of table 4 shows the average number of clusters.
The SWSI participant systems UOY and UBC-AS
used labeled data for parameter estimation. The au-
thors of I2R, UPV SI and UMND2 have empirically
chosen values for their parameters.
The next subsection presents the evaluation of
GCM as well as the results of SWSI systems. Ini-
tially, we provide a brief discussion on the differ-
ences between the two evaluation schemes of SWSI
that will allow for a better understanding of GCM
performance.
4.2 Analysis of results and discussion
Evaluation of WSI methods is a difficult task. For
instance, 1c1inst (Table 4) achieves perfect purity
2CW performed 200 iterations for all experiments, because
it is not guaranteed to converge.
41
System Unsupervised Evaluation Sup.
FSc. Pur. Ent. # Cl. Recall
Col-Sm-org 78.0 88.6 31.0 5.9 86.4
Col-Bl-org 73.1 89.6 29.0 8.0 85.6
Col-Sm-w 80.9 88.0 32.5 4.3 85.5
Col-Bl-w 78.1 88.3 31.7 5.4 84.3
UBC-AS 80.8 83.6 43.5 1.6 80.7
UPV SI 69.9 87.4 30.9 7.2 82.5
I2R 68.0 88.4 29.7 3.1 86.8
UMND2 67.1 85.8 37.6 1.7 84.5
UOY 65.8 89.8 25.5 11.3 81.6
1c1w-MFS 80.7 82.4 46.3 1 80.9
1c1inst 6.6 100 0 73.1 N/A
Table 4: Evaluation of WSI systems and baselines.
and entropy. However, F-Score of 1c1inst is low,
because the GS senses are spread among clusters,
decreasing unsupervised recall. Supervised recall of
1c1inst is undefined, because each cluster tags only
one instance. Hence, clusters tagging instances in
the test corpus do not tag any instances in the train
corpus and the mapping cannot be performed. 1c1w
achieves high F-Score due to the dominance of MFS
in the testing corpus. However, its purity, entropy
and supervised recall are much lower than other sys-
tems, because it only induces the dominant sense.
Clustering solutions that achieve high supervised
recall do not necessarily achieve high F-Score,
mainly because F-Score penalises systems for in-
ducing more clusters than the corresponding GS
classes, as 1cl1inst does. Supervised evaluation
seems to be more neutral regarding the number of
clusters, since clusters are mapped into a weighted
vector of senses. Thus, inducing a number of clus-
ters similar to the number of senses is not a require-
ment for good results (Agirre and Soroa, 2007a).
High supervised recall means high purity and en-
tropy, as in I2R, but not vice versa, as in UOY. UOY
produces many clean clusters, however these are un-
reliably mapped to senses due to insufficient train-
ing data. On the contrary, I2R produces a few clean
clusters, which are mapped more reliably.
Comparing the performance of SWSI systems
shows that none performs well in both evaluation
settings, in effect being biased against one of the
schemes. However, this is not the case for the collo-
cational WSI method, which achieves a high perfor-
mance in both evaluation settings.
Table 6 presents the results of applying the graph
System Bound Unsupervised Evaluation Sup.
type FSc. Pur. Ent. # Cl. Recall
Col-Sm MaxR 79.3 90.5 26.6 7.0 88.6
Col-Sm MinR 62.9 89.0 26.7 12.7 78.8
Col-Bl MaxR 72.9 91.8 23.2 9.6 88.7
Col-Bl MinR 57.5 89.0 26.4 14.4 76.2
Col-Sm MaxF 83.2 90.0 28.7 4.9 86.6
Col-Sm MinF 43.6 90.2 22.1 17.6 83.7
Col-Bl MaxF 81.1 90.0 28.7 5.3 81.8
Col-Bl MinF 34.1 90.5 20.5 20.4 81.5
Table 5: Upper and lower performance bounds for sys-
tems Col-Sm and Col-Bl.
connectivity measures of section 3 in order to choose
the parameter values for the collocational WSI sys-
tem, for each word separately. The evaluation is
done both for Col-Sm and Col-Bl that use and ignore
smoothing, respectively.
To evaluate the supervised recall performance
using the graph connectivity measures, we com-
puted both the upper and lower bounds of Col-Sm,
i.e. the best and worst supervised recall, respectively
(MaxR and MinR in table 5). In the former case,
we selected the parameter combination per target
word that performs best (Col-Sm, MaxR in table 5),
which resulted in 88.6% supervised recall (F-Score:
79.3%), while in the latter we selected the worst per-
forming one, which resulted in 78.8% supervised re-
call (F-Score: 62.9%). In table 6 we observe that
the supervised recall of all measures is significantly
lower than the upper bound. However, all measures
perform significantly better than the lower bound
(McNemar?s test, confidence level: 95%); the small-
est difference is 4.9%, in the case of weighted edge
density. The picture is the same for Col-Bl.
In the same vein, we computed both the upper and
lower bounds of Col-Sm in terms of F-Score, 83.2%
and 43.6%, respectively (Col-Sm, MinF and MaxF
in table 5). The performance of the system is lower
than the upper bound, for all GCM. Despite that, we
observe that all measures except edge density and
weighted edge density outperform the lower bound
by large margins.
The comparison of GCM performance against
the lower and upper bounds of Col-Sm and Col-Bl
shows that GCM are able to identify useful differ-
ences regarding the degree of connectivity of in-
duced clusters, and in effect suggest parameter val-
ues that perform significantly better than the worst
42
Col-Sm Col-Bl
Unsupervised Evaluation Sup. Unsupervised Evaluation Sup.
Graph Connectivity Measure FSc Pur. Ent. # Cl. Recall FSc Pur. Ent. # Cl. Recall
Average Degree 79.2 87.2 34.2 3.9 84.8 77.5 31.3 88.4 5.7 83.8
Average Weighted Degree 77.1 87.8 32.0 5.5 84.2 75.1 28.3 89.6 8.5 83.3
Average Cluster Coefficient 72.5 88.8 28.5 9.1 83.9 68.7 24.0 90.9 12.9 83.9
Average Weighted Cluster Coefficient 65.8 88.4 28.0 9.6 84.1 68.9 22.4 91.3 13.9 83.7
Graph Entropy 67.0 89.6 25.9 12.3 83.8 68.5 22.1 91.8 14.4 84.4
Weighted Graph Entropy 72.7 89.4 28.1 9.6 84.1 72.2 23.5 91.2 12.5 84.0
Edge Density 47.8 91.8 19.4 18.4 84.8 42.0 16.9 92.8 21.9 84.1
Weighted Edge Density 53.4 90.2 23.1 15.5 83.7 42.2 17.1 92.7 21.9 83.9
Table 6: Unsupervised & supervised evaluation of the collocational WSI approach using graph connectivity measures.
case. However, they are all unable to approximate
the upper bound for both evaluation schemes, which
is also the case for the supervised estimation of pa-
rameters per target word (Col-Sm-w and Col-Bl-w).
In Table 6, we also observe that all measures
achieve higher supervised recall scores than the
MFS baseline. The increase is statistically signif-
icant (McNemar?s test, confidence level: 95%) in
all cases. This result shows that irrespective of the
number of clusters produced (low F-Score), GCM
are able to estimate a set of parameters that provides
clean clusters (low entropy), which when mapped to
GS senses improve upon the most frequent heuristic,
unlike the majority of unsupervised WSD systems.
Regarding the comparison between different
GCM, we observe that average degree and weighted
average degree for Col-Sm (Col-Bl) perform
closely to Col-Sm-w (Col-Bl-w) for both evaluation
schemes. This is due to the fact that they produce a
number of clusters similar to Col-Sm-w (Col-Bl-w),
while at the same time their distributions of clusters
over the target words? instances are also similar.
On the contrary, the remaining GCM tend to pro-
duce larger numbers of clusters compared to both
Col-Sm-w (Col-Bl-w) and the GS, in effect being
penalised by F-Score. As it has already been men-
tioned, supervised recall is less affected by a large
number of clusters, which causes small differences
among GCM.
Determining whether the weighted or unweighted
version of GCM performs better depends on the
GCM itself. Weighted graph entropy performs in all
cases better than the unweighted version. For aver-
age cluster coefficient and edge density, we cannot
extract a safe conclusion. Unweighted average de-
gree performs better than the weighted version.
5 Conclusion and future work
In this paper, we explored the use of eight graph con-
nectivity measures for unsupervised estimation of
free parameters of a collocational graph-based WSI
system. Given a parameter setting and the associ-
ated induced clustering solution, each cluster was
scored according to the connectivity degree of its
corresponding subgraph, as assessed by a particular
graph connectivity measure. Each clustering solu-
tion was then assigned the average of its clusters?
scores, and the highest scoring one was selected.
Evaluation on the nouns of SemEval-2007 WSI
task (SWSI) showed that all eight graph connectiv-
ity measures choose parameters for which the corre-
sponding performance of the system is significantly
higher than the lower performance bound, for both
the supervised and unsupervised evaluation scheme.
Moreover, the selected parameters produce results
which outperform the MFS baseline by a statisti-
cally significant amount in the supervised evalua-
tion scheme. The best performing measures, average
degree and weighted average degree, perform com-
parably well to the set of parameters chosen by a
supervised parameter estimation. In general, graph
connectivity measures can quantify significant dif-
ferences regarding the degree of connectivity of in-
duced clusters.
Future work focuses on further exploiting graph
connectivity measures. Graph theoretic literature
proposes a variety of measures capturing graph
properties. Some of these measures might help in
improving WSI performance, while at the same time
keeping graph-based WSI systems totally unsuper-
vised.
43
References
Eneko Agirre and Aitor Soroa. 2007a. Semeval-2007
task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 7?12, Prague, Czech Republic. Associa-
tion for Computational Linguistics.
Eneko Agirre and Aitor Soroa. 2007b. Ubc-as: A graph
based unsupervised system for induction and classi-
fication. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 346?349, Prague, Czech Republic. Association
for Computational Linguistics.
Eneko Agirre, Olatz Ansa, Eduard Hovy, and David Mar-
tinez. 2001. Enriching wordnet concepts with topic
signatures, Sep.
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73?
80, New York City, June. Association for Computa-
tional Linguistics.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text corpora
using formal concept analysis. Journal of Artificial In-
telligence research, 24:305?339.
Beate Dorow and Dominic Widdows. 2003. Discover-
ing corpusspecific word senses. In Proceedings 10th
conference of the European chapter of the ACL, pages
79?82, Budapest, Hungary.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
John R. Firth. 1957. A synopsis of linguistic theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57?60, New York
City, USA. Association for Computational Linguistics.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Carey Priebe. 2007. Cross-instance tuning of un-
supervised document clustering algorithms. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 252?259, Rochester, New York, April.
Association for Computational Linguistics.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
sense induction using graphs of collocations. In In
Proceedings of the 18th European Conference on Ar-
tificial Intelligence, (ECAI-2008), Patras, Greece.
R. Navigli and M. Lapata. 2007. Graph connectiv-
ity measures for unsupervised word sense disambigua-
tion. In 20th International Joint Conference on Artifi-
cial Intelligence (IJCAI 2007), pages 1683?1688, Hy-
derabad, India, January.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In KDD ?02: Proceedings
of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 613?
619, New York, NY, USA. ACM Press.
Jean Veronis. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech & Language,
18(3):223?252, July.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Meeting of
the Association for Computational Linguistics, pages
189?196.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of
the wikipedia category graph for NLP applications. In
Proceedings of the Second Workshop on TextGraphs:
Graph-Based Algorithms for Natural Language Pro-
cessing, pages 1?8, Rochester, NY, USA. Association
for Computational Linguistics.
44
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 117?122,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 14: Evaluation Setting for Word Sense Induction &
Disambiguation Systems
Suresh Manandhar
Department of Computer Science
University of York
York, UK, YO10 5DD
suresh@cs.york.ac.uk
Ioannis P. Klapaftis
Department of Computer Science
University of York
York, UK, YO10 5DD
giannis@cs.york.ac.uk
Abstract
This paper presents the evaluation setting
for the SemEval-2010 Word Sense Induction
(WSI) task. The setting of the SemEval-2007
WSI task consists of two evaluation schemes,
i.e. unsupervised evaluation and supervised
evaluation. The first one evaluates WSI meth-
ods in a similar fashion to Information Re-
trieval exercises using F-Score. However,
F-Score suffers from the matching problem
which does not allow: (1) the assessment of
the entire membership of clusters, and (2) the
evaluation of all clusters in a given solution. In
this paper, we present the use of V-measure as
a measure of objectively assessing WSI meth-
ods in an unsupervised setting, and we also
suggest a small modification on the supervised
evaluation.
1 Introduction
WSI is the task of identifying the different senses
(uses) of a target word in a given text. WSI is a field
of significant value, because it aims to overcome the
limitations originated by representing word senses
as a fixed-list of dictionary definitions. These lim-
itations of hand-crafted lexicons include the use of
general sense definitions, the lack of explicit seman-
tic and topical relations between concepts (Agirre et
al., 2001), and the inability to reflect the exact con-
tent of the context in which a target word appears
(Ve?ronis, 2004).
Given the significance of WSI, the objective as-
sessment and comparison of WSI methods is cru-
cial. The first effort to evaluate WSI methods un-
der a common framework (evaluation schemes &
dataset) was undertaken in the SemEval-2007 WSI
task (SWSI) (Agirre and Soroa, 2007), where two
separate evaluation schemes were employed. The
first one, unsupervised evaluation, treats the WSI re-
sults as clusters of target word contexts and Gold
Standard (GS) senses as classes. The traditional
clustering measure of F-Score (Zhao et al, 2005) is
used to assess the performance of WSI systems. The
second evaluation scheme, supervised evaluation,
uses the training part of the dataset in order to map
the automatically induced clusters to GS senses. In
the next step, the testing corpus is used to measure
the performance of systems in a Word Sense Disam-
biguation (WSD) setting.
A significant limitation of F-Score is that it does
not evaluate the make up of clusters beyond the
majority class (Rosenberg and Hirschberg, 2007).
Moreover, F-Score might also fail to evaluate clus-
ters which are not matched to any GS class due
to their small size. These two limitations define
the matching problem of F-Score (Rosenberg and
Hirschberg, 2007) which can lead to: (1) identical
scores between different clustering solutions, and
(2) inaccurate assessment of the clustering quality.
The supervised evaluation scheme employs a
method in order to map the automatically induced
clusters to GS senses. As a result, this process might
change the distribution of clusters by mapping more
than one clusters to the same GS sense. The out-
come of this process might be more helpful for sys-
tems that produce a large number of clusters.
In this paper, we focus on analysing the SemEval-
2007 WSI evaluation schemes showing their defi-
ciencies. Subsequently, we present the use of V-
117
measure (Rosenberg and Hirschberg, 2007) as an
evaluation measure that can overcome the current
limitations of F-Score. Finally, we also suggest
a small modification on the supervised evaluation
scheme, which will possibly allow for a more reli-
able estimation of WSD performance. The proposed
evaluation setting will be applied in the SemEval-
2010 WSI task.
2 SemEval-2007 WSI evaluation setting
The SemEval-2007 WSI task (Agirre and Soroa,
2007) evaluates WSI systems on 35 nouns and 65
verbs. The corpus consists of texts of the Wall Street
Journal corpus, and is hand-tagged with OntoNotes
senses (Hovy et al, 2006). For each target word tw,
the task consists of firstly identifying the senses of
tw (e.g. as clusters of target word instances, co-
occurring words, etc.), and secondly tagging the in-
stances of the target word using the automatically
induced clusters. In the next sections, we describe
and review the two evaluation schemes.
2.1 SWSI unsupervised evaluation
Let us assume that given a target word tw, a WSI
method has produced 3 clusters which have tagged
2100 instances of tw. Table 1 shows the number of
tagged instances for each cluster, as well as the com-
mon instances between each cluster and each gold
standard sense.
F-Score is used in a similar fashion to Information
Retrieval exercises. Given a particular gold standard
sense gsi of size ai and a cluster cj of size aj , sup-
pose aij instances in the class gsi belong to cj . Pre-
cision of class gsi with respect to cluster cj is de-
fined as the number of their common instances di-
vided by the total cluster size, i.e. P(gsi, cj) = aijaj .The recall of class gsi with respect to cluster cj is
defined as the number of their common instances di-
vided by the total sense size, i.e. R(gsi, cj) = aijai .The F-Score of gsi with respect to cj , F (gsi, cj), is
then defined as the harmonic mean of P (gsi, cj) and
R(gsi, cj).
The F-Score of class gsi, F (gsi), is the maximum
F (gsi, cj) value attained at any cluster. Finally, the
F-Score of the entire clustering solution is defined
as the weighted average of the F-Scores of each GS
sense (Formula 1), where q is the number of GS
senses and N is the total number of target word in-
gs1 gs2 gs3
cl1 500 100 100
cl2 100 500 100
cl3 100 100 500
Table 1: Clusters & GS senses matrix.
stances. If the clustering is identical to the original
classes in the datasets, F-Score will be equal to one.
In the example of Table 1, F-Score is equal to 0.714.
F ? Score =
q?
i=1
|gsi|
N F (gsi) (1)
As it can be observed, F-Score assesses the qual-
ity of a clustering solution by considering two dif-
ferent angles, i.e. homogeneity and completeness
(Rosenberg and Hirschberg, 2007). Homogeneity
refers to the degree that each cluster consists of
data points, which primarily belong to a single GS
class. On the other hand, completeness refers to the
degree that each GS class consists of data points,
which have primarily been assigned to a single clus-
ter. A perfect homogeneity would result in a preci-
sion equal to 1, while a perfect completeness would
result in a recall equal to 1.
Purity and entropy (Zhao et al, 2005) are also
used in SWSI as complementary measures. How-
ever, both of them evaluate only the homogeneity of
a clustering solution disregarding completeness.
2.2 SWSI supervised evaluation
In supervised evaluation, the target word corpus is
split into a testing and a training part. The training
part is used to map the automatically induced clus-
ters to GS senses. In the next step, the testing corpus
is used to evaluate WSI methods in a WSD setting.
Let us consider the example shown in Table 1 and
assume that this matrix has been created by using the
training part of our corpus. Table 1 shows that cl1 is
more likely to be associated with gs1, cl2 is more
likely to be associated with gs2, and cl3 is more
likely to be associated with gs3. This information
from the training part is utilised to map the clusters
to GS senses.
Particularly, the matrix shown in Table 1 is nor-
malised to produce a matrix M , in which each en-
try depicts the conditional probability P (gsi|clj).
Given an instance I of tw from the testing cor-
pus, a row cluster vector IC is created, in which
118
System F-Sc. Pur. Ent. # Cl. WSD
1c1w-MFS 78.9 79.8 45.4 1 78.7
UBC-AS 78.7 80.5 43.8 1.32 78.5
upv si 66.3 83.8 33.2 5.57 79.1
UMND2 66.1 81.7 40.5 1.36 80.6
I2R 63.9 84.0 32.8 3.08 81.6
UOY 56.1 86.1 27.1 9.28 77.7
1c1inst 9.5 100 0 139 N/A
Table 2: SWSI Unsupervised & supervised evaluation.
each entry k corresponds to the score assigned to
clk to be the winning cluster for instance I . The
product of IC and M provides a row sense vec-
tor, IG, in which the highest scoring entry a de-
notes that gsa is the winning sense for instance I .
For example, if we produce the row cluster vector
[cl1 = 0.8, cl2 = 0.1, cl3 = 0.1], and multiply
it with the normalised matrix of Table 1, then we
would get a row sense vector in which gs1 would be
the winning sense with a score equal to 0.6.
2.3 SWSI results & discussion
Table 2 shows the unsupervised and supervised per-
formance of systems participating in SWSI. As far
as the baselines is concerned, the 1c1w baseline
groups all instances of a target word into a single
cluster, while the 1c1inst creates a new cluster for
each instance of a target word. Note that the 1c1w
baseline is equivalent to the MFS in the supervised
evaluation. As it can be observed, a system with low
entropy (high purity) does not necessarily achieve
high F-Score. This is due to the fact that entropy
and purity only measure the homogeneity of a clus-
tering solution. For that reason, the 1c1inst baseline
achieves a perfect entropy and purity, although its
clustering solution is far from ideal.
On the contrary, F-Score has a significant advan-
tage over purity and entropy, since it measures both
homogeneity (precision) and completeness (recall)
of a clustering solution. However, F-Score suffers
from the matching problem, which manifests itself
either by not evaluating the entire membership of a
cluster, or by not evaluating every cluster (Rosen-
berg and Hirschberg, 2007). The former situation is
present, due to the fact that F-Score does not con-
sider the make-up of the clusters beyond the major-
ity class (Rosenberg and Hirschberg, 2007). For ex-
ample, in Table 3 the F-Score of the clustering so-
gs1 gs2 gs3
cl1 500 0 200
cl2 200 500 0
cl3 0 200 500
Table 3: Clusters & GS senses matrix.
lution is 0.714 and equal to the F-Score of the clus-
tering solution shown in Table 1, although these are
two significantly different clustering solutions. In
fact, the clustering shown in Table 3 should have
a better homogeneity than the clustering shown in
Table 1, since intuitively speaking each cluster con-
tains fewer classes. Moreover, the second clustering
should also have a better completeness, since each
GS class contains fewer clusters.
An additional instance of the matching problem
manifests itself, when F-Score fails to evaluate the
quality of smaller clusters. For example, if we add
in Table 3 one more cluster (cl4), which only tags
50 additional instances of gs1, then we will be able
to observe that this cluster will not be matched to
any of the GS senses, since cl1 is matched to gs1.
Although F-Score will decrease since the recall of
gs1 will decrease, the evaluation setting ignores the
perfect homogeneity of this small cluster.
In Table 2, we observe that no system managed to
outperform the 1c1w baseline in terms of F-Score.
At the same time, some systems participating in
SWSI were able to outperform the equivalent of the
1c1w baseline (MFS) in the supervised evaluation.
For example, UBC-AS achieved the best F-Score
close to the 1c1w baseline. However, by looking at
its supervised recall, we observe that it is below the
MFS baseline.
A clustering solution, which achieves high super-
vised recall, does not necessarily achieve high F-
Score. One reason for that stems from the fact that
F-Score penalises systems for getting the number of
GS classes wrongly, as in 1c1inst baseline. Accord-
ing to Agirre & Soroa (2007), supervised evaluation
seems to be more neutral regarding the number of
induced clusters, because clusters are mapped into a
weighted vector of senses, and therefore inducing a
number of clusters similar to the number of senses
is not a requirement for good results.
However, a large number of clusters might also
lead to an unreliable mapping of clusters to GS
senses. For example, high supervised recall also
119
means high purity and low entropy as in I2R, but not
vice versa as in UOY. UOY produces a large number
of clean clusters, in effect suffering from an unreli-
able mapping of clusters to senses due to the lack of
adequate training data.
Moreover, an additional supervised evaluation of
WSI methods using a different dataset split resulted
in a different ranking, in which all of the systems
outperformed the MFS baseline (Agirre and Soroa,
2007). This result indicates that the supervised eval-
uation might not provide a reliable estimation of
WSD performance, particularly in the case where
the mapping relies on a single dataset split.
3 SemEval-2010 WSI evaluation setting
3.1 Unsupervised evaluation using V-measure
Let us assume that the dataset of a target word tw
comprises of N instances (data points). These data
points are divided into two partitions, i.e. a set of au-
tomatically generated clusters C = {cj |j = 1 . . . n}
and a set of gold standard classes GS = {gsi|gs =
1 . . .m}. Moreover, let aij be the number of data
points, which are members of class gsi and elements
of cluster cj .
V-measure assesses the quality of a clustering so-
lution by explicitly measuring its homogeneity and
its completeness (Rosenberg and Hirschberg, 2007).
Recall that homogeneity refers to the degree that
each cluster consists of data points which primar-
ily belong to a single GS class. V-measure assesses
homogeneity by examining the conditional entropy
of the class distribution given the proposed cluster-
ing, i.e. H(GS|C). H(GS|C) quantifies the re-
maining entropy (uncertainty) of the class distribu-
tion given that the proposed clustering is known. As
a result, when H(GS|C) is 0, we have the perfectly
homogeneous solution, since each cluster contains
only those data points that are members of a single
class. However in an imperfect situation, H(GS|C)
depends on the size of the dataset and the distribu-
tion of class sizes. As a result, instead of taking the
raw conditional entropy, V-measure normalises it by
the maximum reduction in entropy the clustering in-
formation could provide, i.e. H(GS).
Formulas 2 and 3 define H(GS) and H(GS|C).
When there is only a single class (H(GS) = 0), any
clustering would produce a perfectly homogeneous
solution. In the worst case, the class distribution
within each cluster is equal to the overall class dis-
tribution (H(GS|C) = H(GS)), i.e. clustering pro-
vides no new information. Overall, in accordance
with the convention of 1 being desirable and 0 unde-
sirable, the homogeneity (h) of a clustering solution
is 1 if there is only a single class, and 1? H(GS|C)H(GS) in
any other case (Rosenberg and Hirschberg, 2007).
H(GS) = ?
|GS|?
i=1
?|C|
j=1 aij
N log
?|C|
j=1 aij
N (2)
H(GS|C) = ?
|C|?
j=1
|GS|?
i=1
aij
N log
aij?|GS|
k=1 akj
(3)
Symmetrically to homogeneity, completeness refers
to the degree that each GS class consists of data
points, which have primarily been assigned to a sin-
gle cluster. To evaluate completeness, V-measure
examines the distribution of cluster assignments
within each class. The conditional entropy of the
cluster given the class distribution, H(C|GS), quan-
tifies the remaining entropy (uncertainty) of the clus-
ter given that the class distribution is known.
Consequently, when H(C|GS) is 0, we have the
perfectly complete solution, since all the data points
of a class belong to the same cluster. Therefore,
symmetrically to homogeneity, the completeness c
of a clustering solution is 1 if there is only a sin-
gle cluster (H(C) = 0), and 1 ? H(C|GS)H(C) in any
other case. In the worst case, completeness will be
equal to 0, particularly when H(C|GS) is maxi-
mal and equal to H(C). This happens when each
GS class is included in all clusters with a distribu-
tion equal to the distribution of sizes (Rosenberg and
Hirschberg, 2007). Formulas 4 and 5 define H(C)
and H(C|GS). Finally h and c can be combined and
produce V-measure, which is the harmonic mean of
homogeneity and completeness.
H(C) = ?
|C|?
j=1
?|GS|
i=1 aij
N log
?|GS|
i=1 aij
N (4)
H(C|GS) = ?
|GS|?
i=1
|C|?
j=1
aij
N log
aij?|C|
k=1 aik
(5)
Returning to our clustering example in Table 1, its
V-measure is equal to 0.275. In section 2.3, we
also presented an additional clustering (Table 3),
which had the same F-Score as the clustering in Ta-
ble 1, despite the fact that it intuitively had a bet-
ter completeness and homogeneity. The V-measure
120
of the second clustering solution is equal to 0.45,
and higher than the V-measure of the first cluster-
ing. This result shows that V-measure is able to
discriminate between these two clusterings by con-
sidering the make-up of the clusters beyond the ma-
jority class. Furthermore, it is straightforward from
the description in this section, that V-measure evalu-
ates each cluster in terms of homogeneity and com-
pleteness, unlike F-Score which relies on a post-hoc
matching.
3.2 V-measure results & discussion
Table 4 shows the performance of SWSI partici-
pating systems according to V-measure. The last
four columns of Table 4 show the weighted aver-
age homogeneity and completeness for nouns and
verbs. Note that the homogeneity and complete-
ness columns are weighted averages over all nouns
or verbs, and are not used for the calculation of
the weighted average V-measure (second column).
The latter is calculated by measuring for each tar-
get word?s clustering solution the harmonic mean of
homogeneity and completeness separately, and then
producing the weighted average.
As it can be observed in Table 4, all WSI sys-
tems have outperformed the random baseline which
means that they have learned useful information.
Moreover, Table 4 shows that on average all sys-
tems have outperformed the 1c1w baseline, which
groups the instances of a target word to a single clus-
ter. The completeness of the 1c1w baseline is equal
to 1 by definition, since all instances of GS classes
are grouped to a single cluster. However, this solu-
tion is as inhomogeneous as possible and causes a
homogeneity equal to 0 in the case of nouns. In the
verb dataset however, some verbs appear with only
one sense, in effect causing the 1c1w homogeneity
to be equal to 1 in some cases, and the average V-
measure greater than 0.
In Table 4, we also observe that the 1c1inst base-
line achieves a high performance. In nouns only I2R
is able to outperform this baseline, while in verbs the
1c1inst baseline achieves the highest result. By the
definition of homogeneity (section 3.1), this baseline
is perfectly homogeneous, since each cluster con-
tains one instance of a single sense. However, its
completeness is not 0, as one might intuitively ex-
pect. This is due to the fact that V-measure consid-
ers as the worst solution in terms of completeness
the one, in which each class is represented by ev-
ery cluster, and specifically with a distribution equal
to the distribution of cluster sizes (Rosenberg and
Hirschberg, 2007). This worst solution is not equiv-
alent to the 1c1inst, hence completeness of 1c1inst
is greater than 0. Additionally, completeness of this
baseline benefits from the fact that around 18% of
GS senses have only one instance in the test set.
Note however, that on average this baseline achieves
a lower completeness than most of the systems.
Another observation from Table 4 is that upv si
and UOY have a better ranking than in Table 2. Note
that these systems have generated a higher number
of clusters than the GS number of senses. In verbs
UOY has been extensively penalised by the F-Score.
The inspection of their answers shows that both sys-
tems generate highly skewed distributions, in which
a small number of clusters tag the majority of in-
stances, while a larger number tag only a few. As
mentioned in sections 2.1 and 2.3, these small clus-
ters might not be matched to any GS sense, hence
they will decrease the unsupervised recall of a GS
class, and consequently the F-Score. However, their
high homogeneity is not considered in the calcula-
tion of F-Score. On the contrary, V-measure is able
to evaluate the quality of these small clusters, and
provide a more objective assessment.
Finally, in our evaluation we observe that I2R
has on average the highest performance among the
SWSI methods. This is due to its high V-measure in
nouns, but not in verbs. Particularly in nouns, I2R
achieves a consistent performance in terms of ho-
mogeneity and completeness without being biased
towards one of them, as is the case for the rest of
the systems. For example, UOY and upv si achieve
on average the highest homogeneity (42.5 & 32.8
resp.) and the worst completeness (11.5 & 13.2
resp.). The opposite picture is present for UBC-AS
and UMND2. Despite that, UBC-AS and UMND2
perform better than I2R in verbs, due to the small
number of generated clusters (high completeness),
and a reasonable homogeneity mainly due to the ex-
istence of verbs with one GS sense.
3.3 Modified supervised WSI evaluation
In section 2.3, we mentioned that supervised eval-
uation might favor methods which produce many
121
System V-measure Homogeneity Completeness
Total Nouns Verbs Nouns Verbs Nouns Verbs
1c1inst 21.6 19.2 24.3 100.0 100.0 11.3 15.8
I2R 16.5 22.3 10.1 31.6 27.3 20.0 10.0
UOY 15.6 17.2 13.9 38.9 46.6 12.0 11.1
upv si 15.3 18.2 11.9 37.1 28.0 14.5 11.8
UMND2 12.1 12.0 12.2 18.1 15.3 55.8 63.6
UBC-AS 7.8 3.7 12.4 4.0 13.7 90.6 93.0
Rand 7.2 4.9 9.7 12.0 30.0 14.1 14.3
1c1w 6.3 0.0 13.4 0.0 13.4 100.0 100.0
Table 4: V-Measure, homogeneity and completeness of SemEval-2007 WSI systems. The range of V-measure, homo-
geneity & completeness is 0-100.
clusters, since the mapping step can artificially in-
crease completeness. Furthermore, we have shown
that generating a large number of clusters might lead
to an unreliable mapping of clusters to GS senses
due to the lack of adequate training data.
Despite that, the supervised evaluation can be
considered as an application-oriented evaluation,
since it allows the transformation of unsupervised
WSI systems to semi-supervised WSD ones. Given
the great difficulty of unsupervised WSD systems to
outperform the MFS baseline as well as the SWSI
results, which show that some systems outperform
the MFS by a significant amount in nouns, we be-
lieve that this evaluation scheme should be used to
compare against supervised WSD methods.
In section 2.3, we also mentioned that the super-
vised evaluation on two different test/train splits pro-
vided a different ranking of methods, and more im-
portantly a different ranking with regard to the MFS.
To deal with that problem, we believe that it would
be reasonable to perform k-fold cross validation in
order to collect statistically significant information.
4 Conclusion
We presented and discussed the limitations of the
SemEval-2007 evaluation setting for WSI methods.
Based on our discussion, we described the use of
V-measure as the measure of assessing WSI perfor-
mance on an unsupervised setting, and presented the
results of SWSI WSI methods. We have also sug-
gested a small modification on the supervised eval-
uation scheme, which will allow for a more reliable
estimation of WSD performance. The new evalu-
ation setting will be applied in the SemEval-2010
WSI task.
Acknowledgements
This work is supported by the European Commis-
sion via the EU FP7 INDECT project, Grant No.
218086, Research area: SEC-2007-1.2-01 Intelli-
gent Urban Environment Observation System. The
authors would like to thank the anonymous review-
ers for their useful comments.
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the 4rth Interna-
tional Workshop on Semantic Evaluations, pages 7?12,
Prague, Czech Republic, June. ACL.
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching wordnet concepts with topic
signatures. In Proceedings of the NAACL workshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations. ACL.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human
Language Technology / North American Association
for Computational Linguistics conference, New York,
USA.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410?420.
Jean Ve?ronis. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech & Language,
18(3):223?252.
Ying Zhao, George Karypis, and Usam Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discov-
ery, 10(2):141?168.
122
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1263?1271,
Beijing, August 2010
Estimating Linear Models for Compositional Distributional Semantics
Fabio Massimo Zanzotto1
(1) Department of Computer Science
University of Rome ?Tor Vergata?
zanzotto@info.uniroma2.it
Ioannis Korkontzelos
Department of Computer Science
University of York
johnkork@cs.york.ac.uk
Francesca Fallucchi1,2
(2) Universita` Telematica
?G. Marconi?
f.fallucchi@unimarconi.it
Suresh Manandhar
Department of Computer Science
University of York
suresh@cs.york.ac.uk
Abstract
In distributional semantics studies, there
is a growing attention in compositionally
determining the distributional meaning of
word sequences. Yet, compositional dis-
tributional models depend on a large set
of parameters that have not been explored.
In this paper we propose a novel approach
to estimate parameters for a class of com-
positional distributional models: the addi-
tive models. Our approach leverages on
two main ideas. Firstly, a novel idea for
extracting compositional distributional se-
mantics examples. Secondly, an estima-
tion method based on regression models
for multiple dependent variables. Experi-
ments demonstrate that our approach out-
performs existing methods for determin-
ing a good model for compositional dis-
tributional semantics.
1 Introduction
Lexical distributional semantics has been largely
used to model word meaning in many fields as
computational linguistics (McCarthy and Carroll,
2003; Manning et al, 2008), linguistics (Harris,
1964), corpus linguistics (Firth, 1957), and cogni-
tive research (Miller and Charles, 1991). The fun-
damental hypothesis is the distributional hypoth-
esis (DH): ?similar words share similar contexts?
(Harris, 1964). Recently, this hypothesis has been
operationally defined in many ways in the fields of
physicology, computational linguistics, and infor-
mation retrieval (Li et al, 2000; Pado and Lapata,
2007; Deerwester et al, 1990).
Given the successful application to words, dis-
tributional semantics has been extended to word
sequences. This has happened in two ways: (1)
via the reformulation of DH for specific word se-
quences (Lin and Pantel, 2001); and (2) via the
definition of compositional distributional seman-
tics (CDS) models (Mitchell and Lapata, 2008;
Jones and Mewhort, 2007). These are two differ-
ent ways of addressing the problem.
Lin and Pantel (2001) propose the pattern dis-
tributional hypothesis that extends the distribu-
tional hypothesis for specific patterns, i.e. word
sequences representing partial verb phrases. Dis-
tributional meaning for these patterns is derived
directly by looking to their occurrences in a cor-
pus. Due to data sparsity, patterns of different
length appear with very different frequencies in
the corpus, affecting their statistics detrimentally.
On the other hand, compositional distributional
semantics (CDS) propose to obtain distributional
meaning for sequences by composing the vectors
of the words in the sequences (Mitchell and Lap-
ata, 2008; Jones and Mewhort, 2007). This ap-
proach is fairly interesting as the distributional
meaning of sequences of different length is ob-
tained by composing distributional vectors of sin-
gle words. Yet, many of these approaches have a
large number of parameters that cannot be easily
estimated.
In this paper we propose a novel approach to es-
1263
timate parameters for additive compositional dis-
tributional semantics models. Our approach lever-
ages on two main ideas. Firstly, a novel way for
extracting compositional distributional semantics
examples and counter-examples. Secondly, an es-
timation model that exploits these examples and
determines an equation system that represents a
regression problem with multiple dependent vari-
ables. We propose a method to estimate a solu-
tion of this equation system based on the Moore-
Penrose pseudo-inverse matrices (Penrose, 1955).
The rest of the paper is organised as follows:
Firstly, we shortly review existing compositional
distributional semantics (CDS) models (Sec. 2).
Then we describe our model for estimating CDS
models parameters (Sec. 3). In succession, we
introduce a way to extract compositional dis-
tributional semantics examples from dictionaries
(Sec. 4). Then, we discuss the experimental set up
and the results of our linear CDS model with es-
timated parameters with respect to existing CDS
models (Sec. 5).
2 Models for compositional
distributional semantics (CDS)
A CDS model is a function  that computes the
distributional vector of a sequence of words s by
combining the distributional vectors of its com-
ponent words w1 . . .wn. Let(s) be the distribu-
tional vector describing s and ~wi the distributional
vectors describing its component word wi. Then,
the CDS model can be written as:
(s) = (w1 . . .wn) = ~w1  . . . ~wn (1)
This generic model has been fairly studied and
many different functions have been proposed and
tested.
Mitchell and Lapata (2008) propose the fol-
lowing general CDS model for 2-word sequences
s = xy:
(s) = (xy) = f(~x, ~y,R,K) (2)
where ~x and ~y are respectively the distributional
vectors of x and y, R is the particular syntactic
and/or semantic relation connecting x and y, and,
K represents the amount of background knowl-
edge that the vector composition process takes
vector dimensions
betwe
en
gap proce
ss
socialtwo
contact < 11, 0, 3, 0, 11 >
x: close < 27, 3, 2, 5, 24 >
y: interaction < 23, 0, 3, 8, 4 >
Table 1: Example of distributional
frequency vectors for the triple t =
( ~contact, ~close, ~interaction)
into account. Two specialisations of the gen-
eral CDS model are proposed: the basic additive
model and the basic multiplicative model.
The basic additive model (BAM) is written as:
(s) = ?~x+ ?~y (3)
where ? and ? are two scalar parameters. The
simplistic parametrisation is ? = ? = 1. For
example, given the vectors ~x and ~y of Table 1,
BAM (s) =< 50, 3, 5, 13, 28 >.
The basic multiplicative model (BMM) is writ-
ten as:
si = xiyi (4)
where si, xi, and yi are the i-th dimensions of
the vectors (s), ~x, and ~y, respectively. For
the example of Table 1, BMM (s) =< 621, 0,
6, 40, 96 >.
Erk and Pado? (2008) look at the problem in a
different way. Let the general distributional mean-
ing of the word w be ~w. Their model computes a
different vector ~ws that represents the specific dis-
tributional meaning of w with respect to s, i.e.:
~ws = (w, s) (5)
In general, this operator gives different vectors for
each word wi in the sequence s, i.e. (wi, s) 6=
(wj , s) if i 6= j. It also gives different vectors
for a word wi appearing in different sequences sk
and sl, i.e. (wi, sk) 6= (wi, sl) if k 6= l.
The model of Erk and Pado? (2008) was de-
signed to disambiguate the distributional mean-
ing of a word w in the context of the sequence
s. However, substituting the word w with the se-
mantic head h of s, allows to compute the distri-
butional meaning of sequence s as shaped by the
1264
word that is governing the sequence (c.f. Pollard
and Sag (1994)). For example, the distributional
meaning of the word sequence eats mice is gov-
erned by the verb eats. Following this model, the
distributional vector (s) can be written as:
(s) ? (h, s) (6)
The function (h, s) explicitly uses the re-
lation R and the knowledge K of the general
equation 2, being based on the notion of selec-
tional preferences. We exploit the model for se-
quences of two words s=xy where the two words
are related with an oriented syntactic relation r
(e.g. r=adj modifier). For making the syntac-
tic relation explicit, we indicate the sequence as:
s = x r?? y.
Given a word w, the model has to keep track
of its selectional preferences. Consequently, each
word w is represented with a triple:
(~w,Rw, R?1w ) (7)
where ~w is the distributional vector of the word w,
Rw is the set of the vectors representing the direct
selectional preferences of the word w, and R?1w is
the set of the vectors representing the indirect se-
lectional preferences of the word w. Given a set of
syntactic relationsR, the set Rw and R?1w contain
respectively a selectional preference vectorRw(r)
and Rw(r)?1 for each r ? R. Selectional prefer-
ences are computed as in Erk (2007). If x is the
semantic head of sequence s, then the model can
be written as:
(s) = (x, x r?? y) = ~xRy(r) (8)
Otherwise, if y is the semantic head:
(s) = (y, x r?? y) = ~y R?1x (r) (9)
 is in both cases realised using BAM or BMM.
We will call these models: basic additive model
with selectional preferences (BAM-SP) and basic
multiplicative model with selectional preferences
(BMM-SP).
Both Mitchell and Lapata (2008) and Erk and
Pado? (2008) experimented with few empirically
estimated parameters. Thus, the general additive
CDS model has not been adequately explored.
3 Estimating Additive Compositional
Semantics Models from Data
The generic additive model sums the vectors ~x
and ~y in a new vector ~z:
(s) = ~z = A~x+B~y (10)
where A and B are two square matrices captur-
ing the relation R and the background knowledge
K of equation 2. Writing matrices A and B by
hand is impossible because of their large size. Es-
timating these matrices is neither a simple classi-
fication learning problem nor a simple regression
problem. It is a regression problem with multiple
dependent variables. In this section, we propose
our model to solve this regression problem using
a set of training examples E.
The set of training examples E contains triples
of vectors (~z, ~x, ~y). ~x and ~y are the two distribu-
tional vectors of the words x and y. ~z is the ex-
pected distributional vector of the composition of
~x and ~y. Note that for an ideal perfectly perform-
ing CDS model we can write ~z = (xy). How-
ever, in general the expected vector ~z is not guar-
anteed to be equal to the composed one (xy).
Figure 1 reports an example of these triples, i.e.,
t = ( ~contact, ~close, ~interaction), with the re-
lated distributional vectors. The construction of
E is discussed in section 4.
In the rest of the section, we describe how the
regression problem with multiple dependent vari-
ables can be solved with a linear equation system
and we give a possible solution of this equation
system. In the experimental section, we refer to
our model as the estimated additive model (EAM).
3.1 Setting the linear equation system
The matrices A and B of equation 10 can be
joined in a single matrix:
~z =
(
A B
)(~x
~y
)
(11)
For the triple t of table 1, equation 11 is:
~contact =
(
A B
)
(
~close
~interaction
)
(12)
1265
and it can be rewritten as:
?
?????
11
0
3
0
11
?
?????
=
(
A5?5 B5?5
)
?
??????????
27
3
2
5
24
23
0
3
8
4
?
??????????
(13)
Focusing on matrix (AB), we can transpose the
matrices as follows:
~zT =
((
A B
)(~x
~y
))T
=
(
~xT ~yT
)(AT
BT
)
(14)
Matrix (~xT ~yT ) is known and matrix
(
AT
BT
)
is
to be estimated.
Equation 14 is the prototype of our final equa-
tion system. The larger the matrix (AB) to be
estimated, the more equations like 14 are needed.
Given set E that contains n triples (~z, ~x, ~y), we
can write the following system of equations:
?
????
~zT1
~zT2...
~zTn
?
???? =
?
????
(
~xT1 ~yT1
)
(
~xT2 ~yT2
)
...(
~xTn ~yTn
)
?
????
(
AT
BT
)
(15)
The vectors derived from the triples can be seen as
two matrices of n rows, Z and (XY ) related to ~zTi
and (~xTi ~yTi
), respectively. The overall equation
system is then the following:
Z =
(
X Y
)(AT
BT
)
(16)
This equation system represents the constraints
that matrices A and B have to satisfy in order to
be a possible linear CDS model that can at least
describe seen examples. We will hereafter call
? =
(
A B
) and Q = (X Y ). The system
in equation 16 can be simplified as:
Z = Q?T (17)
As Q is a rectangular and singular matrix, it is
not invertible and the system in equation 16 has
no solutions. It is possible to use the principle
of Least Square Estimation for computing an ap-
proximation solution. The idea is to compute the
solution ?? that minimises the residual norm, i.e.:
??T = arg min
?T
?Q?T ? Z?2 (18)
One solution for this problem is the Moore-
Penrose pseudoinverse Q+ (Penrose, 1955) that
gives the following final equation:
??T = Q+Z (19)
In the next section, we discuss how the Moore-
Penrose pseudoinverse is obtained using singular
value decomposition (SVD).
3.2 Computing the pseudo-inverse matrix
The pseudo-inverse matrix can provide an approx-
imated solution even if the equation system has no
solutions. We here compute the Moore-Penrose
pseudoinverse using singular value decomposi-
tion (SVD) that is widely used in computational
linguistics and information retrieval for reducing
spaces (Deerwester et al, 1990).
Moore-Penrose pseudoinverse (Penrose, 1955)
is computed in the following way. Let the original
matrix Q have n rows and m columns and be of
rank r. The SVD decomposition of the original
matrix Q is Q = U?V T where ? is a square di-
agonal matrix of dimension r. Then, the pseudo-
inverse matrix that minimises the equation 18 is:
Q+ = V ?+UT (20)
where the diagonal matrix ?+ is the r ? r trans-
posed matrix of ? having as diagonal elements the
reciprocals of the singular values 1?1 , 1?2 , ..., 1?r of
?.
Using SVD to compute the pseudo-inverse ma-
trix allows for different approximations (Fallucchi
and Zanzotto, 2009). The algorithm for comput-
ing the singular value decomposition is iterative
(Golub and Kahan, 1965). Firstly derived dimen-
sions have higher singular value. Then, dimension
k is more informative than dimension k? > k. We
can consider different values for k to obtain differ-
ent SVD for the approximations Q+k of the origi-nal matrix Q+ in equation 20), i.e.:
Q+k = Vn?k?+k?kUTk?m (21)
1266
where Q+k is a matrix n by m obtained consider-ing the first k singular values.
4 Building positive and negative
examples
As explained in the previous section, estimating
CDS models, needs a set of triples E, similar to
triple t of table 1. This set E should contain pos-
itive examples in the form of triples (~zi, ~xi, ~yi).
Examples are positive in the sense that ~zi =
(xy) for an ideal CDS. There are no available
sets to contain such triples, with the exception of
the set used in Mitchell and Lapata (2008) which
is designed only for testing purposes. It contains
similar and dissimilar pairs of sequences (s1,s2)
where each sequence is a verb-noun pair (vi,ni).
From the positive part of this set, we can only de-
rive quadruples where (v1n1) ? (v2n2) but
we cannot derive the ideal resulting vector of the
composition (vini). Sets used to test multi-
word expression (MWE) detection models (e.g.,
(Schone and Jurafsky, 2001; Nicholson and Bald-
win, 2008; Kim and Baldwin, 2008; Cook et
al., 2008; Villavicencio, 2003; Korkontzelos and
Manandhar, 2009)) are again not useful as con-
taining only valid MWE that cannot be used to
determine the set of training triples needed here.
As a result, we need a novel idea to build sets
of triples to train CDS models. We can leverage
on knowledge stored in dictionaries. In the rest of
the section, we describe how we build the positive
example set E and a control negative example set
NE. Elements of the two sets are pairs (t,s) where
t is a target word s is a sequence of words. t is the
word that represent the distributional meaning of
s in the case ofE. Contrarily, t is totally unrelated
to the distributional meaning of s inNE. The sets
E and NE can be used both for training and for
testing. In the testing phase, we can use these sets
to determine whether a CDS model is good or not
and to compare different CDS models.
4.1 Building Positive Examples using
Dictionaries
Dictionaries as natural repositories of equivalent
expressions can be used to extract positive exam-
ples for training and testing CDS models. The
basic idea is the following: dictionary entries are
declarations of equivalence. Words or, occasion-
ally, multi-word expressions t are declared to be
semantically similar to their definition sequences
s. This happens at least for some sense of the
defined words. We can then observe that t ? s.
For example, we report some sample definitions
of contact and high life:
target word (t) definition sequence (s)
contact close interaction
high life excessive spending
In the first case, a word, i.e. contact, is semanti-
cally similar to a two-word expression, i.e. close
interaction. In the second case, two two-word ex-
pressions are semantically similar.
Then, the pairs (t, s) can be used to model
positive cases of compositional distributional se-
mantics as we know that the word sequence s
is compositional and it describes the meaning of
the word t. The distributional meaning ~t of t is
the expected distributional meaning of s. Conse-
quently, the vector ~t is what the CDS model (s)
should compositionally obtain from the vectors of
the components ~s1 . . . ~sm of s. This way of ex-
tracting similar expressions has some interesting
properties:
First property Defined words t are generally
single words. Thus, we can extract stable and
meaningful distributional vectors for these words
and then compare them to the distributional vec-
tors composed by CDS model. This is an impor-
tant property as we cannot compare directly the
distributional vector ~s of a word sequence s and
the vector (s) obtained by composing its com-
ponents. As the word sequence s grows in length,
the reliability of the vector ~s decreases since the
sequence s becomes rarer.
Second property Definitions s have a large va-
riety of different syntactic structures ranging from
simple structures as Adjective-Noun to more com-
plex ones. This gives the possibility to train and
test CDS models that take into account syntax.
Table 2 represents the distribution of the more
frequent syntactic structures in the definitions of
WordNet1 (Miller, 1995).
1Definitions were extracted from WordNet 3.0 and were
parsed with the Charniak parser (Charniak, 2000)
1267
Freq. Structure
2635 (FRAG (PP (IN) (NP (DT) (JJ) (NN))))
833 (NP (DT) (JJ) (NN))
811 (NP (NNS))
645 (NP (NNP))
623 (S (VP (VB) (ADVP (RB))))
610 (NP (JJ) (NN))
595 (NP (NP (DT) (NN)) (PP (IN) (NP (NN))))
478 (NP (NP (DT) (NN)) (PP (IN) (NP (NNP))))
451 (FRAG (PP (IN) (NP (NN))))
419 (FRAG (RB) (ADJP (JJ)))
375 (S (VP (VB) (PP (IN) (NP (DT) (NN)))))
363 (S (VP (VB) (PP (IN) (NP (NN)))))
342 (NP (NP (DT) (NN)) (PP (IN) (NP (DT) (NN))))
341 (NP (DT) (JJ) (JJ) (NN))
330 (ADJP (RB) (JJ))
307 (NP (JJ) (NNS))
244 (NP (DT) (NN) (NN))
241 (S (NP (NN)) (NP (NP (NNS)) (PP (IN) (NP (DT) (NNP)))))
239 (NP (NP (DT) (JJ) (NN)) (PP (IN) (NP (DT) (NN))))
Table 2: Top 20 syntactic structures of WordNet
definitions
4.2 Extracting Negative Examples from
Word Etymology
In order to devise complete training and testing
sets for CDS models, we need to find a sensible
way to extract negative examples. An option is to
randomly generate totally unrelated triples for the
negative examples set, NE. In this case, due to
data sparseness NE would mostly contain triples
(~z, ~x, ~y) where it is expected that ~z 6= (xy). Yet,
these can be too generic and too loosely related to
be interesting cases.
Instead we attempt to extract sets of negative
pairs (t,s) comparable with the one used for build-
ing the training set E. The target word t should
be a single word and s should be a sequence of
words. The latter should be a sequence of words
related by construction to t but the meaning of t
and s should be unrelated.
The idea is the following: many words are et-
ymologically derived from very old or ancient
words. These words represent a collocation which
is in general not related to the meaning of the
target word. For example, the word philosophy
derives from two Greek words philos (beloved)
and sophia (wisdom). However, the use of the
word philosophy in not related to the collocation
beloved wisdom. This word has lost its origi-
nal compositional meaning. The following table
shows some more etymologically complex words
along with the compositionally unrelated colloca-
tions:
target word compositionally unrelated seq.
municipal receive duty
octopus eight foot
As the examples suggest, we are able to build a
set NE with features similar to the features of
N . In particular, each target word is paired with
a related word sequence derived from its etymol-
ogy. These etymologically complex words are un-
related to the corresponding compositional collo-
cations. To derive a set NE with the above char-
acteristics we can use dictionaries containing ety-
mological information as Wiktionary2.
5 Experimental evaluation
In the previous sections, we presented the esti-
mated additive model (EAM): our approach to es-
timate the parameters of a generic additive model
for CDS. In this section, we experiment with this
model to determine whether it performs better
than existing models: the basic additive model
(BAM), the basic multiplicative model (BMM),
the basic additive model with selectional pref-
erences (BAM-SP), and the basic multiplicative
model with selectional preferences (BMM-SP)
(c.f. Sec. 2). In succession, we explore whether
our estimated additive model (EAM) is better than
any possible BAM obtained with parameter ad-
justment. In the rest of the section, we firstly give
the experimental setup and then we discuss the ex-
periments and the results.
5.1 Experimental setup
Our experiments aim to compare compositional
distributional semantic (CDS) models  with re-
spect to their ability of detecting statistically sig-
nificant difference between sets E and NE. In
particular, the average similarity sim(~z,(xy))
for (~z, ~x, ~y) ? E should be significantly different
from sim(~z,(xy)) for (~z, ~x, ~y) ? NE. In this
section, we describe the chosen similarity mea-
sure sim, statistical significance testing and con-
struction details for the training and testing set.
Cosine similarity was used to compare the con-
text vector ~z representing the target word z with
the composed vector (xy) representing the con-
text vector of sequence x y. Cosine similarity be-
2http://www.wiktionary.org
1268
tween two vectors ~x and ~y of the same dimension
is defined as:
sim(~x, ~y) = ~x ? ~y?~x? ?~y? (22)
where ? is the dot product and ?~a? is the magni-
tude of vector ~a computed the Euclidean norm.
To evaluate whether a CDS model distinguishes
positive examples E from negative examples
NE, we test if the distribution of similarities
sim(~z,(xy)) for (~z, ~x, ~y) ? E is statistically
different from the distribution of the same simi-
larities for (~z, ~x, ~y) ? NE. For this purpose, we
used Student?s t-test for two independent samples
of different sizes. t-test assumes that the two dis-
tributions are Gaussian and determines the prob-
ability that they are similar, i.e., derive from the
same underlying distribution. Low probabilities
indicate that the distributions are highly dissimilar
and that the corresponding CDS model performs
well, as it detects statistically different similarities
for the positive set E and the negative set NE.
Based on the null hypothesis that the means of
the two samples are equal, ?1 = ?2, Student?s t-
test takes into account the sizes N , means M and
variances s2 of the two samples to compute the
following value:
t = (M1 ?M2) ?1
?
2(s21 + s22)
df ?Nh
(23)
where df = N1 + N2 ? 2 stands for the degrees
of freedom and Nh = 2(N?11 + N?12 )?1 is the
harmonic mean of the sample sizes. Given the
statistic t and the degrees of freedom df , we can
compute the corresponding p-value, i.e., the prob-
ability that the two samples derive from the same
distribution. The null hypothesis can be rejected if
the p-value is below the chosen threshold of statis-
tical significance (usually 0.1, 0.05 or 0.01), oth-
erwise it is accepted. In our case, rejecting the
null hypothesis means that the similarity values of
instances of E are significantly different from in-
stances of NE, and that the corresponding CDS
model perform well. p-value can be used as a per-
formance ranking function for CDS models.
We constructed two sets of instances: (a) a
set containing Adjective-Noun or Noun-Noun se-
NN set VN set
BAM 0.05690 0.50753
BMM 0.20262 0.37523
BAM-SP 0.42574 0.01710
BMM-SP <1.00E-10 0.23552
EAM (k=20) 0.00431 0.00453
Table 3: Probability of confusing E and NE with
different CDS models
quences (NN set); and (b) a set containing Verb-
Noun sequences (VN set). Capturing different
syntactic relations, these two sets can support that
our results are independent from the syntactic re-
lation between the words of each sequence. For
each set, we used WordNet for extracting positive
examples E and Wiktionary for extracting nega-
tive examples NE as described in Section 4. We
obtained the following sets: (a) NN consists of
1065 word-sequence pairs from WordNet defini-
tions and 377 pairs extracted from Wiktionary;
and (b) VN consists of 161 word-sequence pairs
from WordNet definitions and 111 pairs extracted
from Wiktionary. We have then divided these two
sets in two parts of 50% each, for training and
testing. Instances of the training part of E have
been used to estimate matricesA andB for model
EAM , while the testing parts have been used for
testing all models. Frequency vectors for all sin-
gle words occurring in the above pairs were con-
structed from the British National Corpus using
sentences as contextual windows and words as
features. The resulting space has 689191 features.
5.2 Results and Analysis
The first set of experiments compares EAM with
other existing CDS models: BAM, BMM, BAM-
SP, and BMM-SP. Results are shown in Table 3.
The table reports the p-value, i.e., the probability
of confusing the positive set E and the negative
set NE for all models. Lower probabilities char-
acterise better models. Probabilities below 0.05
indicate that the model detects a statistically sig-
nificant difference between setsE andNE. EAM
has been computed with k = 20 different dimen-
sions for the pseudo-inverse matrix. The two basic
additive models (BAM and BAM-SP) have been
computed for ? = ? = 1.
1269
NN set V N set
Figure 1: p-values of BAM with different values for parameter ? (where ? = 1 ? ?) and of EAM for
different approximations of the SVD pseudo-inverse matrix (k)
The first observation is that EAM models sig-
nificantly separate positive from negative exam-
ples for both sets. This is not the case for any
of the other models. Only, the selectional prefer-
ences based models in two cases have this prop-
erty, but this cannot be generalised: BAM-SP on
the VN set and BMM-SP on the NN set. In gen-
eral, these models do not offer the possibility of
separating positive from negative examples.
In the second set of experiments, we attempt to
investigate whether simple parameter adjustment
of BAM can perform better than EAM. Results are
shown in figure 1. Plots show the basic additive
model (BAM) with different values for parameter
? (where ? = 1??) and EAM computed for dif-
ferent approximations of the SVD pseudo-inverse
matrix (i.e., with different k). The x-axis of the
plots represents parameter ? and the y-axis repre-
sents the probability of confusing the positive set
E and the negative setNE. The representation fo-
cuses on the performance ofBAM with respect to
different ? values. The performance of EAM for
different k values is represented with horizontal
lines. Probabilities of different models are directly
comparable. Line SS represents the threshold of
statistical significance; the value below which the
detected difference between the E and NE sets
becomes statistically significant.
Experimental results show some interesting
facts: While BAM for ? > 0 perform better than
EAM computed with k = 1 in the NN set, they
do not perform better in the VN set. EAM with
k = 1 has 1 degree of freedom corresponding to
1 parameter, the same as BAM. The parameter of
EAM is tuned on the training set, in contrast to
?, the parameter of BAM. Increasing the number
of considered dimensions, k of EAM, estimated
models outperform BAM for all values of param-
eter ?. Moreover, EAM detect a statistically sig-
nificant difference between theE and theNE sets
for k ? 10 and k = 20 for the NN set and the
VN set set, respectively. Simple parametrisation
of a BAM does not outperform the proposed esti-
mated additive model.
6 Conclusions
In this paper, we presented an innovative method
to estimate linear compositional distributional se-
mantics models. The core of our approach con-
sists on two parts: (1) providing a method to es-
timate the regression problem with multiple de-
pendent variables and (2) providing a training set
derived from dictionary definitions. Experiments
showed that our model is highly competitive with
respect to state-of-the-art models for composi-
tional distributional semantics.
References
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In proceedings of the 1st NAACL,
pages 132?139, Seattle, Washington.
Cook, Paul, Afsaneh Fazly, and Suzanne Stevenson.
2008. The VNC-Tokens Dataset. In proceedings
of the LREC Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008), Marrakech,
Morocco.
1270
Deerwester, Scott C., Susan T. Dumais, Thomas K.
Landauer, George W. Furnas, and Richard A. Harsh-
man. 1990. Indexing by latent semantic analysis.
Journal of the American Society of Information Sci-
ence, 41(6):391?407.
Erk, Katrin and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?
906. Association for Computational Linguistics.
Erk, Katrin. 2007. A simple, similarity-based model
for selectional preferences. In proceedings of ACL.
Association for Computer Linguistics.
Fallucchi, Francesca and Fabio Massimo Zanzotto.
2009. SVD feature selection for probabilistic tax-
onomy learning. In proceedings of the Workshop on
Geometrical Models of Natural Language Seman-
tics, pages 66?73. Association for Computational
Linguistics, Athens, Greece.
Firth, John R. 1957. Papers in Linguistics. Oxford
University Press, London.
Golub, Gene and William Kahan. 1965. Calculat-
ing the singular values and pseudo-inverse of a ma-
trix. Journal of the Society for Industrial and Ap-
plied Mathematics, Series B: Numerical Analysis,
2(2):205?224.
Harris, Zellig. 1964. Distributional structure. In Katz,
Jerrold J. and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Jones, Michael N. and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information
in a composite holographic lexicon. Psychological
Review, 114:1?37.
Kim, Su N. and Timothy Baldwin. 2008. Standard-
ised evaluation of english noun compound inter-
pretation. In proceedings of the LREC Workshop:
Towards a Shared Task for Multiword Expressions
(MWE 2008), pages 39?42, Marrakech, Morocco.
Korkontzelos, Ioannis and Suresh Manandhar. 2009.
Detecting compositionality in multi-word expres-
sions. In proceedings of ACL-IJCNLP 2009, Sin-
gapore.
Li, Ping, Curt Burgess, and Kevin Lund. 2000. The
acquisition of word meaning through global lexical
co-occurrences. In proceedings of the 31st Child
Language Research Forum.
Lin, Dekang and Patrick Pantel. 2001. DIRT-
discovery of inference rules from text. In Proceed-
ings of the ACM Conference on Knowledge Discov-
ery and Data Mining (KDD-01). San Francisco, CA.
Manning, Christopher D., Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
UK.
McCarthy, Diana and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Miller, George A. and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, VI:1?28.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-based
models of semantic composition. In proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio.
Association for Computational Linguistics.
Nicholson, Jeremy and Timothy Baldwin. 2008. Inter-
preting compound nominalisations. In proceedings
of the LREC Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008), pages 43?45,
Marrakech, Morocco.
Pado, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?
199.
Penrose, Roger. 1955. A generalized inverse for ma-
trices. In Proceedings of Cambridge Philosophical
Society.
Pollard, Carl J. and Ivan A. Sag. 1994. Head-driven
Phrase Structured Grammar. Chicago CSLI, Stan-
ford.
Schone, Patrick and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictio-
nary headwords a solved problem? In Lee, Lil-
lian and Donna Harman, editors, proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 100?108.
Villavicencio, Aline. 2003. Verb-particle construc-
tions and lexical resources. In proceedings of
the ACL 2003 workshop on Multiword expressions,
pages 57?64, Morristown, NJ, USA. Association for
Computational Linguistics.
1271
Coling 2010: Poster Volume, pages 481?489,
Beijing, August 2010
Bilingual lexicon extraction from comparable corpora using
in-domain terms
Azniah Ismail
Department of Computer Science
University of York
azniah@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
University of York
suresh@cs.york.ac.uk
Abstract
Many existing methods for bilingual
lexicon learning from comparable corpora
are based on similarity of context vectors.
These methods suffer from noisy vectors
that greatly affect their accuracy. We
introduce a method for filtering this noise
allowing highly accurate learning of
bilingual lexicons. Our method is based
on the notion of in-domain terms which
can be thought of as the most important
contextually relevant words. We provide
a method for identifying such terms.
Our evaluation shows that the proposed
method can learn highly accurate bilin-
gual lexicons without using orthographic
features or a large initial seed dictionary.
In addition, we also introduce a method
for measuring the similarity between
two words in different languages without
requiring any initial dictionary.
1 Introduction
In bilingual lexicon extraction, the context-based
approach introduced by Rapp (1995) is widely
used (Fung, 1995; Diab and Finch, 2000; among
others). The focus has been on learning from
comparable corpora since the late 1990s (Rapp,
1999; Koehn and Knight, 2002; among others).
However, so far, the accuracy of bilingual lexi-
con extraction using comparable corpora is quite
poor especially when orthographic features are
not used. Moreover, when orthographic features
are not used, a large initial seed dictionary is es-
sential in order to acquire higher accuracy lexicon
(Koehn and Knight, 2002). This means that cur-
rent methods are not suitable when the language
pairs are not closely related or when a large initial
seed dictionary is unavailable.
When learning from comparable corpora, a
large initial seed dictionary does not necessarily
guarantee higher accuracy since the source and
target texts are poorly correlated. Thus, inducing
highly accurate bilingual lexicon from compara-
ble corpora has so far been an open problem.
In this paper, we present a method that is able
to improve the accuracy significantly without re-
quiring a large initial bilingual dictionary. Our
approach is based on utilising highly associated
terms in the context vector of a source word.
For example, the source word powers is highly
associated with the context word delegation. We
note that, firstly, both share context terms such as
parliament and affairs. And, secondly, the trans-
lation equivalents of powers and delegation in the
target language are not only highly associated but
they also share context terms that are the trans-
lation equivalents of parliament and affairs (see
Figure 1).
2 Related work
Most of the early work in bilingual lexicon ex-
traction employ an initial seed dictionary. A large
bilingual lexicon with 10k to 20k entries is neces-
sary (Fung, 1995; Rapp, 1999).
Koehn and Knight (2002) introduce techniques
for constructing the initial seed dictionary auto-
matically. Their method is based on using identi-
cal spelling features. The accuracy of such initial
bilingual lexicon is almost 90.0 percent and can
be increased by restricting the word length (Koehn
and Knight, 2002). Koehn and Knight found ap-
proximately 1000 identical words in their German
481
Figure 1: An example of in-domain terms that co-occur in English and Spanish. The source word is
powers and the target word is poderes. The word delegation and delegacion are the highly associated
words with the source word and the target word respectively. Their in-domain terms, as shown in the
middle, can be used to map the source word in context of word delegation to its corresponding target
word in context of delegacion.
and English monolingual corpora. They expanded
the lexicon with the standard context-based ap-
proach and achieved about 25.0 percent accuracy
(Koehn and Knight, 2002).
Similar techniques were used in Haghighi et
al. (2008) who employ dimension reduction in
the extraction method. They recorded 58.0 per-
cent as their best F1 score for the context vec-
tor approach on non-parallel comparable corpora
containing Wikipedia articles. However, their
method scores less on comparable corpora con-
taining distinct sentences derived from the Eu-
roparl English-Spanish corpus.
3 Learning in-domain terms
In the standard context vector approach, we as-
sociate each source word and target word with
their context vectors. The source and target con-
text vectors are then compared using the initial
seed dictionary and a similarity measure. Learn-
ing from comparable corpora is particularly prob-
lematic due to data sparsity, as important context
terms may not occur in the training corpora while
some may occur but with low frequency and can
be missed. Some limitations may also be due to
the size of the initial seed dictionary being small.
The initial seed dictionary can also contribute
irrelevant or less relevant features that can mis-
lead the similarity measure especially when the
number of dimensions is large. The approach we
adopt attempts to overcome this problem.
In Figure 1, for the source word powers, dele-
gation is the highly associated word. Both powers
and delegation share common contextual terms
such as parliament and affairs. Now the transla-
tion equivalent of delegation is delegacion. For
the potential translation equivalent poderes, we
see that the common contextual terms shared by
powers and poderes are terms parlamento (par-
liament) and asuntos (affairs).
482
Figure 2: An example of English-Spanish lexicon learnt for the source word powers. On the top,
the system suggested competencias and rejected poderes when powers is associated with community,
democracy or independence. The word poderes is suggested when powers is associated with justice or
delegation.
We observe that these common contextual
terms are simultaneously the first-order and
second-order context terms of the target word.
They are the shared context terms of the target
word and its highly associated context term. We
define these terms as in-domain terms. These in-
domain terms can be used to map words to their
corresponding translations. The highly associated
context terms can be thought of as sense discrim-
inators that differentiate the different uses of the
target word. In Figure 2, we show how delegation
helps in selecting between the ?control or influ-
ence? sense of powers while rejecting the ?ability
or skill? sense.
In this paper, our focus is not on sense disam-
biguation and we follow current evaluation meth-
ods for bilingual lexicon extraction. However, it is
clear that our method can be adapted for building
sense disambiguated bilingual lexicons.
3.1 Identifying highly associated words
To identify the context terms CT (WS) of a source
word WS , as in (Rapp, 1999), we use log-
likelihood ratio (LL) Dunning (1993). We choose
all words with LL > t1 where t1 is a threshold.
The highly associated words then are the top k
highest ranked context terms. In our experiments,
we only choose the top 100 highest ranked context
terms as our highly associated terms.
In order to compute the log-likelihood ratio of
target word a to co-occur with context word b, we
create a contingency table. The contingency table
contains the observed values taken from a given
corpus. An example of the contingency table is
shown in Table 1.
C[i,j] community ? community
powers 124 1831 1955 C(powers)
? powers 11779 460218 471997 C(? powers)
11903 462049
C(community) C(? community)
Here C[i, j] denotes the count of the number of sentences in
which i co-occurs with j.
Total corpus size: N = 473952 in the above
Table 1: Contingency table for observed values of
target word powers and context word community.
The LL value of a target word a and context
word b is given by:
LL(a, b) =
?
i?{a,?a},j?{b,?b}
2C(i, j) log C(i, j)NC(i) C(j)
3.1.1 Identifying in-domain terms
In our work, to find the translation equivalent of a
source word WS , we do not use the context terms
CT (WS). Instead, we use the in-domain terms
IDT (WS ,WR). For each highly associated term
483
WR, we get different in-domain terms. Further-
more, IDT (WS ,WR) is a subset of CT (WS).
The in-domain terms of WS given the context
terms WR is given by:
ID(WS ,WR) = CT (WS) ? CT (WR)
Programme and public are some of the examples
of in-domain terms of powers given community as
the highly associated term.
3.1.2 Finding translations pairs
Note that ID(WS ,WR) is an in-domain term
vector in the source language. Let WT be a
potential translation equivalent for WS . Let,
tr(WR) be a translation equivalent for WR. Let
ID(WT , tr(WR)) be an in-domain term vector in
the target language.
We use tr(WS |WR) to denote the translation
proposed for WS given the highly associated term
WR. We compute tr(WS |WR) using:
tr(WS |WR) =
argmax
WT
sim(ID(WS ,WR), ID(WT , tr(WR)))
Our method learns translation pairs that are
conditioned on highly associated words (WR). Ta-
ble 2 provides a sample of English-Spanish lexi-
con learnt for the word power with different WR.
English Spanish
WS WR tr(WR) WT
Sim
powers
competencias 0.9876
poderes 0.9744community comunidad
independiente 0.9501
competencias 0.9948
poderes 0.9915democracy democracia
independiente 0.9483
competencias 0.9939
poderes 0.9745independence independencia
independiente 0.9633
poderes 0.9922
competencias 0.3450justice justicia
independiente 0.9296
poderes 0.9568
competencias 0.9266delegation delegacion
independiente 0.8408
Table 2: A sample of translation equivalents learnt
for powers.
In the next section, we introduce a similarity
measure that operates on the context vectors in the
source language and the target language without
requiring a seed dictionary.
4 Rank-binning similarity measure
Most existing methods for computing similarity
cannot be directly employed for measuring the
similarity between in-domain term context vec-
tors since each context vector is in a different lan-
guage. A bilingual dictionary can be assumed
but that greatly diminishes the practicality of the
method.
We address this by making an assumption. We
assume that the relative distributions of in-domain
context terms of translation equivalent pairs are
roughly comparable in the source language and
in the target language. For example, consider
the log-likelihood values of the in-domain terms
for the translation pair agreement-acuerdo (condi-
tioned on the highly associated term association-
associacion) given in Figure 3. We note that the
distribution of in-domain terms are comparable al-
though not identical. Thus, the distribution can be
used as a clue to derive translation pairs but we
need a method to compute similarity of the vector
of in-domain terms.
Rank-binning or rank histograms are usually
used as a diagnostic tool to evaluate the spread of
an ensemble rather than as a verification method.
Wong (2009) use the method of rank-binning to
roughly examine performance of a system on
learning lightweight ontologies. We apply the
rank-binning procedure for measuring the similar-
ity of word pairs.
Pre-processing step:
1. Let WS be a source language word and
x1, x2, ..., xn be the set of n context terms
ranked in descending log-likelihood values
of WS (see Table 3).
2. We transform the rank values of context
terms xk into the range [0,1] using:
zk =
rank(xk)? 1
n? 1
484
Figure 3: Similar distribution of in-domain terms
for agreement with association and acuerdo with
asociacion.
Binning procedure
We divide the interval [0, 1] into g bins1 of equal
length. Let b1, . . . , bg denote the g bins. Then
we map the in-domain terms vector ID(WS ,WR)
into the binned vector b1, . . . , bg. For each xk ?
ID(WS ,WR), this mapping is done by using the
corresponding zk from the pre-processing step.
For each bin, we count the number of different in-
domain terms that are mapped into this bin. Thus,
if the range of the first bin b1 is [0, 0.009] then eu-
ropean, legislative, parliament are mapped into b1
i.e. b1 = 3. The bins are normalised by dividing
with | ID(WS ,WR) |.
Rank binning similarity
We use Euclidean distance to compute similarity
between bins. Given, bins P = p1, . . . , pg and
Q = q1, . . . , qg, the Euclidean distance is given
by:
dist(P,Q) =
????
g?
i=1
(pi, qi)2
1We used the following formula to estimate the number
of bins:
g = 1 + 3.3 ? log (| ID(WS ,WR) |)
CT (powers)
Context term LL rank zk
european 491.33 1 0.00000
legislative 482.19 2 0.00406
parliament 408.26 3 0.00813
: : : :
: : : :
: : : :
public 16.96 245 0.99186
programme 15.40 246 0.99593
representatives 15.32 247 1.00000
n = 247
Table 3: Some examples of transformed values of
each term in CT (powers).
In the next section, we describe the setup in-
cluding the data, the lexicon and the evaluation
used in our experiments.
5 Experimental setup
5.1 Data
For comparable text, we derive English and Span-
ish distinct sentences from the Europarl parallel
corpora. We split the corpora into three parts ac-
cording to year. We used about 500k sentences
for each language in the experiments. This ap-
proach is further explained in Ismail and Man-
andhar (2009) and is similar to Koehn and Knight
(2001) and Haghighi et al (2008).
5.2 Pre-processing
For corpus pre-processing, we use sentence
boundary detection and tokenization on the raw
text before we clean the tags and filter stop words.
We sort and rank words in the text according to
their frequencies. For each of these words, we
compute their context term log-likelihood values.
5.3 Lexicon
In the experiment, a bilingual lexicon is required
for evaluation. We extract our evaluation lexicon
from the Word Reference2 free online dictio-
nary. This extracted bilingual lexicon has low cov-
erage.
2http://wordreference.com
485
5.4 Evaluation
In the experiments, we considered the task of
building a bilingual English-Spanish lexicon be-
tween the 2000 high frequency source and target
words, where we required each individual word
to have at least a hundred highly associated con-
text terms that are not part of the initial seed dic-
tionary. Different highly associated WR terms
for a given WT might derive similar (WS ,WT )
pairs. In this case, we only considered one of
the (WS ,WT ) pairs. In future work, we would
like to keep these for word sense discrimination
purposes. Note that we only considered proposed
translation pairs whose similarity values are above
a threshold t2.
We used the F1 measure to evaluate the pro-
posed lexicon against the evaluation lexicon. If
either WS or WT in the proposed translation pairs
is not in the evaluation lexicon, we considered the
translation pairs as unknown, although the pro-
posed translation pairs are correct. Recall is de-
fined as the proportion of the proposed lexicon di-
vided by the size of the lexicon and precision is
given by the number of correct translation pairs at
a certain recall value.
6 Experiments
In this section, we look into how the in-domain
context vectors affect system performance. We
also examine the potential of rank-binning simi-
larity measure.
6.1 From standard context vector to
in-domain context vector
Most research in bilingual lexicon extraction so
far has employed the standard context vector ap-
proach. In order to explore the potential of the
in-domain context vectors, we compare the sys-
tems that use in-domain approach against systems
that use the standard approach. We also employ
different sets of seed lexicon in each system to be
used in the similarity measure:
? Lex700: contains 700 cognate pairs from a
few Learning Spanish Cognate websites3.
3such as http://www.colorincolorado.org
and http://www.language-learning-advisor.com
? Lex100: contains 100 bilingual entries of the
most frequent words in the source corpus that
have translation equivalents in the extracted
evaluation lexicon. We select the top one
hundred words in the source corpus, so that
their translation equivalents is within the first
2000 high frequency words in the target cor-
pus.
? Lex160: contains words with similar spelling
that occur in both corpora. We used 160
word pairs with an edit distance value less
than 2, where each word is longer than 4
characters.
Models using the standard approach are de-
noted according to the size of the particular lex-
icon used in their context similarity measure,
i.e. CV-100 for using Lex100, CV-160 for using
Lex160 and CV-700 for using Lex700. We use IDT
to denote our model. We use lexicon sizes to dis-
tinguish the different variants, e.g. IDT-CV100 for
using Lex100, IDT-CV160 for using Lex160 and
IDT-CV700 for using Lex700.
With CV-700, the system achieved 52.6 per-
cent of the best F1 score. Using the same seed
dictionary, the best F1 score has increased about
20 percent points with IDT-CV700 recorded 73.1
percent. IDT-CV100 recorded about 15.0 percent
higher best F1 score than CV-100 with 80.9 and
66.4 percent respectively. Using an automatically
derived seed dictionary, IDT-CV160 yielded 70.0
percent of best F1 score while CV-160 achieved
62.4 percent. Results in Table 4 shows various
precisions px at recall values x.
Model P0.10 P0.25 P0.33 P0.50 BestF1score
CV-700 58.3 61.2 64.8 55.2 52.6
CV-100 52.0 53.0 47.2 44.8 66.4
CV-160 68.5 56.8 48.8 48.8 62.4
IDT-CV700 83.3 90.2 82.0 66.7 73.1
IDT-CV100 80.0 75.8 66.7 69.4 80.9
IDT-CV160 90.0 80.6 73.9 69.2 70.0
Table 4: Performance of different models.
486
6.2 Similarity measure using rank-binning
We use RB to denote our model based on the
rank-binning approach. Running RB means that
no seed dictionary is involved in the similarity
measure. We also ran the similarity measure in
the IDT (IDT-RB160) by employing the derived
Lex160 for the in-domain steps.
We ran several tests using IDT-RB160 with dif-
ferent numbers of bins. The results are illustrated
in Figure 4. The IDT-RB160 yielded 63.7 percent
of best F1 score with 4 bins. However, the F1
score starts to drop from 61.1 to 53.0 percent with
6 and 8 bins respectively. With 3 and 2 bins the
IDT-RB160 yielded 63.7 and 62.0 percent of best
F1 score respectively. Using 1 bin is not be pos-
sible as all values fall under one bin. Thus, the
rank-binning similarity measure for the rest of the
experiments where RB is mentioned, refers to a 4
bins setting.
Figure 4: Performance of IDT-RB160 with differ-
ent numbers of bins.
While systems using the standard context simi-
larity measure yielded scores higher than 50.0 per-
cent of best F1, the RB achieved only 39.2 per-
cent. However, RB does not employ an initial
dictionary and does not use orthographic features.
As mentioned above, the system scored higher
when the similarity measure was used in the IDT
(i.e. IDT-RB160). Note that Lex160 is derived au-
tomatically so the approach can also be consid-
ered as unsupervised. The system performance
is slightly lower compared to the conventional
CV-160. However, IDT-CV160 outperforms both
of the systems (see Figure 5).
Figure 5: Performance of different unsupervised
models.
Overall, systems that exploit in-domain terms
yielded higher F1 scores compared to the conven-
tional context vector approach.
6.3 Comparison with CCA
Previous work in extracting bilingual lexicons
from comparable corpora generally employ the
conventional context vector approach. Haghighi
et al (2008) focused on applying canonical cor-
relation analysis (CCA), a dimension reduction
technique, to improve the method. They were us-
ing smaller comparable corpora, taken from the
first 50k sentences of English Europarl and the
second 50k of Spanish Europarl, and different ini-
tial seed dictionary. Hence, we tested CCA in our
experimental setup. In CV-700 setting, using CCA
yields 57.5 percent of the best F1 score compared
to 73.1 percent of the best F1 score with IDT that
we reported in Section 6.2.
7 Discussion
7.1 Potential of in-domain terms
Our experiments clearly demonstrate that the use
of in-domain terms achieves higher F1 scores
compared to conventional methods. It also shows
that our method improves upon earlier reported
dimension reduction methods. From our obser-
vation, the number of incorrect translation pairs
487
were further reduced when the context terms were
filtered. Recall that the in-domain terms in the
target language were actually the shared context
terms of the target word and its highly associ-
ated context terms. Nevertheless, this approach
actually depends on the initial bilingual lexicon
in order to translate those highly associated con-
text terms into the source language. Table 5
shows some examples of most confidence trans-
lation pairs proposed by the IDT-CV100.
English Spanish Sim score Correct?
principle principio 0.9999 Yes
government estado 0.9999 No
government gobierno 0.9999 Yes
resources recursos 0.9999 Yes
difficult dificil 0.9999 Yes
sector competencia 0.9998 No
sector sector 0.9998 Yes
programme programa 0.9998 Yes
programme comunidad 0.9998 No
agreement acuerdo 0.9998 Yes
Table 5: Some examples of most confident trans-
lation pairs proposed by IDT-CV100 ranked by
similarity scores.
7.2 Seed dictionary variation
The initial seed dictionary plays a major role in
extracting bilingual lexicon from comparable cor-
pora. There are a few different ways for us to
derive a seed dictionary. Recall that Lex700 and
Lex100, that are used in the experiments, are de-
rived using different methods. The F1 scores of
the system using Lex100 were much higher com-
pared to the system using Lex700. Thus, extend-
ing Lex100 with additional high frequency words
may provide higher accuracy.
One important reason is that all bilingual en-
tries in Lex100 occur frequently in the corpora.
Although the size of Lex700 is larger, it is not sur-
prising that most of the words never occur in the
corpora, such as volleyball and romantic. How-
ever, using Lex160 is more interesting since it is
derived automatically from the corpora, though
one should realize that the relationship between
the language pair used in the respective mono-
lingual corpora, English and Spanish, may have
largely affect the results. Thus, for other sys-
tems involving unrelated language pairs, the rank-
binning similarity measure might be a good op-
tion.
7.3 Word sense discrimination ability
As mentioned in Section 5.4, each source word
may have more than one highly associated context
term, WR. Different WR may suggest different
target words for the same source word. For exam-
ple, given the source word powers and the highly
associated word community, competencias is pro-
posed as the best translation equivalent. On the
other hand, for same source word powers, when
the highly associated word is delegation, the tar-
get word poderes is suggested.
8 Conclusion
We have developed a method to improve the F1
score in extracting bilingual lexicon from compa-
rable corpora by exploiting in-domain terms. This
method also performs well without using an ini-
tial seed dictionary. More interestingly, our work
reveals the potential of building word sense dis-
ambiguated lexicons.
References
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL 2008, Colum-
bus, Ohio.
Azniah Ismail and Suresh Manandhar. 2009. Utiliz-
ing contextually relevant terms in bilingual lexicon
extraction In Workshop on Unsupervised and Min-
imally Supervised Learning of Lexical Semantics,
Boulder, Colorado.
Mona Diab and Steve Finch. 2000. A statistical word-
level translation model for comparable corpora. In
Proceedings of the Conference on Content-based
multimedia information access (RIAO).
Pascale Fung. 1995. Compiling bilingual lexicon en-
tries from a non-parallel English-Chinese corpus. In
Proceedings of the 3rd Annual Workshop on Very
Large Corpora, Boston, Massachusetts, 173-183.
Philipp Koehn and Kevin Knight. 2001. Knowledge
sources for word-level translation models. In Pro-
ceedings of the Conference on empirical method in
natural language processing (EMNLP).
488
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL 2002, Philadelphia, USA,
9-16.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the ACL 33,
320-322.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the ACL 37, 519-
526.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistic, volume 19(1), 61-74.
Wilson Yiksen Wong. 2009. Learning lightweight on-
tologies from text across different domains using the
web as background knowledge. Ph.D. Thesis. Uni-
versity of Western Australia
489
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 745?755,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Word Sense Induction & Disambiguation Using
Hierarchical Random Graphs
Ioannis P. Klapaftis
Department of Computer Science
University of York
United Kingdom
giannis@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
University of York
United Kingdom
suresh@cs.york.ac.uk
Abstract
Graph-based methods have gained attention in
many areas of Natural Language Processing
(NLP) including Word Sense Disambiguation
(WSD), text summarization, keyword extrac-
tion and others. Most of the work in these ar-
eas formulate their problem in a graph-based
setting and apply unsupervised graph cluster-
ing to obtain a set of clusters. Recent studies
suggest that graphs often exhibit a hierarchi-
cal structure that goes beyond simple flat clus-
tering. This paper presents an unsupervised
method for inferring the hierarchical group-
ing of the senses of a polysemous word. The
inferred hierarchical structures are applied to
the problem of word sense disambiguation,
where we show that our method performs sig-
nificantly better than traditional graph-based
methods and agglomerative clustering yield-
ing improvements over state-of-the-art WSD
systems based on sense induction.
1 Introduction
A number of NLP problems can be cast into a graph-
based framework, in which entities are represented
as vertices in a graph and relations between them are
depicted by weighted or unweighted edges. For in-
stance, in unsupervised WSD a number of methods
(Widdows and Dorow, 2002; Ve?ronis, 2004; Agirre
et al, 2006) have constructed word co-occurrence
graphs for a target polysemous word and applied
graph-clustering to obtain the clusters (senses) of
that word.
Similarly in text summarization, Mihalcea (2004)
developed a method, in which sentences are rep-
resented as vertices in a graph and edges between
them are drawn according to their common tokens
or words of a given POS category, e.g. nouns.
Graph-based ranking algorithms, such as PageRank
(Brin and Page, 1998), were then applied in order
to determine the significance of sentences. In the
same vein, graph-based methods have been applied
to other problems such as determining semantic sim-
ilarity of text (Ramage et al, 2009).
Recent studies (Clauset et al, 2006; Clauset et
al., 2008) suggest that graphs exhibit a hierarchi-
cal structure (e.g. a binary tree), in which vertices
are divided into groups that are further subdivided
into groups of groups, and so on, until we reach the
leaves. This hierarchical structure provides addi-
tional information as opposed to flat clustering by
explicitly including organisation at all scales of a
graph (Clauset et al, 2008). In this paper, we present
an unsupervised method for inferring the hierarchi-
cal structure (binary tree) of a graph, in which ver-
tices are the contexts of a polysemous word and
edges represent the similarity between contexts. The
method that we use to infer that hierarchical struc-
ture is the Hierarchical Random Graphs (HRGs) al-
gorithm due to Clauset et al (2008).
The binary tree produced by our method groups
the contexts of a polysemous word at different
heights of the tree. Thus, it induces the senses of
that word at different levels of sense granularity. To
evaluate our method, we apply it to the problem of
noun sense disambiguation showing that inferring
the hierarchical structure using HRGs provides ad-
ditional information from the observed graph lead-
ing to improved WSD performance compared to: (1)
745
Figure 1: Stages of the proposed method.
simple flat clustering, and (2) traditional agglomera-
tive clustering. Finally, we compare our results with
state-of-the-art sense induction systems and show
that our method yields improvements. Figure 1
shows the different stages of the proposed method
that we describe in the following sections.
2 Related work
Typically, graph-based methods, when applied to
unsupervised sense disambiguation represent each
word wi co-occurring with the target word tw as a
vertex. Two vertices are connected via an edge if
they co-occur in one or more contexts of tw. Once
the co-occurrence graph of tw has been constructed,
different graph clustering algorithms are applied to
induce the senses. Each cluster (induced sense) con-
sists of a set of words that are semantically related to
the particular sense. Figure 2 shows an example of
a graph for the target word paper that appears with
two different senses scholarly article and newspa-
per.
Ve?ronis (2004) has shown that co-occurrence
graphs are small-world networks that contain highly
dense subgraphs representing the different clusters
(senses) of the target word (Ve?ronis, 2004). To iden-
tify these dense regions Ve?ronis?s algorithm itera-
tively finds their hubs, where a hub is a vertex with a
very high degree. The degree of a vertex is defined to
be the number of edges incident to that vertex. The
identified hub is then deleted along with its direct
neighbours from the graph producing a new cluster.
For example, in Figure 2 the highest degree ver-
tex, news, is the first hub, which would be deleted
along with its direct neighbours. The deleted re-
gion corresponds to the newspaper sense of the tar-
get word paper. Ve?ronis (2004) further processed
the identified clusters (senses), in order to assign the
rest of graph vertices to the identified clusters by
utilising the minimum spanning tree of the original
graph.
In Agirre et al (2006), the algorithm of Ve?ronis
(2004) is analysed and assessed on the SensEval-3
dataset (Snyder and Palmer, 2004), after optimis-
ing its parameters on the SensEval-2 dataset (Ed-
monds and Dorow, 2001). The results show that the
WSD F-Score outperforms the Most Frequent Sense
(MFS) baseline by approximately 10%, while induc-
ing a large number of clusters (with averages of 60
to 70).
Another graph-based method is presented in
(Dorow and Widdows, 2003). They extract only
noun neighbours that appear in conjunctions or dis-
junctions with the target word. Additionally, they
extract second-order co-occurrences. Nouns are rep-
resented as vertices, while edges between vertices
are drawn, if their associated nouns co-occur in con-
junctions or disjunctions more than a given num-
ber of times. This co-occurrence frequency is also
used to weight the edges. The resulting graph is
then pruned by removing the target word and ver-
tices with a low degree. Finally, the MCL algorithm
(Dongen, 2000) is used to cluster the graph and pro-
duce a set of clusters (senses) each one consisting of
a set of contextually related words.
Chinese Whispers (CW) (Biemann, 2006) is a
parameter-free1 graph clustering method that has
been applied in sense induction to cluster the co-
occurrence graph of a target word (Biemann, 2006),
as well as a graph of collocations related to the tar-
get word (Klapaftis and Manandhar, 2008). The
evaluation of the collocational-graph method in the
SemEval-2007 sense induction task (Agirre and
Soroa, 2007) showed promising results.
All the described methods for sense induction ap-
1One needs to specify only the number of iterations. The
number of clusters is generated automatically.
746
Figure 2: Graph of words for the target word paper.
Numbers inside vertices correspond to their degree.
Figure 3: Running example of graph creation
ply flat graph clustering methods to derive the clus-
ters (senses) of a target word. As a result, they ne-
glect the fact that their constructed graphs often ex-
hibit a hierarchical structure that is useful in several
tasks including word sense disambiguation.
3 Building a graph of contexts
This section describes the process of creating a
graph of contexts for a polysemous target word. Fig-
ure 3 provides a running example of the different
stages of our method. In the example, the target
word paper appears with the scholarly article sense
in the contexts A, B, and with the newspaper sense
in the contexts C and D.
3.1 Corpus preprocessing
Let bc denote the base corpus consisting of the con-
texts containing the target word tw. In our work,
a context is defined as a paragraph2 containing the
target word.
The aim of this stage is to capture nouns contex-
tually related to tw. Initially, the target word is re-
moved from bc and part-of-speech tagging is applied
to each context. Following the work in (Ve?ronis,
2004; Agirre et al, 2006) only nouns are kept and
lemmatised. In the next step, the distribution of each
noun in the base corpus is compared to the distri-
bution of the same noun in a reference corpus3 us-
ing the log-likelihood ratio (G2) (Dunning, 1993).
Nouns with a G2 below a pre-specified threshold
(parameter p1) are removed from each paragraph of
the base corpus. The upper left part of Figure 3
shows the words kept as a result of this stage.
3.2 Graph creation
Graph vertices: To create the graph of vertices, we
represent each context ci as a vertex in a graph G.
Graph edges: Edges between the vertices of the
graph are drawn based on their similarity, defined
in Equation 1, where simcl(ci, cj) is the colloca-
tional weight of contexts ci, cj and simwd(ci, cj)
is their bag-of-words weight. If the edge weight
W (ci, cj) is above a prespecified threshold (param-
eter p3), then an edge is drawn between the corre-
sponding vertices in the graph.
W (ci, cj) =
1
2
(simcl(ci, cj) + simwd(ci, cj)) (1)
Collocational weight: The limited polysemy of col-
locations can be exploited to compute the similarity
between contexts ci and cj . In our setting, a colloca-
tion is a juxtaposition of two nouns within the same
context. Thus, given a context ci, each of its nouns
is combined with any other noun yielding a total of
(N
2
)
collocations for a context with N nouns. Each
collocation, clij is weighted using the log-likelihood
ratio (G2) (Dunning, 1993) and is filtered out if the
G2 is below a prespecified threshold (parameter p2).
At the end of this process, each context ci of tw is
associated with a vector of collocations (vi). The
upper right part of Figure 3 shows the collocations
associated with each context of our example.
2Our definition of context is equivalent to an instance of the
target word in the SemEval-2007 sense induction task dataset
(Agirre and Soroa, 2007).
3The British National Corpus, 2001, Distributed by Oxford
University Computing Services.
747
Given two contexts ci and cj , we calculate their
collocational weight using the Jaccard coefficient
on the collocational vectors, i.e. simcl(ci, cj) =
|vi?vj |
|vi?vj |
. The selection of Jaccard is based on the work
of Weeds et al (2004), who analyzed the variation
in a word?s distributionally nearest neighbours with
respect to a variety of similarity measures. Their
analysis showed that there are three classes of mea-
sures, i.e. those selecting distributionally more gen-
eral neighbours (e.g. cosine), those selecting distri-
butionally less general neighbours (e.g. AMCRM-
Precision (Weeds et al, 2004)) and those without a
bias towards the distributional generality of a neigh-
bour (e.g. Jaccard). In our setting, we are interested
in calculating the similarity between two contexts
without any bias. We selected Jaccard, since the rest
of that class?s measures are based on pointwise mu-
tual information that assigns high weights to infre-
quent events.
Bag-of-words weight: Estimating context similar-
ity using collocations may provide reliable estimates
regarding the existence of an edge in the graph, how-
ever, it also suffers from data sparsity. For this rea-
son, we also employ a bag-of-words model. Specif-
ically, each context ci is associated with a vector gi
that contains the nouns kept as result of the corpus
preprocessing stage. The upper left part of Figure
3 shows the words associated with each context of
our example. Given two contexts ci and cj , we cal-
culate their bag-of-words weight using the Jaccard
coefficient on the word vectors, i.e. simwd(ci, cj) =
|gi?gj |
|gi?gj |
.
The collocational weight and bag-of-words
weight are averaged to derive the edge weight be-
tween two contexts as defined in Equation 1. The
resulting graph of our running example is shown on
the bottom of Figure 3. This graph is the input to the
hierarchical random graphs method (Clauset et al,
2008) described in the next section.
4 Hierarchical Random Graphs for sense
induction
In this section, we describe the process of inferring
the hierarchical structure of the graph of contexts
using hierarchical random graphs (Clauset et al,
2008).
Figure 4: Two dendrograms for the graph in Figure 3.
4.1 The Hierarchical Random Graph model
A dendrogram is a binary tree with n leaves and
n ? 1 parents. Figure 4 shows an example of two
dendrograms with 4 leaves and 3 parents. Given a
set of n contexts that we need to arrange hierarchi-
cally, let us denote by G = (V,E) the graph of con-
texts, where V = {v0, v1 . . . vn} is the set of ver-
tices, E = {e0, e1 . . . em} is the set of edges and
ek = {vi, vj}.
Given an undirected graph G, each of its n ver-
tices is a leaf in a dendrogram, while the internal
nodes of that dendrogram indicate the hierarchical
relationships among the leaves. We denote this or-
ganisation byD = {D1, D2, . . . Dn?1}, where each
Dk is an internal node. Every pair of nodes (vi, vj)
is associated with a unique Dk, which is their low-
est common ancestor in the tree. In this manner D
partitions the edges that exist in G.
The primary assumption in the hierarchical ran-
dom graph model is that edges in G exist indepen-
dently, but with a probability that is not identically
distributed. In particular, the probability that an edge
{vi, vj} exists in G is given by a parameter ?k asso-
ciated with Dk, the lowest common ancestor of vi
and vj in D. In this manner, the topological struc-
ture D and the vector of probabilities ~? define the
HRG given by H(D, ~?) (Clauset et al, 2008).
748
4.2 HRG parameterisation
Assuming a uniform prior over all HRGs, the target
is to identify the parameters of D and ~?, so that the
chosen HRG is statistically similar to G. Let Dk be
an internal node of dendrogram D and f(Dk) be the
number of edges between the vertices of the subtrees
of the subtree rooted at Dk that actually exist in G.
For example, in Figure 4(A), f(D2) = 1, because
there is one edge in G connecting vertices B and C.
Let l(Dk) be the number of leaves in the left subtree
of Dk, and r(Dk) be the number of leaves in the
right subtree. For example in Figure 4(A), l(D2) =
2 and r(D2) = 2. The likelihood of the hierarchical
random graph (D, ~?) is defined in Equation 2, where
A(Dk) = l(Dk)r(Dk)? f(Dk).
L(D, ~?) =
?
Dk?D
?f(Dk)k (1? ?k)
A(Dk) (2)
The probabilities ?k that maximise the likelihood
of a dendrogram D can be easily estimated using
the method of MLE i.e ?k =
f(Dk)
l(Dk)r(Dk)
. Substi-
tuting this into Equation 2 yields Equation 3. For
numerical reasons, it is more convenient to work
with the logarithm of the likelihood which is defined
in Equation 4, where h(?k) = ??k log ?k ? (1 ?
?k) log (1? ?k).
L(D) =
?
Dk?D
[?
?k
k (1? ?k)
1??k ]l(Dk)r(Dk) (3)
logL(D) = ?
?
Dk?D
h(?k)l(Dk)r(Dk) (4)
As can be observed, each term ?l(Dk)r(Dk)h(?k)
is maximised when ?k approaches 0 or 1. This
means that high-likelihood dendrograms partition
vertices into subtrees, such that the connections
among their vertices in the observed graph are either
very rare or very common (Clauset et al, 2008). For
example, consider the two dendrograms in Figures
4(A) and 4(B). We observe that 4(A) is more likely
than 4(B), since it provides a better division of the
network leaves. Particularly, the likelihood of 4(A)
is L(D1) = (11 ? (1? 1)1) ? (11 ? (1? 1)1) ? (0.251 ?
(1 ? 0.25)3) = 0.105, while the likelihood of 4(B)
is L(D2) = (00 ? (1? 0)1) ? (11 ? (1? 1)1) ? (0.52 ?
(1? 0.5)2) = 0.062.
4.2.1 MCMC sampling
Finding the values of ?k using the MLE method
is straightforward. However, this is not the case
for maximising the likelihood function over the
space of all possible dendrograms. Given a graph
with n vertices, i.e. n leaves in each dendrogram,
the total number of different dendrograms is super-
exponential ((2n? 3)!! ?
?
2(2n)n?1e?n) (Clauset
et al, 2006).
To deal with this problem, we use a Markov
Chain Monte Carlo (MCMC) method that samples
dendrograms from the space of dendrogram mod-
els with probability proportional to their likelihood.
Each time MCMC samples a dendrogram with a
new highest likelihood, that dendrogram is stored.
Hence, our goal is to choose the highest likelihood
dendrogram once MCMC has converged.
Following the work in (Clauset et al, 2008),
we pick a set of transitions between dendrograms,
where a transition is a re-arrangement of the sub-
trees of a dendrogram. In particular, given a current
dendrogram Dcurr, each internal node Dk of Dcurr
is associated with three subtrees of Dcurr. For in-
stance, in Figure 5A, the subtrees st1 and st2 are
derived from the two children of Dk and the third
st3 from its sibling. Given a current dendrogram,
Dcurr, the algorithm proceeds as follows:
1. Choose an internal node, Dk ? Dcurr uni-
formly.
2. Generate two possible new configurations of
the subtrees of Dk (See Figure 5).
3. Choose one of the configurations uniformly to
generate a new dendrogram, Dnext.
4. Accept or reject Dnext according to
Metropolis-Hastings (MH) rule.
5. If transition is accepted, then Dcurr = Dnext.
6. GOTO 1.
According to MH rule (Newman and Barkema,
1999), a transition is accepted if logL(Dnext) ?
logL(Dcurr); otherwise the transition is accepted
with probability L(Dnext)L(Dcurr) . These transitions define
an ergodic Markov chain, hence its stationary distri-
bution can be reached (Clauset et al, 2008).
749
Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration,
(C) second alternative configuration. Note that swapping st1, st2 in (A) results in an equivalent tree. Hence, this
configuration is excluded.
In our experiments, we noticed that the algorithm
converged relatively quickly. The same behaviour
(roughly O(n2) steps) was also noticed in Clauset et
al. (2008), when considering graphs with thousands
of vertices.
5 HRGs for sense disambiguation
5.1 Sense mapping
The output of HRG learning is a dendrogramD with
n leaves (contexts) and n?1 internal nodes. To per-
form sense disambiguation, we mapped the internal
nodes to gold standard senses using a sense-tagged
corpus. Such a sense-tagged corpus is needed when
induced word senses need to be mapped to a gold
standard sense inventory.
Instead of using a hard mapping from the den-
drogram internal nodes to the Gold Standard (GS)
senses, we use a soft probabilistic mapping and cal-
culateP (sk|Di), i.e the probability of sense sk given
node Di. Let F (Di) be the set of training contexts
grouped by internal node Di. Let F ?(sk) be the set
of training contexts that are tagged with sense sk.
Then the conditional probability, P (sk|Di), is de-
fined in Equation 5.
P (sk|Di) =
|F (Di) ? F ?(sk)|
|F (Di)|
(5)
Table 1 provides a sense-tagged corpus for the
running example of Figure 3. Using this corpus
and the tree in Figure 4(A), P (s1|D2) = 23 and
P (s2|D2) = 13 . In Figure 4(A) the rest of the calcu-
lated conditional probabilities are given.
5.2 Sense tagging
For evaluation we compared the proposed method
against the current state-of-the-art sense induction
GS sense Context ID Context words
s1 A journal, scholar, observation
science, paper
s1 B scholar, scholar, author,
publication, paper
s2 D times, guardian,
journalist, paper
Table 1: Sense-tagged corpus for the example in Figure 3
systems in the WSD task. We followed the setting
of SemEval-2007 sense induction task (Agirre and
Soroa, 2007). In this setting, the base corpus (bc)
(Section 3.1) for a target word consists both of the
training and testing corpus. As a result, a testing
context cj of tw is a leaf in the generated dendro-
gram. The process of disambiguating cj is straight-
forward exploiting the structural information pro-
vided by HRGs.
w(sk, cj) =
?
Di?H(cj)
P (sk|Di) ? ?i (6)
w(s?, cj) = argmax sk(w(sk, cj)) (7)
Let H(cj) denote the set of parents for context cj .
Then, the weight assigned to sense sk is the sum of
weighted scores provided by each identified parent.
This is shown in Equation 6, where ?i is the proba-
bility associated with each internal nodeDi from the
hierarchical random graph (see Figure 4(A)). This
probability reflects the discriminating ability of in-
ternal nodes.
Finally, the highest weight determines the win-
ning sense for context cj (Equation 7). In our ex-
ample (Figure 4(A)), w(s1, C) = (0 ?1+ 23 ?0.25) =
0.16 andw(s2, C) = (1?1+ 13 ?0.25) = 1.08. Hence,
s2 is the winning sense.
750
Parameter Range
G2 word threshold (p1) 15,25,35,45
G2 collocation threshold (p2) 10,15,20
Edge similarity threshold (p3) 0.05,0.09,0.13
Table 2: Parameter values used in the evaluation.
6 Evaluation
6.1 Evaluation setting & baselines
We evaluate our method on the nouns of the
SemEval-2007 word sense induction task (Agirre
and Soroa, 2007) under the second evaluation setting
of that task, i.e. supervised evaluation. Specifically,
we use the standard WSD measures of precision and
recall in order to produce their harmonic mean (F-
Score). The official scoring software of that task has
been used in our evaluation. Note that the unsuper-
vised measures of that task are not directly applica-
ble to our induced hierarchies, since they focus on
assessing flat clustering methods.
The first aim of our evaluation is to test whether
inferring the hierarchical structure of the constructed
graphs improves WSD performance. For that reason
our first baseline, Chinese Whispers Unweighted
version (CWU), takes as input the same unweighted
graph of contexts as HRGs in order to produce a
flat clustering. The set of produced clusters is then
mapped to GS senses using the training dataset and
performance is then measured on the testing dataset.
We followed the same sense mapping method as in
the SemEval-2007 sense induction task (Agirre and
Soroa, 2007).
Our second baseline, Chinese Whispers Weighted
version (CWW), is similar to the previous one, with
the difference that the edges of the input graph
are weighted using Equation 1. For clustering the
graphs of CWU and CWW we employ, Chinese
Whispers4 (Biemann, 2006).
The second aim of our evaluation is to assess
whether the hierarchical structure inferred by HRGs
is more informative than the hierarchical struc-
ture inferred by traditional Hierarchical Clustering
(HAC). Hence, our third baseline, takes as input a
similarity matrix of the graph vertices and performs
bottom-up clustering with average-linkage, which
has already been used in WSI in (Pantel and Lin,
4The number of iterations for CW was set to 200.
2003) and was shown to have superior or similar per-
formance to single-linkage and complete-linkage in
the related problem of learning a taxonomy of senses
(Klapaftis and Manandhar, 2010).
To calculate the similarity matrix of vertices we
follow a process similar to the one used in Sec-
tion 4.2 for calculating the probability of an inter-
nal node. The similarity between two vertices is
calculated according to the degree of connected-
ness among their direct neighbours. Specifically,
we would like to assign high similarity to pairs of
vertices, whose neighbours are close to forming a
clique.
Given two vertices (contexts) ci and cj , let
N(ci, cj) be the set of their neighbours andK(ci, cj)
be the set of edges between the vertices inN(ci, cj).
The maximum number of edges that could exist be-
tween vertices in N(ci, cj) is
(|N(ci,cj)|
2
)
. Thus, the
similarity of ci, cj is set equal to the number of
edges that actually exist in that neighbourhood di-
vided by the total number of edges that could exist
( |K(ci,cj)|
(|N(ci,cj)|2 )
).
The disambiguation process using the HAC tree
is identical to the one presented in Section 5.2 with
the only difference that the internal probability, ?i,
in Equation 6 does not exist for HAC. Hence, we re-
placed it with the factor 1|H(Di)| , whereH(Di) is the
set of children of internal node Di. This factor pro-
vides lower weights for nodes high in the tree, since
their discriminating ability will possibly be lower.
6.2 Results & discussion
Table 2 shows the parameter values used in the eval-
uation. Figure 6(A) shows the performance of the
proposed method against the baselines for p3 = 0.05
and different p1 and p2 values. Figure 6(B) il-
lustrates the results of the same experiment using
p3 = 0.09. In both figures, we observe that HRGs
outperform the CWU baseline under all parameter
combinations. In particular, all of the 12 perfor-
mance differences for p3 = 0.09 are statistically
significant using McNemar?s test at 95% confidence
level, while for p3 = 0.05 only 2 out of the 12 per-
formance differences were not judged as significant
from the test.
The picture is the same for p3 = 0.13, where
CWU performs significantly worse than for p3 =
751
Figure 6: Performance analysis of HRGs, CWU, CWW & HAC for different parameter combinations (Table 2). (A)
All combinations of p1, p2 and p3 = 0.05. (B) All combinations of p1, p2 and p3 = 0.09.
0.05 and p3 = 0.09. Specifically, the largest perfor-
mance difference between HRGs and CWU is 9.4%
at p1 = 25, p2 = 10 and p3 = 0.13. Setting the ver-
tex similarity threshold (p3) equal to 0.13 leads to
more sparse and disconnected graphs, which causes
Chinese Whispers to produce a large number of clus-
ters. This leads to sparsity problems and unreliable
mapping of clusters to GS senses due to the lack of
adequate training data. In contrast, HRGs suffer less
at this high threshold, although their performance
when p3<0.13 is better.
This picture does not change for the weighted ver-
sion of Chinese Whispers (CWW) which performs
worse than CWU. This is because CWW produces
a smaller number of clusters than CWU that con-
flate the target word senses. It seems that using
weighted edges creates a bias towards the MFS, in
effect missing rare senses of a target word. This
means that a number of words in the bag-of-words
context vectors and collocations in the collocational
context vectors (Section 3.2) are associated to more
than one sense of the target word and most strongly
associated to the MFS. As a result, increasing the p1
threshold to 25 and 35 leads to a higher performance
for CWW, since many of these words and colloca-
tions are filtered out.
Overall, the comparison of HRGs against the
CWU and CWW baselines has shown that inferring
the hierarchical structure of observed graphs leads
to improved WSD performance as opposed to using
flat clustering. This is because HRGs are able to in-
fer both the hierarchical structure of the graph and
include the probabilities, ?k, associated with each
internal node. These probabilities reflect the dis-
criminating ability of each node, offering informa-
tion missed by flat clustering.
In Figures 6(A) and 6(B) we observe that HRGs
perform significantly better than HAC. In particular,
all of their performance differences are statistically
significant for these parameter values. The largest
performance difference is 6.0% at p1 = 45, p2 = 10
and p3 = 0.05. However, this picture is not the same
when considering a higher context similarity thresh-
old (p3 = 0.13) as Figure 7 shows. In particular,
HRGs and HAC perform similarly for p3 = 0.13,
while the majority of performance differences are
not statistically significant.
The similar behaviour of HRGs and HAC at this
threshold is caused both by the worse performance
of HRGs and the improved performance of HAC as
opposed to lower p3 values. As it has been men-
tioned, setting p3 = 0.13 leads to sparse and dis-
connected graphs. Additionally, the likelihood func-
tion (Equation 3) is maximised when the probabil-
ity, ?k, of an internal node, Dk, approaches 0 or 1.
This creates a bias towards dendrograms, in which a
large number of internal nodes have zero probabil-
ity. These dendrograms might be a good-fit to the
observed graph, but not to the GS.
In contrast, HAC is less affected, because it never
considers creating an internal node, when the max-
imum similarity among any pair of two candidate
752
Figure 7: Performance of HRGs and HAC for different
parameter combinations (Table 2). All combinations of
p1, p2 and p3 ? 0.13.
subtrees is zero. Additionally, our experiments show
that HAC is unable to deal with noise when con-
sidering sparse graphs (p3<0.13). For that reason,
the F-Score of HAC increases as the edge similarity
threshold decreases.
To further investigate this issue and test whether
HAC is able to achieve a higher F-Score than HRGs
in higher p3 values, we executed two more experi-
ments for HAC and HRGs increasing p3 to 0.17 and
0.21 respectively. In the first case we observed that
the performance of HAC remained relatively stable
compared to p3 = 0.13, while in the second case the
performance of HAC decreased as Figure 7 shows.
In both cases, HAC performed significantly better
than HRGs.
Overall, the comparison of HRGs against HAC
has shown that HRGs perform significantly better
than HAC when considering connected or less sparse
graphs (p3<0.13). This is due to the fact that HAC
creates dendrograms, in which connections within
the clusters are dense, while connections between
the clusters are sparse, i.e. it only considers assorta-
tive structures. In contrast, HRGs also consider dis-
assortative dendrograms, i.e. dendrograms in which
vertices are less likely to be connected on small
scales than on large ones, as well as mixtures of
assortative and disassortative (Clauset et al, 2008).
This is achieved by allowing the probability ?k of
a node k to vary arbitrarily throughout the dendro-
gram.
HAC performs similarly or better than HRGs for
largely disconnected and sparse graphs, because
HRGs become biased towards disassortative trees
which are not a good fit to the GS (Figure 7). De-
spite that, our evaluation has also shown that the best
performance of HAC (F-Score = 86.0% at p1 = 15,
p2 = 10, p3 = 0.13) is significantly lower than
the best performance of HRGs (F-Score = 87.6% at
p1 = 35, p2 = 10, p3 = 0.09).
6.3 Comparison to state-of-the-art methods
Table 3 compares the best performing parameter
combination of our method against state-of-the-art
methods. Table 3 also includes the best performance
of our baselines, i.e HAC, CWU and CWW.
Brody & Lapata (2009) presented a sense induc-
tion method that is related to Latent Dirichlet Al-
location (Blei et al, 2003). In their work, they
model the target word instances as samples from a
multinomial distribution over senses which are suc-
cessively characterized as distributions over words
(Brody and Lapata, 2009). A significant advantage
of their method is the inclusion of more than one
layer in the LDA setting, where each layer corre-
sponds to a different feature type e.g. dependency
relations, bigrams, etc. The inclusion of different
feature types as separate models in the sense in-
duction process can easily be modeled in our set-
ting, by inferring a different hierarchy of target word
instances according to each feature type, and then
combining all of them to a consensus tree. In this
work, we have focused on extracting a single hierar-
chy combining word co-occurrence and bigram fea-
tures.
Niu et al (2007) developed a vector-based
method that performs sense induction by group-
ing the contexts of a target word using three types
of features, i.e. POS tags of neighbouring words,
word co-occurrences and local collocations. The se-
quential information bottleneck algorithm (Slonim
et al, 2002) is applied for clustering. HRGs perform
slightly better than the methods of Brody & Lap-
ata (2009) and Niu et al (2007), although the dif-
ferences are not significant (McNemar?s test at 95%
confidence level).
Klapaftis & Manandhar (2008) developed a
graph-based sense induction method, in which ver-
tices correspond to collocations related to the tar-
get word and edges between vertices are drawn ac-
753
System Performance (%)
HRGs 87.6
(Brody and Lapata, 2009) 87.3
(Niu et al, 2007) 86.8
(Klapaftis and Manandhar, 2008) 86.4
HAC 86.0
CWU 85.1
CWW 84.7
(Pedersen, 2007) 84.5
MFS 80.9
Table 3: HRGs against recent methods & baselines.
cording to the co-occurrence frequency of the cor-
responding collocations. The constructed graph is
smoothed to identify more edges between vertices
and then clustered using Chinese Whispers (Bie-
mann, 2006). This method is related to the basic
inputs of our presented method. Despite that, it is
a flat clustering method that ignores the hierarchical
structure exhibited by observed graphs. The previ-
ous section has shown that inferring the hierarchical
structure of graphs leads to superior WSD perfor-
mance.
Pedersen (2007) presented SenseClusters, a
vector-based method that clusters second order co-
occurrence vectors using k-means, where k is auto-
matically determined using the Adapted Gap Statis-
tic (Pedersen and Kulkarni, 2006). As can be ob-
served, HRGs perform significantly better than the
methods of Pedersen (2007) and Klapaftis & Man-
andhar (2008) (McNemar?s test at 95% confidence
level).
Finally, Table 3 shows that the best performing
parameter combination of HRGs achieves a signifi-
cantly higher F-Score than the best performing pa-
rameter combination of HAC, CWU and CWW. Fur-
thermore, HRGs outperform the most frequent sense
baseline by 6.7%.
7 Conclusion & future work
We presented an unsupervised method for inferring
the hierarchical grouping of the senses of a polyse-
mous word. Our method creates a graph, in which
vertices correspond to contexts of a polysemous tar-
get word and edges between them are drawn ac-
cording to their similarity. The hierarchical random
graphs algorithm (Clauset et al, 2008) was applied
to the constructed graph in order to infer its hierar-
chical structure, i.e. binary tree.
The learned tree provides an induction of the
senses of a given word at different levels of sense
granularity and was applied to the problem of WSD.
The WSD process mapped the tree?s internal nodes
to GS senses using a sense tagged corpus, and then
tagged new instances by exploiting the structural in-
formation provided by the tree.
Our experimental results have shown that our
graphs exhibit hierarchical organisation that can
be captured by HRGs, in effect providing im-
proved WSD performance compared to flat cluster-
ing. Additionally, our comparison against hierarchi-
cal agglomerative clustering with average-linkage
has shown that HRGs perform significantly better
than HAC when the graphs do not suffer from spar-
sity (disconnected graphs). The comparison with
state-of-the-art sense induction systems has shown
that our method yields improvements.
Our future work focuses on using different feature
types, e.g. dependency relations, second-order co-
occurrences, named entities and others to construct
our undirected graphs and then applying HRGs, in
order to measure the impact of each feature type
on the induced hierarchical structures within a WSD
setting. Moreover, following the work in (Clauset et
al., 2008), we are also working on using MCMC in
order to sample more than one dendrogram at equi-
librium, and then combine them to a consensus tree.
This consensus tree might be able to express a larger
amount of topological features of the initial undi-
rected graph.
Finally in terms of evaluation, our future work
also focuses on evaluating HRGs using a fine-
grained sense inventory, extending the evaluation on
the SemEval-2010 WSI task dataset (Manandhar et
al., 2010) as well as applying HRGs to other related
tasks such as taxonomy learning.
Acknowledgements
This work is supported by the European Com-
mission via the EU FP7 INDECT project, Grant
No.218086, Research area: SEC-2007-1.2-01 Intel-
ligent Urban Environment Observation System. The
authors would like to thank the anonymous review-
ers for their useful comments.
754
References
Eneko Agirre and Aitor. Soroa. 2007. Semeval-2007
Task 02: Evaluating Word Sense Induction and Dis-
crimination Systems. In Proceedings of SemEval-
2007, pages 7?12, Prague, Czech Republic.
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle,
and Aitor Soroa. 2006. Two Graph-based Algorithms
for State-of-the-art WSD. In Proceedings of EMNLP-
2006, pages 585?593, Sydney, Australia.
Chris Biemann. 2006. Chinese Whispers - An Efficient
Graph Clustering Algorithm and its Application to
Natural Language Processing Problems. In Proceed-
ings of TextGraphs, pages 73?80, New York, USA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res., 3:993?1022.
Sergey Brin and Lawrence Page. 1998. The Anatomy
of a Large-Scale Hypertextual Web Search Engine.
Comput. Netw. ISDN Syst., 30(1-7):107?117.
Samuel Brody and Mirella Lapata. 2009. Bayesian Word
Sense Induction. In Proceedings of EACL-2009, pages
103?111, Athens, Greece. ACL.
Aaron Clauset, Cristopher Moore, and Mark E. J. New-
man. 2006. Structural Inference of Hierarchies in Net-
works. In Proceedings of the ICML-2006 Workshop
on Social Network Analysis, pages 1?13, Pittsburgh,
USA.
Aaron Clauset, Cristopher Moore, and Mark E. J. New-
man. 2008. Hierarchical Structure and the Prediction
of Missing Links in Networks. Nature, 453(7191):98?
101.
Stijn Dongen. 2000. Performance Criteria for Graph
Clustering and Markov Cluster Experiments. Tech-
nical report, CWI (Centre for Mathematics and Com-
puter Science), Amsterdam, The Netherlands.
Beate Dorow and Dominic Widdows. 2003. Discovering
Corpus-specific Word Senses. In Proceedings of the
EACL-2003, pages 79?82, Budapest, Hungary.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19(1):61?74.
Phil Edmonds and Beate Dorow. 2001. Senseval-2:
Overview. In Proceedings of SensEval-2, pages 1?5,
Toulouse, France.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
Sense Induction Using Graphs of Collocations. In
Proceedings of ECAI-2008, pages 298?302, Patras,
Greece.
Ioannis P. Klapaftis and Suresh Manandhar. 2010. Tax-
onomy Learning Using Word Sense Induction. In Pro-
ceedings of NAACL-HLT-2010, pages 82?90, Los An-
geles, California, June. ACL.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. Semeval-2010
Task 14: Word Sense Induction & Disambiguation. In
Proceedings of SemEval-2, Uppsala, Sweden. ACL.
Rada Mihalcea. 2004. Graph-based Ranking Algorithms
for Sentence Extraction, Applied to Text Summariza-
tion. In Proceedings of the ACL 2004 on Interactive
poster and demonstration sessions, page 20, Morris-
town, NJ, USA.
Mark Newman and Gerard Barkema. 1999. Monte Carlo
Methods in Statistical Physics. Oxford: Clarendon
Press, New York, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 2007.
I2R: Three Systems for Word Sense Discrimination,
Chinese Word Sense Disambiguation, and English
Word Sense Disambiguation. In Proceedings of
SemEval-2007, pages 177?182, Prague, Czech Repub-
lic.
Patrick Pantel and Dekang Lin. 2003. Automatically
Discovering Word Senses. In Proceedings of NAACL-
HLT-2003, pages 21?22, Morristown, NJ, USA.
Ted Pedersen and Anagha Kulkarni. 2006. Automatic
Cluster Stopping With Criterion Functions and the gap
Statistic. In Proceedings of the 2006 Conference of the
North American Chapter of the ACL on Human Lan-
guage Technology, pages 276?279, Morristown, NJ,
USA.
Ted Pedersen. 2007. UMND2 : Senseclusters Applied to
the Sense Induction Task of Senseval-4. In Proceed-
ings of SemEval-2007, pages 394?397, Prague, Czech
Republic.
Daniel Ramage, Anna N. Rafferty, and Christopher D.
Manning. 2009. Random Walks for Text Semantic
Similarity. In Proceedings of TextGraphs-4, Suntec,
Singapore, August.
Noam Slonim, Nir Friedman, and Naftali Tishby. 2002.
Unsupervised Document Classification Using Sequen-
tial Information Maximization. In SIGIR 2002, pages
129?136, New York, NY, USA. ACM.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish All-words Task. In Rada Mihalcea and Phil Ed-
monds, editors, In Proceedings of Senseval-3, pages
41?43, Barcelona, Spain.
Jean Ve?ronis. 2004. Hyperlex: Lexical Cartography for
Information Retrieval. Computer Speech & Language,
18(3):223?252.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of COLING-2004, pages
10?15, Morristown, NJ, USA.
Dominic Widdows and Beate Dorow. 2002. A Graph
Model for Unsupervised Lexical Acquisition. In Pro-
ceedings of Coling-2002, pages 1?7, Morristown, NJ,
USA.
755
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 654?663,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Probabilistic Hierarchical Clustering of
Morphological Paradigms
Burcu Can
Department of Computer Science
University of York
Heslington, York, YO10 5GH, UK
burcucan@gmail.com
Suresh Manandhar
Department of Computer Science
University of York
Heslington, York, YO10 5GH, UK
suresh@cs.york.ac.uk
Abstract
We propose a novel method for learning
morphological paradigms that are struc-
tured within a hierarchy. The hierarchi-
cal structuring of paradigms groups mor-
phologically similar words close to each
other in a tree structure. This allows detect-
ing morphological similarities easily lead-
ing to improved morphological segmen-
tation. Our evaluation using (Kurimo et
al., 2011a; Kurimo et al 2011b) dataset
shows that our method performs competi-
tively when compared with current state-of-
art systems.
1 Introduction
Unsupervised morphological segmentation of a
text involves learning rules for segmenting words
into their morphemes. Morphemes are the small-
est meaning bearing units of words. The learn-
ing process is fully unsupervised, using only raw
text as input to the learning system. For example,
the word respectively is split into morphemes re-
spect, ive and ly. Many fields, such as machine
translation, information retrieval, speech recog-
nition etc., require morphological segmentation
since new words are always created and storing
all the word forms will require a massive dictio-
nary. The task is even more complex, when mor-
phologically complicated languages (i.e. agglu-
tinative languages) are considered. The sparsity
problem is more severe for more morphologically
complex languages. Applying morphological seg-
mentation mitigates data sparsity by tackling the
issue with out-of-vocabulary (OOV) words.
In this paper, we propose a paradigmatic ap-
proach. A morphological paradigm is a pair
(StemList, SuffixList) such that each concatena-
tion of Stem+Suffix (where Stem ? StemList and
Suffix ? SuffixList) is a valid word form. The
learning of morphological paradigms is not novel
as there has already been existing work in this area
such as Goldsmith (2001), Snover et al(2002),
Monson et al(2009), Can and Manandhar (2009)
and Dreyer and Eisner (2011). However, none of
these existing approaches address learning of the
hierarchical structure of paradigms.
Hierarchical organisation of words help cap-
ture morphological similarities between words in
a compact structure by factoring these similarities
through stems, suffixes or prefixes. Our inference
algorithm simultaneously infers latent variables
(i.e. the morphemes) along with their hierarchical
organisation. Most hierarchical clustering algo-
rithms are single-pass, where once the hierarchi-
cal structure is built, the structure does not change
further.
The paper is structured as follows: section 2
gives the related work, section 3 describes the
probabilistic hierarchical clustering scheme, sec-
tion 4 explains the morphological segmenta-
tion model by embedding it into the clustering
scheme and describes the inference algorithm
along with how the morphological segmentation
is performed, section 5 presents the experiment
settings along with the evaluation scores, and fi-
nally section 6 presents a discussion with a com-
parison with other systems that participated in
Morpho Challenge 2009 and 2010 .
2 Related Work
We propose a Bayesian approach for learning of
paradigms in a hierarchy. If we ignore the hierar-
chical aspect of our learning algorithm, then our
654
walk walking talked  talks
{walk}{0,ing} {talk}{ed,s} {quick}{0,ly}
quick quickly
{walk, talk, quick}{0,ed,ing,ly, s}
{walk, talk}{0,ed,ing,s}
Figure 1: A sample tree structure.
method is similar to the Dirichlet Process (DP)
based model of Goldwater et al(2006). From
this perspective, our method can be understood
as adding a hierarchical structure learning layer
on top of the DP based learning method proposed
in Goldwater et al(2006). Dreyer and Eisner
(2011) propose an infinite Diriclet mixture model
for capturing paradigms. However, they do not
address learning of hierarchy.
The method proposed in Chan (2006) also
learns within a hierarchical structure where La-
tent Dirichlet Allocation (LDA) is used to find
stem-suffix matrices. However, their work is su-
pervised, as true morphological analyses of words
are provided to the system. In contrast, our pro-
posed method is fully unsupervised.
3 Probabilistic Hierarchical Model
The hierarchical clustering proposed in this work
is different from existing hierarchical clustering
algorithms in two aspects:
? It is not single-pass as the hierarchical struc-
ture changes.
? It is probabilistic and is not dependent on a
distance metric.
3.1 Mathematical Definition
In this paper, a hierarchical structure is a binary
tree in which each internal node represents a clus-
ter.
Let a data set be D = {x1, x2, . . . , xn} and
T be the entire tree, where each data point xi is
located at one of the leaf nodes (see Figure 2).
Here, Dk denotes the data points in the branch
Tk. Each node defines a probabilistic model for
words that the cluster acquires. The probabilistic
Di
Dk
Dj
X1 X2 X3 X4
Figure 2: A segment of a tree with with internal nodes
Di, Dj , Dk having data points {x1, x2, x3, x4}. The
subtree below the internal node Di is called Ti, the
subtree below the internal node Dj is Tj , and the sub-
tree below the internal node Dk is Tk.
model can be denoted as p(xi|?) where ? denotes
the parameters of the probabilistic model.
The marginal probability of data in any node
can be calculated as:
p(Dk) =
?
p(Dk|?)p(?|?)d? (1)
The likelihood of data under any subtree is de-
fined as follows:
p(Dk|Tk) = p(Dk)p(Dl|Tl)p(Dr|Tr) (2)
where the probability is defined in terms of left Tl
and right Tr subtrees. Equation 2 provides a re-
cursive decomposition of the likelihood in terms
of the likelihood of the left and the right sub-
trees until the leaf nodes are reached. We use the
marginal probability (Equation 1) as prior infor-
mation since the marginal probability bears the
probability of having the data from the left and
right subtrees within a single cluster.
4 Morphological Segmentation
In our model, data points are words to be clus-
tered and each cluster represents a paradigm. In
the hierarchical structure, words will be organised
in such a way that morphologically similar words
will be located close to each other to be grouped
in the same paradigms. Morphological similarity
refers to at least one common morpheme between
words. However, we do not make a distinction be-
tween morpheme types. Instead, we assume that
each word is organised as a stem+suffix combina-
tion.
4.1 Model Definition
Let a dataset D consist of words to be analysed,
where each word wi has a latent variable which is
655
the split point that analyses the word into its stem
si and suffix mi:
D = {w1 = s1 +m1, . . . , wn = sn +mn}
The marginal likelihood of words in the node k
is defined such that:
p(Dk) = p(Sk)p(Mk)
= p(s1, s2, . . . , sn)p(m1,m2, . . . ,mn)
The words in each cluster represents a
paradigm that consists of stems and suffixes. The
hierarchical model puts words sharing the same
stems or suffixes close to each other in the tree.
Each word is part of all the paradigms on the
path from the leaf node having that word to the
root. The word can share either its stem or suffix
with other words in the same paradigm. Hence,
a considerable number of words can be generated
through this approach that may not be seen in the
corpus.
We postulate that stems and suffixes are gen-
erated independently from each other. Thus, the
probability of a word becomes:
p(w = s+m) = p(s)p(m) (3)
We define two Dirichlet processes to generate
stems and suffixes independently:
Gs|?s, Ps ? DP (?s, Ps)
Gm|?m, Pm ? DP (?m, Pm)
s|Gs ? Gs
m|Gm ? Gm
where DP (?s, Ps) denotes a Dirichlet process
that generates stems. Here, ?s is the concentration
parameter, which determines the number of stem
types generated by the Dirichlet process. The
smaller the value of the concentration parameter,
the less likely to generate new stem types the pro-
cess is. In contrast, the larger the value of concen-
tration parameter, the more likely it is to generate
new stem types, yielding a more uniform distribu-
tion over stem types. If ?s < 1, sparse stems are
supported, it yields a more skewed distribution.
To support a small number of stem types in each
cluster, we chose ?s < 1.
Here, Ps is the base distribution. We use the
base distribution as a prior probability distribu-
tion for morpheme lengths. We model morpheme
?s ?m
Ps PmGs Gm
si mi
wi
L N
n
Figure 3: The plate diagram of the model, representing
the generation of a word wi from the stem si and the
suffix mi that are generated from Dirichlet processes.
In the representation, solid-boxes denote that the pro-
cess is repeated with the number given on the corner
of each box.
lengths implicitly through the morpheme letters:
Ps(si) =
?
ci?si
p(ci) (4)
where ci denotes the letters, which are distributed
uniformly. Modelling morpheme letters is a way
of modelling the morpheme length since shorter
morphemes are favoured in order to have fewer
factors in Equation 4 (Creutz and Lagus, 2005b).
The Dirichlet process,DP (?m, Pm), is defined
for suffixes analogously. The graphical represen-
tation of the entire model is given in Figure 3.
Once the probability distributions G =
{Gs, Gm} are drawn from both Dirichlet pro-
cesses, words can be generated by drawing a stem
from Gs and a suffix from Gm. However, we do
not attempt to estimate the probability distribu-
tions G; instead, G is integrated out. The joint
probability of stems is calculated by integrating
out Gs:
p(s1, s2, . . . , sM )
=
?
p(Gs)
L
?
i=1
p(si|Gs)dGs
(5)
where L denotes the number of stem tokens. The
joint probability distribution of stems can be tack-
led as a Chinese restaurant process. The Chi-
nese restaurant process introduces dependencies
between stems. Hence, the joint probability of
656
stems S = {s1, . . . , sL} becomes:
p(s1, s2, . . . , sL)
= p(s1)p(s2|s1) . . . p(sM |s1, . . . , sM?1)
= ?(?s)
?(L+ ?s)
?K?1s
K
?
i=1
Ps(si)
K
?
i=1
(nsi ? 1)!
(6)
where K denotes the number of stem types. In
the equation, the second and the third factor corre-
spond to the case where novel stems are generated
for the first time; the last factor corresponds to the
case in which stems that have already been gener-
ated for nsi times previously are being generated
again. The first factor consists of all denominators
from both cases.
The integration process is applied for proba-
bility distributions Gm for suffixes analogously.
Hence, the joint probability of suffixes M =
{m1, . . . ,mN} becomes:
p(m1,m2, . . . ,mN )
= p(m1)p(m2|m1) . . . p(mN |m1, . . . ,mN?1)
= ?(?)
?(N + ?)
?T
T
?
i=1
Pm(mi)
T
?
i=1
(nmi ? 1)!
(7)
where T denotes the number of suffix types and
nmi is the number of stem types mi which have
been already generated.
Following the joint probability distribution of
stems, the conditional probability of a stem given
previously generated stems can be derived as:
p(si|S?si , ?s, Ps)
=
?
?
?
nS?sisi
L?1+?s if si ? S
?si
?s?Ps(si)
L?1+?s otherwise
(8)
where nS?sisi denotes the number of stem in-
stances si that have been previously generated,
where S?si denotes the stem set excluding the
new instance of the stem si.
The conditional probability of a suffix given the
other suffixes that have been previously generated
is defined similarly:
p(mi|M?mi , ?m, Pm)
=
?
?
?
nM?mimi
N?1+?m if mi ?M
?mi
?m?Pm(mi)
N?1+?m otherwise
(9)
where nM
?i
k
mi is the number of instances mi that
have been generated previously where M?mi is
plugg+ed skew+ed
exclaim+ed
borrow+s borrow+ed
liken+s liken+ed
consist+s
consist+ed
Figure 4: A portion of a sample tree.
the set of suffixes, excluding the new instance of
the suffix mi.
A portion of a tree is given in Figure 4. As
can be seen on the figure, all words are lo-
cated at leaf nodes. Therefore, the root node
of this subtree consists of words {plugg+ed,
skew+ed, exclaim+ed, borrow+s, borrow+ed,
liken+s, liken+ed, consist+s, consist+ed}.
4.2 Inference
The initial tree is constructed by randomly choos-
ing a word from the corpus and adding this into a
randomly chosen position in the tree. When con-
structing the initial tree, latent variables are also
assigned randomly, i.e. each word is split at a ran-
dom position (see Algorithm 1).
We use Metropolis Hastings algorithm (Hast-
ings, 1970), an instance of Markov Chain Monte
Carlo (MCMC) algorithms, to infer the optimal
hierarchical structure along with the morphologi-
cal segmentation of words (given in Algorithm 2).
During each iteration i, a leaf node Di = {wi =
si +mi} is drawn from the current tree structure.
The drawn leaf node is removed from the tree.
Next, a node Dk is drawn uniformly from the tree
657
Algorithm 1 Creating initial tree.
1: input: data D = {w1 = s1 + m1, . . . , wn =
sn +mn},
2: initialise: root? D1 where
D1 = {w1 = s1 +m1}
3: initialise: c? n? 1
4: while c >= 1 do
5: Draw a word wj from the corpus.
6: Split the word randomly such that wj =
sj +mj
7: Create a new node Dj where Dj =
{wj = sj +mj}
8: Choose a sibling node Dk for Dj
9: Merge Dnew ? Dj ?Dk
10: Remove wj from the corpus
11: c? c? 1
12: end while
13: output: Initial tree
to make it a sibling node to Di. In addition to a
sibling node, a split point wi = s
?
i + m
?
i is drawn
uniformly. Next, the node Di = {wi = s
?
i + m
?
i}
is inserted as a sibling node to Dk. After updating
all probabilities along the path to the root, the new
tree structure is either accepted or rejected by ap-
plying the Metropolis-Hastings update rule. The
likelihood of data under the given tree structure is
used as the sampling probability.
We use a simulated annealing schedule to up-
date PAcc:
PAcc =
(
pnext(D|T )
pcur(D|T )
)
1
?
(10)
where ? denotes the current temperature,
pnext(D|T ) denotes the marginal likelihood
of the data under the new tree structure, and
pcur(D|T ) denotes the marginal likelihood of
data under the latest accepted tree structure. If
(pnext(D|T ) > pcur(D|T )) then the update is
accepted (see line 9, Algorithm 2), otherwise, the
tree structure is still accepted with a probability
of pAcc (see line 14, Algorithm 2). In our
experiments (see section 5) we set ? to 2. The
system temperature is reduced in each iteration
of the Metropolis Hastings algorithm:
? ? ? ? ? (11)
Most tree structures are accepted in the earlier
stages of the algorithm, however, as the tempera-
Algorithm 2 Inference algorithm
1: input: data D = {w1 = s1 + m1, . . . , wn =
sn + mn}, initial tree T , initial temperature
of the system ?, the target temperature of the
system ?, temperature decrement ?
2: initialise: i ? 1, w ? wi = si + mi,
pcur(D|T )? p(D|T )
3: while ? > ? do
4: Remove the leaf node Di that has the
word wi = si +mi
5: Draw a split point for the word such that
wi = s
?
i +m
?
i
6: Draw a sibling node Dj
7: Dm ? Di ?Dj
8: Update pnext(D|T )
9: if pnext(D|T ) >= pcur(D|T ) then
10: Accept the new tree structure
11: pcur(D|T ) ? pnext(D|T )
12: else
13: random ? Normal(0, 1)
14: if random <
(
pnext(D|T )
pcur(D|T )
)
1
? then
15: Accept the new tree structure
16: pcur(D|T ) ? pnext(D|T )
17: else
18: Reject the new tree structure
19: Re-insert the node Di at its pre-
vious position with the previous
split point
20: end if
21: end if
22: w ? wi+1 = si+1 +mi+1
23: ? ? ? ? ?
24: end while
25: output: A tree structure where each node
corresponds to a paradigm.
ture decreases only tree structures that lead lead to
a considerable improvement in the marginal prob-
ability p(D|T ) are accepted.
An illustration of sampling a new tree structure
is given in Figure 5 and 6. Figure 5 shows that
D0 will be removed from the tree in order to sam-
ple a new position on the tree, along with a new
split point of the word. Once the leaf node is re-
moved from the tree, the parent node is removed
from the tree, as the parent node D5 will consist
of only one child. Figure 6 shows that D8 is sam-
pled to be the sibling node of D0. Subsequently,
the two nodes are merged within a new cluster that
658
D5
D1
D6
D2 D3 D4D0
D7
D8
Figure 5: D0 will be removed from the tree.
D9
D1
D6
D2 D3 D4 D0
D7
D8
Figure 6: D8 is sampled to be the sibling of D0.
introduces a new node D9.
4.3 Morphological Segmentation
Once the optimal tree structure is inferred, along
with the morphological segmentation of words,
any novel word can be analysed. For the segmen-
tation of novel words, the root node is used as it
contains all stems and suffixes which are already
extracted from the training data. Morphological
segmentation is performed in two ways: segmen-
tation at a single point and segmentation at multi-
ple points.
4.3.1 Single Split Point
In order to find single split point for the mor-
phological segmentation of a word, the split point
yielding the maximum probability given inferred
stems and suffixes is chosen to be the final analy-
sis of the word:
argmax
j
p(wi = sj +mj |Droot, ?m, Pm, ?s, Ps)
(12)
where Droot refers to the root of the entire tree.
Here, the probability of a segmentation of a
given word given Droot is calculated as given be-
low:
p(wi = sj +mj |Droot, ?m, Pm, ?s, Ps) =
p(sj |Sroot, ?s, Ps) p(mj |Mroot, ?m, Pm)
(13)
where Sroot denotes all the stems in Droot and
Mroot denotes all the suffixes in Droot. Here
p(sj |Sroot, ?s, Ps) is calculated as given below:
p(si|Sroot, ?s, Ps) =
?
?
?
nSrootsi
L+?s if si ? Sroot
?s?Ps(si)
L+?s otherwise
(14)
Similarly, p(mj |Mroot, ?m, Pm) is calculated
as:
p(mi|Mroot, ?m, Pm) =
?
?
?
nMrootmi
N+?m if mi ?Mroot
?m?Pm(mi)
N+?m otherwise
(15)
4.3.2 Multiple Split Points
In order to discover words with multiple split
points, we propose a hierarchical segmentation
where each segment is split further. The rules for
generating multiple split points is given by the fol-
lowing context free grammar:
w ? s1 m1|s2 m2 (16)
s1 ? s m|s s (17)
s2 ? s (18)
m1 ? m m (19)
m2 ? s m|m m (20)
Here, s is a pre-terminal node that generates all
the stems from the root node. And similarly, m is
a pre-terminal node that generates all the suffixes
from the root node. First, using Equation 16, the
word (e.g. housekeeper) is split into s1 m1 (e.g.
housekeep+er) or s2 m2 (house+keeper). The first
segment is regarded as a stem, and the second
segment is either a stem or a suffix, consider-
ing the probability of having a compound word.
Equation 12 is used to decide whether the sec-
ond segment is a stem or a suffix. At the sec-
ond segmentation level, each segment is split once
more. If the first production rule is followed in
the first segmentation level, the first segment s1
can be analysed as s m (e.g. housekeep+?) or s s
659
!"#$%&%%'%(
!"#$% &%%'%(
!"#$% ) &%%' %(
Figure 7: An example that depicts how the word
housekeeper can be analysed further to find more split
points.
(e.g. house+keep) (Equation 17). The decision
to choose which production rule to apply is made
using:
s1 ?
{
s s if p(s|S, ?s, Ps) > p(m|M,?m, Pm)
s m otherwise
(21)
where S and M denote all the stems and suffixes
in the root node.
Following the same production rule, the second
segment m1 can only be analysed as m m (er+?).
We postulate that words cannot have more than
two stems and suffixes always follow stems. We
do not allow any prefixes, circumfixes, or infixes.
Therefore, the first production rule can output two
different analyses: s m m m and s s m m (e.g.
housekeep+er and house+keep+er).
On the other hand, if the word is analysed as
s2 m2 (e.g. house+keeper), then s2 cannot be
analysed further. (e.g. house). The second seg-
ment m2 can be analysed further, such that s m
(stem+suffix) (e.g. keep+er, keeper+?) or m m
(suffix+suffix). The decision to choose which pro-
duction rule to apply is made as follows:
m2 ?
{
s m if p(s|S, ?s, Ps) > p(m|M,?m, Pm)
mm otherwise
(22)
Thus, the second production rule yields two
different analyses: s s m and s m m (e.g.
house+keep+er or house+keeper).
5 Experiments & Results
Two sets of experiments were performed for the
evaluation of the model. In the first set of exper-
iments, each word is split at single point giving a
single stem and a single suffix. In the second set
of experiments, potentially multiple split points

	

  
   

 
 
 
  
     





































Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 82?90,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Taxonomy Learning Using Word Sense Induction
Ioannis P. Klapaftis
Department of Computer Science
The University of York
York, UK, YO10 5DD
giannis@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
The University of York
York, UK, YO10 5DD
suresh@cs.york.ac.uk
Abstract
Taxonomies are an important resource for a
variety of Natural Language Processing (NLP)
applications. Despite this, the current state-
of-the-art methods in taxonomy learning have
disregarded word polysemy, in effect, devel-
oping taxonomies that conflate word senses.
In this paper, we present an unsupervised
method that builds a taxonomy of senses
learned automatically from an unlabelled cor-
pus. Our evaluation on two WordNet-derived
taxonomies shows that the learned taxonomies
capture a higher number of correct taxonomic
relations compared to those produced by tradi-
tional distributional similarity approaches that
merge senses by grouping the features of each
word into a single vector.
1 Introduction
A concept or a sense, s, can be defined as the mean-
ing of a word or a multiword expression. A con-
cept s can be linguistically realised by more than one
word while at the same time a wordw can be the lin-
guistic realisation of more than one concept. Given
a set of concepts S, taxonomy learning is the task of
hierarchically classifying the elements in S in an au-
tomatic manner. For example, consider a set of con-
cepts linguistically realised by the words/multiword
expressions LAN, computer network, internet, mesh-
work, gauze, snood. Taxonomy learning methods
produce taxonomies, such as the ones shown in Fig-
ures 1 (a) and 1 (b).
By observing Figure 1 (a), we can express IS-
A statements, such as Internet IS-A Computer Net-
work etc. However, the same does not apply to the
Figure 1: A labelled and an unlabelled concept taxonomy
taxonomy in Figure 1 (b), since this taxonomy is not
fully labelled. Despite this, its hierarchical organ-
isation clearly shows that the concepts are divided
into groups, which are further subdivided into sub-
groups and so forth, until we reach a level where
each concept belongs to its own group. Unlabelled
taxonomies are typically produced by agglomera-
tive hierarchical clustering algorithms (King, 1967;
Sneath and Sokal, 1973).
The knowledge encoded in taxonomies can be
utilised in a range of NLP applications. For in-
stance, taxonomies can be used in information re-
trieval to expand a user query with semantically re-
lated words or to enhance document representation
by abstracting from plain words and adding concep-
tual information (Cimiano, 2006). WordNet?s (Fell-
baum, 1998) taxonomic relations have also been
used in Word Sense Disambiguation (WSD) (Nav-
igli and Velardi, 2004b). In named entity recog-
nition, methods relying on gazetteers could make
82
use of automatically acquired taxonomies (Cimiano,
2006), while question answering systems have also
benefited (Moldovan and Novischi, 2002).
Despite the wide uses of taxonomies, the majority
of methods disregard or do not deal effectively with
word polysemy, in effect, developing taxonomies
that conflate the senses of words (see Section 2).
In this work, we show that Word Sense Induction
(WSI) can be effectively employed to address this
limitation of existing methods.
We present a novel method that employs WSI to
generate the different senses of a set of target words
from an unlabelled corpus and then produces a tax-
onomy of senses using Hierarchical Agglomerative
Clustering (HAC) (King, 1967; Sneath and Sokal,
1973). We evaluate our method on two WordNet-
derived sub-taxonomies and show that our method
leads to the development of concept hierarchies that
capture a higher number of correct taxonomic rela-
tions in comparison to those generated by current
distributional similarity approaches.
2 Related work
Initial research on taxonomy learning focused on
identifying in a given text lexico-syntactic patterns
that suggest hyponymy relations (Hearst, 1992). For
instance, the pattern NP0 such as NP1,. . . ,NPn
suggests that NP0 is a hypernym of NPi. For ex-
ample, given the phrase Fruits, such as oranges, ap-
ples,..., the above pattern would suggest that fruit
is a hypernym of orange and apple. These pattern-
based approaches operate at the word level by learn-
ing lexical relations between words rather than be-
tween senses of words.
In the same spirit, other work attempted to exploit
the regularities of dictionary entries to identify hy-
ponymy relations (Amsler, 1981). For example in
WordNet, WAN is defined as a computer network
that spans . . . . Hence, one can easily induce that
WAN is a hyponym of computer network by assum-
ing that the first noun phrase in the definition is a hy-
pernym of the target word. These approaches learn
lexical relations at the sense level since dictionaries
separate the senses of a word. However this would
be true if and only if the glosses of the dictionaries
were sense-annotated, which is not the case for the
majority of electronic dictionaries (Cimiano, 2006).
Another limitation is that taxonomies are built ac-
cording to the sense distinctions present in dictio-
naries and not according to the actual use of words
in the corpus.
The majority of taxonomy learning approaches
are based on the distributional hypothesis (Harris,
1968). Typically, distributional similarity methods
(Cimiano et al, 2004; Cimiano et al, 2005; Faure
and Ne?dellec, 1998; Reinberger and Spyns, 2004;
Caraballo, 1999) utilise syntactic dependencies such
as subject/verb, object/verb relations, conjunctive
and appositive constructions and others. These de-
pendencies are used to extract the features that serve
as the dimensions of the vector space. Each target
noun is then represented as a vector of extracted fea-
tures where the frequency of co-occurrence of the
target noun with each feature is used to calculate the
weight of that feature. The constructed vectors are
the input to hierarchical clustering or formal concept
analysis (Ganter and Wille, 1999) to produce a tax-
onomy. These approaches assume that a target noun
is monosemous creating one vector of features for
each target noun. This limitation can lead to a num-
ber of problems.
Firstly, the constructed taxonomies might be bi-
ased towards the inclusion of taxonomic relation-
ships between the most frequent senses of tar-
get nouns, ignoring interesting taxonomic relations
where less frequent senses are present. For exam-
ple, consider the word house. Current distributional
similarity methods would possibly capture the hy-
ponyms of its Most Frequent Sense (MFS1), how-
ever ignoring the hyponyms of less frequent senses
of house, e.g. casino, theater, etc. Given that word
senses typically follow a Zipf distribution, these
methods construct vectors dominated by the MFS of
words. This bias significantly degrades the useful-
ness of learned taxonomies.
Secondly, given that distributional similarity ap-
proaches rely on the computation of pairwise simi-
larities between target words, merging their senses
to a single vector might lead to unreliable similarity
estimates. For example, merging the features of the
different senses of house could provide a lower sim-
ilarity with its monosemous hyponym beach house,
since only the first sense of house is related to beach
1WordNet: A dwelling that serves as living quarters . . .
83
house. This problem might lead both to inclusion
of incorrect or loss of correct taxonomic relations.
In our work, we aim to overcome these drawbacks
by identifying the different senses with which target
words appear in text and then building a hierarchy
of the identified senses.
Soft clustering approaches (Reinberger and
Spyns, 2004; Reinberger et al, 2003) have also been
applied to taxonomy learning to deal with polysemy.
These methods associate each verb with a vector of
features, where each feature is a noun appearing as
a subject or object of that verb. That way a noun can
appear in different vectors, hence in different clus-
ters during hierarchical clustering as a result of its
polysemy. However, the underlying assumption is
that a verb is monosemous with respect to its associ-
ated vector of nouns. This assumption is not always
valid and can cause the problems mentioned above.
Other work in taxonomy learning exploits the
head/modifier relationships to create taxonomic re-
lations (Buitelaar et al, 2004; Hwang, 1999;
Sa?nchez and Moreno, 2005). These relations are
used to create: (1) a class (concept) for each head,
and (2) subclasses by adding nominal or adjectival
modifiers. For example, credit card IS-A card. The
corresponding hyponymy relations are learned at the
lexical level disregarding word polysemy. Some of
these approaches identified the problem of polysemy
and applied sense disambiguation with respect to
WordNet in order to capture the different senses of a
target term (Navigli and Velardi, 2004b; Navigli and
Velardi, 2004a). Specifically, the taxonomy built by
exploiting head/modifiers relations was modified ac-
cording to WordNet?s hyponymy relations between
senses of disambiguated terms. One important de-
ficiency of using sense disambiguation is that dic-
tionaries miss many domain-specific senses. Addi-
tionally, the fixed-list of senses paradigm prohibits
learning word senses according to their use in con-
text. The use of sense induction we propose in this
paper aims to overcome these limitations.
3 Method
Given a set of words W , a WSI method is applied
to each wi ? W (Section 3.1). The outcome of the
first stage is a set of senses, S, where each swi ? S
denotes the i-th sense of word w ? W . This set
Figure 2: WSI for network & LAN
of senses is the input to hierarchical clustering that
produces a hierarchy of senses (Section 3.2).
3.1 Word sense induction
WSI is the task of identifying the senses of a tar-
get word in a given text. Recent WSI methods
were evaluated under the framework of SemEval-
2007 WSI task (SWSI) (Agirre and Soroa, 2007).
The evaluation framework defines two types of as-
sessment, i.e. evaluation in: (1) a clustering and
(2) a WSD setting. Based on this evaluation, we se-
lected the method of Klapaftis & Manandhar (2008)
(henceforth referred to as KM) that achieves high F-
score in both evaluation schemes as compared to the
systems participating in SWSI. We briefly describe
KM mentioning its parameters used in our evalua-
tion (Section 4). Figures 2 (a) and 2 (b) describe the
different steps for inducing the senses of the target
words network and LAN.
Corpus preprocessing: The input to KM is a
base corpus bc, in which the target word w appears
in each paragraph. In Figure 2 (a), the base cor-
pus consists of the paragraphs A, B, C and D. The
aim of this stage is to capture nouns contextually
84
related to w. Initially, the target word is removed
from bc, part-of-speech tagging is applied to each
paragraph, only nouns are kept and lemmatised. In
the next step, the distribution of each noun is com-
pared to the distribution of the same noun in a ref-
erence corpus2 using the log-likelihood ratio (G2)
(Dunning, 1993). Nouns with a G2 below a pre-
specified threshold (parameter p1) are removed from
each paragraph. Figure 2 (a) shows the remaining
nouns for each paragraph of bc.
Graph creation & clustering: In the setting of
KM, a collocation is a juxtaposition of two nouns
within the same paragraph. Thus, each noun is com-
bined with any other noun yielding a total of
(N
2
)
collocations for a paragraph with N nouns. Each
collocation, cij , is assigned a weight that measures
the relative frequency of two nouns co-occurring.
This weight is the average of the conditional prob-
abilities p(ni|nj) and p(nj |ni), where p(ni|nj) =
f(cij)
f(nj)
, f(cij) is the number of paragraphs nouns ni,
nj co-occur and f(nj) is the number of paragraphs
in which nj appears. Collocations are filtered with
respect to their frequency (parameter p2) and weight
(parameter p3). Each retained collocation is rep-
resented as a vertex. Edges between vertices are
present, if two collocations co-occur in one or more
paragraphs. Figure 2 (a) shows that this process has
generated 24 collocations for the target word net-
work. On the top right of the figure we also observe
the collocations associated with each paragraph.
In the next step, a smoothing technique is applied
to discover new edges between vertices. The weight
applied to each edge connecting vertices vi and vj
(collocations cab, cde) is the maximum of their con-
ditional probabilities (max(p(cab|cde), p(cde|cab))).
Finally, the graph is clustered using Chinese whis-
pers (Biemann, 2006). The final output is a set of
senses, each one represented by a set of contextually
related collocations. In Figure 2, we generated two
senses for network and one sense for LAN.
3.2 Hierarchical clustering of senses
Given the set of senses S, our task at this point is to
hierarchically classify the senses using HAC. Con-
sider for example the words network and LAN, and
2The British National Corpus, 2001, Distributed by Oxford
University Computing Services.
Senses computer meshwork LAN
network
computer network 1 0.0 0.66
meshwork 0.0 1 0.14
LAN 0.66 0.14 1
Table 1: Similarity matrix for HAC.
Figure 3: WSI & HAC example
let us assume that the WSI process has generated
the senses in Figures 2 (a) and 2 (b). HAC oper-
ates by treating each sense as a singleton cluster and
then successively merging the most similar clusters
according to a pre-defined similarity function. This
process iterates until all clusters have been merged
into a single cluster taken to be the root.
To calculate the pairwise similarities between
senses we exploit the attributes that represent each
sense, i.e. their collocations. Let BC be the cor-
pus resulting from the union of the base corpora of
all words in W . In our example, BC would consist
of the paragraphs, in which the words network and
LAN appear, i.e. A, B, ..., G. An induced sense tags
a paragraph, if one or more of its collocations ap-
pear in that paragraph. Thus, each induced sense is
associated with a set of paragraph labels that denote
the paragraphs tagged by that sense. Figure 3 shows
the paragraph labels tagged by each sense of our ex-
ample. Finally, given two senses sai , s
b
i and their
corresponding sets of tagged paragraphs fai and f
b
i ,
we use the Jaccard coefficient to calculate their sim-
ilarity, i.e. JC(sai , s
b
i) =
|fai ?f
b
i |
|fai ?f
b
i |
, where skj denotes
the j-th sense of word k. The resulting similarity
matrix of our example is shown in Table 1. Given
that matrix, HAC would first group computer net-
work and LAN as they have the highest similarity
(Figure 3). In the final iteration, the remaining two
clusters (Cluster 1 & meshwork) would be grouped
to the root.
An important parameter of HAC is the choice
of the technique for calculating cluster similarities.
Note that as we move towards the higher levels of
85
the taxonomy clusters contain more than one sets of
tagged paragraphs (Figure 3 - Cluster 1), hence the
choice of the similarity function is crucial. We ex-
periment with three techniques, i.e. single-linkage,
complete-linkage and average-linkage. The first one
defines the similarity between two clusters as the
maximum similarity among all the pairs of their cor-
responding feature sets. The second considers the
minimum similarity among all the pairs, while the
third calculates the average similarity of all the pairs.
4 Evaluation
We evaluate our method with respect to two
WordNet-derived sub-taxonomies (Section 4.3). For
that reason, it is necessary to map the induced senses
to WordNet before applying HAC. Note that the
mapping process might map more than one induced
senses to the same WordNet sense. In that case,
these induced senses are merged to a single one
along with their corresponding collocations.
4.1 Mapping WSI clusters to WordNet senses
The process of mapping the induced senses to Word-
Net is straightforward. Let w ? W be a word with
n senses in WordNet. A WordNet sense i of w is de-
noted bywswi , i = [1, n]. Let us also assume that the
WSI method has produced m senses for w, where
each sense j is denoted as swj , j = [1,m]. Each in-
duced sense swj is associated with a set of features
fwj as in the previous section. These features are the
paragraphs (paragraph labels) of BC tagged by swj .
In the next step, each WordNet sense wswi is associ-
ated with its WordNet signature gwi that contains the
following semantic features: hypernyms/hyponyms,
meronyms/holonyms and synonyms of wswi . For
example, the signature of the fifth WordNet sense
of network would contain internet, cyberspace and
other semantically related words. Table 2 shows par-
tial signatures for each sense of network.
The signature gwi is used to formalise the Word-
Net sense wswi as a set of features q
w
i . These fea-
tures are the paragraphs (paragraph labels) of BC
that contain one or more of the aforementioned se-
mantically related to wswi words that exist in g
w
i .
Given an induced sense swj , a similarity score is cal-
culated between swj and each WordNet sense of w.
The maximum score determines the WordNet sense
WordNet sense Semantically related words/phrases
1 reticulum, RF, RAS
2 communication system/equipment
3 gauze, snood, tulle
4 reseau, reticle, reticulation
5 net, internet, cyberspace
Table 2: Semantically related words/phrases to network
label that will be assigned to swj , i.e. label(s
w
j ) =
argmaxi JC(f
w
j , q
w
i ), where JC is the Jaccard sim-
ilarity coefficient. In the example of Figure 2 (a),
the computer network sense would be mapped to the
fifth WordNet sense of network, since there is a sig-
nificant overlap between the paragraphs tagged by
the induced and that WordNet sense.
4.2 Evaluation measures
For the purposes of this section we present one gold
standard taxonomy (Figure 1 (a)) and a second de-
rived from our method (Figure 1 (b)). The compari-
son of these taxonomies is based on the semantic co-
topy of a node, which has also been used in (Maed-
che and Staab, 2002; Cimiano et al, 2005). In par-
ticular, the semantic cotopy of a node is defined as
the set of all its super- and subnodes excluding the
root and including that node. For example, the se-
mantic cotopy of computer network in Figure 1 (a)
is {computer network, internet, LAN}. There are
two issues, which make the evaluation difficult.
The first one is that HAC produces a taxonomy in
which all internal nodes are unlabelled, as opposed
to the gold standard taxonomy. In Figure 1 (b), we
have manually labelled internal nodes with their IDs
for clarity. For example, the semantic cotopy of the
node New Cluster 1 in Figure 1 (b) is {computer net-
work, internet, LAN, New Cluster 1, New Cluster
0}. By comparing the cotopies of nodes computer
network in Figure 1 (a) and New Cluster 1 in Fig-
ure 1 (b), we observe that the automatic method has
successfully grouped all of the hypernyms and hy-
ponyms of computer network under New Cluster 1.
However, the corresponding cotopies are not iden-
tical, because the cotopy of New Cluster 1 also in-
cludes the labels produced by HAC.
To deal with this problem, we use a version of se-
mantic cotopy for nodes in the automatically learned
taxonomy which excludes nodes that do not exist in
WordNet. That way the semantic cotopies of New
Cluster 1 in Figure 1 (b) and computer network in
86
Figure 1 (a) will yield maximum similarity.
The second issue is that the nodes that exist in the
gold standard taxonomy are leaf nodes in the auto-
matically learned taxonomy. As a result, the seman-
tic cotopy of LAN in Figure 1 (b) is {LAN} since
all of its supernodes do not exist in WordNet. In
contrast, the semantic cotopy of LAN in Figure 1
(a) is {LAN, computer network}. We observe that
there is an overlap between the two cotopies derived
by the existence of the same concept in both tax-
onomies, i.e. LAN. In fact, all of the leaf nodes of
a learned taxonomy will have a small overlap with
the corresponding concept in the gold standard. For
this problem, we observe that in our automatically
learned taxonomies it does not make sense to cal-
culate the semantic cotopy of leaf nodes. On the
contrary, we need to evaluate the internal nodes that
group the leaf nodes. Let us assume the following
notation:
TA = automatically learned taxonomy
?i = node in a taxonomy
C(TA) = internal nodes + leaf nodes of TA
I(TA) = internal nodes of TA
TG = gold standard taxonomy
C(TG) = internal nodes + leaf nodes of TG
I(TG) = internal nodes of TG
hyper(?i) = supernodes of ?i excluding the root
hypo(?i) = subnodes of ?i including ?i
For ?i ? I(TA), the semantic cotopy is defined as:
SC ?(?i) = (hyper(?i) ? hypo(?i)) ? C(TG)
For ?i ? C(TG), the semantic cotopy is defined as:
SC ??(?i) = (hyper(?i) ? hypo(?i))
P (?i, ?j) =
|SC ?(?i) ? SC ??(?j)|
|SC ?(?i)|
(1)
R(?i, ?j) =
|SC ?(?i) ? SC ??(?j)|
|SC ??(?j)|
(2)
F (?i, ?j) =
2P (?i, ?j)R(?i, ?j)
P (?i, ?j) +R(?i, ?j)
(3)
Precision, recall and harmonic mean of node ?i ?
I(TA) with respect to node ?j ? C(TG) are de-
fined in Equations 1, 2 and 3. The F-score, FS, of
node ?i ? I(TA) is the maximum F attained at any
?j ? C(TG) (FS(?i) = argmaxj F (?i, ?j)). Fi-
nally, the similarity TS of the entire taxonomy to
the gold standard taxonomy is the average of the
F-scores of each ?i ? I(TA) (Equation 4). The
TS(TA, TG) in Figure 1 is 0.9. All nodes of TA
have a perfect match, apart from New Cluster 0 and
New Cluster 2, which are matched against computer
network and meshwork respectively, having a per-
fect precision but a lower recall since the cotopies
of computer network and meshwork consist of three
concepts. The automatically learned taxonomy has
two redundant clusters that decrease its similarity.
TS(TA, TG) =
1
|I(TA)|
?
?i?I(TA)
FS(?i) (4)
The similarity measure TS(TA, TG) provides the
similarity of the automatically learned taxonomy to
the gold standard one, but it is not symmetric. Cal-
culating the taxonomic similarity one way might not
provide accurate results, in cases where TA misses
senses of the gold standard. This is due to the
fact that we would only evaluate the internal nodes
of TA, partially ignoring the fact that TA might
have missed some parts of the gold standard taxon-
omy. For that reason, we also calculate TS(TG, TA)
which provides the similarity of the gold standard
taxonomy to the automatically learned one. Fi-
nally, taxonomic similarities are combined to pro-
duce their harmonic mean (Equation 5).
TxSm(TA, TG) =
2TS(TG, TA)TS(TA, TG)
TS(TG, TA) + TS(TA, TG)
(5)
4.3 Evaluation datasets & setting
The first gold standard taxonomy is derived by ex-
tracting from WordNet al the hyponyms of the
senses of the word network. The extracted taxonomy
contains 29 senses linguistically realized by 24 word
sets (one sense might be expressed with more than
one words), since network has 5 senses and reseau
has 2 senses in the gold standard taxonomy. Note
that we have disregarded senses only expressed by
multiword expressions. The average polysemy of
words is around 1.7. The second taxonomy is de-
rived by extracting the concepts under the senses of
the word speaker. The speaker taxonomy contains
52 senses linguistically realized by 50 word sets,
since speaker has 3 senses included in the taxonomy.
The average polysemy of words is around 1.58.
To create our datasets3 we use the Yahoo! search
api4. For each word w in each of the datasets, we is-
3Available in http://www.cs.york.ac.uk/aig/projects/indect/taxlearn
4http://developer.yahoo.com/search/ [Accessed:10/06/2009]
87
Parameter Range
G2 threshold (p1) 5,10
Collocation frequency (p2) 4,6,8
Collocation weight (p3) 0.1,0.2,0.3,0.4
Table 3: Chosen parameters for the KM WSI method.
sue a query to Yahoo! that contains w and we down-
load a maximum of 1000 pages. In cases where
a particular sense is expressed by more than one
word, the query was formulated by including all the
words and putting the keyword OR between them.
For each page we extracted fragments of text (para-
graphs) that occur in <p> </p> html tags. We ex-
tracted 58956 and 78691 paragraphs for the network
and speaker dataset respectively. The reason we ex-
tracted on average less content for the second dataset
was that Yahoo! provided a small number of results
for rare words such as alliterator, anecdotist, etc.
Table 3 shows the parameter ranges for the WSI
method. Our method is evaluated according to these
parameters. Our first baseline is RAND, which per-
forms a random hierarchical clustering of senses to
produce a binary tree. In each iteration two clusters
are randomly chosen and form a new cluster, until
we end up with one cluster taken to be the root. The
performance of RAND is calculated by executing the
random algorithm 10 times and then averaging the
results. The second baseline is the taxonomy most
frequent sense baseline (TL MFS), in which we do
not perform WSI. Instead, given a parameter setting
and a word w, all the collocations of w are grouped
into one vector, which will possibly be dominated
by collocations related to the MFS of w. WordNet
mapping takes place and finally HAC with average-
linkage is applied to create the taxonomy.
4.4 Results & discussion
Figures 4 (a) and 4 (b) show the performance
of HAC with single-linkage (HAC SNG), average-
linkage (HAC AVG) and complete-linkage (HAC
CMP) against RAND for p1 = 5 and different com-
binations of p2 and p3. It is clear that HAC SNG and
HAC AVG outperform RAND by very large margins
under all parameter combinations. In the network
dataset, both of them achieve their highest distance
from RAND (27.84%) at p2 = 8 and p3 = 0.2. In the
speaker dataset, their highest distance from RAND
(20.97% and 19.63% respectively) is achieved at
p2 = 4 and p3 = 0.1. HAC CMP performs worse
than the other HAC versions, yet it clearly outper-
forms RAND in all but one parameter combinations
(p1 = 5, p2 = 6, p3 = 0.4) in the speaker dataset.
Generally, for collocation weight equal to 0.4 the
performance of all HAC versions drops. At this
high collocation weight the WSI method produces a
larger number of small clusters than in lower thresh-
olds. This issue negatively affects both the map-
ping process and HAC. For example in the speaker
dataset, for p1 = 5, p2 = 8 and p3 = 0.1 our tax-
onomies contained 86.54% of the gold standard tax-
onomy senses. Increasing the collocation weight to
0.2 did not have any effect, but increasing the weight
to 0.3 and then 0.4 led to 71.15% and 65.38% sense
coverage. Overall, our conclusion is that all HAC
versions exploit the WSI method and learn useful
information better than chance. The picture is the
same for p1 = 10.
Figures 4 (c) and 4 (d) show the performance of
HAC versions against the TL MFS baseline in the
same parameter setting as above. We observe that
both HAC SNG and HAC AVG perform significantly
better than TL MFS apart from p3 = 0.4, in which
case all HAC versions perform worse. In the network
dataset, the largest performance difference for HAC
SNG is 10.12% and for HAC AVG 9.9% at p2 = 6
and p3 = 0.2. In the speaker dataset, the largest per-
formance difference for HAC SNG is 10.83% and
for HAC AVG 7.83% at p2 = 8 and p3 = 0.2. HAC
CMP performs worse than TL MFS under most pa-
rameter settings in both datasets. The picture is the
same for p1 = 10.
Overall, the analysis of the WSI-based taxonomy
learning approach against TL MFS shows that HAC
SNG and HAC AVG perform better than TL MFS
under all parameter combinations for both datasets.
The main reason for their superior performance is
that their learned taxonomies contain a higher num-
ber of senses than TL MFS as a result of the sense
induction process. This greater sense coverage leads
to the discovery of a higher number of correct taxo-
nomic relations between senses than TL MFS, hence
in a better performance. To conclude, our results
verify our hypothesis and suggest that the unsuper-
vised learning of word senses contributes to produc-
ing taxonomies with a higher similarity to the gold
standard ones than traditional distributional similar-
ity methods.
88
Figure 4: Performance analysis of the proposed method for p1 = 5 and different combinations of p2 and p3.
Despite that, our evaluation also shows that in
most cases HAC CMP is unable to exploit the in-
duced senses and performs worse than TL MFS,
HAC SNG and HAC AVG. This result was not ex-
pected, since HAC SNG employs a local criterion to
merge two clusters and does not consider the global
structure of the clusters, in effect, being biased to-
wards elongated clusters. The observation of the
gold standard taxonomies shows that they consist
both of cohyponym concepts which are expected
to be contextually related, but also of cohyponyms
which are not expected to appear in similar contexts.
For example, someone would expect a high similar-
ity between WAN, LAN, or between snood and tulle.
However, the same does not apply for snood and
cheesecloth or tulle and grillwork, because cheese-
cloth and grillwork appear in significantly different
contexts than snood and tulle. Despite that, all of
them are cohyponyms. This issue is more prevalent
in the speaker dataset, where concepts such as loud-
speaker, tannoy, woofer are expected to be contex-
tually related, while cohyponyms such as whisperer,
lecturer and interviewer are not. This means that the
gold standard taxonomies include elongated clusters
and explains the superior performance of HAC SNG.
This issue is not affecting HAC AVG, but it has a sig-
nificant effect on HAC CMP. Generally, HAC CMP
employs a non-local criterion by considering the di-
ameter of a candidate cluster. This results in com-
pact clusters with small diameters, as opposed to
elongated ones.
5 Conclusion
We presented an unsupervised method for taxonomy
learning that employs WSI to identify the senses of
target words and then builds a taxonomy of these
senses using HAC. We have shown that dealing with
polysemy by means of sense induction helps to de-
velop taxonomies that capture a higher number of
correct taxonomic relations than traditional distribu-
tional similarity methods, which associate each tar-
get word with one vector of features, in effect, merg-
ing its senses.
Acknowledgements
This work is supported by the European Commis-
sion via the EU FP7 INDECT project, Grant No.
218086, Research area: SEC-2007-1.2-01 Intelli-
gent Urban Environment Observation System.
89
References
E. Agirre and A. Soroa. 2007. SemEval-2007 Task
02: Evaluating Word Sense Induction and Discrimi-
nation Systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations, pages 7?12,
Prague, Czech Republic.
R. A. Amsler. 1981. A Taxonomy for English Nouns and
Verbs. In Proceedings of the 19th ACL Conference,
pages 133?138, Stanford, California.
C. Biemann. 2006. Chinese Whispers - An Efficient
Graph Clustering Algorithm and its Application to
Natural Language Processing Problems. In Proceed-
ings of TextGraphs, pages 73?80, New York,USA.
P. Buitelaar, D. Olejnik, and M. Sintek. 2004. A Ptote?ge?
Plug-in for Ontology Extraction from Text Based on
Linguistic Analysis. In Proceedings of the 1st Euro-
pean Semantic Web Symposium, pages 31?44, Crete,
Greece. CEUR-WS.org.
S. A. Caraballo. 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. In Pro-
ceedings of the 37th ACL Conference, pages 120?126,
College Park, Maryland.
P. Cimiano, A. Hotho, and S. Staab. 2004. Compar-
ing Conceptual, Divisive and Agglomerative Cluster-
ing for Learning Taxonomies from Text. In Proceed-
ings of the 16th ECAI Conference, pages 435?439, Va-
lencia, Spain.
P. Cimiano, A. Hotho, and S. Staab. 2005. Learning
Concept Hieararchies from Text Corpora Using For-
mal Concept Analysis. Journal of Artificial Intelli-
gence Research, 24:305?339.
P. Cimiano. 2006. Ontology Learning and Population
from Text: Algorithms, Evaluation and Applications.
Springer-Verlag New York, Inc., Secaucus, NJ, USA.
T. Dunning. 1993. Accurate Methods for the Statistics of
Surprise and Coincidence. Computational Linguistics,
19(1):61?74.
D. Faure and C. Ne?dellec. 1998. A Corpus-based Con-
ceptual Clustering Method for Verb Frames and On-
tology Acquisition. In LREC workshop on Adapting
lexical and corpus resources to sublanguages and ap-
plications, pages 5?12, Granada, Spain.
C. Fellbaum. 1998. Wordnet: An Electronic Lexical
Database. MIT Press, Cambridge, Massachusetts,
USA.
B. Ganter and R. Wille. 1999. Formal Concept Anal-
ysis: Mathematical Foundations. Springer-Verlag
New York, Inc., Secaucus, NJ, USA. Translator-C.
Franzke.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley, New York, USA.
M. A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
the 14th Coling Conference, pages 539?545, Nantes,
France.
C. H. Hwang. 1999. Incompletely and Imprecisely
Speaking: Using Dynamic Ontologies for Represent-
ing and Retrieving Information. In Proceedings of
the 6th International Workshop on Knowledge Repre-
sentation Meets Databases, pages 14?20, Linkoping,
Sweden. CEUR-WS.org.
B. King. 1967. Step-wise Clustering Procedures. Jour-
nal of the American Statistical Association, 69:86?
101.
I. P. Klapaftis and S. Manandhar. 2008. Word Sense In-
duction Using Graphs of Collocations. In Proceedings
of the 18th ECAI Conference, pages 298?302, Patras,
Greece. IOS Press.
A. Maedche and S. Staab. 2002. Measuring Similarity
between Ontologies. In Proceedings of the European
Conference on Knowledge Acquisition and Manage-
ment (EKAW), pages 251?263, London,UK. Springer-
Verlag.
D. Moldovan and A. Novischi. 2002. Lexical Chains
for Question Answering. In Proceedings of the 19th
Coling Conference, pages 1?7, Taipei, Taiwan.
R. Navigli and P. Velardi. 2004a. Learning Domain On-
tologies from Document Warehouses and Dedicated
web Sites. Computational Linguistics, 30(2):151?179.
R. Navigli and P. Velardi. 2004b. Structural Semantic In-
terconnection: a Knowledge-based Approach to Word
Sense Disambiguation. In Proceedings of Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 179?
182, Barcelona, Spain.
M.L. Reinberger and P. Spyns. 2004. Discovering
Knowledge in Texts for the Learning of Dogma-
inspired Ontologies. In Proceedings of the ECAI
Workshop on Ontology Learning and Population,
pages 19?24, Valencia, Spain.
M. L. Reinberger, P. Spyns, W. Daelemans, and R. Meers-
man. 2003. Mining for Lexons: Applying Unsuper-
vised Learning Methods to create ontology bases. In
CoopIS/DOA/ODBASE, pages 803?819.
D. Sa?nchez and A. Moreno. 2005. Web-scale Taxon-
omy Learning. In Proceedings of the Workshop on
Learning and Extending Ontologies by using Machine
Learning methods, pages 53?60, Bonn, Germany.
P. H. A. Sneath and R. R. Sokal. 1973. Numerical Taxon-
omy, The Principles and Practice of Numerical Clas-
sification. W. H. Freeman, San Francisco, USA.
90
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 636?644,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Can Recognising Multiword Expressions Improve Shallow Parsing?
Ioannis Korkontzelos, Suresh Manandhar
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
{johnkork, suresh}@cs.york.ac.uk
Abstract
There is significant evidence in the literature
that integrating knowledge about multiword
expressions can improve shallow parsing ac-
curacy. We present an experimental study to
quantify this improvement, focusing on com-
pound nominals, proper names and adjective-
noun constructions. The evaluation set of
multiword expressions is derived from Word-
Net and the textual data are downloaded from
the web. We use a classification method to
aid human annotation of output parses. This
method allows us to conduct experiments on
a large dataset of unannotated data. Experi-
ments show that knowledge about multiword
expressions leads to an increase of between
7.5% and 9.5% in accuracy of shallow pars-
ing in sentences containing these multiword
expressions.
1 Introduction
Multiword expressions are sequences of words that
tend to co-occur more frequently than chance and
are characterised by various levels of idiosyncracy
(Baldwin et al, 2003; Baldwin, 2006). There is ex-
tended literature on various issues relevant to mul-
tiword expression; recognition, classification, lexi-
cography, etc. (see Section 6). The vast majority of
these publications identifies as motivation for mul-
tiword expression research its potential contribution
to deep or shallow parsing. On the other side of this
issue, the state-of-the-art parsing systems seem to
ignore the fact that treating multiword expressions
as syntactic units would potentially increase parser?s
accuracy.
In this paper, we present an experimental study
attempting to estimate the contribution of integrat-
ing multiword expressions into shallow parsing. We
focus on multiword expressions that consist of two
successive tokens; in particular, compound nominals
proper names and adjective-noun constructions. We
also present a detailed classification method to aid
human annotation during the procedure of deciding
if a parse is correct or wrong. We present experi-
mental results about the different classes of changes
that occur in the parser output while unifying multi-
word expression components.
We conclude that treating known multiwords ex-
pressions as singletons leads to an increase of be-
tween 7.5% and 9.5% in accuracy of shallow pars-
ing of sentences containing these multiword expres-
sions. Increase percentages are higher for multiword
expressions that consist of an adjective followed by
a noun (12% to 15%); and even higher for non-
compositional multiword expressions1 that consist
of an adjective and a noun (15.5% to 19.5%).
The rest of the paper is structured as follows: In
Section 2 we present how multiword expressions can
be annotated in text and used by a shallow parser. In
Section 3 we present an overview of our experimen-
tal process. Section 4 explains how the set of target
multiword expressions and textual corpora were cre-
ated. In Section 5 we present and discuss the results
of the experimental process. In Section 6 we present
parts of the related literature. Section 7 concludes
the paper and proposes some future work.
1Compositionality is defined as the degree to which the
meaning of a multiword expression can be predicted by com-
bining the meanings of its components (Nunberg et al, 1994).
636
2 Annotating Multiword expressions
In this paper, we present a study to inspect the ex-
tent to which knowledge of multiword expressions
improves shallow parsing. Our approach focuses
on English multiword expressions that appear as se-
quences in text. In particular, we focus on com-
pound nominals (e.g. lemon tree), proper names
(e.g. prince Albert) and adjective-noun construc-
tions (e.g. red carpet).
Shallow or deep parsing should treat multiword
expression as units that cannot be divided in any
way. We replace the multiword expression tokens
with a special made up token, i.e. the multiword ex-
pression constituents joined with an underscore. For
example, we replace all occurrences of ?lemon tree?
with ?lemon tree?.
We choose to replace the multiword expression
words with a token that does not exist in the dictio-
nary of the part of speech tagger. This is quite an
important decision. Usually, a part of sheech tagger
assigns to an unknown words the part of speech that
best fits to it with respect to the parts of speech of
the words around it and the training data. This is a
desirable behaviour for our purposes.
The experimental results of our study quantify the
difference between the shallow parser output of a big
number of sentences after the replacement and the
shallow parser output of the same sentences before
the replacement. The comparison is done ignoring
changes of parts of speech, assigned by the part of
speech tagger.
3 Evaluation
The target of our experiment is to evaluate whether
replacing the multiword expression tokens with a
single token, unknown to the part of speech tagger,
improves shallow parsing accuracy. The ideal way
to perform this evaluation would be to use a cor-
pus with manual annotation about parsing and mul-
tiword expressions. Given this corpus we would be
able to measure the accuracy of a shallow (or deep)
parser before and after replacing multiword expres-
sions. However, to the best of our knowledge there
is no corpus available to include this type of annota-
tions in English.
Instead, there are two options: Firstly, we can
use treebank data, where manual parsing annotation
Figure 1: Evaluation process
is readily available, and manually annotate multi-
word expressions. The advantage of this approach
is that results are directly comparable with other re-
sults of the literature, due to the use of benchmark
data. Manual annotation of multiword expressions
is a very time- and effort-consuming process due to
the large size of most treebanks. Alternatively, mul-
tiword expression annotation could be done using a
method of recognition. Annotating the multiword
expressions that appear in WordNet could be a safe
decision, in terms of correctness, however, WordNet
is reported to have limited coverage of multiword
expressions (Baldwin, 2006; Laporte and Voyatzi,
2008). WordNet covers only 9.1 % and 16.1 % of the
datasets of Nicholson and Baldwin (2008) (484 noun
compounds) and Kim and Baldwin (2008) (2169
noun compounds), respectively.
Secondly, we can use a set of multiword expres-
sions as a starting point and then create corpora that
contain instances of these multiword expressions. In
succession, these sentences need to be manually an-
notated in terms of parsing, and this requires huge
human effort. Alternatively, we can parse the cor-
pora before and after replacing the multiword ex-
pression and then compare the parser output. This
is the evaluation procedure that we chose to follow,
and is shown in Figure 1.
The above procedure is only able to retrieve in-
stances where the replacement of the multiword ex-
pression leads to a different parsing, a different allo-
cation of tokens to phrases. It is not able to spot in-
stances where the parser output remains unchanged
after the replacement, no matter if they are correct.
Since we are interested in measuring if replacing
637
Example A - Replacement causes no change
Before: [NP they] [VP jumped] [PP over] [NP a
bonfire] and [VP rolled] [NP a fire wheel] .
After: [NP they] [VP jumped] [PP over] [NP a
bonfire] and [VP rolled] [NP a fire wheel] .
Example B - Replacement corrects an error
Before: [NP the blades] [VP ignited] and [NP he]
[VP threw] [NP the fire] wheel up
[PP into] [NP the air] .
After: [NP the blades] [VP ignited] and [NP he]
[VP threw] [NP the fire wheel] [PRT up]
[PP into] [NP the air] .
Table 1: 2 shallow parsing examples. Multiword expres-
sion: ?fire wheel?
multiword expressions with a single token improves
parsing accuracy, we are not interested in instances
that remain unchanged. We focus on instances that
changed; either they were corrected or they were
made wrong or they remain erroneous. For example,
the shallow parser output for example A in Table 1
did not change after the replacement. Example B in
Table 1 shows a sentence which was corrected after
the replacement.
Instead of manually annotating the sentences
whose parser output changed after the replacement
as corrected or not, we identify a number of change
classes under which we classify all these sentences.
In the following section, we present the change
classes. For each we thoroughly discuss whether
its form guarantees that its sentences are wrongly
parsed before the change and correctly parsed after
the change. In this case, the sentences of the corre-
sponding class should be counted as false positives.
We also discuss the opposite; if the form of each
change class guarantees that its sentences are cor-
rectly parsed before the change and wrongly parsed
after the change. In this case, the sentences of the
corresponding class should be counted as true nega-
tives. For this discussion we hypothesize that among
the possible output shallow parses for a given sen-
tence the correct one has (a) the smallest number
phrases, and (b) the smallest number of tokens not
assigned to any phrase.
3.1 Shallow parsing change classes
In this section, we present a classification of cases
where the shallow parser output of the sentence is
Figure 2: Change classes (following the notation of Bille
(2005)). Triangles denote phrases and uppercase bold let-
ters V...Z denote phrase labels. Lowercase letters k...n
denote parsing leaves. For change classes P2LMw and
L2PMw, X includes the multiword expression tokens.
For change classes P2L and L2P it does not. For change
class MwA, the multiword expression tokens are not as-
signed to the same phrase Y or Z.
different from the parser output of the same sen-
tence after replacing the multiword expression with
a single token. The secondary focus of this discus-
sion is to estimate whether the specific form of each
change class can lead to a safe conclusion about if
the parser output of the sentence under discussion:
(a) was wrong before the replacement and was then
corrected, (b) was correct before the replacement
and was then made wrong, or (c) was wrong before
the replacement and remained wrong. For this dis-
cussion, we refer to words that are not assigned to
any phase in the shallow parser output as ?leaves?.
Hypothesis: We base our analysis on the hypoth-
esis that among the possible output shallow parses
for a given sentence the correct one has (a) the small-
est number phrases, and (b) the smallest number of
leaves. The theoretical intuitions behind the hypoth-
esis are: (a) parse trees with just leaves are par-
tial parse trees and hence should not be preferred
over complete parse trees. (b) when mistaken parse
638
trees are generally larger (with more phrases). We
checked the hypothesis by manually annotating 80
randomly chosen instances; 10 for each change class
that is counted as correct or wrong (see Table 2). 74
instances validated the hypothesis (92.5%).
Table 2 shows one example for each change class.
Figure 2 presents the classes as transformations be-
tween trees, following the notation of Bille (2005).
Change class P2LMw (Phrase to Leaves includ-
ing the Multiword expression) Before replacing the
multiword expression sequence with a single to-
ken, the multiword expression is assigned to some
phrase, possibly together with other words. After
the replacement, the components of that phrase are
not assigned to any phrase, but instead as leaves.
Change class P2L (Phrase to Leaves excluding
the multiword expression) Similarly to change class
P2LMw, before the replacement, some successive
tokens excluding the multiword expression itself are
assigned to some phrase. After the replacement, the
components of that phrase appear as leaves.
Change class L2PMw (Leaves to Phrase includ-
ing the Multiword expression) The changes covered
by this class are the opposite changes of change class
P2LMw. Before the replacing the multiword expres-
sion sequence with a single token, the multiword ex-
pression sequence is not assign to any phrase possi-
bly among other words. After the replacement, the
multiword expression is assigned to a phrase.
Change class L2P (Leaves to Phrase excluding
the multiword expression) Similarly to change class
L2PMw, before the replacement, one or more suc-
cessive tokens excluding the multiword expression
itself appear as leaves. After the replacement, these
tokens are assigned to a phrase.
Change class PL2P (Phrases or Leaves to Phrase)
After the replacement, the tokens of more than one
phrases or leaves are assigned to a single phrase.
Change class P2PL (Phrase to Phrases or Leaves)
In contrast to change class PL2P, after the replace-
ment, the tokens of one phrase either are assigned to
more than one phrases or appear as leaves.
Change class PN (Phrase label Name) After re-
placing the multiword expression sequence with a
single token, one phrase appears with a different
phrase label, although it retains exactly the same
component tokens.
Change class PoS (Part of Speech) After replac-
ing the multiword expression sequence with a single
token, one or more tokens appears with a different
part of speech. This class of changes comes from the
part of speech tagger, and are out of the scope of this
study. Thus, in the results section we show a size es-
timate of this class, and then we present results about
change classes, ignoring change class PoS.
Change class P2P (Phrases to less Phrases) After
replacing the multiword expression sequence with a
single token, the component tokens of more than one
successive phrases? are assigned to a different set of
successive phrases ?. However, it is always the case
that phrases ? are less than phrases ? (|?| < |?|).
Change class MwA (Multiword expression
Allocation) Before replacing the multiword ex-
pression sequence, the multiword expression
constituents are assigned to different phrases.
The instances of change classes where the parser
output after the replacement has more parsing leaves
or phrases than before are counted towards sen-
tences that were parsed wrongly after the replace-
ment. For these classes, change classes P2LMw,
P2L and P2PL, most probably the parser output after
the replacement is wrong.
In contrast, the instances of change classes where
a sequence of tokens is assigned to a phrase, or many
phrases are merged are counted towards sentences
that were parsed wrongly before the replacement
and correctly after the replacement. These changes,
that are described by classes L2PMw, L2P, PL2P
and P2P, most probably describe improvements in
shallow parsing. The instances of change class MwA
are counted as correct after the replacement because
by definition all tokens of a multiword expression
are expected to be assigned to the same phrase.
The instances of change class PN can be either
correct or wrong after the replacement. For this rea-
son, we present our results as ranges (see Table 4).
The minimum value is computed when the instances
of class PN are counted as wrong after the replace-
ment. In contrast, the maximum value is computed
when the instances of this class are counted as cor-
rect after the replacement.
3.2 Shallow parsing complex change classes
During the inspection of instances where the shal-
low parser output before the replacement is dif-
639
P
2L
M
w B [NP the(DT) action(NN) officer(NN)] [NP logistic(JJ) course(NN)] [VP is(VBZ) designed(VBN) ] 7
[VP to(TO) educate(VB) ] and(CC) [VP train(VB)] [NP military(JJ) personnel(NNS)] ...
A the(DT) action officer(NN) [NP logistic(JJ) course(NN)] [VP is(VBZ) designed(VBN)]
[VP to(TO) educate(VB) ] and(CC) [VP train(VB)] [NP military(JJ) personnel(NNS)] ...
P
2L B ... [NP the(DT) action(NN) officer(NN)] [PP in(IN)] [NP armenia(NN)] [VP signed(VBN)] ... 7
A ... [NP the(DT) action officer(NN)] in(IN) [NP armenia(NN) ] [VP signed(VBN) ] ...
L
2P
M
w B ?(?) affirmative(JJ) action(NN) officer(NN) ?(?) [NP aao(NN)] [VP refers(VBZ)] [PP to(TO)]
X
[NP the(DT) regional(JJ) affirmative(JJ) action(NN) officer(NN)] or(CC) [NP director(NN)] ...
A ?(?) [NP affirmative(JJ) action officer(NN)] ?(?) [NP aao(NN)] [VP refers(VBZ)] [PP to(TO)]
[NP the(DT) regional(JJ) affirmative(JJ) action officer(NN)] or(CC) [NP director(NN)] ...
L
2P B [NP the(DT) action(NN) officer(NN) ] usually(RB) [VP delivers(VBZ)] ... X
A [NP the(DT) action officer(NN) ] [ADVP usually(RB)] [VP delivers(VBZ)] ...
P
L
2P
B ... [VP to(TO) immediately(RB) report(VB)] [NP the(DT) incident(NN)] [PP to(TO)] [NP the(DT)
X
equal(JJ) opportunity(NN)] and(CC) [NP affirmative(JJ) action(NN) officer(NN)] .(.)
A ... [VP to(TO) immediately(RB) report(VB)] [NP the(DT) incident(NN)] [PP to(TO)] [NP the(DT)
equal(JJ) opportunity(NN) and(CC) affirmative(JJ) action officer(NN)] .(.)
P
2P
L B ... [NP action(NN) officer(NN)] [VP shall(MD) prepare(VB) and(CC) transmit(VB)] ...
7
A ... [NP action officer(NN)] [VP shall(MD) prepare(VB)] and(CC) [VP transmit(VB)] ...
P
N B ... [NP an(DT) action(NN) officer(NN)] [SBAR for(IN)] [NP communications(NNS)] ... ?
A ... [NP an(DT) action officer(NN)] [PP for(IN)] [NP communications(NNS)] ...
Po
S B ... [NP security(NN) officer(NN)] or(CC) ?(?) [NP youth(JJ) action(NN) officer(NN) ] .(.) ?(?)
?
A ... [NP security(NN) officer(NN)] or(CC) ?(?) [NP youth(NN) action officer(NN)] .(.) ?(?)
P
2P
B ... ,(,) [PP as(IN)] [NP a(DT) past(JJ) action(NN) officer(NN)] and(CC) command(NN) and(CC)
X
control(NN) and(CC) [NP intelligence(NN) communications(NNS) inspector(NN)] ...
A ... ,(,) [PP as(IN)] [NP a(DT) past(JJ) action officer(NN) and(CC) command(NN) and(CC)
(control(NN) ] and(CC) [NP intelligence(NN) communications(NNS) inspector(NN)] ...
M
w
A B the(DT) campus(NN) affirmative(JJ) action(NN) [NP officer(NN)] [VP serves(VBZ)] ...
X
A [NP the(DT) campus(NN) affirmative(JJ) action officer(NN)] [VP serves(VBZ)]...
Table 2: Examples for change classes. Multiword expression: ?action officer?. Parts of speech appear within paren-
theses. ?B? stands for ?before? and ?A? for ?after? (multiword expression replacement). Xor 7 denote change classes
that count positively or negatively towards improving shallow parsing. ? denotes classes that are treated specially.
ferent from the shallow parser output after the re-
placement, we came across a number of instances
that were classified in more than one class of the
previous subsection. In other words, two or more
classes of change happened. For example, in a num-
ber of instances, before the replacement, the multi-
word expression constituents are assigned to differ-
ent phrases (change class MwA). After the replace-
ment, the tokens of more than one phrases are as-
signed to a single phrase (change class PL2P). These
instances consist new complex change classes and
are named as the sum of names of the participating
classes. The instances of the example above consist
the complex change class PL2P+MwA.
4 Target multiword expressions and
corpora collection
We created our set of target multiword expres-
sions using WordNet 3.0 (Miller, 1995). Out of its
52, 217 multiword expressions we randomly chose
120. Keeping the ones that consist of two tokens
resulted in the 118 expressions of Table 3. Manu-
ally inspecting these multiword expressions proved
that they are all compound nominals, proper names
or adjective-noun constructions. Each multiword
expression was manually tagged as compositional
or non-compositional, following the procedure de-
scribed in Korkontzelos and Manandhar (2009). Ta-
ble 3 shows the chosen multiword expressions to-
gether with information about their compositionality
and the parts of speech of their components.
640
Compositional Multiword expressions (Noun - Noun sequences)
action officer (3119) bile duct (21649) cartridge brass (479) field mushroom (789) fire wheel (480)
key word (3131) king snake (2002) labor camp (3275) life form (5301) oyster bed (1728)
pack rat (3443) palm reading (4428) paper chase (1115) paper gold (1297) paper tiger (1694)
picture palace (2231) pill pusher (924) pine knot (1026) potato bean (265) powder monkey (1438)
prison guard (4801) rat race (2556) road agent (1281) sea lion (9113) spin doctor (1267)
tea table (62) telephone service (9771) upland cotton (3235) vegetable sponge (806) winter sweet (460)
Non-Compositional Multiword expressions (Noun - Noun sequences)
agony aunt (751) air conditioner (24202) band aid (773) beach towel (1937) car battery (3726)
checker board (1280) corn whiskey (1862) corner kick (2882) cream sauce (1569) fire brigade (5005)
fish finger (1423) flight simulator (5955) honey cake (843) jazz band (6845) jet plane (1466)
laser beam (16716) lemon tree (3805) lip service (3388) love letter (3265) luggage van (964)
memory device (4230) monkey puzzle (1780) motor pool (3184) power cord (5553) prince Albert (2019)
sausage pizza (598) savoy cabbage (1320) surface fire (2607) torrey tree (10) touch screen (9654)
water snake (2649) water tank (5158) wood aster (456)
Compositional Multiword expressions (Adjective - Noun sequences)
basic color (2453) cardiac muscle (6472) closed chain (1422) common iguana (668) cubic meter (4746)
eastern pipistrel (128) graphic designer (8228) hard candy (2357) ill health (2055) kinetic theory (2934)
male parent (1729) medical report (3178) musical harmony (1109) mythical monster (770) red fox (10587)
relational adjective (279) parking brake (7199) petit juror (991) taxonomic category (1277) thick skin (1338)
toxic waste (7220) universal donor (1454) parenthesis-free notation (113)
Non-Compositional Multiword expressions (Adjective - Noun sequences)
black maria (930) dead end (5256) dutch oven (4582) golden trumpet (607) green light (5960)
high jump (4455) holding pattern (3622) joint chiefs (2865) living rock (985) magnetic head (2457)
missing link (5314) personal equation (873) personal magnetism (2869) petit four (1506) pink lady (1707)
pink shower (351) poor devil (1594) public eye (3231) quick time (2323) red devil (2043)
red dwarf (6526) red tape (2024) round window (1380) silent butler (332) small beer (2302)
small voice (4313) stocking stuffer (7486) sweet bay (1367) teddy boy (2413) think tank (4586)
Table 3: 118 multiword expressions randomly chosen from WordNet. The size of the respective corpus in sentences
appears within parentheses.
For each multiword expression we created a dif-
ferent corpus. Each consists of webtext snippets of
length 15 to 200 tokens in which the multiword ex-
pression appears. Snippets were collected follow-
ing Korkontzelos and Manandhar (2009). Given a
multiword expression, a set of queries is created:
All synonyms of the multiword expression extracted
from WordNet are collected2. The multiword ex-
pression is paired with each synonym to create a set
of queries. For each query, snippets are collected
by parsing the web-pages returned by Yahoo!. The
union of all snippets produces the multiword expres-
sion corpus.
In Table 3, the number of collected corpus sen-
tences for each multiword expression are shown
within parentheses. GENIA tagger (Tsuruoka et al,
2005) was used as part of speech tagger. SNoW-
based Shallow Parser (Munoz et al, 1999) was used
for shallow parsing.
2e.g. for ?red carpet?, corpora are collected for ?red carpet?
and ?carpet?. The synonyms of ?red carpet? are ?rug?, ?carpet?
and ?carpeting?.
5 Experimental results and discussion
The corpora collecting procedure of Section 4 re-
sulted in a corpus of 376, 007 sentences, each one
containing one or more multiword expressions. In
85, 527 sentences (22.75%), the shallow parser out-
put before the replacement is different than the shal-
low parser output after the replacement. 7.20% of
these change instances are due to one or more parts
of speech changes, and are classified to change class
PoS. In other words, in 7.20% of cases where there
is a difference between the shallow parses before
and after replacing the multiword expression tokens
there is one or more tokens that were assigned a dif-
ferent part of speech. However, excluding parts of
speech from the comparison, there is no other dif-
ference between the two parses.
The focus of this study is to quantify the effect
of unifying multiword expressions in shallow pars-
ing. Part of speech tagging is a component of our ap-
proach and parts of speech are not necessarily parts
of the parser output. For this reason, we chose to
ignore part of speech changes, the changes of class
PoS. Below, we discuss results for all other classes.
641
Multiword Shallow Parsing
expressions improvement
class PS sentences min. max.
On average - 376,007 7.47% 9.49%
Comp. N N 93,166 5.54% 7.19%
Non-Comp. N N 127,875 3.66% 4.44%
Comp. J N 68,707 7.34% 9.21%
Non-Comp. J N 86,259 15.32% 19.67%
- N N 221,041 4.45% 5.60%
J N 154,966 11.78% 15.03%
Comp. - 161,873 6.30% 8.05%
Non-Comp. - 214,134 8.36% 10.57%
Table 4: Summary of results. PS: parts of speech, Comp:
compositional, N: noun, J: adjective, min.: minimum,
max.: maximum.
Table 4 shows a summary of our results. The first
two columns describe classes of multiword expres-
sion with respect to compositionality and the parts
of speech of the component words. The first line ac-
counts for the average of all multiword expressions,
the second one for compositional multiword expres-
sions made of nouns, etc. The third column shows
the number of corpus sentences of each class.
For each one of the classes of Table 4, the fourth
and fifth columns show the minimum and maxi-
mum improvement in shallow parsing, respectively,
caused by unifying multiword expression tokens.
Let ?X? be the function that returns the number of
instances assigned to change class X . With respect
to the discussion of Subsection 3.1 about how the in-
stances of each class should be counted towards the
final results, the minimum and maximum improve-
ments in shallow parsing are:
min = ??P2LMw???P2L?+?L2PMw?+?L2P?+
+?PL2P???P2PL?+?PL2P+MwA?+
+?P2P?+?P2P+MwA???PN? (1)
max = ??P2LMw???P2L?+?L2PMw?+?L2P?+
+?PL2P???P2PL?+?PL2P+MwA?+
+?P2P?+?P2P+MwA?+?PN? (2)
On average of all multiword expressions, unify-
ing multiword expression tokens contributes from
7.47% to 9.49% in shallow parsing accuracy. It
should be noted that this improvement is reported
on sentences which contain at least one known mul-
tiword expression. To project this improvement on
any general text, one needs to know the percentage
of sentences that contain known multiword expres-
Figure 3: Average change percentages per change class.
sions. Then the projected improvement can be com-
puted by multiplying these two percentages.
Table 4 shows that the increase in shallow pars-
ing accuracy is lower for expressions that consist of
nouns than for those that consist of an adjective and
a noun. Moreover, the improvement is higher for
non-compositional expressions than compositional
ones. This is expected, due to the idiosyncratic na-
ture of non-compositional multiword expressions.
The highest improvement, 15.32% to 19.67%, oc-
curs for non-compositional multiword expressions
that consist of an adjective followed by a noun.
Figure 3 shows the percentage of each class over
the sum of sentences whose parse before unify-
ing multiword expression tokens is different for the
parse after the replacement. The most common
change class is PL2P. It contains sentences in the
shallow parser output of which many phrases or
leaves were all assigned to a single phrase. 34.03%
of the changes are classified in this class. The least
common classes are change classes P2L, L2PMw
and L2P. Each of these accounts for less than 3%
of the overall changes.
6 Related Work
There have been proposed several ways to clas-
sify multiword expressions according to various
properties such as compositionality and institution-
alisation3 (Moon, 1998; Sag et al, 2002; Bald-
win, 2006). There is a large variety of meth-
ods in the literature that address recognising mul-
tiword expressions or some subcategory. Mc-
Carthy (2006) divides multiword expression detect-
3Institutionalisation is the degree that a multiword expres-
sion is accepted as lexical item through consistent use over time.
642
ing methods into statistical (e.g. pointwise mutual
information (PMI)), translation-based, dictionary-
based, substitution-based, and distributional. Sta-
tistical methods score multiword expression candi-
dates based on co-occurrence counts (Manning and
Schutze, 1999; Dunning, 1993; Lin, 1999; Frantzi et
al., 2000). Translation-based methods usually take
advantage of alignment to discover potential multi-
word expressions (Venkatapathy and Joshi, 2005).
Other methods use dictionaries to reveal semantic
relationships between the components of potential
multiword expressions and their context (Baldwin
et al, 2003; Hashimoto et al, 2006). Substitution-
based methods decide for multiword expressions
by substituting their components with other similar
words and measuring their frequency of occurrence
(Lin, 1999; Fazly and Stevenson, 2006). These tech-
niques can be enriched with selectional preference
information (Van de Cruys and Moiro?n, 2007; Katz
and Giesbrecht, 2006). Fazly and Stevenson (2007)
propose measures for institutionalisation, syntactic
fixedness and compositionality based on the selec-
tional preferences of verbs. There are several studies
relevant to detecting compositionality of noun-noun,
verb-particle and light verb constructions and verb-
noun pairs (e.g. Katz and Giesbrecht (2006)).
To the best of our knowledge there are no ap-
proaches integrating multiword expression knowl-
edge in deep or shallow parsing. However, there
are several attempts to integrate other forms of lex-
ical semantics into parsing. Bikel (2000) merged
the Brown portion of the Penn Treebank with Sem-
Cor, and used it to evaluate a generative bilexical
model for joint word sense disambiguation and pars-
ing. Similarly, Agirre et al (Agirre et al, 2008)
integrated semantic information in the form of se-
mantic classes and observed significant improve-
ment in parsing and PP attachment tasks. Xiong et
al. (2005) integrated first-sense and hypernym fea-
tures in a generative parse model applied to the Chi-
nese Penn Treebank and achieved significant im-
provement over their baseline model. Fujita et
al. (2007) extended this work by implementing a
discriminative parse selection model, incorporating
word sense information and achieved great improve-
ments as well. Examples of integrating selectional
preference information into parsing are Dowding et
al. (1994) and Hektoen (1997).
7 Conlusion and future work
In this paper, we presented an experimental study
attempting to estimate the contribution of unify-
ing multiword expression components into shallow
parsing. The evaluation is done based on 118 multi-
word expressions extracted from WordNet 3.0. They
consist of two successive components and are in
particular, compound nominals, proper names or
adjective-noun constructions.
Instead of using pre-annotated text, we collected
sentences that contain the above multiword expres-
sions from the web. We applied shallow parsing be-
fore and after unifying multiword expression tokens
and compared the outputs. We presented a detailed
classification of changes in the shallow parser out-
put to aid human annotation during the procedure of
deciding if a parser output is correct or wrong.
We presented experimental results about change
classes and about the overall improvement of uni-
fying multiword expression tokens with respect to
compositionality and the parts of speech of their
components. We conclude that unifying the tokens
of known multiwords expressions leads to an in-
crease of between 7.5% and 9.5% in accuracy of
shallow parsing of sentences that contain these mul-
tiword expressions. Increase percentages are higher
on adjective-noun constructions (12% to 15%); and
even higher on non-compositional adjective-noun
constructions (15.5% to 19.5%).
Future work will focus in conducting similar ex-
periments for multiword expressions longer than
two words. One would expect that due to their
size, a wrong interpretation of their structure would
affect the shallow parser output more than it does
for multiword expressions consisting of two words.
Thus, unifying multiword expressions longer than
two words would potentially contribute more to
shallow parsing accuracy.
Furthermore, the evaluation results presented in
this paper could be strengthened by adding man-
ual multiword expression annotation to some tree-
bank. This would provide a way to avoid the change
class analysis presented in Subsection 3.1 and com-
pute statistics more accurately. Finally, the results of
this paper suggest that implementing a parser able
to recognise multiword expressions would be very
helpful towards high accuracy parsing.
643
References
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of ACL, pages 317?325,
USA. ACL.
T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows.
2003. An empirical model of multiword expression
decomposability. In proceedings of the ACL workshop
on MWEs, pages 89?96, USA. ACL.
T. Baldwin. 2006. Compositionality and multiword ex-
pressions: Six of one, half a dozen of the other? In
proceedings of the ACL workshop on MWEs, Aus-
tralia. ACL.
D. Bikel. 2000. A statistical model for parsing and word-
sense disambiguation. In proceedings of the 2000
Joint SIGDAT conference: EMNLP/VLC, pages 155?
163, USA. ACL.
P. Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1-
3):217?239.
J. Dowding, R. Moore, F. Andryt, and D. Moran.
1994. Interleaving syntax and semantics in an efficient
bottom-up parser. In proceedings of ACL, pages 110?
116, USA. ACL.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
A. Fazly and S. Stevenson. 2006. Automatically con-
structing a lexicon of verb phrase idiomatic combina-
tions. In Proceedings of EACL, pages 337?344, Italy.
A. Fazly and S. Stevenson. 2007. Distinguishing sub-
types of multiword expressions using linguistically-
motivated statistical measures. In proceedings of the
ACL workshop on MWEs, pages 9?16, Czech Repub-
lic. ACL.
K. Frantzi, S. Ananiadou, and H. Mima. 2000. Auto-
matic recognition of multi-word terms: the c-value/nc-
value method. International Journal on Digital Li-
braries, 3(2):115?130.
S. Fujita, F. Bond, S. Oepen, and T. Tanaka. 2007. Ex-
ploiting semantic information for hpsg parse selection.
In proceedings of DeepLP, pages 25?32, USA. ACL.
C. Hashimoto, S. Sato, and T. Utsuro. 2006. Detecting
japanese idioms with a linguistically rich dictionary.
Language Resources and Evaluation, 40(3):243?252.
E. Hektoen. 1997. Probabilistic parse selection based
on semantic cooccurrences. In proceedings of IWPT,
pages 113?122, USA.
G. Katz and E. Giesbrecht. 2006. Automatic identi-
fication of non-compositional multi-word expressions
using latent semantic analysis. In proceedings of the
ACL workshop on MWEs, pages 12?19, Australia.
ACL.
S. Kim and T. Baldwin. 2008. Standardised evaluation
of english noun compound interpretation. In proceed-
ings of the LREC workshop on MWEs, pages 39?42,
Morocco.
I. Korkontzelos and S. Manandhar. 2009. Detecting
compositionality in multi-word expressions. In pro-
ceedings of ACL-IJCNLP, Singapore.
E. Laporte and S. Voyatzi. 2008. An Electronic Dictio-
nary of French Multiword Adverbs. In proceedings of
the LREC workshop on MWEs, pages 31?34, Marocco.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In proceedings of ACL, pages
317?324, USA. ACL.
C. Manning and H. Schutze, 1999. Foundations of Sta-
tistical NLP, Collocations, chapter 5. MIT Press.
D. McCarthy. 2006. Automatic methods to detect
the compositionality of MWEs. presentation slides.
url: www.sunum.org/myfiles/B2/McCarthyCollocId-
ioms06.ppt last accessed: 28/11/2009.
G. Miller. 1995. Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39?41.
R. Moon. 1998. Fixed Expressions and Idioms in En-
glish. A Corpus-based Approach. Oxford: Clarendon
Press.
M. Munoz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. In pro-
ceedings of EMNLP/VLC, pages 168?178, USA.
J. Nicholson and T. Baldwin. 2008. Interpreting com-
pound nominalisations. In proceedings of the LREC
workshop on MWEs, pages 43?45, Morocco.
G. Nunberg, T. Wasow, and I. Sag. 1994. Idioms. Lan-
guage, 70(3):491?539.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword expressions: A pain
in the neck for nlp. In proceedings of CICLing, pages
1?15, Mexico.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. Ad-
vances in Informatics, pages 382?392.
T. Van de Cruys and B. Moiro?n. 2007. Semantics-based
multiword expression extraction. In proceedings of the
ACL workshop on MWEs, pages 25?32, Czech Repub-
lic. ACL.
S. Venkatapathy and A. Joshi. 2005. Measuring the rela-
tive compositionality of verb-noun (V-N) collocations
by integrating features. In proceedings of HLT, pages
899?906, USA. ACL.
D. Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian. 2005. Pars-
ing the penn chinese treebank with semantic knowl-
edge. In proceedings of IJCNLP, pages 70?81, Korea.
644
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1425?1434,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving Question Recommendation by Exploiting Information Need
Shuguang Li
Department of Computer Science
University of York, YO10 5DD, UK
sgli@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
University of York, YO10 5DD, UK
suresh@cs.york.ac.uk
Abstract
In this paper we address the problem of ques-
tion recommendation from large archives of
community question answering data by ex-
ploiting the users? information needs. Our
experimental results indicate that questions
based on the same or similar information need
can provide excellent question recommenda-
tion. We show that translation model can be
effectively utilized to predict the information
need given only the user?s query question. Ex-
periments show that the proposed information
need prediction approach can improve the per-
formance of question recommendation.
1 Introduction
There has recently been a rapid growth in the num-
ber of community question answering (CQA) ser-
vices such as Yahoo! Answers1, Askville2 and
WikiAnswer3 where people answer questions post-
ed by other users. These CQA services have built up
very large archives of questions and their answers.
They provide a valuable resource for question an-
swering research. Table 1 is an example from Ya-
hoo! Answers web site. In the CQA archives, the
title part is the user?s query question, and the user?s
information need is usually expressed as natural lan-
guage statements mixed with questions expressing
their interests in the question body part.
In order to avoid the lag time involved with wait-
ing for a personal response and to enable high quali-
1http://answers.yahoo.com
2http://askville.amazon.com
3http://wiki.answers.com
ty answers from the archives to be retrieved, we need
to search CQA archives of previous questions that
are closely associated with answers. If a question
is found to be interesting to the user, then a previ-
ous answer can be provided with very little delay.
Question search and question recommendation are
proposed to facilitate finding highly relevant or po-
tentially interesting questions. Given a user?s ques-
tion as the query, question search tries to return
the most semantically similar questions from the
question archives. As the complement of question
search, we define question recommendation as rec-
ommending questions whose information need is the
same or similar to the user?s original question. For
example, the question ?What aspects of my com-
puter do I need to upgrade ...? with the informa-
tion need ?... making a skate movie, my computer
freezes, ...? and the question ?What is the most cost
effective way to expend memory space ...? with in-
formation need ?... in need of more space for mu-
sic and pictures ...? are both good recommendation
questions for the user in Table 1. So the recommend-
ed questions are not necessarily identical or similar
to the query question.
In this paper, we discuss methods for question
recommendation based on using the similarity be-
tween information need in the archive. We also
propose two models to predict the information need
based on the query question even if there?s no infor-
mation need expressed in the body of the question.
We show that with the proposed models it is possi-
ble to recommend questions that have the same or
similar information need.
The remainder of the paper is structured as fol-
1425
Q Title If I want a faster computer
should I buy more memory or s-
torage space? ...
Q Body I edit pictures and videos so I
need them to work quickly. Any
advice?
Answer ... If you are running out of s-
pace on your hard drive, then
... to boost your computer speed
usually requires more RAM ...
Table 1: Yahoo! Answers question example
lows. In section 2, we briefly describe the related
work on question search and recommendation. Sec-
tion 3 addresses in detail how we measure the sim-
ilarity between short texts. Section 4 describes two
models for information need prediction that we use
for the experiment. Section 5 tests the performance
of the proposed models for the task of question rec-
ommendation. Section 7 is the conclusion of this
paper.
2 Related Work
2.1 Question Search
Burke et al (1997) combined a lexical metric and a
simple semantic knowledge-based (WordNet) simi-
larity method to retrieve semantically similar ques-
tions from frequently asked question (FAQ) data.
Jeon et al (2005a) retrieved semantically similar
questions from Korean CQA data by calculating the
similarity between their answers. The assumption
behind their research is that questions with very sim-
ilar answers tend to be semantically similar. Jeon
et al (2005b) also discussed methods for grouping
similar questions based on using the similarity be-
tween answers in the archive. These grouped ques-
tion pairs were further used as training data to es-
timate probabilities for a translation-based question
retrieval model. Wang et al (2009) proposed a tree
kernel framework to find similar questions in the C-
QA archive based on syntactic tree structures. Wang
et al (2010) mined lexical and syntactic features to
detect question sentences in CQA data.
2.2 Question Recommendation
Wu et al (2008) presented an incremental auto-
matic question recommendation framework based
on probabilistic latent semantic analysis. Question
recommendation in their work considered both the
users? interests and feedback. Duan et al (2008)
made use of a tree-cut model to represent question-
s as graphs of topic terms. Questions were recom-
mended based on this topic graph. The recommend-
ed questions can provide different aspects around the
topic of the query question.
The above question search and recommendation
research provide different ways to retrieve question-
s from large archives of question answering data.
However, none of them considers the similarity or
diversity between questions by exploring their infor-
mation needs.
3 Short Text Similarity Measures
In question retrieval systems accurate similarity
measures between documents are crucial. Most tra-
ditional techniques for measuring the similarity be-
tween two documents mainly focus on comparing
word co-occurrences. The methods employing this
strategy for documents can usually achieve good re-
sults, because they may share more common words
than short text snippets. However the state-of-the-
art techniques usually fail to achieve desired results
due to short questions and information need texts.
In order to measure the similarity between short
texts, we make use of three kinds of text similari-
ty measures: TFIDF based, Knowledge based and
Latent Dirichlet Allocation (LDA) based similarity
measures in this paper. We will compare their per-
formance for the task of question recommendation
in the experiment section.
3.1 TFIDF
Baeza-Yates and Ribeiro-Neto (1999) provides a T-
FIDF method to calculate the similarity between two
texts. Each document is represented by a term vec-
tor using TFIDF score. The similarity between two
text Di and Dj is the cosine similarity in the vector
space model:
cos(Di, Dj) =
DTi Dj
?Di??Dj?
1426
This method is used in most information retrieval
systems as it is both efficient and effective. Howev-
er if the query text contains only one or two words
this method will be biased to shorter answer texts
(Jeon et al, 2005a). We also found that in CQA data
short contents in the question body cannot provide
any information about the users? information needs.
Based on the above two reasons, in the test data sets
we do not include the questions whose information
need parts contain only a few noninformative words
.
3.2 Knowledge-based Measure
Mihalcea et al (2006) proposed several knowledge-
based methods for measuring the semantic level sim-
ilarity of texts to solve the lexical chasm problem be-
tween short texts. These knowledge-based similarity
measures were derived from word semantic similar-
ity by making use of WordNet. The evaluation on a
paraphrase recognition task showed that knowledge-
based measures outperform the simpler lexical level
approach.
We follow the definition in (Mihalcea et al, 2006)
to derive a text-to-text similarity metric mcs for two
given texts Di and Dj :
mcs(Di, Dj) =
?
w?Di
maxSim(w,Dj) ? idf(w)
?
w?Di
idf(w)
+
?
w?Dj
maxSim(w,Di) ? idf(w)
?
w?Dj
idf(w)
For each word w in Di, maxSim(w,Dj) com-
putes the maximum semantic similarity between w
and any word in Dj . In this paper we choose lin
(Lin, 1998) and jcn (Jiang and Conrath, 1997) to
compute the word-to-word semantic similarity.
We only choose nouns and verbs for calculating
mcs. Additionally, when w is a noun we restrict
the words in document Di (and Dj) to just nouns.
Similarly, when w is a verb, we restrict the words in
document Di (and Dj) to just verbs.
3.3 Probabilistic Topic Model
Celikyilmaz et al (2010) presented probabilistic
topic model based methods to measure the similar-
ity between question and candidate answers. The
candidate answers were ranked based on the hidden
topics discovered by Latent Dirichlet Allocation (L-
DA) methods.
In contrast to the TFIDF method which measures
?common words?, short texts are not compared to
each other directly in probabilistic topic models. In-
stead, the texts are compared using some ?third-
party? topics that relate to them. A passage D in the
retrieved documents (document collection) is repre-
sented as a mixture of fixed topics, with topic z get-
ting weight ?(D)z in passage D and each topic is a
distribution over a finite vocabulary of words, with
word w having a probability ?(z)w in topic z. Gibbs
Sampling can be used to estimate the corresponding
expected posterior probabilities P (z|D) = ??(D)z and
P (w|z) = ??(z)w (Griffiths and Steyvers, 2004).
In this paper we use two LDA based similarity
measures in (Celikyilmaz et al, 2010) to measure
the similarity between short information need texts.
The first LDA similarity method uses KL divergence
to measure the similarity between two documents
under each given topic:
simLDA1(Di, Dj) =
1
K
K?
k=1
10W (D
(z=k)
i ,D
(z=k)
j )
W (D(z=k)i , D
(z=k)
j ) =
?KL(D(z=k)i ?
D(z=k)i +D
(z=k)
j
2
)
?KL(D(z=k)j ?
D(z=k)i +D
(z=k)
j
2
)
W (D(z=k)i , D
(z=k)
j ) calculates the similarity be-
tween two documents under topic z = k using KL
divergence measure. D(z=k)i is the probability distri-
bution of words in document Di given a fixed topic
z.
The second LDA similarity measure from (Grif-
fiths and Steyvers, 2004) treats each document as a
probability distribution of topics:
simLDA2(Di, Dj) = 10
W (??(Di),??(Dj))
where ??(Di) is document Di?s probability distribu-
tion of topics as defined earlier.
1427
4 Information Need Prediction using
Statistical Machine Translation Model
There are two reasons that we need to predict in-
formation need. It is often the case that the query
question does not have a question body part. So we
need a model to predict the information need part
based on the query question in order to recommend
questions based on the similarity of their informa-
tion needs. Another reason is that information need
prediction plays a crucial part not only in Question
Answering but also in information retrieval (Liu et
al., 2008). In this paper we propose an information
need prediction method based on a statistical ma-
chine translation model.
4.1 Statistical Machine Translation Model
(f(s), e(s)), s = 1,...,S is a parallel corpus. In a
sentence pair (f, e), source language String, f =
f1f2...fJ has J words, and e = e1e2...eI has I word-
s. And alignment a = a1a2...aJ represents the map-
ping information from source language words to tar-
get words.
Statistical machine translation models estimate
Pr(f|e), the translation probability from source lan-
guage string e to target language string f (Och et al,
2003):
Pr(f|e) =
?
a
Pr(f, a|e)
EM-algorithm is usually used to train the align-
ment models to estimate lexicon parameters p(f |e).
In E-step, the counts for one sentence pair (f ,e)
are:
c(f |e; f, e) =
?
a
Pr(a|f, e)
?
i,j
?(f, fj)?(e, eaj )
Pr(a|f, e) = Pr(f, a|e)/Pr(a|e)
In the M-step, lexicon parameters become:
p(f |e) ?
?
s
c(f |e; f(s), e(s))
Different alignment models such as IBM-1 to
IBM-5 (Brown et al, 1993) and HMM model (Och
and Ney, 2000) provide different decompositions of
Pr(f ,a|e). For different alignment models differ-
ent approaches were proposed to estimate the cor-
responding alignments and parameters. The detail-
s can be found in (Och et al, 2003; Brown et al,
1993).
4.2 Information Need Prediction
After estimating the statistical translation probabili-
ties, we treat the information need prediction as the
process of ranking words by p(w|Q), the probability
of generating word w from question Q:
P (w|Q) = ?
?
t?Q
Ptr(w|t)P (t|Q)+(1??)P (w|C)
The word-to-word translation probability
Ptr(w|t) is the probability of word w is translated
from a word t in question Q using the translation
model. The above formula uses linear interpolation
smoothing of the document model with the back-
ground language model P (t|C). ? is the smoothing
parameter. P (t|Q) and P (t|C) are estimated using
the maximum likelihood estimator.
One important consideration is that statistical ma-
chine translation models first estimate Pr(f|e) and
then calculate Pr(e|f) using Bayes? theorem to min-
imize ordering errors (Brown et al, 1993):
Pr(e|f) =
Pr(f|e)Pr(e)
Pr(f)
But in this paper, we skip this step as we found out
the order of words in information need part is not
an important factor. In our collected CQA archive,
question title and information need pairs can be con-
sidered as a type of parallel corpus, which is used
for estimating word-to-word translation probabili-
ties. More specifically, we estimated the IBM-4
model by GIZA++4 with the question part as the
source language and information need part as the tar-
get language.
5 Experiments and Results
5.1 Text Preprocessing
The questions posted on community QA sites often
contain spelling or grammar errors. These errors in-
4http://fjoch.com/GIZA++.html
1428
Test c Test t
Methods MRR Precision@5 Precision@10 MRR Precision@5 Precision@10
TFIDF 84.2% 67.1% 61.9% 92.8% 74.8% 63.3%
Knowledge1 82.2% 65.0% 65.6% 78.1% 67.0% 69.6%
Knowledge2 76.7% 54.9% 59.3% 61.6% 53.3% 58.2%
LDA1 92.5% 68.8% 64.7% 91.8% 75.4% 69.8%
LDA2 61.5% 55.3% 60.2% 52.1% 57.4% 54.5%
Table 2: Question recommendation results without information need prediction
Test c Test t
Methods MRR Precision@5 Precision@10 MRR Precision@5 Precision@10
TFIDF 86.2% 70.8% 64.3% 95.1% 77.8% 69.3%
Knowledge1 82.2% 65.0% 66.6% 76.7% 68.0% 68.7%
Knowledge2 76.7% 54.9% 60.2% 61.6% 53.3% 58.2%
LDA1 95.8% 72.4% 68.2% 96.2% 79.5% 69.2%
LDA2 61.5% 55.3% 58.9% 68.1% 58.3% 53.9%
Table 3: Question recommendation results with information need predicted by translation model
fluence the calculation of similarity and the perfor-
mance of information retrieval (Zhao et al, 2007;
Bunescu and Huang, 2010). In this paper, we use
an open source software afterthedeadline5 to auto-
matically correct the spelling errors in the question
and information need texts first. We also made use
of Web 1T 5-gram6 to implement an N-Gram based
method (Cheng et al, 2008) to further filter out the
false positive corrections and re-rank correction sug-
gestions (Mudge, 2010). The texts are tagged by
Brill?s Part-of-Speech Tagger7 as the rule-based tag-
ger is more robust than the state-of-art statistical tag-
gers for raw web contents. This tagging informa-
tion is only used for WordNet similarity calculation.
Stop word removal and lemmatization are applied
to the all the raw texts before feeding into machine
translation model training, the LDA model estimat-
ing and similarity calculation.
5.2 Construction of Training and Testing Sets
We made use of the questions crawled from Yahoo!
Answers for the estimating models and evaluation.
More specifically, we obtained 2 million questions
under two categories at Yahoo! Answers: ?travel?
5http://afterthedeadline.com
6http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?cata
logId=LDC2006T13
7http://www.umiacs.umd.edu/ jimmylin/resources.html
(1 million), and ?computers&internet? (1 million).
Depending on whether the best answers have been
chosen by the asker, questions from Yahoo! answers
can be divided into ?resolved? and ?unresolved? cat-
egories. From each of the above two categories, we
randomly selected 200 resolved questions to con-
struct two testing data sets: ?Test t? (?travel?), and
?Test c? (?computers&internet?). In order to mea-
sure the information need similarity in our experi-
ment we selected only those questions whose infor-
mation needs part contained at least 3 informative
words after stop word removal. The rest of the ques-
tions ?Train t? and ?Train c? under the two categories
are left for estimating the LDA topic models and the
translation models. We will show how we obtain
these models later.
5.3 Experimental Setup
For each question (query question) in ?Test t? or
?Test c?, we used the words in the question title part
as the main search query and the other words in the
information need part as search query expansion to
retrieve candidate recommended questions from Ya-
hoo! Answers website. We obtained an average of
154 resolved questions under ?travel? or ?computer-
s&internet? category, and three assessors were in-
volved in the manual judgments.
Given a question returned by a recommendation
1429
method, two assessors are asked to label it with
?good? or ?bad?. The third assessor will judge the
conflicts. The assessors are also asked to read the in-
formation need and answer parts. If a recommended
question is considered to express the same or similar
information need, the assessor will label it ?good?;
otherwise, the assessor will label it as ?bad?.
Three measures for evaluating the recommenda-
tion performance are utilized. They are Mean Re-
ciprocal Rank (MRR), top five prediction accura-
cy (precision@5) and top ten prediction accuracies
(precision@10) (Voorhees and Tice, 2004; Cao et
al., 2008). In MRR the reciprocal rank of a query
question is the multiplicative inverse of the rank of
the first ?good? recommended question. The top five
prediction accuracy for a query question is the num-
ber of ?good? recommended questions out of the top
five ranked questions and the top ten accuracy is cal-
culated out of the top ten ranked questions.
5.4 Similarity Measure
The first experiment conducted question recommen-
dation based on their information need parts. Dif-
ferent text similarity methods described in section
3 were used to measure the similarity between the
information need texts. In TFIDF similarity mea-
sure (TFIDF), the idf values for each word were
computed from frequency counts over the entire
Aquaint corpus8. For calculating the word-to-word
knowledge-based similarity, a WordNet::Similarity
Java implementation9 of the similarity measures lin
(Knowledge2) and jcn (Knowledge1) is used in this
paper. For calculating topic model based similarity,
we estimated two LDA models from ?Train t? and
?Train c? using GibbsLDA++10. We treated each
question including the question title and the infor-
mation need part as a single document of a sequence
of words. These documents were preprocessed be-
fore being fed into LDA model. 1800 iterations for
Gibbs sampling 200 topics parameters were set for
each LDA model estimation.
The results in table 2 show that TFIDF and LDA1
methods perform better for recommending questions
than the others. After further analysis of the ques-
tions recommended by both methods, we discov-
8http://ldc.upenn.edu/Catalog/docs/LDC2002T31
9http://cogs.susx.ac.uk/users/drh21/
10http://gibbslda.sourceforge.net
Q1: If I want a faster computer should I buy
more memory or storage space?
InfoN If I want a faster computer should I buy
more memory or storage space? What-
s the difference? I edit pictures and
videos so I need them to work quickly.
...
RQ1 Would buying 1gb memory upgrade
make my computer faster?
InfoN I have an inspiron B130. It has 512mb
memory now. I would add another 1gb
into 2nd slot ...
RQ2 whats the difference between memory
and hard drive space on a computer and
why is.....?
InfoN see I am starting edit videos on my com-
puter but i am running out of space. why
is so expensive to buy memory but not
external drives? ...
Q2: Where should my family go for spring
break?
InfoN ... family wants to go somewhere for
a couple days during spring break ...
prefers a warmer climate and we live in
IL, so it shouldn?t be SUPER far away.
... a family road trip. ...
RQ1 Whats a cheap travel destination for
spring break?
InfoN I live in houston texas and i?m trying to
find i inexpensive place to go for spring
break with my family.My parents don?t
want to spend a lot of money due to the
economy crisis, ... a fun road trip...
RQ2 Alright you creative deal-seekers, I need
some help in planning a spring break
trip for my family
InfoN Spring break starts March 13th and goes
until the 21st ... Someplace WARM!!!
Family-oriented hotel/resort ... North
American Continent (Mexico, America,
Jamaica, Bahamas, etc.) Cost= Around
$5,000 ...
Table 4: Question recommendation results by LDA mea-
suring the similarity between information needs
1430
ered that the ordering of the recommended questions
from TFIDF and LDA1 are quite different. TFIDF
similarity method prefers texts with more common
words, while the LDA1 method can find the rela-
tion between the non-common words between short
texts based on a series of third-party topics. The L-
DA1 method outperforms the TFIDF method in two
ways: (1) the top recommended questions? informa-
tion needs share less common words with the query
question?s; (2) the top recommended questions span
wider topics. The questions highly recommended by
LDA1 can suggest more useful topics to the user.
Knowledge-based methods are also shown to per-
form worse than TFIDF and LDA1. We found that
some words were mis-tagged so that they were not
included in the word-to-word similarity calculation.
Another reason for the worse performance is that the
words out of the WordNet dictionary were also not
included in the similarity calculation.
The Mean Reciprocal Rank score for TFIDF and
LDA1 are more than 80%. That is to say, we are able
to recommend questions to the users by measuring
their information needs. The first two recommended
questions for Q1 and Q2 using LDA1 method are
shown in table 4. InfoN is the information need part
associated with each question.
In the preprocessing step, some words were suc-
cessfully corrected such as ?What should I do this
saturday? ... and staying in a hotell ...? and ?my
faimly is traveling to florda ...?. However, there are
still a small number of texts such as ?How come my
Gforce visualization doesn?t work?? and ?Do i need
an Id to travel from new york to maimi?? failed to
be corrected. So in the future, a better method is
expected to correct these failure cases.
5.5 Information Need Prediction
There are some retrieved questions whose informa-
tion need parts are empty or become empty or al-
most empty (one or two words left) after the prepro-
cessing step. The average number of such retrieved
questions for each query question is 10 in our exper-
iment. The similarity ranking scores of these ques-
tions are quite low or zero in the previous experi-
ment. In this experiment, we will apply information
need prediction to the questions whose information
needs are missing in order to find out whether we
improve the recommendation task.
The question and information need pairs in both
?Train t? and ?Train c? training sets were used to
train two IBM-4 translation models by GIZA++
toolkit. These pairs were also preprocessed before
training. And the pairs whose information need part
become empty after preprocessing were disregard-
ed.
During the experiment, we found that some of the
generated words in the information need parts are
themselves. This is caused by the self translation
problem in translation model: the highest transla-
tion score for a word is usually given to itself if
the target and source languages are the same (Xue
et al, 2008). This has always been a tough ques-
tion: not using self-translated words can reduce re-
trieval performance as the information need parts
need the terms to represent the semantic meanings;
using self-translated words does not take advantage
of the translation approach. To tackle this problem,
we control the number of the words predicted by the
translation model to be exactly twice the number of
words in the corresponding preprocessed question.
The predicted information need words for the re-
trieved questions are shown in Table 5. In Q1, the in-
formation need behind question ?recommend web-
site for custom built computer parts? may imply
that the users need to know some information about
building computer parts such as ?ram? and ?moth-
erboard? for a different purpose such as ?gaming?.
While in Q2, the user may want to compare comput-
ers in different brands such as ?dell? and ?mac? or
consider the ?price? factor for ?purchasing a laptop
for a college student?.
We also did a small scale comparison between the
generated information needs against the real ques-
tions whose information need parts are not empty.
Q3 and Q4 in Table 5 are two examples. The orig-
inal information need for Q3 is ?looking for beauti-
ful beaches and other things to do such as museum-
s, zoos, shopping, and great seafood? in CQA. The
generated content for Q3 contains words in wider
topics such as ?wedding?, ?surf ? and the price infor-
mation (?cheap?). This reflects that there are some
other users asking similar questions with the same
or other interests.
From the results in Table 3, we can see that the
performance of most similarity methods were im-
proved by making use of information need predic-
1431
tion. Different similarity measures received differ-
ent degrees of improvement. LDA1 obtained the
highest improvement followed by the TFIDF based
method. These two approaches are more sensitive to
the contents generated by a translation model.
However we found out that in some cases the L-
DA1 model failed to give higher scores to good rec-
ommendation questions. For example, Q5, Q6, and
Q7 in table 5 were retrieved as recommendation can-
didates for the query question in Table 1. All of the
three questions were good recommendation candi-
dates, but only Q6 ranked fifth while Q5 and Q7
were out of the top 30 by LDA1 method. Moreover,
in a small number of cases bad recommendation
questions received higher scores and jeopardized the
performance. For example, for query question ?How
can you add subtitles to videos?? with information
need ?... add subtitles to a music video ... got off
youtube ...download for this ...?, a retrieved ques-
tion ?How would i add a music file to a video clip.
...? was highly recommended by TFIDF approach
as predicted information need contained ?youtube?,
?video?, ?music?, ?download ?, ... .
The MRR score received an improvement from
92.5% to 95.8% in the ?Test c? and from 91.8% to
96.2% in ?Test t?. This means that the top one ques-
tion recommended by our methods can be quite well
catering to the users? information needs. The top
five precision and the top ten precision scores us-
ing TFIDF and LDA1 methods also received dif-
ferent degrees of improvement. Thus, we can im-
prove the performance of question recommendation
by predicting information needs.
6 Conclusions
In this paper we addressed the problem of recom-
mending questions from large archives of commu-
nity question answering data based on users? infor-
mation needs. We also utilized a translation mod-
el and a LDA topic model to predict the informa-
tion need only given the user?s query question. D-
ifferent information need similarity measures were
compared to prove that it is possible to satisfy user?s
information need by recommending questions from
large archives of community QA. The Latent Dirich-
let alocation based approach was proved to perfor-
m better on measuring the similarity between short
Q1: Please recommend A good website for
Custom Built Computer parts?
InfoN custom, site, ram, recommend, price,
motherboard, gaming, ...
Q2: What is the best laptop for a college stu-
dent?
InfoN know, brand, laptop, college, buy, price,
dell, mac, ...
Q3: What is the best Florida beach for a honey-
moon?
InfoN Florida, beach, honeymoon, wedding, surf,
cheap, fun, ...
Q4: Are there any good clubs in Manchester
InfoN club, bar, Manchester, music, age, fun,
drink, dance, ...
Q5: If i buy a video card for my computer will
that make it faster?
InfoN nvidia, video, ati, youtube, card, buy, win-
dow, slow, computer, graphics, geforce,
faster, ...
Q6: If I buy a bigger hard drive for my laptop,
will it make my computer run faster or just
increase the memory?
InfoN laptop, ram, run, buy, bigger, memory,
computer, increase, gb, hard, drive, faster,
...
Q7: Is there a way I can make my computer
work faster rather than just increasing the
ram or harware space?
InfoN space, speed, ram, hardware, main, gig, s-
low, computer, increase, work, gb, faster,
...
Table 5: Information need prediction examples using
IBM-4 translation model
1432
texts in the semantic level than traditional method-
s. Experiments showed that the proposed transla-
tion based language model for question information
need prediction further enhanced the performance of
question recommendation methods.
References
Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval. Addison-Wesley
Longman Publishing Co., Inc., Boston, MA, USA.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, Robert L. Mercer. 1993. The mathematics of
statistical machine translation: parameter estimation.
Computational Linguistics, v.19 n.2, June 1993.
Razvan Bunescu and Yunfeng Huang. 2010. Learning the
Relative Usefulness of Questions in Community QA.
Proceedings of the Conference on Empirical Method-
s in Natural Language Processing (EMNLP) , Cam-
bridge, MA.
Robin D. Burke and Kristian J. Hammond and Vladimir
A. Kulyukin and Steven L. Lytinen and Noriko To-
muro and Scott Schoenberg. 1997. Question answer-
ing from frequently-asked question files: Experiences
with the FAQ Finder system. AI Magazine, 18, 57C66.
Yunbo Cao, Huizhong Duan, Chin-Yew Lin, Yong Yu,
and Hsiao-Wuen Hon. 2008. Recommending Ques-
tions Using the MDL-based Tree Cut Model. In: Proc.
of the 17th Int. Conf. on World Wide Web, pp. 81-90.
Asli Celikyilmaz and Dilek Hakkani-Tur and Gokhan
Tur. 2010. LDA Based Similarity Modeling for Ques-
tion Answering. In NAACL 2010 C Workshop on Se-
mantic Search.
Charibeth Cheng, Cedric Paul Alberto, Ian Anthony
Chan, and Vazir Joshua Querol. 2008. SpellCheF:
Spelling Checker and Corrector for Filipino. Journal
of Research in Science, Computing and Engineering,
North America, 4, sep. 2008.
Lynn Silipigni Connaway and Chandra Prabha. 2005. An
overview of the IMLS Project ?Sense-making the in-
formation confluence: The whys and hows of college
and university user satisficing of information needs?.
Presented at Library of Congress Forum, American
Library Association Midwinter Conference, Boston,
MA, Jan 16, 2005.
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In HLT-ACL, pages
156C164.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. Natl Acad Sci 101:5228C5235.
Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005a.
Finding semantically similar questions based on their
answers. In Proc. of SIGIR05.
Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005b.
Finding similar questions in large question and an-
swer archives. In CIKM, pages 84C90.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxono-
my. In Proceedings of International Conference on Re-
search in Computational Linguistics, Taiwan.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning (ICML ?98),
Jude W. Shavlik (Ed.). Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA, 296-304.
Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008.
Predicting information seeker satisfaction in commu-
nity question answering. In Proceedings of the 31st
annual international ACM SIGIR conference on Re-
search and development in information retrieval (SI-
GIR ?08). ACM, New York, NY, USA, 483-490.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
national conference on Artificial intelligence (AAAI
?06), pages 775C780. AAAI Press.
Raphael Mudge. 2010. The design of a proofreading soft-
ware service. In Proceedings of the NAACL HLT 2010
Workshop on Computational Linguistics and Writing:
Writing Processes and Authoring Aids (CL&W ?10).
Association for Computational Linguistics, Morris-
town, NJ, USA, 24-32.
Franz Josef Och, Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation.
Proceedings of the 18th conference on Computational
linguistics, July 31-August 04, Saarbrucken, Germany.
Franz Josef Och, Hermann Ney. 2003.A Systematic Com-
parison of Various Statistical Alignment Models. Com-
putational Linguistics, volume 29, number 1, pp. 19-
51 March 2003.
Jahna Otterbacher, Gunes Erkan, Dragomir R. Radev.
2009. Biased LexRank: Passage retrieval using ran-
dom walks with question-based priors. Information
Processing and Management: an International Journal,
v.45 n.1, p.42-54, January, 2009.
Chandra Prabha, Lynn Silipigni Connaway, Lawrence
Olszewski, Lillie R. Jenkins. 2007. What is enough?
Satisficing information needs. Journal of Documenta-
tion (January, 63,1).
Ellen Voorhees and Dawn Tice. 2000. The TREC-8 ques-
tion answering track evaluation. In Text Retrieval
Conference TREC-8, Gaithersburg, MD.
Kai Wang, Yanming Zhao, and Tat-Seng Chua. 2009.
A syntactic tree matching approach to finding similar
1433
questions in community-based qa services. In SIGIR,
pages 187C194.
Kai Wang and Tat-Seng Chua. 2010. Exploiting salient
patterns for question detection and question retrieval
in community-based question answering. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING ?10). Association for
Computational Linguistics, Stroudsburg, PA, USA,
1155-1163.
Hu Wu, Yongji Wang, and Xiang Cheng. 2008. Incremen-
tal probabilistic latent semantic analysis for automatic
question recommendation. In RecSys.
Xiaobing Xue, Jiwoon Jeon, W. Bruce Croft. 2008. Re-
trieval models for question and answer archives. In
SIGIR?08, pages 475C482. ACM.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning
Question Paraphrases for QA from Encarta Logs. In
Proceedings of International Joint Conferences on Ar-
tificial Intelligence (IJCAI), pages 1795-1800.
1434
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 63?68,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 14: Word Sense Induction & Disambiguation
Suresh Manandhar
Department of Computer Science
University of York, UK
Ioannis P. Klapaftis
Department of Computer Science
University of York, UK
Dmitriy Dligach
Department of Computer Science
University of Colorado, USA
Sameer S. Pradhan
BBN Technologies
Cambridge, USA
Abstract
This paper presents the description and
evaluation framework of SemEval-2010
Word Sense Induction & Disambiguation
task, as well as the evaluation results of 26
participating systems. In this task, partici-
pants were required to induce the senses of
100 target words using a training set, and
then disambiguate unseen instances of the
same words using the induced senses. Sys-
tems? answers were evaluated in: (1) an
unsupervised manner by using two clus-
tering evaluation measures, and (2) a su-
pervised manner in a WSD task.
1 Introduction
Word senses are more beneficial than simple word
forms for a variety of tasks including Information
Retrieval, Machine Translation and others (Pantel
and Lin, 2002). However, word senses are usually
represented as a fixed-list of definitions of a manu-
ally constructed lexical database. Several deficien-
cies are caused by this representation, e.g. lexical
databases miss main domain-specific senses (Pan-
tel and Lin, 2002), they often contain general defi-
nitions and suffer from the lack of explicit seman-
tic or contextual links between concepts (Agirre
et al, 2001). More importantly, the definitions of
hand-crafted lexical databases often do not reflect
the exact meaning of a target word in a given con-
text (V?eronis, 2004).
Unsupervised Word Sense Induction (WSI)
aims to overcome these limitations of hand-
constructed lexicons by learning the senses of a
target word directly from text without relying on
any hand-crafted resources. The primary aim of
SemEval-2010 WSI task is to allow comparison
of unsupervised word sense induction and disam-
biguation systems.
The target word dataset consists of 100 words,
50 nouns and 50 verbs. For each target word, par-
ticipants were provided with a training set in or-
der to learn the senses of that word. In the next
step, participating systems were asked to disam-
biguate unseen instances of the same words using
their learned senses. The answers of the systems
were then sent to organisers for evaluation.
2 Task description
Figure 1 provides an overview of the task. As
can be observed, the task consisted of three
separate phases. In the first phase, train-
ing phase, participating systems were provided
with a training dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to use this training dataset to induce the senses
of the target word. No other resources were al-
lowed with the exception of NLP components for
morphology and syntax. In the second phase,
testing phase, participating systems were pro-
vided with a testing dataset that consisted of a
set of target word (noun/verb) instances (sen-
tences/paragraphs). Participants were then asked
to tag (disambiguate) each testing instance with
the senses induced during the training phase. In
the third and final phase, the tagged test instances
were received by the organisers in order to evalu-
ate the answers of the systems in a supervised and
an unsupervised framework. Table 1 shows the to-
tal number of target word instances in the training
and testing set, as well as the average number of
senses in the gold standard.
The main difference of the SemEval-2010 as
compared to the SemEval-2007 sense induction
task is that the training and testing data are treated
separately, i.e the testing data are only used for
sense tagging, while the training data are only used
63
Figure 1: Training, testing and evaluation phases of SemEval-2010 Task 14
Training set Testing set Senses (#)
All 879807 8915 3.79
Nouns 716945 5285 4.46
Verbs 162862 3630 3.12
Table 1: Training & testing set details
for sense induction. Treating the testing data as
new unseen instances ensures a realistic evalua-
tion that allows to evaluate the clustering models
of each participating system.
The evaluation framework of SemEval-2010
WSI task considered two types of evaluation.
In the first one, unsupervised evaluation, sys-
tems? answers were evaluated according to: (1) V-
Measure (Rosenberg and Hirschberg, 2007), and
(2) paired F-Score (Artiles et al, 2009). Nei-
ther of these measures were used in the SemEval-
2007 WSI task. Manandhar & Klapaftis (2009)
provide more details on the choice of this evalu-
ation setting and its differences with the previous
evaluation. The second type of evaluation, super-
vised evaluation, follows the supervised evalua-
tion of the SemEval-2007 WSI task (Agirre and
Soroa, 2007). In this evaluation, induced senses
are mapped to gold standard senses using a map-
ping corpus, and systems are then evaluated in a
standard WSD task.
2.1 Training dataset
The target word dataset consisted of 100 words,
i.e. 50 nouns and 50 verbs. The training dataset
for each target noun or verb was created by follow-
ing a web-based semi-automatic method, similar
to the method for the construction of Topic Signa-
tures (Agirre et al, 2001). Specifically, for each
WordNet (Fellbaum, 1998) sense of a target word,
we created a query of the following form:
<Target Word> AND <Relative Set>
The <Target Word> consisted of the target
word stem. The <Relative Set> consisted of a
disjunctive set of word lemmas that were related
Word Query
Sense
Sense 1 failure AND (loss OR nonconformity OR test
OR surrender OR ?force play? OR ...)
Sense 2 failure AND (ruination OR flop OR bust
OR stall OR ruin OR walloping OR ...)
Table 2: Training set creation: example queries for
target word failure
to the target word sense for which the query was
created. The relations considered were WordNet?s
hypernyms, hyponyms, synonyms, meronyms and
holonyms. Each query was manually checked by
one of the organisers to remove ambiguous words.
The following example shows the query created
for the first
1
and second
2
WordNet sense of the
target noun failure.
The created queries were issued to Yahoo!
search API
3
and for each query a maximum of
1000 pages were downloaded. For each page we
extracted fragments of text that occurred in <p>
</p> html tags and contained the target word
stem. In the final stage, each extracted fragment of
text was POS-tagged using the Genia tagger (Tsu-
ruoka and Tsujii, 2005) and was only retained, if
the POS of the target word in the extracted text
matched the POS of the target word in our dataset.
2.2 Testing dataset
The testing dataset consisted of instances of the
same target words from the training dataset. This
dataset is part of OntoNotes (Hovy et al, 2006).
We used the sense-tagged dataset in which sen-
tences containing target word instances are tagged
with OntoNotes (Hovy et al, 2006) senses. The
texts come from various news sources including
CNN, ABC and others.
1
An act that fails
2
An event that does not accomplish its intended purpose
3
http://developer.yahoo.com/search/ [Access:10/04/2010]
64
G1
G
2
G
3
C
1
10 10 15
C
2
20 50 0
C
3
1 10 60
C
4
5 0 0
Table 3: Clusters & GS senses matrix.
3 Evaluation framework
For the purposes of this section we provide an ex-
ample (Table 3) in which a target word has 181
instances and 3 GS senses. A system has gener-
ated a clustering solution with 4 clusters covering
all instances. Table 3 shows the number of com-
mon instances between clusters and GS senses.
3.1 Unsupervised evaluation
This section presents the measures of unsuper-
vised evaluation, i.e V-Measure (Rosenberg and
Hirschberg, 2007) and (2) paired F-Score (Artiles
et al, 2009).
3.1.1 V-Measure evaluation
Let w be a target word with N instances (data
points) in the testing dataset. Let K = {C
j
|j =
1 . . . n} be a set of automatically generated clus-
ters grouping these instances, and S = {G
i
|i =
1 . . .m} the set of gold standard classes contain-
ing the desirable groupings of w instances.
V-Measure (Rosenberg and Hirschberg, 2007)
assesses the quality of a clustering solution by ex-
plicitly measuring its homogeneity and its com-
pleteness. Homogeneity refers to the degree that
each cluster consists of data points primarily be-
longing to a single GS class, while completeness
refers to the degree that each GS class consists of
data points primarily assigned to a single cluster
(Rosenberg and Hirschberg, 2007). Let h be ho-
mogeneity and c completeness. V-Measure is the
harmonic mean of h and c, i.e. VM =
2?h?c
h+c
.
Homogeneity. The homogeneity, h, of a clus-
tering solution is defined in Formula 1, where
H(S|K) is the conditional entropy of the class
distribution given the proposed clustering and
H(S) is the class entropy.
h =
{
1, if H(S) = 0
1?
H(S|K)
H(S)
, otherwise
(1)
H(S) = ?
|S|
?
i=1
?
|K|
j=1
a
ij
N
log
?
|K|
j=1
a
ij
N
(2)
H(S|K) = ?
|K|
?
j=1
|S|
?
i=1
a
ij
N
log
a
ij
?
|S|
k=1
a
kj
(3)
When H(S|K) is 0, the solution is perfectly
homogeneous, because each cluster only contains
data points that belong to a single class. How-
ever in an imperfect situation, H(S|K) depends
on the size of the dataset and the distribution of
class sizes. Hence, instead of taking the raw con-
ditional entropy, V-Measure normalises it by the
maximum reduction in entropy the clustering in-
formation could provide, i.e. H(S). When there
is only a single class (H(S) = 0), any clustering
would produce a perfectly homogeneous solution.
Completeness. Symmetrically to homogeneity,
the completeness, c, of a clustering solution is de-
fined in Formula 4, where H(K|S) is the condi-
tional entropy of the cluster distribution given the
class distribution and H(K) is the clustering en-
tropy. When H(K|S) is 0, the solution is perfectly
complete, because all data points of a class belong
to the same cluster.
For the clustering example in Table 3, homo-
geneity is equal to 0.404, completeness is equal to
0.37 and V-Measure is equal to 0.386.
c =
{
1, if H(K) = 0
1?
H(K|S)
H(K)
, otherwise
(4)
H(K) = ?
|K|
?
j=1
?
|S|
i=1
a
ij
N
log
?
|S|
i=1
a
ij
N
(5)
H(K|S) = ?
|S|
?
i=1
|K|
?
j=1
a
ij
N
log
a
ij
?
|K|
k=1
a
ik
(6)
3.1.2 Paired F-Score evaluation
In this evaluation, the clustering problem is trans-
formed into a classification problem. For each
cluster C
i
we generate
(
|C
i
|
2
)
instance pairs, where
|C
i
| is the total number of instances that belong to
cluster C
i
. Similarly, for each GS class G
i
we gen-
erate
(
|G
i
|
2
)
instance pairs, where |G
i
| is the total
number of instances that belong to GS class G
i
.
Let F (K) be the set of instance pairs that ex-
ist in the automatically induced clusters and F (S)
be the set of instance pairs that exist in the gold
standard. Precision can be defined as the number
of common instance pairs between the two sets to
the total number of pairs in the clustering solu-
tion (Equation 7), while recall can be defined as
the number of common instance pairs between the
two sets to the total number of pairs in the gold
65
standard (Equation 8). Finally, precision and re-
call are combined to produce the harmonic mean
(FS =
2?P ?R
P+R
).
P =
|F (K) ? F (S)|
|F (K)|
(7)
R =
|F (K) ? F (S)|
|F (S)|
(8)
For example in Table 3, we can generate
(
35
2
)
in-
stance pairs for C
1
,
(
70
2
)
for C
2
,
(
71
2
)
for C
3
and
(
5
2
)
for C
4
, resulting in a total of 5505 instance
pairs. In the same vein, we can generate
(
36
2
)
in-
stance pairs for G
1
,
(
70
2
)
for G
2
and
(
75
2
)
for G
3
. In
total, the GS classes contain 5820 instance pairs.
There are 3435 common instance pairs, hence pre-
cision is equal to 62.39%, recall is equal to 59.09%
and paired F-Score is equal to 60.69%.
3.2 Supervised evaluation
In this evaluation, the testing dataset is split into a
mapping and an evaluation corpus. The first one
is used to map the automatically induced clusters
to GS senses, while the second is used to evaluate
methods in a WSD setting. This evaluation fol-
lows the supervised evaluation of SemEval-2007
WSI task (Agirre and Soroa, 2007), with the dif-
ference that the reported results are an average
of 5 random splits. This repeated random sam-
pling was performed to avoid the problems of the
SemEval-2007 WSI challenge, in which different
splits were providing different system rankings.
Let us consider the example in Table 3 and as-
sume that this matrix has been created by using the
mapping corpus. Table 3 shows that C
1
is more
likely to be associated with G
3
, C
2
is more likely
to be associated with G
2
, C
3
is more likely to be
associated with G
3
and C
4
is more likely to be as-
sociated with G
1
. This information can be utilised
to map the clusters to GS senses.
Particularly, the matrix shown in Table 3 is nor-
malised to produce a matrix M , in which each
entry depicts the estimated conditional probabil-
ity P (G
i
|C
j
). Given an instance I of tw from
the evaluation corpus, a row cluster vector IC is
created, in which each entry k corresponds to the
score assigned to C
k
to be the winning cluster for
instance I . The product of IC and M provides a
row sense vector, IG, in which the highest scor-
ing entry a denotes that G
a
is the winning sense.
For example, if we produce the row cluster vector
[C
1
= 0.8, C
2
= 0.1, C
3
= 0.1, C
4
= 0.0], and
System VM (%) VM (%) VM (%) #Cl
(All) (Nouns) (Verbs)
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
KSU KDD 15.7 18 12.4 17.5
Duluth-WSI 9 11.4 5.7 4.15
Duluth-WSI-SVD 9 11.4 5.7 4.15
Duluth-R-110 8.6 8.6 8.5 9.71
Duluth-WSI-Co 7.9 9.2 6 2.49
KCDC-PCGD 7.8 7.3 8.4 2.9
KCDC-PC 7.5 7.7 7.3 2.92
KCDC-PC-2 7.1 7.7 6.1 2.93
Duluth-Mix-Narrow-Gap 6.9 8 5.1 2.42
KCDC-GD-2 6.9 6.1 8 2.82
KCDC-GD 6.9 5.9 8.5 2.78
Duluth-Mix-Narrow-PK2 6.8 7.8 5.5 2.68
Duluth-MIX-PK2 5.6 5.8 5.2 2.66
Duluth-R-15 5.3 5.4 5.1 4.97
Duluth-WSI-Co-Gap 4.8 5.6 3.6 1.6
Random 4.4 4.2 4.6 4
Duluth-R-13 3.6 3.5 3.7 3
Duluth-WSI-Gap 3.1 4.2 1.5 1.4
Duluth-Mix-Gap 3 2.9 3 1.61
Duluth-Mix-Uni-PK2 2.4 0.8 4.7 2.04
Duluth-R-12 2.3 2.2 2.5 2
KCDC-PT 1.9 1 3.1 1.5
Duluth-Mix-Uni-Gap 1.4 0.2 3 1.39
KCDC-GDC 7 6.2 7.8 2.83
MFS 0 0 0 1
Duluth-WSI-SVD-Gap 0 0 0.1 1.02
Table 4: V-Measure unsupervised evaluation
multiply it with the normalised matrix of Table 3,
then we would get a row sense vector in which G
3
would be the winning sense with a score equal to
0.43.
4 Evaluation results
In this section, we present the results of the 26
systems along with two baselines. The first base-
line, Most Frequent Sense (MFS), groups all test-
ing instances of a target word into one cluster. The
second baseline, Random, randomly assigns an in-
stance to one out of four clusters. The number
of clusters of Random was chosen to be roughly
equal to the average number of senses in the GS.
This baseline is executed five times and the results
are averaged.
4.1 Unsupervised evaluation
Table 4 shows the V-Measure (VM) performance
of the 26 systems participating in the task. The last
column shows the number of induced clusters of
each system in the test set.The MFS baseline has a
V-Measure equal to 0, since by definition its com-
pleteness is 1 and homogeneity is 0. All systems
outperform this baseline, apart from one, whose
V-Measure is equal to 0. Regarding the Random
baseline, we observe that 17 perform better, which
indicates that they have learned useful information
better than chance.
Table 4 also shows that V-Measure tends to
favour systems producing a higher number of clus-
66
System FS (%) FS (%) FS (%) #Cl
(All) (Nouns) (Verbs)
MFS 63.5 57.0 72.7 1
Duluth-WSI-SVD-Gap 63.3 57.0 72.4 1.02
KCDC-PT 61.8 56.4 69.7 1.5
KCDC-GD 59.2 51.6 70.0 2.78
Duluth-Mix-Gap 59.1 54.5 65.8 1.61
Duluth-Mix-Uni-Gap 58.7 57.0 61.2 1.39
KCDC-GD-2 58.2 50.4 69.3 2.82
KCDC-GDC 57.3 48.5 70.0 2.83
Duluth-Mix-Uni-PK2 56.6 57.1 55.9 2.04
KCDC-PC 55.5 50.4 62.9 2.92
KCDC-PC-2 54.7 49.7 61.7 2.93
Duluth-WSI-Gap 53.7 53.4 53.9 1.4
KCDC-PCGD 53.3 44.8 65.6 2.9
Duluth-WSI-Co-Gap 52.6 53.3 51.5 1.6
Duluth-MIX-PK2 50.4 51.7 48.3 2.66
UoY 49.8 38.2 66.6 11.54
Duluth-Mix-Narrow-Gap 49.7 47.4 51.3 2.42
Duluth-WSI-Co 49.5 50.2 48.2 2.49
Duluth-Mix-Narrow-PK2 47.8 37.1 48.2 2.68
Duluth-R-12 47.8 44.3 52.6 2
Duluth-WSI-SVD 41.1 37.1 46.7 4.15
Duluth-WSI 41.1 37.1 46.7 4.15
Duluth-R-13 38.4 36.2 41.5 3
KSU KDD 36.9 24.6 54.7 17.5
Random 31.9 30.4 34.1 4
Duluth-R-15 27.6 26.7 28.9 4.97
Hermit 26.7 24.4 30.1 10.78
Duluth-R-110 16.1 15.8 16.4 9.71
Table 5: Paired F-Score unsupervised evaluation
ters than the number of GS senses, although V-
Measure does not increase monotonically with the
number of clusters increasing. For that reason,
we introduced the second unsupervised evaluation
measure (paired F-Score) that penalises systems
when they produce: (1) a higher number of clus-
ters (low recall) or (2) a lower number of clusters
(low precision), than the GS number of senses.
Table 5 shows the performance of systems us-
ing the second unsupervised evaluation measure.
In this evaluation, we observe that most of the sys-
tems perform better than Random. Despite that,
none of the systems outperform the MFS baseline.
It seems that systems generating a smaller number
of clusters than the GS number of senses are bi-
ased towards the MFS, hence they are not able to
perform better. On the other hand, systems gen-
erating a higher number of clusters are penalised
by this measure. Systems generating a number of
clusters roughly the same as the GS tend to con-
flate the GS senses lot more than the MFS.
4.2 Supervised evaluation results
Table 6 shows the results of this evaluation for a
80-20 test set split, i.e. 80% for mapping and 20%
for evaluation. The last columns shows the aver-
age number of GS senses identified by each sys-
tem in the five splits of the evaluation datasets.
Overall, 14 systems outperform the MFS, while 17
of them perform better than Random. The ranking
of systems in nouns and verbs is different. For in-
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.4 59.4 66.8 1.51
Duluth-WSI 60.5 54.7 68.9 1.66
Duluth-WSI-SVD 60.5 54.7 68.9 1.66
Duluth-WSI-Co-Gap 60.3 54.1 68.6 1.19
Duluth-WSI-Co 60.8 54.7 67.6 1.51
Duluth-WSI-Gap 59.8 54.4 67.8 1.11
KCDC-PC-2 59.8 54.1 68.0 1.21
KCDC-PC 59.7 54.6 67.3 1.39
KCDC-PCGD 59.5 53.3 68.6 1.47
KCDC-GDC 59.1 53.4 67.4 1.34
KCDC-GD 59.0 53.0 67.9 1.33
KCDC-PT 58.9 53.1 67.4 1.08
KCDC-GD-2 58.7 52.8 67.4 1.33
Duluth-WSI-SVD-Gap 58.7 53.2 66.7 1.01
MFS 58.7 53.2 66.6 1
Duluth-R-12 58.5 53.1 66.4 1.25
Hermit 58.3 53.6 65.3 2.06
Duluth-R-13 58.0 52.3 66.4 1.46
Random 57.3 51.5 65.7 1.53
Duluth-R-15 56.8 50.9 65.3 1.61
Duluth-Mix-Narrow-Gap 56.6 48.1 69.1 1.43
Duluth-Mix-Narrow-PK2 56.1 47.5 68.7 1.41
Duluth-R-110 54.8 48.3 64.2 1.94
KSU KDD 52.2 46.6 60.3 1.69
Duluth-MIX-PK2 51.6 41.1 67.0 1.23
Duluth-Mix-Gap 50.6 40.0 66.0 1.01
Duluth-Mix-Uni-PK2 19.3 1.8 44.8 0.62
Duluth-Mix-Uni-Gap 18.7 1.6 43.8 0.56
Table 6: Supervised recall (SR) (test set split:80%
mapping, 20% evaluation)
stance, the highest ranked system in nouns is UoY,
while in verbs Duluth-Mix-Narrow-Gap. It seems
that depending on the part-of-speech of the target
word, different algorithms, features and parame-
ters? tuning have different impact.
The supervised evaluation changes the distri-
bution of clusters by mapping each cluster to a
weighted vector of senses. Hence, it can poten-
tially favour systems generating a high number of
homogeneous clusters. For that reason, we applied
a second testing set split, where 60% of the testing
corpus was used for mapping and 40% for eval-
uation. Reducing the size of the mapping corpus
allows us to observe, whether the above statement
is correct, since systems with a high number of
clusters would suffer from unreliable mapping.
Table 7 shows the results of the second super-
vised evaluation. The ranking of participants did
not change significantly, i.e. we observe only dif-
ferent rankings among systems belonging to the
same participant. Despite that, Table 7 also shows
that the reduction of the mapping corpus has a dif-
ferent impact on systems generating a larger num-
ber of clusters than the GS number of senses.
For instance, UoY that generates 11.54 clusters
outperformed the MFS by 3.77% in the 80-20 split
and by 3.71% in the 60-40 split. The reduction of
the mapping corpus had a minimal impact on its
performance. In contrast, KSU KDD that gener-
ates 17.5 clusters was below the MFS by 6.49%
67
System SR (%) SR (%) SR (%) #S
(All) (Nouns) (Verbs)
UoY 62.0 58.6 66.8 1.66
Duluth-WSI-Co 60.1 54.6 68.1 1.56
Duluth-WSI-Co-Gap 59.5 53.5 68.3 1.2
Duluth-WSI-SVD 59.5 53.5 68.3 1.73
Duluth-WSI 59.5 53.5 68.3 1.73
Duluth-WSI-Gap 59.3 53.2 68.2 1.11
KCDC-PCGD 59.1 52.6 68.6 1.54
KCDC-PC-2 58.9 53.4 67.0 1.25
KCDC-PC 58.9 53.6 66.6 1.44
KCDC-GDC 58.3 52.1 67.3 1.41
KCDC-GD 58.3 51.9 67.6 1.42
MFS 58.3 52.5 66.7 1
KCDC-PT 58.3 52.2 67.1 1.11
Duluth-WSI-SVD-Gap 58.2 52.5 66.7 1.01
KCDC-GD-2 57.9 51.7 67.0 1.44
Duluth-R-12 57.7 51.7 66.4 1.27
Duluth-R-13 57.6 51.1 67.0 1.48
Hermit 57.3 52.5 64.2 2.27
Duluth-R-15 56.5 50.0 66.1 1.76
Random 56.5 50.2 65.7 1.65
Duluth-Mix-Narrow-Gap 56.2 47.7 68.6 1.51
Duluth-Mix-Narrow-PK2 55.7 46.9 68.5 1.51
Duluth-R-110 53.6 46.7 63.6 2.18
Duluth-MIX-PK2 50.5 39.7 66.1 1.31
KSU KDD 50.4 44.3 59.4 1.92
Duluth-Mix-Gap 49.8 38.9 65.6 1.04
Duluth-Mix-Uni-PK2 19.1 1.8 44.4 0.63
Duluth-Mix-Uni-Gap 18.9 1.5 44.2 0.56
Table 7: Supervised recall (SR) (test set split:60%
mapping, 40% evaluation)
in the 80-20 split and by 7.83% in the 60-40 split.
The reduction of the mapping corpus had a larger
impact in this case. This result indicates that the
performance in this evaluation also depends on the
distribution of instances within the clusters. Sys-
tems generating a skewed distribution, in which a
small number of homogeneous clusters tag the ma-
jority of instances and a larger number of clusters
tag only a few instances, are likely to have a bet-
ter performance than systems that produce a more
uniform distribution.
5 Conclusion
We presented the description, evaluation frame-
work and assessment of systems participating in
the SemEval-2010 sense induction task. The eval-
uation has shown that the current state-of-the-art
lacks unbiased measures that objectively evaluate
clustering.
The results of systems have shown that their
performance in the unsupervised and supervised
evaluation settings depends on cluster granularity
along with the distribution of instances within the
clusters. Our future work will focus on the assess-
ment of sense induction on a task-oriented basis as
well as on clustering evaluation.
Acknowledgements
We gratefully acknowledge the support of the EU
FP7 INDECT project, Grant No. 218086, the Na-
tional Science Foundation Grant NSF-0715078,
Consistent Criteria for Word Sense Disambigua-
tion, and the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-
AGILE Team.
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating Word Sense Induction and Dis-
crimination Systems. In Proceedings of SemEval-
2007, pages 7?12, Prague, Czech Republic. ACL.
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching Wordnet Concepts With
Topic Signatures. ArXiv Computer Science e-prints.
Javier Artiles, Enrique Amig?o, and Julio Gonzalo.
2009. The role of named entities in web people
search. In Proceedings of EMNLP, pages 534?542.
ACL.
Christiane Fellbaum. 1998. Wordnet: An Electronic
Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL, Com-
panion Volume: Short Papers on XX, pages 57?60.
ACL.
Suresh Manandhar and Ioannis P. Klapaftis. 2009.
Semeval-2010 Task 14: Evaluation Setting for Word
Sense Induction & Disambiguation Systems. In
DEW ?09: Proceedings of the Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions, pages 117?122, Boulder, Colorado,
USA. ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text. In KDD ?02: Proceedings
of the 8th ACM SIGKDD Conference, pages 613?
619, New York, NY, USA. ACM.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A Conditional Entropy-based External
Cluster Evaluation Measure. In Proceedings of the
2007 EMNLP-CoNLL Joint Conference, pages 410?
420, Prague, Czech Republic.
Yoshimasa Tsuruoka and Jun??chi Tsujii. 2005. Bidi-
rectional Inference With the Easiest-first Strategy
for Tagging Sequence Data. In Proceedings of
the HLT-EMNLP Joint Conference, pages 467?474,
Morristown, NJ, USA.
Jean V?eronis. 2004. Hyperlex: Lexical Cartography
for Information Retrieval. Computer Speech & Lan-
guage, 18(3):223?252.
68
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 355?358,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
UoY: Graphs of Unambiguous Vertices
for Word Sense Induction and Disambiguation
Ioannis Korkontzelos, Suresh Manandhar
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
{johnkork, suresh}@cs.york.ac.uk
Abstract
This paper presents an unsupervised
graph-based method for automatic word
sense induction and disambiguation. The
innovative part of our method is the as-
signment of either a word or a word pair
to each vertex of the constructed graph.
Word senses are induced by clustering the
constructed graph. In the disambiguation
stage, each induced cluster is scored ac-
cording to the number of its vertices found
in the context of the target word. Our sys-
tem participated in SemEval-2010 word
sense induction and disambiguation task.
1 Introduction
There exists significant evidence that word sense
disambiguation is important for a variety of nat-
ural language processing tasks: machine transla-
tion, information retrieval, grammatical analysis,
speech and text processing (Veronis, 2004). How-
ever, the ?fixed-list? of senses paradigm, where the
senses of a target word is a closed list of defini-
tions coming from a standard dictionary (Agirre
et al, 2006), was long ago abandoned. The rea-
son is that sense lists, such as WordNet (Miller,
1995), miss many senses, especially domain-
specific ones (Pantel and Lin, 2002). The miss-
ing concepts are not recognised. Moreover, senses
cannot be easily related to their use in context.
Word sense induction methods can be divided
into vector-space models and graph based ones.
In a vector-space model, each context of a target
word is represented as a feature vector, e.g. fre-
quency of cooccurring words (Katz and Gies-
brecht, 2006). Context vectors are clustered and
the resulting clusters represent the induced senses.
Recently, graph-based methods have been em-
ployed for word sense induction (Agirre and
Soroa, 2007). Typically, graph-based methods
represent each context word of the target word as
a vertex. Two vertices are connected via an edge
if they cooccur in one or more instances. Once
the cooccurrence graph has been constructed, dif-
ferent graph clustering algorithms are applied to
partition the graph. Each cluster (partition) con-
sists of a set of words that are semantically related
to the particular sense (Veronis, 2004). The poten-
tial advantage of graph-based methods is that they
can combine both local and global cooccurrence
information (Agirre et al, 2006).
Klapaftis and Manandhar (2008) presented a
graph-based approach that represents pairs of
words as vertices instead of single words. They
claimed that single words might appear with more
than one senses of the target word, while they hy-
pothesize that a pair of words is unambiguous.
Hard-clustering the graph will potentially identify
less conflating senses of the target word.
In this paper, we relax the above hypothesis be-
cause in some cases a single word is unambiguous.
We present a method that generates two-word ver-
tices only when a single word vertex is unambigu-
ous. If the word is judged as unambiguous, then it
is represented as a single-word vertex. Otherwise,
it is represented as a pair-of-words vertex.
The approach of Klapaftis and Manandhar
(2008) achieved good results in both evaluation
settings of the SemEval-2007 task. A test in-
stance is disambiguated towards one of the in-
duced senses if one or more pairs of words rep-
resenting that sense cooccur in the test instance.
This creates a sparsity problem, because a cooc-
currence of two words is generally less likely than
the occurrence of a single word. We expect our ap-
proach to address the data sparsity problem with-
out conflating the induced senses.
2 Word Sense Induction
In this section we present our word sense in-
duction and disambiguation algorithms. Figure
355
1 shows an example showing how the sense in-
duction algorithm works: The left side of part
I shows the context nouns of four snippets con-
taining the target noun ?chip?. The most rele-
vant of these nouns are represented as single word
vertices (part II). Note that ?customer? was not
judged to be significantly relevant. In addition,
the system introduced several vertices represent-
ing pairs of nouns. For example, note the vertex
?company potato?. The set of sentences contain-
ing the context word ?company? was judged as
very different from the set of sentences contain-
ing ?company? and ?potato?. Thus, our system
hypothesizes that probably ?company? and ?com-
pany potato? are relevant to different senses of
?chip?, and allows them to be clustered accord-
ingly. Vertices whose content nouns or pairs of
nouns cooccur in some snippet are connected with
an edge (part III and right side of part I). Edge
weights depend upon the conditional probabilities
of the occurrence frequencies of the vertex con-
tents in a large corpus, e.g. w
2,6
in part III. Hard-
clustering the graph produces the induced senses
of ?chip?: (a) potato crisp, and (b) microchip.
In the following subsections, the system is de-
scribed in detail. Figure 2 shows a block diagram
overview of the sense induction system. It consists
of three main components: (a) corpus preprocess-
ing, (b) graph construction, and (c) clustering.
In a number of different stages, the system uses
a reference corpus to count occurrences of word
or word pairs. It is chosen to be large because fre-
quencies of words in a large corpus are more sig-
nificant statistically. Ideally we would use the web
or another large repository, but for the purposes of
the SemEval-2010 task we used the union of all
snippets of all target words.
2.1 Corpus Preprocessing
Corpus preprocessing aims to capture words that
are contextually related to the target word. Ini-
tially, all snippets
1
that contain the target word are
lemmatised and PoS tagged using the GENIA tag-
ger
2
. Words that occur in a stoplist are filtered out.
Instead of using all words as context, only nouns
are kept, since they are more discriminative than
verbs, adverbs and adjectives, that appear in a va-
riety of different contexts.
1
We refer to instances of the target word as snippets, since
they can be either sentences or paragraphs.
2
www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger
Figure 1: An example showing how the proposed
word sense induction system works.
Nouns that occur infrequently in the reference
corpus are removed (parameter P
1
). Then, log-
likelihood ratio (LL) (Dunning, 1993) is em-
ployed to compare the distribution of each noun
to its distribution in reference corpus. The null
hypothesis is that the two distributions are simi-
lar. If this is true, LL is small value and the cor-
responding noun is removed (parameter P
2
). We
also filter out nouns that are more indicative in the
reference corpus than in the target word corpus;
i.e. the nouns whose relative frequency in the for-
mer is larger than in the latter. At the end of this
stage, each snippet is a list of lemmatised nouns
contextually related to the target word.
2.2 Constructing the Graph
All nouns appearing in the list of the previous
stage output are represented as graph vertices.
Moreover, some vertices representing pairs of
nouns are added. Each noun within a snippet is
combined with every other, generating
(
n
2
)
pairs.
Log-likelihood filtering with respect to the refer-
ence corpus is used to filter out unimportant pairs.
Thereafter, we aim to keep only pairs that might
refer to a different sense of the target word than
their component nouns. For each pair we construct
a vector containing the snippet IDs in which they
occur. Similarly we construct a vector for each
component noun. We discard a pair if its vector is
very similar to both the vectors of its component
nouns, otherwise we represent it as a vertex pair.
Dice coefficient was used as a similarity measure
and parameter P
4
as threshold value.
Edges are drawn based on cooccurrence of the
corresponding vertices contents in one or more
snippets. Edges whose respective vertices con-
tents are infrequent are rejected. The weight ap-
356
Figure 2: A: Block diagram presenting the system overview. B, C, D: Block diagrams further analysing
the structure of complex components of A. Parameter names appear within square brackets.
plied to each edge is the maximum of the condi-
tional probabilities of the corresponding vertices
contents (e.g. w
2,6
, part III, figure 1). Low weight
edges are filtered out (parameter P
3
).
2.3 Clustering the Graph
Chinese Whispers (CW) (Biemann, 2006) was
used to cluster the graph. CW is a randomised
graph-clustering algorithm, time-linear to the
number of edges. The number of clusters it pro-
duces is automatically inferred. Evaluation has
shown that CW suits well in sense induction appli-
cations, where class distributions are often highly
skewed. In our experiments, CW produced less
clusters using a constant mutation rate (5%).
To further reduce the number of induced clus-
ters, we applied a post-processing stage, which
exploits the one sense per collocation property
(Yarowsky, 1995). For each cluster l
i
, we gener-
ated the set S
i
of all snippets that contain at least
one vertex content of l
i
. Then, any clusters l
a
and
l
b
were merged if S
a
? S
b
or S
a
? S
b
.
3 Word Sense Disambiguation
The induced senses are used to sense-tag each test
instance of the target word (snippet). Given a snip-
pet, each induced cluster is assigned a score equal
to the number of its vertex contents (single or pairs
of words) occurring in the snippet. The instance is
assigned to the sense with the highest score or with
equal weights to all highest scoring senses.
4 Tuning parameter and inducing senses
The algorithm depends upon 4 parameters: P
1
thresholds frequencies and P
3
collocation weights.
P
2
is the LL threshold and P
4
the similarity thresh-
old for discarding pair-of-nouns vertices.
We chose P
1
? {5, 10, 15}, P
2
?
{2, 3, 4, 5, 10, 15, 25, 35}, P
3
? {0.2, 0.3, 0.4}
and P
4
? {0.2, 0.4, 0.6, 0.8}. The parameter tun-
ing was done using the trial data of the SemEval-
2010 task and on the noun data of correspond-
ing SemEval-2007 task. Parameters were tuned
by choosing the maximum supervised recall. For
both data sets, the chosen parameter values were
P
1
? 10, P
3
? 0.4 and P
4
? 0.8. Due to the
size difference of the datasets, for the Semeval-
2010 trial data P
2
? 3, while for the SemEval-
2007 noun data P
2
? 10. The latter was adopted
because the size of training data was announced to
be large. We induced senses on the training data
and then disambiguated the test data instances.
5 Evaluation results
Three different measures, V-Measure, F-Score,
and supervised recall on word sense disambigua-
tion task, were used for evaluation. V-Measure
and F-Score are unsupervised. Supervised recall
was measured on two different data splits. Table 1
shows the performance of our system, UoY, for all
measures and in comparison with the best, worst
and average performing system and the random
and most frequent sense (MFS) baselines. Results
are shown for all words, and nouns and verbs only.
357
System V-Msr F-Sc S-R
80
S-R
60
A
l
l
UoY 15.70 49.76 62.44 61.96
Best 16.20 63.31 62.44 61.96
Worst 0.00 16.10 18.72 18.91
Average 6.36 48.72 54.95 54.27
MFS 0.00 63.40 58.67 58.25
Random 4.40 31.92 57.25 56.52
N
o
u
n
s
UoY 20.60 38.23 59.43 58.62
Best 20.60 57.10 59.43 58.62
Average 7.08 44.42 47.85 46.90
Worst 0.00 15.80 1.55 1.52
MFS 0.00 57.00 53.22 52.45
Random 4.20 30.40 51.45 50.21
V
e
r
b
s
UoY 8.50 66.55 66.82 66.82
Best 15.60 72.40 69.06 68.59
Average 5.95 54.23 65.25 65.00
Worst 0.10 16.40 43.76 44.23
MFS 0.00 72.70 66.63 66.70
Random 4.64 34.10 65.69 65.73
Table 1: Summary of results (%). V-Msr: V-
Measure, F-Sc: F-Score, S-R
X
: Supervised recall
under data split: X% training, (100-X)% test
Table 2 shows the ranks of UoY for all evalu-
ation categories. Our system was generally very
highly ranked. It outperformed the random base-
line in all cases and the MFS baseline in measures
but F-Score. No participant system managed to
achive higher F-Score than the MFS baseline.
The main disadvantage of the system seems to
be the large number of induced senses. The rea-
sons are data sparcity and tuning on nouns, that
might have led to parameters that induce more
senses. However, the system performs best among
systems that produce comparable numbers of clus-
ters. Table 3 shows the number of senses of UoY
and the gold-standard. UoY produces significantly
more senses than the gold-standard, especially for
nouns, while for verbs figures are similar.
The system achieves low F-Scores, because this
measure favours fewer induced senses. Moreover,
we observe that most scores are lower for verbs
than nouns. This is probably because parameters
are tuned on nouns and because in general nouns
appear with more senses than verbs, allowing our
system to adapt better. As an overall conclusion,
each evaluation measure is more or less biased to-
wards small or large numbers of induced senses.
6 Conclusion
We presented a graph-based approach for word
sense induction and disambiguation. Our ap-
proach represents as a graph vertex an unambigu-
ous unit: (a) a single word, if it is judged as unam-
biguous, or (b) a pair of words, otherwise. Graph
edges model the cooccurrences of the content of
V-Msr F-Sc S-R
80
S-R
60
All 2 15 1 1
Nouns|Verbs 1|3 18|6 1|16 1|15
Table 2: Ranks of UoY (out of 26 systems)
All Nouns Verbs
Gold-standard 3.79 4.46 3.12
UoY 11.54 17.32 5.76
Table 3: Number of senses
the vertices that they join. Hard-clustering the
graph induces a set of senses. To disambiguate
a test instance, we assign it to the induced sense
whose vertices contents occur mostly in the in-
stance. Results show that our system achieves very
high recall and V-measure performance, higher
than both baselines. It achieves low F-Scores due
to the large number of induced senses.
References
E. Agirre and A. Soroa. 2007. Semeval-2007 task 02:
Evaluating word sense induction and discrimination
systems. In proceedings of SemEval-2007, Czech
Republic. ACL.
E. Agirre, D. Martinez, O. Lopez de Lacalle, and
A. Soroa. 2006. Two graph-based algorithms for
state-of-the-art wsd. In proceedings of EMNLP,
Sydney, Australia. ACL.
C. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In proceedings
of TextGraphs, New York City. ACL.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
G. Katz and E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional multi-word expressions
using latent semantic analysis. In proceedings of the
ACL workshop on Multi-Word Expressions, Sydney,
Australia. ACL.
I. Klapaftis and S. Manandhar. 2008. Word sense in-
duction using graphs of collocations. In proceedings
of ECAI-2008, Patras, Greece.
G. Miller. 1995. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38(11):39?41.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In proceedings of KDD-2002, New York,
NY, USA. ACM Press.
J. Veronis. 2004. Hyperlex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252, July.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In proceed-
ings of ACL, Cambridge, MA, USA. ACL.
358
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27?35,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 4: Aspect Based Sentiment Analysis
Maria Pontiki
Institute for Language
and Speech Processing,
?Athena? Research Center
mpontiki@ilsp.gr
Haris Papageorgiou
Institute for Language
and Speech Processing,
?Athena? Research Center
xaris@ilsp.gr
Dimitrios Galanis
Institute for Language
and Speech Processing,
?Athena? Research Center
galanisd@ilsp.gr
Ion Androutsopoulos
Dept. of Informatics
Athens University of
Economics and Business
ion@aueb.gr
John Pavlopoulos
Dept. of Informatics,
Athens University of
Economics and Business
annis@aueb.gr
Suresh Manandhar
Dept. of Computer Science,
University of York
suresh@cs.york.ac.uk
Abstract
Sentiment analysis is increasingly viewed
as a vital task both from an academic and
a commercial standpoint. The majority of
current approaches, however, attempt to
detect the overall polarity of a sentence,
paragraph, or text span, irrespective of the
entities mentioned (e.g., laptops) and their
aspects (e.g., battery, screen). SemEval-
2014 Task 4 aimed to foster research in the
field of aspect-based sentiment analysis,
where the goal is to identify the aspects
of given target entities and the sentiment
expressed for each aspect. The task pro-
vided datasets containing manually anno-
tated reviews of restaurants and laptops, as
well as a common evaluation procedure. It
attracted 163 submissions from 32 teams.
1 Introduction
With the proliferation of user-generated content on
the web, interest in mining sentiment and opinions
in text has grown rapidly, both in academia and
business. Early work in sentiment analysis mainly
aimed to detect the overall polarity (e.g., positive
or negative) of a given text or text span (Pang et
al., 2002; Turney, 2002). However, the need for a
more fine-grained approach, such as aspect-based
(or ?feature-based?) sentiment analysis (ABSA),
soon became apparent (Liu, 2012). For example,
laptop reviews not only express the overall senti-
ment about a specific model (e.g., ?This is a great
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
laptop?), but also sentiments relating to its spe-
cific aspects, such as the hardware, software, price,
etc. Subsequently, a review may convey opposing
sentiments (e.g., ?Its performance is ideal, I wish
I could say the same about the price?) or objective
information (e.g., ?This one still has the CD slot?)
for different aspects of an entity.
ABSA is critical in mining and summarizing
opinions from on-line reviews (Gamon et al.,
2005; Titov and McDonald, 2008; Hu and Liu,
2004a; Popescu and Etzioni, 2005). In this set-
ting, ABSA aims to identify the aspects of the en-
tities being reviewed and to determine the senti-
ment the reviewers express for each aspect. Within
the last decade, several ABSA systems of this kind
have been developed for movie reviews (Thet et
al., 2010), customer reviews of electronic products
like digital cameras (Hu and Liu, 2004a) or net-
book computers (Brody and Elhadad, 2010), ser-
vices (Long et al., 2010), and restaurants (Ganu et
al., 2009; Brody and Elhadad, 2010).
Previous publicly available ABSA benchmark
datasets adopt different annotation schemes within
different tasks. The restaurant reviews dataset of
Ganu et al. (2009) uses six coarse-grained aspects
(e.g., FOOD, PRICE, SERVICE) and four overall
sentence polarity labels (positive, negative, con-
flict, neutral). Each sentence is assigned one or
more aspects together with a polarity label for
each aspect; for example, ?The restaurant was ex-
pensive, but the menu was great.? would be as-
signed the aspect PRICE with negative polarity and
FOOD with positive polarity. In the product re-
views dataset of Hu and Liu (2004a; 2004b), as-
pect terms, i.e., terms naming aspects (e.g., ?ra-
dio?, ?voice dialing?) together with strength scores
(e.g., ?radio?: +2, ?voice dialing?: ?3) are pro-
27
vided. No predefined inventory of aspects is pro-
vided, unlike the dataset of Ganu et al.
The SemEval-2014 ABSA Task is based on lap-
top and restaurant reviews and consists of four
subtasks (see Section 2). Participants were free to
participate in a subset of subtasks and the domains
(laptops or restaurants) of their choice.
2 Task Description
For the first two subtasks (SB1, SB2), datasets on
both domains (restaurants, laptops) were provided.
For the last two subtasks (SB3, SB4), datasets only
for the restaurant reviews were provided.
Aspect term extraction (SB1): Given a set of
review sentences, the task is to identify all as-
pect terms present in each sentence (e.g., ?wine?,
?waiter?, ?appetizer?, ?price?, ?food?). We require
all the aspect terms to be identified, including as-
pect terms for which no sentiment is expressed
(neutral polarity). These will be useful for con-
structing an ontology of aspect terms and to iden-
tify frequently discussed aspects.
Aspect term polarity (SB2): In this subtask,
we assume that the aspect terms are given (as de-
scribed in SB1) and the task is to determine the po-
larity of each aspect term (positive, negative, con-
flict, or neutral). The conflict label applies when
both positive and negative sentiment is expressed
about an aspect term (e.g., ?Certainly not the best
sushi in New York, however, it is always fresh?).
An alternative would have been to tag the aspect
term in these cases with the dominant polarity, but
this in turn would be difficult to agree on.
Aspect category detection (SB3): Given a
predefined set of aspect categories (e.g., PRICE,
FOOD) and a set of review sentences (but without
any annotations of aspect terms and their polari-
ties), the task is to identify the aspect categories
discussed in each sentence. Aspect categories are
typically coarser than the aspect terms as defined
in SB1, and they do not necessarily occur as terms
in the sentences. For example, in ?Delicious but
expensive?, the aspect categories FOOD and PRICE
are not instantiated through specific aspect terms,
but are only inferred through the adjectives ?deli-
cious? and ?expensive?. SB1 and SB3 were treated
as separate subtasks, thus no information linking
aspect terms to aspect categories was provided.
Aspect category polarity (SB4): For this sub-
task, aspect categories for each review sentence
are provided. The goal is to determine the polar-
ity (positive, negative, conflict, or neutral) of each
aspect category discussed in each sentence.
Subtasks SB1 and SB2 are useful in cases where
no predefined inventory of aspect categories is
available. In these cases, frequently discussed as-
pect terms of the entity can be identified together
with their overall sentiment polarities. We hope to
include an additional aspect term aggregation sub-
task in future (Pavlopoulos and Androutsopoulos,
2014b) to cluster near-synonymous (e.g., ?money?,
?price?, ?cost?) or related aspect terms (e.g., ?de-
sign?, ?color?, ?feeling?) together with their aver-
aged sentiment scores as shown in Fig. 1.
Figure 1: Aggregated aspect terms and average
sentiment polarities for a target entity.
Subtasks SB3 and SB4 are useful when a pre-
defined inventory of (coarse) aspect categories is
available. A table like the one of Fig. 1 can then
also be generated, but this time using the most
frequent aspect categories to label the rows, with
stars showing the proportion of reviews express-
ing positive vs. negative opinions for each aspect
category.
3 Datasets
3.1 Data Collection
The training and test data sizes are provided in Ta-
ble 1. The restaurants training data, consisting of
3041 English sentences, is a subset of the dataset
from Ganu et al. (2009), which included annota-
tions for coarse aspect categories (as in SB3) and
overall sentence polarities. We added annotations
for aspect terms occurring in the sentences (SB1),
aspect term polarities (SB2), and aspect category
polarities (SB4). Additional restaurant reviews
were collected and annotated (from scratch) in
the same manner and used as test data (800 sen-
tences). The laptops dataset contains 3845 English
28
sentences extracted from laptop custumer reviews.
Human annotators tagged the aspect terms (SB1)
and their polarities (SB2); 3045 sentences were
used for training and 800 for testing (evaluation).
Domain Train Test Total
Restaurants 3041 800 3841
Laptops 3045 800 3845
Total 6086 1600 7686
Table 1: Sizes (sentences) of the datasets.
3.2 Annotation Process
For a given target entity (a restaurant or a lap-
top) being reviewed, the annotators were asked to
provide two types of information: aspect terms
(SB1) and aspect term polarities (SB2). For the
restaurants dataset, two additional annotation lay-
ers were added: aspect category (SB3) and aspect
category polarity (SB4).
The annotators used BRAT (Stenetorp et al.,
2012), a web-based annotation tool, which was
configured appropriately for the needs of the
ABSA task.
1
Figure 2 shows an annotated sen-
tence in BRAT, as viewed by the annotators.
Stage 1: Aspect terms and polarities. During
a first annotation stage, the annotators tagged all
the single or multiword terms that named par-
ticular aspects of the target entity (e.g., ?I liked
the service and the staff, but not the food? ?
{?service?, ?staff?, ?food?}, ?The hard disk is very
noisy?? {?hard disk?}). They were asked to tag
only aspect terms explicitly naming particular as-
pects (e.g., ?everything about it? or ?it?s expen-
sive? do not name particular aspects). The as-
pect terms were annotated as they appeared, even
if misspelled (e.g., ?warrenty? instead of ?war-
ranty?). Each identified aspect term also had to be
assigned a polarity label (positive, negative, neu-
tral, conflict). For example, ?I hated their fajitas,
but their salads were great? ? {?fajitas?: nega-
tive, ?salads?: positive}, ?The hard disk is very
noisy?? {?hard disk?: negative}.
Each sentence of the two datasets was anno-
tated by two annotators, a graduate student (an-
notator A) and an expert linguist (annotator B).
Initially, two subsets of sentences (300 from each
dataset) were tagged by annotator A and the anno-
tations were inspected and validated by annotator
1
Consult http://brat.nlplab.org/ for more in-
formation about BRAT.
B. The disagreements between the two annotators
were confined to borderline cases. Taking into ac-
count the types of these disagreements (discussed
below), annotator A was provided with additional
guidelines and tagged the remainder of the sen-
tences in both datasets.
2
When A was not confi-
dent, a decision was made collaboratively with B.
When A and B disagreed, a decision was made
collaboratively by them and a third expert annota-
tor. Most disagreements fall into one of the fol-
lowing three types:
Polarity ambiguity: In several sentences, it was
unclear if the reviewer expressed positive or neg-
ative opinion, or no opinion at all (just reporting
a fact), due to lack of context. For example, in
?12.44 seconds boot time? it is unclear if the re-
viewer expresses a positive, negative, or no opin-
ion about the aspect term ?boot time?. In future
challenges, it would be better to allow the annota-
tors (and the participating systems) to consider the
entire review instead of each sentence in isolation.
Multi-word aspect term boundaries: In sev-
eral cases, the annotators disagreed on the exact
boundaries of multi-word aspect terms when they
appeared in conjunctions or disjunctions (e.g.,
?selection of meats and seafoods?, ?noodle and
rices dishes?, ?school or office use?). In such
cases, we asked the annotators to tag as a sin-
gle aspect term the maximal noun phrase (the en-
tire conjunction or disjunction). Other disagree-
ments concerned the extent of the aspect terms
when adjectives that may or may not have a sub-
jective meaning were also present. For example,
if ?large? in ?large whole shrimp? is part of the
dish name, then the guidelines require the adjec-
tive to be included in the aspect term; otherwise
(e.g., in ?large portions?) ?large? is a subjectivity
indicator not to be included in the aspect term. De-
spite the guidelines, in some cases it was difficult
to isolate and tag the exact aspect term, because of
intervening words, punctuation, or long-term de-
pendencies.
Aspect term vs. reference to target entity: In
some cases, it was unclear if a noun or noun phrase
was used as the aspect term or if it referred to the
entity being reviewed as whole. In ?This place
is awesome?, for example, ?place? most probably
refers to the restaurant as a whole (hence, it should
not be tagged as an aspect term), but in ?Cozy
2
The guidelines are available at: http://alt.qcri.
org/semeval2014/task4/data/uploads/.
29
Figure 2: A sentence in the BRAT tool, annotated with four aspect terms (?appetizers?, ?salads?, ?steak?,
?pasta?) and one aspect category (FOOD). For aspect categories, the whole sentence is tagged.
place and good pizza? it probably refers to the am-
bience of the restaurant. A broader context would
again help in some of these cases.
We note that laptop reviews often evaluate each
laptop as a whole, rather than expressing opinions
about particular aspects. Furthermore, when they
express opinions about particular aspects, they of-
ten do so by using adjectives that refer implicitly
to aspects (e.g., ?expensive?, ?heavy?), rather than
using explicit aspect terms (e.g., ?cost?, ?weight?);
the annotators were instructed to tag only explicit
aspect terms, not adjectives implicitly referring to
aspects. By contrast, restaurant reviews contain
many more aspect terms (Table 2, last column).
3
Dataset Pos. Neg. Con. Neu. Tot.
LPT-TR 987 866 45 460 2358
LPT-TE 341 128 16 169 654
RST-TR 2164 805 91 633 3693
RST-TE 728 196 14 196 1134
Table 2: Aspect terms and their polarities per do-
main. LPT and RST indicate laptop and restau-
rant reviews, respectively. TR and TE indicate the
training and test set.
Another difference between the two datasets
is that the neutral class is much more frequent
in (the aspect terms of) laptops, since laptop re-
views often mention features without expressing
any (clear) sentiment (e.g., ?the latest version does
not have a disc drive?). Nevertheless, the positive
class is the majority in both datasets, but it is much
more frequent in restaurants (Table 2). The ma-
jority of the aspect terms are single-words in both
datasets (2148 in laptops, 4827 in restaurants, out
of 3012 and 4827 total aspect terms, respectively).
Stage 2: Aspect categories and polarities. In
this task, each sentence needs to be tagged with
the aspect categories discussed in the sentence.
The aspect categories are FOOD, SERVICE, PRICE,
AMBIENCE (the atmosphere and environment of
3
We count aspect term occurrences, not distinct terms.
a restaurant), and ANECDOTES/MISCELLANEOUS
(sentences not belonging in any of the previous
aspect categories).
4
For example, ?The restau-
rant was expensive, but the menu was great? is
assigned the aspect categories PRICE and FOOD.
Additionally, a polarity (positive, negative, con-
flict, neutral) for each aspect category should be
provided (e.g., ?The restaurant was expensive, but
the menu was great?? {PRICE: negative, FOOD:
positive}.
One annotator validated the existing aspect cat-
egory annotations of the corpus of Ganu et al.
(2009). The agreement with the existing anno-
tations was 92% measured as average F
1
. Most
disagreements concerned additions of missing as-
pect category annotations. Furthermore, the same
annotator validated and corrected (if needed) the
existing polarity labels per aspect category anno-
tation. The agreement for the polarity labels was
87% in terms of accuracy and it was measured
only on the common aspect category annotations.
The additional 800 sentences (not present in Ganu
et al.?s dataset) were used for testing and were an-
notated from scratch in the same manner. The dis-
tribution of the polarity classes per category is pre-
sented in Table 3. Again, ?positive? is the majority
polarity class while the dominant aspect category
is FOOD in both the training and test restaurant
sentences.
Determining the aspect categories of the sen-
tences and their polarities (Stage 2) was an easier
task compared to detecting aspect terms and their
polarities (Stage 1). The annotators needed less
time in Stage 2 and it was easier to reach agree-
ment. Exceptions were some sentences where it
was difficult to decide if the categories AMBIENCE
or ANECDOTES/MISCELLANEOUS applied (e.g.,
?One of my Fav spots in the city?). We instructed
the annotators to classify those sentences only in
ANECDOTES/MISCELLANEOUS, if they conveyed
4
In the original dataset of Ganu et al. (2009), ANECDOTES
and MISCELLANEOUS were separate categories, but in prac-
tice they were difficult to distinguish and we merged them.
30
Positive Negative Conflict Neutral Total
Category Train Test Train Test Train Test Train Test Train Test
FOOD 867 302 209 69 66 16 90 31 1232 418
PRICE 179 51 115 28 17 3 10 1 321 83
SERVICE 324 101 218 63 35 5 20 3 597 172
AMBIENCE 263 76 98 21 47 13 23 8 431 118
ANECD./MISC. 546 127 199 41 30 15 357 51 1132 234
Total 2179 657 839 159 163 52 500 94 3713 1025
Table 3: Aspect categories distribution per sentiment class.
general views about a restaurant, without explic-
itly referring to its atmosphere or environment.
3.3 Format and Availability of the Datasets
The datasets of the ABSA task were provided in
an XML format (see Fig. 3). They are avail-
able with a non commercial, no redistribution li-
cense through META-SHARE, a repository de-
voted to the sharing and dissemination of language
resources (Piperidis, 2012).
5
4 Evaluation Measures and Baselines
The evaluation of the ABSA task ran in two
phases. In Phase A, the participants were asked
to return the aspect terms (SB1) and aspect cate-
gories (SB3) for the provided test datasets. Subse-
quently, in Phase B, the participants were given
the gold aspect terms and aspect categories (as
in Fig. 3) for the sentences of Phase A and they
were asked to return the polarities of the aspect
terms (SB2) and the polarities of the aspect cate-
gories of each sentence (SB4).
6
Each participat-
ing team was allowed to submit up to two runs
per subtask and domain (restaurants, laptops) in
each phase; one constrained (C), where only the
provided training data and other resources (e.g.,
publicly available lexica) excluding additional an-
notated sentences could be used, and one uncon-
strained (U), where additional data of any kind
could be used for training. In the latter case, the
teams had to report the resources they used.
To evaluate aspect term extraction (SB1) and as-
pect category detection (SB3) in Phase A, we used
5
The datasets can be downloaded from http://
metashare.ilsp.gr:8080/. META-SHARE (http:
//www.meta-share.org/) was implemented in the
framework of the META-NET Network of Excellence
(http://www.meta-net.eu/).
6
Phase A ran from 9:00 GMT, March 24 to 21:00 GMT,
March 25, 2014. Phase B ran from 9:00 GMT, March 27 to
17:00 GMT, March 29, 2014.
the F
1
measure, defined as usually:
F
1
=
2 ? P ?R
P + R
(1)
where precision (P ) and recall (R) are defined as:
P =
|S ?G|
|S|
, R =
|S ?G|
|G|
(2)
Here S is the set of aspect term or aspect category
annotations (in SB1 and SB3, respectively) that a
system returned for all the test sentences (of a do-
main), and G is the set of the gold (correct) aspect
term or aspect category annotations.
To evaluate aspect term polarity (SB2) and as-
pect category polarity (SB4) detection in Phase B,
we calculated the accuracy of each system, defined
as the number of correctly predicted aspect term
or aspect category polarity labels, respectively, di-
vided by the total number of aspect term or aspect
category annotations. Recall that we used the gold
aspect term and category annotations in Phase B.
We provided four baselines, one per subtask:
7
Aspect term extraction (SB1) baseline: A se-
quence of tokens is tagged as an aspect term in
a test sentence (of a domain), if it is listed in a
dictionary that contains all the aspect terms of the
training sentences (of the same domain).
Aspect term polarity (SB2) baseline: For each
aspect term t in a test sentence s (of a particu-
lar domain), this baseline checks if t had been
encountered in the training sentences (of the do-
main). If so, it retrieves the k most similar to s
training sentences (of the domain), and assigns to
the aspect term t the most frequent polarity it had
in the k sentences. Otherwise, if t had not been en-
countered in the training sentences, it is assigned
the most frequent aspect term polarity label of the
7
Implementations of the baselines and further information
about the baselines are available at: http://alt.qcri.
org/semeval2014/task4/data/uploads/.
31
<sentence id="11351725#582163#9">
<text>Our waiter was friendly and it is a shame that he didnt have a supportive
staff to work with.</text>
<aspectTerms>
<aspectTerm term="waiter" polarity="positive" from="4" to="10"/>
<aspectTerm term="staff" polarity="negative" from="74" to="79"/>
</aspectTerms>
<aspectCategories>
<aspectCategory category="service" polarity="conflict"/>
</aspectCategories>
</sentence>
Figure 3: An XML snippet that corresponds to the annotated sentence of Fig. 2.
training set. The similarity between two sentences
is measured as the Dice coefficient of the sets of
(distinct) words of the two sentences. For exam-
ple, the similarity between ?this is a demo? and
?that is yet another demo? is
2?2
4+5
= 0.44.
Aspect category extraction (SB3) baseline: For
every test sentence s, the k most similar to s train-
ing sentences are retrieved (as in the SB2 base-
line). Then, s is assigned the m most frequent as-
pect category labels of the k retrieved sentences;
m is the most frequent number of aspect category
labels per sentence among the k sentences.
Aspect category polarity (SB4): This baseline
assigns to each aspect category c of a test sentence
s the most frequent polarity label that c had in the
k most similar to s training sentences (of the same
domain), considering only training sentences that
have the aspect category label c. Sentence similar-
ity is computed as in the SB2 baseline.
For subtasks SB2 and SB4, we also use a major-
ity baseline that assigns the most frequent polarity
(in the training data) to all the aspect terms and as-
pect categories. The scores of all the baselines and
systems are presented in Tables 4?6.
5 Evaluation Results
The ABSA task attracted 32 teams in total and 165
submissions (systems), 76 for phase A and 89 for
phase B. Based on the human-annotation experi-
ence, the expectations were that systems would
perform better in Phase B (SB3, SB4, involving
aspect categories) than in Phase A (SB1, SB2, in-
volving aspect terms). The evaluation results con-
firmed our expectations (Tables 4?6).
5.1 Results of Phase A
The aspect term extraction subtask (SB1) attracted
24 teams for the laptops dataset and 24 teams for
the restaurants dataset; consult Table 4.
Laptops Restaurants
Team F
1
Team F
1
IHS RD. 74.55? DLIREC 84.01*
DLIREC 73.78* XRCE 83.98
DLIREC 70.4 NRC-Can. 80.18
NRC-Can. 68.56 UNITOR 80.09
UNITOR 67.95* UNITOR 79.96*
XRCE 67.24 IHS RD. 79.62?
SAP RI 66.6 UWB 79.35*
IITP 66.55 SeemGo 78.61
UNITOR 66.08 DLIREC 78.34
SeemGo 65.99 ECNU 78.24
ECNU 65.88 SAP RI 77.88
SNAP 62.4 UWB 76.23
DMIS 60.59 IITP 74.94
UWB 60.39 DMIS 72.73
JU CSE. 59.37 JU CSE. 72.34
lsis lif 56.97 Blinov 71.21*
USF 52.58 lsis lif 71.09
Blinov 52.07* USF 70.69
UFAL 48.98 EBDG 69.28*
UBham 47.49 UBham 68.63*
UBham 47.26* UBham 68.51
SINAI 45.28 SINAI 65.41
EBDG 41.52* V3 60.43*
V3 36.62* UFAL 58.88
COMMIT. 25.19 COMMIT. 54.38
NILCUSP 25.19 NILCUSP 49.04
iTac 23.92 SNAP 46.46
iTac 38.29
Baseline 35.64 Baseline 47.15
Table 4: Results for aspect term extraction (SB1).
Stars indicate unconstrained systems. The ? indi-
cates a constrained system that was not trained on
the in-domain training dataset (unlike the rest of
the constrained systems), but on the union of the
two training datasets (laptops, restaurants).
32
Restaurants Restaurants
Team F
1
Team Acc.
NRC-Can. 88.57 NRC-Can. 82.92
UNITOR 85.26* XRCE 78.14
XRCE 82.28 UNITOR 76.29*
UWB 81.55* SAP RI 75.6
UWB 81.04 SeemGo 74.63
UNITOR 80.76 SA-UZH 73.07
SAP RI 79.04 UNITOR 73.07
SNAP 78.22 UWB 72.78
Blinov 75.27* UWB 72.78*
UBham 74.79* lsis lif 72.09
UBham 74.24 UBham 71.9
EBDG 73.98* EBDG 69.75
SeemGo 73.75 SNAP 69.56
SINAI 73.67 COMMIT. 67.7
JU CSE. 70.46 Blinov 65.65*
lsis lif 68.27 Ualberta. 65.46
ECNU 67.29 JU CSE. 64.09
UFAL 64.51 ECNU 63.41
V3 60.20* UFAL 63.21
COMMIT. 59.3 iTac 62.73*
iTac 56.95 ECNU 60.39*
SINAI 60.29
V3 47.21
Baseline 65.65
Baseline 63.89 Majority 64.09
Table 5: Results for aspect category detection
(SB3) and aspect category polarity (SB4). Stars
indicate unconstrained systems.
Overall, the systems achieved significantly
higher scores (+10%) in the restaurants domain,
compared to laptops. The best F
1
score (74.55%)
for laptops was achieved by the IHS RD. team,
which relied on Conditional Random Fields (CRF)
with features extracted using named entity recog-
nition, POS tagging, parsing, and semantic anal-
ysis. The IHS RD. team used additional reviews
from Amazon and Epinions (without annotated
terms) to learn the sentiment orientation of words
and they trained their CRF on the union of the
restaurant and laptop training data that we pro-
vided; the same trained CRF classifier was then
used in both domains.
The second system, the unconstrained system of
DLIREC, also uses a CRF, along with POS and
dependency tree based features. It also uses fea-
tures derived from the aspect terms of the train-
ing data and clusters created from additional re-
views from YELP and Amazon. In the restaurants
domain, the unconstrained system of DLIREC
ranked first with an F
1
of 84.01%, but the best
unconstrained system, that of XRCE, was very
close (83.98%). The XRCE system relies on a
parser to extract syntactic/semantic dependencies
(e.g., ?dissapointed???food?). For aspect term ex-
traction, the parser?s vocabulary was enriched with
the aspect terms of the training data and a term
list extracted from Wikipedia and Wordnet. A set
of grammar rules was also added to detect multi-
word terms and associate them with the corre-
sponding aspect category (e.g., FOOD, PRICE).
The aspect category extraction subtask (SB3)
attracted 18 teams. As shown in Table 5, the best
score was achieved by the system of NRC-Canada
(88.57%), which relied on five binary (one-vs-all)
SVMs, one for each aspect category. The SVMs
used features based on various types of n-grams
(e.g., stemmed) and information from a lexicon
learnt from YELP data, which associates aspect
terms with aspect categories. The latter lexicon
significantly improved F
1
. The constrained UN-
ITOR system uses five SVMs with bag-of-words
(BoW) features, which in the unconstrained sub-
mission are generalized using distributional vec-
tors learnt from Opinosis and TripAdvisor data.
Similarly, UWB uses a binary MaxEnt classifier
for each aspect category with BoW and TF-IDF
features. The unconstrained submission of UWB
also uses word clusters learnt using various meth-
ods (e.g., LDA); additional features indicate which
clusters the words of the sentence being classi-
fied come from. XRCE uses information identi-
fied by its syntactic parser as well as BoW features
to train a logistic regression model that assigns to
the sentence probabilities of belonging to each as-
pect category. A probability threshold, tuned on
the training data, is then used to determine which
categories will be assigned to the sentence.
5.2 Results of Phase B
The aspect term polarity detection subtask (SB2)
attracted 26 teams for the laptops dataset and 26
teams for the restaurants dataset. DCU and NRC-
Canada had the best systems in both domains (Ta-
ble 6). Their scores on the laptops dataset were
identical (70.48%). On the laptops dataset, the
DCU system performed slightly better (80.95%
vs. 80.15%). For SB2, both NRC-Canada and
DCU relied on an SVM classifier with features
33
mainly based on n-grams, parse trees, and sev-
eral out-of-domain, publicly available sentiment
lexica (e.g., MPQA, SentiWordnet and Bing Liu?s
Opinion Lexicon). NRC-Canada also used two
automatically compiled polarity lexica for restau-
rants and laptops, obtained from YELP and Ama-
zon data, respectively. Furthermore, NRC-Canada
showed by ablation experiments that the most use-
ful features are those derived from the sentiment
lexica. On the other hand, DCU used only publicly
available lexica, which were manually adapted by
filtering words that do not express sentiment in
laptop and restaurant reviews (e.g., ?really?) and
by adding others that were missing and do express
sentiment (e.g., ?mouthwatering?).
The aspect category polarity detection subtask
(SB4) attracted 20 teams. NRC-Canada again had
the best score (82.92%) using an SVM classifier.
The same feature set as in SB2 was used, but it
was further enriched to capture information re-
lated to each specific aspect category. The second
team, XRCE, used information from its syntactic
parser, BoW features, and an out-of-domain senti-
ment lexicon to train an SVM model that predicts
the polarity of each given aspect category.
6 Conclusions and Future Work
We provided an overview of Task 4 of SemEval-
2014. The task aimed to foster research in aspect-
based sentiment analysis (ABSA). We constructed
and released ABSA benchmark datasets contain-
ing manually annotated reviews from two domains
(restaurants, laptops). The task attracted 163 sub-
missions from 32 teams that were evaluated in four
subtasks centered around aspect terms (detecting
aspect terms and their polarities) and coarser as-
pect categories (assigning aspect categories and
aspect category polarities to sentences). The task
will be repeated in SemEval-2015 with additional
datasets and a domain-adaptation subtask.
8
In the
future, we hope to add an aspect term aggrega-
tion subtask (Pavlopoulos and Androutsopoulos,
2014a).
Acknowledgements
We thank Ioanna Lazari, who provided an ini-
tial version of the laptops dataset, Konstantina Pa-
panikolaou, who carried out a critical part of the
8
Consult http://alt.qcri.org/semeval2015/
task12/.
Laptops Restaurants
Team Acc. Team Acc.
DCU 70.48 DCU 80.95
NRC-Can. 70.48 NRC-Can. 80.15?
SZTE-NLP 66.97 UWB 77.68*
UBham 66.66 XRCE 77.68
UWB 66.66* SZTE-NLP 75.22
lsis lif 64.52 UNITOR 74.95*
USF 64.52 UBham 74.6
SNAP 64.06 USF 73.19
UNITOR 62.99 UNITOR 72.48
UWB 62.53 SeemGo 72.31
IHS RD. 61.62 lsis lif 72.13
SeemGo 61.31 UWB 71.95
ECNU 61.16 SA-UZH 70.98
ECNU 61.16* IHS RD. 70.81
SINAI 58.71 SNAP 70.81
SAP RI 58.56 ECNU 70.72
UNITOR 58.56* ECNU 70.72*
SA-UZH 58.25 INSIGHT. 70.72
COMMIT 57.03 SAP RI 69.92
INSIGHT. 57.03 EBDG 68.6
UMCC. 57.03* UMCC. 66.84*
UFAL 56.88 UFAL 66.57
UMCC. 56.11 UMCC. 66.57
EBDG 55.96 COMMIT 65.96
JU CSE. 55.65 JU CSE. 65.52
UO UA 55.19* Blinov 63.58*
V3 53.82 iTac 62.25*
Blinov 52.29* V3 59.78
iTac 51.83* SINAI 58.73
DLIREC 36.54 DLIREC 42.32*
DLIREC 36.54* DLIREC 41.71
IITP 66.97 IITP 67.37
Baseline 51.37 Baseline 64.28
Majority 52.14 Majority 64.19
Table 6: Results for the aspect term polarity sub-
task (SB2). Stars indicate unconstrained systems.
The ? indicates a constrained system that was not
trained on the in-domain training dataset (unlike
the rest of the constrained systems), but on the
union of the two training datasets. IITP?s original
submission files were corrupted; they were resent
and scored after the end of the evaluation period.
annotation process, and Juli Bakagianni, who sup-
ported our use of the META-SHARE platform.
We are also very grateful to the participants for
their feedback. Maria Pontiki and Haris Papageor-
giou were supported by the IS-HELLEANA (09-
34
72-922) and the POLYTROPON (KRIPIS-GSRT,
MIS: 448306) projects.
References
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL, pages 804?812, Los An-
geles, California.
Michael Gamon, Anthony Aue, Simon Corston-Oliver,
and Eric K. Ringger. 2005. Pulse: Mining customer
opinions from free text. In IDA, pages 121?132,
Madrid, Spain.
Gayatree Ganu, Noemie Elhadad, and Am?elie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In Proceedings of
WebDB, Providence, Rhode Island, USA.
Minqing Hu and Bing Liu. 2004a. Mining and sum-
marizing customer reviews. In Proceedings of KDD,
pages 168?177, Seattle, WA, USA.
Minqing Hu and Bing Liu. 2004b. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI,
pages 755?760, San Jose, California.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
Chong Long, Jie Zhang, and Xiaoyan Zhu. 2010. A
review selection approach for accurate feature rating
estimation. In Proceedings of COLING (Posters),
pages 766?774, Beijing, China.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86, Philadelphia, Pennsylvania,
USA.
John Pavlopoulos and Ion Androutsopoulos. 2014a.
Aspect term extraction for sentiment analysis: New
datasets, new evaluation measures and an improved
unsupervised method. In Proceedings of LASM-
EACL, pages 44?52, Gothenburg, Sweden.
John Pavlopoulos and Ion Androutsopoulos. 2014b.
Multi-granular aspect aggregation in aspect-based
sentiment analysis. In Proceedings of EACL, pages
78?87, Gothenburg, Sweden.
Stelios Piperidis. 2012. The META-SHARE language
resources sharing infrastructure: Principles, chal-
lenges, solutions. In Proceedings of LREC-2012,
pages 36?42, Istanbul, Turkey.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT/EMNLP, pages 339?346, Van-
couver, British Columbia, Canada.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. BRAT: a web-based tool for NLP-assisted
text annotation. In Proceedings of EACL, pages
102?107, Avignon, France.
Tun Thura Thet, Jin-Cheon Na, and Christopher S. G.
Khoo. 2010. Aspect-based sentiment analysis of
movie reviews on discussion boards. J. Information
Science, 36(6):823?848.
Ivan Titov and Ryan T. McDonald. 2008. A joint
model of text and aspect ratings for sentiment sum-
marization. In Proceedings of ACL, pages 308?316,
Columbus, Ohio, USA.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of ACL, pages
417?424, Philadelphia, Pennsylvania, USA.
35
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 54?62,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 7: Analysis of Clinical Text
Sameer Pradhan
1
, No
?
emie Elhadad
2
, Wendy Chapman
3
,
Suresh Manandhar
4
and Guergana Savova
1
1
Harvard University, Boston, MA,
2
Columbia University, New York, NY
3
University of Utah, Salt Lake City, UT,
4
University of York, York, UK
{sameer.pradhan,guergana.savova}@childrens.harvard.edu, noemie.elhadad@columbia.edu,
wendy.chapman@utah.edu, suresh@cs.york.ac.uk
Abstract
This paper describes the SemEval-2014,
Task 7 on the Analysis of Clinical Text
and presents the evaluation results. It fo-
cused on two subtasks: (i) identification
(Task A) and (ii) normalization (Task B)
of diseases and disorders in clinical reports
as annotated in the Shared Annotated Re-
sources (ShARe)
1
corpus. This task was
a follow-up to the ShARe/CLEF eHealth
2013 shared task, subtasks 1a and 1b,
2
but
using a larger test set. A total of 21 teams
competed in Task A, and 18 of those also
participated in Task B. For Task A, the
best system had a strict F
1
-score of 81.3,
with a precision of 84.3 and recall of 78.6.
For Task B, the same group had the best
strict accuracy of 74.1. The organizers
have made the text corpora, annotations,
and evaluation tools available for future re-
search and development at the shared task
website.
3
1 Introduction
A large amount of very useful information?both
for medical researchers and patients?is present
in the form of unstructured text within the clin-
ical notes and discharge summaries that form a
patient?s medical history. Adapting and extend-
ing natural language processing (NLP) techniques
to mine this information can open doors to bet-
ter, novel, clinical studies on one hand, and help
patients understand the contents of their clini-
cal records on the other. Organization of this
1
http://share.healthnlp.org
2
https://sites.google.com/site/shareclefehealth/
evaluation
3
http://alt.qcri.org/semeval2014/task7/
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shared task helps establish state-of-the-art bench-
marks and paves the way for further explorations.
It tackles two important sub-problems in NLP?
named entity recognition and word sense disam-
biguation. Neither of these problems are new to
NLP. Research in general-domain NLP goes back
to about two decades. For an overview of the
development in the field through roughly 2009,
we refer the refer to Nadeau and Sekine (2007).
NLP has also penetrated the field of bimedical
informatics and has been particularly focused on
biomedical literature for over the past decade. Ad-
vances in that sub-field has also been documented
in surveys such as one by Leaman and Gonza-
lez (2008). Word sense disambiguation also has
a long history in the general NLP domain (Nav-
igli, 2009). In spite of word sense annotations in
the biomedical literature, recent work by Savova
et al. (2008) highlights the importance of annotat-
ing them in clinical notes. This is true for many
other clinical and linguistic phenomena as the var-
ious characteristics of the clinical narrative present
a unique challenge to NLP. Recently various ini-
tiatives have led to annotated corpora for clini-
cal NLP research. Probably the first comprehen-
sive annotation performed on a clinical corpora
was by Roberts et al. (2009), but unfortunately
that corpus is not publicly available owing to pri-
vacy regulations. The i2b2 initiative
4
challenges
have focused on such topics as concept recog-
nition (Uzuner et al., 2011), coreference resolu-
tion (Uzuner et al., 2012), temporal relations (Sun
et al., 2013) and their datasets are available to the
community. More recently, the Shared Annotated
Resources (ShARe)
1
project has created a corpus
annotated with disease/disorder mentions in clini-
cal notes as well as normalized them to a concept
unique identifier (CUI) within the SNOMED-CT
subset of the Unified Medical Language System
5
4
http://www.i2b2.org
5
https://uts.nlm.nih.gov/home.html
54
Train Development Test
Notes 199 99 133
Words 94K 88K 153K
Disorder mentions 5,816 5,351 7,998
CUI-less mentions 1,639 (28%) 1,750 (32%) 1,930 (24%)
CUI-ied mentions 4,117 (72%) 3,601 (67%) 6,068 (76%)
Contiguous mentions 5,165 (89%) 4,912 (92%) 7,374 (92%)
Discontiguous mentions 651 (11%) 439 (8%) 6,24 (8%)
Table 1: Distribution of data in terms of notes and disorder mentions across the training, development
and test sets. The disorders are further split according to two criteria ? whether they map to a CUI or
whether they are contiguous.
(UMLS) (Campbell et al., 1998). The task of nor-
malization is a combination of word/phrase sense
disambiguation and semantic similarity where a
phrase is mapped to a unique concept in an on-
tology (based on the description of that concept in
the ontology) after disambiguating potential am-
biguous surface words, or phrases. This is espe-
cially true with abbreviations and acronyms which
are much more common in clinical text (Moon et
al., 2012). The SemEval-2014 task 7 was one of
nine shared tasks organized at the SemEval-2014.
It was designed as a follow up to the shared tasks
organized during the ShARe/CLEF eHealth 2013
evaluation (Suominen et al., 2013; Pradhan et al.,
2013; Pradhan et al., 2014). Like the previous
shared task, we relied on the ShARe corpus, but
with more data for training and a new test set. Fur-
thermore, in this task, we provided the options to
participants to utilize a large corpus of unlabeled
clinical notes. The rest of the paper is organized as
follows. Section 2 describes the characteristics of
the data used in the task. Section 3 describes the
tasks in more detail. Section 4 explains the evalu-
ation criteria for the two tasks. Section 5 lists the
participants of the task. Section 6 discusses the re-
sults on this task and also compares them with the
ShARe/CLEF eHealth 2013 results, and Section 7
concludes.
2 Data
The ShARe corpus comprises annotations over
de-identified clinical reports from a US intensive
care department (version 2.5 of the MIMIC II
database
6
) (Saeed et al., 2002). It consists of
discharge summaries, electrocardiogram, echocar-
diogram, and radiology reports. Access to data
was carried out following MIMIC user agreement
requirements for access to de-identified medical
6
http://mimic.physionet.org ? Multiparameter Intelligent
Monitoring in Intensive Care
data. Hence, all participants were required to reg-
ister for the evaluation, obtain a US human sub-
jects training certificate
7
, create an account to the
password-protected MIMIC site, specify the pur-
pose of data usage, accept the data use agree-
ment, and get their account approved. The anno-
tation focus was on disorder mentions, their var-
ious attributes and normalizations to an UMLS
CUI. As such, there were two parts to the annota-
tion: identifying a span of text as a disorder men-
tion and normalizing (or mapping) the span to a
UMLS CUI. The UMLS represents over 130 lex-
icons/thesauri with terms from a variety of lan-
guages and integrates resources used world-wide
in clinical care, public health, and epidemiology.
A disorder mention was defined as any span of text
which can be mapped to a concept in SNOMED-
CT and which belongs to the Disorder semantic
group
8
. It also provided a semantic network in
which every concept is represented by its CUI
and is semantically typed (Bodenreider and Mc-
Cray, 2003). A concept was in the Disorder se-
mantic group if it belonged to one of the follow-
ing UMLS semantic types: Congenital Abnormal-
ity; Acquired Abnormality; Injury or Poisoning;
Pathologic Function; Disease or Syndrome; Men-
tal or Behavioral Dysfunction; Cell or Molecu-
lar Dysfunction; Experimental Model of Disease;
Anatomical Abnormality; Neoplastic Process; and
Signs and Symptoms. The Finding semantic type
was left out as it is very noisy and our pilot study
showed lower annotation agreement on it. Follow-
ing are the salient aspects of the guidelines used to
7
The course was available free of charge on the Internet, for example,
via the CITI Collaborative Institutional Training Initiative at
https://www.citiprogram.org/Default.asp
or, the US National Institutes of Health (NIH) at
http://phrp.nihtraining.com/users.
8
Note that this definition of Disorder semantic group did not include the
Findings semantic type, and as such differed from the one of UMLS Seman-
tic Groups, available at http://semanticnetwork.nlm.nih.gov/
SemGroups
55
annotate the data.
? Annotations represent the most specific dis-
order span. For example, small bowel ob-
struction is preferred over bowel obstruction.
? A disorder mention is a concept in the
SNOMED-CT portion of the Disorder se-
mantic group.
? Negation and temporal modifiers are not con-
sidered part of the disorder mention span.
? All disorder mentions are annotated?even
the ones related to a person other than the pa-
tient and including acronyms and abbrevia-
tions.
? Mentions of disorders that are coreferen-
tial/anaphoric are also annotated.
Following are a few examples of disorder men-
tions from the data.
Patient found to have lower extremity DVT. (E1)
In example (E1), lower extremity DVT is marked
as the disorder. It corresponds to CUI C0340708
(preferred term: Deep vein thrombosis of lower
limb). The span DVT can be mapped to CUI
C0149871 (preferred term: Deep Vein Thrombo-
sis), but this mapping would be incorrect because
it is part of a more specific disorder in the sen-
tence, namely lower extremity DVT.
A tumor was found in the left ovary. (E2)
In example (E2), tumor ... ovary is annotated as a
discontiguous disorder mention. This is the best
method of capturing the exact disorder mention
in clinical notes and its novelty is in the fact that
either such phenomena have not been seen fre-
quently enough in the general domain to gather
particular attention, or the lack of a manually
curated general domain ontology parallel to the
UMLS.
Patient admitted with low blood pressure. (E3)
There are some disorders that do not have a rep-
resentation to a CUI as part of the SNOMED CT
within the UMLS. However, if they were deemed
important by the annotators then they were anno-
tated as CUI-less mentions. In example (E3), low
blood pressure is a finding and is normalized as
a CUI-less disorder. We constructed the annota-
tion guidelines to require that the disorder be a
reasonable synonym of the lexical description of a
SNOMED-CT disorder. There are a few instances
where the disorders are abbreviated or shortened
in the clinical note. One example is w/r/r, which
is an abbreviation for concepts wheezing (CUI
C0043144), rales (CUI C0034642), and ronchi
(CUI C0035508). This abbreviation is also some-
times written as r/w/r and r/r/w. Another is gsw for
gunshot wound and tachy for tachycardia. More
details on the annotation scheme is detailed in the
guidelines
9
and in a forthcoming manuscript. The
annotations covered about 336K words. Table 1
shows the quantity of the data and the split across
the training, development and test sets as well as
in terms of the number of notes and the number of
words.
2.1 Annotation Quality
Each note in the training and development set was
annotated by two professional coders trained for
this task, followed by an open adjudication step.
By the time we reached annotating the test data,
the annotators were quite familiar with the anno-
tation and so, in order to save time, we decided
to perform a single annotation pass using a senior
annotator. This was followed by a correction pass
by the same annotator using a checklist of frequent
annotation issues faced earlier. Table 2 shows the
inter-annotator agreement (IAA) statistics for the
adjudicated data. For the disorders we measure the
agreement in terms of the F
1
-score as traditional
agreement measures such as Cohen?s kappa and
Krippendorf?s alpha are not applicable for measur-
ing agreement for entity mention annotation. We
computed agreements between the two annotators
as well as between each annotator and the final ad-
judicated gold standard. The latter is to give a
sense of the fraction of corrections made in the
process of adjudication. The strict criterion con-
siders two mentions correct if they agree in terms
of the class and the exact string, whereas the re-
laxed criteria considers overlapping strings of the
9
http://goo.gl/vU8KdW
Disorder CUI
Relaxed Strict Relaxed Strict
F
1
F
1
Acc. Acc.
A1-A2 90.9 76.9 77.6 84.6
A1-GS 96.8 93.2 95.4 97.3
A2-GS 93.7 82.6 80.6 86.3
Table 2: Inter-annotator (A1 and A2) and gold
standard (GS) agreement as F
1
-score for the Dis-
order mentions and their normalization to the
UMLS CUI.
56
Institution User ID Team ID
University of Pisa, Italy attardi UniPI
University of Lisbon, Portugal francisco ULisboa
University of Wisconsin, Milwaukee, USA ghiasvand UWM
University of Colorado, Boulder, USA gung CLEAR
University of Guadalajara, Mexico herrera UG
Taipei Medical University, Taiwan hjdai TMU
University of Turku, Finland kaewphan UTU
University of Szeged, Hungary katona SZTE-NLP
Queensland University of Queensland, Australia kholghi QUT AEHRC
KU Leuven, Belgium kolomiyets KUL
Universidade de Aveiro, Portugal nunes BioinformaticsUA
University of the Basque Country, Spain oronoz IxaMed
IBM, India parikh ThinkMiners
easy data intelligence, India pathak ezDI
RelAgent Tech Pvt. Ltd., India ramanan RelAgent
Universidad Nacional de Colombia, Colombia riveros MindLab-UNAL
IIT Patna, India sikdar IITP
University of North Texas, USA solomon UNT
University of Illinois at Urbana Champaign, USA upadhya CogComp
The University of Texas Health Science Center at Houston, USA wu UTH CCB
East China Normal University, China yi ECNU
Table 3: Participant organization and the respective User IDs and Team IDs.
same class as correct. The reason for checking
the class is as follows. Although we only use the
disorder mention in this task, the corpus has been
annotated with some other UMLS types as well
and therefore there are instances where a differ-
ent UMLS type is assigned to the same character
span in the text by the second annotator. If exact
boundaries are not taken into account then the IAA
agreement score is in the mid-90s. For the task of
normalization to CUIs, we used accuracy to assess
agreement. For the relaxed criterion, all overlap-
ping disorder spans with the same CUI were con-
sidered correct. For the strict criterion, only disor-
der spans with identical spans and the same CUI
were considered correct.
3 Task Description
The participants were evaluated on the following
two tasks:
? Task A ? Identification of the character spans
of disorder mentions.
? Task B ? Normalizing disorder mentions to
SNOMED-CT subset of UMLS CUIs.
For Task A, participants were instructed to develop
a system that predicts the spans for disorder men-
tions. For Tasks B, participants were instructed
to develop a system that predicts the UMLS CUI
within the SNOMED-CT vocabulary. The input to
Task B were the disorder mention predictions from
Task A. Task B was optional. System outputs ad-
hered to the annotation format. Each participant
was allowed to submit up to three runs. The en-
tire set of unlabeled MIMIC clinical notes (exclud-
ing the test notes) were made available to the par-
ticipants for potential unsupervised approaches to
enhance the performance of their systems. They
were allowed to use additional annotations in their
systems, but this counted towards the total allow-
able runs; systems that used annotations outside
of those provided were evaluated separately. The
evaluation for all tasks was conducted using the
blind, withheld test data. The participants were
provided a training set containing clinical text as
well as pre-annotated spans and named entities for
disorders (Tasks A and B).
4 Evaluation Criteria
The following evaluation criteria were used:
? Task A ? The system performance was eval-
uated against the gold standard using the
F
1
-score of the Precision and Recall values.
There were two variations: (i) Strict; and (ii)
Relaxed. The formulae for computing these
metrics are mentioned below.
Precision = P =
D
tp
D
tp
+ D
fp
(1)
Recall = R =
D
tp
D
tp
+ D
fn
(2)
Where, D
tp
= Number of true positives dis-
order mentions; D
fp
= Number of false pos-
itives disorder mentions; D
fn
= Number of
false negative disorder mentions. In the strict
case, a span was counted as correct if it was
identical to the gold standard span, whereas
57
Task A
Strict Relaxed
Team ID User ID Run P R F
1
P R F
1
Data
(%) (%) (%) (%) (%) (%)
UTH CCB wu 0 84.3 78.6 81.3 93.6 86.6 90.0 T+D
UTH CCB wu 1 80.8 80.5 80.6 91.6 90.7 91.1 T+D
UTU kaewphan 1 76.5 76.7 76.6 88.6 89.9 89.3 T+D
UWM ghiasvand 0 78.7 72.6 75.5 91.1 85.6 88.3 T+D
UTH CCB wu 2 68.0 84.9 75.5 83.8 93.5 88.4 T+D
UTU kaewphan 0 77.3 72.4 74.8 90.1 85.6 87.8 T
IxaMed oronoz 1 68.1 78.6 73.0 87.2 89.0 88.1 T+D
UWM ghiasvand 0 77.5 67.9 72.4 90.9 81.2 85.8 T
RelAgent ramanan 0 74.1 70.1 72.0 89.5 84.0 86.7 T+D
IxaMed oronoz 0 72.9 70.1 71.5 88.5 80.8 84.5 T+D
ezDI pathak 1 75.0 68.2 71.4 91.5 82.7 86.9 T
CLEAR gung 0 80.7 63.6 71.2 92.0 72.3 81.0 T
ezDI pathak 0 75.0 67.7 71.2 91.4 81.9 86.4 T
ULisboa francisco 0 75.3 66.3 70.5 91.4 81.5 86.2 T
ULisboa francisco 1 75.2 66.0 70.3 90.9 80.6 85.5 T
ULisboa francisco 2 75.2 66.0 70.3 90.9 80.6 85.5 T
BioinformaticsUA nunes 0 81.3 60.5 69.4 92.9 69.3 79.4 T+D
ThinkMiners parikh 0 73.4 65.0 68.9 89.2 80.2 84.4 T
ThinkMiners parikh 1 74.9 61.7 67.7 90.7 75.8 82.6 T
ECNU yi 0 75.4 61.1 67.5 89.8 72.2 80.0 T+D
UniPI attardi 2 71.2 60.1 65.2 89.7 76.6 82.6 T+D
UNT solomon 0 64.7 62.8 63.8 81.5 79.9 80.7 T+D
UniPI attardi 1 65.9 61.2 63.5 90.2 77.5 83.4 T+D
BioinformaticsUA nunes 2 75.3 53.8 62.8 86.5 62.1 72.3 T+D
BioinformaticsUA nunes 1 60.0 62.1 61.0 69.8 72.3 71.0 T+D
UniPI attardi 0 53.9 68.4 60.2 77.8 88.5 82.8 T+D
CogComp upadhya 1 63.9 52.9 57.9 82.3 68.3 74.6 T+D
CogComp upadhya 2 64.1 52.0 57.4 82.9 67.5 74.4 T+D
CogComp upadhya 0 63.6 51.5 56.9 81.9 66.5 73.4 T+D
TMU hjdai 0 52.4 57.6 54.9 91.4 76.5 83.3 T+D
MindLab-UNAL riveros 2 56.1 53.4 54.7 76.9 67.7 72.0 T
MindLab-UNAL riveros 1 57.8 51.5 54.5 77.7 65.4 71.0 T
TMU hjdai 1 62.2 42.9 50.8 89.9 65.2 75.6 T+D
IITP sikdar 0 50.0 47.9 48.9 81.5 79.7 80.6 T+D
IITP sikdar 1 47.3 45.8 46.5 78.9 77.6 78.2 T+D
IITP sikdar 2 45.0 48.1 46.5 76.9 82.6 79.6 T+D
MindLab-UNAL riveros 0 32.1 56.5 40.9 43.9 72.5 54.7 T
SZTE-NLP katona 1 54.7 25.2 34.5 88.4 40.1 55.1 T
SZTE-NLP katona 2 54.7 25.2 34.5 88.4 40.1 55.1 T
QUT AEHRC kholghi 0 38.7 29.8 33.7 90.6 70.9 79.5 T+D
SZTE-NLP katona 0 57.1 20.5 30.2 91.8 32.5 48.0 T
KUL kolomiyets 0 65.5 17.8 28.0 72.1 19.6 30.8 P
UG herrera 0 11.4 23.4 15.3 25.9 49.0 33.9 P
Table 4: Performance on test data for participating systems on Task A ? Identification of disorder men-
tions.
Task A
Strict Relaxed
Team ID User ID Run P R F
1
P R F
1
Data
(%) (%) (%) (%) (%) (%)
hjdai TMU 1 0.687 0.922 0.787 0.952 1.000 0.975 T
wu UTH CCB 0 0.877 0.710 0.785 0.962 0.789 0.867 T
wu UTH CCB 1 0.828 0.747 0.785 0.941 0.853 0.895 T
Best ShARe/CLEF-2013 performance 0.800 0.706 0.750 0.925 0.827 0.873 T
ghiasvand UWM 0 0.827 0.675 0.743 0.958 0.799 0.871 T
pathak ezDI 0 0.813 0.670 0.734 0.954 0.800 0.870 T
pathak ezDI 1 0.809 0.667 0.732 0.954 0.801 0.871 T
wu UTH CCB 2 0.657 0.790 0.717 0.806 0.893 0.847 T
francisco ULisboa 1 0.803 0.646 0.716 0.954 0.781 0.858 T
francisco ULisboa 2 0.803 0.646 0.716 0.954 0.781 0.858 T
francisco ULisboa 0 0.796 0.642 0.711 0.959 0.793 0.868 T
oronoz IxaMed 0 0.766 0.650 0.703 0.936 0.752 0.834 T
oronoz IxaMed 1 0.660 0.721 0.689 0.899 0.842 0.870 T
hjdai TMU 0 0.667 0.414 0.511 0.912 0.591 0.717 T
sikdar IITP 0 0.525 0.430 0.473 0.862 0.726 0.788 T
sikdar IITP 2 0.467 0.440 0.453 0.812 0.775 0.793 T
sikdar IITP 1 0.493 0.410 0.448 0.828 0.706 0.762 T
Table 5: Performance on development data for participating systems on Task A ? Identification of disor-
der mentions.
58
in the relaxed case, a span overlapping with
the gold standard span was also considered
correct.
? Task B ? Accuracy was used as the perfor-
mance measure for Task 1b. It was defined as
follows:
Accuracy
strict
=
D
tp
?N
correct
T
g
(3)
Accuracy
relaxed
=
D
tp
?N
correct
D
tp
(4)
Where, D
tp
= Number of true positive disor-
der mentions with identical spans as in the
gold standard; N
correct
= Number of cor-
rectly normalized disorder mentions; and T
g
= Total number of disorder mentions in the
gold standard. For Task B, the systems were
only evaluated on annotations they identified
in Task A. Relaxed accuracy only measured
the ability to normalize correct spans. There-
fore, it was possible to obtain very high val-
ues for this measure by simply dropping any
mention with a low confidence span.
5 Participants
A total of 21 participants from across the world
participated in Task A and out of them 18 also par-
ticipated in Task B. Unfortunately, although inter-
ested, the ThinkMiners team (Parikh et al., 2014)
could not participate in Task B owing to some
UMLS licensing issues. The participating organi-
zations along with the contact user?s User ID and
their chosen Team ID are mentioned in Table 3.
Eight teams submitted three runs, six submitted
two runs and seven submitted just one run. Out
of these, only 13 submitted system description pa-
pers. We based our analysis on those system de-
scriptions.
6 System Results
Tables 4 and 6 show the performance of the sys-
tems on Tasks A and B. None of the systems used
any additional annotated data so we did not have
to compare them separately. Both tables mention
performance of all the different runs that the sys-
tems submitted. Given the many variables, we de-
liberately left the decision on how many and how
to define these runs to the individual participant.
They used various different ways to differentiate
their runs. Some, for example, UTU (Kaewphan et
al., 2014), did it based on the composition of train-
ing data, i.e., whether they used just the training
data or both the training and the development data
for training the final system, which highlighted
the fact that adding development data to training
bumped the F
1
-score on Task A by about 2 percent
points. Some participants, however, did not make
use of the development data in training their sys-
tems. This was partially due to the fact that we had
not explicitly mentioned in the task description
that participants were allowed to use the develop-
ment data for training their final models. In order
to be fair, we allowed some users an opportunity
to submit runs post evaluation where they used the
exact same system that they used for evaluation
but used the development data as well. We added
a column to the results tables showing whether the
participant used only the training data (T) or both
training and development data (T+D) for training
their system. It can be seen that even though the
addition of development data helps, there are still
systems that perform in the lower percentile who
have used both training and development data for
training, indicating that both the features and the
machine learning classifier contribute to the mod-
els. A novel aspect of the SemEval-2014 shared
task that differentiates it from the ShARE/CLEF
task?other than the fact that it used more data and
a new test set?is the fact that SemEval-2014 al-
lowed the use of a much larger set of unlabeled
MIMIC notes to inform the models. Surprisingly,
only two of the systems (ULisboa (Leal et al.,
2014) and UniPi (Attardi et al., 2014)) used the
unlabeled MIMIC corpus to generalize the lexical
features. Another team?UTH CCB(Zhang et al.,
2014)?used off-the-shelf Brown clusters
10
as op-
posed to training them on the unlabeled MIMIC
II data. For Task B, the accuracy of a system
using the strict metric was positively correlated
with its recall on the disorder mentions that were
input to it (i.e., recall for Task A), and did not
get penalized for lower precision. Therefore one
could essentially gain higher accuracy in Task B
by tuning a system to provide the highest men-
tion recall in Task A potentially at the cost of pre-
cision and the overall F
1
-score and using those
mentions as input for Task B. This can be seen
from the fact that the run 2 for UTH CCB (Zhang
et al., 2014) system with the lowest F
1
-score has
10
Personal conversation with the participants as it was not
very clear in the system description paper.
59
Task B
Strict Relaxed
Team ID User ID Run Acc. Acc. Data
(%) (%)
UTH CCB wu 2 74.1 87.3 T+D
UTH CCB wu 1 70.8 88.0 T+D
UTH CCB wu 0 69.4 88.3 T+D
UWM ghiasvand 0 66.0 90.9 T+D
RelAgent ramanan 0 63.9 91.2 T+D
UWM ghiasvand 0 61.7 90.8 T
IxaMed oronoz 0 60.4 86.2 T+D
UTU kaewphan 1 60.1 78.3 T+D
ezDI pathak 1 59.9 87.8 T
ezDI pathak 0 59.2 87.4 T
UTU kaewphan 0 57.7 79.7 T
BioinformaticsUA nunes 1 53.1 85.5 T+D
BioinformaticsUA nunes 0 52.7 87.0 T+D
CLEAR gung 0 52.5 82.5 T
TMU hjdai 0 48.9 84.9 T+D
UNT solomon 0 47.0 74.8 T+D
UniPI attardi 0 46.7 68.3 T+D
BioinformaticsUA nunes 2 46.3 86.1 T+D
MindLab-UNAL riveros 2 46.1 86.3 T
IxaMed oronoz 1 43.9 55.8 T+D
MindLab-UNAL riveros 0 43.5 77.1 T
UniPI attardi 1 42.8 69.9 T+D
UniPI attardi 2 41.7 69.3 T+D
MindLab-UNAL riveros 1 41.1 79.7 T
ULisboa francisco 2 40.5 61.5 T
ULisboa francisco 1 40.4 61.2 T
ULisboa francisco 0 40.2 60.6 T
ECNU yi 0 36.4 59.5 T+D
TMU hjdai 1 35.8 83.4 T+D
IITP sikdar 0 33.3 69.6 T+D
IITP sikdar 2 33.2 69.1 T+D
IITP sikdar 1 31.9 69.6 T+D
CogComp upadhya 1 25.3 47.9 T+D
CogComp upadhya 2 24.8 47.7 T+D
CogComp upadhya 0 24.4 47.3 T+D
KUL kolomiyets 0 16.5 92.8 P
UG herrera 0 12.5 53.4 P
Table 6: Performance on test data for participat-
ing systems on Task B ? Normalization of disorder
mentions to UMLS (SNOMED-CT subset) CUIs.
Task B
Strict Relaxed
Team ID User ID Run Acc. Acc. Data
(%) (%)
TMU hjdai 0 0.716 0.777 T
TMU hjdai 1 0.716 0.777 T
UTH CCB wu 2 0.713 0.903 T
UTH CCB wu 1 0.680 0.910 T
UTH CCB wu 0 0.647 0.910 T
UWM ghiasvand 0 0.623 0.923 T
ezDI pathak 0 0.603 0.900 T
ezDI pathak 1 0.600 0.899 T
Best ShARe/CLEF-2013 performance 0.589 0.895 T
IxaMed oronoz 0 0.556 0.855 T
IxaMed oronoz 1 0.421 0.584 T
ULisboa francisco 2 0.388 0.601 T
ULisboa francisco 1 0.385 0.596 T
ULisboa francisco 0 0.377 0.588 T
IITP sikdar 2 0.318 0.724 T
IITP sikdar 0 0.312 0.725 T
IITP sikdar 1 0.299 0.730 T
Table 7: Performance on development data
for some participating systems on Task B ?
Normalization of disorder mentions to UMLS
(SNOMED-CT subset) CUIs.
the best accuracy for Task B and vice-versa for
run 0 with run 1 in between the two. In order to
fairly compare the performance between two sys-
tems one would have to provide perfect mentions
as input to Task B. One of the systems?UWM
Ghiasvand and Kate (2014)?did run some abla-
tion experiments using gold standard mentions as
input to Task B and obtained a best performance
of 89.5F
1
-score (Table 5 of Ghiasvand and Kate
(2014)) as opposed to 62.3 F
1
-score (Table 7) in
the more realistic setting which is a huge differ-
ence. In the upcoming SemEval-2014 where this
same evaluation is going to carried out under Task
14, we plan to perform supplementary evaluation
where gold disorder mentions would be input to
the system while attempting Task B. An inter-
esting outcome of planning a follow-on evalua-
tion to the ShARe/CLEF eHealth 2013 task was
that we could, and did, use the test data from the
ShARe/CLEF eHealth 2013 task as the develop-
ment set for this evaluation. After the main eval-
uation we asked participants to provide the sys-
tem performance on the development set using the
same number and run convention that they submit-
ted for the main evaluation. These results are pre-
sented in Tables 5 and 7. We have inserted the best
performing system score from the ShARe/CLEF
eHealth 2013 task in these tables. For Task A, re-
ferring to Tables 4 and 5, there is a boost of 3.7
absolute percent points for the F
1
-score over the
same task (Task 1a) in the ShARe/CLEF eHealth
2013. For Task B, referring to Tables 6 and 7, there
is a boost of 13.7 percent points for the F
1
-score
over the same task (Task 1b) in the ShARe/CLEF
eHealth 2013 evaluation. The participants used
various approaches for tackling the tasks, rang-
ing from purely rule-based/unsupervised (RelA-
gent (Ramanan and Nathan, 2014), (Matos et
al., 2014), KUL
11
) to a hybrid of rules and ma-
chine learning classifiers. The top performing sys-
tems typically used the latter. Various versions
of the IOB formulation were used for tagging the
disorder mentions. None of the standard varia-
tions on the IOB formulation were explicitly de-
signed or used to handle discontiguous mentions.
Some systems used novel variations on this ap-
proach. Probably the simplest variation was ap-
plied by the UWM team (Ghiasvand and Kate,
2014). In this formulation the following labeled
sequence ?the/O left/B atrium/I is/O moderately/O
11
Personal communication with participant.
60
dilated/I? can be used to represent the discontigu-
ous mention left atrium...dilated, and can be con-
structed as such from the output of the classifica-
tion. The most complex variation was the one used
by the UTH CCB team (Zhang et al., 2014) where
they used the following set of tags?B, I, O, DB,
DI, HB, HI. This variation encodes discontiguous
mentions by adding four more tags to the I, O and
B tags. These are variations of the B and I tags
with either a D or a H prefix. The prefix H indi-
cates that the word or word sequence is the shared
head, and the prefix D indicates otherwise. An-
other intermediate approach used by the ULisboa
team (Leal et al., 2014) with the tagset?S, B, I,
O, E and N. Here, S represents the single token
entity to be recognized, E represents the end of an
entity (which is part of one of the prior IOB vari-
ations) and an N tag to identify non-contiguous
mentions. They don?t provide an explicit exam-
ple usage of this tag set in their paper. Yet another
variation was used by the SZTE-NLP team (Ka-
tona and Farkas, 2014). This used tags B, I, L, O
and U. Here, L is used for the last token similar to
E earlier, and U is used for a unit-token mention,
similar to S earlier. We believe that the only ap-
proach that can distinguish between discontiguous
disorders that share the same head word/phrase is
the one used by the UTH CCB team (Zhang et
al., 2014). The participants used various machine
learning classifiers such as MaxEnt, SVM, CRF in
combination with rich syntactic and semantic fea-
tures to capture the disorder mentions. As men-
tioned earlier, a few participants used the avail-
able unlabeled data and also off-the-shelf clusters
to better generalize features. The use of vector
space models such as cosine similarities as well
as continuous distributed word vector representa-
tions was useful in the normalization task. They
also availed of tools such as MetaMap and cTakes
to generate features as well as candidate CUIs dur-
ing normalizations.
7 Conclusion
We have created a reference standard with high
inter-annotator agreement and evaluated systems
on the task of identification and normalization
of diseases and disorders appearing in clinical
reports. The results have demonstrated that an
NLP system can complete this task with reason-
ably high accuracy. We plan to annotate another
evaluation using the same data as part of the in
the SemEval-2015, Task 14
12
adding another task
of template filling where the systems will iden-
tify and normalize ten attributes the identified dis-
ease/disorder mentions.
Acknowledgments
We greatly appreciate the hard work and feed-
back of our program committee members and an-
notators David Harris, Jennifer Green and Glenn
Zaramba. Danielle Mowery, Sumithra Velupillai
and Brett South for helping prepare the manuscript
by summarizing the approaches used by various
systems. This shared task was partially sup-
ported by Shared Annotated Resources (ShARe)
project NIH 5R01GM090187 and Temporal His-
tories of Your Medical Events (THYME) project
(NIH R01LM010090 and U54LM008748).
References
Giuseppe Attardi, Vitoria Cozza, and Daniele Sartiano.
2014. UniPi: Recognition of mentions of disorders
in clinical text. In Proceedings of the International
Workshop on Semantic Evaluations, Dublin, Ireland,
August.
Olivier Bodenreider and Alexa McCray. 2003. Ex-
ploring semantic groups through visual approaches.
Journal of Biomedical Informatics, 36:414?432.
Keith E. Campbell, Diane E. Oliver, and Edward H.
Shortliffe. 1998. The Unified Medical Language
System: Towards a collaborative approach for solv-
ing terminologic problems. J Am Med Inform Assoc,
5(1):12?16.
Omid Ghiasvand and Rohit J. Kate. 2014. UWM: Dis-
order mention extraction from clinical text using crfs
and normalization using learned edit distance pat-
terns. In Proceedings of the International Workshop
on Semantic Evaluations, Dublin, Ireland, August.
Suwisa Kaewphan, Kai Hakaka1, and Filip Ginter.
2014. UTU: Disease mention recognition and nor-
malization with crfs and vector space representa-
tions. In Proceedings of the International Workshop
on Semantic Evaluations, Dublin, Ireland, August.
Melinda Katona and Rich?ard Farkas. 2014. SZTE-
NLP: Clinical text analysis with named entity recog-
nition. In Proceedings of the International Work-
shop on Semantic Evaluations, Dublin, Ireland, Au-
gust.
Andr?e Leal, Diogo Gonc?alves, Bruno Martins, and
Francisco M. Couto. 2014. ULisboa: Identifica-
tion and classification of medical concepts. In Pro-
ceedings of the International Workshop on Semantic
Evaluations, Dublin, Ireland, August.
12
http://alt.qcri.org/semeval2015/task14
61
Robert Leaman and Graciela Gonzalez. 2008. Ban-
ner: an executable survey of advances in biomedical
named entity recognition. In Pacific Symposium on
Biocomputing, volume 13, pages 652?663.
S?ergio Matos, Tiago Nunes, and Jos?e Lu??s Oliveira.
2014. BioinformaticsUA: Concept recognition in
clinical narratives using a modular and highly ef-
ficient text processing framework. In Proceedings
of the International Workshop on Semantic Evalua-
tions, Dublin, Ireland, August.
Sungrim Moon, Serguei Pakhomov, and Genevieve B
Melton. 2012. Automated disambiguation of
acronyms and abbreviations in clinical texts: Win-
dow and training size considerations. In AMIA Annu
Symp Proc, pages 1310?1319.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
Roberto Navigli. 2009. Word sense disambiguation.
ACM Computing Surveys, 41(2):1?69, February.
Ankur Parikh, Avinesh PVS, Joy Mustafi, Lalit Agar-
walla, and Ashish Mungi. 2014. ThinkMiners:
SemEval-2014 task 7: Analysis of clinical text. In
Proceedings of the International Workshop on Se-
mantic Evaluations, Dublin, Ireland, August.
Sameer Pradhan, No?emie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, Wendy W. Chapman, and Guergana
Savova. 2013. Task 1: ShARe/CLEF eHealth
Evaluation Lab 2013. In Working Notes of CLEF
eHealth Evaluation Labs.
Sameer Pradhan, No?emie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, Wendy W. Chapman, and Guergana
Savova. 2014. Evaluating the state of the art in
disorder recognition and normalization of the clin-
ical narrative. In Journal of the American Medical
Informatics Association (to appear).
S. V. Ramanan and P. Senthil Nathan. 2014. RelA-
gent: Entity detection and normalization for diseases
in clinical records: a linguistically driven approach.
In Proceedings of the International Workshop on Se-
mantic Evaluations, Dublin, Ireland, August.
Angus Roberts, Robert Gaizauskas, Mark Hepple,
George Demetriou, Yikun Guo, Ian Roberts, and
Andrea Setzer. 2009. Building a semantically an-
notated corpus of clinical texts. J Biomed Inform,
42(5):950?66.
Mohammed Saeed, C. Lieu, G. Raber, and R.G. Mark.
2002. MIMIC II: a massive temporal ICU patient
database to support research in intelligent patient
monitoring. Comput Cardiol, 29.
Guergana K. Savova, A. R. Coden, I. L. Sominsky,
R. Johnson, P. V. Ogren, P. C. de Groen, and C. G.
Chute. 2008. Word sense disambiguation across
two domains: Biomedical literature and clinical
notes. J Biomed Inform, 41(6):1088?1100, Decem-
ber.
Weiyi Sun, Anna Rumshisky, and
?
Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 Challenge. Journal of the American Med-
ical Informatics Association, 20(5):806?13.
Hanna Suominen, Sanna Salanter?a, Sumithra Velupil-
lai, Wendy W. Chapman, Guergana Savova,
Noemie Elhadad, Sameer Pradhan, Brett R. South,
Danielle L. Mowery, Gareth J. F. Jones, Johannes
Leveling, Liadh Kelly, Lorraine Goeuriot, David
Martinez, and Guido Zuccon. 2013. Overview of
the ShARe/CLEF eHealth evaluation lab 2013. In
Working Notes of CLEF eHealth Evaluation Labs.
?
Ozlem Uzuner, Brett R South, Shuying Shen, and
Scott L DuVall. 2011. 2010 i2b2/VA challenge on
concepts, assertions, and relations in clinical text.
Journal of the American Medical Informatics Asso-
ciation, 18(5):552?556.
?
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Jour-
nal of American Medical Informatics Association,
19(5):786?791, September.
Yaoyun Zhang, Jingqi Wang, Buzhou Tang, Yonghui
Wu, Min Jiang, Yukun Chen, and Hua Xu. 2014.
UTH CCB: A report for SemEval 2014 task 7 anal-
ysis of clinical text. In Proceedings of the Interna-
tional Workshop on Semantic Evaluations, Dublin,
Ireland, August.
62
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 54?60,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Exemplar-based Word-Space Model for Compositionality Detection: Shared
task system description
Siva Reddy
University of York, UK
siva@cs.york.ac.uk
Suresh Manandhar
University of York, UK
suresh@cs.york.ac.uk
Diana McCarthy
Lexical Computing Ltd, UK
diana@dianamccarthy.co.uk
Spandana Gella
University of York, UK
spandana@cs.york.ac.uk
Abstract
In this paper, we highlight the problems of
polysemy in word space models of compo-
sitionality detection. Most models represent
each word as a single prototype-based vec-
tor without addressing polysemy. We propose
an exemplar-based model which is designed
to handle polysemy. This model is tested for
compositionality detection and it is found to
outperform existing prototype-based models.
We have participated in the shared task (Bie-
mann and Giesbrecht, 2011) and our best per-
forming exemplar-model is ranked first in two
types of evaluations and second in two other
evaluations.
1 Introduction
In the field of computational semantics, to represent
the meaning of a compound word, two mechanisms
are commonly used. One is based on the distribu-
tional hypothesis (Harris, 1954) and the other is on
the principle of semantic compositionality (Partee,
1995, p. 313).
The distributional hypothesis (DH) states that
words that occur in similar contexts tend to have
similar meanings. Using this hypothesis, distribu-
tional models like the Word-space model (WSM,
Sahlgren, 2006) represent a target word?s meaning
as a context vector (location in space). The simi-
larity between two meanings is the closeness (prox-
imity) between the vectors. The context vector of a
target word is built from its distributional behaviour
observed in a corpus. Similarly, the context vector of
a compound word can be built by treating the com-
pound as a single word. We refer to such a vector as
a DH-based vector.
The other mechanism is based on the principle of
semantic compositionality (PSC) which states that
the meaning of a compound word is a function of,
and only of, the meaning of its parts and the way
in which the parts are combined. If the meaning of
a part is represented in a WSM using the distribu-
tional hypothesis, then the principle can be applied
to compose the distributional behaviour of a com-
pound word from its parts without actually using the
corpus instances of the compound. We refer to this
as a PSC-based vector. So a PSC-based is composed
of component DH-based vectors.
Both of these two mechanisms are capable of de-
termining the meaning vector of a compound word.
For a given compound, if a DH-based vector and
a PSC-based vector of the compound are projected
into an identical space, one would expect the vec-
tors to occupy the same location i.e. both the vectors
should be nearly the same. However the principle
of semantic compositionality does not hold for non-
compositional compounds, which is actually what
the existing WSMs of compositionality detection ex-
ploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006;
Schone and Jurafsky, 2001). The DH-based and
PSC-based vectors are expected to have high simi-
larity when a compound is compositional and low
similarity for non-compositional compounds.
Most methods in WSM (Turney and Pantel, 2010)
represent a word as a single context vector built from
merging all its corpus instances. Such a representa-
tion is called the prototype-based modelling (Mur-
phy, 2002). These prototype-based vectors do not
54
distinguish the instances according to the senses of
a target word. Since most compounds are less am-
biguous than single words, there is less need for dis-
tinguishing instances in a DH-based prototype vec-
tor of a compound and we do not address that here
but leave ambiguity of compounds for future work.
However the constituent words of the compound are
more ambiguous. When DH-based vectors of the
constituent words are used for composing the PSC-
based vector of the compound, the resulting vec-
tor may contain instances, and therefore contexts,
that are not relevant for the given compound. These
noisy contexts effect the similarity between the PSC-
based vector and the DH-based vector of the com-
pound. Basing compositionality judgements on a
such a noisy similarity value is no longer reliable.
In this paper, we address this problem of pol-
ysemy of constituent words of a compound using
an exemplar-based modelling (Smith and Medin,
1981). In exemplar-based modelling of WSM (Erk
and Pado?, 2010), each word is represented by all its
corpus instances (exemplars) without merging them
into a single vector. Depending upon the purpose,
only relevant exemplars of the target word are acti-
vated and then these are merged to form a refined
prototype-vector which is less-noisy compared to
the original prototype-vector. Exemplar-based mod-
els are more powerful than prototype-based ones be-
cause they retain specific instance information.
We have evaluated our models on the validation
data released in the shared task (Biemann and Gies-
brecht, 2011). Based on the validation results, we
have chosen three systems for public evaluation and
participated in the shared task (Biemann and Gies-
brecht, 2011).
2 Word Space Model
In this section, construction of WSM for all our ex-
periments is described. We use Sketch Engine1 (Kil-
garriff et al, 2004) to retrieve all the exemplars for
a target word or a pattern using corpus query lan-
guage. Let w1 w2 be a compound word with con-
stituent words w1 and w2. Ew denotes the set of
exemplars of w. Vw is the prototype vector of the
word w, which is built by merging all the exemplars
in Ew
1Sketch Engine http://www.sketchengine.co.uk
For the purposes of producing a PSC-based vector
for a compound, a vector of a constituent word is
built using only the exemplars which do not contain
the compound. Note that the vectors are sensitive
to a compound?s word-order since the exemplars of
w1 w2 are not the same as w2 w1.
We use other WSM settings following Mitchell
and Lapata (2008). The dimensions of the WSM
are the top 2000 content words in the given corpus
(along with their coarse-grained part-of-speech in-
formation). Cosine similarity (sim) is used to mea-
sure the similarity between two vectors. Values at
the specific positions in the vector representing con-
text words are set to the ratio of the probability of
the context word given the target word to the overall
probability of the context word. The context window
of a target word?s exemplar is the whole sentence of
the target word excluding the target word. Our lan-
guage of interest is English. We use the ukWaC cor-
pus (Ferraresi et al, 2008) for producing out WSMs.
3 Related Work
As described in Section 1, most WSM models for
compositionality detection measure the similarity
between the true distributional vector Vw1w2 of the
compound and the composed vector Vw1?w2 , where
? denotes a compositionality function. If the simi-
larity is high, the compound is treated as composi-
tional or else non-compositional.
Giesbrecht (2009); Katz and Giesbrecht (2006);
Schone and Jurafsky (2001) obtained the compo-
sitionality vector of w1 w2 using vector addition
Vw1?w2 = aVw1 + bVw2 . In this approach, if
sim(Vw1?w2 , Vw1w2) > ?, the compound is clas-
sified as compositional, where ? is a threshold for
deciding compositionality. Global values of a and b
were chosen by optimizing the performance on the
development set. It was found that no single thresh-
old value ? held for all compounds. Changing the
threshold alters performance arbitrarily. This might
be due to the polysemous nature of the constituent
words which makes the composed vector Vw1?w2
filled with noisy contexts and thus making the judge-
ment unpredictable.
In the above model, if a=0 and b=1, the result-
ing model is similar to that of Baldwin et al (2003).
They also observe similar behaviour of the thresh-
55
old ?. We try to address this problem by addressing
the polysemy in WSMs using exemplar-based mod-
elling.
The above models use a simple addition based
compositionality function. Mitchell and Lapata
(2008) observed that a simple multiplication func-
tion modelled compositionality better than addi-
tion. Contrary to that, Guevara (2011) observed
additive models worked well for building composi-
tional vectors. In our work, we try using evidence
from both compositionality functions, simple addi-
tion and simple multiplication.
Bannard et al (2003); McCarthy et al (2003) ob-
served that methods based on distributional similar-
ities between a phrase and its constituent words help
when determining the compositionality behaviour of
phrases. We therefore also use evidence from the
similarities between each constituent word and the
compound.
4 Our Approach: Exemplar-based Model
Our approach works as follows. Firstly, given a
compound w1 w2, we build its DH-based proto-
type vector Vw1w2 from all its exemplars Ew1w2 .
Secondly, we remove irrelevant exemplars in Ew1
and Ew2 of constituent words and build the refined
prototype vectors Vwr1 and Vwr2 of the constituent
words w1 and w2 respectively. These refined vec-
tors are used to compose the PSC-based vectors 2 of
the compound. Related work to ours is (Reisinger
and Mooney, 2010) where exemplars of a word are
first clustered and then prototype vectors are built.
This work does not relate to compositionality but to
measuring semantic similarity of single words. As
such, their clusters are not influenced by other words
whereas in our approach for detecting composition-
ality, the other constituent word plays a major role.
We use the compositionality functions, sim-
ple addition and simple multiplication to build
Vwr1+wr2 and Vwr1?wr2 respectively. Based on
the similarities sim(Vw1w2 , Vwr1), sim(Vw1w2 , Vwr2),
sim(Vw1w2 , Vwr1+wr2) and sim(Vw1w2 , Vwr1?wr2), we
decide if the compound is compositional or non-
compositional. These steps are described in a little
more detail below.
2Note that we use two PSC-based vectors for representing a
compound.
4.1 Building Refined Prototype Vectors
We aim to remove irrelevant exemplars of one con-
stituent word with the help of the other constituent
word?s distributional behaviour. For example, let
us take the compound traffic light. Light occurs
in many contexts such as quantum theory, optics,
lamps and spiritual theory. In ukWaC, light has
316,126 instances. Not all these exemplars are rel-
evant to compose the PSC-based vector of traffic
light. These irrelevant exemplars increases the se-
mantic differences between traffic light and light and
thus increase the differences between Vtraffic?light
and Vtraffic light. sim(Vlight, Vtraffic light) is found to be
0.27.
Our intuition and motivation for exemplar re-
moval is that it is beneficiary to choose only the
exemplars of light which share similar contexts of
traffic since traffic light should have contexts sim-
ilar to both traffic and light if it is compositional.
We rank each exemplar of light based on common
co-occurrences of traffic and also words which are
distributionally similar to traffic. Co-occurrences of
traffic are the context words which frequently occur
with traffic, e.g. car, road etc. Using these, the
exemplar from a sentence such as ?Cameras capture
cars running red lights . . .? will be ranked higher
than one which does not have contexts related to
traffic. The distributionally similar words to traffic
are the words (like synonyms, antonyms) which are
similar to traffic in that they occur in similar con-
texts, e.g. transport, flow etc. Using these distri-
butionally similar words helps reduce the impact of
data sparseness and helps prioritise contexts of traf-
fic which are semantically related. We use Sketch
Engine to compute the scores of a word observed
in a given corpus. Sketch Engine scores the co-
occurrences (collocations) using logDice motivated
by (Curran, 2003) and distributionally related words
using (Rychly? and Kilgarriff, 2007; Lexical Com-
puting Ltd., 2007). For a given word, both of these
scores are normalised in the range (0,1)
All the exemplars of light are ranked based on
the co-occurrences of these collocations and distri-
butionally related words of traffic using
strafficE ? Elight =
?
c ? E
xEc ? y
traffic
c (1)
where strafficE ? Elight stands for the relevance score of the
56
exemplar E w.r.t. traffic, c for context word in the
exemplar E, xEc is the coordinate value (contextual
score) of the context word c in the exemplar E and
ytrafficc is the score of the context word c w.r.t. traffic.
A refined prototype vector of light is then built by
merging the top n exemplars of light
Vlightr =
n?
ei?Etrafficlight ;i=0
ei (2)
where Etrafficlight are the set of exemplars of light
ranked using co-occurrence information from the
other constituent word traffic. n is chosen such that
sim(Vlightr , Vtraffic light) is maximised. This similar-
ity is observed to be greatest using just 2286 (less
than 1%) of the total exemplars of light. After ex-
emplar removal, sim(Vlightr , Vtraffic light) increased to
0.47 from the initial value of 0.27. Though n is cho-
sen by maximising similarity, which is not desirable
for non-compositional compounds, the lack of simi-
larity will give the strongest possible indication that
a compound is not compositional.
4.2 Building Compositional Vectors
We use the compositionality functions, simple ad-
dition and simple multiplication to build composi-
tional vectors Vwr1+wr2 and Vwr1?wr2 . These are as de-
scribed in (Mitchell and Lapata, 2008). In model ad-
dition, Vw1?w2 = aVw1 + bVw2 , all the previous ap-
proaches use static values of a and b. Instead, we use
dynamic weights computed from the participating
vectors using a =
sim(Vw1w2 ,Vw1 )
sim(Vw1w2 ,Vw1 )+sim(Vw1w2 ,Vw2 )
and b = 1?a. These weights differ from compound
to compound.
4.3 Compositionality Judgement
To judge if a compound is compositional or non-
compositional, previous approaches (see Section 3)
base their judgement on a single similarity value. As
discussed, we base our judgement based on the col-
lective evidences from all the similarity values using
a linear equation of the form
?(Vwr1 , Vwr2) = a0 + a1.sim(Vw1w2 , Vwr1)
+ a2.sim(Vw1w2 , Vwr2) (3)
+ a3.sim(Vw1w2 , Vwr1+wr2)
+ a4.sim(Vw1w2 , Vwr1?wr2)
Model APD Acc.
Exm-Best 13.09 88.0
Pro-Addn 15.42 76.0
Pro-Mult 17.52 80.0
Pro-Best 15.12 80.0
Table 1: Average Point Difference (APD) and Av-
erage Accuracy (Acc.) of Compositionality Judge-
ments
where the value of ? denotes the compositionality
score. The range of ? is in between 0-100. If ? ?
34, the compound is treated as non-compositional,
34 < ? < 67 as medium compositional and ? ?
67 as highly compositional. The parameters ai?s
are estimated using ordinary least square regression
by training over the training data released in the
shared task (Biemann and Giesbrecht, 2011). For
the three categories ? adjective-noun, verb-object
and subject-verb ? the parameters are estimated sep-
arately.
Note that if a1 = a2 = a4 = 0, the model bases
its judgement only on addition. Similarly if a1 =
a2 = a3 = 0, the model bases its judgement only on
multiplication.
We also experimented with combinations such as
?(Vwr1 , Vw2) and ?(Vw1 , Vwr2) i.e. using refined vec-
tor for one of the constituent word and the unrefined
prototype vector for the other constituent word.
4.4 Selecting the best model
To participate in the shared task, we have selected
the best performing model by evaluating the mod-
els on the validation data released in the shared task
(Biemann and Giesbrecht, 2011). Table 1 displays
the results on the validation data. The average point
difference is calculated by taking the average of the
difference in a model?s score ? and the gold score
annotated by humans, over all compounds. Table 1
also displays the overall accuracy of coarse grained
labels ? low, medium and high.
Best performance for verb(v)-object(o) com-
pounds is found for the combination ?(Vvr , Vor) of
Equation 3. For subject(s)-verb(v) compounds, it is
for ?(Vsr , Vvr) and a3 = a4 = 0. For adjective(j)-
noun(n) compounds, it is ?(Vjr , Vn). We are not
certain of the reason for this difference, perhaps
there may be less ambiguity of words within specific
grammatical relationships or it may be simply due to
57
TotPrd Spearman ? Kendalls ?
Rand-Base 174 0.02 0.02
Exm-Best 169 0.35 0.24
Pro-Best 169 0.33 0.23
Exm 169 0.26 0.18
SharedTaskNextBest 174 0.33 0.23
Table 2: Correlation Scores
the actual compounds in those categories. We leave
analysis of this for future work. We combined the
outputs of these category-specific models to build
the best model Exm-Best.
For comparison, results of standard mod-
els prototype addition (Pro-Addn) and prototype-
multiplication (Pro-Mult) are also displayed in Table
1. Pro-Addn can be represented as ?(Vw1 , Vw2) with
a1 = a2 = a4 = 0. Pro-Mult can be represented as
?(Vw1 , Vw2) with a1 = a2 = a3 = 0. Pro-Best is
the best performing model in prototype-based mod-
elling. It is found to be ?(Vw1 , Vw2). (Note: De-
pending upon the compound type, some of the ai?s
in Pro-Best may be 0).
Overall, exemplar-based modelling excelled in
both the evaluations, average point difference and
coarse-grained label accuracies. The systems Exm-
Best, Pro-Best and Exm ?(Vwr1 , Vwr2) were submit-
ted for the public evaluation in the shared task. All
the model parameters were estimated by regression
on the task?s training data separately for the 3 com-
pound types as described in Section 4.3. We perform
the regression separately for these classes to max-
imise performance. In the future, we will investigate
whether these settings gave us better results on the
test data compared to setting the values the same re-
gardless of the category of compound.
5 Shared Task Results
Table 2 displays Spearman ? and Kendalls ? corre-
lation scores of all the models. TotPrd stands for
the total number of predictions. Rand-Base is the
baseline system which randomly assigns a compo-
sitionality score for a compound. Our model Exm-
Best was the best performing system compared to
all other systems in this evaluation criteria. Shared-
TaskNextBest is the next best performing system
apart from our models. Due to lemmatization er-
rors in the test data, our models could only predict
judgements for 169 out of 174 compounds.
All ADJ-NN V-SUBJ V-OBJ
Rand-Base 32.82 34.57 29.83 32.34
Zero-Base 23.42 24.67 17.03 25.47
Exm-Best 16.51 15.19 15.72 18.6
Pro-Best 16.79 14.62 18.89 18.31
Exm 17.28 15.82 18.18 18.6
SharedTaskBest 16.19 14.93 21.64 14.66
Table 3: Average Point Difference Scores
All ADJ-NN V-SUBJ V-OBJ
Rand-Base 0.297 0.288 0.308 0.30
Zero-Base 0.356 0.288 0.654 0.25
Most-Freq-Base 0.593 0.673 0.346 0.65
Exm-Best 0.576 0.692 0.5 0.475
Pro-Best 0.567 0.731 0.346 0.5
Exm 0.542 0.692 0.346 0.475
SharedTaskBest 0.585 0.654 0.385 0.625
Table 4: Coarse Grained Accuracy
Table 3 displays average point difference scores.
Zero-Base is a baseline system which assigns a score
of 50 to all compounds. SharedTaskBest is the over-
all best performing system. Exm-Best was ranked
second best among all the systems. For ADJ-NN
and V-SUBJ compounds, the best performing sys-
tems in the shared task are Pro-Best and Exm-Best
respectively. Our models did less well on V-OBJ
compounds and we will explore the reasons for this
in future work.
Table 4 displays coarse grained scores. As above,
similar behaviour is observed for coarse grained ac-
curacies. Most-Freq-Base is the baseline system
which assigns the most frequent coarse-grained la-
bel for a compound based on its type (ADJ-NN, V-
SUBJ, V-OBJ) as observed in training data. Most-
Freq-Base outperforms all other systems.
6 Conclusions
In this paper, we examined the effect of polysemy
in word space models for compositionality detec-
tion. We showed exemplar-based WSM is effective
in dealing with polysemy. Also, we use multiple
evidences for compositionality detection rather than
basing our judgement on a single evidence. Over-
all, performance of the Exemplar-based models of
compositionality detection is found to be superior to
prototype-based models.
58
References
Baldwin, T., Bannard, C., Tanaka, T., and Widdows,
D. (2003). An empirical model of multiword ex-
pression decomposability. In Proceedings of the
ACL 2003 workshop on Multiword expressions:
analysis, acquisition and treatment - Volume 18,
MWE ?03, pages 89?96, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bannard, C., Baldwin, T., and Lascarides, A. (2003).
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 work-
shop on Multiword expressions: analysis, ac-
quisition and treatment - Volume 18, MWE ?03,
pages 65?72, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Biemann, C. and Giesbrecht, E. (2011). Distri-
butional semantics and compositionality 2011:
Shared task description and results. In Pro-
ceedings of DISCo-2011 in conjunction with ACL
2011.
Curran, J. R. (2003). From distributional to semantic
similarity. Technical report, PhD Thesis, Univer-
sity of Edinburgh.
Erk, K. and Pado?, S. (2010). Exemplar-based mod-
els for word meaning in context. In Proceed-
ings of the ACL 2010 Conference Short Papers,
ACLShort ?10, pages 92?97, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the WAC4 Workshop at
LREC 2008, Marrakesh, Morocco.
Giesbrecht, E. (2009). In search of semantic com-
positionality in vector spaces. In Proceedings
of the 17th International Conference on Concep-
tual Structures: Conceptual Structures: Leverag-
ing Semantic Technologies, ICCS ?09, pages 173?
184, Berlin, Heidelberg. Springer-Verlag.
Guevara, E. R. (2011). Computing semantic com-
positionality in distributional semantics. In Pro-
ceedings of the Ninth International Conference on
Computational Semantics, IWCS ?2011.
Harris, Z. S. (1954). Distributional structure. Word,
10:146?162.
Katz, G. and Giesbrecht, E. (2006). Automatic
identification of non-compositional multi-word
expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underly-
ing Properties, MWE ?06, pages 12?19, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Kilgarriff, A., Rychly, P., Smrz, P., and Tugwell, D.
(2004). The sketch engine. In Proceedings of EU-
RALEX.
Lexical Computing Ltd. (2007). Statistics used in
the sketch engine.
McCarthy, D., Keller, B., and Carroll, J. (2003).
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions: analysis,
acquisition and treatment - Volume 18, MWE ?03,
pages 73?80, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceed-
ings of ACL-08: HLT, pages 236?244, Columbus,
Ohio. Association for Computational Linguistics.
Murphy, G. L. (2002). The Big Book of Concepts.
The MIT Press.
Partee, B. (1995). Lexical semantics and compo-
sitionality. L. Gleitman and M. Liberman (eds.)
Language, which is Volume 1 of D. Osherson (ed.)
An Invitation to Cognitive Science (2nd Edition),
pages 311?360.
Reisinger, J. and Mooney, R. J. (2010). Multi-
prototype vector-space models of word mean-
ing. In Proceedings of the 11th Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-
2010), pages 109?117.
Rychly?, P. and Kilgarriff, A. (2007). An efficient
algorithm for building a distributional thesaurus
(and other sketch engine developments). In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Ses-
sions, ACL ?07, pages 41?44, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Sahlgren, M. (2006). The Word-Space Model: Us-
ing distributional analysis to represent syntag-
59
matic and paradigmatic relations between words
in high-dimensional vector spaces. PhD thesis,
Stockholm University.
Schone, P. and Jurafsky, D. (2001). Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem? In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?01.
Smith, E. E. and Medin, D. L. (1981). Categories
and concepts / Edward E. Smith and Douglas L.
Medin. Harvard University Press, Cambridge,
Mass. :.
Turney, P. D. and Pantel, P. (2010). From frequency
to meaning: vector space models of semantics. J.
Artif. Int. Res., 37:141?188.
60
