Alternative Approaches for Generating Bodies of Grammar Rules
Gabriel Infante-Lopez and Maarten de Rijke
Informatics Institute, University of Amsterdam
{infante,mdr}@science.uva.nl
Abstract
We compare two approaches for describing and gen-
erating bodies of rules used for natural language
parsing. In today?s parsers rule bodies do not ex-
ist a priori but are generated on the fly, usually with
methods based on n-grams, which are one particu-
lar way of inducing probabilistic regular languages.
We compare two approaches for inducing such lan-
guages. One is based on n-grams, the other on min-
imization of the Kullback-Leibler divergence. The
inferred regular languages are used for generating
bodies of rules inside a parsing procedure. We com-
pare the two approaches along two dimensions: the
quality of the probabilistic regular language they
produce, and the performance of the parser they
were used to build. The second approach outper-
forms the first one along both dimensions.
1 Introduction
N -grams have had a big impact on the state of the
art in natural language parsing. They are central
to many parsing models (Charniak, 1997; Collins,
1997, 2000; Eisner, 1996), and despite their sim-
plicity n-gram models have been very successful.
Modeling with n-grams is an induction task (Gold,
1967). Given a sample set of strings, the task is to
guess the grammar that produced that sample. Usu-
ally, the grammar is not be chosen from an arbitrary
set of possible grammars, but from a given class.
Hence, grammar induction consists of two parts:
choosing the class of languages amongst which to
search and designing the procedure for performing
the search. By using n-grams for grammar induc-
tion one addresses the two parts in one go. In par-
ticular, the use of n-grams implies that the solu-
tion will be searched for in the class of probabilis-
tic regular languages, since n-grams induce prob-
abilistic automata and, consequently, probabilistic
regular languages. However, the class of probabilis-
tic regular languages induced using n-grams is a
proper subclass of the class of all probabilistic reg-
ular languages; n-grams are incapable of capturing
long-distance relations between words. At the tech-
nical level the restricted nature of n-grams is wit-
nessed by the special structure of the automata in-
duced from them, as we will see in Section 4.2.
N -grams are not the only way to induce regular
languages, and not the most powerful way to do so.
There is a variety of general methods capable of in-
ducing all regular languages (Denis, 2001; Carrasco
and Oncina, 1994; Thollard et al, 2000). What is
their relevance for natural language parsing? Re-
call that regular languages are used for describing
the bodies of rules in a grammar. Consequently, the
quality and expressive power of the resulting gram-
mar is tied to the quality and expressive power of the
regular languages used to describe them. And the
quality and expressive power of the latter, in turn,
are influenced directly by the method used to induce
them. These observations give rise to a natural ques-
tion: can we gain anything in parsing from using
general methods for inducing regular languages in-
stead of methods based on n-grams? Specifically,
can we describe the bodies of grammatical rules
more accurately and more concisely by using gen-
eral methods for inducing regular languages?
In the context of natural language parsing we
present an empirical comparison between algo-
rithms for inducing regular languages using n-
grams on the one hand, and more general algorithms
for learning the general class of regular language on
the other hand. We proceed as follows. We gen-
erate our training data from the Wall Street Journal
Section of the Penn Tree Bank (PTB), by transform-
ing it to projective dependency structures, following
(Collins, 1996), and extracting rules from the result.
These rules are used as training material for the rule
induction algorithms we consider. The automata
produced this way are then used to build grammars
which, in turn, are used for parsing.
We are interested in two different aspects of the
use of probabilistic regular languages for natural
language parsing: the quality of the induced au-
tomata and the performance of the resulting parsers.
For evaluation purposes, we use two different met-
rics: perplexity for the first aspect and percentage
of correct attachments for the second. The main re-
sults of the paper are that, measured in terms of per-
plexity, the automata induced by algorithms other
than n-grams describe the rule bodies better than
automata induced using n-gram-based algorithms,
and that, moreover, the gain in automata quality
is reflected by an improvement in parsing perfor-
mance. We also find that the parsing performance
of both methods (n-grams vs. general automata) can
be substantially improved by splitting the training
material into POS categories. As a side product,
we find empirical evidence to suggest that the effec-
tiveness of rule lexicalization techniques (Collins,
1997; Sima?an, 2000) and parent annotation tech-
niques (Klein and Manning, 2003) is due to the fact
that both lead to a reduction in perplexity in the au-
tomata induced from training corpora.
Section 2 surveys our experiments, and later sec-
tions provide details of the various aspects. Sec-
tion 3 offers details on our grammatical frame-
work, PCW-grammars, on transforming automata
to PCW-grammars, and on parsing with PCW-
grammars. Section 4 explains the starting point of
this process: learning automata, and Section 5 re-
ports on parsing experiments. We discuss related
work in Section 6 and conclude in Section 7.
2 Overview
We want to build grammars using different algo-
rithms for inducing their rules. Our main question
is aimed at understanding how different algorithms
for inducing regular languages impact the parsing
performance with those grammars. A second issue
that we want to explore is how the grammars per-
form when the quality of the training material is im-
proved, that is, when the training material is sep-
arated into part of speech (POS) categories before
the regular language learning algorithms are run.
We first transform the PTB into projective depen-
dencies structures following (Collins, 1996). From
the resulting tree bank we delete all lexical informa-
tion except POS tags. Every POS in a tree belonging
to the tree-bank has associated to it two different,
possibly empty, sequences of right and left depen-
dents, respectively. We extract all these sequences
for all trees, producing two different sets containing
right and left sequences of dependents respectively.
These two sets form the training material used for
building four different grammars. The four gram-
mars differ along two dimensions: the number of
automata used for building them and the algorithm
used for inducing the automata. As to the latter di-
mension, in Section 4 we use two algorithms: the
Minimum Discriminative Information (MDI) algo-
rithm, and a bigram-based algorithm. As to the for-
mer dimension, two of the grammars are built us-
ing only two different automata, each of which is
built using the two sample set generated from the
PTB. The other two grammars were built using two
automata per POS, exploiting a split of the train-
ing samples into multiple samples, two samples per
POS, to be precise, each containing only those sam-
ples where the POS appeared as the head.
The grammars built from the induced automata
are so-called PCW-grammars (see Section 3), a for-
malism based on probabilistic context free gram-
mars (PCFGs); as we will see in Section 3, inferring
them from automata is almost immediate.
3 Grammatical Framework
We briefly detail the grammars we work with
(PCW-grammars), how automata give rise to these
grammars, and how we parse using them.
3.1 PCW-Grammars
We need a grammatical framework that models
rule bodies as instances of a regular language and
that allows us to transform automata to gram-
mars as directly as possible. We decided to em-
bed them in the general grammatical framework of
CW-grammars (Infante-Lopez and de Rijke, 2003):
based on PCFGs, they have a clear and well-
understood mathematical background and we do not
need to implement ad-hoc parsing algorithms.
A probabilistic constrained W-grammar (PCW-
grammar) consists of two different sets of PCF-like
rules called pseudo-rules and meta-rules respec-
tively and three pairwise disjoint sets of symbols:
variables, non-terminals and terminals. Pseudo-
rules and meta-rules provide mechanisms for build-
ing ?real? rewrite rules. We use ? w=? ? to indicate
that ? should be rewritten as ?. In the case of PCW-
grammars, rewrite rules are built by first selecting a
pseudo-rule, and then using meta-rules for instanti-
ating all the variables in the body of the pseudo-rule.
To illustrate these concepts, we provide an exam-
ple. Let W = (V,NT, T, S, m??, s??) be a CW-
grammar such that the set of variable, non-terminals
meta-rules pseudo-rules
Adj m??0.5 AdjAdj S s??1 AdjNoun
Adj m??0.5 Adj Adj s??0.1 big
Noun s??1 ball
.
.
.
and terminals are defined as follows: V = {Adj },
NT = {S, Adj , Noun}, T = {ball , big , fat ,
red , green , . . .}. As usual, the numbers attached
to the arrows indicate the probabilities of the rules.
The rules defined by W have the following shape:
S w=? Adj ? Noun . Suppose now that we want to
build the rule S w=? Adj Adj Noun . We take the
pseudo-rule S s??1 Adj Noun and instantiate the
variable Adj with Adj Adj to get the desired rule.
The probability for it is 1 ? 0.5 ? 0.5, that is, the
probability of the derivation for Adj Adj times the
probability of the pseudo-rule used. Trees for this
particular grammar are flat, with a main node S and
all the adjectives in it as daughters. An example
derivation is given in Figure 1(a).
3.2 From Automata to Grammars
Now that we have introduced PCW-grammars, we
describe how we build them from the automata
that we are going to induce in Section 4. Since
we will induce two families of automata (?Many-
Automata? where we use two automata per POS,
and ?One-Automaton? where we use only two au-
tomata to fit every POS), we need to describe two
automata-to-grammar transformations.
Let?s start with the case where we build two au-
tomata per POS. Let w be a POS in the PTB; let AwL
and AwR be the two automata associated to it. Let GwL
and GwR be the PCFGs equivalent to AwL and AwR, re-
spectively, following (Abney et al, 1999), and let
SwL and SwR be the starting symbols of GwL and GwR,
respectively. We build our final grammar G with
starting symbol S, by defining its meta-rules as the
disjoint union of all rules in GwL and GwR (for all POS
w), its set of pseudo-rules as the union of the sets
{W s??1 SwLwSwR and S
s??1 SwLwSwR}, where
W is a unique new variable symbol associated to w.
When we use two automata for all parts of
speech, the grammar is defined as follows. Let AL
and AR be the two automata learned. Let GL and
GR be the PCFGs equivalent to AL and AR, and let
SL and SR be the starting symbols of GL and GR,
respectively. Fix a POS w in the PTB. Since the au-
tomata are deterministic, there exist states SwL and
SwR that are reachable from SL and SR, respectively,
by following the arc labeled with w. Define a gram-
mar as in the previous case. Its starting symbol is S,
its set of meta-rules is the disjoint union of all rules
in GwL and GwR (for all POS w), its set of pseudo-
rules is {W s??1 SwLwSwR , S
s??1 SwLwSwR :
w is a POS in the PTB and W is a unique new vari-
able symbol associated to w}.
3.3 Parsing PCW-Grammars
Parsing PCW-grammars requires two steps: a
generation-rule step followed by a tree-building
step. We now explain how these two steps can be
carried out in one go. Parsing with PCW-grammars
can be viewed as parsing with PCF grammars. The
main difference is that in PCW-parsing derivations
for variables remain hidden in the final tree. To clar-
ify this, consider the trees depicted in Figure 1; the
tree in part (a) is the CW-tree corresponding to the
word red big green ball, and the tree in part (b) is
the same tree but now the instantiations of the meta-
rules that were used have been made visible.
S
Adj
red
Adj
big
Adj
green
Noun
ball
S
Adj 1
Adj 1
Adj 1
Adj
red
Adj
big
Adj
green
Noun
ball
(a) (b)
Figure 1: (a) A tree generated by W . (b) The same
tree with meta-rule derivations made visible.
To adapt a PCFG to parse CW-grammars, we
need to define a PCF grammar for a given PCW-
grammar by adding the two sets of rules while mak-
ing sure that all meta-rules have been marked some-
how. In Figure 1(b) the head symbols of meta-rules
have been marked with the superscript 1. After pars-
ing the sentence with the PCF parser, all marked
rules should be collapsed as shown in part (a).
4 Building Automata
The four grammars we intend to induce are com-
pletely defined once the underlying automata have
been built. We now explain how we build those au-
tomata from the training material. We start by de-
tailing how the material is generated.
4.1 Building the Sample Sets
We transform the PTB, sections 2?22, to depen-
dency structures, as suggested by (Collins, 1999).
All sentences containing CC tags are filtered out,
following (Eisner, 1996). We also eliminate all
word information, leaving only POS tags. For each
resulting dependency tree we extract a sample set of
right and left sequences of dependents as shown in
Figure 2. From the tree we generate a sample set
with all right sequences of dependents {, , }, and
another with all left sequences {, , red big green}.
The sample set used for automata induction is the
union of all individual tree sample sets.
4.2 Learning Probabilistic Automata
Probabilistic deterministic finite state automata
(PDFA) inference is the problem of inducing a
stochastic regular grammar from a sample set of
strings belonging to an unknown regular language.
The most direct approach for solving the task is by
SJJ
jj
red
JJ
jj
big
JJ
jj
green
nn
ball
ballgreenbigred
(a) (b)
jj jj nn
left right left right left right
    red big green 
(c)
Figure 2: (a), (b) Dependency representations of
Figure 1. (c) Sample instances extracted from this
tree.
using n-grams. The n-gram induction algorithm
adds a state to the resulting automaton for each se-
quence of symbols of length n it has seen in the
training material; it also adds an arc between states
a? and ?b labeled b, if the sequence a?b appears
in the training set. The probability assigned to the
arc (a?, ?b) is proportional to the number of times
the sequence a?b appears in the training set. For the
remainder, we take n-grams to be bigrams.
There are other approaches to inducing regular
grammars besides ones based on n-grams. The first
algorithm to learn PDFAs was ALERGIA (Carrasco
and Oncina, 1994); it learns cyclic automata with
the so-called state-merging method. The Minimum
Discrimination Information (MDI) algorithm (Thol-
lard et al, 2000) improves over ALERGIA and uses
Kullback-Leibler divergence for deciding when to
merge states. We opted for the MDI algorithm as
an alternative to n-gram based induction algorithms,
mainly because their working principles are rad-
ically different from the n-gram-based algorithm.
The MDI algorithm first builds an automaton that
only accepts the strings in the sample set by merg-
ing common prefixes, thus producing a tree-shaped
automaton in which each transition has a probability
proportional to the number of times it is used while
generating the positive sample.
The MDI algorithm traverses the lattice of all
possible partitions for this general automaton, at-
tempting to merge states that satisfy a trade-off that
can be specified by the user. Specifically, assume
that A1 is a temporary solution of the algorithm
and that A2 is a tentative new solution derived from
A1. ?(A1, A2) = D(A0||A2) ? D(A0||A1) de-
notes the divergence increment while going from
A1 to A2, where D(A0||Ai) is the Kullback-Leibler
divergence or relative entropy between the two
distributions generated by the corresponding au-
tomata (Cover and Thomas, 1991). The new solu-
tion A2 is compatible with the training data if the
divergence increment relative to the size reduction,
that is, the reduction of the number of states, is small
enough. Formally, let alha denote a compatibil-
ity threshold; then the compatibility is satisfied if
?(A1,A2)
|A1|?|A2| < alpha. For this learning algorithm,
alpha is the unique parameter; we tuned it to get
better quality automata.
4.3 Optimizing Automata
We use three measures to evaluate the quality of
a probabilistic automaton (and set the value of
alpha optimally). The first, called test sample
perplexity (PP), is based on the per symbol log-
likelihood of strings x belonging to a test sam-
ple according to the distribution defined by the au-
tomaton. Formally, LL = ? 1|S|
?
x?S log (P (x)),
where P (x) is the probability assigned to the string
x by the automata. The perplexity PP is defined as
PP = 2LL. The minimal perplexity PP = 1 is
reached when the next symbol is always predicted
with probability 1 from the current state, while
PP = |?| corresponds to uniformly guessing from
an alphabet of size |?|.
The second measure we used to evaluate the qual-
ity of an automaton is the number of missed samples
(MS). A missed sample is a string in the test sam-
ple that the automaton failed to accept. One such
instance suffices to have PP undefined (LL infinite).
Since an undefined value of PP only witnesses the
presence of at least one MS we decided to count the
number of MS separately, and compute PP without
taking MS into account. This choice leads to a more
accurate value of PP, while, moreover, the value of
MS provides us with information about the general-
ization capacity of automata: the lower the value of
MS, the larger the generalization capacities of the
automaton. The usual way to circumvent undefined
perplexity is to smooth the resulting automaton with
unigrams, thus increasing the generalization capac-
ity of the automaton, which is usually paid for with
an increase in perplexity. We decided not to use
any smoothing techniques as we want to compare
bigram-based automata with MDI-based automata
in the cleanest possible way. The PP and MS mea-
sures are relative to a test sample; we transformed
section 00 of the PTB to obtain one.1
1If smoothing techniques are used for optimizing automata
based on n-grams, they should also be used for optimizing
MDI-based automata. A fair experiment for comparing the
two automata-learning algorithms using smoothing techniques
would consist of first building two pairs of automata. The first
pair would consist of the unigram-based automaton together
The third measure we used to evaluate the quality
of automata concerns the size of the automata. We
compute NumEdges and NumStates (the number of
edges and the number of states of the automaton).
We used PP, US, NumEdges, and NumStates to
compare automata. We say that one automaton is of
a better quality than another if the values of the 4
indicators are lower for the first than for the sec-
ond. Our aim is to find a value of alpha that
produces an automaton of better quality than the
bigram-based counterpart. By exhaustive search,
using all training data, we determined the optimal
value of alpha. We selected the value of alpha
for which the MDI-based automaton outperforms
the bigram-based one.2
We exemplify our procedure by considering au-
tomata for the ?One-Automaton? setting (where we
used the same automata for all parts of speech). In
Figure 3 we plot all values of PP and MS computed
for different values of alpha, for each training set
(i.e., left and right). From the plots we can identify
values of alpha that produce automata having bet-
ter values of PP and MS than the bigram-based ones.
All such alphas are the ones inside the marked
areas; automata induced using those alphas pos-
sess a lower value of PP as well as a smaller num-
ber of MS, as required. Based on these explorations
MDI Bigrams
Right Left Right Left
NumEdges 268 328 20519 16473
NumStates 12 15 844 755
Table 1: Automata sizes for the ?One-Automaton?
case, with alpha = 0.0001.
we selected alpha = 0.0001 for building the au-
tomata used for grammar induction in the ?One-
Automaton? case. Besides having lower values of
PP and MS, the resulting automata are smaller than
the bigram based automata (Table 1). MDI com-
presses information better; the values in the tables
with an MDI-based automaton outperforming the unigram-
based one. The second one, a bigram-based automata together
with an MDI-based automata outperforming the bigram-based
one. Second, the two n-gram based automata smoothed into a
single automaton have to be compared against the two MDI-
based automata smoothed into a single automaton. It would
be hard to determine whether the differences between the final
automata are due to smoothing procedure or to the algorithms
used for creating the initial automata. By leaving smoothing
out of the picture, we obtain a clearer understanding of the dif-
ferences between the two automata induction algorithms.
2An equivalent value of alpha can be obtained indepen-
dently of the performance of the bigram-based automata by
defining a measure that combines PP and MS. This measure
should reach its maximum when PP and MS reach their mini-
mums.
suggest that MDI finds more regularities in the sam-
ple set than the bigram-based algorithm.
To determine optimal values for the ?Many-
Automata? case (where we learned two automata
for each POS) we used the same procedure as
for the ?One-Automaton? case, but now for ev-
ery individual POS. Because of space constraints
we are not able to reproduce analogues of Fig-
ure 3 and Table 1 for all parts of speech. Figure 4
contains representative plots; the remaining plots
are available online at http://www.science.
uva.nl/?infante/POS.
Besides allowing us to find the optimal alphas,
the plots provide us with a great deal of informa-
tion. For instance, there are two remarkable things
in the plots for VBP (Figure 4, second row). First,
it is one of the few examples where the bigram-
based algorithm performs better than the MDI al-
gorithm. Second, the values of PP in this plot are
relatively high and unstable compared to other POS
plots. Lower perplexity usually implies better qual-
ity automata, and as we will see in the next section,
better automata produce better parsers. How can we
obtain lower PP values for the VBP automata? The
class of words tagged with VBP harbors many dif-
ferent behaviors, which is not surprising, given that
verbs can differ widely in terms of, e.g., their sub-
categorization frames. One way to decrease the PP
values is to split the class of words tagged with VBP
into multiple, more homogeneous classes. Note
from Figures 3 and 4 that splitting the original sam-
ple sets into POS-dependent sets produces a huge
decrease on PP. One attempt to implement this idea
is lexicalization: increasing the information in the
POS tag by adding the lemma to it (Collins, 1997;
Sima?an, 2000). Lexicalization splits the class of
verbs into a family of singletons producing more ho-
mogeneous classes, as desired. A different approach
(Klein and Manning, 2003) consists in adding head
information to dependents; words tagged with VBP
are then split into classes according to the words that
dominate them in the training corpus.
Some POS present very high perplexities, but
tags such as DT present a PP close to 1 (and 0 MS)
for all values of alpha. Hence, there is no need
to introduce further distinctions in DT, doing so will
not increase the quality of the automata but will in-
crease their number; splitting techniques are bound
to add noise to the resulting grammars. The plots
also indicate that the bigram-based algorithm cap-
tures them as well as the MDI algorithm.
In Figure 4, third row, we see that the MDI-based
automata and the bigram-based automata achieve
the same value of PP (close to 5) for NN, but
 0
 5
 10
 15
 20
 25
 5e-05  0.0001  0.00015  0.0002  0.00025  0.0003  0.00035  0.0004
Alpha
Unique Automaton - Left Side
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
 0
 5
 10
 15
 20
 25
 30
 5e-05  0.0001  0.00015  0.0002  0.00025  0.0003  0.00035  0.0004
Alpha
Unique Automaton - Right Side
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
Figure 3: Values of PP and MS for automata used in building One-Automaton grammars. (X-axis): alpha.
(Y-axis): missed samples (MS) and perplexity (PP). The two constant lines represent the values of PP and
MS for the bigram-based automata.
 3
 4
 5
 6
 7
 8
 9
0.
0e
+0
0
2.
0e
-0
5
4.
0e
-0
5
6.
0e
-0
5
8.
0e
-0
5
1.
0e
-0
4
1.
2e
-0
4
1.
4e
-0
4
1.
6e
-0
4
1.
8e
-0
4
2.
0e
-0
4
Alpha
VBP - LeftSide
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
 3
 4
 5
 6
 7
 8
 9
0.
0e
+0
0
2.
0e
-0
5
4.
0e
-0
5
6.
0e
-0
5
8.
0e
-0
5
1.
0e
-0
4
1.
2e
-0
4
1.
4e
-0
4
1.
6e
-0
4
1.
8e
-0
4
2.
0e
-0
4
Alpha
VBP - LeftSide
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
 0
 5
 10
 15
 20
 25
 30
0.
0e
+0
0
2.
0e
-0
5
4.
0e
-0
5
6.
0e
-0
5
8.
0e
-0
5
1.
0e
-0
4
1.
2e
-0
4
1.
4e
-0
4
1.
6e
-0
4
1.
8e
-0
4
2.
0e
-0
4
Alpha
NN - LeftSide
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
 0
 5
 10
 15
 20
 25
 30
0.
0e
+0
0
2.
0e
-0
5
4.
0e
-0
5
6.
0e
-0
5
8.
0e
-0
5
1.
0e
-0
4
1.
2e
-0
4
1.
4e
-0
4
1.
6e
-0
4
1.
8e
-0
4
2.
0e
-0
4
Alpha
NN - RightSide
MDI Perplex. (PP)
Bigram Perplex. (PP)
MDI Missed Samples (MS)
Bigram Missed Samples (MS)
Figure 4: Values of PP and MS for automata for ad-hoc automata
the MDI misses fewer examples for alphas big-
ger than 1.4e ? 04. As pointed out, we built the
One-Automaton-MDI using alpha = 0.0001 and
even though the method allows us to fine-tune each
alpha in the Many-Automata-MDI grammar, we
used a fixed alpha = 0.0002 for all parts of speech,
which, for most parts of speech, produces better au-
tomata than bigrams. Table 2 lists the sizes of the
automata. The differences between MDI-based and
bigram-based automata are not as dramatic as in
the ?One-Automaton? case (Table 1), but the former
again have consistently lower NumEdges and Num-
States values, for all parts of speech, even where
bigram-based automata have a lower perplexity.
MDI Bigrams
POS Right Left Right Left
DT NumEdges 21 14 35 39
NumStates 4 3 25 17
VBP NumEdges 300 204 2596 1311
NumStates 50 45 250 149
NN NumEdges 104 111 3827 4709
NumStates 6 4 284 326
Table 2: Automata sizes for the three parts of speech
in the ?Many-Automata? case, with alpha =
0.0002 for parts of speech.
5 Parsing the PTB
We have observed remarkable differences in quality
between MDI-based and bigram-based automata.
Next, we present the parsing scores, and discuss the
meaning of the measures observed for automata in
the context of the grammars they produce. The mea-
sure that translates directly from automata to gram-
mars is automaton size. Since each automaton is
transformed into a PCFG, the number of rules in
the resulting grammar is proportional to the number
of arcs in the automaton, and the number of non-
terminals is proportional to the number of states.
From Table 3 we see that MDI compresses informa-
tion better: the sizes of the grammars produced by
the MDI-based automata are an order of magnitude
smaller that those produced using bigram-based au-
tomata. Moreover, the ?One-Automaton? versions
substantially reduce the size of the resulting gram-
mars; this is obviously due to the fact that all POS
share the same underlying automaton so that infor-
mation does not need to be duplicated across parts
of speech. To understand the meaning of PP and
One Automaton Many Automata
MDI Bigram MDI Bigram
702 38670 5316 68394
Table 3: Number of rules in the grammars built.
MS in the context of grammars it helps to think of
PCW-parsing as a two-phase procedure. The first
phase consists of creating the rules that will be used
in the second phase. And the second phase con-
sists in using the rules created in the first phase as a
PCFG and parsing the sentence using a PCF parser.
Since regular expressions are used to build rules, the
values of PP and MS quantify the quality of the set
of rules built for the second phase: MS gives us a
measure of the number rule bodies that should be
created but that will not be created, and, hence, it
gives us a measure of the number of ?correct? trees
that will not be produced. PP tells us how uncertain
the first phase is about producing rules.
Finally, we report on the parsing accuracy. We
use two measures, the first one (%Words) was pro-
posed by Lin (1995) and was the one reported in
(Eisner, 1996). Lin?s measure computes the frac-
tion of words that have been attached to the right
word. The second one (%POS) marks as correct a
word attachment if, and only if, the POS tag of the
head is the same as that of the right head, i.e., the
word was attached to the correct word-class, even
though the word is not the correct one in the sen-
tence. Clearly, the second measure is always higher
than the first one. The two measures try to cap-
ture the performance of the PCW-parser in the two
phases described above: (%POS) tries to capture
the performance in the first phase, and (%Words) in
the second phase. The measures reported in Table 4
are the mean values of (%POS) and (%Words) com-
puted over all sentences in section 23 having length
at most 20. We parsed only those sentences because
the resulting grammars for bigrams are too big:
parsing all sentences without any serious pruning
techniques was simply not feasible. From Table 4
MDI Bigrams
%Words %POS %Words %POS
One-Aut. 0.69 0.73 0.59 0.63
Many-Aut. 0.85 0.88 0.73 0.76
Table 4: Parsing results for the PTB
we see that the grammars induced with MDI out-
perform the grammars created with bigrams. More-
over, the grammar using different automata per POS
outperforms the ones built using only a single au-
tomaton per side (left or right). The results suggest
that an increase in quality of the automata has a di-
rect impact on the parsing performance.
6 Related Work and Discussion
Modeling rule bodies is a key component of parsers.
N -grams have been used extensively for this pur-
pose (Collins 1996, 1997; Eisner, 1996). In these
formalisms the generative process is not considered
in terms of probabilistic regular languages. Con-
sidering them as such (like we do) has two ad-
vantages. First, a vast area of research for induc-
ing regular languages (Carrasco and Oncina, 1994;
Thollard et al, 2000; Dupont and Chase, 1998)
comes in sight. Second, the parsing device itself can
be viewed under a unifying grammatical paradigm
like PCW-grammars (Chastellier and Colmerauer,
1969; Infante-Lopez and de Rijke, 2003). As PCW-
grammars are PCFGs plus post tree transformations,
properties of PCFGs hold for them too (Booth and
Thompson, 1973).
In our comparison we optimized the value of
alpha, but we did not optimize the n-grams, as
doing so would mean two different things. First,
smoothing techniques would have to be used to
combine different order n-grams. To be fair, we
would also have to smooth different MDI-based au-
tomata, which would leave us in the same point.
Second, the degree of the n-gram. We opted for
n = 2 as it seems the right balance of informative-
ness and generalization. N -grams are used to model
sequences of arguments, and these hardly ever have
length > 3, making higher degrees useless. To make
a fair comparison for the Many-Automata grammars
we did not tune the MDI-based automata individu-
ally, but we picked a unique alpha.
MDI presents a way to compact rule informa-
tion on the PTB; of course, other approaches exists.
In particular, Krotov et al (1998) try to induce a
CW-grammar from the PTB with the underlying as-
sumption that some derivations that were supposed
to be hidden were left visible. The attempt to use
algorithms other than n-grams-based for inducing
of regular languages in the context of grammar in-
duction is not new; for example, Kruijff (2003) uses
profile hidden models in an attempt to quantify free
order variations across languages; we are not aware
of evaluations of his grammars as parsing devices.
7 Conclusions and Future Work
Our experiments support two kinds of conclusions.
First, modeling rules with algorithms other than
n-grams not only produces smaller grammars but
also better performing ones. Second, the proce-
dure used for optimizing alpha reveals that some
POS behave almost deterministically for selecting
their arguments, while others do not. These find-
ings suggests that splitting classes that behave non-
deterministically into homogeneous ones could im-
prove the quality of the inferred automata. We saw
that lexicalization and head-annotation seem to at-
tack this problem. Obvious questions for future
work arise: Are these two techniques the best way to
split non-homogeneous classes into homogeneous
ones? Is there an optimal splitting?
Acknowledgments
We thank our referees for valuable comments. Both
authors were supported by the Netherlands Organi-
zation for Scientific Research (NWO) under project
number 220-80-001. De Rijke was also supported
by grants from NWO, under project numbers 365-
20-005, 612.069.006, 612.000.106, 612.000.207,
and 612.066.302.
References
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In Proc. 37th
Annual Meeting of the ACL, pages 542?549.
T. Booth and R. Thompson. 1973. Applying probability
measures to abstract languages. IEEE Transaction on
Computers, C-33(5):442?450.
R. Carrasco and J. Oncina. 1994. Learning stochastic
regular grammars by means of state merging method.
In Proc. ICGI-94, Springer, pages 139?150.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. 14th Nat.
Conf. on Artificial Intelligence, pages 598?603.
G. Chastellier and A. Colmerauer. 1969. W-grammar.
In Proc. 1969 24th National Conf., pages 511?518.
M. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proc. 34th Annual
Meeting of the ACL, pages 184?191.
M. Collins. 1997. Three generative, lexicalized models
for statistical parsing. In Proc. 35th Annual Meeting
of the ACL and 8th Conf. of the EACL, pages 16?23.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania, PA.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. ICML-2000, Stanford, Ca.
T. Cover and J. Thomas. 1991. Elements of Information
Theory. Jonh Wiley and Sons, New York.
F. Denis. 2001. Learning regular languages from simple
positive examples. Machine Learning, 44(1/2):37?66.
P. Dupont and L. Chase. 1998. Using symbol cluster-
ing to improve probabilistic automaton inference. In
Proc. ICGI-98, pages 232?243.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING-
96, pages 340?245, Copenhagen, Denmark.
J. Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Advances in Probabilistic and
Other Parsing Technologies, pages 29?62. Kluwer.
E. M. Gold. 1967. Language identification in the limit.
Information and Control, 10:447?474.
G. Infante-Lopez and M. de Rijke. 2003. Natural lan-
guage parsing with W-grammars. In Proc. CLIN
2003.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proc. 41st Annual Meeting of the ACL.
A. Krotov, M. Hepple, R.J. Gaizauskas, and Y. Wilks.
1998. Compacting the Penn Treebank grammar. In
Proc. COLING-ACL, pages 699?703.
G. Kruijff. 2003. 3-phase grammar learning. In Proc.
Workshop on Ideas and Strategies for Multilingual
Grammar Development.
D. Lin. 1995. A dependency-based method for evaluat-
ing broad-coverage parsers. In Proc. IJCAI-95.
K. Sima?an. 2000. Tree-gram Parsing: Lexical Depen-
dencies and Structual Relations. In Proc. 38th Annual
Meeting of the ACL, pages 53?60, Hong Kong, China.
F. Thollard, P. Dupont, and C. de la Higuera. 2000.
Probabilistic DFA inference using kullback-leibler di-
vergence and minimality. In Proc. ICML 2000.
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 58?65,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Upper Bounds for Unsupervised Parsing with Unambiguous
Non-Terminally Separated Grammars
Franco M. Luque and Gabriel Infante-Lopez
Grupo de Procesamiento de Lenguaje Natural
Universidad Nacional de C?rdoba & CONICET
Argentina
{francolq|gabriel}@famaf.unc.edu.ar
Abstract
Unambiguous Non-Terminally Separated
(UNTS) grammars have properties that
make them attractive for grammatical in-
ference. However, these properties do not
state the maximal performance they can
achieve when they are evaluated against a
gold treebank that is not produced by an
UNTS grammar. In this paper we inves-
tigate such an upper bound. We develop
a method to find an upper bound for the
unlabeled F1 performance that any UNTS
grammar can achieve over a given tree-
bank. Our strategy is to characterize all
possible versions of the gold treebank that
UNTS grammars can produce and to find
the one that optimizes a metric we define.
We show a way to translate this score into
an upper bound for the F1. In particular,
we show that the F1 parsing score of any
UNTS grammar can not be beyond 82.2%
when the gold treebank is the WSJ10 cor-
pus.
1 Introduction
Unsupervised learning of natural language has re-
ceived a lot of attention in the last years, e.g., Klein
and Manning (2004), Bod (2006a) and Seginer
(2007). Most of them use sentences from a tree-
bank for training and trees from the same treebank
for evaluation. As such, the best model for un-
supervised parsing is the one that reports the best
performance.
Unambiguous Non-Terminally Separated
(UNTS) grammars have properties that make
them attractive for grammatical inference. These
grammars have been shown to be PAC-learnable
in polynomial time (Clark, 2006), meaning that
under certain circumstances, the underlying
grammar can be learned from a sample of the
underlying language. Moreover, UNTS grammars
have been successfully used to induce grammars
from unannotated corpora in competitions of
learnability of formal languages (Clark, 2007).
UNTS grammars can be used for modeling nat-
ural language. They can be induced using any
training material, the induced models can be eval-
uated using trees from a treebank, and their per-
formance can be compared against state-of-the-
art unsupervised models. Different learning al-
gorithms might produce different grammars and,
consequently, different scores. The fact that the
class of UNTS grammars is PAC learnable does
not convey any information on the possible scores
that different UNTS grammars might produce.
From a performance oriented perspective it might
be possible to have an upper bound over the set
of possible scores of UNTS grammars. Knowing
an upper bound is complementary to knowing that
the class of UNTS grammars is PAC learnable.
Such upper bound has to be defined specifically
for UNTS grammars and has to take into account
the treebank used as test set. The key question
is how to compute it. Suppose that we want to
evaluate the performance of a given UNTS gram-
mar using a treebank. The candidate grammar pro-
duces a tree for each sentence and those trees are
compared to the original treebank. We can think
that the candidate grammar has produced a new
version of the treebank, and that the score of the
grammar is a measure of the closeness of the new
treebank to the original treebank. Finding the best
upper bound is equivalent to finding the closest
UNTS version of the treebank to the original one.
Such bounds are difficult to find for most classes
of languages because the search space is the
set of all possible versions of the treebank that
might have been produced by any grammar in the
class under study. In order to make the problem
tractable, we need the formalism to have an easy
way to characterize all the versions of a treebank
58
it might produce. UNTS grammars have a special
characterization that makes the search space easy
to define but whose exploration is NP-hard.
In this paper we present a way to characterize
UNTS grammars and a metric function to mea-
sure the closeness between two different version
of a treebank. We show that the problem of find-
ing the closest UNTS version of the treebank can
be described as Maximum Weight Independent Set
(MWIS) problem, a well known NP-hard problem
(Karp, 1972). The exploration algorithm returns
a version of the treebank that is the closest to the
gold standard in terms of our own metric.
We show that the F1-measure is related to our
measure and that it is possible to find and upper
bound of the F1-performance for all UNTS gram-
mars. Moreover, we compute this upper bound for
the WSJ10, a subset of the Penn Treebank (Mar-
cus et al, 1994) using POS tags as the alphabet.
The upper bound we found is 82.2% for the F1
measure. Our result suggest that UNTS grammars
are a formalism that has the potential to achieve
state-of-the-art unsupervised parsing performance
but does not guarantee that there exists a grammar
that can actually achieve the 82.2%.
To the best of our knowledge, there is no pre-
vious research on finding upper bounds for perfor-
mance over a concrete class of grammars. In Klein
and Manning (2004), the authors compute an up-
per bound for parsing with binary trees a gold tree-
bank that is not binary. This upper bound, that is
88.1% for the WSJ10, is for any parser that returns
binary trees, including the concrete models devel-
oped in the same work. But their upper bound does
not use any specific information of the concrete
models that may help them to find better ones.
The rest of the paper is organized as follows.
Section 2 presents our characterization of UNTS
grammars. Section 3 introduces the metric we op-
timized and explains how the closest version of the
treebank is found. Section 4 explains how the up-
per bound for our metric is translated to an up-
per bound of the F1 score. Section 5 presents our
bound for UNTS grammars using the WSJ10 and
finally Section 6 concludes the paper.
2 UNTS Grammars and Languages
Formally, a context free grammar G =
(?, N, S, P ) is said to be Non-Terminally Sepa-
rated (NTS) if, for all X,Y ? N and ?, ?, ? ?
(? ? N)? such that X ?? ??? and Y ?? ?, we
have that X ?? ?Y ? (Clark, 2007). Unambiguous
NTS (UNTS) grammars are those NTS grammars
that parses unambiguously every instance of the
language.
Given any grammar G, a substring s of r ?
L(G) is called a constituent of r if and only if there
is an X in N such that S ?? uXv ?? usv = r.
In contrast, a string s is called a non-constituent or
distituent of r ? L(G) if s is not a constituent of r.
We say that s is a constituent of a language L(G)
if for every r that contains s, s is a constituent of
r. In contrast, s is a distituent of L(G) if for every
r where s occurs, s is a distituent of r.
An interesting characterization of finite UNTS
grammars is that every substring that appear in
some string of the language is always a constituent
or always a distituent. In other words, if there is a
string r in L(G) for which s is a constituent, then
s is a constituent of L(G). By means of this prop-
erty, if we ignore the non-terminal labels, a finite
UNTS language is fully determined by its set of
constituents C. We can show this property for fi-
nite UNTS languages. We believe that it can also
be shown for non-finite cases, but for our purposes
the finite cases suffices, because we use grammars
to parse finite sets of sentences, specifically, the
sentences of test treebanks. We know that for ev-
ery finite subset of an infinite language produced
by a UNTS grammar G, there is a UNTS gram-
mar G? whose language is finite and that parses
the finite subset as G. If we look for the upper
bound among the grammars that produce a finite
language, this upper bound is also an upper bound
for the class of infinite UNTS grammars.
The UNTS characterization plays a very im-
portant role in the way we look for the upper
bound. Our method focuses on how to determine
which of the constituents that appear in the gold
are actually the constituents that produce the up-
per bound. Suppose that a given gold treebank
contains two strings ? and ? such that they occur
overlapped. That is, there exist non-empty strings
??, ?, ?? such that ? = ??? and ? = ??? and
????? occurs in the treebank. If C is the set of
constituents of a UNTS grammar it can not have
both ? and ?. It might have one or the other, but
if both belong to C the resulting language can not
be UNTS. In order to find the closest UNTS gram-
mar we design a procedure that looks for the sub-
set of all substrings that occur in the sentences of
the gold treebank that can be the constituent set C
59
of a grammar. We do not explicitly build a UNTS
grammar, but find the set C that produces the best
score.
We say that two strings ? and ? are compatible
in a language L if they do not occur overlapped
in L, and hence they both can be members of C.
If we think of L as a subset of an infinite lan-
guage, it is not possible to check that two overlap-
ping strings do not appear overlapped in the ?real?
language and hence that they are actually com-
patible. Nevertheless, we can guarantee compat-
ibility between two strings ?, ? by requiring that
they do not overlap at all, this is, that there are
no non-empty strings ??, ?, ?? such that ? = ???
and ? = ???. We call this type of compatibility
strong compatibility. Strong compatibility ensures
that two strings can belong to C regardless of L.
In our experiments we focus on finding the best set
C of compatible strings.
Any set of compatible strings C extracted from
the gold treebank can be used to produce a new
version of the treebank. For example, Figure 1
shows two trees from the WSJ Penn Treebank.
The string ?in the dark? occurs as a constituent in
(a) and as a distituent in (b). If C contains ?in the
dark?, it can not contain ?the dark clouds? given
that they overlap in the yield of (b). As a con-
sequence, the new treebank correctly contains the
subtree in (a) but not the one in (b). Instead, the
yield of (b) is described as in (c) in the new tree-
bank.
C defines a new version of the treebank that sat-
isfies the UNTS property. Our goal is to obtain a
treebank T ? such that (a) T ? and T are treebanks
over the same set of sentences, (b) T ? is UNTS,
and (c) T ? is the closest treebank to T in terms of
performance. The three of them imply that any
other UNTS grammar is not as similar as the one
we found.
3 Finding the Best UNTS Grammar
As our goal is to find the closest grammar in terms
of performance, we need to define first a weight
for each possible grammar and second, an algo-
rithm that searches for the grammar with the best
weight. Ideally, the weight of a candidate gram-
mar should be in terms of F1, but we can show
that optimization of this particular metric is com-
putationally hard. Instead of defining F1 as their
score, we introduce a new metric that is easier to
optimize, we find the best grammar for this met-
ric, and we show that the possible values of F1
can be bounded by a function that takes this score
as argument. In this section we present our metric
and the technique we use to find a grammar that
reports the best value for our metric.
If the original treebank T is not produced by any
UNTS grammar, then there are strings in T that
are constituents in some sentences and that are dis-
tituents in some other sentences. For each one of
them we need a procedure to decide whether they
are members of C or not. If a string ? appears a
significant number of times more as a constituent
than as a distituent the procedure may choose to
include it in C at the price of being wrong a few
times. That is, the new version of T has all occur-
rences of ? either as constituents or as distituents.
The treebank that has all of its occurrences as con-
stituents differs from the original in that there are
some occurrences of ? that were originally dis-
tituents and are marked as constituents. Similarly,
if ? is marked as distituent in the new treebank, it
has occurrences of ? that were constituents in T .
The decision procedure becomes harder when
all the substrings that appear in the treebank are
considered. The increase in complexity is a con-
sequence of the number of decisions the procedure
needs to take and the way these decisions interfere
one with another. We show that the problem of
determining the set C is naturally embedded in a
graph NP-hard problem. We define a way to look
for the optimal grammars by translating our prob-
lem to a well known graph problem. Let L be the
the set of sentences in a treebank, and let S(L) be
all the possible non-empty proper substrings of L.
We build a weighted undirected graph G in terms
of the treebank as follows. Nodes in G correspond
to strings in S(L). The weight of a node is a func-
tion w(s) that models our interest of having s se-
lected as a constituent; w(s) is defined in terms of
some information derived from the gold treebank
T and we discuss it later in this section. Finally,
two nodes a and b are connected by an edge if their
two corresponding strings conflict in a sentence of
T (i.e., they are not compatible in L).
Not all elements of L are in S(L). We did not
include L in S(L) for two practical reasons. The
first one is that to require L in S(L) is too re-
strictive. It states that all strings in L are in fact
constituents. If two string ab and bc of L oc-
cur overlapped in a third string abc then there is
no UNTS grammar capable of having the three of
60
(a)
PRP
we
VBP
?re
IN
in
DT
the
JJ
dark
(b)
IN
in DT
the
JJ
dark
NNS
clouds
(c)
IN
in
DT
the
JJ
dark
NNS
clouds
Figure 1: (a) and (b) are two subtrees that show ?in the dark? as a constituent and as a distituent respec-
tively. (c) shows the result of choosing ?in the dark? as a constituent.
them as constituents. The second one is that in-
cluding them produces graphs that are too sparse.
If they are included in the graph, we know that
any solution should contain them, consequently,
all their neighbors do not belong to any solution
and they can be removed from the graph. Our ex-
periments show that the graph that results from re-
moving nodes related to nodes representing strings
in L are too small to produce any interesting result.
By means of representing the treebank as a
graph, selecting a set of constituents C ? S(L)
is equivalent to selecting an independent set of
nodes in the graph. An independent set is a sub-
set of the set of nodes that do not have any pair
of nodes connected by an edge. Clearly, there are
exponentially many possible ways to select an in-
dependent set, and each of these sets represents a
set of constituents. But, since we are interested in
the best set of constituents, we associate to each
independent set C the weight W (C) defined as
?
s?C w(s). Our aim is then to find a set Cmax
that maximizes this weight. This problem is a well
known problem of graph theory known in the lit-
erature as the Maximum Weight Independent Set
(MWIS) problem. This problem is also known to
be NP-hard (Karp, 1972).
We still have to choose a definition for w(s).
We want to find the grammar that maximizes F1.
Unfortunately, F1 can not be expressed in terms of
a sum of weights. Maximization of F1 is beyond
the expressiveness of our model, but our strategy
is to define a measure that correlates with F1 and
that can be expressed as a sum of weights.
In order to introduce our measure, we first de-
fine c(s) and d(s) as the number of times a string
s appears in the gold treebank T as a constituent
and as a distituent respectively. Observe that if
we choose to include s as a constituent of C, the
resulting treebank T ? contains all the c(s) + d(s)
occurrences of s as a constituent. c(s) of the s oc-
currences in T ? are constituents as they are in T
and d(s) of the occurrences are constituents in T ?
but are in fact distituents in T . We want to max-
imize c(s) and minimize d(s) at the same time.
This can be done by defining the contribution of a
string s to the overall score as
w(s) = c(s)? d(s).
With this definition of w, the weight W (C) =
?
s?C w(s) becomes the number of constituents
of T ? that are in T minus the number of con-
stituents that do not. If we define the number of
hits to be H(C) =
?
s?C c(s) and the number of
misses to be M(C) =?s?C d(s) we have that
W (C) = H(C)?M(C). (1)
As we confirm in Section 5, graphs tend to be
very big. In order to reduce the size of the graphs,
if a string s has w(s) ? 0, we do not include its
corresponding node in the graph. An independent
set that does not include s has an equal or higher
W than the same set including s.
For example, let T be the treebank in Fig-
ure 2 (a). The sets of substrings such that
w(c) ? 0 is {da, cd, bc, cda, ab, bch}. The
graph that corresponds to this set of strings is
given in Figure 3. Nodes corresponding to
strings {dabch, bcda, abe, abf, abg, bci, daj} are
not shown in the figure because the strings do
not belong to S(L). The figure also shows the
weights associated to the substrings according to
their counts in Figure 2 (a). The shadowed nodes
correspond to the independent set that maximizes
W . The trees in the Figure 2 (b) are the sentences
of the treebank parsed according the optimal inde-
pendent set.
4 An Upper Bound for F1
Even though finding the independent set that max-
imizes W is an NP-Hard problem, there are in-
stances where it can be effectively computed, as
we show in the next section. The set Cmax max-
imizes W for the WSJ10 and we know that all
others C produces a lower value of W . In other
words, the set Cmax produce a treebank Tmax that
61
(a)
d a
b c h
(da)((bc)h)
b
c d a
b((cd)a)
a b e
(ab)e
a b f
(ab)f
a b g
(ab)g
b c i
(bc)i
d a j
(da)j
(b)
d
a b c h
d(ab)ch
b
c d a
b((cd)a)
a b e
(ab)e
a b f
(ab)f
a b g
(ab)g
b c i
bci
d a j
daj
Figure 2: (a) A gold treebank. (b) The treebank generated by the grammar C = L ? {cd, ab, cda}.
Figure 3: Graph for the treebank of Figure 2.
is the closest UNTS version to the WSJ10 in terms
of W . We can compute the precision, recall and
F1 for Cmax but there is no warranty that the F1
score is the best for all the UNTS grammars. This
is the case because F1 and W do not define the
same ordering over the family of candidate con-
stituent sets C: there are gold treebanks T (used
for computing the metrics), and sets C1, C2 such
that F1(C1) < F1(C2) and W (C1) > W (C2).
For example, consider the gold treebank T in Fig-
ure 4 (a). The table in Figure 4 (b) displays two
sets C1 and C2, the treebanks they produce, and
their values of F1 and W . Note that C2 is the re-
sult of adding the string ef to C1, also note that
c(ef) = 1 and d(ef) = 2. This improves the F1
score but produces a lower W .
The F1 measure we work with is the one de-
fined in the recent literature of unsupervised pars-
ing (Klein and Manning, 2004). F1 is defined in
terms of Precision and Recall as usual, and the last
two measures are micro-averaged measures that
include full-span brackets, and that ignore both
unary branches and brackets of span one. For sim-
plicity, the previous example does not count the
full-span brackets.
As the example shows, the upper bound for W
might not be an upper bound of F1, but it is pos-
sible to find a way to define an upper bound of
F1 using the upper bound of W . In this section
we define a function f with the following prop-
erty. Let X and Y be the sets of W -weights and
F1-weights for all possible UNTS grammars re-
spectively. Then, if w is an upper bound of X ,
then f(w) is an upper bound of Y . The function f
is defined as follows:
f(w) = F1
( 1
2? wK
, 1
)
(2)
where F1(p, r) = 2prp+r , and K =
?
s?ST c(s) is
the total number of constituents in the gold tree-
bank T . From it, we can also derive values for
precision and recall: precision 12? wK and recall 1.
A recall of 1 is clearly an upper bound for all the
possible values of recall, but the value given for
precision is not necessarily an upper bound for all
the possible values of precision. It might exist a
grammar having a higher value of precision but
whose F1 has to be below our upper bound.
The rest of section shows that f(W ) is an up-
per bound for F1, the reader not interested in the
technicalities can skip it.
The key insight for the proof is that both metrics
F1 and W can be written in terms of precision and
recall. Let T be the treebank that is used to com-
pute all the metrics. And let T ? be the treebank
produced by a given constituent set C. If a string
s belongs to C, then its c(s) + d(s) occurrences
in T ? are marked as constituents. Moreover, s is
correctly tagged a c(s) number of times while it
is incorrectly tagged a d(s) number of times. Us-
ing this, P , R and F1 can be computed for C as
follows:
P (C) =
P
s?C c(s)
P
s?C c(s)+d(s)
= H(C)H(C)+M(C) (3)
R(C) =
P
s?C c(s)
K
= H(C)K (4)
F1(C) = 2P (C)R(C)P (C)+R(C)
= 2H(C)K+H(C)+M(C)
62
(a)
a b c
(ab)c
a b d
a(bd)
e f g
(ef)g
e f h
efh
e f i
efi
(b)
C T ?C P R F1 W
C1 = {abc, abd, efg, efh, efi, ab} {(ab)c, (ab)d, efg, efh, efi} 50% 33% 40% 1 ? 1 = 0
C2 = {abc, abd, efg, efh, efi, ab, ef} {(ab)c, (ab)d, (ef)g, (ef)h, (ef)i} 40% 67% 50% 2 ? 3 = ?1
Figure 4: (a) A gold treebank. (b) Two grammars, the treebanks they generate, and their scores.
W can also be written in terms of P and R as
W (C) = (2? 1P (C))R(C)K (5)
This formula is proved to be equivalent to Equa-
tion (1) by replacing P (C) and R(C) with equa-
tions (3) and (4) respectively. Using the last two
equations, we can rewrite F1 and W taking p and
r, representing values of precision and recall, as
parameters:
F1(p, r) = 2prp+ r
W (p, r) = (2? 1p)rK (6)
Using these equations, we can prove that f
correctly translates upper bounds of W to upper
bounds of F1 using calculus. In contrast to F1,
W not necessarily take values between 0 and 1. In-
stead, it takes values between K and ??. More-
over, it is negative when p < 12 , and goes to ??
when p goes to 0. Let C be an arbitrary UNTS
grammar, and let pC , rC and wC be its precision,
recall and W -weight respectively. Let w be our
upper bound, so that wC ? w. If f1C is defined
as F1(pC , rC) we need to show that f1C ? f(w).
We bound f1C in two steps. First, we show that
f1C ? f(wC)
and second, we show that
f(wC) ? f(w).
The first inequality is proved by observing that
f1C and f(wC) are the values of the function
f1(r) = F1
( 1
2? wCKr
, r
)
at the points r = rC and r = 1 respectively.
This function corresponds to the line defined by
the F1 values of all possible models that have a
fixed weight W = wC . The function is monoton-
ically increasing in r, so we can apply it to both
sides of the following inequality rC ? 1, which is
trivially true. As result, we get f1C ? f(wC) as
required. The second inequality is proved by ob-
serving that f(w) is monotonically increasing in
w, and by applying it to both sides of the hypothe-
sis wc ? w.
5 UNTS Bounds for the WSJ10 Treebank
In this section we focus on trying to find real upper
bounds building the graph for a particular treebank
T . We find the best independent set, we build the
UNTS version Tmax of T and we compute the up-
per bound for F1. The treebank we use for exper-
iments is the WSJ10, which consists of the sen-
tences of the WSJ Penn Treebank whose length
is at most 10 words after removing punctuation
marks (Klein and Manning, 2004). We also re-
moved lexical entries transforming POS tags into
our terminal symbols as it is usually done (Klein
and Manning, 2004; Bod, 2006a).
We start by finding the best independent set. To
solve the problem in the practice, we convert it
into an Integer Linear Programming (ILP) prob-
lem. ILP is also NP-hard (Karp, 1972), but there
is software that implements efficient strategies for
solving some of its instances (Achterberg, 2004).
ILP problems are defined by three parameters.
First, there is a set of variables that can take val-
ues from a finite set. Second, there is an objective
function that has to be maximized, and third, there
is a set of constraints that must be satisfied. In our
case, we define a binary variable xs ? {0, 1} for
every node s in the graph. Its value is 1 or 0, that
respectively determines the presence or absence of
s in the set Cmax. The objective function is
?
s?S(L)
xsw(s)
The constraints are defined using the edges of the
63
graph. For every edge (s1, s2) in the graph, we
add the following constraint to the problem:
xs1 + xs2 ? 1
The 7422 trees of the WSJ10 treebank have a
total of 181476 substrings of length ? 2, that
form the set S(L) of 68803 different substrings.
The number of substrings in S(L) does not grow
too much with respect to the number of strings in
L because substrings are sequences of POS tags,
meaning that each substring is very frequent in the
corpus. If substrings were made out of words in-
stead of POS tags, the number of substrings would
grow much faster, making the problem harder to
solve. Moreover, removing the strings s such that
w(s) ? 0 gives a total of only 7029 substrings.
Since there is a node for each substring, the result-
ing graph contains 7029 nodes. Recall that there
is an edge between two strings if they occur over-
lapped. Our graph contains 1204 edges. The ILP
version has 7029 variables, 1204 constraints and
the objective function sums over 7029 variables.
These numbers are summarized in Table 1.
The solution of the ILP problem is a set of
6583 variables that are set to one. This set corre-
sponds to a set Cmax of nodes in our graph of the
same number of elements. Using Cmax we build
a new version Tmax of the WSJ10, and compute
its weight W , precision, recall and F1. Their val-
ues are displayed in Table 2. Since the elements
of L were not introduced in S(L), elements of L
are not necessarily in Cmax, but in order to com-
pute precision and recall, we add them by hand.
Strictly speaking, the set of constituents that we
use for building Tmax is Cmax plus the full span
brackets.
We can, using equation (2), compute the up-
per bound of F1 for all the possible scores of all
UNTS grammars that use POS tags as alphabet:
f(wmax) = F1
( 1
2? wmaxK
, 1
)
= 82.2%
The precision for this upper bound is
P (wmax) =
1
2? wmaxK
= 69.8%
while its recall is R = 100%. Note from the pre-
vious section that P (wmax) is not an upper bound
for precision but just the precision associated to
the upper bound f(wmax).
Gold constituents K 35302
Strings |S(L)| 68803
Nodes 7029
Edges 1204
Table 1: Figures for the WSJ10 and its graph.
Hits H 22169
Misses M 2127
Weight W 20042
Precision P 91.2%
Recall R 62.8%
F1 F1 74.4%
Table 2: Summary of the scores for Cmax.
Table 3 shows results that allow us to com-
pare the upper bounds with state-of-the-art pars-
ing scores. BestW corresponds to the scores of
Tmax and UBoundF1 is the result of our transla-
tion function f . From the table we can see that
an unsupervised parser based on UNTS grammars
may reach a sate-of-the-art performance over the
WSJ10. RBranch is a WSJ10 version where all
trees are binary and right branching. DMV, CCM
and DMV+CCM are the results reported in Klein
and Manning (2004). U-DOP and UML-DOP
are the results reported in Bod (2006b) and Bod
(2006a) respectively. Incremental refers to the re-
sults reported in Seginer (2007).
We believe that our upper bound is a generous
one and that it might be difficult to achieve it for
two reasons. First, since the WSJ10 corpus is
a rather flat treebank, from the 68803 substrings
only 10% of them are such that c(s) > d(s). Our
procedure has to decide among this 10% which
of the strings are constituents. An unsupervised
method has to choose the set of constituents from
the set of all 68803 possible substrings. Second,
we are supposing a recall of 100% which is clearly
too optimistic. We believe that we can find a
tighter upper bound by finding an upper bound for
recall, and by rewriting f in equation (2) in terms
of the upper bound for recall.
It must be clear the scope of the upper bound
we found. First, note that it has been computed
over the WSJ10 treebank using the POS tags as
the alphabet. Any other alphabet we use, like for
example words, or pairs of words and POS tags,
changes the relation of compatibility among the
substrings, making a completely different universe
64
Model UP UR F1
RBranch 55.1 70.0 61.7
DMV 46.6 59.2 52.1
CCM 64.2 81.6 71.9
DMV+CCM 69.3 88.0 77.6
U-DOP 70.8 88.2 78.5
UML-DOP 82.9
Incremental 75.6 76.2 75.9
BestW(UNTS) 91.2 62.8 74.4
UBoundF1(UNTS) 69.8 100.0 82.2
Table 3: Performance on the WSJ10 of the most
recent unsupervised parsers, and our upper bounds
on UNTS.
of UNTS grammars. Second, our computation of
the upper bound was not made for supersets of the
WSJ10. Supersets such as the entire Penn Tree-
bank produce bigger graphs because they contain
longer sentences and various different sequences
of substrings. As the maximization of W is an
NP-hard problem, the computational cost of solv-
ing bigger instances grows exponentially. A third
limitation that must be clear is about the models
affected by the bound. The upper bound, and in
general the method, is only applicable to the class
of formal UNTS grammars, with only some very
slight variants mentioned in the previous sections.
Just moving to probabilistic or weighted UNTS
grammars invalidates all the presented results.
6 Conclusions
We present a method for assessing the potential of
UNTS grammars as a formalism for unsupervised
parsing of natural language. We assess their po-
tential by finding an upper bound of their perfor-
mance when they are evaluated using the WSJ10
treebank. We show that any UNTS grammars can
achieve at most 82.2% of F1 measure, a value
comparable to most state-of-the-art models. In or-
der to compute this upper bound we introduced
a measure that does not define the same ordering
among UNTS grammars as the F1, but that has
the advantage of being computationally easier to
optimize. Our measure can be used, by means of
a translation function, to find an upper bound for
F1. We also showed that the optimization proce-
dure for our metric maps into an NP-Hard prob-
lem, but despite this fact we present experimen-
tal results that compute the upper bound for the
WSJ10 when POS tags are treated as the grammar
alphabet.
From a more abstract perspective, we intro-
duced a different approach to assess the usefulness
of a grammatical formalism. Usually, formalism
are proved to have interesting learnability proper-
ties such as PAC-learnability or convergence of a
probabilistic distribution. We present an approach
that even though it does not provide an effective
way of computing the best grammar in an unsu-
pervised fashion, it states the upper bound of per-
formance for all the class of UNTS grammars.
Acknowledgments
This work was supported in part by grant PICT
2006-00969, ANPCyT, Argentina. We would like
to thank Pablo Rey (UDP, Chile) for his help
with ILP, and Demetrio Mart?n Vilela (UNC, Ar-
gentina) for his detailed review.
References
Tobias Achterberg. 2004. SCIP - a framework to in-
tegrate Constraint and Mixed Integer Programming.
Technical report.
Rens Bod. 2006a. An all-subtrees approach to unsu-
pervised parsing. In Proceedings of COLING-ACL
2006.
Rens Bod. 2006b. Unsupervised parsing with U-DOP.
In Proceedings of CoNLL-X.
Alexander Clark. 2006. PAC-learning unambiguous
NTS languages. In Proceedings of ICGI-2006.
Alexander Clark. 2007. Learning deterministic con-
text free grammars: The Omphalos competition.
Machine Learning, 66(1):93?110.
Richard M. Karp. 1972. Reducibility among com-
binatorial problems. In R. E. Miller and J. W.
Thatcher, editors, Complexity of Computer Compu-
tations, pages 85?103. Plenum Press.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In Proceedings
of ACL 42.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Yoav Seginer. 2007. Fast unsupervised incremental
parsing. In Proceedings of ACL 45.
65
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 8?14, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Data-driven computational linguistics at FaMAF-UNC, Argentina
Laura Alonso i Alemany and Gabriel Infante-Lopez
Grupo de Procesamiento de Lenguaje Natural
Seccio?n de Ciencias de la Computacio?n
Facultad de Matema?tica, Astronom??a y F??sica
Universidad Nacional de Co?rdoba
Co?rdoba, Argentina
{gabriel|alemany}@famaf.unc.edu.ar
Abstract
This paper provides a survey of some on-
going research projects in computational lin-
guistics within the group of Natural Language
Processing at the University of Co?rdoba, Ar-
gentina. We outline our future plans and spot-
light some opportunities for collaboration.
1 Introduction
In this paper we present our group, describe its
members, research agenda, interests and possible
collaboration opportunities. The research agenda
of the NLP group contains diverse lines of work.
As a group, we have a special interest in produc-
ing language technologies for our languages, at a
level comparable in performance with the state-of-
the-art technology for English. We are developing
such technology by deeply understanding its under-
ling models and either adapting them to our lan-
guages or by creating new ones.
In this paper we present only those related to Nat-
ural Language Parsing and data-driven characterisa-
tion of linguistic phenomena. For both lines we pro-
vide a small survey of our results so far, we describe
our current research questions and we spotlight pos-
sible opportunities of collaboration.
The paper is organized as follows. The follow-
ing Section describes the group, its composition,
projects and goals. Section 3 briefly introduces the
research agenda related to natural language pars-
ing and structure finding. Section 4 sketches the
work on data-driven characterisation of linguistic
phenomena in three main parts: semi-structured text
mining, characterisation of verbal behaviour and
mining of relations in biomedical text. Finally, Sec-
tion 5 presents outlines our overall vision for collab-
oration with other researchers in the Americas.
2 Description of the group
The NLP Group1 is part of the Computer Science
section at the Facultad de Matema?tica, Astronom??a
y F??sica, at the Universidad Nacional de Co?rdoba.
The group was started in 2005, with two full time re-
searchers who had just got their doctorate degree in
Amsterdam and Barcelona. Then, in 2009 and 2010
three more full-time researchers joined the group,
coming from the Universities of Geneva and Nancy.
As of 2010, the group has 5 faculty researchers,
4 PhD students and several undergraduate students.
The computer science section has around 20 mem-
bers ? including the NLP group, faculty members
and PhD students.
The faculty researchers are, by alphabetical order:
? Laura Alonso Alemany, working in text mining
and data-driven systematization of language.
? Carlos Areces, investigating different reason-
ing tasks and their applications in natural lan-
guage processing.
? Luciana Benotti, investigates the addition of
pragmatic abilities into dialogue systems.
? Paula Estrella, working in Machine Transla-
tion.
? Gabriel Infante-Lopez, working on Natural
Language Parsing and Structure Finding.
1http://www.cs.famaf.unc.edu.ar/?pln/
8
One of the main aims of the group has been ed-
ucation, both at undergraduate and graduate lev-
els. Computer Science is an under-developed area
in Argentina, and Natural Language Processing even
more so. When the group was created, there were
very few NLP researchers in the country, and they
worked in isolation, with little connection to other
researchers from neighbouring countries. One of
the strategic goals of our University and of the NLP
group itself were to create a critical mass of re-
searchers in NLP. To that aim, we worked on in-
corporating researchers to our group and establish-
ing relations with other groups. Researchers were
incorporated via special programmes from both the
Faculty and the Argentinean Government to increase
the number of doctors in Computer Science in the
scientific system in Argentina.
Most of our efforts in the first years went to raise
awareness about the area and provide foundational
and advanced courses. This policy lead to a signifi-
cant number of graduation theses2 and to the incor-
poration of various PhD students to our group.
We taught several undergraduate and graduate
courses on various NLP topics at our own Univer-
sity, at the University of R??o Cuarto, at the Univer-
sity of Buenos Aires and at the Universidad de la
Repu?blica (Uruguay), as well as crash courses at the
Society for Operative Investigations (SADIO) and
at the Conferencia Latinoamericana de Informa?tica
(CLEI 2008). We also gave several talks at vari-
ous universities in the country, and participated in
local events, like JALIMI?05 (Jornadas Argentinas
de Lingu???stica Informa?tica: Modelizacio?n e Inge-
nier??a) or the Argentinean Symposium on Artificial
Intelligence.
Since the beginning of its activities, the group
has received funding for two major basic research
projects, funded by the Argentinean Agency for the
Development of Science and Technology. A third
such project is pending approval.
We have a special interest in establishing work-
ing relations and strengthening the synergies with
the research community in NLP, both within South
America and the rest of the world. We have had sci-
entific and teaching exchanges with the NLP group
2http://cs.famaf.unc.edu.ar/?pln/
Investigacion/tesis_grado/tesis_grado.html
in Montevideo, Uruguay. From that collaboration,
the Microbio project emerged3, bringing together
researchers on NLP from Chile, Brazil, Uruguay,
France and Argentina. This project was funded
by each country?s scientific institutions (MinCyT,
in the case of Argentina) within STIC-AmSud4,
a scientific-technological cooperation programme
aimed to promote and strengthen South America re-
gional capacities and their cooperation with France
in the area of Information Technologies and Com-
munication. Within this project, we hosted the kick-
off workshop on February 2008, with attendants rep-
resenting all groups in the project.
We have also had billateral international cooper-
ation in some smaller projects. Together with the
CNR-INRIA in Rennes, France, we have worked in
a project concerning the smallest grammar problem.
We tackle the same problem, finding small gram-
mars in two different domains: ADN sequences
and Natural Language sentences. In collaboration
with several universities in Spain (UB, UOC, UPC,
EHU/UPV), we have taken part in the major basic
research programme KNOW5, aiming to aggregate
meaning, knowledge and reasoning to current infor-
mation technologies. This project has now received
funding to carry on a continuating project6.
Moreover, we are putting forward some propos-
als for further international collaboration. Follow-
ing the path opened by the Microbio project, we
are working on a proposal to the Ecos Sud pro-
gramme for joint collaboration with research teams
in France7.
We are also working in strengthening relations
within Argentinean NLP groups. To that aim, we are
collaborating with the NLP group at the University
of Buenos Aires in the organization of the School
on Computational Linguistics ELiC8, with several
grants for students sponsored by NAACL. We are
also putting forward a proposal for a workshop on
3http://www.microbioamsud.net/
4http://www.sticamsud.org/
5KNOW project: http://ixa.si.ehu.es/know.
6Representation of Semantic Knowledge, TIN2009-14715-
C04-03 (Plan Nacional de I+D+i 2008-2011).
7ECOS-SUD programme: http://www.mincyt.gov.
ar/coopinter_archivos/bilateral/francia.
htm.
8ELiC school on Computational Linguistics: http://
www.glyc.dc.uba.ar/elic2010/.
9
NLP to be co-located with the IBERAMIA confer-
ence on Artificial Intelligence, to be held at Bah??a
Blanca on November 2010.
3 Natural Language Parsing and
Structure Finding
3.1 Unsupervised Parsing
Unsupervised parsing of Natural Language Syntax
is a key technology for the development of lan-
guage technology. It is specially important for lan-
guages that have either small treebanks or none at
all. Clearly, there is a big difference between pro-
ducing or using a treebank for evaluation and pro-
ducing or using them for training. In the former
case, the size of the treebank can be significantly
smaller. In our group, we have investigated differ-
ent approaches to unsupervised learning of natural
language. and we are currently following two dif-
ferent lines, one that aims at characterizing the po-
tential of a grammar formalism to learn a given tree-
bank structure and a second that uses only regular
automata to learn syntax.
Characterization of Structures In (Luque and
Infante-Lopez, 2009) we present a rather unusual
result for language learning. We show an upper
bound for the performance of a class of languages
when a grammar from that class is used to parse
the sentences in any given treebank. The class of
languages we studied is the defined by Unambigu-
ous Non-Terminally Separated (UNTS) grammars
(Clark, 2006). UNTS grammars are interesting be-
cause, first, they have nice learnability properties
like PAC learnability (Clark, 2006), and, second,
they are used as the background formalism that won
the Omphalos competition (Clark, 2007). Our strat-
egy consists on characterizing all possible ways of
parsing all the sentences in a treebank using UNTS
grammars, then, we find the one that is closest to the
treebank. We show that, in contrast to the results ob-
tained for learning formal languages, UNTS are not
capable of producing structures that score as state-
of-the-art models on the treebanks we experimented
with.
Our results are for a particular, very specific type
of grammar. We are currently exploring how to
widen our technique to provide upper bounds to a
more general class of languages. Our technique does
not state how to actually produce a grammar that
performs as well as the upper bound, but it can be
useful for determining how to transform the training
material to make upper bounds go up. In particu-
lar we have defined a generalization of UNTS gram-
mars, called k-l-UNTS grammars, that transform a
word w in the training material in a 3-uple ??,w, ??
where ? contains the k previous symbols to w and
? contains the l symbols following w. Intuitively, k-
l-UNTS augments each word with a variable length
context. It turns out that the resulting class of lan-
guages is more general than UNTS grammars: they
are PAC learnable, they can be learned with the same
learning algorithm as UNTS and, moreover, their
upper bound for performance is much higher than
for UNTS. Still, it might be the case that the exist-
ing algorithm for finding UNTS is not the right one
for learning the structure of a treebank, it might be
the case that strings in the PTB have not been pro-
duced by a k-l-UNTS grammar. We are currently
investigating how to produce an algorithm that fits
better the structure given in a treebank.
Learning Structure Using Probabilistic Au-
tomata DMV+CCM (Klein and Manning, 2004;
Klein and Manning, 2002) is a probabilistic model
for unsupervised parsing, that can be successfully
trained with the EM algorithm to achieve state of
the art performance. It is the combination of the
Constituent-Context Model, that models unlabeled
constituent parsing, and the DependencyModel with
Valence, that models projective dependency parsing.
On the other hand, CCM encodes the probability that
a given string of POS tags is a constituent. DMV is
more of our interest in this work, because it encodes
a top-down generative process where the heads gen-
erate their dependents to both directions until there
is a decision to stop, in a way that resembles suc-
cessful supervised dependency models such as in
(Collins, 1999). The generation of dependents of
a head on a specific direction can be seen as an im-
plicit probabilistic regular language generated by a
probabilistic deterministic finite automaton.
Under this perspective, the DMV model is in fact
an algorithm for learning several automata at the
same time. All automata have in common that they
have the same number of states and the same num-
ber of arcs between states, which is given by the def-
10
inition of the DMV model. Automata differ in that
they have different probabilities assigned to the tran-
sitions. The simple observation that DMV actually
suppose a fixed structure for the automata it induces
might explain its poor performance with freer order
languages like Spanish. Using our own implementa-
tion (see (Luque, 2009)) we have empirically tested
that DMV+CMVworks well in languages with strict
word order, like English, but for other languages
with freer word order, like Spanish, DMV+CMV
performance decreases dramatically. In order to
improve DMV+CCM performance for this type of
languages, the structure of the automaton might be
modified, but since the DMV model has an ad hoc
learning algorithm, a new parametric learning algo-
rithm has to be defined. We are currently investigat-
ing different automaton structures for different lan-
guages and we are also investigating not only the
induction of the parameters for fixed structure, but
also inducing the structure of the automata itself.
3.2 Smallest Grammar and Compression for
Natural Language
The smallest grammar problem has been widely
studied in the literature. The aim of the problem is
to find the smallest (smallest in the sense of number
of symbols that occur in the grammar) context free
grammar that produces only one given string. The
smallest grammar can be thought as a relaxation of
the definition of Kolmogorov Complexity where the
complexity is given by a context free grammar in-
stead of a Turing machine. It is believed that the
smallest grammar can be used both for computing
optimal compression codes and for finding meaning-
ful patterns in strings.
Moreover, since the procedure for finding the
smallest grammar is in fact a procedure that assigns
a tree structure to a string, the smallest grammar
problem is, in fact, a particular case of unsupervised
parsing that has a very particular objective function
to be optimized.
Since the search space is exponentially big, all
existing algorithms are in fact heuristics that look
for a small grammar. In (Carrascosa et al, 2010)
we presented two algorithms that outperform all ex-
isting heuristics. We have produce and algorithm
that produces 10% smaller grammars for natural lan-
guage strings and 1.5% smaller grammars for DNA
sequences.
Even more, we show evidence that it is possi-
ble to find grammars that share approximately the
same small score but that have very little structure
in common. Moreover, the structure that is found
by the smallest grammar algorithm for the sentences
in PTB have little in common with the structure that
the PTB defines for those sentences.
Currently, we are trying to find answers to two dif-
ferent questions. First, is there a small piece of struc-
ture that is common to all grammars having compa-
rable sizes? and second, can the grammars that are
found by our algorithms be used for improving com-
pression algorithms?
4 Data-driven characterisation of
linguistic phenomena
4.1 Semi-structured text mining
One of our lines of research is to apply standard text
mining techniques to unstructured text, mostly user
generated content like that found in blogs, social net-
works, short messaging services or advertisements.
Our main corpus of study is constituted by classi-
fied advertisements from a local newspaper, but one
of our lines of work within this project is to assess
the portability of methods and techniques to differ-
ent genres.
The goals we pursue are:
creating corpora and related resources, and mak-
ing them publicly available. A corpus of news-
paper advertisements and a corpus of short text
messages are underway.
normalization of text bringing ortographic vari-
ants of a word (mostly abbreviations) to a
canonical form. To do that, we apply machine
learning techniques to learn the parameters for
edit distances, as in (Go?mez-Ballester et al,
1997; Ristad and Yanilos, 1998; Bilenko and
Mooney, 2003; McCallum et al, 2005; Oncina
and Sebban, 2006). We build upon previous
work on normalization by (Choudhury et al,
2007; Okazaki et al, 2008; Cook and Steven-
son, 2009; Stevenson et al, 2009). Prelimi-
nary results show a significant improvement of
learned distances over standard distances.
11
syntactic analysis applying a robust shallow pars-
ing approach aimed to identify entities and their
modifiers.
ontology induction from very restricted domains,
to aid generalization in the step of information
extraction. We will be following the approach
presented in (Michelson and Knoblock, 2009).
information extraction inducing templates from
corpus using unsupervised and semi-
supervised techniques, and using induced
templates to extract information to populate
a relational database, as in (Michelson and
Knoblock, 2006).
data mining applying traditional knowledge dis-
covery techniques on a relational database pop-
ulated by the information extraction techniques
used in the previous item.
This line of research has been funded for three
years (2009-2012) by the Argentinean Ministry for
Science and Technology, within the PAE project, as
a PICT project (PAE-PICT-2007-02290).
This project opens many opportunities for collab-
oration. The resulting corpora will be of use for lin-
guistic studies. The results of learning edit distances
to find abbreviations can also be used by linguists as
an input to study the regularities found in this kind
of genres, as proposed in (Alonso Alemany, 2010).
We think that some joint work on learning string
edit distances would be very well integrated within
this project. We are also very interested in collabo-
rations with researchers who have some experience
in NLP in similar genres, like short text messages or
abbreviations in medical papers.
Finally, interactions with data mining communi-
ties, both academic and industrial, would surely be
very enriching for this project.
4.2 Characterisation of verbal behaviour
One of our research interests is the empirical charac-
terization of the subcategorization of lexical items,
with a special interest on verbs. This line of work
has been pursued mainly within the KNOW project,
in collaboration with the UB-GRIAL group9.
Besides the theoretical interest of describing the
behaviour of verbs based on corpus evidence, this
9http://grial.uab.es/
line has an applied aim, namely, enriching syntac-
tic analyzers with subcategorization information, to
help resolving structural ambiguities by using lexi-
cal information. We have focused on the behaviour
of Spanish verbs, and implemented some of our find-
ings as a lexicalized enhancement of the dependency
grammars used by Freeling10. An evaluation of the
impact of this information on parsing accuracy is un-
derway.
We have applied clustering techniques to obtain
a corpus-based characterization of the subcatego-
rization behaviour of verbs (Alonso Alemany et al,
2007; Castello?n et al, 2007). We explored the be-
haviour of the 250 most frequent verbs of Spanish
on the SenSem corpus (Castello?n et al, 2006), man-
ually annotated with the analysis of verbs at various
linguistic levels (sense, aspect, voice, type of con-
struction, arguments, role, function, etc.). Apply-
ing clustering techniques to the instances of verbs in
these corpus, we obtained coarse-grained classes of
verbs with the same subcategorization. A classifier
was learned from considering clustered instances as
classes. With this classifier, verbs in unseen sen-
tences were assigned a subcategorization behaviour.
Also with the aim of associating subcategoriza-
tion information to verbs using evidence found
in corpora, we developed IRASubcat (Altamirano,
2009). IRASubcat11. is a highly flexible system de-
signed to gather information about the behaviour of
verbs from corpora annotated at any level, and in
any language. It identifies patterns of linguistic con-
stituents that co-occur with verbs, detects optional
constituents and performs hypothesis testing of the
co-occurrence of verbs and patterns.
We have also been working on connecting pred-
icates in FrameNet and SenSem, using WordNet
synsets as an interlingua (Alonso Alemany et al,
SEPLN). We have found many dissimilarities be-
tween FrameNet and SenSem, but have been able
to connect some of their predicates and enrich these
resources with information from each other.
We are currently investigating the impact of dif-
ferent kinds of information on the resolution of pp-
attachment ambiguities in Spanish, using the AN-
CORA corpus (Taule? et al, 2006). We are exploring
10http://www.lsi.upc.edu/?nlp/freeling/
11http://www.irasubcat.com.ar/
12
the utility of various WordNet-related information,
like features extracted from the Top Concept Ontol-
ogy, in combination with corpus-based information,
like frequencies of occurrence and co-occurrence of
words in corpus.
The line of research of characterisation of verbal
behaviour presents many points for collaboration.
In collaboration with linguists, the tools and meth-
ods that we have explained here provide valuable in-
formation for the description and systematization of
subcategorization of verbs and other lexical pieces.
It would be very interesting to see whether these
techniques, that have been successfully applied to
Spanish, apply to other languages or with different
resources. We are also interested in bringing to-
gether information from different resources or from
different sources (corpora, dictionaries, task-specific
lexica, etc.), in order to achieve richer resources.
We also have an interest for the study of hypothe-
sis testing as applied to corpus-based computational
linguistics, to get some insight on the information
that these techniques may provide to guide research
and validate results.
4.3 Discovering relations between entitites
As a result of the Microbio project, we have devel-
oped a module to detect relations between entities
in biomedical text (Bruno, 2009). This module has
been trained with the GENIA corpus (Kim et al,
2008), obtaining good results (Alonso Alemany and
Bruno, 2009). We have also explored different ways
to overcome the data sparseness problem caused by
the small amount of manually annotated examples
that are available in the GENIA corpus. We have
used the corpus as the initial seed of a bootstrapping
procedure, generalized classes of relations via the
GENIA ontology and generalized classes via clus-
tering. Of these three procedures, only generaliza-
tion via an ontology produced good results. How-
ever, we have hopes that a more insightful charac-
terization of the examples and smarter learning tech-
niques (semi-supervised, active learning) will im-
prove the results for these other lines.
Since this area of NLP has ambitious goals, op-
portunities for collaboration are very diverse. In
general, we would like to join efforts with other re-
searchers to solve part of these complex problems,
with a special focus in relations between entities and
semi-supervised techniques.
5 Opportunities for Collaboration
We are looking for opportunities of collaboration
with other groups in the Americas, producing a syn-
ergy between groups. We believe that we can artic-
ulate collaboration by identifying common interests
and writing joint proposals. In Argentina there are
some agreements for billateral or multi-lateral col-
laboration with other countries or specific institu-
tions of research, which may provide a framework
for starting collaborations.
We are looking for collaborations that promote
the exchange of members of the group, specially
graduate students. Our aim is to gain a level of col-
laboration strong enough that would consider, for
example, co-supervision of PhD students. Ideally,
co-supervised students would spend half of their
time in each group, tackle a problem that is common
for both groups and work together with two super-
visors. The standard PhD scholarship in Argentina,
provided by Conicet, allows such modality of doc-
torate studies, as long as financial support for travels
and stays abroad is provided by the co-supervising
programme. We believe that this kind of collabora-
tion is one that builds very stable relations between
groups, helps students learn different research id-
iosyncrasies and devotes specific resources to main-
tain the collaboration.
References
Laura Alonso Alemany and Santiago E. Bruno. 2009.
Learning to learn biological relations from a small
training set. In CiCLing, pages 418?429.
Laura Alonso Alemany, Irene Castello?n, and Nevena
Tinkova Tincheva. 2007. Obtaining coarse-grained
classes of subcategorization patterns for spanish. In
RANLP?07.
Laura Alonso Alemany, Irene Castello?n, Egoitz Laparra,
and German Rigau. SEPLN. Evaluacio?n de me?todos
semi-automa?ticos para la conexio?n entre FrameNet y
SenSem. In 2009.
Laura Alonso Alemany. 2010. Learning parameters
for an edit distance can learn us tendencies in user-
generated content. Invited talk at NLP in the So-
cial Sciences, Instituto de Altos Estudios en Psicolo-
gia y Ciencias Sociales, Buenos Aires, Argentina, May
2010.
13
I. Romina Altamirano. 2009. Irasubcat: Un sistema
para adquisicio?n automa?tica de marcos de subcatego-
rizacio?n de piezas le?xicas a partir de corpus. Master?s
thesis, Facultad de Matema?tica, Astronom??a y F??sica,
Universidad Nacional de Co?rdoba, Argentina.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adap-
tive duplicate detection using learnable string simi-
larity measures. In Proceedings of the ninth ACM
SIGKDD.
Santiago E. Bruno. 2009. Deteccio?n de relaciones entre
entidades en textos de biomedicina. Master?s thesis,
Facultad de Matema?tica, Astronom??a y F??sica, Univer-
sidad Nacional de Co?rdoba, Argentina.
Rafael Carrascosa, Franc?ois Coste, Matthias Galle?, and
Gabriel Infante-Lopez. 2010. Choosing Word Occur-
rences for the Smallest Grammar Problem. In Pro-
ceedings of LATA 2010. Springer.
Irene Castello?n, Ana Ferna?ndez-Montraveta, Glo`ria
Va?zquez, Laura Alonso, and Joanan Capilla. 2006.
The SENSEM corpus: a corpus annotated at the syntac-
tic and semantic level. In 5th International Conference
on Language Resources and Evaluation (LREC 2006).
Irene Castello?n, Laura Alonso Alemany, and Nevena Tin-
kova Tincheva. 2007. A procedure to automatically
enrich verbal lexica with subcategorization frames. In
Proceedings of the Argentine Simposium on Artificial
Intelligence, ASAI?07.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. Int. J. Doc. Anal. Recognit.,
10(3):157?174.
Alexander Clark. 2006. Pac-learning unambiguous nts
languages. In International Colloquium on Grammat-
ical Inference, pages 59?71.
Alexander Clark. 2007. Learning deterministic context
free grammars: the omphalos competition. Machine
Learning, 66(1):93?110.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania, PA.
Paul Cook and Suzanne Stevenson. 2009. An unsuper-
vised model for text message normalization. In Work-
shop on Computational Approaches to Linguistic Cre-
ativity. NAACL HLT 2009.
E. Go?mez-Ballester, M. L. Mico?-Andre?s, J. Oncina,
and M. L. Forcada-Zubizarreta. 1997. An empir-
ical method to improve edit-distance parameters for
a nearest-neighbor-based classification task. In VII
Spanish Symposium on Pattern Recognition and Image
Analysis, Barcelona, Spain.
Jin D. Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1).
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In ACL, pages 128?135.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. of ACL 42.
Franco Luque and Gabriel Infante-Lopez. 2009. Upper
bounds for unsupervised parsing with unambiguous
non-terminally. In International Workshop Compu-
tational Linguistic Aspects of Grammatical Inference.
EACL, Greece.
Franco M. Luque. 2009. Implementation of the
DMV+CCM parser. http://www.cs.famaf.
unc.edu.ar/?francolq/en/proyectos/
dmvccm.
AndrewMcCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceedings
of the Proceedings of the Twenty-First Conference An-
nual Conference on Uncertainty in Artificial Intelli-
gence (UAI-05), pages 388?395, Arlington, Virginia.
AUAI Press.
Matthew Michelson and Craig A. Knoblock. 2006.
Phoebus: a system for extracting and integrating data
from unstructured and ungrammatical sources. In
AAAI?06: proceedings of the 21st national conference
on Artificial intelligence, pages 1947?1948. AAAI
Press.
Matthew Michelson and Craig A. Knoblock. 2009. Ex-
ploiting background knowledge to build reference sets
for information extraction. In Proceedings of the 21st
International Joint Conference on Artific ial Intelli-
gence (IJCAI-2009), Pasadena, CA.
Naoaki Okazaki, Sophia Ananiadou, and Jun?ichi Tsujii.
2008. A discriminative alignment model for abbrevia-
tion recognition. In COLING ?08: Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 657?664, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Jose? Oncina and Marc Sebban. 2006. Learning stochas-
tic edit distance: Application in handwritten character
recognition. Pattern Recognition, 39(9):1575?1587.
E. S. Ristad and P. N. Yanilos. 1998. Learning string edit
distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20:522?532.
Mark Stevenson, Yikun Guo, Abdulaziz Al Amri, and
Robert Gaizauskas. 2009. Disambiguation of biomed-
ical abbreviations. In BioNLP ?09: Proceedings of the
Workshop on BioNLP, pages 71?79, Morristown, NJ,
USA. Association for Computational Linguistics.
M. Taule?, M.A. Mart??, and M. Recasens. 2006. Ancora:
Multilevel annotated corpora for catalan and spanish.
In LREC?06.
14
