Proceedings of the 12th Conference of the European Chapter of the ACL, pages 77?85,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
???? ??? ?????? ? ??? ? ? ????? ?? ?? ???? ? ?? ??
? ????? ?? ??????? ? ????? ? ???? ? ?
?? ?? ?? ?????? ? ?
?????? ?? ?? ? ??? ??? ? ?????? ??
???? ??? ? ?? ???? ?? ??? ???? ? ? ? ???
??? ????? ???
??????????????
?????????? ???? ? ?
?? ?? ??? ??? ? ? ? ????? ?? ?? ? ??? ??? ? ?
??? ??? ? ?? ? ? ?? ?? ?????? ???? ? ??? ??
?????? ?? ???? ????????? ?? ??????? ???? ?? ???
???? ?????? ??? ? ??? ?? ?? ??? ? ?? ???? ? ??
???? ???????????? ?????????? ??? ???? ???????
??? ?? ?? ?? ????? ???? ?? ?? ?? ? ? ??? ?? ?
?? ?? ??? ? ? ? ??? ?????? ?? ??? ?? ?? ? ???????? ?
? ????? ??????? ??? ??? ?????? ?? ?????? ???
?? ?? ???? ??? ????? ?? ?? ?? ???????? ??? ??????
? ???? ? ?? ?? ???? ?? ?? ?? ???? ? ? ? ?? ?? ??
??? ??? ?? ?? ??? ? ? ?? ? ????? ??? ?? ? ????? ?
?? ?? ??? ? ? ??? ?? ?? ?? ? ??? ? ??? ?? ? ?
??? ??? ? ?? ? ? ???? ?? ? ??????? ?? ?? ???
????? ??? ???????? ?? ???????????? ????
????? ??? ?? ??? ???? ????????? ????? ?? ??
??? ? ?? ?? ? ???? ??? ? ? ?? ? ???? ?? ? ??? ??
??? ???? ?????? ??? ???? ??? ??? ? ?? ? ?? ??
????????????? ???????? ??????????? ??? ???
??? ? ??? ?? ? ?? ?? ?????? ?? ???? ??? ???
?? ?? ???? ? ?? ?? ???? ? ?? ?? ???? ???? ????
? ?? ?? ??????? ??? ? ???? ?? ?? ??? ? ?
?? ? ??
? ? ??? ?????? ? ? ?????? ????????? ?? ? ?? ?
? ?
???????? ?? ?? ? ??? ??? ? ? ??? ??? ??? ? ? ? ?
?????????? ?? ?????? ?????????????? ????????????
??? ???? ??? ?? ?? ??? ? ??????? ?? ?? ?????? ??
????? ?? ? ???? ? ??? ??? ??? ? ? ?? ??? ??? ?
?? ? ? ??? ??? ?? ?? ? ? ?? ?? ? ? ???????? ?? ?? ?
????? ?? ?????? ?? ???? ??? ? ??? ?? ?? ??? ? ??
???? ? ?? ?? ?? ??? ?????? ??? ? ?? ?? ??? ??
????????????? ? ???????????? ???? ??? ? ????
????????? ? ?????? ???????????? ?? ? ??????
? ???? ? ?? ?? ?? ? ?? ? ??????????? ?? ? ?????? ???
?? ?? ? ?? ?? ? ? ?? ?? ?? ?? ???? ?? ? ?? ??? ?? ? ? ??? ?? ?
? ? ? ?? ? ????? ?? ?? ?? ? ? ? ? ? ? ? ?? ? ? ??? ?? ?? ?
?????? ??? ????? ?????? ??? ???? ???????? ????
??? ???? ?? ?? ???? ???? ??? ???? ???????? ? ? ?
???? ??? ?? ?? ????? ? ? ???? ?????? ? ?? ?? ? ???? ??
???? ? ? ???????? ?? ?? ? ? ? ?? ? ? ???? ? ?? ?
??????? ? ??????? ?? ? ??????? ? ? ? ????
?? ?????? ?? ???? ? ? ? ?? ? ??? ?? ?? ???? ?? ? ? ?? ?
? ??? ?????? ??? ???????? ????? ??? ????? ??????
?? ?????????? ????? ?? ??? ? ???? ??????????????? ?
?? ????? ? ?? ? ? ?? ?? ? ?? ?? ????? ? ? ???? ???
????
??? ? ?? ????? ? ?? ?? ? ?? ?? ? ?????? ?? ?? ?
?? ???? ? ??? ? ??? ?? ?? ??? ?? ? ?? ????????
??? ????? ? ? ????? ? ? ?? ? ??? ?? ??? ??????? ????
???? ??????? ?? ???? ???????? ??????? ??? ??????
???????????? ???? ? ? ???? ?????? ?????? ?? ?????
?? ? ? ?? ??? ? ??? ?? ??? ???? ???? ??? ??? ??? ??
? ????? ????? ? ????? ?????? ?? ???????? ???????? ? ?
?? ?? ? ??? ????? ?? ?? ??? ? ?? ?? ?? ? ? ?????
???? ??? ???? ?? ??? ??? ???? ?????? ?? ?????????
?? ? ??? ??? ???? ??? ?? ??? ??? ? ?????? ??????????
?? ?? ??? ??? ? ??? ?? ???? ?? ?? ? ??????? ????? ? ?
?? ?? ? ???? ??????? ???? ? ? ??? ?????? ?? ?? ?
?? ? ??? ? ?? ?? ??? ? ??? ??? ????? ??? ?????? ?? ?
?? ? ??????????? ???? ??? ?????? ???????? ????????
??? ?????
? ? ??????????? ??? ?????????????? ??????
????????? ?? ?
??? ?????? ?? ??? ?? ??? ???? ? ????? ?? ????? ???
????????? ?? ? ??? ?? ? ?????????? ? ?? ? ???? ????? ?
??? ? ??? ?? ? ???? ??? ??? ? ????????? ?? ???
??? ??? ?? ??? ? ?? ? ???? ?? ? ????? ? ? ??? ??
??? ???? ?? ??? ? ?? ?? ? ?? ?? ?? ? ? ? ?? ????
?? ? ?? ????? ? ?????? ???? ??? ??? ??? ??? ??
??? ????? ??????? ??? ?????? ?????? ???? ? ??? ?? ?? ?
??? ?? ??????? ?? ?? ? ?? ?? ??? ?? ? ? ?? ? ?? ??
?? ? ?? ?? ???? ???? ???? ?????? ? ??? ??? ?? ????
??? ??? ? ???? ?? ? ?? ???? ?? ?? ? ?? ??? ? ??? ? ? ? ?? ?
??? ? ?? ?? ?? ???? ?
77
??? ??? ???? ????????? ??? ? ?????? ? ?? ??? ????? ???
??? ? ??? ??? ???????? ?????? ???? ?????? ?? ?????? ???
????? ??????? ??????? ?? ?????? ?? ??????? ???
???? ?????? ?? ??? ????????? ?????? ??? ? ??????? ??
?? ? ????? ? ????? ??? ?? ?? ? ????? ? ? ? ???????
??? ?? ????? ?? ??? ?? ? ??? ??? ??? ?? ? ? ????
????????? ?????????? ??????? ??? ??????????
??? ??
? ? ?????????????? ??? ?????????????? ??????
????????? ?? ?
????????? ????? ?????????? ??? ??????? ??? ????
?? ? ???????? ???? ???????? ?????? ??? ? ??? ?? ? ?
?? ?????? ???? ???? ?? ? ??? ??? ????? ???? ????? ??
??? ? ??? ? ? ????? ?? ?? ?????? ? ?? ???? ?? ?? ?
??? ?? ??? ? ? ? ??? ??? ?? ??? ?? ??? ?? ?? ?? ?
???????? ??????? ??? ???? ???? ???????? ????
??????????? ???????????? ???? ?????????? ?? ?
??????? ?? ???? ??? ???? ??? ??????? ????
?? ?? ??? ? ?? ???? ??? ??? ???? ??? ?? ?? ??
??? ? ? ? ?? ?? ???? ??? ?? ?? ?? ? ? ?? ?? ? ? ?? ?
??? ?? ??? ???? ???????? ? ???? ? ?? ?? ???? ?? ?
??? ?? ??? ?? ? ???? ???? ? ???? ???? ? ?? ?? ?? ?? ?
????? ????? ?? ??? ??? ??? ??? ???? ??? ???????
???????? ??? ??????????? ??? ?????? ?????????
???????? ???? ??? ??? ?????????? ?? ??? ? ?? ?? ???
??? ? ??? ?? ??? ?? ?? ? ??? ??? ??? ?? ??? ?? ? ? ?
?? ??? ? ???? ????? ????? ????? ? ???????????????????
????????
?? ? ???? ? ??? ? ? ? ?? ?? ??? ??? ?????? ??
?? ?????? ? ??? ? ????? ??? ?????? ??? ? ??? ??
? ???? ?? ? ?? ? ????? ?? ?? ????? ?????? ????????
??? ? ??? ?? ??? ? ? ? ?? ?? ???? ????? ?? ? ?????
??? ?? ? ? ?? ??? ?? ? ?? ??? ?? ?? ?? ?? ??? ?? ??
??? ???? ??????? ??? ? ????????? ???? ?????? ?? ?
?? ??? ??? ????? ?? ? ?? ? ???? ???? ??? ?? ? ? ?? ???
??? ? ??? ?????? ? ?? ?? ? ???? ????? ??? ??? ?? ?
?? ?? ??? ? ? ???? ?????? ??? ?? ?? ??? ?? ??
?? ?? ? ?? ???? ??? ?????? ??? ??? ?? ? ? ?? ?? ? ?
???????? ?????? ??????????? ???? ?????? ???
??? ?? ??? ?? ?? ?????? ??????????? ? ?? ?? ??
? ?? ? ???? ?? ?? ??? ? ???? ??? ?? ??? ? ?? ?????
? ??? ????? ???? ???? ??????? ??? ? ????? ?????? ??
?? ? ?????? ??????? ???
? ? ???? ?? ? ???? ?????? ? ?? ???????????????
? ???? ?? ?? ?
??? ??? ?? ? ???????? ? ? ????? ?? ? ??? ???????
?????? ?? ?? ?? ? ?? ???? ?? ??? ???? ?? ?? ???? ?
??? ???? ?? ??? ??? ?? ??? ?? ? ? ? ???? ? ??? ??? ????
??? ?? ??? ?????? ??? ???? ??? ????? ??????? ???
???? ??? ? ???????? ?? ??? ?? ?? ?? ??? ????? ???
??????? ?????? ?? ?? ?? ?? ?? ?? ?? ? ??? ??? ??? ??? ?? ?
???????? ??? ? ?? ??? ??? ??? ?? ? ? ????? ?????? ????
?? ?? ???? ?? ?? ?? ?????? ?? ?? ? ? ??? ?? ? ?
? ?????? ? ??? ? ?? ? ?? ? ???????? ?? ??????? ?? ???
?? ??? ?? ? ? ?? ??? ??? ? ?? ? ? ?? ? ?? ? ?? ?????
???? ? ?? ?? ??? ? ?????? ??? ?? ??? ??? ?? ??????
??? ???? ?? ? ??????? ????? ??? ? ? ???? ???? ??
??? ? ? ? ? ?????? ?? ?? ?? ??? ??????? ??? ??? ??
? ???? ?? ?? ? ?????? ???? ?? ???? ?? ????? ? ?? ?
???? ?? ????? ??
?? ? ?????? ? ?? ?????? ??? ???? ???????? ?
?? ???? ?? ? ??? ? ??? ?? ? ? ?? ??? ?? ? ???? ????
? ????? ? ?? ? ? ???? ???? ? ??? ??? ???? ?? ? ??? ??
?????? ? ?? ?? ? ?? ?? ??? ?? ? ? ???????? ?? ??? ???
? ? ?? ????? ??? ??? ?? ?????? ? ?? ?? ? ??????? ??
???? ?????? ???????? ????? ?????? ? ? ?? ???? ??? ?????
?? ? ??????? ?? ????? ?? ??????? ???? ????? ? ? ? ?? ???
?? ?????? ? ?? ??? ???? ??? ?? ? ? ? ?? ? ?????
???????? ????????? ?? ??? ?????????? ????????
???? ???? ??? ???? ? ? ??? ? ???????? ?? ?? ?
??? ? ?? ?? ??? ? ??? ??? ???? ??? ?? ? ? ? ?? ??????
?????? ??? ??????? ?? ? ? ? ?? ? ? ??? ? ?? ???
?? ? ??? ?? ?? ?????? ????? ?? ??????????? ?? ?
? ?????? ??
????? ???? ???? ? ?? ????? ?? ???? ??? ?? ? ? ? ?
?????????? ? ??? ???? ???????? ??? ? ??????
??? ??? ?? ? ????? ?????? ?? ???????? ?? ?? ???? ?
??????? ? ? ???? ????? ? ? ???? ?? ? ? ?? ??????
?? ?????? ? ???? ????? ??? ? ??? ??? ????? ???? ???
? ? ????????? ????????? ?? ?? ?? ??????? ??? ???? ?
?? ?? ? ?????? ??????? ? ?? ?? ? ????? ???? ? ? ?? ?
??? ? ??? ?? ??? ? ? ? ?? ???? ??? ?? ?? ? ? ? ?
??????? ???????? ??? ? ?? ? ?? ?? ?? ? ?????? ????
? ? ???? ??? ? ??? ?? ? ???? ?? ? ??? ?? ??? ??? ?
????? ?
? ???? ??? ?? ???? ? ?? ? ? ? ??? ??? ??
?????? ? ? ? ? ??
??? ? ?? ?? ?? ??? ? ?? ?? ? ????? ??? ???????? ? ? ?
?? ? ??? ? ??? ?? ? ? ??? ??? ? ?? ? ? ? ??? ??
? ??????? ?? ?? ? ??? ?? ??? ? ?? ?? ??? ???????? ? ?? ?
??? ?????? ??????????? ?? ??????????? ??????
??????????? ?????? ????? ?? ????? ????? ???
???????? ? ??? ? ???? ?? ?? ?? ? ?? ? ? ???? ???? ?
?? ? ??????????? ? ??? ? ?? ??? ?? ?? ????? ? ???
???? ?? ? ? ?????? ??? ?? ??? ? ???? ? ? ???? ? ?
????????? ??? ???? ? ?????? ????????????? ????????
?? ? ????????? ??????? ?? ? ????? ??? ??????
?? ?????? ? ?? ? ????? ?? ? ? ?? ???? ????? ?
???????? ?? ???????? ??
?? ? ????? ?? ?? ?? ? ??? ?? ? ??? ??? ??????
????? ????? ????? ??? ???? ?????????? ????
???????? ? ??????? ?? ?? ? ???? ????? ? ??? ? ???
??? ??? ?? ??? ??? ???? ?? ?????? ??? ??????? ?
?? ? ???? ??? ?? ? ? ?? ? ?? ??? ?? ?????? ?? ?
????? ? ??? ?? ? ??? ?? ? ? ??? ? ? ??? ??? ?? ????????
??? ???? ???????? ????? ? ???? ????? ??? ???? ?
78
?? ? ???? ???? ?????? ? ??????? ? ??? ????? ??
???? ?? ??? ??? ? ?? ? ????? ?? ? ? ? ?? ? ??? ??? ???
?? ??? ??? ??? ?? ???? ??? ?? ? ???? ??? ???? ??????
???? ?? ? ???? ? ? ?? ? ??? ? ??? ?? ?? ?? ?? ??? ???
????? ? ??????? ? ? ?????????
? ?????? ??????????????? ??????????????????????
? ?????????????????? ?????????????
?????? ?????????????????? ?????????? ??????????
? ???? ????? ?????????????? ???? ???????
?? ??? ? ??? ? ??? ? ????? ????? ??? ???? ???
? ? ?? ? ????? ??? ? ??? ??? ?? ? ? ?? ?? ?????? ???
?? ? ?? ??? ? ???? ? ?? ?? ? ??? ??? ?? ??? ???????? ????
?????? ?? ???????????? ????? ?????? ??? ? ????
?? ?????? ? ??? ? ????? ?? ? ? ? ? ?? ? ?????
?????? ??? ???? ?? ???????????? ???????? ??????
????????? ???? ??????????? ???????????????? ????
???? ?? ?? ? ???? ?????? ??? ??? ?? ?? ??? ????
???? ?? ? ? ?? ??? ??? ???? ??? ??? ? ? ? ? ?
?? ?? ?? ? ??? ??? ? ??? ?? ??? ?? ? ??????? ??? ?
?? ???? ?? ??? ?? ?????? ?? ?? ???? ?? ?? ? ????? ???
?????????? ??????? ? ??????? ? ?????? ? ????
??? ? ??? ?? ???? ?? ? ? ?? ?? ? ? ?? ??? ?? ? ?
??? ?? ??? ? ??? ? ? ?? ?????? ??? ??? ?? ?? ?
? ???? ??? ? ??? ? ? ?? ? ??? ??? ?? ? ??? ??? ?? ? ?
?? ?????? ? ??
?? ? ?? ?? ?? ?? ? ? ? ? ?? ? ??? ?? ??? ??? ?
????? ? ?? ????? ????? ??? ?????????? ????? ???? ?
?? ?? ???????? ? ???? ???? ?? ? ???????? ? ? ??????
?? ? ??? ? ??? ?? ? ? ?? ??? ??? ??? ? ?? ? ?????
??????? ?? ? ???? ?????? ?? ??? ??????? ?? ?? ??? ??
??? ?????? ?? ??????? ?? ??????????? ?????? ??? ?
?? ? ????? ?? ?? ?? ? ??? ???? ?? ? ? ?? ? ? ?
??????? ?? ??? ??????? ? ??? ????? ? ?? ??? ????? ?? ???
???? ????? ? ? ?? ?? ?? ? ??? ?? ??? ????? ? ? ???? ????
?? ??? ??? ??? ?? ??? ???? ????? ? ? ?? ?? ??????????
?? ? ?? ?????????????????
?? ? ??? ??? ?? ? ????? ??? ?? ?????? ?? ???
???? ??? ? ??? ? ?????? ?? ? ???? ? ? ???????????
??? ?? ? ?? ? ? ?? ???? ? ? ?? ?? ?? ? ??? ??? ? ? ?
?? ? ???? ?? ?????? ?? ?? ?? ? ??? ? ??? ?? ????
?? ?????? ??? ??????? ??? ?? ?? ? ? ? ?? ? ??? ?? ??
? ?????? ??? ?? ?? ? ?????? ???? ?? ?? ????? ???? ?? ?
??? ? ??? ??? ?????????? ? ??? ?? ? ??????? ? ? ?? ?
???????? ??????? ???????????? ???????????????
??? ???? ? ?????????? ????? ????? ??? ? ????? ? ???
? ??????? ?? ? ?? ? ????? ??
???? ?????? ? ??? ?? ? ????? ?? ??? ?????? ? ??
????? ? ??????? ?? ?? ???? ??? ? ??? ?? ?? ???
?? ?? ? ??? ??? ? ??? ? ?? ??? ? ?? ? ? ?? ??? ? ??? ? ? ?
? ?? ?????
?? ? ? ? ? ? ?? ?? ?? ??? ?? ? ?? ?????? ?? ??? ???? ?? ?? ?
??? ?? ? ? ? ?? ?????? ?? ??
?? ?? ?? ?? ????? ?? ?? ??? ???? ?? ?? ? ? ? ?? ?? ?? ?????
???? ? ????? ? ?? ?? ????? ?? ?
???? ?? ?? ? ??? ? ???? ????? ? ?? ?? ? ?? ? ????????? ?
? ??? ???? ? ??? ????????????????? ???? ? ??? ??????
?? ? ??? ?? ?? ? ??????????? ?? ??? ?? ???????????
??? ?? ??? ?????? ? ?? ???? ? ???? ??? ??? ? ??? ??
???? ??? ?? ??? ??? ????? ?????? ??????? ??? ?? ?
?????????? ?? ?? ?? ? ???? ????? ?? ?? ? ????? ? ?
?? ?? ? ? ?? ? ?? ????????? ???? ? ??? ?? ??? ?? ?
??????????? ??? ?????? ???????? ??? ?????? ???
?? ? ???? ???? ??? ?? ?? ?? ?? ???? ? ? ??? ?? ?? ?? ?
???? ? ? ?? ? ??? ??? ?????? ? ?? ???? ?? ? ? ???? ? ?
??? ? ??? ??? ????
?? ? ???? ??? ? ? ?? ?? ????? ????? ???? ??? ? ????
?? ?? ??? ?????? ???? ??? ? ?? ?? ??? ?? ??? ??? ?
?? ?????? ???? ? ??? ??? ? ????? ?? ?? ????? ?????? ?
???????? ???????????? ???????????? ???? ???????
??? ? ?? ? ?? ??? ?? ??? ????? ??? ? ??? ?? ????
???????????? ????????? ?? ? ????????? ? ???
???? ?????? ?? ? ????? ??? ??? ??? ?? ??? ??? ?????
?? ?????? ?????? ???? ????? ????
???? ????? ??????????? ??? ????????? ???
??????? ?? ?? ?? ? ??? ??? ?? ?? ? ? ?? ? ??? ? ??? ???
????? ????? ???? ?????????? ???????? ??????
?? ???? ?? ?? ???? ? ??? ???? ?? ??????? ? ? ??? ?
?? ?????? ?? ???? ?????? ??? ???? ?? ?? ? ???
???? ???? ?? ???? ?? ?? ???? ?? ? ? ?? ?? ? ??? ?????? ?
?? ???? ? ?????? ??? ?? ?? ? ?????? ??????? ? ?? ?? ?
??? ????? ??? ? ??? ?? ??? ???? ??? ??? ?? ?? ???? ?
????? ? ? ? ?? ?? ? ??? ???? ????????? ? ??? ??? ?????
??? ????????? ?? ??? ? ???
?? ???? ????? ???????????? ?? ????? ? ???? ????
?? ? ??? ???? ??? ???? ??? ? ????? ? ?? ? ??? ???
???? ??? ?? ??? ???? ???? ??? ??? ??? ?? ?? ?? ?
??? ?????? ???
????????????????????????????????????????????
????????????????????????????????????????
???????? ???
???????????????????????? ????????????????
??????????????????????????????????????? ???????????
?? ???? ????? ???? ????? ? ???? ?? ??? ?
??? ? ? ?????? ?? ? ?? ? ????? ???????? ? ? ?? ??? ??
????????? ????? ?? ????????? ???????? ?? ?????
?? ?? ?????????????? ? ?? ??? ????? ? ???? ? ????
?? ? ???
? ????????????? ???? ?????? ??? ????
???? ? ?? ???? ????
?? ?????? ? ????? ? ?? ??????? ? ? ?? ? ????????
?? ? ?? ??? ?? ? ?????? ?? ????? ? ???? ? ??? ? ?
?? ?? ?? ?? ??? ?????????????????? ?? ?????? ? ???
??????????????? ????????? ????? ????????????
???????????? ?? ???? ???? ??? ???? ?????????????
???? ??? ?? ? ?????? ? ?? ??? ? ??? ? ????? ? ?? ? ? ??
? ?????? ??? ??? ?????? ???????? ?
79
????? ???? ?????? ? ??? ? ?? ? ?? ?????? ?? ???
??? ?? ?? ? ? ??? ?? ?? ? ????? ? ? ? ?? ?? ??? ?????? ???
??? ?????? ??? ??????? ?? ?????? ????? ????? ????? ?
??? ? ??? ??? ?????????????? ?? ? ?? ??? ???? ? ? ????
????????????? ??? ????????? ??? ????? ??????????? ?
??? ?? ?? ?????? ?? ? ????? ??? ?? ? ?? ?????? ?? ? ??
??? ???? ? ??? ???? ? ? ??? ? ??? ??? ?????? ????? ? ?
?? ? ?? ????????? ? ? ?? ? ?? ??? ????? ??? ? ? ?? ?
? ????? ???? ? ??? ?????? ?? ?? ???? ?????
??? ??? ? ? ? ??????? ???? ? ???? ?????? ??
??? ?? ?????? ??????????? ? ??????? ???????? ?? ???
???? ??? ? ??? ?? ????? ???? ?? ? ????? ?? ????? ???
?? ??????????? ?? ???? ?????? ??? ?? ??? ??
??? ?????????????? ????? ? ??? ? ??? ????? ??
??? ?? ? ?? ???????? ? ? ?? ???? ? ??? ? ?? ?????
???? ? ? ? ? ??? ??? ? ? ? ?? ???? ?? ?? ? ??? ??
??? ???? ?? ?? ?? ??? ?????? ?? ??? ? ? ??? ??
? ? ???? ? ? ????? ???? ? ? ? ?????? ? ? ? ??? ? ?
???? ? ?? ?? ???? ?? ?? ?? ? ??? ? ? ?????? ????? ??
??? ???? ? ? ?? ???? ?????? ???? ? ???? ??
? ???? ? ? ??? ? ? ? ?? ? ??? ?
???? ? ?? ??? ?????? ?? ? ????? ????? ??? ??
?? ?? ???? ??? ??? ? ?????? ??? ??
?? ? ? ???? ? ???? ?? ????? ? ???? ??? ?? ? ???
????? ??? ??? ?? ??? ?? ?? ? ?????? ?? ??? ??? ?
???? ??? ?? ? ??? ?? ? ??? ?? ?? ? ????? ? ? ????? ???
?? ??????????? ?? ???? ???????? ?? ????? ?? ??? ??
??? ? ? ? ?????????? ??? ???????? ? ??? ??????
????? ??? ?? ?? ?? ?? ? ? ?? ? ?? ?? ??? ?? ? ?
?? ?? ?? ??
?? ??????? ????? ? ??? ?? ? ?? ?? ? ?? ?? ?? ?
???? ?? ???? ? ? ? ?? ??? ???? ?? ? ??? ? ??? ?
?? ?? ? ? ?? ? ??? ?? ? ??? ?? ? ?? ??? ???? ??? ? ?
?? ??? ?? ?? ? ??? ?? ? ??? ?? ?? ? ???? ?? ? ?
????? ??? ???????????? ???????? ?? ?? ?? ?? ?? ?
??? ? ??? ?? ???? ?? ??? ?? ??? ?? ?? ??????
?? ? ????? ??? ?????? ? ??? ? ?? ?? ??? ?? ? ????
?? ? ??????? ? ? ?? ? ??? ?? ? ?? ? ? ???? ?? ? ?? ?
??? ???????????? ???????????? ??? ??? ???????
????? ??? ?????? ???? ???? ? ????? ?? ?? ??? ?? ?
??? ?? ?? ?? ? ??? ???? ? ? ??? ?? ?? ??? ???????? ???
? ????? ?? ?? ???? ?? ? ? ?? ? ? ???? ??? ????? ?? ?
??? ?????? ??? ???????? ?? ?? ?? ? ?? ? ??? ???? ????
???? ? ?? ??? ?? ??? ? ? ? ?? ????? ??? ??????? ????
???? ?????? ??? ?? ??? ?? ?? ?????? ? ?? ?? ??????
? ? ?? ? ????? ?? ? ? ?? ? ???? ??????? ?? ?? ?????
??? ? ??? ? ? ? ?? ? ??? ???? ??? ??? ? ? ? ?? ?? ?
?? ?? ??? ?? ?? ???
????? ??? ? ?? ?? ?? ?? ?? ? ? ?? ? ??? ?? ????
???? ???? ? ?? ? ?? ? ? ? ? ? ? ??? ??? ? ? ?? ???? ???????
?? ? ? ?? ? ?? ?? ? ? ???? ? ?? ? ? ? ? ?? ?? ?? ? ? ?? ? ??? ?? ?
? ??? ??? ? ??? ??? ?? ????? ???
?
???
?
?
???
?
? ???????????? ? ??????? ??
?? ?
?? ????? ???????? ?? ??? ?? ? ?? ? ?? ? ? ?? ?
??? ? ??? ?? ? ?????? ?????? ? ??????? ? ?? ? ?? ? ?????
???????? ?? ? ? ?? ??? ?? ?? ?? ? ??? ???????? ?? ?
? ?? ??? ??? ?? ??? ?????? ? ? ??? ? ????? ?? ?? ? ??? ??
??????? ?? ?? ? ? ?? ?? ??? ?? ? ? ?? ??? ??? ? ? ?
? ?? ?????????? ????? ??? ??? ? ??? ????? ? ?? ????
???? ?? ? ??? ?? ? ? ? ?? ?? ?? ? ?? ???? ??? ??? ??? ?
??????? ? ??? ????????????? ? ? ? ? ? ?? ??? ???
? ???? ??????? ????? ??
?????? ?? ??? ????????? ??? ??? ?????? ??? ?
? ? ?? ? ???? ? ? ???? ?????? ?? ???? ?????? ? ????
????? ??? ? ???? ?? ? ??? ?? ??? ??????? ?? ??????
???????????? ????? ????????? ???? ?????????
??? ??? ?? ?? ?? ? ?? ??? ????? ? ? ?? ?? ?? ? ????
??? ? ??? ??? ??? ??? ?? ??? ?? ?? ? ?? ?? ???????????? ?
??? ?? ?? ? ? ?? ? ? ???? ? ? ??? ? ??? ?? ?? ???? ? ?
?? ??? ?? ????????? ? ?? ? ??? ??? ? ? ? ?? ? ????
????? ??? ? ? ? ?? ?????? ?? ?? ?? ????? ??? ?? ?
????? ? ???????????? ???? ?? ????? ? ??? ??? ?? ??
????? ??? ?? ? ?? ??? ?? ????????? ? ??? ?? ??? ??? ??
? ?? ???? ?? ? ?? ? ???? ???? ???????? ?? ? ? ??
????????? ?? ? ? ? ???????? ? ???? ??????? ?????
???? ?????? ??
? ???? ? ??? ????? ? ?? ?? ?? ??? ? ?
? ? ?????? ?? ?? ????????? ? ??????
?? ?? ????????? ?????????? ???? ??? ??? ?? ???? ? ?
?? ? ? ?? ?? ???? ????? ??? ??? ????? ??? ?
?? ??? ???? ?????? ?? ? ?????? ??? ? ???? ? ????
????? ?????? ????? ? ? ???? ??? ??? ? ??? ? ??? ? ? ?
?? ? ??? ???? ???? ??? ? ?? ?? ? ?? ?? ???? ? ?? ??
??? ? ?? ? ??????? ?? ?? ?? ? ??? ?? ??????? ? ????
??? ? ??? ? ? ????????? ??? ?? ? ??? ?? ?? ?
???? ?????????? ???? ?????????? ?????? ????
?? ?????? ??? ? ?? ? ?? ?? ?? ???? ?? ?? ?? ?
????? ?? ?? ? ?? ?? ???? ???? ??? ? ?? ?? ? ????? ?
??????? ??? ???? ?????? ??
?? ?? ? ?? ?????? ?? ???? ? ? ? ??? ?? ?? ?? ?
???????? ?????????? ??? ? ??? ?????? ???? ??? ? ???
??? ?? ?? ? ???? ?????? ? ? ? ?? ? ?? ? ? ?? ?? ?
? ?????????????? ?? ???????? ?? ??? ????? ?????? ??
??? ?? ?????? ??? ??? ?? ?? ?? ?? ?? ? ??? ? ??? ?
???? ?????? ?? ??? ??? ? ??? ?? ? ?? ??? ?? ?
???????? ??????? ?? ???? ?? ?? ?? ?? ? ?? ?? ?? ??? ??
? ????? ? ?? ?? ??? ????? ??? ??? ? ???? ?? ?? ???
? ??? ?? ?? ??? ??? ?? ? ??? ???? ? ? ?? ? ????
?? ?? ?? ?? ?? ???? ? ??? ? ? ? ?? ??? ?? ?? ??? ?? ?? ???? ?
??? ??? ??? ? ?? ?? ? ???? ??? ??? ?? ? ? ? ?? ? ?? ?? ? ??? ?
???? ??? ??? ?? ???? ??? ??? ? ?? ? ??? ??? ???? ? ??? ???
?? ???? ?????
80
?????? ?? ???????? ????? ??????? ???? ??????
? ?? ?? ?? ??? ?? ??? ?? ? ??? ??? ??? ??? ??
??? ??? ?? ??? ??? ?? ????? ??? ??? ?? ? ??? ? ??? ?
???????? ??? ??????? ? ?? ? ???? ???????????
?? ?????? ? ?? ?? ? ??????? ??? ? ? ? ? ??? ?? ? ?? ??
??? ???? ??? ??? ?? ?? ? ??????? ?? ??? ???????? ?
??? ????????? ?? ?? ?? ??? ??????? ????
?? ?? ??? ?????? ?? ???? ??? ??? ???? ?????? ?
???? ??? ?? ? ??? ??? ?? ??? ?? ?? ? ? ?? ?? ??? ???
??????? ?? ??? ????? ??????? ? ????? ? ?? ????? ?
?? ???? ?? ?? ? ??????? ?? ???? ?????? ? ?? ?? ??? ?? ?
? ?? ?? ?? ? ? ? ?????? ?? ?? ??? ?? ?? ?? ? ? ?? ?
???????? ??? ?????????? ????????? ????? ??
?? ? ?????? ? ?? ? ? ? ? ? ???? ?? ?? ??????? ???????? ?
? ?? ?? ? ? ??? ? ??? ??? ?? ?? ????? ?? ?? ??? ?
???? ???? ??? ? ?? ?? ??????? ? ?? ? ? ? ? ? ???? ???
??? ??? ?? ? ????? ?? ?? ? ?????? ? ??? ? ??? ?? ????
?? ?????? ?? ?? ????????? ?????? ????? ?? ?????
??? ??? ??? ? ?? ??? ?? ???? ? ? ??? ???? ??
?? ???? ???? ?????? ?
?? ? ? ?? ? ? ?? ? ??? ?? ??? ????? ?? ??? ????
?? ?? ???? ? ??????? ? ?? ??? ???? ? ???? ? ??? ?? ?
????? ??? ???????? ? ? ????? ?? ? ??? ? ??? ?? ? ?
??? ??? ? ?? ? ?? ???? ??? ?? ??? ????? ?? ? ???
?? ???????? ?? ??? ?? ???? ??? ? ? ???? ?? ??? ???
? ??? ?? ?? ? ?? ?? ?? ? ? ?? ??? ? ? ?????? ? ?? ??
??? ? ????? ?? ??? ? ????????????? ?? ?????? ? ??
? ? ???? ?? ?? ? ??????
?? ? ???????? ? ?? ?? ? ???? ?? ?? ???? ???? ?? ?
?? ?? ???
???? ?? ?? ? ? ? ?? ???? ? ?? ?????
?? ? ? ? ? ? ?? ??? ??
???
? ?????? ? ?? ? ? ? ?? ???? ? ?? ?????
?? ? ? ? ? ?? ?????
?
??? ? ?? ??? ? ???? ????????? ? ??? ?? ??? ??
? ? ?? ? ????? ? ????? ? ?? ? ? ????? ? ?? ?? ? ????
? ?? ?? ??? ?? ?? ??? ?? ?? ? ? ?? ? ?? ?? ????? ? ?
??? ?? ?????? ?? ? ?????? ??? ?? ? ? ? ?? ? ???? ? ?? ??
? ?? ??? ? ??? ?? ??? ? ? ? ?? ?? ?? ??? ?? ? ???
?? ???? ? ? ? ?????? ? ??? ?? ? ? ?? ? ??? ???????
? ?????? ??? ? ? ?? ? ??? ??? ?? ????? ??? ??? ?? ?
? ????? ? ??? ????? ? ? ??? ??? ?? ??? ?? ????? ? ?
??? ? ??? ? ?????? ?? ? ?? ? ?? ?? ???? ???????? ?? ??
????? ? ?? ?? ?? ? ????? ?? ?? ?? ????? ???? ?????? ?
???? ?????? ??? ???? ???? ???? ??????????? ????
??? ??????? ? ?????? ??? ? ? ??? ??? ???? ?? ?? ???
????????? ??? ?? ? ??? ?? ? ? ?? ? ??? ? ??? ?? ? ?? ???
???? ????? ?? ??? ?? ? ?? ?? ????? ? ? ??? ? ??? ? ???
???? ???????? ??????? ??? ????? ???? ???? ?????
???? ???????? ?? ? ? ?? ? ?? ??? ??? ? ? ?? ?? ?? ? ?????
?? ?? ??
?? ? ? ????? ? ??? ?? ?? ?????? ? ?? ?? ??
?????????? ? ??????? ?????? ? ?? ????? ?????? ??? ?
???? ?? ? ? ?? ?? ??? ???? ?? ?? ??? ?? ??? ????? ???? ?
?? ? ????? ??? ?? ?? ? ? ???? ? ? ?? ? ?? ? ?? ? ? ????
?? ??? ??? ?? ? ?? ?? ?? ? ? ?? ?? ??? ?? ???
????????? ????? ??????? ?????????? ???? ????
????? ? ? ? ? ?? ??? ?? ????? ? ? ??? ? ??? ? ? ???
?? ?? ? ????? ?? ?? ?? ?? ? ? ?? ?? ??????????? ?? ?
?????????? ??????? ???? ???????? ??? ????
? ????? ?? ??? ? ?? ?? ? ????? ? ?? ?????? ?? ?? ??
?? ? ? ?? ? ??????? ??? ??????? ?? ?? ?? ?? ?
????? ???
? ? ??? ?? ??? ? ?????? ????? ? ?? ?????? ? ???? ?
??? ?? ?? ??? ? ??? ?? ? ? ??? ??? ?? ? ? ?? ?
????????? ?? ?? ? ?? ??? ??? ? ??? ? ? ?
?? ? ???? ?????? ?? ??? ? ?? ?? ?? ? ????? ?? ? ? ?? ?
????? ?????? ?? ??? ??? ?? ? ???? ??? ? ? ?? ?
? ????? ??
?? ????? ???
???????? ? ???? ? ???
???? ??????????? ??????? ?????? ???????
??? ?????? ?? ? ?? ???? ? ??? ??? ? ? ?? ? ??? ????? ?
?? ? ?????? ?? ?? ????? ???? ??????? ?? ? ??? ???
???????? ??? ????? ? ?? ?? ??? ?? ? ? ?? ???????? ? ?? ?
???????????????? ??? ? ???? ?????????? ????
????? ??? ? ??????? ???? ?????? ??? ?? ?? ? ?? ?? ? ?
?? ????? ? ????? ?????? ? ????
?? ? ???? ??? ??? ? ? ?? ? ??? ?? ??? ??
?? ??? ? ?? ?? ??? ?????? ????? ???????? ?????? ?
? ? ??? ?? ?? ??? ????? ??? ?? ?? ?? ? ???? ?
?????? ?? ???????? ?????? ?? ??? ???? ? ??? ????
?? ? ? ? ? ? ?????? ? ? ???? ? ??? ??? ???? ?? ? ?? ?
??? ?? ????? ?????? ??? ?????? ?????? ???? ????? ?
?? ?????? ?? ??? ???? ??? ? ??? ???? ???? ?? ?????
? ? ??? ?? ??? ???? ???? ??? ?? ??? ??? ?? ? ????
? ??? ?? ? ????? ?? ? ?? ? ? ????? ? ???? ?? ? ? ?? ?
? ??? ??? ???? ?? ??? ?? ??? ???? ?? ?? ? ?? ?? ?? ???
?? ?????????? ??? ???? ??????? ??? ???? ??????
? ???? ???? ? ? ??? ??? ? ??? ???? ?????? ???? ???
? ?? ? ??? ?????? ??? ? ?????? ?? ??? ?????? ?? ? ?
??? ?? ??? ????????????? ?? ? ?? ????? ? ??? ????
? ????? ?? ?? ? ??? ?????? ? ? ??? ? ? ?? ? ? ????
??? ??? ?? ?? ??? ?? ?? ? ?????? ??? ? ? ? ?? ? ?
?? ? ?? ? ???? ? ? ???? ??? ?? ? ???? ?????? ? ?? ?? ?
???????????????? ???????? ??????? ???? ???
????? ?? ?? ? ?? ?? ?? ??? ?? ????? ? ??? ??
81
?????? ???? ???? ?????????? ???????? ?? ????
??? ?? ??? ?? ?????? ???? ?????? ??
? ?? ? ???? ???? ? ????? ?? ??? ???????
? ? ???? ? ?
? ? ??? ?? ????????? ??? ???? ?? ??
?? ???? ???????????? ???????? ????? ?????? ?
???????? ? ??????? ?? ?? ? ???? ?????? ? ? ??? ??? ?
??? ??? ? ?? ??? ?? ? ??? ?? ? ?? ?? ?? ? ? ??? ??
??? ?? ???? ??? ???? ????????? ??? ??? ? ??? ?? ????
????? ? ?? ??????? ????? ????? ??? ???? ??? ??? ???? ?
??? ? ??? ?? ???? ?????? ?? ? ??? ??? ? ?? ?? ? ???
??? ?? ??? ? ? ?? ? ????? ???? ???? ?? ? ?????????
??? ???? ?? ??? ??? ?? ?? ? ???? ? ??????? ?? ?? ?
???? ? ? ?? ? ??? ???? ????????? ????? ???? ? ? ?
?? ?? ?? ?? ? ? ?? ????? ??? ??? ??? ?? ?? ? ?? ?
??? ??? ?? ??
? ? ?? ????? ??? ?? ??? ??? ?????? ??? ?? ??? ?
? ???? ? ? ? ? ???? ?? ? ???? ? ? ? ???? ?? ??? ?
??? ??? ?? ?? ???? ?? ? ?? ??????? ? ?? ?? ? ?? ?
?? ?? ?? ??? ? ? ?? ?? ???? ?? ??????????? ?? ?
?? ???? ? ?? ? ?? ??? ? ?? ??? ?? ?? ? ?? ?? ?
???????? ?????? ? ???????? ??? ????????????
?????? ?? ???? ?????????? ??????????? ??? ?????? ?
?????? ??? ??????????? ???? ????????????????
???????????????? ??????? ?? ???? ??????????? ?
????? ???? ?? ????? ???? ???????????? ?????
?? ? ????? ???? ???? ?????????? ???? ???????????
???? ?????????? ???? ???? ?? ???? ??????? ?
?????????? ??? ??? ??????????? ??? ?????????
??????? ??????????
????? ?? ???? ? ??? ???? ??????? ???????? ?? ?? ?
????????? ? ?? ?? ??? ? ??? ? ??? ?????? ? ??? ?? ?
???? ?????? ? ? ?? ? ??? ?? ? ???? ????? ??????? ????
?? ?? ? ??? ???????? ? ?? ? ?? ?? ??? ?? ??? ???? ???
? ?? ?? ? ?? ??? ?? ?? ?? ??? ??? ? ??? ? ?? ? ?? ?
??? ??? ?? ? ? ?? ?? ? ? ???????? ??? ?? ??? ?? ? ? ?
?? ? ??? ???? ????????? ????? ? ?? ?? ???????? ?? ?
??? ???? ??? ?????? ?? ? ? ??? ?? ?? ? ??? ??? ?? ??
???? ?? ? ? ??? ???? ?????? ???? ? ?? ?? ?? ? ???
??? ?? ?
???? ???????? ???????? ?????? ???? ???
? ????? ?? ?? ? ??? ?? ? ? ???????????? ?? ?? ? ???
?????????? ????????????? ?? ? ??? ?? ? ??? ??????
??? ? ??? ?? ???? ? ??????? ?? ???? ?????? ?? ?
????? ??? ??? ??? ??? ?? ? ???? ?? ? ? ?? ?? ? ? ??
???? ?? ?? ???? ?? ?? ?? ?? ???? ??? ??? ?? ???
??? ??? ? ???? ?? ? ??? ???? ??? ? ? ???????? ? ?
??? ?? ???? ?? ???? ?????? ????
?? ? ? ?? ?? ??? ? ?? ? ??? ? ??? ??? ?? ?? ???
?? ?? ? ? ?? ??? ??? ?? ?? ?? ? ??? ???? ???????? ?
????? ?? ? ?????? ? ?????? ?? ??? ??? ??? ??
??? ?? ? ?? ?? ? ?????? ????? ? ?? ??? ????? ?? ?
??? ? ?? ?? ?? ? ????? ?? ???? ? ??? ??? ?? ??
?????????? ??? ???????? ? ?? ?? ???? ?? ?????
??? ?? ??? ?? ? ??????????? ??? ???????? ? ???
?????? ?? ? ? ?? ?? ?????? ?? ? ?? ? ??????? ? ? ?? ?
??? ?? ??? ? ? ??
?? ? ?? ? ?????? ???? ??? ?? ??? ????? ? ?
??? ??? ? ??? ? ?????? ??? ??? ???? ?? ?????? ???
?? ?? ? ?????? ????? ?? ? ??? ? ??? ?? ?? ?? ? ???? ???
??? ? ?? ?? ? ????? ??? ?????????? ????? ????? ???
?? ??????? ?? ??? ?????? ?? ?? ?? ????????????? ? ??
???? ? ?????? ????? ? ??? ?? ?? ? ? ?? ? ??? ? ??? ??
?? ?????????? ?? ? ??? ?? ??? ????? ??? ?????? ??? ????
?? ?????????? ?? ?????????????? ???? ?? ????
???? ??????????? ???????? ?? ???? ????????
????????? ????? ?? ? ???????????? ????? ??? ?? ??
??? ? ?? ???????????? ?? ? ?????? ??? ? ?? ? ??????? ??
???? ?????? ????????????? ??????? ??? ??????? ?? ????
??? ?? ??? ??? ?? ????? ?????? ? ?? ?? ?? ???? ????? ?
?? ????? ? ?? ?? ?? ?? ???????? ?? ???
?? ?? ????? ???????????? ???? ? ? ???? ????? ?
???????? ????? ?? ???? ???????? ? ? ? ???????? ? ?
?? ? ???? ? ? ??? ??? ?? ?? ? ? ???? ? ? ?? ??
???? ? ?? ? ?? ? ??????? ?? ???? ?????? ?? ?? ?? ??
????? ?? ? ?? ?? ? ?? ? ?? ???? ?????? ?? ?? ? ???
????????? ????? ???? ???????????? ??????????
?????? ??? ?? ?? ??? ? ?? ?? ? ??? ???? ??? ?????
?? ??? ???????? ? ? ????????? ?
? ? ??? ?? ?? ?? ???????? ?? ?????????
?? ???? ????????????? ????? ? ??? ?? ??? ??? ???
?? ? ???? ???? ??? ?? ?? ? ??? ???? ???? ??? ?? ?
??????? ?? ???? ?????? ?? ???? ?? ?? ? ????? ?? ?? ??
???? ? ?? ? ?? ?? ???? ? ??? ???????????? ????? ????
???? ???????? ??? ????? ?????? ?????? ????? ????
?????????? ?? ??????? ? ?????? ? ?????? ??? ?? ?? ?
?? ??? ?? ?????? ? ??? ???? ?? ? ??????? ??? ?? ?
?? ? ?? ?? ? ? ???? ? ? ?? ??? ? ? ?????? ? ??????
??? ? ???? ?? ???? ?????? ?? ? ?????? ??
?? ???? ????? ????????? ?? ????? ??????????? ?
????? ??? ? ?? ???? ? ? ?? ? ? ?? ?? ???? ????? ?
? ? ???? ??????????? ? ? ?????? ??? ?? ??????
?? ?????? ? ??? ??? ?? ?? ???? ?????? ?? ?? ???
??????????? ? ???????? ? ? ???? ?????????? ???
??? ?? ??? ?? ? ??? ?????????? ? ?????? ? ???? ???
?? ? ??? ???? ? ?? ?? ?? ??? ??????? ?? ???? ???
?? ? ?? ?? ???? ?? ?? ???? ? ?? ??? ???? ????? ???
?? ??????? ?? ? ?? ? ??? ? ? ????? ?????? ? ???? ? ?
???? ?????? ??? ? ??? ?? ?? ?? ????? ??
???? ??? ???? ?? ? ????????? ????????? ????
????????? ?? ??? ?? ???? ??? ?? ????? ???? ????? ?
?? ?? ???? ?????? ?? ?? ??? ? ???? ? ? ???? ? ?? ???
?? ? ?? ?????? ?? ?? ?? ???? ? ?? ?? ???? ?? ?
? ??? ??? ????? ? ? ? ?? ???? ? ??? ?????????? ?? ?? ?? ?
?? ????? ? ?? ? ?? ???? ?
????? ?? ???? ?? ? ?????? ????? ??? ? ??? ?? ??? ?
82
???????? ????????? ???? ????????? ?? ??????????
???? ?????? ??
?? ????? ???
??????? ?????? ? ???
?????????????? ? ???
??? ?? ??? ? ?? ?? ?? ? ?????? ????? ?? ??? ?? ???
?? ? ?? ? ? ????? ?? ?? ?? ? ?? ? ? ? ??? ?? ?? ?
?????? ????????? ???????????? ?????????????
???????? ?? ?? ? ??????? ?? ?? ? ?? ?? ?? ??? ??? ?? ?
?? ?? ?? ? ? ????? ? ? ?? ????? ? ???? ?? ??????
?? ?? ??? ?? ??? ? ???? ? ? ? ?? ????? ? ???? ?? ?
??? ? ?? ? ????? ? ?? ?? ? ? ?? ?? ?? ? ? ???????
? ??????? ? ?? ?? ??? ?? ? ????? ?? ?? ?? ?? ?? ?? ?? ?
? ????? ? ? ?? ????? ??????????? ?? ? ?? ?? ?? ??? ??
? ? ???????????? ?? ?????? ? ?? ?? ? ??????? ??
?? ??? ?? ? ??? ????? ? ? ?? ?????? ????????????
???? ?????? ???????????? ??????????? ??? ????
??? ?? ??? ????? ????????? ? ?? ?? ?? ? ? ?? ?? ??
? ????????? ??????? ??? ????? ??????? ? ?
??? ??? ?????? ??? ??? ? ? ? ? ?????? ?? ???
????? ? ?????? ?? ?? ? ? ?? ????? ? ??????? ?? ?? ?
??????? ??? ?? ??? ?? ?? ? ????? ??? ??????? ? ? ?
?????? ????????? ?????? ???????????? ????? ???
??? ?? ??? ??? ? ?? ???? ???????? ?? ? ??? ? ? ??
????? ????????????? ??????? ??????? ? ???? ?????? ?
? ? ?? ?? ? ?????? ???? ????? ??????????? ? ?? ??
?? ????? ?????? ? ????? ?? ???????? ???
?? ? ?? ?? ?? ? ? ?? ?? ?? ????? ?? ??? ? ?? ? ?
????????? ???? ????????? ????????? ????? ???
? ?? ? ?????? ??????? ?? ?? ??????? ????????? ????
?? ???? ???? ??????? ????? ??????? ??? ? ?? ????
????? ? ??? ???? ?? ?? ??? ? ?? ? ???? ?? ???? ?
???? ??? ???? ?????????? ??? ????????????????
?? ????? ? ?? ????? ??? ??? ????? ????????? ?? ?
?? ??? ? ??? ?? ?? ? ?? ??? ?????? ? ? ??????
?? ??? ???? ? ??? ?? ?? ?? ? ? ???? ??? ? ????? ?? ?
?????? ??????? ? ??? ??????? ? ? ?????? ?? ????
?? ???? ? ??????? ?? ? ??? ? ? ? ?? ? ??? ? ??????
??? ??? ??? ???????? ? ?????? ???
??? ??? ??? ? ?? ? ?? ??? ???? ? ? ????? ?? ?
? ?????? ?? ?? ?? ?? ?? ???? ?????? ??? ?? ?? ????
? ? ?? ??? ???????? ?? ?? ? ??????? ??? ?? ? ??? ???
? ???? ?? ????? ?? ?? ??? ?????????? ??? ?? ?? ?? ?
???? ?? ??????????? ? ?????????? ????????????
? ?????? ? ??? ? ?? ? ??????? ?? ???? ?????? ? ??
??? ?? ??? ?? ?? ?? ? ?????????? ??? ?? ? ? ?? ? ????
??? ? ??? ?? ?? ?? ? ???? ?? ? ?? ??? ?? ?? ???
?? ?? ???? ? ??? ?? ????? ? ? ? ????? ? ?? ?? ?? ?
?? ? ??? ? ??? ?? ?? ??????? ??? ? ?????? ???????
? ???? ? ? ???? ?????? ?? ??? ?? ?? ?? ? ?? ? ????? ? ?
?????? ???????? ? ? ???? ????????? ???? ????
??? ???? ???? ? ? ??? ????????? ???? ?? ??? ???? ?
?????? ????????? ?? ???? ??????? ??? ??????????
??????????? ????????????? ???????? ?????? ???
?????????? ???? ?????????? ????????????? ?????
? ?? ? ? ? ??? ?? ???? ?? ? ?? ????? ??? ????? ? ? ?
?? ? ? ????? ?????????? ?? ?? ????? ????? ???? ? ?
?? ??? ?? ? ??? ?? ? ? ???? ?????? ? ????? ???????
??????? ?? ?? ? ??????? ?? ??? ????? ? ?? ??? ?? ? ? ?
? ? ? ?? ?? ?? ? ? ?? ???? ?? ???? ? ?? ????? ???
? ?? ??????? ? ??
??? ??? ??? ? ?? ? ????? ????? ? ? ?? ?? ??
???????????? ? ???????????? ???? ??????????
????? ?????? ? ???? ? ? ???? ?????? ?? ?? ?? ?????
? ????? ?? ?? ?? ??? ?? ?? ? ?? ? ????? ???? ?? ?
?????????? ????? ???????????? ????? ????? ?????
????? ? ?? ? ?? ? ?? ? ?????????? ??? ?? ? ????? ??
? ?? ?? ???? ?????? ??? ?? ? ??????? ?? ?? ? ?????
???? ??????? ??? ???? ??? ??????? ??????????
???? ?????? ?? ?? ? ?? ? ?? ? ? ???? ??? ??? ??? ?? ???
? ???? ??? ? ???? ??? ????? ??? ?? ? ?? ? ?? ? ? ?? ?
??? ? ??? ?? ???????? ?? ?? ? ??????? ?? ??? ?? ???
??? ????
???? ????? ??? ?? ??????? ? ??? ? ?? ??? ???
???? ??? ???? ??? ???? ??????? ???? ????????
????? ???? ?????? ?????? ??? ??? ? ?? ? ????? ?
?? ?? ????? ????? ?? ???? ?????????
? ? ????????? ?????????? ??? ???????? ???
??? ?? ?? ? ??????
??? ???? ????? ??? ???????? ? ???? ???? ?? ?? ? ?
??? ? ?? ?? ????? ?? ?? ? ? ?? ? ? ?? ??? ??
??????? ?? ???? ?????? ? ???? ? ? ? ??? ? ??? ??? ??
??????? ???? ??????????? ???? ?????????? ????
?? ?? ??? ???? ?? ? ?? ??????? ????????? ?? ?? ?
??? ??
?? ??????? ???? ????? ??? ??? ??? ??? ??
???? ?????????? ???? ???????? ?? ???????????
?? ? ??? ??????? ??? ? ??? ??? ?? ?? ?? ? ?????????
?? ? ?? ? ?? ? ? ? ???? ? ? ???? ?????? ? ??? ?? ???????
? ??? ??? ??? ?? ? ?????? ??? ???? ?????? ????? ?????
?? ? ?? ? ??? ??? ?? ?????? ? ?? ?? ? ??????? ??? ?? ?
???? ?????????????? ?????? ?????????? ????????????? ???
?? ????????? ????
?? ? ? ? ?? ? ?????????? ? ? ? ?? ????????? ??? ?? ??
?? ? ?? ? ? ? ??? ?????? ?? ? ???? ? ???? ?? ?
? ???? ?? ??? ? ?? ?? ????? ?? ??? ????? ??? ??? ??
????? ??? ??????????? ???????? ??????????
??????????? ??? ??? ? ???? ?? ? ??????? ??
??? ??? ?? ??? ???? ?? ? ??????? ?? ?????? ??
???????????? ?? ???? ?? ? ????? ?? ? ? ??? ? ?? ?? ?
????? ? ? ?????? ????
???????? ???????? ??? ????? ???? ??????? ???
???????? ?????????? ?? ?? ?????????? ??????
??? ? ? ?????? ? ?? ???? ? ????? ??????????? ?????
?? ???? ?? ?? ? ? ?? ???? ???? ? ??? ?? ? ???? ??
??? ?? ? ??? ??? ?? ? ?? ?????? ?? ??? ??? ? ????? ?
83
?????? ??? ???????? ??????? ????????? ????
?????????? ? ??? ?????????? ???? ?????????
??? ????? ????? ? ?? ? ? ???? ? ??? ??? ??? ???
??? ? ? ?? ??? ?? ?????? ??? ?????? ???? ??? ??
??? ???? ? ? ???
?? ?? ? ? ?? ?? ???????? ???????? ?? ?? ??? ?
?? ? ?????? ???? ??? ?? ?? ?? ?? ?? ??? ??? ??
??? ???? ????? ??? ? ?? ?? ????? ? ?? ? ?? ?
?? ? ?? ????? ??? ???????????? ?? ? ??????? ??? ??
?????????? ??????? ? ??????????? ? ? ???????
????????????? ??? ??? ???????? ?? ????????
??? ????? ?? ?? ? ?? ? ? ????? ??? ?? ? ?????? ?
?? ?????? ? ?????? ??? ? ??? ? ? ???? ??? ??? ???? ??
?? ?????? ??? ? ?? ?? ? ? ?? ? ? ? ?? ??? ? ??? ??
??? ? ? ???????????????? ? ?? ? ? ? ????????? ? ????
?? ?? ????? ??? ? ? ?? ?? ?? ??? ????? ????? ?? ????
?? ?? ? ????? ?? ?? ? ? ? ?? ? ? ?? ?? ? ?? ??? ??
????? ???? ??? ?? ?? ??? ??? ???? ? ???? ? ?? ? ? ?? ?
??? ? ? ? ?? ?? ? ?? ?? ???? ?? ???? ? ?? ?? ???
????? ???????? ?????????? ??? ???? ???
? ?? ?????? ?? ?? ??????? ? ?
?? ?? ?? ? ? ?????? ? ? ? ? ???? ?? ?? ? ??? ? ?? ??
?? ? ??????????? ??? ?????????????? ??????
? ?????? ???? ? ? ??? ? ?? ?? ?? ?????? ?? ????
??? ??????? ? ? ? ?? ? ?? ???? ?? ?? ???? ?? ??
?? ??? ? ?? ?? ?? ??? ??? ????? ??? ?? ?? ??? ? ?
?? ? ???? ?????? ??? ? ??? ?? ? ? ??? ??? ? ?? ? ?
?? ?? ???? ? ??? ??? ?? ??? ???? ??? ??? ??? ? ? ?
??? ? ?? ? ??? ??? ? ? ? ? ? ? ?? ?? ?? ?????? ??
??????? ????
?? ?? ???? ?? ? ??? ????? ???? ????? ? ???
?? ? ???????? ???? ???? ???? ??? ???? ??? ??? ?
?? ?????????? ?????? ??????????? ????????????
???? ? ?? ?? ?? ???????? ?????????????????? ? ??? ?
?? ? ??? ?? ??? ? ? ? ?? ??? ?? ?? ?? ??? ? ?? ?? ? ?
? ? ?????? ?? ?????? ?? ?????? ??? ???????????? ?
?? ???? ?? ??? ?? ?? ???? ? ?? ????? ?? ???? ? ??
??? ?????? ??? ??? ? ?? ??? ?? ??? ?? ?? ??? ? ?? ?? ?
????? ??? ???????? ? ? ????? ?? ? ???? ?????? ? ? ?
??? ??? ??? ? ??
???? ???????? ??? ??????????? ??? ???? ???
?? ?? ???? ??????????? ??? ? ? ? ?? ??? ??????? ?
?? ? ? ?? ?? ? ?? ? ?? ??? ? ??? ? ? ????? ????? ??
???? ? ?? ? ?? ?? ?? ??? ?? ?? ? ?????????? ? ? ?? ?
??????? ? ? ???? ?? ????? ?? ? ?? ?? ????? ? ? ?? ?
??? ????? ??? ???? ????? ?? ? ?? ????? ????
?? ???? ??? ?? ? ?? ?? ???? ? ? ???? ??? ??? ? ?
? ????????? ? ?????? ? ??????????? ???? ?????
? ??? ???? ?? ? ??? ??? ? ? ???? ?? ?? ?? ??????????
??? ?? ??? ????? ??? ? ???? ? ?
????? ???? ????
?? ??? ? ??? ? ?? ?? ?? ?? ???? ? ???? ???? ?? ? ?? ?
???? ?????????? ??? ???? ??? ???? ???????
?? ??? ????? ?? ???????? ??? ??? ? ?? ???? ? ?? ?
?? ??????
??? ??????
????????? ??????????? ????? ???? ?? ??? ? ??? ???
? ? ?? ??? ???? ?? ?? ?? ?? ? ???? ?? ?? ? ? ???
??? ??? ?? ? ? ? ?? ? ???? ??? ???? ?? ?? ???? ? ?
????? ???? ??? ?? ?? ???? ??????? ?????? ?? ?
?? ? ??
??? ???????? ??? ??????? ?? ???? ??? ? ?? ?????? ? ?
?????????? ???? ?????????????? ?????????????
?? ?? ????? ?? ??? ??????? ? ???? ???
????? ??? ??? ???????? ???? ??? ?? ??? ? ????????
?? ?? ? ? ??? ??????? ?? ? ??? ??? ?? ??? ?????
??? ? ?? ?? ???? ??? ???? ?? ?? ??? ?? ? ? ???
??? ??? ?? ? ? ? ?? ? ?? ? ?? ?? ?? ??? ??? ?? ?
?? ??? ??? ?? ?? ???? ????? ??? ?? ? ??? ??? ? ?
?????????????? ????? ??????? ??????? ??? ????
???? ?????? ?? ? ?? ?? ????? ?? ???? ?????? ???????
??? ???? ?? ??? ? ??? ??
?????? ?? ??? ?? ????? ? ?? ?????? ??????? ??? ?? ? ???
?????? ??????? ??? ??? ??? ?? ??????? ??? ? ?? ?
??? ?? ? ?? ? ?? ??? ? ??? ??? ?????? ? ????? ? ???
? ??? ??????? ??????? ??? ???? ??????????? ????
?????????????? ???????????? ?????? ??????????
?????? ? ???? ??
????????? ???? ??? ?? ??? ???? ?? ??? ? ????? ?? ?
????? ?? ?? ?? ? ?????? ? ? ??????? ?? ? ? ?????? ?
?? ? ? ? ???? ?? ?? ? ????? ???? ???? ?? ??????
??? ?????????? ????????????? ??
?? ??? ???????? ?? ??? ?????? ???? ? ? ??? ?? ??? ?
?? ?? ??? ? ?????? ?? ??? ?? ??? ?? ? ???????? ?
?? ? ? ? ???? ?? ??? ????? ?? ? ?? ??? ??? ?? ? ? ?
?? ? ??????? ???? ??? ??? ??? ?? ?? ?? ? ???? ??? ?
??????????? ???? ?????????????? ????????????
??????? ???? ?? ??????? ??
????? ? ????? ?? ??? ??? ?? ?? ? ????? ?? ? ? ? ? ?
????????????? ? ?? ? ??????? ????? ??????? ?
???? ?? ?? ? ? ? ????? ??????? ??????? ??? ????
???? ?????? ?? ??? ?? ????? ?????? ?????????????
??? ???? ?? ??? ? ???? ??
????? ? ????? ?? ??? ?????? ??? ??? ? ??? ? ?? ?
? ? ?????? ??? ? ? ????? ?? ??? ? ??? ?? ? ? ? ? ?
???? ? ?? ?? ? ?? ???????????? ??? ???? ??????
??? ???? ?? ?? ??? ?????? ????? ? ?? ???? ???
???? ? ? ??? ?????? ??? ?? ?? ????? ?? ???? ???
???? ? ? ????? ?? ????????? ???? ????? ??
?????? ??? ???? ???
???? ???? ?? ?? ????? ??? ??? ?? ? ?????? ?? ?? ??
? ? ? ? ? ???? ?????? ?????????????? ???????? ?
? ? ?????? ? ?? ? ? ? ???? ?? ?? ? ?? ? ??? ??? ??
84
??????? ? ? ?? ? ???? ?????? ?? ? ?? ?? ????? ??
???? ?????? ????? ?? ????? ??? ????? ??
???? ?? ??? ?? ? ? ??? ? ?? ????? ??? ??? ???? ?? ?
??????? ????????? ? ????? ???? ???????????
?? ??????? ? ??? ??? ???? ? ?????? ? ?? ? ?? ?? ???
??????????? ????? ?????????????? ???????? ???
?? ?? ???? ?? ? ??? ???????? ?? ???? ?? ??????
???????? ?? ? ??? ???
????? ? ???????? ??? ????? ?????????? ? ??????
?? ??? ???? ?? ?? ? ?? ??? ? ???? ? ?? ??? ?? ?? ?
???? ?????? ?????????????? ???????? ????????
???? ? ? ?? ? ?????? ? ???? ??
???? ??? ? ?? ? ? ??? ?????? ???? ??? ??? ?????? ?
?? ???? ? ?? ? ?? ?????
?????? ??? ?????? ???? ??? ??? ?? ???? ? ????? ???
??? ?? ???? ?? ???????? ?? ???? ? ? ??? ?????? ?
?????????? ???? ? ?? ?? ?? ?? ? ???? ?? ?? ? ?? ?
???? ?? ? ? ? ?????? ??? ?? ???????????? ??? ????
?? ? ?? ?? ?? ?????? ?? ???? ????? ????? ?????
? ??? ?? ??? ???? ?? ??? ???? ?? ?? ?? ?? ????? ??
???????????? ????????? ??????? ?????????????
? ???
?????? ? ???????????? ? ????? ????????????? ???
?? ? ??????? ??? ? ??????? ??? ??? ??????? ????
??? ??? ???? ? ??? ?? ? ??? ?????
?????? ??????? ??? ???????? ??? ?? ???? ?? ??? ?? ???
?? ??? ??
???? ???? ??????? ?? ??? ?????? ??? ??????
???????????? ????? ??? ????????????? ????????
??? ??? ?????? ? ?? ? ?? ?? ??? ??? ?? ? ? ? ?? ? ????
?? ??? ???? ?? ?? ? ?? ?? ????? ??? ?? ?? ???? ?
?????? ???? ?? ???? ?? ????? ??? ???? ???
?? ???? ? ????? ?? ?? ????? ? ? ???? ? ?? ? ? ?
? ? ?????? ??? ? ? ? ???? ?? ?? ? ?? ??? ??? ?? ? ? ?
??? ? ???? ?? ?? ?? ?? ??????? ??
???? ??? ?? ??? ??? ? ? ?? ????? ??? ? ????????
?? ?? ? ? ???????????? ??????? ?? ??? ???? ??? ?
??????? ? ? ?? ???? ??? ???? ?? ?? ??? ?? ?? ???
??? ??? ?? ? ? ? ?? ? ?? ? ?? ? ?? ?? ????????????
???????? ???????????? ????? ??? ? ?? ? ? ?? ????
???? ?????? ?? ? ?? ?? ????? ?? ???? ?????? ???????
????? ?? ????? ??? ????? ????
??? ? ? ?? ??????? ???? ?? ???? ???? ?? ?????? ?
????? ? ? ????? ???? ???? ???? ??? ? ?????? ??? ?
???????????? ?? ????????? ?? ???????? ??????? ?
?? ?? ? ? ?? ??? ???? ?? ??? ? ?? ? ? ??? ??? ??? ?
? ? ???? ???
??? ? ? ?? ??????? ??? ??????? ?? ?? ????? ??? ?
??????????? ??????????? ??? ????????? ????????????
???? ? ? ?? ?? ??? ? ?? ??????? ? ?? ???
???? ?? ? ?????????? ?? ??? ? ? ??? ? ?? ??? ??? ?
???? ??? ? ??? ???? ??? ?? ? ???? ?? ?? ? ? ?
???? ?? ??? ??? ?? ?? ? ?? ??? ??? ?? ? ? ? ?? ? ?? ?
?? ? ?? ?? ???????????? ???? ? ? ???? ?????? ? ? ???
??????? ? ? ?? ? ???? ?????? ?? ? ?? ?? ????? ??
???????????? ?????? ??????? ????? ??????????
?? ?????
????? ????? ? ? ? ?????????? ? ??? ???? ??? ????
???? ??? ??? ?????? ?? ????? ?? ? ?? ? ?????
?? ?? ???? ? ????? ? ????? ?????? ?? ? ?????? ??
??????? ? ? ?? ? ???? ?????? ?? ? ?? ?? ????? ??
???? ??????? ? ??? ??? ??? ???? ? ??
?? ??? ???? ?? ??? ? ??? ? ???? ???? ? ???? ?? ? ?
????? ??????? ? ????? ?? ? ??? ???? ? ??? ? ?? ??
?? ?????? ?? ???? ? ????? ?? ?? ??? ?? ?? ?? ???
???????? ?????? ?????????????? ???????????? ????
?? ????? ? ? ??? ? ??? ?????? ? ??? ??? ?? ? ? ? ?? ? ????
?? ??? ???? ?? ??? ???? ?? ??????? ???? ? ?? ????
?? ??
??? ? ? ? ????? ? ?? ? ???? ? ??????? ? ??????
??????? ?? ?? ? ?????? ?? ??????? ?? ?? ? ? ??? ?
???? ????????? ? ???????? ???? ???? ??????
???? ? ?? ?? ? ???? ??? ???? ? ? ??? ? ???? ?
? ? ?? ?? ??
????? ? ??? ??? ??? ? ????? ??????? ?? ??? ??? ????
??? ?? ????? ???????? ????? ? ?? ??? ??? ?? ? ? ?
?? ? ?? ??? ???? ?? ??? ???? ?? ?? ???? ????? ? ???
???? ? ? ?? ?????? ?? ? ??????? ??
??? ? ???? ??? ??? ? ???????? ????? ??? ????? ?? ?
???? ??? ???? ????? ?? ? ? ? ?????? ? ?? ?? ?
?? ? ?? ? ? ? ???? ?? ?? ? ?? ??? ??? ?? ? ? ? ?? ?
? ? ?? ?? ??? ?? ? ????? ? ? ???? ? ?????????
?????????? ?? ?????????? ???????? ?? ????????
???? ? ? ??? ?????? ???????????? ??? ?? ???
??? ? ??? ??
85
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1279?1290, Dublin, Ireland, August 23-29 2014.
Global Methods for Cross-lingual Semantic Role and Predicate Labelling
Lonneke van der Plas
Institute for NLP
Pfaffenwaldring 5B
70569 Stuttgart, Germany
vdplasme@ims.uni-stuttgart.de
Marianna Apidianaki
LIMSI-CNRS
Rue John von Neumann
91405 Orsay Cedex, France
marianna@limsi.fr
Chenhua Chen
Institute for NLP
Pfaffenwaldring 5B
70569 Stuttgart, Germany
cch.chenhua@googlemail.com
Abstract
We address the problem of transferring semantic annotations to new languages using parallel
corpora. Previous work has transferred these annotations on a token-to-token basis, an approach
that is sensitive to alignment errors and translation shifts. We present a global approach to transfer
that aggregates information across the whole parallel corpus and leads to more robust labellers.
We build two global models, one for predicate labelling and one for role labelling, each tailored
to the task at hand. We show that the combination of direct and global methods outperforms
previous results.
1 Introduction
With the proliferation of the Internet in non-English speaking countries, the need for multilingual
processing becomes more and more pressing. Various efforts have focused on developing language-
independent NLP tools and extending to other languages tools that had been exclusive to English. Fur-
thermore, several annotation efforts have been devoted to developing resources for different languages,
needed for supervised learning (Haji?c et al., 2009). However, there is still a large number of languages
for which corpora with semantic annotations do not exist. Since manual annotation is a costly and time-
consuming approach to resource development, cross-lingual annotation transfer offers an alternative.
Semantic parsing or semantic role labelling (SRL) is the task of automatically labelling predicates
and arguments with predicate-argument structure. This level of analysis provides a more stable semantic
representation across syntactically different sentences. The example sentences (1a) and (1b) illustrate
how the semantic annotation remains stable across the locative alternation of the verb load.
(1) a. [AGENT Jessica] [REL-LOAD.01 loaded] [THEME boxes] [DESTINATION into the wagon].
b. [AGENT Jessica] [REL-LOAD.01 loaded] [DESTINATION the wagon] [THEME with boxes].
Also in the cross-lingual setting, the predicate-argument structure of a sentence is considered to be
more stable than its syntactic form as the English sentence in (2a) and its French translation in (2b)
show:
(2) a. [EXPERIENCER Mary] [REL-LIKE.01 liked] [CONTENT the idea]. (English)
b. [CONTENT L?id?ee] a [REL-LIKE.01 plu] [EXPERIENCER `a Marie]. (French)
This is why several pieces of work have transferred semantic annotations from a source language, for
which semantic annotations exist, to a target language using parallel corpora (Pad?o, 2007; Basili et al.,
2009; Annesi and Basili, 2010). These transfer methods rely on the assumption of semantic equivalence
of the original and the translated sentences, but also on correct and complete alignments between words
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1279
or constituents in those sentences. We will refer to these traditional methods as direct transfer because
the semantic annotations are transferred directly from token to token. Although direct transfer methods
are straightforward and easy to implement, they are vulnerable to missing or incorrect alignments which
lead to missing and erroneous annotations in the target language. Consequently, non-literal translations
and translation shifts present major problems for these methods.
In this paper we propose a global approach to the cross-lingual transfer of PropBank (Palmer et al.,
2005) semantic annotations that aggregates information at the corpus level and, as a consequence, is
more robust to non-literal translations and alignment errors. Our global approach involves two steps:
in the learning step, two global models are learned on the basis of role and predicate annotations in the
source language (English). In the labelling step, these models assign labels to verbs and their arguments
in the target language (French) without consulting any parallel data. Contrary to previous work, we
build separate models for the transfer of semantic role and predicate annotation because predictors for
the two models are different in nature. We model cross-lingual transfer of predicate labels as a cross-
lingual word sense disambiguation (WSD) task because this fits well with the lexical nature of the task:
annotating French verbs with English predicate labels. Our approach to predicate labelling needs word
alignments but instead of relying on local (token-to-token) correspondences like the direct method, it
exploits alignment information gathered from the whole corpus thus avoiding transfer errors caused by
local misalignments. Our model for cross-lingual semantic role labelling
1
is based on syntactic-semantic
mappings learned from a gold annotated monolingual corpus. The SRL method does not need aligned
data. Our methods are knowledge-lean as our predicate labelling method only needs a part of speech
(PoS) tagger in the two languages and no syntactic information on either side, in contrast to previous
work. For SRL, a syntactic parser for the target language is needed, but no joint semantic-syntactic
parsing framework as was the case in previous work (van der Plas et al., 2011). The requirements of the
global annotation transfer methods in terms of data and annotations, and their differences from direct
transfer, are illustrated in Figure 1.
Our contribution is three-fold. First, we present a global approach to semantic annotation transfer that
corrects token-level mistakes found in traditional direct transfer methods. We show the strengths and
limitations of global vs. direct transfer and explain how the two can be combined. Second, in contrast to
previous work, we address the two tasks of cross-lingual predicate labelling and cross-lingual semantic
role labelling by building two separate models tailored to the task at hand. We show how the predicate
labels produced by our high-coverage and knowledge-lean model for cross-lingual predicate labelling
are successfully used as predictors for semantic role labelling. Third, due to its knowledge-lean and
flexible character, our method adapts relatively easily to other language pairs without requiring semantic
lexicons in the target language.
In the next section, we present related work on cross-lingual annotation transfer. In Section 3 we
present the data used in our experiments and in Section 4 we briefly discuss direct transfer. The two
global methods proposed in this paper are presented in Section 5. We report and discuss our results in
Section 6, before concluding.
2 Related work
Transferring annotation from one language to another in order to train monolingual tools for new lan-
guages was first introduced by Yarowsky and Ngai (2001). In their approach, token-level part-of-speech
(PoS) and noun phrase bracketing information was projected across word-aligned bitext and this partial
annotation served to estimate the parameters of a model that generalized from the noisy projection in a
robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting
syntactic information across languages. They create type-level tag dictionaries by aggregating over pro-
jected token-level information extracted from bi-text and use label propagation on a similarity graph to
smooth and expand the label distributions. A different approach to cross-lingual PoS tagging is proposed
1
Most unsupervised approaches consider argument identification as a separate task that is omitted (Lang and Lapata, 2010)
or performed heuristically (Lang and Lapata, 2011). We focus on semantic role labelling in this paper and consider argument
identification as given.
1280
semantic annotations(predicates + roles) PoS tags
semantic annotations(predicates + roles)
semantic annotations(predicates)  
PoS tags semantic annotations(predicates) 
modelfor predicate labelling
semantic annotations(roles)syntactic annotations
syntactic annotations
semantic annotations(roles) 
modelfor role labelling
modelfor role labelling
syntactic annotations
FR FR
FR FR
FR
FRFREN EN
EN
EN
EN
FR
 learning 
 learning  labelling 
 labelling 
modeltransferhand-built cross-lingual syntactic mappings
OR
annotationtransfer
direct transfer                   global predicate labelling                global semantic role labelling
Parallel corpus
Parallel corpus
Parallel corpus
EN: EnglishFR: French
Figure 1: Direct vs. global cross-lingual transfer of semantic annotations.
by T?ackstr?om et al. (2013) who couple token and type constraints to guide learning. Our approach to
cross-lingual semantic role labelling follows this vein. Instead of solely relying on token-level informa-
tion acquired from word-alignments, we combine this with type-level information captured by our global
methods which are trained on the entire corpus. We however are concerned with semantic annotations
and not PoS.
Transfer of semantic annotation has started off with direct transfer of FrameNet semantic annotations
(Pad?o, 2007; Basili et al., 2009; Annesi and Basili, 2010). With the addition of a learning step and the
use of PropBank data, Van der Plas et al. (2011) have scaled up previous efforts. They show that a joint
semantic-syntactic parser trained on the output of direct transfer produces better parses than the input it
received by aggregating information across multiple examples. In their work, transfer of predicate labels
and semantic role labels is done in one step. The model needs an aggressive filter to compensate for
missing annotations on the predicate level after direct transfer. This filter successively leads to drops in
performance for the role labellings. Here, we build two separate global models that complement direct
transfer instead of relying on it.
The same emphasis on learning is found in cross-lingual model transfer where source language models
are adapted to work on the target language directly. For semantic role labelling, Kozhevnikov and Titov
(2013) use shared feature representations (syntactic and lexical) to adapt a source model to a target-
language model. The ideas behind their cross-lingual model adaptation resemble the ideas behind our
global method for semantic role labelling. However, in contrast to their work we do not consider the
predicate labelling as given because, as manual annotations show (van der Plas et al., 2010), this task is
not trivial. We first build a tailored global model for cross-lingual predicate labelling and then use the
predicted predicate labels for semantic role labelling.
3 Data
In our experiments, we use the English-French part of the Europarl corpus (Koehn, 2005). The dataset
is tokenised and lowercased and only sentence pairs corresponding to a one-to-one sentence alignment
with lengths ranging from one to 40 tokens on both French and English sides are considered. Further-
more, because translation shifts are known to pose problems for the automatic projection of semantic
roles across languages (Pad?o, 2007), we select only those parallel sentences in Europarl that are direct
1281
translations from English to French or vice versa. In the end, we have a parallel corpus of 276-thousand
sentence pairs.
The English part of the parallel corpus is annotated by a freely-available syntactic-semantic parser
(Henderson et al., 2008; Titov et al., 2009) trained on the CoNLL 2009 training set (the Penn Treebank
corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005) and NomBank labels
(Meyers, 2007)). The probabilistic model is a joint generative model of syntactic and semantic depen-
dencies that maximises the joint probability of the dependencies while building two separate structures.
The WSD classifier used for predicate labelling is trained on the parallel training corpus tagged with
semantic roles on the English side. The candidate predicate labels that are considered by the classifier
for each French verb are the labels of its English translations in the training corpus. Verbs on the English
side are replaced by the corresponding predicate label where available. Then both parts of the corpus are
lemmatized and tagged by part of speech (Schmid, 1994) and the parallel files are rebuilt (one sentence
per line) by replacing words on both sides by the corresponding ?lemma PoS tag? pair. The corpus is
then word aligned in both directions using GIZA++ (Och and Ney, 2003) and a lexicon is built from
intersecting alignments. Lexicon entries for French verbs contain the English predicate labels to which
they were aligned in the training corpus. The entry for the verb encourager, for instance, contains seven
predicate labels: {urge.01, foster.01, stimulate.01, promote.02, encourage.01, encourage.02, renew.01},
two of which correspond to the same English verb (encourage). We keep labels with an alignment
confidence score above 0.01 according to GIZA++.
Contrary to our predicate labelling model, the role labelling model needs syntactic information in
the target language. For parsing French, we use the dependency parser described in Titov and Hender-
son (2007). We train the parser on the dependency version of the French Paris 7 treebank (Candito et al.,
2009), achieving 87.2% labelled accuracy on this data set. The French Treebank (Abeill?e et al., 2003) is a
treebank of 21,564 sentences annotated with constituency annotation. We use the automatic dependency
conversion of the French Treebank into dependency format (Candito et al., 2009) to train the French
syntactic parser that is used to annotate the French part of the parallel corpus.
For testing, we use the hand-annotated data described in Van der Plas et al. (2010). We randomly
split those 1000 sentences into test and development set containing 500 sentences each. We use the
development set for the current experiments, which contains 1,917 core roles in total. We limit our
experiments to verbal predicates because the semantic annotations on French test sentences are limited
to verbal predicates.
4 Direct cross-lingual transfer
Before explaining the global methods, we present the direct semantic transfer (DST) method proposed
by Van der Plas et al. (2011) that we use for comparisons and combinations throughout this paper. The
method is based on the Direct Correspondence Assumption for syntactic dependency trees proposed by
Hwa et al. (2005). The transfer proceeds as follows: For any pair of sentences E and F that are translations
of each other in the parallel corpus, we transfer the semantic relationship R(x
E
, y
E
) to R(x
F
, y
F
) if and
only if there exists a word-alignment between x
E
and x
F
and between y
E
and y
F
, and we transfer the
semantic property P (x
E
) to P (x
F
) if and only if there exists a word-alignment between x
E
and x
F
.
The relationships that are transferred are semantic role dependencies and the properties are predicate
senses. These are transferred from the English part of the parallel training corpus that is automatically
annotated with syntactic-semantic analyses, as explained in the previous section.
5 Global cross-lingual transfer of semantic annotations
In contrast to direct transfer where annotations are transferred on a token-to-token basis in word-aligned
sentences, we propose two global methods for cross-lingual transfer, one for predicates and one for
semantic roles, that both consist of a learning and a labelling step. Our methods are globally defined
and as a consequence rely less on local translation correspondences than previous methods, which makes
them less vulnerable to missing and incorrect alignment links.
1282
5.1 Global cross-lingual predicate labelling
In cross-lingual predicate labelling, our aim is to put predicate labels that originate from the English
side of the parallel corpus on the French verbs in the other side of the corpus. The predicate labels
contain the English verb and its sense. For example, ?give.01? stands for the first sense of the verb
give. As the predicate label contains a lot of lexical information, putting the correct English predicate
label on a French verb is very close to Word Sense Disambiguation (WSD), the task of automatically
identifying the meaning of words in context (Navigli, 2009). In the cross-lingual variant of this task, the
candidate senses are the words? translations in other languages and WSD aims at predicting semantically
correct translations for words in context (Resnik and Yarowsky, 2000; Ng et al., 2003; Carpuat and
Wu, 2007; Apidianaki, 2009). The main difference between cross-lingual WSD and our cross-lingual
transfer of predicate labels is that we do not search for correct translations of French words but for the
most appropriate predicate labels in context (i.e. verbs disambiguated with a predicate sense).
The global predicate labelling method consists of a learning step and a labelling step. During learning,
we compute estimates for annotation transfer on the basis of the word alignments between English and
French predicates over the entire parallel training corpus. At labelling time, we label French verbs
with English predicate labels without the need for parallel data or alignments. The method is language-
independent and only requires minimal linguistic resources (PoS information).
In terms of coverage, a predicate label is provided for all French verbs in the test set for which in-
formation was retained during training and not only for aligned ones, in contrast to direct transfer. We
expect to augment the recall when using global estimates and hope that the effect on precision is not too
negative.
Learning
For each French verb (v) in the lexicon built as described in Section 3, we want to be able to identify its
correct predicate label in a new context by choosing one among its candidate labels (L) retained from
the training corpus. A feature vector is built for each candidate label L
i
(1 ? i ? |L|) found for the verb
v in the lexicon, following the procedure described in Apidianaki et al. (2012). For each candidate label,
we extract the content word co-occurrences of the verb v in the French sentences where it translates
an English verb tagged with this label in the training corpus. The retained French words constitute the
features of the vector built for that label. Let N be the number of features retained for each label L
i
of
the verb v from the corresponding French contexts. Each feature F
j
(1 ? j ? N ) receives a total weight
with the label (tw(F
j
, L
i
)) which is learned from the data and defined as the product of the feature?s
global weight (gw(F
j
)) and its local weight with that label (lw(F
j
, L
i
)). The global weight of a feature
F
j
is a function of the number n of candidate labels of v to which F
j
is related, and of the probabilities
(p
ij
) that F
j
co-occurs with instances of the verb v corresponding to each of the labels:
gw(F
j
) = 1?
?
n
i=1
p
ij
log(p
ij
)
n
(1)
Each p
ij
is computed as the ratio of the co-occurrence counts of F
j
with v when it is aligned to a label
L
i
to the total number of features (N ) seen with this candidate label:
p
ij
=
cooc count(F
j
, L
i
)
N
(2)
The local weight between feature F
j
and label L
i
(lw(F
j
, L
i
)) directly depends on the number of times
they occur together:
lw(F
j
, L
i
) = log(cooc count(F
j
, L
i
)) (3)
The intuition underlying this weighting scheme is that if an interesting semantic relation exists between a
feature F
j
and a specific predicate label L
i
of a verb v, then we expect the probability (p
ij
) of the feature
F
j
occurring in the contexts where v is translated by this label to be larger than if they were independent.
In other words, a feature gets a high total weight (tw) with a label when it appears frequently in the
corresponding French contexts and rarely in the contexts of the other labels.
1283
Labelling
Predicate identification on the French side is done by selecting verbs based on the PoS labels provided by
the tagger and subsequently filtering out modals and instances of the verb ?etre (be).
2
The most suitable
predicate labels are then assigned to the retained French verbs by the disambiguation classifier. The
context of a new instance of a French verb is compared to the weighted feature vectors (V
i
?s) built for
its candidate labels as described above, and an association score is assigned to each label. To facilitate
comparison with the vectors, the new contexts (sentences) are lemmatised and PoS tagged on the fly
(with TreeTagger) and the content word co-occurrences of the French verb are gathered in a bag of
words. If common features (CF s) are found between the new context and the vector of a label (L
i
), their
association score corresponds to the mean of the weights of their shared features with L
i
found in the
corresponding vector. In Equation 4, (CF
j
)
|CF |
j=1
is the set of common features between a label vector V
i
and the new context C and tw is the total weight of a CF with label L
i
, computed as explained in the
previous section.
assoc score(V
i
, C) =
?
|CF |
j=1
tw(CF
j
, L
i
)
|CF |
(4)
The label that receives the highest association score with the new context is returned and serves to
annotate the corresponding French verb.
5.2 Global cross-lingual role labelling
For role labelling, we adopt a different strategy. Whereas predicate labels include a lot of lexical infor-
mation, role labels do not. However, for role labels there is another source of information that helps to
define global estimates: the correlation between syntax and semantics.
Previous work in monolingual unsupervised semantic role induction (Grenager and Manning, 2006;
Lang and Lapata, 2010; Lang and Lapata, 2011) showed that mapping rules that assign semantic roles to
arguments of a verb based on the syntactic functions of these arguments, represent a baseline that is very
hard to beat. This strong correlation between syntactic labels and semantic role labels in the PropBank
annotation has been shown in detail by Merlo and Van der Plas (2009). In contrast to previous work on
monolingual unsupervised semantic role induction, we add the predicate label as a predictor. The core
arguments of the verb, that are the numbered labels in PropBank, are known to be verb-specific. We
have access to predicate labels assigned by the cross-lingual predicate labelling method described in the
previous section and exploit them for role labelling.
For a given predicate, diathesis alternations are the major source of variation in propositions. They
give rise to different syntactic structures, while the semantic roles remain stable. For example, the sen-
tence ?I gave the book to Jean? is syntactically different from ?I gave Jean the book?, but semantic roles
on the three arguments stay the same. We will show in a feasibility study that the effect of diathesis alter-
nations on the correlation between syntax and semantics is limited. In a cross-lingual setting, structural
divergences (Dorr, 1994) are expected to reduce the correlation between syntax and semantics. An ex-
ample is the difference in syntactic structure between the sentences ?Tu me manques? vs. ?I miss you?,
which are translations of each other, however the semantic roles are the same across languages.
As our global method is not restricted to alignments at labelling time, we are able to classify all given
arguments
3
and not just those that are aligned in a parallel corpus. In this way, we believe that the
negative effect of structural divergences and diathesis alternations is limited. Moreover, we show how
mild supervision from the partial annotations that result from the direct transfer can potentially remedy
these difficulties.
Learning syntactic-semantic mappings
The syntactic-semantic mapping rules that are exploited by our model for role labelling are extracted
from gold-annotated monolingual data. As a consequence, the extracted rules are of high quality which
2
We exclude the verb ?etre because its English counterpart (be) is not annotated in the CoNLL-2009 data used in our experi-
ments.
3
We focus on the classification of core semantic roles because diathesis alternations and cross-lingual divergences mainly
involve these roles.
1284
would not be the case if parallel data was used. Manually annotated parallel corpora are very sparse and
automatic parsing introduces errors which might be propagated by the direct transfer methods and result
in noisy annotations. Using gold-standard monolingual data thus ensures the quality of the mappings
exploited by our global model.
We build a model that determines the most suitable semantic role label r for a given argument of
a given predicate p, based on its syntactic dependency label d.
4
We simply compute the maximum
likelihood estimates (MLE) and count occurrences of the following triples < p, d, r > in a large body of
English gold semantically and syntactically annotated data.
P
MLE
(r|p, d) =
count(p, d, r)
count(p, d)
(5)
In the cross-lingual setting, the mapping rules extracted from the English training data are applied to
French. We learn the correspondences between English and French syntactic labels in a data-driven way
by syntactically annotating both sides of our parallel training corpus. We base the cross-lingual syntactic
mapping on alignment counts between syntactic labels in the parallel corpus parsed syntactically both on
the English and the French side (cf. Section 3). An alternative that needs no parallel data is to study the
annotation guidelines for the two languages and determine the cross-lingual correspondences between
syntactic labels by hand.
The syntactic label set used for French (Candito et al., 2009) is less fine-grained than the English labels
(20 versus 36). As a consequence, the mapping from English syntactic labels to French treebank labels
is for the most part a many-to-one mapping, which leads to information loss but suffices for our purpose
as will be shown in the next section.
Once the correspondences between the syntactic labels of the two syntactic annotation frameworks are
discovered, the cross-lingual transfer of syntactic-semantic mappings consists in substituting the English
syntactic labels with their French counterparts to adapt the model described above.
Labelling
For role labelling, we use estimates derived from the training data (see Equation 5) to determine the most
suitable role of a given argument. Because a particular triple in the test set might not have been seen
during training, we backoff to 2-tuples that discard the predicate label, and backoff to A1 if neither the
dependency label nor the predicate has been seen in training.
To treat the R-suffix, which takes care of anaphoric arguments, we use the following simple rule: for
the monolingual setting all arguments with PoS-tags ?WDT?, ?WP?, and ?WRB? receive the R-suffix.
In the cross-lingual setting, we translate the PoS tags to the single French PoS tag ?PROREL?. We do not
treat the C-prefix, which takes care of discontinuous arguments, because there were only a few examples.
We do not accept duplicate semantic roles, a constraint that leads to valid role configurations in general
(Punyakanok et al., 2008). We expect the more prominent semantic roles, such as A0 and A1, to appear
earlier in the sentence than semantic roles with higher numbers. We therefore attribute semantic roles of
a predicate from left to right.
5.3 Combining direct and global cross-lingual transfer
Direct transfer methods generally have low recall, we however expect them to be more precise than the
global methods. In our combined method, we use the annotations assigned by direct transfer as the
backbone and fill missing labels by the global methods. The annotations from direct transfer restrict
the possible roles the global method adds. We expect, as an additional benefit of this combination,
that the partial annotations from direct transfer together with the no-duplicate-role constraint described
above will remedy problems related to diathesis alternations. Although the probabilities computed will
favour the canonical alternation in general, the partial annotations may prevent a canonical analysis in a
particular proposition. Consider the following alternation example: Mary presented the flowers to John
vs. the less canonical alternation Mary presented John with the flowers. Although the most probable role
4
We chose not to include the complete dependency path from predicate to argument because of data sparseness. We select
the dependency label on the arc that points to the argument under discussion.
1285
Predicate identification and labelling
Labelled Unlabelled
Prec Rec F Prec Rec F
1 Direct 51 29 37 93 57 71
2 Global 45 39 42 95 83 89
3 Combined 45 45 45 92 91 91
4 Plas11 68 25 37 98 36 53
5 Plas11(f) 56 46 51 97 80 87
6 Manual 61 57 59 97 89 93
Table 1: Percent recall, precision and F-measure for predicate
identification and labelling.
Cross-lingual semantic role labelling
1 Direct 35
2 Global 68
3 Combined 73
4 Most frequent semantic role 48
Table 2: Percent accuracy for se-
mantic role labelling
for the prep relation would be A2, based on the canonical alternation, partial annotations on Mary (A0)
and John (A2) in combination with the no-duplicate-role constraint would rule that out and the next most
probable label would be put on with: A1.
6 Results and discussion
We ran experiments using the two global methods described in Section 5 separately and combined with
direct transfer. In this section, we present the results and compare to several baselines and upper bounds
from manual annotations and previous work.
6.1 Cross-lingual predicate labelling
Table 1 shows the results of cross-lingual predicate labelling (Labelled) and identification (Unlabelled).
The first row shows the results from using the traditional direct transfer method. The second row presents
results from the global method where we use cross-lingual WSD to label predicates. The third row
combines direct and global transfer, as explained in Section 5.3. For comparison, we present results
when using the parser from Van der Plas et al. (2011) on our test data: the fourth row contains results
when using all (unfiltered) data, the fifth row when using data filtered for incomplete predicate labellings.
We show an upper bound in the last row which corresponds to the inter-annotator agreement for manual
annotation on a random set of 100 sentences (van der Plas et al., 2010).
Overall the figures, including the upper bound from manual annotations, are not very high. Annotating
French verbs with English predicate labels is a hard task. When we look at the differences between the
three automatic methods, we see that recall is very low (29%) for the direct method. From the recall
figures for unlabelled predicates, we see that the direct method leaves many predicates without a label.
The global method has a much better recall, 39%, and a slightly lower precision. The best results
are however attained when the two methods are combined, that is, when global transfer is used to fill
in missing predicates from direct transfer. We get an F-measure of 45% which is a big improvement
over the baseline of direct transfer, which attained 37%. These results show that the global method for
predicate labelling improves recall without sacrificing precision too much.
We compare these results also to the results obtained by Van der Plas et al. (2011)?s three step model,
where a parser trained on transferred annotations annotates in turn the test sentences. We see that the
current method gives better results (recall and F-measure) when the parser is trained on unfiltered data.
An aggressive filter, that removes more than half of the data and leads to a big drop in performance for
argument labelling (recall that argument and predicate labelling is done in parallel in this model) finally
leads to a result that outperforms ours. This result is not surprising because the parser has access to much
more expressive syntax. Note that our global method only needs a PoS tagger in the source language
and no syntactic information nor joint semantic-syntactic parsing frameworks. It is thus knowledge-
lean and easier to apply to languages without a parser, a difference that should be taken into account
in the interpretation of the results. However, we can learn from these results that structural information
is beneficial. In future work, we plan to include word position information in our cross-lingual WSD
method. This will give the method access to structural information while keeping it knowledge-lean.
In Figure 2, we give an example that illustrates the contribution of the global method. In this example,
1286
English (automatic): There is in particular one amendment, let [let.01] me point [point.02] out, concerning [con-
cern.01] the energy sector, which, in my capacity as rapporteur, I see [see.01] as particularly important.
Transfer: Il y a notamment un amendement, je le souligne, concernant [concern.01] le secteur de l??energie, qui me
para??t en tant que rapporteur particuli`erement important.
CLWSD: Il y a notamment un amendement, je le souligne [stress.01], concernant [concern.01] le secteur de l??energie,
qui me para??t [seem.01] en tant que rapporteur particuli`erement important.
Figure 2: Predicate label addition and correction using CLWSD.
the cross-lingual WSD method annotates more verbs than the direct transfer approach: labels [stress.01]
and [seem.01] assigned during disambiguation, are missing from the first sentence after transfer. Even
with a high quality word alignment, it would not be possible to get these labels from the English source
sentence through direct transfer because they are simply not there, due to the non-literal translation. This
example shows the limitations of token-to token direct transfer and how the global method compensates
for that by using information aggregated across the whole parallel corpus.
6.2 Global cross-lingual role labelling
Though already supported by previous work (Grenager and Manning, 2006; Lang and Lapata, 2010;
Lang and Lapata, 2011), we tested the hypothesis that syntactic-semantic mappings provide good ap-
proximations for semantic role labelling, especially when adding predicate information. We therefore
first ran a monolingual feasibility study by collecting counts from the CoNLL 2009 training set and test-
ing on the CoNLL 2009 test set. The accuracy attained with this simple method is 79%. This shows that
in a monolingual setting, the predicate label combined with the syntactic label of the argument are good
predictors for the semantic role of the argument. This number can serve as a baseline for semantic role
labelling given the correct predicate label.
In previous sections, we discussed diathesis alternations as problematic for using syntactic-semantic
mapping rules. To measure the importance of diathesis alternations we need to measure the variation in
a large corpus. By applying the mapping rules learned from the training data on the same data we get
an idea of the amount of variation. We get an accuracy of 86%. Although the 14% probably contains
the most interesting examples from a linguistic point of view, these results on monolingual data show
that predicate-centered syntactic-semantic mapping rules are a promising direction for improving recall
in direct transfer methods.
Table 2 shows the results
5
for semantic role labelling for the three cross-lingual transfer methods
and the baseline of applying the most frequent semantic role label ?A1?. For the global and the com-
bined methods we use the predicate labels provided by the cross-lingual WSD method. The numbers
in Table 2 provide performance numbers given the predicate from the cross-lingual WSD method. We
discussed in Subsection 5.2 that, when applying syntactic-semantic mapping rules cross-lingually, dif-
ferences in annotation framework and cross-lingual divergences are at play. We see indeed that when
applying syntactic-semantic mapping rules cross-lingually the accuracy drops from 79 to 68%. This
drop in performance when applying to French the syntactic-semantic mappings that were learned on En-
glish data is not too important. This accuracy number is, in any case, much better than the results from
direct transfer. This is mainly due to the low recall of direct transfer which results in very few but rather
precise (87%) semantic roles. It is therefore very useful to use the direct transfer method as a backbone
that restricts the labels we get from global transfer by imposing consistency with the available annotation
(no-duplicate-argument-constraint). By combining the two methods, we get 73% accuracy that is not far
from the 79% in the monolingual setting. In future work, we would like to investigate whether the drop
in performance between the monolingual and cross-lingual setting is larger for languages that are less
related.
We also compare our results to previous work on cross-lingual transfer of semantic roles. Kozhevnikov
and Titov (2013) evaluate on the full test set described in Subsection 3 (1000 sentences), they use gold
predicates instead of predicted predicates and evaluate on both core roles and adjuncts. The authors
shared with us their results for core roles only: 74% and 77%, when using original and transferred
5
As we focus on argument labelling (and not identification) we provide accuracy scores.
1287
syntax, respectively. We use original syntax and should therefore compare to the 74%. When we use
gold predicate annotations as Kozhevnikov and Titov (2013) did, instead of the predicate labels obtained
through cross-lingual WSD, and test on all 1000 sentences, we attain 75% for the combined method and
71% for the global method. These results compare favourably with their results. This is encouraging
because their model uses a larger feature set that includes (cross-lingual) lexical features, the unlabelled
dependency graph and PoS information. Interestingly, they attain better scores when they use a trans-
ferred syntactic model instead of the original syntax. This result seems in line with our discussion on the
loss of information when trying to map the English syntactic label inventory to the French inventory. We
keep syntactic model transfer in mind for future work.
Because we consider the arguments as given, while Van der Plas et al. (2011) do both argument
identification and labelling for all core roles and adjuncts, and provide precision and recall given the
predicate only, we cannot directly compare to their results. We however include their results for the sake
of completeness. Their parser results in 65% F-score.
Applying A1 (the most frequent semantic role) to the entire data set gives us 48% accuracy. That is
much higher than results from transfer, again due to the low recall of the direct transfer method, but much
lower than the results of the combined and global methods.
7 Conclusion
We have introduced a global approach to transfer that aggregates information at the corpus level thereby
correcting and complementing the annotations from traditional direct transfer methods that suffer from
token-level mistakes. We show that the combination of direct transfer (a high-precision method) and
global methods (high in recall) outperforms previous results.
In contrast to previous work, we transfer predicate annotations and semantic role annotations by build-
ing two separate models tailored to the task at hand. We show how the predicate labels produced by our
high-coverage model for cross-lingual predicate labelling are successfully used as predictors for semantic
role labelling.
In future work, we would like to feed structural information to the cross-lingual WSD method such
as information about word position, which would preserve its knowledge-lean character without need
for syntactic parsing. Furthermore, we intend to use cross-lingual WSD for labelling adjuncts (non-
core semantic roles) since this task is also rather lexical in nature. Last but not least, we want to add
argument identification which will allow to propose a complete SRL annotation framework based on
global information.
Acknowledgements
This research was funded and supported by the German Research Foundation (Deutsche Forschungsge-
meinschaft, DFG) as part of the SFB 732.
References
A. Abeill?e, L. Cl?ement, and F. Toussenel. 2003. Building a treebank for French. In Treebanks: Building and
Using Parsed Corpora. Kluwer Academic Publishers.
P. Annesi and R. Basili. 2010. Cross-lingual alignment of FrameNet annotations through Hidden Markov Models.
In Proceedings of CICLing.
M. Apidianaki, G. Wisniewski, A. Sokolov, A. Max, and F. Yvon. 2012. WSD for n-best reranking and local
language modeling in SMT. In Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 1?9, Jeju, Republic of Korea, July. Association for Computational Linguistics.
M. Apidianaki. 2009. Data-driven Semantic Analysis for Multilingual WSD and Lexical Selection in Translation.
In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics
(EACL-09), pages 77?85, Athens, Greece.
1288
R. Basili, D. De Cao, D. Croce, B. Coppola, and A. Moschitti, 2009. Computational Linguistics and Intelligent Text
Processing, chapter Cross-Language Frame Semantics Transfer in Bilingual Corpora, pages 332?345. Springer
Berlin / Heidelberg.
M.-H. Candito, B. Crabb?e, P. Denis, and F. Gu?erin. 2009. Analyse syntaxique du franc?ais : des constituants
aux d?ependances. In Proceedings of la Conf?erence sur le Traitement Automatique des Langues Naturelles
(TALN?09), Senlis, France.
M. Carpuat and D. Wu. 2007. Improving Statistical Machine Translation using Word Sense Disambiguation. In
Proceedings of the Joint EMNLP-CoNLL Conference, pages 61?72, Prague, Czech Republic.
D. Das and S. Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 600?609, Portland, Oregon, USA, June. Association for Computational Linguistics.
B. Dorr. 1994. Machine translation divergences: A formal description and proposed solution. Computational
Linguistics, 20(4):597?633.
T. Grenager and C. Manning. 2006. Unsupervised discovery of a statistical verb lexicon. In Proceedings of
EMNLP.
J. Haji?c, M. Ciaramita, R. Johansson, D. Kawahara, M. A. Mart??, L. M`arquez, A. Meyers, J. Nivre, S. Pad?o,
J.
?
Step?anek, P. Stra?n?ak, M. Surdeanu, N. Xue, and Y. Zhang. 2009. The CoNLL-2009 shared task: Syntactic and
semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009).
J. Henderson, P. Merlo, G. Musillo, and I. Titov. 2008. A latent variable model of synchronous parsing for
syntactic and semantic dependencies. In Proceedings of CONLL 2008, pages 178?182.
R. Hwa, P. Resnik, A.Weinberg, C. Cabezas, and O. Kolak. 2005. Bootstrapping parsers via syntactic projection
accross parallel texts. Natural language engineering, 11:311?325.
P. Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of MT Summit
X, pages 79?86, Phuket, Thailand.
M. Kozhevnikov and I. Titov. 2013. Crosslingual transfer of semantic role models. In In Proceedings of the
51th Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August. Association for
Computational Linguistics.
J. Lang and M. Lapata. 2010. Unsupervised induction of semantic roles. In Human Language Technologies:
The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pages 939?947, Los Angeles, California, June. Association for Computational Linguistics.
J. Lang and M. Lapata. 2011. Unsupervised semantic role induction via split-merge clustering. In Proceedings of
the 49th Annual Meeting of the Association for Computational Linguistics.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn
Treebank. Comp. Ling., 19:313?330.
P. Merlo and L. van der Plas. 2009. Abstraction and generalisation in semantic role labels: PropBank, VerbNet or
both? In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP, pages 288?296, Suntec, Singapore.
A. Meyers. 2007. Annotation guidelines for NomBank - noun argument structure for PropBank. Technical report,
New York University.
R. Navigli. 2009. Word Sense Disambiguation: a Survey. ACM Computing Surveys, 41(2):1?69.
H. T. Ng, B. Wang, and Y. S. Chan. 2003. Exploiting Parallel Texts for Word Sense Disambiguation: An
Empirical Study. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,
pages 455?462, Sapporo, Japan.
F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational
Linguistics, 29:19?51.
S. Pad?o. 2007. Cross-lingual Annotation Projection Models for Role-Semantic Information. Ph.D. thesis, Saarland
University.
1289
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31:71?105.
V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role
labeling. Computational Linguistics, 34(2):257?287.
P. Resnik and D. Yarowsky. 2000. Distinguishing Systems and Distinguishing Senses: New Evaluation Methods
for Word Sense Disambiguation. Natural Language Engineering, 5(3):113?133.
H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Processing, pages 44?49, Manchester, UK, September.
http://www.ims.uni-stuttgart.de/?schmid/.
O. T?ackstr?om, D. Das, S. Petrov, R. McDonald, and J. Nivre. 2013. Token and type constraints for cross-lingual
part-of-speech tagging. In Transactions of the ACL. Association for Computational Linguistics, March.
I. Titov and J. Henderson. 2007. A latent variable model for generative dependency parsing. In Proceedings of
the International Conference on Parsing Technologies (IWPT-07), pages 144?155, Prague, Czech Republic.
I. Titov, J. Henderson, P. Merlo, and G. Musillo. 2009. Online graph planarisation for synchronous parsing
of semantic and syntactic dependencies. In Proceedings of the twenty-first international joint conference on
artificial intelligence (IJCAI-09), Pasadena, California, July.
L. van der Plas, T. Samard?zi?c, and P. Merlo. 2010. Cross-lingual validity of PropBank in the manual annotation of
French. In In Proceedings of the 4th Linguistic Annotation Workshop (The LAW IV), Uppsala, Sweden.
L. van der Plas, P. Merlo, and J. Henderson. 2011. Scaling up cross-lingual semantic annotation transfer. In Pro-
ceedings of the 49th Annual Meeting of the Association for Computational Linguistics and the Human Language
Technologies conference.
D. Yarowsky and G. Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across
aligned corpora. In Proceedings of the second meeting of the North American Chapter of the Association
for Computational Linguistics on Language technologies, NAACL ?01, pages 1?8, Stroudsburg, PA, USA.
Association for Computational Linguistics.
1290
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1476?1485,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Latent Semantic Word Sense Induction and Disambiguation
Tim Van de Cruys
RCEAL
University of Cambridge
United Kingdom
tv234@cam.ac.uk
Marianna Apidianaki
Alpage, INRIA & Univ Paris Diderot
Sorbonne Paris Cite?, UMRI-001
75013 Paris, France
marianna.apidianaki@inria.fr
Abstract
In this paper, we present a unified model for
the automatic induction of word senses from
text, and the subsequent disambiguation of
particular word instances using the automati-
cally extracted sense inventory. The induction
step and the disambiguation step are based on
the same principle: words and contexts are
mapped to a limited number of topical dimen-
sions in a latent semantic word space. The in-
tuition is that a particular sense is associated
with a particular topic, so that different senses
can be discriminated through their association
with particular topical dimensions; in a similar
vein, a particular instance of a word can be dis-
ambiguated by determining its most important
topical dimensions. The model is evaluated on
the SEMEVAL-2010 word sense induction and
disambiguation task, on which it reaches state-
of-the-art results.
1 Introduction
Word sense induction (WSI) is the task of automati-
cally identifying the senses of words in texts, with-
out the need for handcrafted resources or manually
annotated data. The manual construction of a sense
inventory is a tedious and time-consuming job, and
the result is highly dependent on the annotators and
the domain at hand. By applying an automatic proce-
dure, we are able to only extract the senses that are
objectively present in a particular corpus, and it al-
lows for the sense inventory to be straightforwardly
adapted to a new domain.
Word sense disambiguation (WSD), on the other
hand, is the closely related task of assigning a sense
label to a particular instance of a word in context,
using an existing sense inventory. The bulk of WSD
algorithms up till now use pre-defined sense inven-
tories (such as WordNet) that often contain fine-
grained sense distinctions, which poses serious prob-
lems for computational semantic processing (Ide
and Wilks, 2007). Moreover, most WSD algorithms
take a supervised approach, which requires a signifi-
cant amount of manually annotated training data.
The model presented here induces the senses of
words in a fully unsupervised way, and subsequently
uses the induced sense inventory for the unsuper-
vised disambiguation of particular occurrences of
words. The induction step and the disambiguation
step are based on the same principle: words and
contexts are mapped to a limited number of topical
dimensions in a latent semantic word space. The
key idea is that the model combines tight, synonym-
like similarity (based on dependency relations) with
broad, topical similarity (based on a large ?bag of
words? context window). The intuition in this is that
the dependency features can be disambiguated by
the topical dimensions identified by the broad con-
textual features; in a similar vein, a particular in-
stance of a word can be disambiguated by determin-
ing its most important topical dimensions (based on
the instance?s context words).
The paper is organized as follows. Section 2
presents some previous research on distributional
similarity and word sense induction. Section 3 gives
an overview of our method for word sense induction
and disambiguation. Section 4 provides a quantita-
tive evaluation and comparison to other algorithms
in the framework of the SEMEVAL-2010 word sense
1476
induction and disambiguation (WSI/WSD) task. The
last section draws conclusions, and lays out a num-
ber of future research directions.
2 Previous Work
2.1 Distributional similarity
According to the distributional hypothesis of mean-
ing (Harris, 1954), words that occur in similar con-
texts tend to be semantically similar. In the spirit
of this by now well-known adage, numerous algo-
rithms have sprouted up that try to capture the se-
mantics of words by looking at their distribution in
texts, and comparing those distributions in a vector
space model.
One of the best known models in this respect is
latent semantic analysis ? LSA (Landauer and Du-
mais, 1997; Landauer et al, 1998). In LSA, a term-
document matrix is created, that contains the fre-
quency of each word in a particular document. This
matrix is then decomposed into three other matrices
with a mathematical factorization technique called
singular value decomposition (SVD). The most im-
portant dimensions that come out of the SVD are said
to represent latent semantic dimensions, according
to which nouns and documents can be represented
more efficiently. Our model also applies a factoriza-
tion technique (albeit a different one) in order to find
a reduced semantic space.
Context is a determining factor in the nature of
the semantic similarity that is induced. A broad con-
text window (e.g. a paragraph or document) yields
broad, topical similarity, whereas a small context
yields tight, synonym-like similarity. This has lead
a number of researchers to use the dependency rela-
tions that a particular word takes part in as contex-
tual features. One of the most important approaches
is Lin (1998). An overview of dependency-based
semantic space models is given in Pado? and Lapata
(2007).
2.2 Word sense induction
The following paragraphs provide a succinct
overview of word sense induction research. A thor-
ough survey on word sense disambiguation (includ-
ing unsupervised induction algorithms) is presented
in Navigli (2009).
Algorithms for word sense induction can roughly
be divided into local and global ones. Local WSI
algorithms extract the different senses of a word on
a per-word basis, i.e. the different senses for each
word are determined separately. They can be further
subdivided into context-clustering algorithms and
graph-based algorithms. In the context-clustering
approach, context vectors are created for the differ-
ent instances of a particular word, and those con-
texts are grouped into a number of clusters, repre-
senting the different senses of the word. The con-
text vectors may be represented as first or second-
order co-occurrences (i.e. the contexts of the target
word are similar if the words they in turn co-occur
with are similar). The first one to propose this idea
of context-group discrimination was Schu?tze (1998),
and many researchers followed a similar approach
to sense induction (Purandare and Pedersen, 2004).
In the graph-based approach, on the other hand, a
co-occurrence graph is created, in which nodes rep-
resent words, and edges connect words that appear
in the same context (dependency relation or context
window). The senses of a word may then be discov-
ered using graph clustering techniques (Widdows
and Dorow, 2002), or algorithms such as HyperLex
(Ve?ronis, 2004) or Pagerank (Agirre et al, 2006). Fi-
nally, Bordag (2006) recently proposed an approach
that uses word triplets to perform word sense induc-
tion. The underlying idea is the ?one sense per col-
location? assumption, and co-occurrence triplets are
clustered based on the words they have in common.
Global algorithms take an approach in which the
different senses of a particular word are determined
by comparing them to, and demarcating them from,
the senses of other words in a full-blown word space
model. The best known global approach is the one
by Pantel and Lin (2002). They present a global
clustering algorithm ? coined clustering by commit-
tee (CBC) ? that automatically discovers word senses
from text. The key idea is to first discover a set of
tight, unambiguous clusters, to which possibly am-
biguous words can be assigned. Once a word has
been assigned to a cluster, the features associated
with that particular cluster are stripped off the word?s
vector. This way, less frequent senses of the word
may be discovered.
Van de Cruys (2008) proposes a model for sense
induction based on latent semantic dimensions. Us-
ing an extension of non-negative matrix factoriza-
1477
tion, the model induces a latent semantic space
according to which both dependency features and
broad contextual features are classified. Using the
latent space, the model is able to discriminate be-
tween different word senses. The model presented
below is an extension of this approach: whereas the
model described in Van de Cruys (2008) is only able
to perform word sense induction, our model is ca-
pable of performing both word sense induction and
disambiguation.
3 Methodology
3.1 Non-negative Matrix Factorization
Our model uses non-negative matrix factorization ?
NMF (Lee and Seung, 2000) in order to find latent
dimensions. There are a number of reasons to prefer
NMF over the better known singular value decompo-
sition used in LSA. First of all, NMF allows us to min-
imize the Kullback-Leibler divergence as an objec-
tive function, whereas SVD minimizes the Euclidean
distance. The Kullback-Leibler divergence is better
suited for language phenomena. Minimizing the Eu-
clidean distance requires normally distributed data,
and language phenomena are typically not normally
distributed. Secondly, the non-negative nature of the
factorization ensures that only additive and no sub-
tractive relations are allowed. This proves partic-
ularly useful for the extraction of semantic dimen-
sions, so that the NMF model is able to extract much
more clear-cut dimensions than an SVD model. And
thirdly, the non-negative property allows the result-
ing model to be interpreted probabilistically, which
is not straightforward with an SVD factorization.
The key idea is that a non-negative matrix A is
factorized into two other non-negative matrices, W
and H
Ai?j ?Wi?kHk?j (1)
where k is much smaller than i, j so that both in-
stances and features are expressed in terms of a few
components. Non-negative matrix factorization en-
forces the constraint that all three matrices must be
non-negative, so all elements must be greater than or
equal to zero.
Using the minimization of the Kullback-Leibler
divergence as an objective function, we want to
find the matrices W and H for which the Kullback-
Leibler divergence between A and WH (the multipli-
cation of W and H) is the smallest. This factoriza-
tion is carried out through the iterative application
of update rules. Matrices W and H are randomly
initialized, and the rules in 2 and 3 are iteratively ap-
plied ? alternating between them. In each iteration,
each vector is adequately normalized, so that all di-
mension values sum to 1.
Ha? ? Ha?
?
iWia
Ai?
(WH)i?
?
kWka
(2)
Wia ?Wia
?
?Ha?
Ai?
(WH)i?
?
vHav
(3)
3.2 Word sense induction
Using an extension of non-negative matrix factoriza-
tion, we are able to jointly induce latent factors for
three different modes: words, their window-based
(?bag of words?) context words, and their depen-
dency relations. Three matrices are constructed that
capture the pairwise co-occurrence frequencies for
the different modes. The first matrix contains co-
occurrence frequencies of words cross-classified by
dependency relations, the second matrix contains
co-occurrence frequencies of words cross-classified
by words that appear in the noun?s context window,
and the third matrix contains co-occurrence frequen-
cies of dependency relations cross-classified by co-
occurring context words. NMF is then applied to the
three matrices and the separate factorizations are in-
terleaved (i.e. the results of the former factorization
are used to initialize the factorization of the next ma-
trix). A graphical representation of the interleaved
factorization algorithm is given in figure 1.
The procedure of the algorithm goes as follows.
First, matrices W, H, G, and F are randomly initial-
ized. We then start our first iteration, and compute
the update of matrix W (using equation 3). Matrix
W is then copied to matrix V, and the update of
matrix G is computed (using equation 2). The trans-
pose of matrix G is again copied to matrix U, and
the update ofF is computed (again using equation 2).
As a last step, matrix F is copied to matrix H, and
we restart the iteration loop until a stopping criterion
(e.g. a maximum number of iterations, or no more
significant change in objective function; we used the
1478
= xW H
= xV G
= xU F
j
i
s
k
i
j
kAwords xdependency relations
B
words xcontext words
Ccontext words xdependency relations
k k
k
k
i
j
i
s
j
s s
Figure 1: A graphical representation of the interleaved
NMF algorithm
former one) is reached.1 When the factorization is
finished, the three different modes (words, window-
based context words and dependency relations) are
all represented according to a limited number of la-
tent factors.
Next, the factorization that is thus created is used
for word sense induction. The intuition is that a par-
ticular, dominant dimension of an ambiguous word
is ?switched off?, in order to reveal other possible
senses of the word. Formally, we proceed as follows.
Matrix H indicates the importance of each depen-
dency relation given a topical dimension. With this
knowledge, the dependency relations that are respon-
sible for a certain dimension can be subtracted from
the original noun vector. This is done by scaling
down each feature of the original vector according
to the load of the feature on the subtracted dimen-
sion, using equation 4.
t = v(u1 ? hk) (4)
Equation 4 multiplies each dependency feature of
the original noun vector v with a scaling factor, ac-
cording to the load of the feature on the subtracted
dimension (hk ? the vector of matrix H that corre-
sponds to the dimension we want to subtract). u1 is
a vector of ones with the same length as hk. The re-
sult is vector t, in which the dependency features rel-
1Note that this is not the only possibly way of interleaving
the different factorizations, but in our experiments we found that
different constellations lead to similar results.
evant to the particular topical dimension have been
scaled down.
In order to determine which dimension(s) are re-
sponsible for a particular sense of the word, the
method is embedded in a clustering approach. First,
a specific word is assigned to its predominant sense
(i.e. the most similar cluster). Next, the dominant
semantic dimension(s) for this cluster are subtracted
from the word vector, and the resulting vector is
fed to the clustering algorithm again, to see if other
word senses emerge. The dominant semantic dimen-
sion(s) can be identified by folding vector c ? repre-
senting the cluster centroid ? into the factorization
(equation 5). This yields a probability vector b over
latent factors for the particular centroid.
b = cHT (5)
A simple k-means algorithm is used to com-
pute the initial clustering, using the non-factorized
dependency-based feature vectors (matrix A). k-
means yields a hard clustering, in which each noun
is assigned to exactly one (dominant) cluster. In the
second step, we determine for each noun whether
it can be assigned to other, less dominant clusters.
First, the salient dimension(s) of the centroid to
which the noun is assigned are determined. The cen-
troid of the cluster is computed by averaging the fre-
quencies of all cluster elements except for the tar-
get word we want to reassign. After subtracting the
salient dimensions from the noun vector, we check
whether the vector is reassigned to another cluster
centroid. If this is the case, (another instance of) the
noun is assigned to the cluster, and the second step
is repeated. If there is no reassignment, we continue
with the next word. The target element is removed
from the centroid to make sure that only the dimen-
sions associated with the sense of the cluster are sub-
tracted. When the algorithm is finished, each noun
is assigned to a number of clusters, representing its
different senses.
We use two different methods for selecting the fi-
nal number of candidate senses. The first method,
NMFcon , takes a conservative approach, and only
selects candidate senses if ? after the subtraction of
salient dimensions ? another sense is found that is
more similar2 to the adapted noun vector than the
2We use the cosine measure for our similarity calculations.
1479
dominant sense. The second method, NMFlib , is
more liberal, and also selects the next best cluster
centroid as candidate sense until a certain similarity
threshold ? is reached.3
3.3 Word sense disambiguation
The sense inventory that results from the induction
step can now be used for the disambiguation of in-
dividual instances as follows. For each instance of
the target noun, we extract its context words, i.e. the
words that co-occur in the same paragraph, and rep-
resent them as a probability vector f . Using matrix
G from our factorization model (which represents
context words by semantic dimensions), this vector
can be folded into the semantic space, thus represent-
ing a probability vector over latent factors for the
particular instance of the target noun (equation 6).
d = fGT (6)
Likewise, the candidate senses of the noun (repre-
sented as centroids) can be folded into our seman-
tic space using matrix H (equation 5). This yields
a probability distribution over the semantic dimen-
sions for each centroid. As a last step, we com-
pute the Kullback-Leibler divergence between the
context vector and the candidate centroids, and se-
lect the candidate centroid that yields the lowest di-
vergence as the correct sense. The disambiguation
process is represented graphically in figure 2.
3.4 Example
Let us clarify the process with an example for the
noun chip. The sense induction algorithm finds the
following candidate senses:4
1. cache, CPU, memory, microprocessor, proces-
sor, RAM, register
2. bread, cake, chocolate, cookie, recipe, sand-
wich
3. accessory, equipment, goods, item, machinery,
material, product, supplies
3Experimentally (examining the cluster output), we set ? =
0.2
4Note that we do not use the word sense to hint at a lexico-
graphic meaning distinction; rather, sense in this case should be
regarded as a more coarse-grained and topic-related entity.
G'
k
sscontext vector
k
cluster centroid
j
cluster centroid
j
cluster centroid
j
H'
k
j
k
k
k
Figure 2: Graphical representation of the disambiguation
process
Each candidate sense is associated with a centroid
(the average frequency vector of the cluster?s mem-
bers), that is folded into the semantic space, which
yields a ?semantic fingerprint?, i.e. a distribution
over the semantic dimensions. For the first sense,
the ?computer? dimension will be the most impor-
tant. Likewise, for the second and the third sense the
?food? dimension and the ?manufacturing? dimension
will be the most important.5
Let us now take a particular instance of the noun
chip, such as the one in (1).
(1) An N.V. Philips unit has created a com-
puter system that processes video images
3,000 times faster than conventional systems.
Using reduced instruction - set comput-
ing, or RISC, chips made by Intergraph of
Huntsville, Ala., the system splits the im-
age it ?sees? into 20 digital representations,
each processed by one chip.
Looking at the context of the particular instance of
chip, a context vector is created which represents
the semantic content words that appear in the same
paragraph (the extracted content words are printed
in boldface). This context vector is again folded
into the semantic space, yielding a distribution over
the semantic dimensions. By selecting the lowest
5In the majority of cases, the induced dimensions indeed
contain such clear-cut semantics, so that the dimensions can be
rightfully labeled as above.
1480
Kullback-Leibler divergence between the semantic
probability distribution of the target instance and the
semantic probability distributions of the candidate
senses, the algorithm is able to assign the ?computer?
sense of the target noun chip.
4 Evaluation
4.1 Dataset
Our word sense induction and disambiguation
model is trained and tested on the dataset of the
SEMEVAL-2010 WSI/WSD task (Manandhar et al,
2010). The SEMEVAL-2010 WSI/WSD task is based
on a dataset of 100 target words, 50 nouns and 50
verbs. For each target word, a training set is pro-
vided from which the senses of the word have to
be induced without using any other resources. The
training set for a target word consists of a set of
target word instances in context (sentences or para-
graphs). The complete training set contains 879,807
instances, viz. 716,945 noun and 162,862 verb in-
stances.
The senses induced during training are used for
disambiguation in the testing phase. In this phase,
the system is provided with a test set that consists
of unseen instances of the target words. The test
set contains 8,915 instances in total, of which 5,285
nouns and 3,630 verbs. The instances in the test
set are tagged with OntoNotes senses (Hovy et al,
2006). The system needs to disambiguate these in-
stances using the senses acquired during training.
4.2 Implementational details
The SEMEVAL training set has been part of speech
tagged and lemmatized with the Stanford Part-Of-
Speech Tagger (Toutanova and Manning, 2000;
Toutanova et al, 2003) and parsed with Malt-
Parser (Nivre et al, 2006), trained on sections 2-
21 of the Wall Street Journal section of the Penn
Treebank extended with about 4000 questions from
the QuestionBank6 in order to extract dependency
triples. The SEMEVAL test set has only been tagged
and lemmatized, as our disambiguation model does
not use dependency triples as features (contrary to
the induction model).
6http://maltparser.org/mco/english_
parser/engmalt.html
We constructed two different models ? one for
nouns and one for verbs. For each model, the matri-
ces needed for our interleaved NMF factorization are
extracted from the corpus. The noun model was built
using 5K nouns, 80K dependency relations, and 2K
context words (excluding stop words) with highest
frequency in the training set, which yields matrices
of 5K nouns ? 80K dependency relations, 5K nouns
? 2K context words, and 80K dependency relations
? 2K context words. The model for verbs was con-
structed analogously, using 3K verbs, and the same
number of dependency relations and context words.
For our initial k-means clustering, we set k = 600
for nouns, and k = 400 for verbs. For the under-
lying interleaved NMF model, we used 50 iterations,
and factored the model to 50 dimensions.
4.3 Evaluation measures
The results of the systems participating in the
SEMEVAL-2010 WSI/WSD task are evaluated both
in a supervised and in an unsupervised manner.
The supervised evaluation in the SEMEVAL-2010
WSI/WSD task follows the scheme of the SEMEVAL-
2007 WSI task (Agirre and Soroa, 2007), with some
modifications. One part of the test set is used as a
mapping corpus, which maps the automatically in-
duced clusters to gold standard senses; the other part
acts as an evaluation corpus. The mapping between
clusters and gold standard senses is used to tag the
evaluation corpus with gold standard tags. The sys-
tems are then evaluated as in a standard WSD task,
using recall.
In the unsupervised evaluation, the induced
senses are evaluated as clusters of instances which
are compared to the sets of instances tagged with
the gold standard senses (corresponding to classes).
Two partitions are thus created over the test set of
a target word: a set of automatically generated clus-
ters and a set of gold standard classes. A number of
these instances will be members of both one gold
standard class and one cluster. Consequently, the
quality of the proposed clustering solution is evalu-
ated by comparing the two groupings and measuring
their similarity.
Two evaluation metrics are used during the unsu-
pervised evaluation in order to estimate the quality
of the clustering solutions, the V-Measure (Rosen-
berg and Hirschberg, 2007) and the paired F-
1481
Score (Artiles et al, 2009). V-Measure assesses the
quality of a clustering by measuring its homogeneity
(h) and its completeness (c). Homogeneity refers to
the degree that each cluster consists of data points
primarily belonging to a single gold standard class,
while completeness refers to the degree that each
gold standard class consists of data points primarily
assigned to a single cluster. V-Measure is the har-
monic mean of h and c.
VM =
2 ? h ? c
h+ c
(7)
In the paired F-Score (Artiles et al, 2009) eval-
uation, the clustering problem is transformed into a
classification problem (Manandhar et al, 2010). A
set of instance pairs is generated from the automati-
cally induced clusters, which comprises pairs of the
instances found in each cluster. Similarly, a set of in-
stance pairs is created from the gold standard classes,
containing pairs of the instances found in each class.
Precision is then defined as the number of common
instance pairs between the two sets to the total num-
ber of pairs in the clustering solution (cf. formula 8).
Recall is defined as the number of common instance
pairs between the two sets to the total number of
pairs in the gold standard (cf. formula 9). Preci-
sion and recall are finally combined to produce the
harmonic mean (cf. formula 10).
P =
|F (K) ? F (S)|
|F (K)|
(8)
R =
|F (K) ? F (S)|
|F (S)|
(9)
FS =
2 ? P ?R
P +R
(10)
The obtained results are also compared to two
baselines. The most frequent sense (MFS) baseline
groups all testing instances of a target word into one
cluster. The Random baseline randomly assigns an
instance to one of the clusters.7 This baseline is exe-
cuted five times and the results are averaged.
7The number of clusters in Random was chosen to be
roughly equal to the average number of senses in the gold stan-
dard.
4.4 Results
4.4.1 Unsupervised evaluation
In table 1, we present the performance of a number
of algorithms on the V-measure. We compare our
V-measure scores with the scores of the best-ranked
systems in the SEMEVAL 2010 WSI/WSD task, both
for the complete data set and for nouns and verbs
separately. The fourth column shows the average
number of clusters induced in the test set by each
algorithm. The MFS baseline has a V-Measure equal
to 0, since by definition its completeness is 1 and its
homogeneity is 0.
NMFcon ? our model that takes a conservative ap-
proach in the induction of candidate senses ? does
not beat the random baseline. NMFlib ? our model
that is more liberal in inducing senses ? reaches bet-
ter results. With 11.8%, it scores similar to other
algorithms that induce a similar average number of
clusters, such as Duluth-WSI (Pedersen, 2010).
Pedersen (2010) has shown that the V-Measure
tends to favour systems producing a higher number
of clusters than the number of gold standard senses.
This is reflected in the scores of our models as well.
VM (%) all noun verb #cl
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
KSU KDD 15.7 18.0 12.4 17.50
NMFlib 11.8 13.5 9.4 4.80
Duluth-WSI 9.0 11.4 5.7 4.15
Random 4.4 4.2 4.6 4.00
NMFcon 3.9 3.9 3.9 1.58
MFS 0.0 0.0 0.0 1.00
Table 1: Unsupervised V-measure evaluation on SE-
MEVAL test set
Motivated by the large divergences in the sys-
tem rankings on the different metrics used in the
SEMEVAL-2010 WSI/WSD task, Pedersen evaluated
the metrics themselves. His evaluation relied on
the assumption that a good measure should assign
low scores to random baselines. Pedersen showed
that the V-Measure continued to improve as random-
ness increased. We agree with Pedersen?s conclu-
sion that the V-Measure results should be interpreted
with caution, but we still report the results in order
1482
to perform a global comparison, on all metrics, of
our system?s performance to the systems that partic-
ipated to the SEMEVAL task.
Contrary to V-Measure, paired F-score is a fairly
reliable measure and the only one that managed to
identify and expose random baselines in the above
mentioned metric evaluation. This means that the
random systems used for testing were ranked low
when a high number of random senses was used.
In table 2, the paired F-Score of a number of al-
gorithms is given. The paired F-Score penalizes sys-
tems when they produce a higher number of clusters
(low recall) or a lower number of clusters (low pre-
cision) than the gold standard number of senses. We
again compare our results with the scores of the best-
ranked systems in the SEMEVAL-2010 WSI/WSD
TASK.
FS (%) all noun verb #cl
MFS 63.5 57.0 72.7 1.00
Duluth-WSI-SVD-Gap 63.3 57.0 72.4 1.02
NMFcon 60.2 54.6 68.4 1.58
NMFlib 45.3 42.2 49.8 5.42
Duluth-WSI 41.1 37.1 46.7 4.15
Random 31.9 30.4 34.1 4.00
Table 2: Unsupervised paired F-score evaluation on SE-
MEVAL testset
NMFcon reaches a score of 60.2%, which is again
similar to other algorithms that induce the same av-
erage number of clusters. NMFlib scores 45.3%, in-
dicating that the algorithm is able to retain a rea-
sonable F-Score while at the same time inducing a
significant number of clusters. This especially be-
comes clear when comparing its score to the other
algorithms.
4.4.2 Supervised evaluation
In the supervised evaluation, the automatically in-
duced clusters are mapped to gold standard senses,
using the mapping corpus (i.e. one part of the test
set). The obtained mapping is used to tag the evalu-
ation corpus (i.e. the other part of the test set) with
gold standard tags, which means that the methods
are evaluated in a standard WSD task.
Table 3 shows the recall of our algorithms in the
supervised evaluation, again compared to other algo-
rithms evaluated in the SEMEVAL-2010 WSI/WSD
task.
SR (%) all noun verb #S
NMFlib 62.6 57.3 70.2 1.82
UoY 62.4 59.4 66.8 1.51
Duluth-WSI 60.5 54.7 68.9 1.66
NMFcon 60.3 54.5 68.8 1.21
MFS 58.7 53.2 66.6 1.00
Random 57.3 51.5 65.7 1.53
Table 3: Supervised recall for SEMEVAL testset, 80%
mapping, 20% evaluation
NMFlib gets 62.6%, which makes it the best scor-
ing algorithm on the supervised evaluation. NMFcon
reaches 60.3%, which again indicates that it is in the
same ballpark as other algorithms that induce a sim-
ilar average number of senses.
Some doubts have been cast on the representative-
ness of the supervised recall results as well. Accord-
ing to Pedersen (2010), the supervised learning al-
gorithm that underlies this evaluation method tends
to converge to the Most Frequent Sense (MFS) base-
line, because the number of senses that the classi-
fier assigns to the test instances is rather low. We
think these shortcomings indicate the need for the
development of new evaluation metrics, capable of
providing a more accurate evaluation of the perfor-
mance of WSI systems. Nevertheless, these metrics
still constitute a useful testbed for comparing the per-
formance of different systems.
5 Conclusion and future work
In this paper, we presented a model based on latent
semantics that is able to perform word sense induc-
tion as well as disambiguation. Using latent topi-
cal dimensions, the model is able to discriminate be-
tween different senses of a word, and subsequently
disambiguate particular instances of a word. The
evaluation results indicate that our model reaches
state-of-the-art performance compared to other sys-
tems that participated in the SEMEVAL-2010 word
sense induction and disambiguation task. Moreover,
our global approach is able to reach similar perfor-
mance on an evaluation set that is tuned to fit the
needs of local approaches. The evaluation set con-
1483
tains an enormous amount of contexts for only a
small number of target words, favouring methods
that induce senses on a per-word basis. A global
approach like ours is likely to induce a more bal-
anced sense inventory using an unbiased corpus, and
is likely to outperform local methods when such an
unbiased corpus is used as input. We therefore think
that the global, unified approach to word sense in-
duction and disambiguation presented here provides
a genuine and powerful solution to the problem at
hand.
We conclude with some issues for future work.
First of all, we would like to evaluate the approach
presented here using a more balanced and unbiased
corpus, and compare its performance on such a cor-
pus to local approaches. Secondly, we would also
like to include grammatical dependency information
in the disambiguation step of the algorithm. For now,
the disambiguation step only uses a word?s context
words; enriching the feature set with dependency in-
formation is likely to improve the performance of
the disambiguation.
Acknowledgments
This work is supported by the Scribo project, funded
by the French ?po?le de compe?titivite?? System@tic,
and by the French national grant EDyLex (ANR-09-
CORD-008).
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the fourth Interna-
tional Workshop on Semantic Evaluations (SemEval),
ACL, pages 7?12, Prague, Czech Republic.
Eneko Agirre, David Mart??nez, Ojer Lo?pez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algo-
rithms for state-of-the-art WSD. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-06), pages 585?593, Syd-
ney, Australia.
Marianna Apidianaki and Tim Van de Cruys. 2011. A
Quantitative Evaluation of Global Word Sense Induc-
tion. In Proceedings of the 12th International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing), published in Springer
Lecture Notes in Computer Science (LNCS), volume
6608, pages 253?264, Tokyo, Japan.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo. 2009.
The role of named entities in web people search. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-09), pages
534?542, Singapore.
Stefan Bordag. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-06), pages 137?144, Trento, Italy.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Human Lan-
guage Technology / North American Association of
Computational Linguistics conference (HLT-NAACL-
06), pages 57?60, New York, NY.
Nancy Ide and Yorick Wilks. 2007. Making Sense About
Sense. In Eneko Agirre and Philip Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211?240.
Thomas Landauer, Peter Foltz, and Darrell Laham. 1998.
An Introduction to Latent Semantic Analysis. Dis-
course Processes, 25:295?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In Ad-
vances in Neural Information Processing Systems, vol-
ume 13, pages 556?562.
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics (COLING-ACL98), volume 2, pages
768?774, Montreal, Quebec, Canada.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dligach,
and Sameer S. Pradhan. 2010. SemEval-2010 Task
14: Word Sense Induction & Disambiguation. In Pro-
ceedings of the fifth International Workshop on Seman-
tic Evaluation (SemEval), ACL-10, pages 63?68, Upp-
sala, Sweden.
Roberto Navigli. 2009. Word Sense Disambiguation: a
Survey. ACM Computing Surveys, 41(2):1?69.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the fifth International
Conference on Language Resources and Evaluation
(LREC-06), pages 2216?2219, Genoa, Italy.
1484
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161?199.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 613?619, Edmonton, Alberta, Canada.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters Ap-
plied to the Sense Induction Task of SemEval-2. In
Proceedings of the fifth International Workshop on Se-
mantic Evaluations (SemEval-2010), pages 363?366,
Uppsala, Sweden.
Amruta Purandare and Ted Pedersen. 2004. Word
Sense Discrimination by Clustering Contexts in Vec-
tor and Similarity Spaces. In Proceedings of the Con-
ference on Computational Natural Language Learning
(CoNLL), pages 41?48, Boston, MA.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the Joint
2007 Conference on Empirical Methods in Natural
Language Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410?420,
Prague, Czech Republic.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97?123.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-2000), pages 63?70.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of the Human Language Technology
/ North American Association of Computational Lin-
guistics conference (HLT-NAACL-03, pages 252?259,
Edmonton, Canada.
Tim Van de Cruys. 2008. Using Three Way Data for
Word Sense Discrimination. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING-08), pages 929?936, Manchester,
UK.
Jean Ve?ronis. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech & Language,
18(3):223?252.
Dominic Widdows and Beate Dorow. 2002. A Graph
Model for Unsupervised Lexical Acquisition. In Pro-
ceedings of the 19th International Conference on Com-
putational Linguistics (COLING-02), pages 1093?
1099, Taipei, Taiwan.
1485
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 178?182, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
LIMSI : Cross-lingual Word Sense Disambiguation using
Translation Sense Clustering
Marianna Apidianaki
LIMSI-CNRS
Rue John Von Neumann
91403 Orsay Cedex, France
marianna@limsi.fr
Abstract
We describe the LIMSI system for the
SemEval-2013 Cross-lingual Word Sense Dis-
ambiguation (CLWSD) task. Word senses are
represented by means of translation clusters
in different languages built by a cross-lingual
Word Sense Induction (WSI) method. Our
CLWSD classifier exploits the WSI output for
selecting appropriate translations for target
words in context. We present the design of the
system and the obtained results.
1 Introduction
This paper describes the LIMSI system that partici-
pated in the Cross-Lingual Word Sense Disambigua-
tion (CLWSD) task of SemEval-2013. The goal of
CLWSD is to predict semantically correct transla-
tions for ambiguous words in context (Resnik and
Yarowsky, 2000; Carpuat and Wu, 2007; Apidi-
anaki, 2009). The CLWSD task of the SemEval-2013
evaluation campaign is a lexical sample task for En-
glish nouns and is divided into two subtasks: the
best subtask where systems are asked to provide a
unique good translation for words in context; the
out-of-five (oof) subtask where systems can propose
up to five semantically related translations for each
target word instance (Lefever and Hoste, 2013). The
CLWSD lexical sample contains 20 nouns and the
test set is composed of 50 instances per noun. Sys-
tem performance is evaluated by comparing the sys-
tem output to a set of gold standard annotations in
five languages: French, Spanish, Italian, Dutch and
German. Participating systems have to provide con-
textually appropriate translations for target words in
context in each or a subset of the target languages.
We apply the CLWSD method proposed by Apid-
ianaki (2009) to three bilingual tasks: English-
Spanish, English-French and English-Italian. The
method exploits the translation clusters generated in
the three target languages by a cross-lingual Word
Sense Induction (WSI) method. The WSI method
clusters the translations of target words in a parallel
corpus using source language context vectors. The
same vectors are exploited during disambiguation in
order to select the most appropriate translations for
new instances of the target words in context.
2 System Description
2.1 Translation clustering
Contrary to monolingual WSI methods which group
the instances of the words into clusters describ-
ing their senses, the cross-lingual WSI method used
here clusters the translations of words in a paral-
lel corpus. The corpus used for French consists
of the English-French parts of Europarl (version 7)
(Koehn, 2005) and of the JRC-Acquis corpus (Stein-
berger et al, 2006), joined together. For English-
Spanish and English-Italian we only use the corre-
sponding parts of Europarl. The corpora are first
tokenized and lowercased using the Moses scripts,
then lemmatized and tagged by part-of-speech (PoS)
using the TreeTagger (Schmid, 1994). Words in the
corpus are replaced by a lemma and PoS tag pair be-
fore word alignment, to resolve categorical ambigu-
ities in context. The corpus is aligned in both trans-
lation directions with GIZA++ (Och and Ney, 2000)
178
Target word French Spanish Italian
range
{ensemble, diversite?, palette,
nombre} {domaine} {porte?e}
{e?ventail, nombre, gamme, se?rie,
ensemble}
{gama, serie, abanico,
diversidad, variedad, espectro,
conjunto} {cantidad, alcance,
a`mbito, nu?mero, tipo, espectro,
rango} {amplitud}
{serie, gamma, spettro, numero,
ventaglio} {ampiezza, portata}
{settore, ambito}
{diversita?, fascia}
mood
{climat, atmosphe`re}, {esprit,
atmosphe`re, ambiance, humeur}
{opinion} {volonte?} {attitude}
{clima, atmo`sfera, ambiente}
{a`nimo, sentimiento} {talante}
{a`nimo, clima, ambiente}
{a`nimo, humor, ambiente}
{clima} {atmosfera}
{chiarezza, predisposizione}
{opinione} {atteggiamento}
mission
{ope?ration, mandat}
{de?le?gation, commission}
{de?le?gation, ta?che, voyage,
ope?ration}
{funcio?n, cometido, objetivo,
tarea} {viaje, tarea, delegacio?n}
{tarea, mandato, cometido}
{mandato, obiettivo, compito,
mission, funzione, operazione,}
{viaggio, mission, commissione,
delegazione}
Table 1: Sense clusters generated by the WSI method in the three languages.
and three bilingual lexicons are built from the align-
ment results (one for each language pair) containing
intersecting alignments. The lexicons contain noun
translations of each English target word in the three
languages. We keep French translations that trans-
late the target words at least 10 times in the train-
ing corpus; for Spanish and Italian, where the corpus
was smaller, the translation frequency threshold was
set to 5.
For each translation Ti of a word w, we extract the
content words that occur in the same sentence as w
whenever it is translated by Ti. These constitute the
features of the vector built for the translation. Let N
be the number of features retained for each Ti from
the corresponding source contexts. Each feature Fj
(1 ? j ? N ) receives a total weight tw(Fj , Ti) de-
fined as the product of the feature?s global weight,
gw(Fj), and its local weight with that translation,
lw(Fj , Ti). The global weight of a feature Fj is a
function of the number Ni of translations (Ti?s) to
which Fj is related, and of the probabilities (pij) that
Fj co-occurs with instances of w translated by each
of the Ti?s:
gw(Fj) = 1?
?
Ti pij log(pij)
Ni
(1)
Each of the pij?s is computed as the ratio between
the co-occurrence frequency of Fj with w when
translated as Ti, denoted as cooc frequency(Fj , Ti),
and the total number of features (N ) seen with Ti:
pij =
cooc frequency(Fj , Ti)
N
(2)
The local weight lw(Fj , Ti) between Fj and Ti di-
rectly depends on their co-occurrence frequency:
lw(Fj , Ti) = log(cooc frequency(Fj , Ti)) (3)
The pairwise similarity of the translation vectors
is calculated using the Weighted Jaccard Coeffi-
cient (Grefenstette, 1994). The similarity score of
each translation pair is compared to a threshold lo-
cally defined for each w, which serves to distinguish
strongly related translations from semantically un-
related ones. The semantically related translations
of a word w are then grouped into clusters. Trans-
lation pairs with a score above the threshold form a
set of initial clusters that might be further enriched
with other translations through an iterative proce-
dure, provided that there are other translations that
are strongly related to the elements in the cluster.1
The clustering stops when all the translations of w
have been clustered and all their relations have been
checked. The algorithm performs a soft clustering
so translations might be found in different clusters.
Final clusters are characterized by global connectiv-
ity, meaning that all their elements are linked by per-
tinent relations. Table 1 gives examples of clusters
generated for CLWSD target words in the three lan-
guages. The clusters group translations carrying the
same sense and their overlaps describe relations be-
tween senses. The translation clusters serve as the
target words? candidate senses from which one has
to be selected during disambiguation.
1The thresholding procedure and the clustering algorithm
are described in detail in Apidianaki and He (2010).
179
Subtask Metric
Spanish French Italian
LIMSI Baseline
Best
system LIMSI Baseline
Best
system LIMSI Baseline
Best
system
Best
P/R 24,7 23,23 32,16 24,56 25,73 30,11 21,2 20,21 25,66
Mode P/R 32,09 27,48 37,11 22,16 20,19 26,62 23,06 19,88 31,61
OOF
P/R 49,01 53,07 61,69 45,37 51,35 59,8 40,25 42,62 53,57
Mode P/R 51,41 57,34 64,65 39,54 47,42 57,57 47,21 41,68 56,61
OOF P/R 98,6 - - 101,75 - - 90,23 - -
(dupl) Mode P/R 51,41 - - 39,54 - - 47,21 - -
Table 2: Results at the SemEval 2013 CLWSD task.
2.2 Word Sense Disambiguation
The vectors used for clustering the translations also
serve for disambiguating new instances of the tar-
get words in context. The new contexts are tok-
enized, lowercased, PoS tagged and lemmatized to
facilitate comparison with the vectors. We use the
features shared by each pair of clustered transla-
tions, or the vector corresponding to the translation
in an one-element cluster. If no CFs exist between
the new context and a pair of translations, WSD is
performed by comparing context information sep-
arately to the vector of each clustered translation.
Once the common features (CFs) between the vec-
tors and the new context are identified, a score is
calculated corresponding to the mean of the weights
of the CFs with the translations (weights assigned to
the features during WSI). In formula 4, CFj is the
set of CFs and NCF is the number of translations Ti
characterized by a CF.
wsd score =
?NCF
i=1
?
j w(Ti, CFj)
NCF ? |CFj |
(4)
The cluster containing the highest ranked transla-
tion or translation pair is selected and assigned to
the new target word instance. If the translations are
present in more than one clusters, a new score is cal-
culated using equation 4 and by taking into account
the weights of the CFs with the other translations
(Ti?s) in the cluster.
3 Evaluation
Systems participating to the CLWSD task have to
provide the most plausible translation for a word
in context in the best subtask, and five semanti-
cally correct translations in oof. The baselines pro-
vided by the organizers are based on the output of
GIZA++ alignments on Europarl. The best base-
line corresponds to the most frequent translation of
the target word in the corpus and the oof baseline
to the five most frequent translations. Our CLWSD
system makes predictions in three languages for all
1000 test instances. If the selected cluster contains
five translations, all of them are proposed in the
oof subtask while if it is bigger, the five most fre-
quent translations are selected. In case of smaller
clusters, the best translation is repeated in the out-
put until reaching five suggestions. Duplicate sug-
gestions were allowed in previous cross-lingual Se-
mEval tasks as a means to boost translations with
high confidence (Mihalcea et al, 2010). However,
as in this year?s CLWSD task the oof system output
has been post-processed by the organizers to keep
only unique translations, the number of predictions
made by our system for some words has been signif-
icantly reduced. This has had a negative impact on
the oof results, as we will show in the next section.
For selecting best translations, each translation of
a target word w is scored separately by comparing its
vector to the new context. In case the highest-ranked
translation has a score lower than 1, the system falls
back to using the most frequent translation (MFT).
To note that frequency information differs from the
one used in the MFT baseline because words in our
corpus were replaced by a lemma and PoS tag pair
prior to alignment. The discrepancy is more ap-
parent in French where MFT is the most frequent
translation of the target word in the joint Europarl
and JRC-Acquis corpus. Five teams participated to
the CLWSD task with a varying number of systems:
twelve systems provided output for Spanish and ten
for French and Italian.
180
4 Results
The results obtained by our system for the best
and oof evaluations in the three languages (Span-
ish, French and Italian) are presented in Table 2. We
contrast them with the baselines provided by the or-
ganizers and with the score of the system that per-
formed best in each subtask. Our system made sug-
gestions for all test instances, so recall (R) coincides
with precision (P). The baselines are quite challeng-
ing, as noted in Lefever and Hoste (2010), especially
the oof one which contains the five most frequent
Europarl translations. These often correspond to the
most frequent translations from different sense clus-
ters and cover multiple senses of the target word.
Our system outperforms the best baseline in all
languages except for French, where the best score
lies near below the baseline. This is not surprising
given that the training corpus for French is the joint
Europarl and JRC-Acquis corpus, which causes a
discrepancy between the selected best translations
and the baseline. The mode precision and recall
scores reflect the capacity of the system to predict
the translations that were most frequently selected
by the annotators for each instance and are thus con-
sidered as the most plausible ones. Our system out-
performs the mode best baselines for all languages.
In the oof task, the system has been penalized
by the elimination of duplicate translations from
the output after submission. In previous work, the
CLWSD system gave very good results when applied,
with some slight variations, to the out-of-ten subtask
of the SemEval-2010 Cross-Lingual Lexical Substi-
tution task where duplicates served to promote trans-
lations with high confidence (Mihalcea et al, 2010;
Apidianaki, 2011). Here, after the post-processing
step, oof suggestions contain in many cases less than
five translations which explains the low scores. In
Table 2 we provide oof results before and after post-
processing the output and show how the system was
affected by this change in evaluation. By boosting
plausible translations, precision and recall scores get
higher while mode scores are naturally not affected.2
As the other systems might have been impacted to
different extents by this change, we cannot estimate
2Precision scores might be inflated, as in the case of French,
because the credit for each item is not divided by the number of
predictions and the annotation frequencies are used.
how this affects the global system ranking.
5 Discussion and future work
We presented a CLWSD system that uses translation
clusters as candidate senses. Disambiguation is per-
formed by comparing the feature vectors that served
for clustering to the context of new target word in-
stances. We observe that the use of a bigger cor-
pus ? as in the case of French ? not only does not
help in this task but actually has a negative impact
on the results. This is due to the inclusion of transla-
tions that are not present in the gold standard (built
from Europarl) and to the discrepancy between most
frequent translations in the large corpus and the Eu-
roparl MFT baselines. This discrepancy affects all
three languages, as words in the training corpora
were replaced by lemma and PoS tag pairs prior to
alignment.
It is important to note that our CLWSD method ex-
ploits the output of another unsupervised semantic
analysis method (WSI) which groups the translations
into clusters. This is an important feature of the sys-
tem and affects the results in two ways. First, the
translation clusters of a word constitute its candi-
date senses from which the CLWSD method selects
the most appropriate one for a given context. This
means that no variation regarding the contents of a
cluster is permitted and that different instances are
tagged by the same set of translations, contrary to
the gold standard annotations which might, at the
same time, be very close and contain some varia-
tions. In the system output, this is the case only
when overlapping clusters are selected for different
instances. Moreover, given that the WSI method is
automatic and that the clusters are not manually val-
idated, the noise that might be introduced during
clustering is propagated and reflected in the disam-
biguation results. So, if a cluster contains one or
more noisy translations, these occur in the disam-
biguation output and naturally count as wrong pre-
dictions. However, in an application setting like
Machine Translation (MT), the translation clusters
could be filtered using information from the target
language context. Future work will focus on inte-
grating this method into MT systems and examining
ways for optimally taking advantage of CLWSD pre-
dictions in this context.
181
References
Marianna Apidianaki and Yifan He. 2010. An algorithm
for cross-lingual sense clustering tested in a MT eval-
uation setting. In Proceedings of the 7th International
Workshop on Spoken Language Translation (IWSLT-
10), pages 219?226, Paris, France.
Marianna Apidianaki. 2009. Data-driven Semantic
Analysis for Multilingual WSD and Lexical Selection
in Translation. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-09), pages 77?85,
Athens, Greece.
Marianna Apidianaki. 2011. Unsupervised Cross-
Lingual Lexical Substitution. In Proceedings of the
First workshop on Unsupervised Learning in NLP in
conjunction with EMNLP, pages 13?23, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In EMNLP-CoNLL, pages 61?72.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Norwell, MA.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of MT
Summit X, pages 79?86, Phuket, Thailand.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2), ACL 2010, pages
15?20, Uppsala, Sweden.
Els Lefever and Ve?ronique Hoste. 2013. SemEval-2013
Task 10: Cross-Lingual Word Sense Disambiguation.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), in conjunction
with the Second Joint Conference on Lexical and Com-
putational Semantcis (*SEM 2013), pages 63?72, At-
lanta, USA.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
SemEval-2010 Task 2: Cross-Lingual Lexical Sub-
stitution. In Proceedings of the 5th International
Workshop on Semantic Evaluations (SemEval-2), ACL
2010, pages 9?14, Uppsala, Sweden.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proceedings
of the 38th Annual Meeting of the Association for
Computational Linguistics (ACL?00), pages 440?447,
Hongkong, China.
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing Systems and Distinguishing Senses: New Evalua-
tion Methods for Word Sense Disambiguation. Natu-
ral Language Engineering, 5(3):113?133.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz? Erjavec, and Dan Tufis?. 2006.
The JRC-Acquis: A multilingual aligned parallel cor-
pus with 20+ languages. In Proceedings of the 5th
International Conference on Language Resources and
Evaluation (LREC?2006), pages 2142?2147.
182
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 13?23,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Cross-Lingual Lexical Substitution
Marianna Apidianaki
Alpage, INRIA & Univ Paris Diderot
Sorbonne Paris Cite?, UMRI-001
75013 Paris, France
Marianna.Apidianaki@inria.fr
Abstract
Cross-Lingual Lexical Substitution (CLLS) is
the task that aims at providing for a target
word in context, several alternative substitute
words in another language. The proposed
sets of translations may come from external
resources or be extracted from textual data.
In this paper, we apply for the first time an
unsupervised cross-lingual WSD method to
this task. The method exploits the results of
a cross-lingual word sense induction method
that identifies the senses of words by cluster-
ing their translations according to their seman-
tic similarity. We evaluate the impact of using
clustering information for CLLS by applying
the WSD method to the SemEval-2010 CLLS
data set. Our system performs better on the
?out-of-ten? measure than the systems that par-
ticipated in the SemEval task, and is ranked
medium on the other measures. We analyze
the results of this evaluation and discuss av-
enues for a better overall integration of unsu-
pervised sense clustering in this setting.
1 Introduction
Lexical Substitution (LS) aims at providing alterna-
tive substitute words (or phrases) for a target word
in context, a process useful for monolingual tasks
such as paraphrasing and textual entailment (Mc-
Carthy and Navigli, 2009). Its multilingual coun-
terpart, Cross-Lingual Lexical Substitution (CLLS),
aims at finding for a target word in context, alter-
native substitute words in another language. CLLS
systems may assist human translators and language
learners, while their output may constitute the in-
put to cross-language Information Retrieval and Ma-
chine Translation (MT) systems (Sinha et al, 2009;
Mihalcea et al, 2010).
The multilingual context in which CLLS is per-
formed permits to override some issues common to
monolingual semantic processing tasks, such as the
selection of an adequate sense inventory and the def-
inition of the granularity of the semantic descrip-
tions. In a multilingual context, word senses can be
easily identified using their translations in other lan-
guages (Resnik and Yarowsky, 2000). Although this
conception of senses presents some theoretical and
practical drawbacks, it provides a standard criterion
for sense delimitation which explains its wide adop-
tion in recent works on multilingual Word Sense
Disambiguation (WSD) and WSD in MT (Carpuat
and Wu, 2007; Ng and Chan, 2007).
In this paper, we explain how semantic clustering
may provide answers to some of the issues posed
by the traditional cross-lingual sense induction ap-
proach, and how it can be efficiently exploited for
CLLS. Given that existing CLLS systems rely on
predefined semantic resources, we show, for the first
time, that CLLS can be performed in a fully un-
supervised manner. The paper is organized as fol-
lows: in the next section, we present some argu-
ments towards unsupervised clustering for cross-
lingual sense induction. The clustering method used
is presented in section 3. Section 4 describes the
SemEval-2010 CLLS task, and section 5 presents
the cross-lingual WSD method used for CLLS. In
section 6, we proceed to a detailed analysis of the
obtained results, before concluding with some av-
enues for future work.
13
2 Cross-lingual sense induction
2.1 Related work
Word sense induction (WSI) methods offer an alter-
native to the use of predefined semantic resources
for NLP. They automatically define the senses of
words from textual data and may adapt the obtained
descriptions to the WSD needs of specific applica-
tions. In a monolingual context, WSI is performed
by exploiting more or less refined distributional in-
formation (Navigli, 2009), while in a multilingual
context WSI is mostly based on translation informa-
tion. In this setting, the senses of words in one lan-
guage are identified by their translations in another
language, usually found in a parallel corpus (Resnik
and Yarowsky, 2000).
This empirical approach to sense induction of-
fers a standard criterion for sense delimitation and,
consequently, dissociates WSD from semantic theo-
ries and predefined semantic inventories. Moreover,
by establishing semantic distinctions pertinent for
translation between the implicated languages, it al-
lows to tune sense induction to the needs of multilin-
gual applications. It has thus been widely adopted in
works on multilingual WSD and WSD in MT, where
senses are derived from parallel data (Diab, 2003;
Ide, 1999; Ide et al, 2002; Ng et al, 2003; Chan et
al., 2007; Carpuat and Wu, 2007). By linking WSD
and its evaluation to translation, this hypothesis also
offers a solution to the problem of non-conformity
of monolingual WSD methods in this setting.
Nevertheless, the assumption of biunivocal (?one-
to-one?) correspondences between senses and trans-
lations is rather simplistic. One word sense may be
translated by different synonymous words in another
language, whose relatedness should be considered
during sense induction. Furthermore, this approach
does not permit to account for cases of parallel am-
biguities (Resnik, 2007), and cases where the senses
of a word share some of their translations (Sinha et
al., 2009). Additional problems arise at the practical
level as the induced senses are uniform and, so, the
constraints used during WSD for selecting between
close and distant senses are similar. Furthermore,
when WSD coincides with lexical selection in MT,
the selection of a translation different from the refer-
ence is considered as wrong even if it is semantically
correct. So, this conception of senses does not per-
mit to penalize WSD errors relatively to their impor-
tance (Resnik and Yarowsky, 2000), unless semantic
resources are used to identify semantic correspon-
dences.
2.2 Cross-lingual sense clustering
Instead of using translations as straightforward
sense indicators, it is possible to perform a more
thorough semantic analysis during cross-lingual
WSI by combining distributional and translation in-
formation. The sense clustering method proposed by
Apidianaki (2008) identifies complex semantic re-
lations between word senses and their translations.
The method is based on the contextual hypothe-
ses of meaning and of semantic similarity (Harris,
1954; Miller and Charles, 1991), which underlie
monolingual WSI methods, and is combined to the
assumption of a semantic correspondence between
words and their translations in real texts (Chester-
man, 1998). Following these hypotheses, informa-
tion coming from the source contexts of a target
word when translated with a precise translation in
a parallel corpus, is used to reveal the senses carried
by the translation. Furthermore, the similarity of the
source contexts reveals the semantic relatedness of
the translations.
This cross-lingual WSI method groups the seman-
tically similar translations of ambiguous words into
clusters that serve to describe their senses instead
of the individual translations. For instance, the tra-
ditional cross-lingual WSI approach would propose
three senses for the English noun coach, correspond-
ing to each of its Spanish translations: entrenador,
autocar and autobu?s.1 However, this solution is not
sound given that the translations autocar and au-
tobu?s are semantically related and do not lexical-
ize distinct senses of the English word, as is the
case with entrenador. Sense clustering permits to
estimate the semantic similarity of the translations
and to not consider synonymous translations as in-
dicators of distinct senses. Consequently, the En-
glish word coach has two senses after sense cluster-
ing: one described by the cluster {autocar, autobu?s}
(the ?bus? sense) and one described by the cluster
{entrenador} (the ?trainer? sense). In the automat-
1This set of translations was extracted from the word aligned
Europarl corpus (Koehn, 2005) after applying a set of filters that
will be described in section 3.
14
ically built bilingual inventories, the senses of the
words in one language are thus described by clus-
ters of their translations in another language.
2.3 Applications
This type of sense clustering has proved to be use-
ful in various application settings. When exploited
in cross-lingual WSD, it permits to assign ?sense-
tags? containing several semantically correct trans-
lations to new instances of words in context (Apid-
ianaki, 2009). Moreover, the use of clustering in-
formation during evaluation allows for a differing
penalization of WSD errors. In an MT evaluation
setting, sense clusters have been integrated into an
MT evaluation metric (METEOR) (Lavie and Agar-
wal, 2007) and brought about an increase of the met-
ric?s correlation with human judgments of transla-
tion quality in different languages (Apidianaki and
He, 2010). The use of sense clusters in this set-
ting permits to identify semantic correspondences
between translations and hypotheses, and to circum-
vent the strict requirement for exact surface corre-
spondences, one of the main critics addressed to MT
evaluation metrics. The same notion of sense clus-
ters has been adopted in the most recent SemEval
Cross-Lingual WSD task (Lefever and Hoste, 2010).
Instead of considering translations as indicators of
distinct senses, as was the case in previous tasks, the
senses of a small number of ambiguous words were
described by manually created clusters of transla-
tions.
We consider that the sense cluster inventories cre-
ated by the unsupervised WSI method proposed by
Apidianaki (2008) would be useful in other applica-
tive contexts as well and, especially, in CLLS. In
unsupervised cross-lingual WSD, the clusters con-
stitute the candidate senses from which one has to
be selected for each new instance of the words in
context. So, when an instance of a word is dis-
ambiguated, a cluster of semantically related trans-
lations is selected on the basis of the source con-
text describing its sense. This is exactly the goal
of CLLS, as described in the relevant task set up
in SemEval-2010, where the systems had to provide
for instances of words in context, several possible
translations in another language (Sinha et al, 2009;
Mihalcea et al, 2010). It seems thus that CLLS con-
stitutes a suitable field for exploiting this sense clus-
tering method and, in what follows, we will try to
evaluate this assumption.
3 Unsupervised clustering for sense
induction
3.1 Bilingual lexicons
The SemEval-2010 CLLS task concerned the pair
of languages English (EN) - Spanish (SP). In or-
der to apply our cross-lingual WSD method to the
data of the SemEval-2010 CLLS task, an EN-SP
sense cluster inventory had first to be built where
the senses of English words would be described by
clusters of their Spanish translations. The training
corpus used for building the sense cluster inventory
is the SP-EN part of Europarl (release v5), which
contains 1,689,850 aligned sentence pairs (Koehn,
2005). Before clustering, some preprocessing steps
are performed. First, the corpus is lemmatized and
tagged by POS (Schmid, 1994). Then sentence pairs
presenting a great difference in length (i.e cases
where one sentence is three times longer than the
other) are eliminated and the corpus is aligned at
the level of word types using Giza++ (Och and Ney,
2003).
Two bilingual lexicons of content words are built
from the alignment results, one for each translation
direction (EN-SP/SP-EN). In the entries of these lex-
icons, source words are associated with the transla-
tions to which they are aligned. As these lexicons
are automatically created, they contain some noise
mainly due to spurious word alignments. In order to
eliminate erroneous translation correspondences, we
first apply a filter which discards translations with
a probability below 0.001 (according to the scores
assigned during word alignment). Then an intersec-
tion filter is applied which discards correspondences
not found in lexicons of both directions. Finally, the
two lexicons are filtered by POS, keeping for each w
only its translations that pertain to the same POS cat-
egory.2 The translations of a word (w) used for clus-
tering are the ones that translate w at least 20 times in
the training corpus. This frequency threshold leaves
out some translations of the source words but has
a double merit: it eliminates erroneous translations
2For instance, for English nouns we retain their noun trans-
lations in Spanish; for verbs, we keep verbs, etc.
15
and reduces data sparseness issues which pose prob-
lems in distributional semantic analysis.
3.2 Clustering based on semantic similarity
The semantic clustering is performed in the target
language by using source language feature vectors.
Each translation of a word w is characterized by
a vector built from the content words that cooccur
with w whenever it is translated by this word in
the aligned sentences of the training corpus.3 The
vector similarity is calculated using a variation of
the Weighted Jaccard measure (Grefenstette, 1994)
which weighs each source context feature according
to its relevance for the estimation of the translations
similarity.
The input of the similarity calculation consists of
the frequency lists of w?s translations. The score as-
signed to a pair of translations indicates their degree
of similarity. Each feature (j) gets a total weight (tw)
relatively to a translation (i), which corresponds to
the product of its global (gw) and its local weight
(lw) with this translation. The gw is based on the dis-
persion of j in the contexts of w, and on its frequency
of cooccurrence (cooc freq) with w when translated
by each i (cf. formula 1). So, it depends on the num-
ber of translations with which j is related (nrels) and
on its probability of cooccurrence with each one of
them (cf. formula 2). The local weight (lw) between
j and i depends on their frequency of cooccurrence
(cf. formula 3).
gw(j) = 1 ?
?
i pij log(pij)
nrels
(1)
pij =
cooc freq of j with i
|js| for i (2)
lw(j, i) = log(cooc freq of j with i) (3)
The Weighted Jaccard (WJ) coefficient of two trans-
lations m and n is given by formula 4.
WJ(m,n) =
?
j min(tw(m, j)tw(n, j))
?
j max(tw(m, j)tw(n, j))
(4)
The pairwise similarity of the translations is thus es-
timated by comparing the corresponding weighted
3We use a stoplist of English function words (conjunctions,
prepositions and articles) that may be erroneously tagged as
content words.
source feature vectors. A similarity score is assigned
to each pair of translations and stored in a table that
is being looked up by the clustering algorithm. The
pertinence of the relation of each translation pair is
estimated by comparing its score to a threshold de-
fined locally for each w by the following iterative
procedure.
1. The initial threshold (T) corresponds to the mean of
the scores (above 0) of the translation pairs of w.
2. The set of translations is segmented into pairs
whose score exceeds the threshold and pairs whose
score is inferior to the threshold, creating two sets
(G1, G2).
3. The average of each set is computed (m1 = average
value of G1, m2 = average value of G2).
4. A new threshold is created that is the average of m1
and m2 (T = (m1 + m2)/2).
5. Go back to step 2, now using the new threshold
computed in step 4, keep repeating until conver-
gence has been reached.
The clustering algorithm groups the translations
by exploiting the similarity calculation results. The
condition for a translation to be included in a cluster
is to have pertinent relations with all the elements
already in the cluster. The clustering stops when all
the translations of w are included in some cluster and
all their relations have been checked. All the ele-
ments of the final clusters are linked to each other by
pertinent relations. The translations not having any
strong relations to other translations are included in
separate one-element clusters.
3.3 The EN-SP sense cluster inventory
In the obtained semantic inventory, the senses of
each English word are described by clusters of its
semantically similar translations in Spanish.4 Some
entries from the EN-SP sense cluster inventory are
presented in Table 1. We provide examples for
words of different POS (nouns, verbs, adjectives and
adverbs) and with varying degrees of polysemy. The
4The inventory contains entries for all English content words
in the corpus. Here, we focus on the target words used in the
CLLS task.
16
POS EN word # SP Ts # occ Sense clusters
Nouns coach 3 265 {entrenador}{autocar, autobu?s}
test 11 3162
{prueba, ensayo, examen} {experimento, ana?lisis, examen, ensayo}
{evaluacio?n} {comprobacio?n} {experimentacio?n, ensayo, ana?lisis, ex-
perimento} {inspeccio?n} {experimento, control, ana?lisis, examen}
{experimentacio?n, control, ana?lisis, experimento} {criterio}
Verbs drop 10 390
{disminuir, reducir, bajar, caer, descender} {retirar} {dejar, abandonar}
{lanzar}
check 5 1343 {examinar} {revisar} {controlar, verificar, comprobar}
Adjs heavy 7 448
{elevado, fuerte, grave, grande}{elevado, enorme}{grave, duro, fuerte,
grande} {grave, alto, elevado}
open 6 6286 {pu?blico, libre, transparente} {pu?blico, franco, transparente} {abierto}{sincero, franco}
Advs around 5 742 {alrededores}{casi, aproximadamente, cerca}{menos}
now 9 33662
{aqu??, actualmente, hoy, ahora bien} {actualmente, ahora, hoy}
{entretanto, aqu??, ahora bien} {de momento}, {adelante}, {por ahora, en-
tretanto}
Table 1: Entries from the EN-SP sense cluster inventory.
third column of the table gives the number of Span-
ish words (SP Ts) translating more than 20 occur-
rences of the English words in the corpus and re-
tained for clustering. This threshold ensures that
the words being clustered are good translations of
the English words. The fourth column of the ta-
ble shows the number of English word occurrences
translated by the retained translations.
As is shown in these examples, the translations
of the English words are not considered as straight-
forward indicators of their senses but are grouped
into clusters describing senses. For instance, the
word drop, which is translated by ten different words
into Spanish (disminuir, reducir, bajar, caer, descen-
der, retirar, dejar, abandonar, lanzar) is not con-
sidered as having ten distinct senses but four, de-
scribed by each cluster of translations: {disminuir,
reducir, bajar, caer, descender}: ?decrease, reduce?,
{retirar}: ?remove, withdraw?, {dejar, abandonar}:
?leave, abandon? and {lanzar}: ?launch?. The
obtained clusters group semantically similar words
which would be erroneously considered as indica-
tors of distinct senses by the traditional cross-lingual
sense induction method.
Another important point is that this algorithm
performs a soft clustering, highly adequate in this
setting. Given that the generated clusters de-
scribe senses, their overlaps describe the relations
between the corresponding senses. For instance,
the two senses of the word test described by the
clusters {experimentacio?n, control, ana?lisis, exper-
imento} and {experimento, control, ana?lisis, exa-
men} share three elements and are closer than those
described by {experimentacio?n, control, ana?lisis,
experimento} and {evaluacio?n}, which have no ele-
ment in common. The first two senses could also be
considered as nuances of a coarser sense (?examina-
tion / analysis?) that could be obtained by merging
the overlapping clusters. Capturing inter-sense re-
lations is important in lexical semantics and numer-
ous works have been criticized for just enumerating
word senses without describing their relations. Dis-
covering these links automatically, as is done with
this sense clustering method, permits to account for
differences in the status of senses during WSD and
its evaluation. It also offers the possibility to au-
tomatically modify the granularity of the obtained
senses according to the WSD needs of the applica-
tions. Moreover, when the sense cluster inventory
is used for cross-lingual WSD, it allows to capture
subtle relations between word usages in cases where
the senses of a word share some of their translations
but not all of them, an issue highlighted in the Se-
mEval CLLS task (Sinha et al, 2009) which will be
presented in the next section.
17
4 The SemEval-2010 CLLS task
In the SemEval-2010 Cross-Lingual Lexical Substi-
tution task, annotators and systems had to provide
several alternative correct translations in Spanish for
English target words in context. Given a paragraph
containing an instance of an English target word, the
annotators had to find as many good substitute trans-
lations as possible for that word in Spanish. Unlike
a full-blown MT task, CLLS targets one word at a
time rather than an entire sentence. So, annotators
were asked to translate the target word and not en-
tire sentences. Moreover, they were asked to supply,
for each instance, as many translations as they felt
were valid and not just one translation, which would
be the case in MT.
The task of the participating systems was then to
predict the translations provided by the annotators
for each target word instance. By analyzing the con-
text of the English target word instances, the sys-
tems had to provide for each instance, several cor-
rect Spanish translations which should fit the given
source language context. The set of target words
in the SemEval CLLS task is composed of Nouns,
Verbs, Adjectives and Adverbs exhibiting a wide va-
riety of substitutes. The annotators were allowed to
use any resources they wanted to in order to supply
substitutes for instances of the English target words.
So, instances of the target words in context were
tagged by sets of Spanish translations.5 The inter-
annotator agreement for this task was calculated as
pairwise agreement between sets of substitutes from
annotators and corresponds to 0.2777.
The sets of translations provided for different in-
stances of a target word could overlap in different
degrees, depending on the meaning of the instances.
These overlaps reveal subtle relations between word
usages in cases where they share some of their trans-
lations but not all of them (Sinha et al, 2009). This
also shows the absence of clear divisions between
usages and senses: usages overlap to different ex-
tents without having identical translations. Although
no clustering of translations from a specific resource
into senses was performed for this task, the interest
of examining the possibility of clustering the transla-
5The average numbers of substitutes provided by the anno-
tators for words of different POS are: 4.47 for nouns, 5.2 for
verbs, 4.99 for adjectives and 4.77 for adverbs.
tions provided by the annotators is highlighted (Mi-
halcea et al, 2010).
5 Cross-lingual WSD
The source language features that revealed the sim-
ilarity of the translations and served to their cluster-
ing (cf. section 3) can be exploited by an unsuper-
vised WSD classifier (Apidianaki, 2009). In order
to disambiguate a new instance of an English word
w, cooccurrence information coming from its con-
text is compared to these feature sets and the clus-
ter that has the highest similarity with the new con-
text is selected. We adopt this WSD method in or-
der to exploit the sense clustering results and per-
form CLLS in an unsupervised manner. Instead of
comparing the new contexts to the features that are
common to all the translations in a cluster (i.e. the
intersection of their source language features), as is
done in the initial method, we compare them to the
features shared by each pair of translations. This in-
creases the coverage of the method, given that these
source features sets are larger than the ones contain-
ing the intersection of the features of all the clus-
tered translations. As the training corpus was lem-
matized and POS-tagged prior to building the fea-
ture vectors (only content word cooccurrences were
retained), the new contexts have to be lemmatized
and POS-tagged as well.
If common features (CFs) are found between the
new context and a translation pair, a score is as-
signed to this ?context-pair? association which cor-
responds to the mean of the weights of the CFs rel-
atively to each translation of the pair. The weights
used here are the total weights (tws) that were as-
signed to the context features relatively to the trans-
lations during the semantic similarity calculation (cf.
section 3.2). In formula 5, i is equal to 2 (i.e. the
number of translations in the pair) and j is the num-
ber of CFs between the translation pair and the new
context.
If the highest-ranked translation pair is found in
just one sense cluster, this cluster is selected as de-
scribing the sense of the new instance. Otherwise,
if the translation pair is found in different clusters,
it is checked whether the CFs characterize the other
translations in these clusters (or some of them). If
this is the case, a score is assigned to each cluster
18
Test instance WSD suggestion Gold annotation
test.n 1698 prueba;ensayo;examen; examen 4;prueba 4;test 1;
board.n 1781 consejo;bordo;junta;comite?;cuenta;
administracio?n;
junta directiva 2;consejo 2;mesa directiva 1;junta
1;junta de ayuda 1;directiva 1;comite 1;comision 1;
drop.v 1288 bajar;disminuir;reducir;caer;descender dejar caer 2;tirar 1;arrojar 1;lanzar 1;soltar 1;dejar1;bajar 1;
check.v 851 comprobar;controlar;verificar; verificar 3;checar 2;confirmar 1;anotar 1;rectificar1;revisar 1;comprobar 1;
yet.r 1766 todav??a;au?n;sin embargo; sin embargo 2;pero 2;no obstante 1;aun 1;todavia 1;
now.r 1019 hoy;aqu??;actualmente;ahora bien; hoy 2;ahora 2;este momento 2;a partir 1;el presente1;de aqui 1;
Table 2: Clusters suggested by the WSD method.
depending on the weights of the features with the
other translations, and the cluster with the highest
score is selected as describing the sense of the new
instance. The score is again calculated by formula 5
but this time i is equal to the number of translations
in the cluster having CFs with the new context.
score =
?
i
?
j tw(i, j)
i ? j (5)
If no CFs are found using the translation pairs, the
WSD algorithm considers each translation?s feature
set separately (which is naturally larger than the fea-
ture sets of the translation pairs). If CFs exist, the
translation with the highest score is selected as well
as the cluster containing it. If the translation is
found in the intersection of different clusters, it is
checked whether the CFs characterize some of the
other translations found in the clusters. If this is the
case, a score is assigned to the clusters depending on
the weights of the features with the translations and
the cluster with the highest score is selected. The
cluster containing the translation pair with the high-
est similarity to the new context is retained as the
sense of the new instance. If no CFs are found in
this way neither, a most frequent sense heuristic is
used which selects the most frequent cluster (i.e. the
one assigned to most of the new instances of w).
For the 1000 test instances in the SemEval CLLS
task, the WSD method proposes 625 clusters with
more than one element and 118 one element clus-
ters.6 The most frequent translation is suggested in
6262 clusters with two elements; 157 clusters with three; 73
with four; 64 with five; 69 clusters with more than five and less
210 cases while the most frequent cluster is chosen
in 43 cases. A cluster is chosen randomly only in
3 cases. In Table 2, we present some suggestions
made by the WSD method for target words of dif-
ferent POS (n: nouns, v: verbs, a: adjectives, r: ad-
verbs) and the corresponding gold standard (GS) an-
notations. For instance, the following occurrence of
the English noun test:
Entries typically identify the age or school grade lev-
els for which the test is appropriate, as well as any
subtests.
is tagged by the Spanish cluster {prueba, examen,
ensayo} during WSD, which is close to the GS an-
notation {examen, prueba, test} and correctly de-
scribes its sense.
The first translation provided in the results is the
word of the cluster that translates most of the En-
glish target word instances in the corpus (and which
is duplicated in order to be reinforced during the
?out-of-ten? evaluation, as we will explain in the next
section). We observe that this most frequent word,
although it is a correct translation (i.e. found in the
GS annotations), does not coincide with the annota-
tors? first choice. This explains the evaluation results
that we present in the next section.
It is also important to note that the system sug-
gests not only translations that have been proposed
by the annotators, but also other semantically perti-
nent translations that were found in the training cor-
pus and which do not exist in the GS annotations.
This is the case, for instance, with the translation
than ten elements; 23 clusters with ten elements and 22 clusters
with more than ten elements.
19
?controlar? of the verb check and the translation ?en-
sayo? proposed for the noun test. This shows that
the suggestions made by the WSD method greatly
depend on the corpus used for training.
6 Evaluation
6.1 The setting
We evaluate our method on the SemEval-2010
CLLS task test set. The metrics used for evalua-
tion are the best and out-of-ten (oot) precision (P)
and recall (R) scores. In the SemEval task, the sys-
tems were allowed to supply as many translations as
they felt fit the context. These suggestions were then
given credit depending on the number of annotators
that had picked each translation. The credit was di-
vided by the number of annotator responses for the
item. For the best score, the credit for the system an-
swers for an item was also divided by the number of
answers provided by the system, which allows more
credit to be given to instances with less variation.
The oot scorer allows up to ten system responses
and does not divide the credit attributed to each
answer by the number of system responses. This
scorer allows duplicates which means that systems
can get inflated scores (i.e. > 100), as the credit
for each item is not divided by the number of substi-
tutes and the frequency of each annotator response
is used. Allowing duplicates permits that the sys-
tems boost their scores with duplicates on transla-
tions with higher probability.7
Two baselines are used for evaluation: a
dictionary-based one (DICT), which contains the
Spanish translations of all target words provided by
an SP-EN dictionary, and a dictionary and corpus-
based one (DICTCORP), where the translations pro-
vided by the dictionary for a given target word are
ranked according to their frequencies in the Spanish
Wikipedia. In DICT, the best baseline is produced
by taking the first translation provided by the dic-
tionary while the oot baseline considers the first ten
translations.
6.2 Results
In order to evaluate our WSD method, we proceed as
follows. If the cluster selected by the WSD method
7The metrics used for evaluation are defined in Mihalcea et
al. (2010).
contains ten translations (or more), all the transla-
tions are given in the oot results. Otherwise, the
translations found in the cluster are proposed and the
most frequent translation is duplicated till reaching
ten elements. For best, we always retain the most
frequent translation of the selected cluster.
Our intuition was that the WSD method, which
assigns sense clusters (i.e. sets of semantically sim-
ilar and, more or less, substitutable translations),
would fit and perform well on the oot subtask of the
SemEval CLLS task. This is confirmed by the re-
sults presented in Table 3.8 Our method (denoted
by ?WSD? in the table) outperforms the 14 systems
that participated in the CLLS task as well as the re-
call (R) and precision (P) baselines. It is important
to note that, contrary to our method which is totally
unsupervised, all the systems that participated in the
SemEval-2010 task used predefined resources. The
second ranked system (SWAT-E), for instance, per-
forms lexical substitution in English and then trans-
lates each substitute into Spanish using two prede-
fined bilingual dictionaries, while SWAT-S does the
inverse, performing lexical substitution in the trans-
lated text (Wicentowski et al, 2010).
Systems R P Mode R Mode P
WSD 180.10 186.25 56.52 58.44
SWAT-E 174.59 174.59 66.94 66.94
SWAT-S 97.98 97.98 79.01 79.01
UvT-v 58.91 58.91 62.96 62.96
UvT-g 55.29 55.29 73.94 73.94
DICT 44.04 44.04 73.53 73.53
DICTCORP 42.65 42.65 71.60 71.60
Table 3: oot results (%)
Another interesting point is that the sense cluster
inventory used by the cross-lingual WSD method is
derived from Europarl, which is the European Par-
liament Proceedings parallel corpus (Koehn, 2005).
Despite this fact, the WSD method that exploits
this inventory performs particularly well on this task
which concerns the semantic analysis and transla-
tion of words of general language. We would thus
expect the results to be even better if the sense induc-
8We report the results obtained by the highest-ranked sys-
tems in the SemEval-2010 CLLS task. The full table of results
can be found in Mihalcea et al (2010).
20
tion and the WSD method were trained on a bigger,
or more general, parallel corpus.
The mode recall and precision (Mode R and Mode
P) metrics evaluate the performance of the systems
in predicting the translation that was most frequently
selected by the annotators, provided that such a
translation extists. To identify the most frequent re-
sponse, we order the system responses according to
their frequency as translations of the target words in
the training corpus. The relatively low scores ob-
tained for the Mode R and Mode P metrics (com-
pared to R and P) are explained by the fact that the
most frequent translation in the training corpus does
not always correspond to the translation that was
most frequently selected by the annotators, although
it may be a good translation for the target word.
The same reason explains the weaker perfor-
mance of the method in the best evaluation subtask
(cf. Table 4), where our system is ranked eighth
compared to the 14 systems that participated in the
task.9 Here too, the best translation according to the
annotators does not correspond to the most frequent
translation in the corpus. This highlights the impact
that the relevance of the training corpus to the do-
mains of the processed texts has on unsupervised
CLLS.
Systems R P Mode R Mode P
UBA-T 27.15 27.15 57.20 57.20
USPWLV 26.81 26.81 58.85 58.85
WLVUSP 25.27 25.27 52.81 52.81
WSD 19.73 19.93 41.29 41.75
UBA-W 19.68 19.68 39.09 39.09
SWAT-S 18.87 18.87 36.63 36.63
IRST-1 15.38 22.16 33.47 45.95
TYO 8.39 8.62 14.95 15.31
DICT 24.34 24.34 50.34 50.34
DICTCORP 15.09 15.09 29.22 29.22
Table 4: best results (%)
Another important factor that has to be taken into
account is that the WSD method that we use is ori-
ented towards multilingual applications (more pre-
cisely MT). In these applications, it is possible to
filter the proposed sense clusters by reference to the
9We report some indicative results from the best subtask.
The full table of results can be found in Mihalcea et al (2010).
target language context (for instance, by using a lan-
guage model) in order to retain the most adequate
translation. It is interesting to note that the systems
that perform better in the best subtask get relatively
low results in the oot subtask, and the inverse. This
is the case, for instance, for UBA-T (Basile and Se-
meraro, 2010), while Aziz and Specia (2010) clearly
specify that their main goal is to maximize the accu-
racy of their system (USPwlv) in choosing the best
translation. A conclusion that can be drawn is that
each subtask has different requirements, which may
be satisfied by different types of methods.
In order to investigate other possible reasons be-
hind the different behavior of the WSD method in
the two evaluation subtasks, we performed the eval-
uation separately for each POS. The results are pre-
sented in Tables 5 and 6.
POS R P Mode R Mode P
Adjs 287.94 296.41 72.44 74.43
Nouns 127.01 141.65 37.78 42.29
Verbs 115.94 121.43 53.17 55.90
Advs 111.46 111.46 65.15 65.15
Table 5: oot results for different POS (%)
POS R P Mode R Mode P
Adjs 30.77 31.00 63.56 64.13
Nouns 14.61 16.29 25.78 28.86
Verbs 14.98 14.98 29.76 29.76
Advs 13.07 13.07 37.88 37.88
Table 6: best results for different POS (%)
In both the oot and best evaluation subtasks, the
best scores are obtained for adjectives. Especially in
the best subtask, where the method seemed to per-
form worse than the other systems, the recall and
precision scores obtained for adjectives (with and
without mode) are higher than those obtained by the
highest-ranked system (cf. Table 4) and much higher
than the baselines. A more detailed look at the ob-
tained results proved that the most frequent transla-
tion of the English adjectives in our training corpus ?
proposed in the best evaluation subtask and empha-
sized in the oot subtask ? is often the most frequent
translation proposed by the annotators. This is not
the case for the other POS, where the most frequent
21
translation in the corpus often does not correspond
to the annotators? first choice. Furthermore, the
translation proposed by the system is not the same
as the most frequent translation of the word in the
general dictionary and the Spanish Wikipedia which
were used, respectively, for the DICT and DICT-
CORP baselines. Consequently, this issue could
probably be resolved if a more balanced corpus was
used for training the WSI and WSD methods.
7 Conclusions and future work
We have shown that Cross-Lingual Lexical Substi-
tution can be performed in a totally unsupervised
manner, if a parallel corpus is available. We applied
an unsupervised cross-lingual WSD method based
on semantic clustering to the SemEval-2010 CLLS
task. The method performs well compared to the
systems that participated in the task, which exploit
predefined lexico-semantic resources. It is ranked
first on the out-of-ten measure and medium on mea-
sures that concern the choice of the best translation.
We wish to pursue this work and explore other ways
for selecting best translations than solely relying on
frequency information. As unsupervised methods
heavily rely on the training data, it would also be
interesting to experiment with different corpora in
order to evaluate the impact of the type and the size
of the corpus on CLLS.
The sense clusters assigned to target word in-
stances during CLLS contain semantically similar
translations of these words, more or less substi-
tutable in the target language context. We consider
that it would be interesting to integrate target lan-
guage information in the CLLS decision process for
selecting best translations. Given that MT is one of
the envisaged applications for this type of task, but
the use of a full-blown MT system would probably
mask system capabilities at a lexical level, a possi-
bility would be to exploit the CLLS system sugges-
tions in a simplified MT task such as word transla-
tion (Vickrey et al, 2005) or lexical selection (Apid-
ianaki, 2009), or in an MT evaluation context. This
would permit to estimate the usefulness of the sys-
tem suggestions in a specific application setting.
References
Marianna Apidianaki and Yifan He. 2010. An algorithm
for cross-lingual sense clustering tested in a MT eval-
uation setting. In Proceedings of the 7th International
Workshop on Spoken Language Translation (IWSLT-
10), pages 219?226, Paris, France.
Marianna Apidianaki. 2008. Translation-oriented sense
induction based on parallel corpora. In Proceedings
of the 6th International Conference on Language Re-
sources and Evaluation (LREC-08), pages 3269?3275,
Marrakech, Morocco.
Marianna Apidianaki. 2009. Data-driven Semantic
Analysis for Multilingual WSD and Lexical Selection
in Translation. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-09), pages 77?85,
Athens, Greece.
Wilker Aziz and Lucia Specia. 2010. USPwlv and
WLVusp: Combining Dictionaries and Contextual In-
formation for Cross-Lingual Lexical Substitution. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluations (SemEval-2), ACL 2010, pages
117?122, Uppsala, Sweden.
Pierpaolo Basile and Giovanni Semeraro. 2010. UBA:
Using Automatic Translation and Wikipedia for Cross-
Lingual Lexical Substitution. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2), ACL 2010, pages 242?247, Uppsala,
Sweden.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Disam-
biguation. In Proceedings of the Joint EMNLP-CoNLL
Conference, pages 61?72, Prague, Czech Republic.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-07), pages 33?40, Prague, Czech Republic.
Andrew Chesterman. 1998. Contrastive Functional
Analysis. John Benjamins Publishing Company, Ams-
terdam/Philadelphia.
Mona Diab. 2003. Word sense disambiguation within
a multilingual framework. Ph.D. dissertation, Univer-
sity of Maryland.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Norwell, MA.
Zelig Harris. 1954. Distributional structure. Word,
10:146?162.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense
discrimination with parallel corpora. In Proceedings
of the ACL Workshop on Word Sense Disambiguation:
22
Recent Successes and Future Directions, pages 54?60,
Philadelphia.
Nancy Ide. 1999. Cross-lingual sense determination:
Can it work? Computers and the Humanities, 34(1-
2):223?234.
Philip Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of MT
Summit X, pages 79?86, Phuket, Thailand.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
Automatic Metric for MT Evaluation with High Levels
of Correlation with Human Judgments. In Proceed-
ings of the ACL-2007 Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Republic.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2), ACL 2010, pages
15?20, Uppsala, Sweden.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation Special Issue on Computational Se-
mantic Analysis of Language: SemEval-2007 and Be-
yond, 43(2):139?159.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
SemEval-2010 Task 2: Cross-Lingual Lexical Sub-
stitution. In Proceedings of the 5th International
Workshop on Semantic Evaluations (SemEval-2), ACL
2010, pages 9?14, Uppsala, Sweden.
George A. Miller and Walter G. Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1?28.
Roberto Navigli. 2009. Word Sense Disambiguation: a
Survey. ACM Computing Surveys, 41(2):1?69.
Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-
2007 Task 11: English lexical sample task via
English-Chinese parallel text. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-2007), pages 54?58, Prague, Czech Repub-
lic.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003.
Exploiting Parallel Texts for Word Sense Disambigua-
tion: An Empirical Study. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 455?462, Sapporo, Japan.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing Systems and Distinguishing Senses: New Evalua-
tion Methods for Word Sense Disambiguation. Natu-
ral Language Engineering, 5(3):113?133.
Philip Resnik. 2007. WSD in NLP applications. In
Eneko Agirre and Philip Edmonds, editors, Word
Sense Disambiguation: Algorithms and Applications,
pages 299?337, Dordrecht. Springer.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Ravi Sinha, Diana McCarthy, and Rada Mihalcea. 2009.
SemEval-2010 Task 2: Cross-Lingual Lexical Substi-
tution. In Proceedings of the NAACL-HLT Workshop
SEW-2009 - Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 76?81, Boulder,
Colorado.
David Vickrey, Luke Biewald, Marc Teyssier, and
Daphne Koller. 2005. Word-Sense Disambiguation
for Machine Translation. In Proceedings of the Joint
Conference on Human Language Technology / Empir-
ical Methods in Natural Language Processing (HLT-
EMNLP), pages 771?778, Vancouver, Canada.
Richard Wicentowski, Maria Kelly, and Rachel Lee.
2010. SWAT: Cross-Lingual Lexical Substitution
using Local Context Matching, Bilingual Dictionar-
ies and Machine Translation. In Proceedings of the
5th International Workshop on Semantic Evaluations
(SemEval-2), ACL, pages 123?128, Uppsala, Sweden.
23
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 330?337,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIMSI @ WMT?12
Hai-Son Le1,2, Thomas Lavergne2, Alexandre Allauzen1,2,
Marianna Apidianaki2, Li Gong1,2, Aure?lien Max1,2,
Artem Sokolov2, Guillaume Wisniewski1,2, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submissions to
the shared translation task. We report results
for French-English and German-English in
both directions. Our submissions use n-code,
an open source system based on bilingual
n-grams. In this approach, both the transla-
tion and target language models are estimated
as conventional smoothed n-gram models; an
approach we extend here by estimating the
translation probabilities in a continuous space
using neural networks. Experimental results
show a significant and consistent BLEU im-
provement of approximately 1 point for all
conditions. We also report preliminary experi-
ments using an ?on-the-fly? translation model.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Seventh Workshop
on Statistical Machine Translation. LIMSI partic-
ipated in the French-English and German-English
tasks in both directions. For this evaluation, we
used n-code, an open source in-house Statistical
Machine Translation (SMT) system based on bilin-
gual n-grams1. The main novelty of this year?s
participation is the use, in a large scale system, of
the continuous space translation models described
in (Hai-Son et al, 2012). These models estimate the
n-gram probabilities of bilingual translation units
using neural networks. We also investigate an alter-
native approach where the translation probabilities
of a phrase based system are estimated ?on-the-fly?
1http://ncode.limsi.fr/
by sampling relevant examples, instead of consider-
ing the entire training set. Finally we also describe
the use in a rescoring step of several additional fea-
tures based on IBM1 models and word sense disam-
biguation information.
The rest of this paper is organized as follows. Sec-
tion 2 provides an overview of the baseline systems
built with n-code, including the standard transla-
tion model (TM). The continuous space translation
models are then described in Section 3. As in our
previous participations, several steps of data pre-
processing, cleaning and filtering are applied, and
their improvement took a non-negligible part of our
work. These steps are summarized in Section 5.
The last two sections report experimental results ob-
tained with the ?on-the-fly? system in Section 6 and
with n-code in Section 7.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o et al,
2006; Crego and Marin?o, 2006). In this framework,
translation is divided in two steps: a source reorder-
ing step and a (monotonic) translation step. Source
reordering is based on a set of learned rewrite rules
that non-deterministically reorder the input words.
Applying these rules result in a finite-state graph of
possible source reorderings, which is then searched
for the best possible candidate translation.
2.1 Features
Given a source sentence s of I words, the best trans-
lation hypothesis t? is defined as the sequence of J
words that maximizes a linear combination of fea-
330
ture functions:
t? = argmax
t,a
{
M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature func-
tion hm and a denotes an alignment between source
and target phrases. Among the feature functions, the
peculiar form of the translation model constitute one
of the main difference between the n-gram approach
and standard phrase-based systems. This will be fur-
ther detailled in section 2.2 and 3.
In addition to the translation model, fourteen
feature functions are combined: a target-language
model (Section 5.3); four lexicon models; six lexi-
calized reordering models (Tillmann, 2004; Crego
et al, 2011) aiming at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two lexi-
cal weights are estimated from the automatic word
alignments. The weights vector ? is learned us-
ing a discriminative training framework (Och, 2003)
(Minimum Error Rate Training (MERT)) using the
newstest2009 as development set and BLEU (Pap-
ineni et al, 2002) as the optimization criteria.
2.2 Standard n-gram translation models
n-gram translation models rely on a specific de-
composition of the joint probability of a sentence
pair P (s, t): a sentence pair is assumed to be
decomposed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL2. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing obtained by ?unfolding? initial word alignments.
In this framework, the basic translation units are
tuples, which are the analogous of phrase pairs and
represent a matching u = (s, t) between a source
s and a target t phrase (see Figure 1). Using the
n-gram assumption, the joint probability of a seg-
2From now on, (s, t) thus denotes an aligned sentence pair,
and we omit the alignment variable a in further developments.
mented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
During the training phase (Marin?o et al, 2006), tu-
ples are extracted from a word-aligned corpus (us-
ing MGIZA++3 with default settings) in such a
way that a unique segmentation of the bilingual
corpus is achieved. A baseline n-gram translation
model is then estimated over a training corpus com-
posed of tuple sequences using modified Knesser-
Ney Smoothing (Chen and Goodman, 1998).
2.3 Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, only those reorder-
ing hypotheses are translated and they are intro-
duced using a set of reordering rules automatically
learned from the word alignments.
In the example in Figure 1, the rule [prix no-
bel de la paix ; nobel de la paix prix] repro-
duces the invertion of the French words that is ob-
served when translating from French into English.
Typically, part-of-speech (POS) information is used
to increase the generalization power of these rules.
Hence, rewrite rules are built using POS rather than
surface word forms (Crego and Marin?o, 2006).
3 SOUL translation models
A first issue with the model described by equa-
tion (2) is that the elementary units are bilingual
pairs. As a consequence, the underlying vocabulary,
hence the number of parameters, can be quite large,
even for small translation tasks. Due to data sparsity
issues, such model are bound to face severe estima-
tion problems. Another problem with (2) is that the
source and target sides play symmetric roles: yet,
in decoding, the source side is known and only the
target side must be predicted.
3.1 A word factored translation model
To overcome these issues, the n-gram probability in
equation (2) can be factored by decomposing tuples
3http://www.kyloo.net/software/doku.php
331
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
in two parts (source and target), and by taking words
as the basic units of the n-gram TM. This may seem
to be a regression with respect to current state-of-
the-art SMT systems, as the shift from the word-
based model of (Brown et al, 1993) to the phrase-
based models of (Zens et al, 2002) is usually con-
sidered as a major breakthrough of the recent years.
Indeed, one important motivation for considering
phrases was to capture local context in translation
and reordering. It should however be emphasized
that the decomposition of phrases into words is only
re-introduced here as a way to mitigate the param-
eter estimation problems. Translation units are still
pairs of phrases, derived from a bilingual segmen-
tation in tuples synchronizing the source and target
n-gram streams. In fact, the estimation policy de-
scribed in section 4 will actually allow us to take into
account larger contexts than is possible with conven-
tional n-gram models.
Let ski denote the k
th word of source tuple si.
Considering the example of Figure 1, s111 denotes
the source word nobel, s411 the source word paix.
We finally denote hn?1(tki ) the sequence made of
the n? 1 words preceding tki in the target sentence:
in Figure 1, h3(t211) thus refers to the three words
context receive the nobel associated with t211 peace.
Using these notations, equation (2) is rewritten as:
P (a, s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (3)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words from ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is:
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
A benefit of this new formulation is that the vo-
cabularies involved only contain words, and are thus
much smaller that tuple vocabularies. These models
are thus less at risk to be plagued by data sparsity is-
sues. Moreover, the decomposition (3) now involves
two models: the first term represents a TM, the sec-
ond term is best viewed as a reordering model. In
this formulation, the TM only predicts the target
phrase, given its source and target contexts.
P (s, t) =
L?
i=1
[ |si|?
k=1
P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
?
|ti|?
k=1
P
(
tki |h
n?1(s1i ), h
n?1(tki )
)] (4)
4 The principles of SOUL
In section 3.1, we defined a n-gram translation
model based on equations (3) and (4). A major diffi-
culty with such models is to reliably estimate their
parameters, the numbers of which grow exponen-
tially with the order of the model. This problem
is aggravated in natural language processing due to
332
the well-known data sparsity issue. In this work,
we take advantage of the recent proposal of (Le et
al., 2011). Using a specific neural network architec-
ture (the Structured OUtput Layer or SOUL model),
it becomes possible to handle large vocabulary lan-
guage modeling tasks. This approach was experi-
mented last year for target language models only and
is now extended to translation models. More details
about the SOUL architecture can be found in (Le et
al., 2011), while its extension to translation models
is more precisely described in (Hai-Son et al, 2012).
The integration of SOUL models for large SMT
tasks is carried out using a two-pass approach: the
first pass uses conventional back-off n-gram trans-
lation and language models to produce a k-best list
(the k most likely translations); in the second pass,
the probability of a m-gram SOUL model is com-
puted for each hypothesis and the k-best list is ac-
cordingly reordered. In all the following experi-
ments, we used a context size for SOUL of m = 10,
and used k = 300. The two decompositions of equa-
tions (3) and (4) are used by introducing 4 scores
during the rescoring step.
5 Corpora and data pre-processing
Concerning data pre-processing, we started from our
submissions from last year (Allauzen et al, 2011)
and mainly upgraded the corpora and the associated
language-dependent pre-processing routines.
5.1 Pre-processing
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte et
al., 2008). Previous experiments have demonstrated
that better normalization tools provide better BLEU
scores: all systems are thus built in ?true-case?.
Compared to last year, the pre-processing of utf-8
characters was significantly improved.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to unknown
forms). When translating from German into En-
glish, the German side is thus normalized using a
specific pre-processing scheme (described in (Al-
lauzen et al, 2010; Durgar El-Kahlout and Yvon,
2010)), which aims at reducing the lexical redun-
dancy by (i) normalizing the orthography, (ii) neu-
tralizing most inflections and (iii) splitting complex
compounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition, for
German, fine-grained POS labels were also needed
for pre-processing and were obtained using the RF-
Tagger (Schmid and Laws, 2008).
5.2 Bilingual corpora
As for last year?s evaluation, we used all the avail-
able parallel data for the German-English language
pair, while only a subpart of the French-English par-
allel data was selected. Word alignment models
were trained using all the data, whereas the transla-
tion models were estimated on a subpart of the par-
allel data: the UN corpus was discarded for this step
and about half of the French-English Giga corpus
was filtered based on a perplexity criterion as in (Al-
lauzen et al, 2011)).
For French-English, we mainly upgraded the
training material from last year by extracting the
new parts from the common data. The word
alignment models trained last year were then up-
dated by running a forced alignment 4 of the new
data. These new word-aligned data was added to
last year?s parallel corpus and constitute the train-
ing material for the translation models and feature
functions described in Section 2. Given the large
amount of available data, three different bilingual
n-gram models are estimated, one for each source of
data: News-Commentary, Europarl, and the French-
English Giga corpus. These models are then added
to the weighted mixture defined by equation (1). For
German-English, we simply used all the available
parallel data to train one single translation models.
5.3 Monolingual corpora and language models
For the monolingual training data, we also used the
same setup as last year. For German, all the train-
ing data allowed in the constrained task were di-
vided into several sets based on dates or genres:
News-Commentary, the news crawled from the Web
grouped by year, and Europarl. For each subset,
a standard 4-gram LM was estimated using inter-
polated Kneser-Ney smoothing (Kneser and Ney,
4The forced alignment step consists in an additional EM it-
eration.
333
1995; Chen and Goodman, 1998). The resulting
LMs are then linearly combined using interpolation
coefficients chosen so as to minimize the perplexity
of the development set. The German vocabulary is
created using all the words contained in the parallel
data and expanded to reach a total of 500k words by
including the most frequent words observed in the
monolingual News data for 2011.
For French and English, the same monolingual
corpora as last year were used5. We did not observe
any perplexity decrease in our attempts to include
the new data specifically provided for this year?s
evaluation. We therefore used the same language
models as in (Allauzen et al, 2011).
6 ?On-the-fly? system
We also developped an alternative approach imple-
menting ?on-the-fly? estimation of the parameter of
a standard phase-based model, using Moses (Koehn
et al, 2007) as the decoder. Implementing on-the-
fly estimation for n-code, while possible in the-
ory, is less appealing due to the computational cost
of estimating a smoothed language model. Given
an input source file, it is possible to compute only
those statistics which are required to translate the
phrases it contains. As in previous works on on-
the-fly model estimation for SMT (Callison-Burch
et al, 2005; Lopez, 2008), we compute a suffix
array for the source corpus. This further enables
to consider only a subset of translation examples,
which we select by deterministic random sampling,
meaning that the sample is chosen randomly with
respect to the full corpus but that the same sample
is always returned for a given value of sample size,
hereafter denoted N . In our experiments, we used
N = 1, 000 and computed from the sample and the
word alignments (we used the same tokenization and
word alignments as in all other submitted systems)
the same translation6 and lexical reordering models
as the standard training scripts of the Moses system.
Experiments were run on the data sets used for
WMT English-French machine translation evalua-
tion tasks, using the same corpora and optimization
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
6An approximation is used for p(f |e), and coherent transla-
tion estimation is used; see (Lopez, 2008).
procedure as in our other experiments. The only no-
table difference is our use of the Moses decoder in-
stead of the n-gram-based system. As shown in Ta-
ble 1, our on-the-fly system achieves a result (31.7
BLEU point) that is slightly worst than the n-code
baseline (32.0) and slightly better than the equiva-
lent Moses baseline (31.5), but does it much faster.
Model estimation for the test file is reduced to 2
hours and 50 minutes, with an additional overhead
for loading and writing files of one and a half hours,
compared to roughly 210 hours for our baseline sys-
tems under comparable hardware conditions.
7 Experimental results
7.1 n-code with SOUL
Table 1 summarizes the experimental results sub-
mitted to the shared translation for French-English
and German-English in both directions. The perfor-
mances are measured in terms of BLEU on new-
stest2011, last year?s test set, and this year?s test
set newstest2012. For the former, BLEU scores are
computed with the NIST script mteva-v13.pl, while
we provide for newstest2012 the results computed
by the organizers 7. The Baseline results are ob-
tained with standard n-gram models estimated with
back-off, both for the bilingual and monolingual tar-
get models. With standard n-gram estimates, the or-
der is limited to n = 4. For instance, the n-code
French-English baseline achieves a 0.5 BLEU point
improvement over a Moses system trained with the
same data setup in both directions.
From Table 1, it can be observed that adding
the SOUL models (translation models and target
language model) consistently improves the base-
line, with an increase of 1 BLEU point. Con-
trastive experiments show that the SOUL target LM
does not bring significant gain when added to the
SOUL translation models. For instance, a gain of
0.3 BLEU point is observed when translating from
French to English with the addition of the SOUL tar-
get LM. In the other translation directions, the differ-
ences are negligible.
7All results come from the official website: http://
matrix.statmt.org/matrix/.
334
Direction System BLEU
test2011 test2012?
en2fr Baseline 32.0 28.9
+ SOUL TM 33.4 29.9
on-the-fly 31.7 28.6
fr2en Baseline 30.2 30.4
+ SOUL TM 31.1 31.5
en2de Baseline 15.4 16.0
+ SOUL TM 16.6 17.0
de2en Baseline 21.8 22.9
+ SOUL TM 22.8 23.9
Table 1: Experimental results in terms of BLEU scores
measured on the newstest2011 and newstest2012. For
newstest2012, the scores are provided by the organizers.
7.2 Experiments with additional features
For this year?s evaluation, we also investigated sev-
eral additional features based on IBM1 models and
word sense disambiguation (WSD) information in
rescoring. As for the SOUL models, these features
are added after the n-best list generation step.
In previous work (Och et al, 2004; Hasan, 2011),
the IBM1 features (Brown et al, 1993) are found
helpful. As the IBM1 model is asymmetric, two
models are estimated, one in both directions. Con-
trary to the reported results, these additional features
do not yield significant improvements over the base-
line system. We assume that the difficulty is to add
information to an already extensively optimized sys-
tem. Moreover, the IBM1 models are estimated on
the same training corpora as the translation system,
a fact that may explain the redundancy of these ad-
ditional features.
In a separate series of experiments, we also add
WSD features calculated according to a variation of
the method proposed in (Apidianaki, 2009). For
each word of a subset of the input (source lan-
guage) vocabulary, a simple WSD classifier pro-
duces a probability distribution over a set of trans-
lations8. During reranking, each translation hypoth-
esis is scanned and the word translations that match
one of the proposed variant are rewarded using an
additional score. While this method had given some
8The difference with the method described in (Apidianaki,
2009) is that no sense clustering is performed, and each transla-
tion is represented by a separate weighted source feature vector
which is used for disambiguation
small gains on a smaller dataset (IWSLT?11), we did
not observe here any improvement over the base-
line system. Additional analysis hints that (i) most
of the proposed variants are already covered by the
translation model with high probabilities and (ii) that
these variants are seldom found in the reference sen-
tences. This means that, in the situation in which
only one reference is provided, the hypotheses with
a high score for the WSD feature are not adequately
rewarded with the actual references.
8 Conclusion
In this paper, we described our submissions to
WMT?12 in the French-English and German-
English shared translation tasks, in both directions.
As for our last year?s participation, our main sys-
tems are built with n-code, the open source Statis-
tical Machine Translation system based on bilingual
n-grams. Our contributions are threefold. First, we
have experimented a new kind of translation mod-
els, where the bilingual n-gram distribution are es-
timated in a continuous space with neural networks.
As shown in past evaluations with target language
model, there is a significant reward for using this
kind of models in a rescoring step. We observed that,
in general, the continuous space translation model
yields a slightly larger improvement than the target
translation model. However, their combination does
not result in an additional gain.
We also reported preliminary results with a sys-
tem ?on-the-fly?, where the training data are sam-
pled according to the data to be translated in order
to train contextually adapted system. While this sys-
tem achieves comparable performance to our base-
line system, it is worth noticing that its total train-
ing time is much smaller than a comparable Moses
system. Finally, we investigated several additional
features based on IBM1 models and word sense dis-
ambiguation information in rescoring. While these
methods have sometimes been reported to help im-
prove the results, we did not observe any improve-
ment here over the baseline system.
Acknowledgment
This work was partially funded by the French State
agency for innovation (OSEO) in the Quaero Pro-
gramme.
335
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
77?85, Athens, Greece, March. Association for Com-
putational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard Un iversity.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
Ilknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franc?ois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Hai-Son, Alexandre Allauzen, and Franc?ois Yvon. 2012.
Continuous space translation models with neural net-
works. In NAACL ?12: Proceedings of the 2012 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology.
Sas?a Hasan. 2011. Triplet Lexicon Models for Statisti-
cal Machine Translation. Ph.D. thesis, RWTH Aachen
University.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 161?168, Boston, Massachusetts, USA,
336
May 2 - May 7. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
337
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?9,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
WSD for n-best reranking and local language modeling in SMT
Marianna Apidianaki, Guillaume Wisniewski?, Artem Sokolov, Aure?lien Max?, Franc?ois Yvon?
LIMSI-CNRS
? Univ. Paris Sud
BP 133, F-91403, Orsay Cedex, France
firstname.lastname@limsi.fr
Abstract
We integrate semantic information at two
stages of the translation process of a state-of-
the-art SMT system. A Word Sense Disam-
biguation (WSD) classifier produces a proba-
bility distribution over the translation candi-
dates of source words which is exploited in
two ways. First, the probabilities serve to
rerank a list of n-best translations produced by
the system. Second, the WSD predictions are
used to build a supplementary language model
for each sentence, aimed to favor translations
that seem more adequate in this specific sen-
tential context. Both approaches lead to sig-
nificant improvements in translation perfor-
mance, highlighting the usefulness of source
side disambiguation for SMT.
1 Introduction
Word Sense Disambiguation (WSD) is the task of
identifying the sense of words in texts by reference
to some pre-existing sense inventory. The selec-
tion of the appropriate inventory and WSD method
strongly depends on the goal WSD intends to serve:
recent methods are increasingly oriented towards
the disambiguation needs of specific end applica-
tions, and explicitly aim at improving the overall
performance of complex Natural Language Process-
ing systems (Ide and Wilks, 2007; Carpuat and Wu,
2007). This task-oriented conception of WSD is
manifested in the area of multilingual semantic pro-
cessing: supervised methods, which were previously
shown to give the best results, are being abandoned
in favor of unsupervised ones that do not rely on pre-
annotated training data. Accordingly, pre-defined
semantic inventories, that usually served to provide
the lists of candidate word senses, are being replaced
by senses relevant to the considered applications and
directly identified from corpora by means of word
sense induction methods.
In a multilingual setting, the sense inventories
needed for disambiguation are generally built from
all possible translations of words or phrases in a par-
allel corpus (Carpuat and Wu, 2007; Chan et al,
2007), or by using more complex representations
of the semantics of translations (Apidianaki, 2009;
Mihalcea et al, 2010; Lefever and Hoste, 2010).
However, integrating this semantic knowledge into
Statistical Machine Translation (SMT) raises sev-
eral challenges: the way in which the predictions of
the WSD classifier have to be taken into account;
the type of context exploited for disambiguation;
the target words to be disambiguated (?all-words?
WSD vs. WSD restricted to target words satisfy-
ing specific criteria); the use of a single classifier
versus building separate classifiers for each source
word; the quantity and type of data used for training
the classifier (e.g., use of raw data or of more ab-
stract representations, such as lemmatization, allow-
ing to deal with sparseness issues), and many oth-
ers. Seemingly, the optimal way to take advantage
of WSD predictions remains an open issue.
In this work, we carry out a set of experiments
to investigate the impact of integrating the predic-
tions of a cross-lingual WSD classifier into an SMT
system, at two different stages of the translation pro-
cess. The first approach exploits the probability dis-
tribution built by the WSD classifier over the set of
translations of words found in the parallel corpus,
1
for reranking the translations in the n-best list gen-
erated by the SMT system. Words in the list that
match one of the proposed translations are boosted
and are thus more likely to appear in the final trans-
lation. Our results on the English-French IWSLT?11
task show substantial improvements in translation
quality. The second approach provides a tighter in-
tegration of the WSD classifier with the rest of the
system: using the WSD predictions, an additional
sentence specific language model is estimated and
used during decoding. These additional local mod-
els can be used as an external knowledge source to
reinforce translation hypotheses matching the pre-
diction of the WSD system.
In the rest of the paper, we present related work
on integrating semantic information into SMT (Sec-
tion 2). The WSD classifier used in the current study
is described in Section 3. We then present the two
approaches adopted for integrating the WSD out-
put into SMT (Section 4). Evaluation results are
presented in Section 5, before concluding and dis-
cussing some avenues for future work.
2 Related work
Word sense disambiguation systems generally work
at the word level: given an input word and its con-
text, they predict its (most likely) meaning. At
the same time, state-of-the-art translation systems
all consider groups of words (phrases, tuples, etc.)
rather than single words in the translation process.
This discrepancy between the units used in MT and
those used in WSD is one of the major difficul-
ties in integrating word predictions into the decoder.
This was, for instance, one of the reasons for the
somewhat disappointing results obtained by Carpuat
and Wu (2005) when the output of a WSD system
was directly incorporated into a Chinese-English
SMT system. Because of this difficulty, other cross-
lingual semantics works have considered only sim-
plified tasks, like blank-filling, without addressing
the integration of the WSD models in full-scale MT
systems (Vickrey et al, 2005; Specia, 2006).
Since the pioneering work of Carpuat and Wu
(2005), several more successful ways to take WSD
predictions into account have been proposed. For
instance, Carpuat and Wu (2007) proposed to gen-
eralize the WSD system so that it performs a fully
phrasal multiword disambiguation. However, given
that the number of phrases is far larger than the num-
ber of words, this approach suffers from sparsity
and computational problems, as it requires training
a classifier for each entry of the phrase table.
Chan et al (2007) introduced a way to modify the
rule weights of a hierarchical translation system to
reflect the predictions of their WSD system. While
their approach and ours are built on the same intu-
ition (an adaptation of a model to incorporate word
predictions) their work is specific to hierarchical
systems, while ours can be applied to any decoder
that uses a language model. Haque et al (2009) et
Haque et al (2010) introduce lexico-syntactic de-
scriptions in the form of supertags as source lan-
guage context-informed features in a phrase-based
SMT and a state-of-the-art hierarchical model, re-
spectively, and report significant gains in translation
quality.
Closer to our work, Mauser et al (2009) and Pa-
try and Langlais (2011) train a global lexicon model
that predicts the bag of output words from the bag
of input words. As no explicit alignment between
input and output words is used, words are chosen
based on the (global) input context. For each input
sentence, the decoder considers these word predic-
tions as an additional feature that it uses to define a
new model score which favors translation hypothe-
ses containing words predicted by the global lexicon
model. A difference between this approach and our
work is that instead of using a global lexicon model,
we disambiguate a subset of the words in the input
sentence by employing a WSD classifier that cre-
ates a probability distribution over the translations
of each word in its context.
The unsupervised cross-lingual WSD classifier
used in this work is similar to the one proposed in
Apidianaki (2009). The original classifier disam-
biguates new instances of words in context by se-
lecting the most appropriate cluster of translations
among a set of candidate clusters found in an auto-
matically built bilingual sense inventory. The sense
inventory exploited by the classifier is created by
a cross-lingual word sense induction (WSI) method
that reveals the senses of source words by grouping
their translations into clusters according to their se-
mantic proximity, revealed by a distributional sim-
ilarity calculation. The resulting clusters represent
2
the source words? candidate senses. This WSD
method gave good results in a word prediction task
but, similarly to the work of Vickrey et al (2005)
and of Specia (2006), the predictions are not inte-
grated into a complete MT system.
3 The WSD classifier
Our WSD classifier is a variation of the one intro-
duced in Apidianaki (2009). The main difference
is that here the classifier serves to discriminate be-
tween unclustered translations of a word and to as-
sign a probability to each translation for new in-
stances of the word in context. Each translation is
represented by a source language feature vector that
the classifier uses for disambiguation. All experi-
ments carried out in this study are for the English
(EN) - French (FR) language pair.
3.1 Source Language Feature Vectors
Preprocessing The information needed by the clas-
sifier is gathered from the EN-FR training data pro-
vided for the IWSLT?11 evaluation task.1 The
dataset consists of 107,268 parallel sentences, word-
aligned in both translation directions using GIZA++
(Och and Ney, 2003). We disambiguate EN words
found in the parallel corpus that satisfy the set of
criteria described below.
Two bilingual lexicons are built from the align-
ment results and filtered to eliminate spurious align-
ments. First, translation correspondences with a
probability lower than a threshold are discarded;2
then translations are filtered by part-of-speech
(PoS), keeping for each word only translations per-
taining to the same grammatical category;3 finally,
only intersecting alignments (i.e., correspondences
found in the lexicons of both directions) are retained.
Given that the lexicons contain word forms, the in-
tersection is calculated based on lemmatization in-
formation in order to perform a generalization over
the contents of the lexicons. For instance, if the EN
adjective regular is translated by habituelle (femi-
1http://www.iwslt2011.org/
2The translation probabilities between word tokens are
found in the translation table produced by GIZA++; the thresh-
old is set to 0.01.
3For this filtering, we employ a PoS and lemmatization lex-
icon built after tagging both parts of the training corpus with
TreeTagger (Schmid, 1994).
nine singular form of the adjective habituel) in the
EN-FR lexicon, but is found to translate habituel
(masculine singular form) in the other direction,
the EN-FR correspondence regular/habituelle is re-
tained (because the two variants of the adjective are
reduced to the same lemma).
All lexicon entries satisfying the above criteria are
retained and used for disambiguation. In these initial
experiments, we disambiguate English words having
less than 20 French translations in the lexicon. Each
French translation of an English word that appears
more than once in the training corpus4 is character-
ized by a weighted English feature vector built from
the training data.
Vector building The feature vectors corresponding
to the translations are built by exploiting information
from the source contexts (Apidianaki, 2008; Grefen-
stette, 1994). For each translation of an EN word w,
we extract the content words that co-occur with w
in the corresponding source sentences of the parallel
corpus (i.e. the content words that occur in the same
sentence as w whenever it is translated by this trans-
lation). The extracted source language words con-
stitute the features of the vector built for the transla-
tion.
For each translation Ti of w, let N be the number
of features retained from the corresponding source
context. Each feature Fj (1 ? j ? N) receives a to-
tal weight tw(Fj,Ti) defined as the product of the
feature?s global weight, gw(Fj), and its local weight
with that translation, lw(Fj,Ti):
tw(Fj,Ti) = gw(Fj) ? lw(Fj,Ti) (1)
The global weight of a feature Fj is a function of
the number Ni of translations (Ti?s) to which Fj is re-
lated, and of the probabilities (pi j) that Fj co-occurs
with instances of w translated by each of the Ti?s:
gw(Fj) = 1?
?Ti pi j log(pi j)
Ni
(2)
Each of the pi j?s is computed as the ratio between
the co-occurrence frequency of Fj with w when
translated as Ti, denoted as cooc frequency(Fj,Ti),
4We do not consider hapax translations because they often
correspond to alignment errors.
3
and the total number of features (N) seen with Ti:
pi j =
cooc frequency(Fj,Ti)
N
(3)
Finally, the local weight lw(Fj,Ti) between Fj and Ti
directly depends on their co-occurrence frequency:
lw(Fj,Ti) = log(cooc frequency(Fj,Ti)) (4)
3.2 Cross-Lingual WSD
The weighted feature vectors corresponding to the
different translations of an English word are used
for disambiguation.5 As noted in Section 3.1, we
disambiguate source words satisfying a set of crite-
ria. Disambiguation is performed by comparing the
vector associated with each translation to the new
context of the words in the input sentences from the
IWSLT?11 test set.
More precisely, the information contained in each
vector is exploited by the WSD classifier to produce
a probability distribution over the translations, for
each new instance of a word in context. We dis-
ambiguate word forms (not lemmas) in order to di-
rectly use the selected translations in the translated
texts. However, we should note that in some cases
this reduces the role of WSD to distinguishing be-
tween different forms of one word and no different
senses are involved. Using more abstract represen-
tations (corresponding to senses) is one of the per-
spectives of this work.
The classifier assigns a score to each transla-
tion by comparing information in the corresponding
source vector to information found in the new con-
text. Given that the vector features are lemmatized,
the new context is lemmatized as well and the lem-
mas of the content words are gathered in a bag of
words. The adequacy of each translation for a new
instance of a word is estimated by comparing the
translation?s vector with the bag of words built from
the new context. If common features are found be-
tween the new context and a translation vector, an
association score is calculated corresponding to the
mean of the weights of the common features rela-
tively to the translation (i.e. found in its vector). In
5The vectors are not used for clustering the translations as
in Apidianaki (2009) but all translations are considered as can-
didate senses.
Equation (5), (CFj)|CF |j=1 is the set of common fea-
tures between the translation vector Vi and the new
context C and tw is the weight of a CF with transla-
tion Ti (cf. formula (1)).
assoc score(Vi,C) =
?|CF |j=1 tw(CFj,Ti)
|CF| (5)
The scores assigned to the different translations of a
source word are normalized to sum up to one.
In this way, a subset of the words that occur in the
input sentences from the test set are annotated with
their translations and the associated scores (contex-
tual probabilities), as shown in the example in Fig-
ure 1.6 The WSD classifier makes predictions only
for the subset of the words found in the source part
of the parallel test set that were retained from the ini-
tial EN-FR lexicon after filtering. Table 1 presents
the total coverage of the WSD method as well as its
coverage for words of different PoS, with a focus
on content words. We report the number of disam-
biguated words for each content PoS (cf. third col-
umn) and the corresponding percentage, calculated
on the basis of the total number of words pertaining
to this PoS (cf. second column). We observe that
the coverage of the method on nouns and adjectives
is higher than the one on verbs. Given the rich ver-
bal morphology of French, several verbs have a very
high number of translations in the bilingual lexicon
(over 20) and are not handled during disambigua-
tion. The same applies to function words (articles,
prepositions, conjunctions, etc.) included in the ?all
PoS? category.
4 Integrating Semantics into SMT
In this section, we present two ways to integrate
WSD predictions into an SMT decoder. The first
one (Section 4.1) is a simple method based on n-
best reranking. This method, already proposed in
the literature (Specia et al, 2008), allows us to eas-
ily evaluate the impact of WSD predictions on au-
tomatic translation quality. The second one (Sec-
tion 4.2) builds on the idea, introduced in (Crego et
al., 2010), of using an additional language model to
6Some source words are tagged with only one translation
(e.g. stones {pierres(1.000)}) because their other translations
in the lexicon occurred only once in the training corpus and,
consequently, were not considered.
4
PoS # of words # of WSD predictions %
Nouns 5535 3472 62.72
Verbs 5336 1269 23.78
Adjs 1787 1249 69.89
Advs 2224 1098 49.37
all content PoS 14882 7088 47.62
all PoS 27596 8463 30.66
Table 1: Coverage of the WSD method
you know, one of the intense {intenses(0.305), forte(0.306), intense(0.389)} pleasures of
travel {transport(0.334), voyage(0.332), voyager(0.334)} and one of the delights of ethnographic
research {recherche(0.225), research(0.167), e?tudes(0.218), recherches(0.222), e?tude(0.167)} is
the opportunity {possibilite?(0.187), chance(0.185), opportunite?s(0.199), occasion(0.222), opportu-
nite?(0.207)} to live amongst those who have not forgotten {oubli(0.401), oublie?s(0.279), ou-
blie?e(0.321)} the old {ancien(0.079), a?ge(0.089), anciennes(0.072), a?ge?es(0.100), a?ge?s(0.063), an-
cienne(0.072), vieille(0.093), ans(0.088), vieux(0.086), vieil(0.078), anciens(0.081), vieilles(0.099)}
ways {fac?ons(0.162), manie`res(0.140), moyens(0.161), aspects(0.113), fac?on(0.139), moyen(0.124),
manie`re(0.161)} , who still feel their past {passe?e(0.269), autrefois(0.350), passe?(0.381)} in the
wind {e?olienne(0.305), vent(0.392), e?oliennes(0.304)} , touch {touchent(0.236), touchez(0.235),
touche(0.235), toucher(0.293)} it in stones {pierres(1.000)} polished by rain {pluie(1.000)} ,
taste {gou?t(0.500), gou?ter(0.500)} it in the bitter {amer(0.360), ame`re(0.280), amertume(0.360)}
leaves {feuilles(0.500), feuillages(0.500)} of plants {usines(0.239), centrales(0.207), plantes(0.347),
ve?ge?taux(0.207)}.
Figure 1: Input sentence with WSD information
directly integrate the prediction of the WSD system
into the decoder.
4.1 N-best List Reranking
A simple way to influence translation hypotheses se-
lection with WSD information is to use the WSD
probabilities of translation variants to produce an ad-
ditional feature appended to the n-best list after its
generation. The feature value should reflect the de-
gree to which a particular hypothesis includes pro-
posed WSD variants for the respective words. Re-
running the standard MERT optimization procedure
on the augmented features gives a new set of model
weights, that are used to rescore the n-best list.
We propose the following method of features con-
struction. Given the phrase alignment information
between a source sentence and a hypothesis, we ver-
ify if one or more of the proposed WSD variants for
the source word occur in the corresponding phrase of
the translation hypothesis. If this is the case, the cor-
responding probabilities are additively accumulated
for the current hypothesis. At the end, two features
are appended to each hypothesis in the n-best list:
the total score accumulated for the hypothesis and
the same score normalized by the number of words
in the hypothesis.
Two MERT initialization schemes were consid-
ered: (1) all model weights are initialized to zero,
and (2) all the weights of ?standard? features are ini-
tialized to the values found by MERT and the new
WSD features to zero.
4.2 Local Language Models
We propose to adapt the approach introduced in
Crego et al (2010) as an alternative way to inte-
grate the WSD predictions within the decoder: for
each sentence to be translated, an additional lan-
guage model (LM) is estimated and taken into ac-
count during decoding. As this additional ?local?
model depends on the source sentence, it can be
used as an external source of knowledge to reinforce
translation hypotheses complying with criteria pre-
dicted from the whole source sentence. For instance,
the unigram probabilities of the additional LM can
be derived from the (word) predictions of a WSD
system, bigram probabilities from the prediction of
phrases and so on and so forth. Although this ap-
proach was suggested in (Crego et al, 2010), this
5
is, to the best of our knowledge, the first time it is
experimentally validated.
In practice, the predictions of the WSD system
described in Section 3 can be integrated by defining,
for each sentence, an additional unigram language
model as follows:
? each translation predicted by the WSD classi-
fier can be generated by the language model
with the probability estimated by the WSD
classifier; no information about the source
word that has been disambiguated is consid-
ered;
? the probability of unknown words is set to a
small arbitrary constant.
Even if most of the words composing the transla-
tion hypothesis are considered as unknown words,
hypotheses that contain the words predicted by the
WSD system still have a higher LM score and are
therefore preferred. Note that even if we only use
unigram language models in our experiments, as
senses are predicted at the word level, our approach
is able to handle disambiguation of phrases as well.
This approach has two main advantages over ex-
isting ways to integrate WSD predictions in an SMT
system. First, no hard decisions are made: errors
of the WSD can be ?corrected? by the translation.
Second, sense disambiguation at the word level is
naturally and automatically propagated at the phrase
level: the additional LM is influencing all phrase
pairs using one of the predicted words.
Compared to the reranking approach introduced
in the previous section, this method results in a
tighter integration with the decoder. In particu-
lar, the WSD predictions are applied before search-
space pruning and are therefore expected to have a
more important role.
5 Evaluation
5.1 Experimental Setting
In all our experiments, we considered the TED-
talk English to French data set provided by the
IWSLT?11 evaluation campaign, a collection of pub-
lic speeches on a variety of topics. We used the
Moses decoder (Koehn et al, 2007).
The TED-talk corpus is a small data set made
of a monolingual corpus (111,431 sentences) used
to estimate a 4-gram language model with KN-
smoothing, and a bilingual corpus (107,268 sen-
tences) used to extract the phrase table. All data
are tokenized, cleaned and converted to lowercase
letters using the tools provided by the WMT orga-
nizers.7 We then use a standard training pipeline to
construct the translation model: the bitext is aligned
using GIZA++, symmetrized using the grow-diag-
final-and heuristic; the phrase table is extracted and
scored using the tools distributed with Moses. Fi-
nally, systems are optimized using MERT on the
934 sentences of the dev-2010 set. All evalua-
tions are performed on the 1,664 sentences of the
test-2010 set.
5.2 Baseline
In addition to the models introduced in Section 4,
we considered two other supplementary models as
baselines. The first one uses the IBM 1 model esti-
mated during the SMT system training as a simple
WSD system: for each source sentence, a unigram
additional language model is defined by taking, for
each source, the 20 best translations according to the
IBM 1 model and their probability. Model 1 has
been shown to be one of the best performing fea-
tures to be added to an SMT system in a reranking
step (Och et al, 2004) and can be seen as a naive
WSD classifier.
To test the validity of our approach, we repli-
cate the ?oracle? experiments of Crego et al (2010)
and estimate the best gain our method can achieve.
These experiments consist in using the reference to
train a local n-gram language model (with n in the
range 1 to 3) which amounts, in the local language
model method of Section 4.2, to assuming that the
WSD system correctly predicted a single translation
for each source word.
5.3 Results
Table 2 reports the results of our experiments. It
appears that, for the considered task, sense disam-
biguation improves translation performance: n-best
rescoring results in a 0.37 BLEU improvement and
using an additional language model brings about an
improvement of up to a 0.88 BLEU. In both cases,
MERT assigns a large weight to the additional fea-
7http://statmt.org/wmt08/scripts.tgz
6
method BLEU METEOR
baseline ? 29.63 53.78
rescoring WSD (zero init) 30.00 54.26WSD (reinit) 29.58 53.96
additional LM
oracle 3-gram 43.56 64.64
oracle 2-gram 39.36 62.92
oracle 1-gram 42.92 69.39
IBM 1 30.18 54.36
WSD 30.51 54.38
Table 2: Evaluation results on the TED-talk task of our two methods to integrate WSD predictions.
PoS baseline WSD
Nouns 67.57 69.06
Verbs 45.97 47.76
Adjectives 51.79 53.94
Adverbs 52.17 56.25
Table 3: Contrastive lexical evaluation: % of words correctly translated within each PoS class
tures during tuning. When rescoring n-best, an im-
provement is observed only when the weights are
initialized to zero and not to the weights resulting
from the previous optimization, maybe because of
the difficulty to exit the local minimum MERT had
found earlier.
As expected, integrating the WSD predictions
with an additional language model results in a larger
improvement than simple rescoring, which shows
the importance of applying this new source of in-
formation early in the translation pipeline, before
search space pruning. Also note that the system us-
ing the IBM 1 predictions is outperformed by the
system using the WSD classifier introduced in Sec-
tion 3, showing the quality of its predictions.
Oracle experiments stress the high potential of
the method introduced in (Crego et al, 2010) as a
way to integrate external sources of knowledge: all
three conditions result in large improvements over
the baseline and the proposed methods. It must,
however, be noted that contrary to the WSD method
introduced in Section 3, these oracle experiments
rely on sense predictions for all source words and
not only content words. Surprisingly enough, pre-
dicting phrases instead of words results only in a
small improvement. Additional experiments are re-
quired to explain why 2-gram oracle achieved such
a low performance.
5.4 Contrastive lexical evaluation
All the measures used for evaluating the impact
of WSD information on translation show improve-
ments, as discussed in the previous section. We
complement these results with another measure of
translation performance, proposed by Max et al
(2010), which allows for a more fine-grained con-
trastive evaluation of the translations produced by
different systems. The method permits to compare
the results produced by the systems on different
word classes and to take into account the source
words that were actually translated. We focus this
evaluation on the classes of content words (nouns,
adjectives, verbs and adverbs) on which WSD had
an important coverage. Our aim is, first, to ex-
plore how these words are handled by a WSD-
informed SMT system (the system using the lo-
cal language models) compared to the baseline sys-
tem that does not exploit any semantic informa-
tion; and, second, to investigate whether their dis-
ambiguation influences the translation of surround-
ing non-disambiguated words.
Table 3 reports the percentage of words cor-
rectly translated by the semantically-informed sys-
tem within each content word class: consistent gains
in translation quality are observed for all parts-of-
speech compared to the baseline, and the best results
are obtained for nouns.
7
baseline WSD
w?2 w?1 w+1 w+2 w?2 w?1 w+1 w+2
Nouns 64.01 68.69 75.17 64.6 65.47 70.46 76.3 66.6
Verbs 68.67 67.58 63 62.19 69.98 68.89 64.85 64.25
Adjectives 63.1 64.39 64.28 66.55 64.09 65.65 64.76 69.33
Adverbs 70.8 69.44 68.67 66.38 71 71.21 70 67.22
Table 4: Impact of WSD prediction on the surrounding words
Table 4 shows how the words surrounding a dis-
ambiguated word w (noun, verb, adjective or adverb)
in the text are handled by the two systems. More
precisely, we look at the translation of words in the
immediate context of w, i.e. at positions w?2, w?1,
w+1 and w+2. The left column reports the percent-
age of correct translations produced by the baseline
system (without disambiguation) for words in these
positions; the right column shows the positive im-
pact that the disambiguation of a word has on the
translation of its neighbors. Note that this time we
look at disambiguated words and their context with-
out evaluating the correctness of the WSD predic-
tions. Nevertheless, even in this case, consistent
gains are observed when WSD information is ex-
ploited. For instance, when a noun is disambiguated,
70.46% and 76.3% of the immediately preceding
(w?1) and following (w+1) words, respectively, are
correctly translated, versus 68.69% and 75.17% of
correct translations produced by the baseline system.
6 Conclusion and future work
The preliminary results presented in this paper on
integrating cross-lingual WSD into a state-of-the-
art SMT system are encouraging. Both adopted ap-
proaches (n-best rescoring and local language mod-
eling) benefit from the predictions of the proposed
cross-lingual WSD classifier. The contrastive eval-
uation results further show that WSD improves not
only the translation of disambiguated words, but also
the translation of neighboring words in the input
texts.
We consider various ways for extending this
work. First, future experiments will involve the use
of more abstract representations of senses than indi-
vidual translations, by applying a cross-lingual word
sense induction method to the training corpus prior
to disambiguation. We will also experiment with
disambiguation at the level of lemmas, to reduce
sparseness issues, and with different ways for han-
dling lemmatized predictions by the SMT systems.
Furthermore, we intend to extend the coverage of the
WSD method by exploring other filtering methods
for cleaning the alignment lexicons, and by address-
ing the disambiguation of words of all PoS.
Acknowledgments
This work was partly funded by the European Union
under the FP7 project META-NET (T4ME), Con-
tract No. 249119, and by OSEO, the French agency
for innovation, as part of the Quaero Program.
References
Marianna Apidianaki. 2008. Translation-oriented Word
Sense Induction Based on Parallel Corpora. In Pro-
ceedings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco.
Marianna Apidianaki. 2009. Data-driven Semantic
Analysis for Multilingual WSD and Lexical Selection
in Translation. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL-09), pages 77?85,
Athens, Greece.
Marine Carpuat and Dekai Wu. 2005. Word Sense Dis-
ambiguation vs. Statistical Machine Translation. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
387?394, Ann Arbor, Michigan.
Marine Carpuat and Dekai Wu. 2007. Improving Sta-
tistical Machine Translation using Word Sense Disam-
biguation. In Proceedings of the Joint EMNLP-CoNLL
Conference, pages 61?72, Prague, Czech Republic.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-07), pages 33?40, Prague, Czech Republic.
8
Josep Maria Crego, Aure?lien Max, and Franc?ois Yvon.
2010. Local lexical adaptation in Machine Transla-
tion through triangulation: SMT helping SMT. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 232?
240, Beijing, China.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Norwell, MA.
Rejwanual Haque, Sudip Naskar, Yanjun Ma, and Andy
Way. 2009. Using supertags as source language con-
text in SMT. In Proceedings of the 13th Annual Meet-
ing of the European Association for Machine Transla-
tion (EAMT 2009), pages 234?241, Barcelona, Spain.
Rejwanul Haque, Sudip Kumar Naskar, Antal Van Den
Bosch, and Andy Way. 2010. Supertags as source lan-
guage context in hierarchical phrase-based SMT. In
Proceedings of AMTA 2010: The Ninth Conference of
the Association for Machine Translation in the Ameri-
cas, pages 210?219, Denver, CO.
N. Ide and Y. Wilks. 2007. Making Sense About Sense.
In E. Agirre and P. Edmonds, editors, Word Sense Dis-
ambiguation, Algorithms and Applications, pages 47?
73. Springer.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual ACL Meeting, Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Els Lefever and Veronique Hoste. 2010. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the 5th International Workshop on
Semantic Evaluations (SemEval-2), ACL 2010, pages
15?20, Uppsala, Sweden.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 210?217,
Singapore, August.
Aure?lien Max, Josep Maria Crego, and Franc?ois Yvon.
2010. Contrastive Lexical Evaluation of Machine
Translation. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
SemEval-2010 Task 2: Cross-Lingual Lexical Sub-
stitution. In Proceedings of the 5th International
Workshop on Semantic Evaluations (SemEval-2), ACL
2010, pages 9?14, Uppsala, Sweden.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In Proceedings of HLT-NAACL 2004, pages 161?
168, Boston, Massachusetts, USA.
Alexandre Patry and Philippe Langlais. 2011. Going be-
yond word cooccurrences in global lexical selection
for statistical machine translation using a multilayer
perceptron. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
658?666, Chiang Mai, Thailand, November.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Lucia Specia, Baskaran Sankaran, and Maria Das
Grac?as Volpe Nunes. 2008. n-Best Reranking for the
Efficient Integration of Word Sense Disambiguation
and Statistical Machine Translation. In Proceedings of
the 9th international conference on Computational lin-
guistics and intelligent text processing, CICLing?08,
pages 399?410, Berlin, Heidelberg. Springer-Verlag.
Lucia Specia. 2006. A Hybrid Relational Approach for
WSD - First Results. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages 55?
60, Sydney, Australia.
David Vickrey, Luke Biewald, Marc Teyssier, and
Daphne Koller. 2005. Word-Sense Disambiguation
for Machine Translation. In Proceedings of the Joint
Conference on Human Language Technology / Empir-
ical Methods in Natural Language Processing (HLT-
EMNLP), pages 771?778, Vancouver, Canada.
9
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 1?10,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Cross-lingual WSD for Translation Extraction
from Comparable Corpora
Marianna Apidianaki
LIMSI-CNRS
Rue John Von Neumann
BP 133, 91403
Orsay Cedex, France
marianna@limsi.fr
Nikola Ljubes?ic?
Dept. of Information Sciences
University of Zagreb
Ivana Luc?ic?a 3, HR-10000
Zagreb, Croatia
nljubesi@ffzg.hr
Darja Fis?er
Department of Translation
University of Ljubljana
As?kerc?eva 2, SI-1000
Ljubljana, Slovenia
darja.fiser@ff.uni-lj.si
Abstract
We propose a data-driven approach to en-
hance translation extraction from compa-
rable corpora. Instead of resorting to an
external dictionary, we translate source
vector features by using a cross-lingual
Word Sense Disambiguation method. The
candidate senses for a feature correspond
to sense clusters of its translations in a
parallel corpus and the context used for
disambiguation consists of the vector that
contains the feature. The translations
found in the disambiguation output con-
vey the sense of the features in the source
vector, while the use of translation clusters
permits to expand their translation with
several variants. As a consequence, the
translated vectors are less noisy and richer,
and allow for the extraction of higher qual-
ity lexicons compared to simpler methods.
1 Introduction
Large-scale comparable corpora are available in
many language pairs and are viewed as a source
of valuable information for multilingual applica-
tions. Identifying translation correspondences in
this type of corpora permits to construct bilingual
lexicons for low-resourced languages, and to com-
plement and reduce the sparseness of existing re-
sources (Munteanu and Marcu, 2005; Snover et
al., 2008). The main assumption behind transla-
tion extraction from comparable corpora is that a
source word and its translation appear in similar
contexts (Fung, 1998; Rapp, 1999). So, in order
to identify a translation correspondence between
the two languages, the contexts of the source word
and the candidate translation have to be compared.
For this comparison to take place, the same vector
space has to be produced, which means that the
vectors of the one language have to be translated
in the other language. This generally assumes the
availability of a bilingual dictionary which might
however not be the case for some language pairs
and domains. Moreover, the classic way in which
a dictionary is put into use, which consists in trans-
lating vector features by their first translation in
the dictionary, neglects semantics. We expect that
a method capable of identifying the correct sense
of the features and translating them accordingly
could contribute to producing cleaner vectors and
to extracting higher quality lexicons.
In this paper, we show how source vectors
can be translated into the target language by a
cross-lingual Word Sense Disambiguation (WSD)
method which exploits the output of data-driven
Word Sense Induction (WSI) (Apidianaki, 2009),
and demonstrate how feature disambiguation en-
hances the quality of the translations extracted
from the comparable corpus. This study extends
our previous work on the topic (Apidianaki et al,
2012) by applying the proposed methods to a com-
parable corpus of general language (built from
Wikipedia) and optimizing various parameters that
affect the quality of the extracted translations. We
expect the disambiguation to have a beneficial im-
pact on the results given that polysemy is a fre-
quent phenomenon in a general, mixed-domain
corpus. Our experiments are carried out on the
English-Slovene language pair but as the methods
are totally data-driven, the approach can be easily
applied to other languages.
The paper is organized as follows: In the next
section, we present some related work on bilin-
gual lexicon extraction from comparable corpora.
Section 3 presents the data used in our experiments
and Section 4 provides details on the approach and
the experimental setup. In Section 5, we report and
discuss the obtained results before concluding and
presenting some directions for future work.
1
2 Related work
The traditional approch to translation extraction
from comparable corpora and most of its exten-
sions (Fung, 1998; Rapp, 1999; Shao and Ng,
2004; Otero, 2007; Yu and Tsujii, 2009; Marsi
and Krahmer, 2010) presuppose the availability
of a bilingual lexicon for translating source vec-
tors into the target language. A translation can-
didate is generally considered as correct if it is
an appropriate translation for at least one sense
of the source word in the dictionary, which of-
ten corresponds to its most frequent sense. An
alternative consists in considering all translations
provided for a word in the dictionary but weight-
ing them by their frequency in the target lan-
guage (Prochasson et al, 2009; Hazem and Morin,
2012). The high quality of the exploited hand-
crafted resources, combined to the skewed distri-
bution of the translations corresponding to differ-
ent word senses, often lead to satisfying results.
Nevertheless, the applicability of the methods is
limited to languages and domains where bilingual
resources are available. Moreover, by promoting
the most frequent sense/translation, this approach
neglects polysemy. We believe that feature dis-
ambiguation can lead to the production of cleaner
vectors and, consequently, to higher quality re-
sults.
The need to bypass pre-existing dictionaries
has been addressed by Koehn and Knight (2002)
who built the initial seed dictionary automatically,
based on identical spelling features between En-
glish and German. Cognate detection has also
been used by Saralegi et al (2008) for extract-
ing word translations from English-Basque com-
parable corpora. The cognate and seed lexicon
approaches have been successfully combined by
Fis?er and Ljubes?ic? (2011) who showed that the re-
sults with an automatically created seed lexicon,
based on language similarity, can be as good as
with a pre-existing dictionary. But all these ap-
proaches work on closely-related languages and
cannot be used as successfully for language pairs
with little lexical overlap, such as English and
Slovene, which is the case in this experiment.
Regarding the translation of the source vectors,
we use contextual information to disambiguate
their features and translate them using clusters
of semantically similar translations in the target
language. A similar idea has been implemented
by Kaji (2003) who performed sense-based word
clustering to extract sets of synonymous transla-
tions from comparable corpora with the help of a
bilingual dictionary.
Using translation clusters permits to expand
feature translation and to suggest multiple seman-
tically correct translations. A similar approach has
been adopted by De?jean et al (2005) who expand
vector translation by using a bilingual thesaurus
instead of a lexicon. In contrast to their work, the
method proposed here does not rely on any exter-
nal knowledge source to determine word senses
or translation equivalents, and is thus fully data-
driven and language independent.
3 Resources
3.1 Comparable corpus
The comparable corpus from which the bilin-
gual lexicon will be extracted is a collection of
English (EN) and Slovene (SL) texts extracted
from Wikipedia. The February 2013 dumps of
Wikipedia articles were downloaded and cleaned
for both languages after which the English cor-
pus was tokenized, part-of-speech (PoS) tagged
and lemmatized with the TreeTagger (Schmid,
1994). The same pre-processing was applied to the
Slovene corpus with the ToTaLe analyzer (Erjavec
et al, 2010) which uses the TnT tagger (Brants,
2000) and was trained on MultextEast corpora.
The Wikipedia corpus contains about 1.5 billion
tokens for English and almost 24 million tokens
for Slovene.
In previous work, we applied our approach to a
specialized comparable corpus from the health do-
main (Apidianaki et al, 2012). The results were
encouraging, showing how translation clustering
and vector disambiguation help to improve the
quality of the translations extracted from the com-
parable corpus. We believe that the positive im-
pact of this approach will be more significant on
lexicon extraction from a general language com-
parable corpus, in which polysemy is more promi-
nent.
3.2 Parallel corpus
The parallel corpus used for clustering and word
sense induction consists of the Slovene-English
parts of Europarl (release v6) (Koehn, 2005) and
of JRC-Acquis (Steinberger et al, 2006) and
amounts to approximately 35M words per lan-
guage. A number of pre-processing steps are ap-
plied to the corpus prior to sense induction, such
2
Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD.
as elimination of sentence pairs with a great dif-
ference in length, lemmatization and PoS tagging
with the TreeTagger (for English) and ToTaLe (for
Slovene) (Erjavec et al, 2010). Next, the cor-
pus is word-aligned with GIZA++ (Och and Ney,
2003) and two bilingual lexicons are extracted,
one for each translation direction (EN?SL/SL?
EN). To clean the lexicons from noisy alignments,
the translations are filtered on the basis of their
alignment score and PoS, keeping only transla-
tions that pertain to the same grammatical cate-
gory as the source word. We retain only intersect-
ing alignments and use for clustering translations
that translate a source word more than 10 times
in the training corpus. This threshold reduces
data sparseness issues that affect the clustering
and eliminates erroneous word alignments. The
filtered EN-SL lexicon contains entries for 6,384
nouns, 2,447 adjectives and 1,814 verbs having
more than three translations in the training corpus.
The parallel corpus, which contains EU texts, is
more specialized than the comparable corpus built
from Wikipedia. This is not the ideal scenario for
this experiment; domain adaptation is important
for the type of semantic processing we want to ap-
ply as there might be a shift in the senses present in
the two corpora. However, as EU texts often con-
tain a lot of general vocabulary, we expect that this
discrepancy will not strongly affect the quality of
the results.
3.3 Gold standard
We evaluate the quality of the bilingual lexicons
extracted from the comparable corpus by compar-
ing them to a gold standard lexicon, which was
built from the aligned English (Fellbaum, 1998)
and Slovene wordnets (Fis?er and Sagot, 2008). We
extracted all English synsets from the Base Con-
cept sets that belong to the Factotum domain and
contain literals with polysemy levels 1-5 and their
Slovene equivalents which have been validated by
a lexicographer. Of 1,589 such synsets, 200 were
randomly selected and used as a gold standard for
automatic evaluation of the method proposed in
this paper.
4 Experimental setup
4.1 Overview of the method
Figure 1 gives an overview of the way informa-
tion mined from the parallel training corpus is ex-
ploited for discovering translations of source (En-
glish) words in the comparable corpus. The par-
allel corpus serves to extract an English-Slovene
seed lexicon and source language context vec-
tors (Par vectors) for the Slovene translations of
English words. These vectors form the input to
the Word Sense Induction (WSI) method which
groups the translations of an English word into
clusters.
The clusters of semantically related Slovene
translations constitute the candidate senses which,
together with the Par vectors, are used for dis-
ambiguating and translating the vectors extracted
from the source (English) side of the comparable
corpus (Comp source). The translated vectors are
then compared to the ones extracted from the tar-
get language (Slovene) side of the comparable cor-
pus (Comp target) and the best translations are se-
lected, for a list of unknown words. All steps of
the proposed method illustrated in Figure 1 will
be detailed in the following sections.
4.2 Translation clustering
The translations of the English words in the lex-
icon built as described in 3.2 are clustered ac-
cording to their semantic proximity using a cross-
lingual Word Sense Induction method (Apidi-
anaki, 2008). For each translation Ti of a word
w, a vector is built from the content word co-
3
Language POS Source word Slovene sense clusters
EN?SL
Nouns
sphere
{krogla} (geometrical shape)
{sfera, podroc?je} (area)
address
{obravnava, res?evanje, obravnavanje} (dealing with)
{naslov} (postal address)
portion
{kos} (piece)
{obrok, porcija} (serving)
{delez?} (share)
figure
{s?tevilka, podatek, znesek} (amount)
{slika} (image)
{osebnost} (person)
Verbs
seal
{tesniti} (to be water-/airtight)
{zapreti, zapec?atiti} (to close an envelope or some other container)
weigh
{pretehtati} (consider possibilities)
{tehtati, stehtati} (check weight)
educate
{pouc?iti} (give information)
{izobraz?evati, izobraziti} (give education)
consume
{potros?iti} (spend money/goods)
{uz?ivati, zauz?iti} (eat/drink)
Adjs
mature
{zrel, odrasel} (adult)
{zorjen, zrel} (ripe)
minor
{nepomemben} (not very important)
{mladoleten, majhen} (under 18 years old)
juvenile
{nedorasel} (not adult/biologically mature yet)
{mladoleten, mladoletnis?ki} (not 18/legally adult yet)
remote
{odmaknjen, odroc?en} (far away and not easily accessible)
{oddaljen daljinski} (controlled from a distance (e.g. remote control))
Table 1: Entries from the English-Slovene sense cluster inventory.
occurrences of w in the parallel sentences where it
is translated by Ti. Let N be the number of features
retained for each Ti from the corresponding source
contexts. Each feature Fj (1 ? j ? N) receives a
total weight with a translation Ti, tw(Fj,Ti), de-
fined as the product of the feature?s global weight,
gw(Fj), and its local weight with that translation,
lw(Fj,Ti). The global weight of a feature Fj is a
function of the number Ni of translations (Ti?s) to
which Fj is related, and of the probabilities (pi j)
that Fj co-occurs with instances of w translated by
each of the Ti?s:
gw(Fj) = 1?
?Ti pi j log(pi j)
Ni
(1)
Each pi j is computed as the ratio of the co-
occurrence frequency of Fj with w when translated
as Ti to the total number of features seen with Ti:
pi j =
cooc frequency(Fj,Ti)
N
(2)
The local weight lw(Fj,Ti) between Fj and Ti di-
rectly depends on their co-occurrence frequency:
lw(Fj,Ti) = log(cooc frequency(Fj,Ti)) (3)
The pairwise similarity of the translations is cal-
culated using the Weighted Jaccard Coefficient
(Grefenstette, 1994).
WJ(Tm,Tn) =
? j min(tw(Tm,Fj), tw(Tn,Fj))
? j max(tw(Tm,Fj), tw(Tn,Fj))
(4)
The similarity score of each translation pair is
compared to a threshold locally defined for each w
using an iterative procedure. The threshold (T ) for
a word w is initially set to the mean of the scores
(above 0) of its translation pairs. The set of trans-
lation pairs of w is then divided into two sets (G1
and G2) according to whether they exceed, or are
inferior to, the threshold. The average of scores of
the translation pairs in each set is computed (m1
and m2) and a new threshold is calculated that is
the average of m1 and m2 (T = (m1+m2)/2). The
new threshold serves to separate again the transla-
tion pairs into two sets, a new threshold is calcu-
lated and the procedure is repeated until conver-
gence.
The semantically similar translations of w are
grouped into clusters. Translation pairs with a
score above the threshold form initial clusters that
4
might be further enriched provided that there exist
additional strongly related translations. Cluster-
ing stops when all translations of w are clustered
and all their relations have been checked. An im-
portant feature of the algorithm is that it performs
soft clustering, so translations can be found in dif-
ferent clusters. The final clusters are characterized
by global connectivity, i.e. all their elements are
linked by pertinent relations.
Table 1 gives examples of clusters obtained for
English words of different PoS with clear sense
distinctions in the parallel corpus. For each En-
glish word, we provide the obtained clusters of
Slovene translations including a description of the
sense described by each cluster. For instance, the
translations for the adjective minor from the train-
ing corpus (nepomemben, mladoleten and majhen)
are grouped into two clusters describing its two
senses: {nepomemben} - ?not very important?
and {mladoleten, majhen} - ?under 18 years old?.
The resulting cluster inventory contains 13,352
clusters in total, for 8,892 words. 2,585 of the
words (1,518 nouns, 554 verbs and 513 adjectives)
have more than one cluster.
In the next section, we explain how the clus-
ters and the corresponding translation vectors are
used for disambiguating the source language vec-
tors extracted from the comparable corpus.
4.3 Cross-lingual vector comparison
4.3.1 Vector building
We build context vectors in the two languages for
nouns occurring at least 50 times in the compa-
rable corpus. The frequency threshold is impor-
tant for the lexicon extraction approach to produce
good results. As features we use three content
words to the left and to the right of the retained
nouns, stopping at the sentence boundary, without
taking into account their position. Log-likelihood
is used to calculate feature weights.
In the reported experiments we focus on the
1,000 strongest features. A portion of these fea-
tures is disambiguated for each headword, de-
pending on the availability of clustering informa-
tion. We observed that disambiguating a smaller
amount of features yielded similar results and in-
cluding additional features did not improve the re-
sults.
4.3.2 Vector translation and disambiguation
Translation correspondences between the two lan-
guages of the comparable corpus are identified by
comparing the source language vectors, built as
described in Section 4.3.1, to the ones of the candi-
date translations. This comparison serves to quan-
tify the similarity of the source and target words
represented by the vectors and the highest ranked
pairs are retained.
For the comparison to take place, the source
vectors have to be translated in the target language.
In most previous work, the vectors were translated
using external seed dictionaries: the first transla-
tion proposed for a word in the dictionary was
used to translate all instances of the word in the
vectors irrespective of their sense. Here, we re-
place the external dictionary with the output of
a data-driven cross-lingual WSD method (Apidi-
anaki, 2009) which renders the method knowledge
light and adaptable to other language pairs.
The translation clusters obtained during WSI
(cf. Section 4.2) describe the senses of the En-
glish words in the parallel corpus. We exploit this
sense inventory for disambiguating the features in
the English vectors extracted from the comparable
corpus. More precisely, we ask the WSD method
to select among the available clusters the one that
correctly translates in Slovene the sense of the En-
glish features in the vectors built from the compa-
rable corpus. The selection is performed by com-
paring information from the context of a feature,
which corresponds to the rest of the vector where
the feature appears, to the source language vectors
of the translations which served to their cluster-
ing. Inside the vectors, the features are ordered
according to their score, calculated as described in
Section 4.3.1. Feature weights filter out the weak
features, i.e. features with a score below the ex-
perimentally set threshold of 0.01. The retained
features are then considered as a bag of words.
On the clusters? side, the information used for
disambiguation is found in the source language
vectors that revealed the similarity of the transla-
tions. If common features (CFs) exist between the
context of a feature and the vectors of the transla-
tions in a cluster, a score is calculated correspond-
ing to the mean of the weights of the CFs with the
clustered translations, where weights correspond
to the total weights (tw?s) computed between fea-
tures and translations during WSI. In formula 5,
CFj is the set of CFs and NCF is the number of
translations Ti characterized by a CF.
wsd score =
?
NCF
i=1 ? j w(Ti,CFj)
NCF ? |CFj|
(5)
5
PoS Feature Assigned Cluster MFT
Nouns
party {oseba, stran, pogodbenica, stranka} stranka
matter {zadeva, vpras?anje} zadeva
Verbs
settle {urediti, res?iti, res?evati} res?iti
follow {upos?tevati, spremljati, slediti} slediti
Adjs
alternative {nadomesten, alternativen} alternativen
involved {vkljuc?en, vpleten} vkljuc?en
Table 2: Disambiguation results.
The cluster that receives the highest score is se-
lected and assigned to the feature as a sense tag.
The features are also tagged with their most fre-
quent translation (MFT) in the parallel corpus,
which sometimes already exists in the cluster se-
lected during WSD.
In Table 2, we present examples of disam-
biguated features of different PoS from the vec-
tor of the word transition. The context used for
disambiguation consists of the other strong fea-
tures in the vector and the cluster that best de-
scribes the sense of the features in this context
is selected. In the last column, we provide the
MFT of the feature in the parallel corpus. In the
examples shown here the MFT translation already
exists in the cluster selected by the WSD method
but this is not always the case. As we will show
in the Evaluation section, the configuration where
the MFT from the cluster assigned during disam-
biguation is selected (called CLMFT) gives better
results than MFT, which shows that the MFT in
the selected cluster is not always the most frequent
alignment for the word in the parallel corpus. Fur-
thermore, the clusters provide supplementary ma-
terial (i.e. multiple semantically correct transla-
tions) for comparing the vectors in the target lan-
guage and improving the baseline results. Still,
MFT remains a very powerful heuristic due to the
skewed distribution of word senses and transla-
tions.
4.4 Vector comparison
The translation clusters proposed during WSD for
the features in the vectors built from the source
side of the comparable corpus serve to translate the
vectors in the target language. In our experiments,
we compare three different ways of translating the
source language features.
1. by keeping the most frequent transla-
tion/alignment of the feature in the parallel
corpus (MFT);
2. by keeping the most frequent translation from
the cluster assigned to the feature during dis-
ambiguation (CLMFT); and
3. by using the same cluster as in the second ap-
proach, but producing features for all transla-
tions in the cluster with the same weight (CL).
The first approach (MFT) serves as the base-
line since, instead of the sense clustering and
WSD results, it just uses the most frequent
sense/alignment heuristic. In the first batch of ex-
periments, we noticed that the results of the CL and
CLMFT approaches heavily depend on the part-of-
speech of the features. So, we divided the CL and
CLMFT approaches into three sub-approaches:
1. translate only nouns, verbs or adjectives with
the clusters and other features with the MFT
approach (CLMFT N, CLMFT V, CLMFT A);
2. translate nouns and adjectives with the clus-
ters and verbs with the MFT approach
(CLMFT NA); and
3. translate nouns and verbs with the clus-
ters and adjectives with the MFT approach
(CLMFT NV).
The distance between the translated source and
the target-language vectors is computed by the
Dice metric. By comparing the translated source
vectors to the target language ones, we obtain a
ranked list of candidate translations for each gold
standard entry.
5 Evaluation
5.1 Metrics
The final result of our method consists in ranked
lists of translation candidates for gold standard en-
tries. We evaluate this output by the mean recipro-
cal rank (MRR) measure which takes into account
6
the rank of the first good translation found for each
entry. Formally, MRR is defined as
MRR =
1
|Q|
|Q|
?
i=1
1
ranki
(6)
where |Q| is the length of the query, i.e. the num-
ber of gold standard entries we compute transla-
tion candidates for, and ranki is the position of the
first correct translation in the candidate list.
5.2 Results
Table 4 shows the translation extraction results
for different configurations. The MFT score is
used as the baseline. We observe that disam-
biguating all features in the vectors (CL) yields
lower results than the baseline compared to se-
lecting only the most frequent translation from the
cluster which slightly outperforms the MFT base-
line. In the CLMFT N, CLMFT NA, CLMFT NV
configurations we disambiguate noun features,
nouns and adjectives, and nouns and verbs, respec-
tively, and translate words of other PoS using the
MFT. In CLMFT N, for instance, nouns are dis-
ambiguated while verbs and adjectives are trans-
lated by the word to which they were most fre-
quently aligned in the parallel corpus. The three
configurations where nouns are disambiguated
(CLMFT N, CLMFT NA, CLMFT NV) give better
results compared to those addressing verbs or ad-
jectives alone. Interestingly, disambiguating only
adjectives gives worse results than disambiguating
only verbs, but the combination of nouns and ad-
jectives outperforms the combination of nouns and
verbs.
In CLMFT, features of all PoS are disambiguated
but we only keep the most frequent translation in
the cluster and ignore the other translations. This
setting gives much better results than CL, where
the whole cluster is used, which highlights two
facts: first, that disambiguation is beneficial for
translation extraction and, second, that the noise
present in the automatically built clusters harms
the quality of the translations extracted from the
comparable corpus. The better score obtained for
CLMFT compared to MFT also shows that, in many
cases, the most frequent translation in the cluster
does not coincide with the most frequent align-
ment of the word in the parallel corpus. So, disam-
biguation helps to select a more appropriate trans-
lation than the MFT approach. This improvement
compared to the baseline shows again that WSD is
MRR
MFT 0.0685
CLMFT 0.0807
CL 0.0434
CLMFT N 0.0817
CLMFT A 0.07
CLMFT V 0.0714
CLMFT NA 0.0842
CLMFT NV 0.08048
Table 3: Results of the experiment.
MRR diff p-value
MFT CLMFT 0.0122 0.1830
MFT CL 0.0251 0.0410
CLMFT CL 0.0373 0.0120
MFT CLMFT NA 0.0157 0.4296
MFT CLMFT NV 0.0120 0.5195
Table 4: Comparison of different configurations.
useful in this setting.
In Table 4, the results for different configura-
tions are compared. The statistical significance of
the difference in the results was calculated by ap-
proximate randomization (1,000 repetitions). We
observe that the differences between the CL and
MFT configurations and the CL and CLMFT ones,
are statistically significant. This confirms that tak-
ing most frequent translations, disambiguated or
not, works better than exploiting all the informa-
tion in the clusters. The remainder of the dif-
ferences in the results are not statistically signif-
icant. One could wonder why the p-values are that
high in case of the MFT setting on one side and
CLMFT NA and CLMFT NV settings on the other
side although the differences in the results are not
that high. The most probable explanation is that
there is a low intersection in correct results and
errors. Because of that, flipping the results be-
tween the two systems ? as performed in approx-
imate randomization ? often generates differences
higher than the initial difference on the original re-
sults.
5.3 Qualitative analysis
Manual evaluation of the results shows that the
procedure can deal with concrete words much bet-
ter than with abstract ones. For example, the cor-
rect translation of the headword enquiry is the
third highest-ranked translation. The results are
7
also much better with monosemous and domain-
specific terms (e.g. the correct translation for cat-
aclysm is the top-ranking candidate). On the other
hand, general and polysemous expressions that
can appear in a wide range of contexts are a much
tougher nut to crack. For example, the correct
translation candidate for word role, which can be
used in a variety of contexts as well as metaphor-
ically, is in the tenth position, whereas no correct
translation was found for transition. However, it
must be noted that even if the correct translation is
not found in the results, the output of our method
is in most cases a very coherent and solid descrip-
tion of the semantic field of the headword in ques-
tion. This means that the list can still be useful for
lexicographers to illicit the correct translation that
is missing, or organize the vocabulary in terms of
their relational-semantic principles.
We have also performed an error analysis in
cases where the correct translation could not be
found among the candidates, which consisted of
checking the 30 strongest disambiguated features
of an erroneously translated headword. We ob-
served cases where the strongest features in the
vectors are either very abstract and generic or too
heterogeneous for our method to be able to per-
form well. This was the case with the headwords
characterisation, antecedent and thread. In cases
where the strongest features represented the con-
cept clearly but the correct translation was not
found, we examined cluster, WSD and MFT qual-
ity, as suggested by the parallel corpus. The main
source of errors in these cases is the noise in the
clusters which is often due to pre-processing er-
rors, especially in the event of multi-word expres-
sions. It seems that clustering is also problematic
for abstract or generic words, where senses might
be lumped together. The WSD step, on the other
hand, does not seem to introduce noise to the pro-
cedure as it is correct in almost all the cases we
have examined.
6 Discussion and conclusion
We have shown how cross-lingual WSD can be
applied to bilingual lexicon extraction from com-
parable corpora. The disambiguation of source
language features using translation clusters con-
stitutes the main contribution of this work and
presents several advantages. First, the method per-
forms disambiguation by using sense descriptions
derived from the data, which clearly differentiates
our method from the approaches based on external
lexicons and extends its applicability to resource-
poor languages. The translation clusters acquired
through WSI serve to disambiguate the features in
the source language context vectors and to pro-
duce less noisy translated vectors. An additional
advantage is that the sense clusters often contain
more than one translation and, therefore, provide
supplementary material for the comparison of the
vectors in the target language.
The results show that data-driven semantic anal-
ysis can help to circumvent the need for an exter-
nal seed dictionary, traditionally considered as a
prerequisite for translation extraction from paral-
lel corpora. Moreover, it is clear that disambiguat-
ing the vectors improves the quality of the ex-
tracted lexicons and manages to beat the simpler,
but yet powerful, most frequent translation heuris-
tic. These encouraging results pave the way to-
wards pure data-driven methods for bilingual lex-
icon extraction. This knowledge-light approach
can be applied to languages and domains that do
not dispose of large-scale seed dictionaries but for
which parallel corpora are available.
An avenue that we intend to explore in future
work is to extract translations corresponding to
different senses of the headwords. Up to now,
research on translation extraction has most of-
ten aimed the identification of one good trans-
lation for a source word in the comparable cor-
pus. This has also been the case because most
works have focused on identifying translations
for specialized terms that do not convey differ-
ent senses. However, words in a general lan-
guage corpus like Wikipedia can be polysemous
and it is important to identify translations corre-
sponding to their different senses. Moreover, pol-
ysemy makes the translation extraction procedure
more difficult, as features corresponding to differ-
ent senses are mingled in the same vector. A way
to discover translations corresponding to different
word senses would be to apply a monolingual WSI
method on the source side of the comparable cor-
pus which would group the closely related usages
of the headwords together, and to then build vec-
tors for each usage group hopefully describing a
distinct sense. Using the generated sets of vectors
separately will allow to extract translations corre-
sponding to different senses of the source words.
8
References
Marianna Apidianaki, Nikola Ljubes?ic?, and Darja
Fis?er. 2012. Disambiguating vectors for bilin-
gual lexicon extraction from comparable corpora.
In Eighth Language Technologies Conference, pages
10?15, Ljubljana, Slovenia.
Marianna Apidianaki. 2008. Translation-oriented
sense induction based on parallel corpora. In Pro-
ceedings of the 6th International Conference on
Language Resources and Evaluation (LREC-08),
pages 3269?3275, Marrakech, Morocco.
Marianna Apidianaki. 2009. Data-driven Semantic
Analysis for Multilingual WSD and Lexical Selec-
tion in Translation. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-09),
pages 77?85, Athens, Greece.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Applied
Natural Language Processing (ANLP-2000), Seat-
tle, WA.
Herve? De?jean, Eric Gaussier, Jean-Michel Renders,
and Fatiha Sadat. 2005. Automatic processing of
multilingual medical terminology: applications to
thesaurus enrichment and cross-language informa-
tion retrieval. Artificial Intelligence in Medicine,
33(2):111?124, February.
Tomaz? Erjavec, Darja Fis?er, Simon Krek, and Nina
Ledinek. 2010. The JOS Linguistically Tagged
Corpus of Slovene. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), Valletta, Malta.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Darja Fis?er and Nikola Ljubes?ic?. 2011. Bilingual lexi-
con extraction from comparable corpora for closely
related languages. In Proceedings of the Inter-
national Conference Recent Advances in Natural
Language Processing 2011, pages 125?131, Hissar,
Bulgaria. RANLP 2011 Organising Committee.
Darja Fis?er and Beno??t Sagot. 2008. Combining mul-
tiple resources to build reliable wordnets. In TSD
2008 - Text Speech and Dialogue, Lecture Notes in
Computer Science, Brno, Czech Republic. Springer.
Pascale Fung. 1998. Machine translation and the in-
formation soup, third conference of the association
for machine translation in the americas, amta ?98,
langhorne, pa, usa, october 28-31, 1998, proceed-
ings. In AMTA, volume 1529 of Lecture Notes in
Computer Science. Springer.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA.
Amir Hazem and Emmanuel Morin. 2012. Ica for
bilingual lexicon extraction from comparable cor-
pora. In Proceedings of the 5th Workshop on Build-
ing and Using Comparable Corpora (BUCC), Istan-
bul, Turkey.
Hiroyuki Kaji. 2003. Word sense acquisition from
bilingual comparable corpora. In HLT-NAACL.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9?16.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
Erwin Marsi and Emiel Krahmer. 2010. Automatic
analysis of semantic similarity in comparable text
through syntactic tree matching. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 752?760,
Beijing, China, August. Coling 2010 Organizing
Committee.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. Computational Lin-
guistics, 31(4):477?504.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Pablo Gamallo Otero. 2007. Learning bilingual lexi-
cons from comparable english and spanish corpora.
In Proceedings of MT Summit XI, pages 191?198.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Machine Translation Summit 2009, page 8.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 519?526, College Park, Maryland, USA,
June. Association for Computational Linguistics.
Xabier Saralegi, In?aki San Vicente, and Antton Gur-
rutxaga. 2008. Automatic extraction of bilingual
terms from comparable corpora in a popular sci-
ence domain. In Proceedings of the Building and
using Comparable Corpora workshop, 6th Interna-
tional Conference on Language Resources and Eval-
uations (LREC), Marrakech, Morocco.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of the International Conference on New Methods
in Language Processing, pages 44?49, Manchester,
UK.
9
Li Shao and Hwee Tou Ng. 2004. Mining new
word translations from comparable corpora. In Pro-
ceedings of Coling 2004, pages 618?624, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
Matthew G. Snover, Bonnie J. Dorr, and Richard M.
Schwartz. 2008. Language and translation model
adaptation using comparable corpora. In EMNLP,
pages 857?866.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, and Dan Tufi. 2006.
The jrc-acquis: A multilingual aligned parallel cor-
pus with 20+ languages. In In Proceedings of the 5th
International Conference on Language Resources
and Evaluation (LREC?2006), pages 2142?2147.
Kun Yu and Junichi Tsujii. 2009. Extracting bilin-
gual dictionary from comparable corpora with de-
pendency heterogeneity. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 121?124, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
10
