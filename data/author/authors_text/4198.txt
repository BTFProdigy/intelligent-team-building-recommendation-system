Robust Interpretation of User Requests for Text Retrieval in a Multimodal
Environment
Alexandra Klein and Estela Puig-Waldmu?ller
Austrian Research Institute for
Artificial Intelligence
Schottengasse 3
A-1010 Vienna, Austria
falex, stellag@oefai.at
Harald Trost
Department of Medical Cybernetics and
Artificial Intelligence, University of Vienna
Freyung 6/2
A-1010 Vienna, Austria
harald@ai.univie.ac.at
Abstract
We describe a parser for robust and flexible inter-
pretation of user utterances in a multi-modal sys-
tem for web search in newspaper databases. Users
can speak or type, and they can navigate and follow
links using mouse clicks. Spoken or written queries
may combine search expressions with browser com-
mands and search space restrictions. In interpreting
input queries, the system has to be fault-tolerant to
account for spontanous speech phenomena as well
as typing or speech recognition errors which often
distort the meaning of the utterance and are difficult
to detect and correct. Our parser integrates shallow
parsing techniques with knowledge-based text re-
trieval to allow for robust processing and coordina-
tion of input modes. Parsing relies on a two-layered
approach: typical meta-expressions like those con-
cerning search, newspaper types and dates are iden-
tified and excluded from the search string to be sent
to the search engine. The search terms which are
left after preprocessing are then grouped according
to co-occurrence statistics which have been derived
from a newspaper corpus. These co-occurrence
statistics concern typical noun phrases as they ap-
pear in newspaper texts.
1 Introduction
In this paper we describe a parser for robust and
flexible interpretation of user utterances in a web-
based multi-modal text retrieving system. The
parser forms part of a system for web search in Aus-
trian newspaper databases. In this system, users can
formulate queries or navigation commands using ut-
terances in both spontaneous spoken or written lan-
guage, and they can navigate and follow links using
mouse clicks. Users are completely free in formu-
lating their utterances and in the use and combina-
tion of the input modes. Typed and spoken utter-
ances may contain combinations of query expres-
sions, browser commands and search space restric-
tions. Users may search for texts with a specific
date, in a specific newspaper or in a specific sec-
tion of a newspaper. They may give complex con-
text descriptions of the texts and they may refer to
previously found texts. A dialogue manager stores
actions and results from previous states and supplies
information in order to construct fully specified for-
mal queries from underspecified user requests.
In order to allow for this freedom in user be-
haviour, flexible processing modules are needed.
For every utterance, the parser and the dialogue
manager must come up with an adequate interpre-
tation. At the same time, in interpreting the in-
put, they have to be robust and fault-tolerant. They
have to cope with typical phenomena of sponta-
neous speech like hesitation, correction and repe-
tition. There may be typographical errors in written
input or ? even more difficult to deal with ? speech
recognition errors from the spoken queries. Such
errors often distort the meaning of the utterance and
are difficult to detect and correct.
In our interpretation component, shallow pars-
ing techniques and knowledge-based text retrieval
methods are combined to allow for robust process-
ing and coordination of input modes. We employ
a two-layered approach. The first layer serves to
separate structure from content, i.e., parts of utter-
ances referring to browser commands and search re-
strictions (temporal expressions, newspaper types or
sections) are analyzed with a combination of key-
word spotting and pattern recognition. The under-
lying assumption is that users will restrict them-
selves to a rather small vocabulary and a limited
range of expressions in expressing this sort of in-
formation (this assumption is also confirmed by our
Wizard-of-Oz experiments). During this process,
stop words (function words and other words typi-
cally not contributing to the content of the query)
are also removed. The remaining words ? which are
assumed to describe the search content ? are then
grouped according to co-occurrence statistics which
have been derived from a newspaper corpus. While
text retrieval with the help of linguistic process-
ing has become rather common, multimodal inter-
action with textual databases on the web is a fairly
recent application of Natural Language Process-
ing. Experience from text retrieval shows that most
information is expressed in adjective-noun, noun-
preposition-noun, and noun-verb groups (Grefen-
stette, 1992). In our specific domain, the third type
can be neglected, because verbs typically denote the
action ? mostly search ? which is already extracted
in the first layer. Thus, co-occurrence statistics con-
sist of typical noun phrases as they appear in news-
paper texts.
2 Empirical evidence and user
experiments
In order to assess user behaviour, we carried
out Wizard-of-Oz experiments (Fraser and Gilbert,
1991). Speech recognition and text retrieval were
simulated. In different sessions the users interacted
with a number of versions of the system: single in-
put mode versions and versions with combinations
of input modes. Their performance in terms of num-
ber of interactions as well as task completion time
was measured, and their comments regarding the in-
terface and the (simulated) system were collected
in a questionnaire. Users were grouped according
to previous experience with search engines and the
web in general. Our results show that both, be-
ginners and advanced users, preferred multimodal
interaction over single input modes, and beginners
in particular were able to speed up task completion
times significantly with the help of a combination of
spoken and written input with mouse clicks (Klein
et al, 2001).
From these experiments, we also obtained a cor-
pus of written and spoken utterances which were
considered in the further design of the system. The
queries which were posed by the users in spoken
language were recorded. The recorded utterances
were later read to a speech recognition system. This
gave us an impression of the number and type of er-
rors to be expected in dealing with queries in spon-
taneous speech.
3 NL Text or Speech Input: Language
Analysis
Users can access articles with spoken or typed ut-
terances. Web queries may relate to the way some
particular piece of infomation is presented and what
this information refers to. They may also express
browser commands or a combination of browser and
query commands while referring either to structure
(Search for Noll in the previous newspaper) or to
content (Search for Noll in the sports? section).1
Within our application web queries may relate to
the way some particular piece of information is pre-
sented (e.g. the browser?s history about the accessed
pages), and what this information refers to (e.g. the
section a search string belongs to). To successfully
interpret such an utterance, one needs to analyze its
structure to find out which of these command modes
the utterance can be assigned to. This is done in a
two-step process. First, each word is looked up in a
lexicon and assigned a semantic category. Second,
certain rules are applied to strings of these seman-
tic categories. As a result, commands and search
restrictions are recognized and the rest of the utter-
ance is passed to search expression interpretation.
3.1 Keyword Spotting and Semantic
Classification
We will now describe in more detail how the user?s
input is parsed within the Natural Language In-
terface, and structured into either search patterns
? consisting of search strings, sections, dates and
timeranges, that are understood by the search en-
gine of the newspaper ? or commands for the Java
browser. Structure is analyzed by a flexible bottom-
up parser using a rule-based mechanism with simple
syntactic patterns.
In the user?s query input, each word of the utterance
is looked up in a lexicon and - if found - assigned a
corresponding semantic category. This lexicon con-
tains a small list of semantic categories, that we con-
sider important for the interpretation of an utterance
in the domain of searching articles and browsing.
The lexicon assigns semantic classes for closed cat-
egories that are:
 nouns denoting search, newspaper, section, links
like ?Suche? (search) or ?Artikel? (article).
 nouns expressing a specific section like
?Wirtschaft? (economy).
 nouns expressing a specific page like ?Homepage?.
 temporal expressions and temporal prepositions
like ?Monat? (month) and ?vor? (ago).
1We will use the italic font for language expressions and the
typewriter font for meta-language expressions.
 expressions indicating something new like in a
?neue? (new) search.
 adjectives and adverbs indicating direction in time
or space, like in ?letzte? (previous) search or in
?letzte? (last) week.
 cardinal and ordinal numbers used in conjunction
with temporal expressions and link expressions,
like in ?zwei? (two) years ago or when opening the
?ersten? (first) link.
 adverbs and connectives indicating constraints on
search mode, like ?nur? (only) and ?nicht? (not).
 prepositions indicating whether the request was to
browse or to search, cf ?zum Sport? (to the sports?
section) versus ?im Sport? (within the sports? sec-
tion).
 stop words.
All words found within the lexicon are replaced
by their corresponding semantic classes, search ex-
pressions are marked as such, and stop words are
deleted.
We distinguish between semantic atoms and se-
mantic classes: atoms by itself do not have a mean-
ing that can be used for searching or browsing com-
mands. They have to be joined following a given
set of rules to form a semantic class. To yield
such a class, rules are applied in ? mostly ? one to
three steps. However, rules are not always neces-
sary, a word may also be mapped onto a semantic
class right away. Our lexicon has about 30 semantic
atoms, from which about 40 semantic classes can be
formed. Certain patterns of semantic classes which
we obtain through lexical look-up can be assigned
new meanings via rules. So, by composing the indi-
vidual meanings, another more abstract meaning is
defined. This compositional approach to interpreta-
tion is supported by the layered approach. The re-
sult of this process is a list of chunks2, where stress
is laid on the content words. The advantage of con-
centrating on chunks is ? especially within German,
a language with a relatively free word order ? that
the order in which chunks occur is much more flex-
ible than the order of words within chunks. This
approach might be too shallow for a deeper seman-
tic analysis, but is sufficient for our needs. So, e.g.
2According to Abney (1991) a chunk is defined in terms of
major heads where a major head is any content word that does
not appear between a function word f and the content word
f selects, OR a pronoun selected by a preposition. [...] The
typical chunk consists of a single content word surrounded by
a constellation of function words, matching a fixed template.
?letzte? (last) plus a time expression would together
yield the new meaning date -1w. To overcome
ambiguities and avoid potential rule conflicts, rules
spanning larger chunks have a higher priority and
are thus preferred, such that ?zuru?ck zum Sport?
(back to sports) would win over ?zuru?ck? (back).
If no rules can be applied to a semantic class, it will
be ignored in the final interpretation.
Summing up the process of the structure analysis,
the partial analyses are stored, a sequence of partial
analyses from the set of rules is chosen, and then
combined to yield larger structures.
3.2 Search String Filter: Extraction of
Adjective-Noun Pairs
In the next step, the content of the query must be an-
alyzed in more detail. In this chapter we will explain
how content analysis is done in our application.
From a corpus of Austrian newspaper texts,
adjective-noun- and adjective-proper-name pairs
were extracted and counted. These pairs were stored
and consulted in query interpretation. Since the
texts are tagged manually, the lists of adjectives and
nouns/proper names contain a considerable number
of errors. Therefore it is necessary to use large
amounts of text; it may even be useful to eventu-
ally introduce a threshold so that only adjective-
noun/proper-name pairs which appear more than
once or a certain number of times are considered.
This of course can not prevent systematic tagging
errors.
A robust stemming algorithm maps all adjective-
noun/proper-name pairs to an approximate ?stem?,
thus eliminating flectional forms which result in
morphological variation which is typical for the
German language. For the purpose of creating a
repository of co-occurrence pairs, we do not care
about proper stemming. Rather, it is our aim to map
various inflectional forms onto one base form.
Spelling variations, numbers etc. are smoothed
as far as it is possible in automatic processing. For
example, ordinal numbers which are labelled as ad-
jectives are reduced to a placeholder for numbers.
Whenever a word is encountered in processing
which can be considered an adjective, it is kept.
Whenever the following word may be a noun or a
proper name, it is checked whether the adjective-
noun/proper-name combination is contained in the
repository of adjective-noun/proper-name combina-
tions which has previously been extracted from a
corpus. If the adjective-noun/proper-name combi-
nation is found, it is passed on to the search engine
as a query. Whenever the combination has not oc-
curred in the corpus, only the noun or proper name
is considered a key word.
Again, inflectional variations as well as different
spellings etc. are mapped onto base forms as far
as possible. The same stemming algorithm is used
which was employed in creating the repository of
adjective-noun/proper name pairs. The robust (and
rough) stemming and categorization algorithms pro-
duce a certain amount of mistakes in the lists of
pairs as well as in the mapping process, but tak-
ing into account larger text corpora evens out these
problems as more text is processed.
Our approach distinguishes noun phrases which
have a record of co-occurrence from noun phrases
which may be spontaneous expressions or modifica-
tions or even errors created by users. For example,
the phrase ?europa?ische Staaten? (European coun-
tries) would be retained while ?beteiligte Staaten?
(participating countries) would be reduced to the
noun. Some adjectives used in search expressions
serve to qualify the global search expression rather
than the noun or proper name in quesion. For ex-
ample, a search for yesterday?s speech would only
yield articles from the day after a speech, not about
the speech in general.
4 Action History: Integration of the
knowledge sources
Multimodal dialogue requires a unified interpreta-
tion of the involved knowledge sources, all input
modes have to be considered. The information
transmitted needs to be interpreted within discourse
context including previous user actions, possibly
with data coming from other input modes.
After the analysis of the user utterance has been
performed in the pattern-matching and the search-
word-extraction modules, the computed meaning of
the utterance has to been interpreted in the context
of the discourse sitiuation. This concerns mostly
the history of previous queries. Here, it is impor-
tant to consult previous queries in all possible input
modes (spoken, typed, mouse clicks). Therefore, a
record of the action history is kept and consulted.
All typed, written and spoken actions are assigned
an entry in the action history where the main param-
eters and their values are collected.
With this contextual information, the meaning of
the user?s utterance as the sum of the results of the
component analyses is computed in the global dis-
course context. Underspecified queries can be in-
terpreted in the discourse contexts, and parameters
are filled. Thus, the results are combined into one
unambiguous command line.
A powerful interaction control is necessary in or-
der to recognize the user?s intent by comparing it to
what the system knows about the addressed entities
and their relation to each other as well as to the data
which are accessible at the specific moment in the
interaction. The interface language between the lan-
guage analysis module and the controller consists of
a fixed set of parameters, which are assigned appro-
priate values:
 DIRECTION
the direction for browsing (forward, backward)
 SECTION
the section in the newspaper (politics, sports, ...)
 SEARCHSTRING
the string which has to be searched by the newspa-
per search engine
 DATE
the date when the article to be searched has ap-
peared (also intervals)
 ZEITUNG (NEWSPAPER)
the newspaper which is supposed to be searched
 OPENLINK
the link in a document which should be followed in
the browser
 OPENURL
the URL which is supposed to be opened by the
browser
The outcome ? or left-hand side ? of a rule-based
simplification can be divided into three command
types:
 Simple Search Command, New Search Com-
mand: E.g. ?Suche nach Camilleri im Kulturres-
sort? (Search for Camilleri in the cultural section)
or ?Neue Suche beginnen mit Krimis? (Start a new
search on thrillers).
 Complex Search Command: Search using the Ac-
tion history. E.g. ?Suche nach Christie im letzten
Ressort? (Search for Christie in the previous sec-
tion).
 Simple History Browsing Commands: Normal
Browsing using the Accessed Page History. E.g.
?Zur na?chsten Seite gehen? (Go to the next page).
 Complex History Browsing Commands: Browse
using the Action history. E.g. ?Geh zum let-
zten Ressort? (Go to the last ressort) or ?Zuru?ck
zur Suche mit Montalbano gehen? (Go back to the
search containing Montalbano).
 WWW Browsing: E.g. ?Geh zum heutigen Sport-
bereich? (Go to today?s sport section), ?den Stan-
dard lesen? (read the Standard) or ?Geh zur Home-
page? (Go home).
 Opening Link Command: E.g.?den ersten Artikel
o?ffnen? (Open the first article)
The action history browsing command refers to
the timeline and the point of reference of a brows-
ing but also of a search command. For instance,
take an utterance, where someone wants to search
for a topic but within a context that was defined in
the previous search. For our application, we would
first have to locate the user?s point of reference and
then execute her search command. If there is no
given reference, we assume by default that a new
time point is created in our time line.
One such command could look like this: the
utterance ?Ich suche etwas u?ber Highsmith im
letzten Ressort? (I am looking for something
about Highsmith within the previous section) would
be mapped to: DIRECTION 0, SECTION x
(where x is the section of the action with in-
dex -1), SEARCHSTRING Highsmith, TIME
nil, ZEITUNG nil. We are not moving in the
timeline, instead we are adding a new search action,
thus the direction is zero. Anyway, the controller
has to look up the action history to fill the value of
the section. The values of all empty parameters will
be filled with the values of the last actions, so in our
example, these parameters have not been explicitly
filled and remain empty (nil).
5 Result: Translating into Http Request or
Browser Command
After the command has been processed by the con-
trol module, it is either executed by the Java browser
or translated into a GET method through an Http
request to the newspaper?s archive database. The
resulting articles are displayed in the Java browser,
another search can be started by the user.
6 Conclusion
We have presented an interpretation component for
natural language user input in a web-based multi-
modal text retrieval system. By applying well-
known and simple methods from shallow parsing
and knowledge-based text retrieval and integrating
them in a novel way we have succeeded in creating
a robust, flexible and efficient parser for our appli-
cation.
An important feature is the distinction between
those parts of utterances relating to structure and
those relating to content. This is achieved by tak-
ing advantage of the fact that only a limited vocab-
ulary and set of expressions are used for the former.
This allows us to employ simple rule-based tech-
niques for their interpretation. The identification of
the content on the other hand is done with the help
of a co-occurrence repository, at the moment con-
sisting of adjective-noun/proper name pairs. In the
future we will have to investigate whether search re-
sults can be improved by inserting other combina-
tions, like noun-preposition-noun triples.
Acknowledgements
This work was supported by the Austrian Science
Fund (FWF) under project number P-13704. Finan-
cial support for ?OFAI is provided by the Austrian
Federal Ministry of Education, Science and Culture.
References
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, editors,
Principle-Based Parsing, Tu?bingen (Germany).
Kluwer Academic Publishers.
Norman M. Fraser and G. Nigel Gilbert. 1991.
Simulating speech systems. Computer Speech
and Language, 5(1):81?99.
Gregory Grefenstette. 1992. Use of Syntactic Con-
text to Produce Term Association Lists for Text
Retrieval. In N.J. Belkin, P. Ingwersen, and A.M.
Pejtersen, editors, Proceedings of the 15th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval, pages 89?97, Copenhagen: Denmark.
ACM Press.
Alexandra Klein, Ingrid Schwank, Michel
Ge?ne?reux, and Harald Trost. 2001. Evaluating
Multimodal Input Modes in a Wizard-of-Oz
Study for the Domain of Web Search. In Ann
Blandford, Jean Vanderdonckt, and Phil Gray,
editors, People and Computer XV ? Interaction
without Frontiers: Joint Proceedings of HCI
2001 and IHM 2001, pages 475?483. Springer:
London, September.
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 19?25,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Identifying Segment Topics in Medical Dictations
Johannes Matiasek, Jeremy Jancsary
Alexandra Klein
Austrian Research Institute for
Artificial Intelligence
Freyung 6, Wien, Austria
firstname.lastname@ofai.at
Harald Trost
Department of Medical Cybernetics
and Artificial Intelligence
of the Center for Brain Research,
Medical University Vienna, Austria
harald.trost@meduniwien.ac.at
Abstract
In this paper, we describe the use of lexi-
cal and semantic features for topic classi-
fication in dictated medical reports. First,
we employ SVM classification to assign
whole reports to coarse work-type cate-
gories. Afterwards, text segments and
their topic are identified in the output
of automatic speech recognition. This
is done by assigning work-type-specific
topic labels to each word based on fea-
tures extracted from a sliding context win-
dow, again using SVM classification uti-
lizing semantic features. Classifier stack-
ing is then used for a posteriori error cor-
rection, yielding a further improvement in
classification accuracy.
1 Introduction
The use of automatic speech recognition (ASR) is
quite common in the medical domain, where for
every consultation or medical treatment a written
report has to be produced. Usually, these reports
are dictated and transcribed afterwards. The use of
ASR can, thereby, significantly reduce the typing
efforts, but, as can be seen in figure 1, quite some
work is left.
complaint dehydration weakness and diarrhea full
stop Mr. Will Shawn is a 81-year-old cold Asian
gentleman who came in with fever and Persian
diaper was sent to the emergency department by his
primary care physician due him being dehydrated
period . . . neck physical exam general alert and
oriented times three known acute distress vital
signs are stable . . . diagnosis is one chronic
diarrhea with hydration he also has hypokalemia
neck number thromboctopenia probably duty liver
cirrhosis . . . a plan was discussed with patient in
detail will transfer him to a nurse and facility
for further care . . . end of dictation
Figure 1: Raw output of speech recognition
When properly edited and formatted, the same
dictation appears significantly more comprehensi-
ble, as can be seen in figure 2.
CHIEF COMPLAINT
Dehydration, weakness and diarrhea.
HISTORY OF PRESENT ILLNESS
Mr. Wilson is a 81-year-old Caucasian gentleman
who came in here with fever and persistent
diarrhea. He was sent to the emergency department
by his primary care physician due to him being
dehydrated.
. . .
PHYSICAL EXAMINATION
GENERAL: He is alert and oriented times three,
not in acute distress.
VITAL SIGNS: Stable.
. . .
DIAGNOSIS
1. Chronic diarrhea with dehydration. He also
has hypokalemia.
2. Thromboctopenia, probably due to liver
cirrhosis.
. . .
PLAN AND DISCUSSION
The plan was discussed with the patient in detail.
Will transfer him to a nursing facility for
further care.
. . .
Figure 2: A typical medical report
Besides the usual problem with recognition er-
rors, section headers are often not dictated or hard
to recognize as such. One task that has to be per-
formed in order to arrive at the structured report
shown in figure 2 is therefore to identify topical
sections in the text and to classify them accord-
ingly.
In the following, we first describe the problem
setup, the steps needed for data preparation, and
the division of the classification task into subprob-
lems. We then describe the experiments performed
and their results.
In the outlook we hint at ways to integrate this
approach with another, multilevel, segmentation
framework.
2 Data Description and Problem Setup
Available corpus data consists of raw recognition
results and manually formatted and corrected re-
ports of medical dictations. 11462 reports were
19
available in both forms, 51382 reports only as cor-
rected transcripts. When analysing the data, it
became clear that the structure of segment topics
varied strongly across different work-types. Thus
we decided to pursue a two-step approach: firstly
classify reports according to their work-type and,
secondly, train and apply work-type specific clas-
sification models for segment topic classification.
2.1 Classification framework
For all classification tasks discussed here, we em-
ployed support-vector machines (SVM, Vapnik
(1995)) as the statistical framework, though in dif-
ferent incarnations and setups. SVMs have proven
to be an effective means for text categorization
(Joachims, 1998) as they are capable to robustly
deal with high-dimensional, sparse feature spaces.
Depending on the task, we experimented with dif-
ferent feature weighting schemes and SVM kernel
functions as will be described in section 3.
2.2 Features used for classification
The usual approach in text categorization is to use
bag-of-word features, i.e. the words occuring in a
document are collected disregarding the order of
their appearance. In the domain of medical dic-
tation, however, often abbreviations or different
medical terms may be used to refer to the same se-
mantic concept. In addition, medical terms often
are multi-word expressions, e.g., ?coronary heart
disease?. Therefore, a better approach for feature
mapping is needed to arrive at features at an ap-
propriate generalization level:
? Tokenization is performed using a large
finite-state lexicon including multi-word
medical concepts extracted from the UMLS
medical metathesaurus (Lindberg et al,
1993). Thus, multi-word terms remain intact.
In addition, numeric quantities in special
(spoken or written) formats or together with a
dimension are mapped to semantic types (e.g.
?blood pressure? or ?physical quantity?), also
using a finite-state transducer.
? The tokens are lemmatized and, if possi-
ble, replaced by the UMLS semantic con-
cept identifier(s) they map to. Thus,
?CHD?, ?coronary disease? and ?coronary
heart disease? all map to the same concept
?C0010068?.
? In addition, also the UMLS semantic type, if
available, is used as a feature, so, in the ex-
ample above, ?B2.2.1.2.1? (Disease or Syn-
drome) is added.
? Since topics in a medical report roughly fol-
low an order, for the segment topic identifica-
tion task also the relative position of a word
in the report (ranging from -1 to +1) is used.
We also explored different weighting schemes:
? binary: only the presence of a feature is in-
dicated.
? term frequency: the number of occurences
of a feature in the segment to be classified is
used as weight.
? TFIDF: a measure popular from information
retrieval, where tfidfi,j of term ti in docu-
ment dj ? D is usually defined as
cti,j
?
i cti,j
. log
|D|
|{dj : ti ? dj}|
An example of how this feature extraction pro-
cess works is given below:
token(s) feature(s) comment
...
an stop word
78 year old QH OLD pattern-based type
female C0085287 UMLS concept
A2.9.2 UMLS semtype
intubated intubate lemmatized (no concept)
with stop word
lung cancer C0242379 UMLS concept
C0684249 UMLS concept
B2.2.1.2.1.2 UMLS semtype
...
2.3 Data Annotation
For the first classification task, i.e. work-type clas-
sification, no further annotation is necessary, ev-
ery report in our data corpus had a label indicating
the work-type. For the segment topic classification
task, however, every token of the report had to be
assigned a topic label.
2.3.1 Analysis of Corrected Transcripts
For the experiments described here, we con-
centrated on the ?Consultations? work-type, for
which clear structuring recommendations, such
as E2184-02 (ASTM International, 2002), exist.
However, in practice the structure of medical re-
ports shows high variation and deviations from
the guidelines, making it harder to come up with
20
an appropriate set of class labels. Therefore, us-
ing the aforementioned standard, we assigned the
headings that actually appeared in the data to the
closest type, introducing new types only when ab-
solutely necessary. Thus we arrived at 23 heading
classes. Every (possibly multi-word) token was
then labeled with the heading class of the last sec-
tion heading occurring before it in the text using a
simple parser.
2.3.2 Aligment and Label Transfer
When inspecting manually corrected reports (cf.
fig. 2), one can easily identify a heading and clas-
sify the topic of the text below it accordingly.
However, our goal is to develop a model for iden-
tifying and classifying segments in the dictation,
thus we have to map the annotation of corrected
reports onto the corresponding ASR output. The
basic idea here is to align the tokens of the cor-
rected report with the tokens in ASR output and to
copy the annotations (cf. figure 3). There are some
problems we have to take care of during align-
ment:
1. non-dictated items in the corrected test (e.g.
punctuation, headings)
2. dictated words that do not occur in the cor-
rected text (meta instructions, repetitions)
3. non-identical but corresponding items
(recognition errors, reformulations)
For this alignment task, a standard string-edit
distance based method is not sufficient. There-
fore, we augment it with a more sophisticated cost
function. It assigns tokens that are similar (ei-
ther from a semantic or from a phonetic point of
view) a low cost for substitution, whereas dissimi-
lar tokens receive a prohibitively expensive score.
Costs for deletion and insertion are assigned in-
versely. Semantic similarity is computed using
Wordnet (Fellbaum, 1998) and UMLS. For pho-
netic matching, the Metaphone algorithm (Philips,
1990) was used (for details see Huber et al (2006)
and Jancsary et al (2007)).
3 Experiments
3.1 Work-Type Categorization
In total we had 62844 written medical reports
with assigned work-type information from differ-
ent hospitals, 7 work-types are distinguished. We
randomly selected approximately a quarter of the
corrected report OP ASR output
. . . . . . . . . . . . . . .
ChiefCompl CHIEF del
ChiefCompl COMPLAINT sub complaint ChiefCompl
ChiefCompl Dehydration sub dehydration ChiefCompl
ChiefCompl , del
ChiefCompl weakness sub weakness ChiefCompl
ChiefCompl and sub and ChiefCompl
ChiefCompl diarrhea sub diarrhea ChiefCompl
ChiefCompl . sub fullstop ChiefCompl
HistoryOfP Mr. sub Mr. HistoryOfP
HistoryOfP Wilson sub Will HistoryOfP
ins Shawn HistoryOfP
HistoryOfP is sub is HistoryOfP
HistoryOfP a sub a HistoryOfP
HistoryOfP 81-year-old sub 81-year-old HistoryOfP
HistoryOfP Caucasian sub cold HistoryOfP
HistoryOfP ins Asian HistoryOfP
HistoryOfP gentleman sub gentleman HistoryOfP
HistoryOfP who sub who HistoryOfP
HistoryOfP came sub came HistoryOfP
HistoryOfP in del
HistoryOfP here sub here HistoryOfP
HistoryOfP with sub with HistoryOfP
HistoryOfP fever sub fever HistoryOfP
HistoryOfP and sub and HistoryOfP
HistoryOfP persistent sub Persian HistoryOfP
HistoryOfP diarrhea sub diaper HistoryOfP
HistoryOfP . del
. . . . . . . . . . . . . . .
Figure 3: Mapping labels via alignment
reports as the training set, the rest was used for
testing. The distribution of the data can be seen in
table 1.
Trainingset Testset Work-Type
649 4.1 1966 4.2 CACardiology
7965 51.0 24151 51.1 CL ClinicalReports
1867 11.9 5590 11.8 CNConsultations
1120 7.2 3319 7.0 DS DischargeSummaries
335 2.1 878 1.8 ER EmergencyMedicine
2185 14.0 6789 14.4 HP HistoryAndPhysicals
1496 9.6 4534 9.6 OROperativeReports
15617 47227 Total
Table 1: Distribution of Work-types
As features for categorization, we used a bag-
of-words approach, but instead of the surface form
of every token of a report, we used its semantic
features as described in section 2.2. As a catego-
rization engine, we used LIBSVM (Chang&Lin,
2001) with an RBF kernel. The features where
weighted with TFIDF. In order to compensate for
different document length, each feature vector was
normalized to unit length. After some param-
eter tuning iterations, the SVM model performs
really well with a microaveraged F11 value of
0.9437. This indicates high overall accuracy, and
the macroaveraged F1 value of 0.9341 shows, that
also lower frequency categories are predicted quite
reliably. The detailed results are shown in table 2.
Thus the first step in the cascaded model, i.e.
the selection of the work-type specific segment
1F1 = 2?precision?recallprecision+recall
21
predicted rec. prec. F1
true CA CL CN DS ER HP OR
CA 1966 1882 53 5 6 0 9 11 0.9573 0.9787 0.9679
CL 24151 25 23675 217 13 18 155 48 0.9803 0.9529 0.9664
CN 5590 1 447 4695 7 17 413 10 0.8399 0.8814 0.8601
DS 3319 1 37 8 3241 2 27 3 0.9765 0.9818 0.9792
ER 878 0 90 7 10 754 13 4 0.8588 0.9425 0.8987
HP 6789 4 512 393 22 7 5838 13 0.8599 0.9040 0.8814
OR 4534 10 31 2 2 2 3 4484 0.9890 0.9805 0.9847
microaveraged 0.9437
macroaveraged 0.9341
Table 2: Work-Type categorization results
topic model, yields reliable performance.
3.2 Segment Topic Classification
In contrast to work-type categorization, where
whole reports need to be categorized, the identifi-
cation of segment topics requires a different setup.
Since not only the topic labels are to be deter-
mined, but also segment boundaries are unknown
in the classification task, each token constitutes
an example under this setting. Segments are then
contiguous text regions with the same topic label.
It is clearly not enough to consider only features
of the token to be classified, thus we include also
contextual and positional features.
3.2.1 Feature and Kernel Selection
In particular, we employ a sliding window ap-
proach, i.e. for each data set not only the token to
be classified, but also the 10 preceding and the 10
following tokens are considered (at the beginning
or towards the end of a report, context is reduced
appropriately). This window defines the text frag-
ment to be used for classifying the center token,
and features are collected from this window again
as described in section 2.2. Additionaly, the rela-
tive position (ranging from -1 to +1) of the center
token is used as a feature.
The rationale behind this setup is that
1. usually topics in medical reports follow an or-
dering, thus relative position may help.
2. holding features also from adjacent segments
might also be helpful since topic succession
also follows typical patterns.
3. a sufficiently sized context might also smooth
label assignment and prevent label oscilla-
tion, since the classification features for ad-
jacent words overlap to a great deal.
A second choice to be made was the selection
of the kernel best suited for this particular classifi-
cation problem. In order to get an impression, we
made a preliminary mini-experiment with just 5
reports each for training (4341 datasets) and test-
ing (3382 datasets), the results of which are re-
ported in table 3.
Accuracy
Feature Weight linear RBF
TFIDF 0.4977 0.3131
TFIDF normalized 0.5544 0.6199
Binary 0.6417 0.6562
Table 3: Preliminary Kernel Comparison
While these results are of course not significant,
two things could be learned from the preliminary
experiment:
1. linear kernels may have similar or even better
performance,
2. training times with LIBSVM with a large
number of examples may soon get infeasible
(we were not able to repeat this experiment
with 50 reports due to excessive runtime).
Since LibSVM solves linear and nonlinear
SVMs in the same way, LibSVM is not particu-
larly efficient for linear SVMs. Therefore we de-
cided to switch to Liblinear (Fan et al, 2008), a
linear classifier optimized for handling data with
millions of instances and features2.
2Indeed, training a model from 669 reports (463994 ex-
amples) could be done in less then 5 minutes!
22
predicted class label (#)
# True Label Total F1 . . . 3 4 . . . 14 . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
3 Diagnosis 40871 0.603 . . . 24391 2864 . . . 8691 . . .
4 DiagAndPlan 21762 0.365 . . . 5479 6477 . . . 7950 . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
14 Plan 31729 0.598 . . . 5714 3419 . . . 21034 . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
Table 4: Confusion matrix (part of)
3.2.2 Segment Topic Classification Results
Experiments were performed on a randomly se-
lected subset of reports from the ?Consultations?
work-type (1338) that were available both in cor-
rected form and in raw ASR output form. An-
notations were constructed for the corrected tran-
scripts, as described in section 2.3, transfer of la-
bels to the ASR output was performed as shown in
section 2.3.2.
Both data sets were split into training and test
sets of equal size (669 reports each), experiments
with different feature weighting schemes have
been performed on both corrected data and ASR
output. The overall results are shown in table 5.
corrected reports ASR output
micro- macro- micro- macro-
Feature weights avg.F1 avg.F1 avg.F1 avg.F1
TFIDF 0.7553 0.5178 0.7136 0.4440
TFIDF norm. 0.7632 0.3470 0.7268 0.3131
Binary 0.7693 0.4636 0.7413 0.3953
Table 5: Segment topic classification results
Consistently, macroaveraged F1 values are
much lower than their microaveraged counterparts
indicating that low-frequency topic labels are pre-
dicted with less accuracy.
Also, segment classification works better with
corrected reports than with raw ASR output. The
reason for that behaviour is
1. ASR data are more noisy due to recognition
errors, and
2. while in corrected reports appropriate section
headers are available (not as header, but the
words) this is not necessarily the case in ASR
output (also the wording of dictated headers
and written headers may be different).
A general note on the used topic labels must
also be made: Due to the nature of our data it
was inevitable to use topic labels that overlap in
some cases. The most prominent example here is
?Diagnosis?, ?Plan?, and ?Diagnosis and Plan?.
The third label clearly subsumes the other two, but
in the data available the physicians often decided
to dictate diagnoses and the respective treatment
in an alternating way, associating each diagnosis
with the appropriate plan. This made it necessary
to include all three labels, with obvious effects that
could easily seen when inspecting the confusion
matrix, a part of which is shown in table 4.
When looking at the misclassifications in these
3 categories it can easily be seen, that they are pre-
dominantly due to overlapping categories.
Another source of problems in the data is the
skewed distribution of segment types in the re-
ports. Sections labelled with one of the four la-
bel categories that weren?t predicted at all (Chief-
Complaints, Course, Procedure, and Time, cf. ta-
ble 6) occur in less than 2% of the reports or are
infrequent and extremely short. This fact had, of
course, undesirable effects on the macroavered F1
scores. Additional difficulties that are similar to
the overlap problem discussed above are strong
thematic similarities between some section types
(e.g., Findings and Diagnosis, or ReasonForEn-
counter andHistoryOfPresentIllness) that result in
a very similar vocabulary used.
Given these difficulties due to the data, the re-
sults are encouraging. There is, however, still
plenty of room left for improvement.
3.3 Improving Topic Classification
Liblinear does not only provide class label pre-
dictions, it is also possible to obtain class proba-
bilities. The usual way then to predict the label
is to choose the one with the highest probability.
When analysing the errors made by the segment
topic classification task described above, it turned
out that often the correct label was ranked second
or third (cf. table 6). Thus, the idea of just taking
23
correct prediction in
Label count best best 2 best 3
Allergies 3456 29.72 71.64 85.21
ChiefComplai 697
Course 30
Diagnosis 43565 64.69 83.29 91.37
DiagAndPlan 19409 35.24 70.45 86.81
DiagnosticSt 35554 82.47 91.34 93.05
Findings 791 0.38 1.26
Habits 2735 7.31 32.69 41.76
HistoryOfPre 122735 92.26 97.55 98.20
Medication 14553 85.87 93.38 95.22
Neurologic 5226 54.08 86.93 89.19
PastHistory 43775 71.13 86.26 88.82
PastSurgical 5752 49.32 78.88 84.47
PhysicalExam 86031 93.56 97.01 97.57
Plan 36476 62.57 84.63 94.65
Practitioner 1262 55.07 76.78 82.73
Procedures 109
ReasonForEnc 15819 25.42 42.35 43.47
ReviewOfSyst 29316 79.81 89.90 91.87
Time 58
Total 467349 76.93 88.65 92.00
Table 6: Ranked predictions
the highest ranked class label could be possibly
improved by a more informed choice.
While the segment topic classifier already takes
contextual features into account, it has still no in-
formation on the classification results of the neigh-
boring text segments. However, there are con-
straints on the length of text segments, thus, e.g.
a text segment of length 1 with a different topic la-
bel than the surrounding text is highly implausible.
Furthermore, there are also regularities in the suc-
cession of topic labels, which can be captured by
the monostratal local classification only indirectly
? if at all.
A look at figure 4 exemplifies how a bet-
ter informed choice of the label could result in
higher prediction accuracy. The segment labelled
?PastHistory? correctly ends 4 tokens earlier than
predicted, and, additionally, this label erroneously
is predicted again for the phrase ?progressive
weight loss?. The correct label, however, has still
a rather high probability in the predicted label
distribution. By means of stacking an additional
classier onto the first one we hope to be able to
correct some of the locally made errors a posteri-
ori.
The setup for the error correction classifier
we experimented with was as follows (it was
performed only for the segment topic classi-
fier trained on ASR output with binary feature
weights):
1. The training set of the classifier was clas-
Label probabilities (%)
True Label Predicted ... 10 11 12 ... 17 18
. . .
= PastHistory [11] age PastHistory 0 95 0 0 0
= PastHistory [11] 63 PastHistory 0 95 0 0 0
= PastHistory [11] and PastHistory 0 95 0 0 1
= PastHistory [11] his PastHistory 0 95 0 0 1
= PastHistory [11] father PastHistory 0 88 0 0 9
= PastHistory [11] died PastHistory 0 90 0 0 8
= PastHistory [11] from PastHistory 0 84 0 0 14
= PastHistory [11] myocardial infa PastHistory 0 81 0 0 17
= PastHistory [11] at PastHistory 0 77 0 0 20
= PastHistory [11] age PastHistory 0 78 0 1 19
= PastHistory [11] 57 PastHistory 0 78 0 1 19
= PastHistory [11] period PastHistory 0 78 0 1 19
- ReviewOfSyst[18] review PastHistory 0 76 0 1 20
- ReviewOfSyst[18] of PastHistory 0 76 0 1 21
- ReviewOfSyst[18] systems PastHistory 0 78 0 0 19
- ReviewOfSyst[18] he PastHistory 1 57 0 1 37
= ReviewOfSyst[18] has ReviewOfSyst 1 32 0 1 58
= ReviewOfSyst[18] had ReviewOfSyst 1 32 0 1 58
- ReviewOfSyst[18] progressive PastHistory 1 49 0 1 42
- ReviewOfSyst[18] weight loss PastHistory 1 60 0 1 32
= ReviewOfSyst[18] period ReviewOfSyst 1 31 0 0 62
= ReviewOfSyst[18] his ReviewOfSyst 1 13 0 1 81
= ReviewOfSyst[18] appetite ReviewOfSyst 1 13 0 1 81
. . .
Figure 4: predicted label probabilites
sified, and the predicted label probabilities
were collected as features.
2. Again, a sliding window (with different
sizes) was used for feature construction. Fea-
tures were set up for each label at each win-
dow position and the respective predicted la-
bel probability was used as its value.
3. A linear classifier was trained on these fea-
tures of the training set
4. This classifier was applied to the results of
classifying the test set with the original seg-
ment topic classifier.
Three different window sizes were used on the
corrected reports, only one window was applied
on ASR output (cf. table 7). As can be seen, each
corrected reports ASR output
micro- macro- micro- macro-
context window avg.F1 avg.F1 avg.F1 avg.F1
No correction 0.7693 0.4636 0.7413 0.3953
[?3, +3] 0.7782 0.4773 - -
[?6, +0] 0.7798 0.4754 - -
[?3, +4] 0.7788 0.4769 0.7520 0.4055
Table 7: A posteriori correction results
context variant improved on both microaveraged
and macroaveraged F1 in a range of 0,9 to 1.4 per-
cent points. Thus, stacked error correction indeed
is possible and able to improve classification re-
sults.
24
4 Conclusion and Outlook
We have presented a 3 step approach to seg-
ment topic identification in dictations of medi-
cal reports. In the first step, a categorization of
work-type is performed on the whole report us-
ing SVM classification employing semantic fea-
tures. The categorization model yields good per-
formance (over 94% accuracy) and is a prerequi-
site for subsequent application of work-type spe-
cific segment classification models.
For segment topic detection, every word was as-
signed a class label based on contextual features
in a sliding window approach. Here also semantic
features were used as a means for feature gener-
alisation. In various experiments, linear models
using binary feature weights had the best perfor-
mance. A posteriori error correction via classifier
stacking additionally improved the results.
When comparing our results to the results of
Jancsary et al (2008), who pursue a multi-level
segmentation aproach using conditional random
fields optimizing over the whole report, the locally
obtained SVM results cannot compete fully. On
label chain 2, which is equivalent to segment top-
ics as investigated here, Jancsary et al (2008) re-
port an estimated accuracy of 81.45 ? 2.14 % on
ASR output (after some postprocessing), whereas
our results, even with a posteriori error correction,
are at least 4 percent points behind. This is prob-
ably due to the fact that the multi-level annotation
employed in Jancsary et al (2008) contains addi-
tional information useful for the learning task, and
constraints between the levels improve segmenta-
tion behavior at the segment boundaries. Never-
theless, our approach has the merit of employing a
framework that can be trained in a fraction of the
time needed for CRF training, and classification
works locally.
An investigation on how to combine these two
complementary approaches is planned for the fu-
ture. The idea here is to use the probability distri-
butions on labels returned by our approach as (ad-
ditional) features in the CRF model. It might be
possible to leave out some other features currently
employed in return, thereby reducing model com-
plexity. The benefit we hope to get by doing so are
shorter training time for CRF training, and, since,
contrary to CRFs, SVMs are a large margin classi-
fication method, hopefully the CRF model can be
improved by the present approach.
Acknowledgments
The work presented here has been carried out in
the context of the Austrian KNet competence net-
work COAST. We gratefully acknowledge fund-
ing by the Austrian Federal Ministry of Economics
and Labour, and ZIT Zentrum fuer Innovation und
Technologie, Vienna. The Austrian Research In-
stitute for Artificial Intelligence is supported by
the Austrian Federal Ministry for Transport, Inno-
vation, and Technology and by the Austrian Fed-
eral Ministry for Science and Research.
References
ASTM International. 2002. ASTM E2184-02: Stan-
dard specification for healthcare document formats.
C.-C. Chang and C.-J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classification. Journal of Machine Learning
Research, 9(2008):1871?1874.
C. Fellbaum. 1998. WordNet: an electronic lexical
database. MIT Press, Cambridge, MA.
M. Huber, J. Jancsary, A. Klein, J. Matiasek, H. Trost.
2006. Mismatch interpretation by semantics-driven
alignment. Proceedings of Konvens 2006.
J. Jancsary, A. Klein, J. Matiasek, H. Trost. 2007.
Semantics-based Automatic Literal Reconstruction
Of Dictations. In Alcantara M. and Declerck
T.(eds.), Semantic Representation of Spoken Lan-
guage 2007 (SRSL7) Universidad de Salamanca,
Spain, pp. 67-74.
J. Jancsary, J. Matiasek, H. Trost. 2008. Reveal-
ing the Structure of Medical Dictations with Con-
ditional Random Fields. Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, pp. 1?10.
T. Joachims. 1998. Text Categorization with Sup-
port Vector Machines: Learning with Many Rele-
vant Features. Proceedings of the European Confer-
ence on Machine Learning. Springer, pp. 137?142.
D.A.B. Lindberg, B.L. Humphreys, A.T. McCray.
1993. The Unified Medical Language System.
Methods of Information in Medicine, (32):281-291.
Lawrence Philips. 1990. Hanging on the metaphone.
Computer Language, 7(12).
V.N. Vapnik 1995. The Nature of Statistical Learning
Theory. Springer.
25
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 22?28,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Domain Knowledge about Medications to Correct Recognition Errors
in Medical Report Creation
Stephanie Schreitter
Alexandra Klein
Johannes Matiasek
Austrian Research Institute
for Artificial Intelligence (OFAI)
Freyung 6/6
1010 Vienna, Austria
firstname.lastname@ofai.at
Harald Trost
Section for Artificial Intelligence
Center for Med. Statistics, Informatics,
and Intelligent Systems
Medical University of Vienna
Freyung 6/2
1010 Vienna, Austria
harald.trost@meduniwien.ac.at
Abstract
We present an approach to analysing auto-
matic speech recognition (ASR) hypotheses
for dictated medical reports based on back-
ground knowledge. Our application area is
prescriptions of medications, which are a fre-
quent source of misrecognitions: In a sam-
ple report corpus, we found that about 40%
of the active substances or trade names and
dosages were recognized incorrectly. In about
25% of these errors, the correct string of words
was contained in the word graph. We have
built a knowledge base of medications based
on information contained in the Unified Med-
ical Language System (UMLS), consisting
of trade names, active substances, strengths
and dosages. From this, we generate a va-
riety of linguistic realizations for prescrip-
tions. Whenever an inconsistency in a pre-
scription is encountered on the best path of
the word graph, the system searches for alter-
native paths which contain valid linguistic re-
alizations of prescriptions consistent with the
knowledge base. If such a path exists, a new
concept edge with a better score is added to
the word graph, resulting in a higher plausi-
bility for this reading. The concept edge can
be used for rescoring the word graph to obtain
a new best path. A preliminary evaluation led
to encouraging results: in nearly half of the
cases where the word graph contained the cor-
rect variant, the correction was successful.
1 Introduction
Automatic speech recognition (ASR) is widely used
in the domain of medical reporting. Users appreciate
the fact that the records can be accessed immediately
after their creation and that speech recognition pro-
vides a hands-free input mode, which is important as
physicians often simultaneously handle documents
such as notes and X-rays (Alapetite et al, 2009).
A drawback of using ASR is the fact that speech-
recognition errors have to be corrected manually by
medical experts before the resulting texts can be
used for electronic patient records, quality control
and billing purposes. This manual post-processing is
time-consuming, which slows down hospital work-
flows.
A number of recognition errors could be avoided
by incorporating explicit domain knowledge. We
consider prescriptions of medications a good start-
ing point as they are common and frequent in the
various medical fields. Furthermore, they contain
trade names and dosages, i.e. proper names and dig-
its, which are frequently misrecognized by ASR in
all domains.
For our approach, we have extracted and adapted
information about medications from the Unified
Medical Language System (UMLS) (Lindberg et al,
1993). This data contains information about trade
names, active substances, strengths and dosages and
can easily be modified, e.g. when new medications
are released.
In the first step, we assessed the potential for im-
provement by analyzing a sample corpus of medical
reports. It turned out that in 4383 dictated reports
which were processed by a speech-recognition sys-
tem, the word-error rate for medications was about
40%, which is slightly higher than the the average
word-error rate of the reports. Examining a sample
22
of word graphs for the reports, we realized that in
about 30% of these errors, the correct string of words
was contained in the word graph, but not ranked as
the best path.
In the following sections, we will first give
an overview of previous approaches to detecting
speech-recognition errors and semantic rescoring of
word-graph hypotheses. Then, we will describe
how we have adapted information about medications
from the UMLS to enhance the word graph with
concept nodes representing domain-specific infor-
mation. Finally, we will illustrate the potential for
improving the speech-recognition result by means
of an evaluation of word graphs for medical reports
which were processed by our system.
2 Extraction of Medication Information,
Error Handling and Semantic Rescoring
(Gold et al, 2008) gives an overview on extract-
ing structured medication information from clinical
narratives. Extracted medication information may
serve as a base for quality control, pharmaceutical
research and the automatic creation of Electronic
Health Records (EHR) from clinical narratives. The
i2b2 Shared Task 2009 focussed on medication ex-
traction, e.g. (Patrick and Li, 2009; Halgrim et al,
2010). These approaches work on written narrative
texts from clinical settings, which may have been
typed by physicians, transcribed by medical tran-
scriptionists or recognized by ASR and corrected by
medical transcriptionists.
In contrast, our approach takes as input word
graphs produced by an ASR system from dictated
texts and aims at minimizing the post-processing re-
quired by human experts.
Speech-recognition systems turn acoustic input
into word graphs, which are directed acyclic graphs
representing the recognized spoken forms and their
confidence scores (Oerder and Ney, 1993). In most
speech-recognition systems, meaning is implicitly
represented in the language model (LM), indicat-
ing the plausibility of sequences of words in terms
of n-grams. It has often been stated that the intro-
duction of an explicit representation of the utterance
meaning will improve recognition results. Naturally,
this works best in limited domains: the larger an
application domain, the more difficult it is to build
an optimal knowledge representation for all possi-
ble user utterances. Limited domains seem to be
more rewarding with regard to coverage and perfor-
mance. Consequently, combining speech recogni-
tion and speech understanding has so far mostly re-
sulted in applications in the field of dialogue systems
where knowledge about the domain is represented in
terms of the underlying database, e.g. (Seneff and
Polifroni, 2000).
Several approaches have investigated the poten-
tial of improving the mapping between the user ut-
terance and the underlying database by constructing
a representation of the utterance meaning. Mean-
ing analysis is either a separate post-processing step
or an integral part of the recognition process. In
some approaches, the recognition result is analyzed
with regards to content to support the dialogue man-
ager in dealing with inconsistencies (Macherey et
al., 2003). As far as dictated input is concerned,
which is not controlled by a dialogue manager, (Voll,
2006) developed a post-ASR error-detection mech-
anism for radiology reports. The hybrid approach
uses statistical as well as rule-based methods. The
knowledge source UMLS is employed for measur-
ing the semantic distance between concepts and for
assessing the coherence of the recognition result.
In other approaches, the analysis of meaning
is integrated into the recognition process. Se-
mantic confidence measurement annotates recogni-
tion hypotheses with additional information about
their assumed plausibility based on semantic scores
(Zhang and Rudnicky, 2001; Sarikaya et al, 2003).
(Gurevych and Porzel, 2003; Gurevych et al, 2003)
present a rescoring approach where the hypothe-
ses in the word graph are reordered according to
semantic information. Usually, conceptual parsers
are employed which construct a parse tree of con-
cepts representing the input text for mapping be-
tween the recognition result and the underlying rep-
resentation. Semantic language modeling (Wanget
al., 2004; Buehler et al, 2005) enhances the lan-
guage model to incorporate sequences of concepts
which are considered coherent and typical for a spe-
cific context. In these approaches, the representa-
tions of the underlying knowledge are created spe-
cially for the applications or are derived from a text
corpus.
In our approach, we aim at developing a prototype
23
for integrating available knowledge sources into the
analysis of the word graph during the recognition
process. We have decided not to integrate the com-
ponent directly into the ASR system but to introduce
a separate post-processing step for the recognition of
information about medications with the word graphs
as interface. This makes it easier to update the med-
ication knowledge base, e.g. if new medications are
released. Furthermore, it is not necessary to retrain
the ASR system language model for each new ver-
sion of the medication knowledge base.
3 Knowledge Base and Text Corpus
For our approach, we prepared a knowledge base
concerning medications and dosages, and we used
a corpus of medical reports, dictated by physicians
in hospitals. The ASR result and a manual transcrip-
tion is available for each report. For a subset of the
corpus, word graphs could be obtained. By aligning
the recognition result with the manual transcriptions,
error regions can be extracted.
3.1 Knowledge Base
As it is our aim to find correct dosages of med-
ications in the word graph, we built a domain-
specific knowledge base which contains medica-
tions and strengths as they occur in prescriptions.
In our sample of medical reports, about 1/3 of the
medications occurred as active ingredients while the
rest were trade names. Therefore, both had to be
covered in our knowledge base which is based on
RxNorm (Liu et al, 2005). RxNorm is a standard-
ized nomenclature for clinical drugs and drug de-
livery devices and part of UMLS, ensuring a broad
coverage of trade names and active ingredients. Of
several available versions of RxNorm, the semantic
branded drug form is the most suitable one for our
purposes as it contains pharmaceutical ingredients,
strengths, and trade names. For example, the trade
name Synthroid R? is listed as follows:
Thyroxine 0.025 MG Oral Tablet [Synthroid R?]
Thyroxine is the active ingredient with the dosage
value 0.025 and the dosage unit milligrams. The
dosage unit form is oral tablet.
We used a RxNorm version with 1,508 active sub-
stances and 7,688 trade names (11,263 trade names
counting the different dosages). The active ingre-
dients in RxNorm are associated with Anatomical
Therapeutic Chemical (ATC) Codes.
3.2 Sample Corpus
The corpus is a random sample of 924 clinical re-
ports which were dictated by physicians from var-
ious specialties and hospitals. The dications were
processed by an ASR system and transcribed by hu-
man experts. Word graphs marked with the best path
(indicating the highest acoustic and language-model
scores) represent the recognition result. Tradenames
are part of the recognition lexicon, but they are fre-
quently misrecognized.
Of the 9196 medications (i.e. trade names and
active substances) in RxNorm, only 330 (3.6%) ap-
peared in the sample corpus.
We searched the corpus for recognition errors
concerning trade names, active ingredients and their
dosages by comparing the manual transcriptions to
the best paths in the word graphs, and a list of the
mismatches (i.e. recognition errors) and their fre-
quencies was compiled. It turned out that 39.3% of
all trade names and active ingredients were recog-
nized incorrectly. The average ASR word-error rate
of the reports was 38.1%. Aproximately 1-2% of the
trade names were not covered by RxNorm.
4 Approach
Our approach consists of a generation mechanism
which anticipates possible spoken forms for the
content of the knowledge base. The word graphs
are searched for trade names or active substances
and, subsequently, matching dosages. New concept
edges are inserted if valid prescriptions are found in
the word graph.
4.1 Detecting Medications in the Word Graph
The (multi-edge) word graphs are scanned, and the
words associated with each edge are compared to the
medications in the knowledge base. Figure 1 shows
a word graph consisting of hypotheses generated by
ASR, which is the input to our system. The dashed
edges indicate the best path, while dotted lines are
hypotheses which are not on the best path.
24
Figure 1: Sample word graph fragment
In case a match, i.e. a trade name or an active sub-
stance, is found, all edges succeeding the medica-
tion edge are searched for dosage values and dosage
units. So far, we only examine the context to the
right-hand side; in the data, we did not encounter
any medications where the dosage occurred before
the trade name or active substance. The following
kinds of fillers between the trade name or active sub-
stance and the dosage are allowed: ?to? and ?of?
as well as non-utterances such as hesitation, noise
and silence; in the corpus, we did not encounter any
other fillers.
4.2 Generation of Spoken Forms and Mapping
The medication found in the word graph is looked up
in RxNorm, and all possible spoken forms of valid
dosage values and dosage units for this medication
are generated. Spoken forms for the medication
names consist of the trade names and the active sub-
stances. Variation in the pronunciation of the trade
names or active substances is handled by the ASR
recognition lexicon. For generating spoken forms of
the dosage values, finite-state tools were used. For
dosage units, we wrote a small grammar. Looking
at two examples, the medication Synthroid R? and
Colace R? (the latter appears in the word graphs in
Figure 2 and Figure 1), the spoken forms shown in
Table 1 are generated. Each box contains the al-
ternative spoken variants. Synthroid R? contains the
active substance Thyroxine and Colace R? contains
the active substance Docusate; users may either re-
fer to the trade name or the active substance, so both
possibilities are generated for each medication and
dosage. RxNorm does not contain the dosage unit
?mcg? (micrograms), which occurred in the reports.
Therefore, microgram dosage values were converted
to milligrams. Since both ?miligram(s)? and ?mi-
crogram(s)? may occur for Synthroid R?, dosage val-
ues for both dosage units are generated. Although
strictly, ?twenty five? and ?twenty-five? are identical
spoken forms, both versions may appear in the word
graph and thus are provided by our system.
Sometimes, a medication may contain several ac-
tive substances, e.g. Hyzaar R?, a medication against
high blood pressure:
Hydrochlorothiazide 12.5 MG / Losartan 50 MG
Oral Tablet [Hyzaar]
25
trade name/ dosage value dosage unit
active
substance
?Synthroid? ?zero point zero two five? ?milligram?
?Thyroxine? ?zero point O two five? ?milligrams?
?O point zero two five?
?O point O two five?
?point zero two five?
?point O two five?
?twenty five? ?microgram?
?twenty-five? ?micrograms?
?two five?
?Colace? ?one hundred? ?miligram?
?Docusate? ?a hundred? ?miligrams?
?hundred?
Table 1: Generated spoken forms found in the word graph
In these cases, the generation of possible spoken
forms also includes different permutations of sub-
stances, as well as a spoken forms containing the
dosage unit either only at the end or after each value
if the dosage unit is identical.
4.3 Inserting Concept Edges
The sequences of words which constitute the word
graph are compared to the spoken forms generated
for the RxNorm knowledge base. The active sub-
stances or trade names serve as a starting point: in
case a trade name is found in the word graph, the
spoken forms for dosages of all active substances are
generated in all permutations. If an active substance
is found in the word graph, only the spoken forms
for the substance dosage are searched in the word
graph.
A new concept edge is inserted into the word
graph for each path matching one of the generated
spoken forms of the medications data base. The in-
serted concept edges span from the first matching
node to the last matching node on the path. Fig-
ure 2 shows the word graph from Figure 1 with an in-
serted concept edge (in bold). For each inserted con-
cept edge, new concept-edge attributes are assigned
containing the IDs of the original edges as children,
their added scores plus an additional concept score
and the sequence of words. Since no large-scale ex-
periments have yet been carried out, so far the con-
cept score which is added to the individual scores of
the children is an arbitrary number which improves
the score of the medication subpath in constrast to
paths which do not contain valid medication infor-
mation. If several competing medication paths are
found, a concept edge is inserted for each path, and
the concept edges can be ranked according to their
acoustic and language-model scores.
5 Evaluation
In the first step, we examined a report sample in or-
der to determine if there are cases where a valid pre-
scription is recognised although the physician did
not mention a prescription. We did not encounter
this phenomenon in our report corpus.
We then applied our method to a sample of 924
word graphs. In this sample,
? 481 valid dosages could be found, although
? only 325 of these were on the best path.
With our approach, for the 156 prescriptions
(32%) which were not on the best path, alternatives
could be reconstructed from the word graph. Based
on the inserted concept edges, the best path can be
rescored.
In order to measure recall, i.e. how many of all
existing prescriptions in the reports can be detected
with our knowledge base, we manually checked
a sample of 132 reports (containing manual tran-
scriptions and ASR results). In this sample, 85 er-
rors concerning medications and/or prescriptions oc-
curred. For 19 of the 85 errors, the correct result was
contained in the word graph. For 8 errors, it could
be reconstructed. So about 9% of the errors concern-
ing medications can be corrected in our sample. For
the cases where the prescription could not be recon-
structed although it was contained in the word graph,
an analysis of the errors is shown in Table 2.
Since new medications are constantly being re-
leased, and trade names change frequently, mis-
matches may be due to the fact that our version of
RxNorm was from a more recent point in time than
the report corpus. We assume that under real-world
conditions, both RxNorm and the medications pre-
scribed by physicians reflect the current situation.
Some problems concerning medication names
and dosage units were caused by missing spoken
forms containing abbreviations, e.g. of dosage units
(mg vs. mg/ml) or names (Lantus vs. Lantus in-
sulin). Here, the coverage needs to be improved.
26
Figure 2: Sample word graph fragment with inserted concept node (left)
Table 2: Error types found in manual evaluation
type of error # example
Word Graph RxNorm
differences in medication names 3 Cardizem CD 120 mg Cardizem 120 mg
between the knowledge base and the word graph
differences in dosage values 4 Tapazole 60 mg Tapazole 10 mg
between the knowledge base and the word graph
differences in dosage units 4 Epogen 20000 units Epogen 20000 ml
between the knowledge base and the word graph
There are also cases where two medications appear
in the word graph, and both had the valid prescrip-
tion strength, therefore the system was not able to
determine the correct medication.
6 Conclusion
In this paper, we present an attempt to reduce
the number of speech-recognition errors concern-
ing prescriptions of medications based on a domain-
specific knowledge base. Our approach uses word
graphs as input and creates new versions of the word
graph with inserted concept edges if more plausi-
ble prescriptions are found. The concept edges can
be used for rescoring the best path. An evaluation
showed that 32% of prescriptions found in the word
graphs were not on the best path but could be re-
constructed. The manual evaluation of 132 reports
shows that our method covers 42% of the prescrip-
tions which are actually spoken during the dictation.
At present, we have only investigated the reduc-
tion of medication misrecognitions in our evalua-
tion. In a larger evaluation, we will determine the ac-
tual impact of our method on the word-error rate of
medical reports. Furthermore, we are working on in-
tegrating additional available knowledge sources so
that the plausibility of prescriptions can also be as-
27
sessed from a broader medical point of view, e.g. in
case two subsequent prescriptions are encountered
in the word graph which are incompatible due to
drug interactions. As a next step, the system can
be extended to compare the prescriptions with the
patient record, e.g. if a patient has medication al-
lergies. So far, our simple solution integrating only
available, constantly updated knowledge about med-
ications has already turned out to be a good starting
point for rescoring word graphs based on domain
knowledge.
Acknowledgments
The work presented here has been carried out in
the context of the Austrian KNet competence net-
work COAST. We gratefully acknowledge funding
by the Austrian Federal Ministry of Economics and
Labour, and ZIT Zentrum fuer Innovation und Tech-
nologie, Vienna. The Austrian Research Institute
for Artificial Intelligence is supported by the Aus-
trian Federal Ministry for Transport, Innovation, and
Technology and by the Austrian Federal Ministry
for Science and Research. The authors would like
to thank the anonymous reviewers for their helpful
comments.
References
A. Alapetite, A., H.B. Andersen, H.B. and M. Hertzumb.
Acceptance of speech recognition by physicians: A
survey of expectations, experiences, and social in-
fluence. International Journal of Human-Computer
Studies 67(1) (2009) 36?49
D. Bu?hler, W. Minker and A. Elciyanti. Us-
ing language modelling to integrate speech
recognition with a flat semantic analysis. In:
6th SIGdial Workshop on Discourse and Di-
alogue, Lisbon, Portugal (September 2005)
http://www.sigdial.org/workshops/workshop6/proceed-
ings/pdf/86-paper.pdf.
S. Gold, N. Elhadad, X. Zhu, J.J. Cimino, G. Hripcsak.
Extracting Structured Medication Event Information
from Discharge Summaries. In: Proceedings of the
AMIA 2008 Symposium.
I. Gurevych and R. Porzel. Using knowledge-based
scores for identifying best speech recognition hy-
pothesis. In: Proceedings of ISCA Tutorial and
Research Workshop on Error Handling in Spoken
Dialogue Systems, Chateau-d?Oex-Vaud, Switzer-
land (2003) 77?81 http://proffs.tk.informatik.tu-
darmstadt.de/TK/abstracts.php3?lang=en&bibtex=1&-
paperID=431.
R. Porzel, I. Gurevych and C. Mu?ller. Ontology-based
contextual coherence scoring. Technical report, Euro-
pean Media Laboratory, Heidelberg, Germany (2003)
http://citeseer.ist.psu.edu/649012.html.
S.R. Halgrim, F. Xia, I. Solti, E. Cadag and O. Uzuner.
Statistical Extraction of Medication Information from
Clinical Records. In: Proc. of AMIA Summit on Trans-
lational Bioinformatics, San Francisco, CA, March
10-12, 2010.
D.A. Lindberg, B.L. Humphreys and A.T. McCray. The
unified medical language system. Methods of In-
formation in Medicine 32(4) (August 1993) 281?291
http://www.nlm.nih.gov/research/umls/.
S. Liu, W. Ma, R. Moore, V. Ganesan and S. Nelson.
Rxnorm: Prescription for electronic drug information
exchange. IT Professional 7(5) (September/October
2005) 17?23
K. Macherey, O. Bender and H. Ney. Multi-level er-
ror handling for tree based dialogue course man-
agement. In: Proceedings of ISCA Tutorial and
Research Workshop on Error Handling in Spo-
ken Dialogue Systems, Chateau-d?Oex-Vaud, Switzer-
land (2003) 123?128, http://www-i6.informatik.rwth-
aachen.de/?bender/papers/isca tutorial 2003.pdf.
M. Oerder and H. Ney. Word graphs: An efficient inter-
face between continuous speech recognition and lan-
guage understanding. In: Proc. IEEE ICASSP?93. Vol-
ume 2. 119?122.
J. Patrick and M. Li. A Cascade Approach to Extracting
Medication Events. In: Proc. Australasian Language
Technology Workshop (ALTA) 2009.
R. Sarikaya, Y. Gao and M. Picheny. Word level confi-
dence measurement using semantic features. In: Proc.
of IEEE ICASSP2003. Volume 1. (April 2003) 604?
607.
S. Seneff and J. Polifroni. Dialogue Management in the
MERCURY Flight Reservation System. In: Satel-
lite Dialogue Workshop, ANLP-NAACL, Seattle (April
2000).
K.D. Voll. A Methodology of Error Detection:
Improving Speech Recognition in Radiology.
PhD thesis, Simon Fraser University (2006)
http://ir.lib.sfu.ca/handle/1892/2734.
K. Wang, Y.Y. Wang and A. Acero. Use and acquisition
of semantic language model. In: HLT-NAACL. (2004)
http://www.aclweb.org/anthology-new/N/N04/N04-
3011.pdf.
R. Zhang and A.I. Rudnicky. Word level confi-
dence annotation using combinations of fea-
tures. In: Proceedings of Eurospeech. (2001)
http://www.speech.cs.cmu.edu/Communicator/papers/-
RecoConf2001.pdf.
28
