Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81?88,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Detecting Erroneous Sentences using Automatically Mined Sequential
Patterns
Guihua Sun ? Xiaohua Liu Gao Cong Ming Zhou
Chongqing University Microsoft Research Asia
sunguihua5018@163.com {xiaoliu, gaocong, mingzhou}@microsoft.com
Zhongyang Xiong John Lee ? Chin-Yew Lin
Chongqing University MIT Microsoft Research Asia
zyxiong@cqu.edu.cn jsylee@mit.edu cyl@microsoft.com
Abstract
This paper studies the problem of identify-
ing erroneous/correct sentences. The prob-
lem has important applications, e.g., pro-
viding feedback for writers of English as
a Second Language, controlling the quality
of parallel bilingual sentences mined from
the Web, and evaluating machine translation
results. In this paper, we propose a new
approach to detecting erroneous sentences
by integrating pattern discovery with super-
vised learning models. Experimental results
show that our techniques are promising.
1 Introduction
Detecting erroneous/correct sentences has the fol-
lowing applications. First, it can provide feedback
for writers of English as a Second Language (ESL)
as to whether a sentence contains errors. Second, it
can be applied to control the quality of parallel bilin-
gual sentences mined from the Web, which are criti-
cal sources for a wide range of applications, such as
statistical machine translation (Brown et al, 1993)
and cross-lingual information retrieval (Nie et al,
1999). Third, it can be used to evaluate machine
translation results. As demonstrated in (Corston-
Oliver et al, 2001; Gamon et al, 2005), the better
human reference translations can be distinguished
from machine translations by a classification model,
the worse the machine translation system is.
?Work done while the author was a visiting student at MSRA
?Work done while the author was a visiting student at MSRA
The previous work on identifying erroneous sen-
tences mainly aims to find errors from the writing of
ESL learners. The common mistakes (Yukio et al,
2001; Gui and Yang, 2003) made by ESL learners
include spelling, lexical collocation, sentence struc-
ture, tense, agreement, verb formation, wrong Part-
Of-Speech (POS), article usage, etc. The previous
work focuses on grammar errors, including tense,
agreement, verb formation, article usage, etc. How-
ever, little work has been done to detect sentence
structure and lexical collocation errors.
Some methods of detecting erroneous sentences
are based on manual rules. These methods (Hei-
dorn, 2000; Michaud et al, 2000; Bender et al,
2004) have been shown to be effective in detect-
ing certain kinds of grammatical errors in the writ-
ing of English learners. However, it could be ex-
pensive to write rules manually. Linguistic experts
are needed to write rules of high quality; Also, it
is difficult to produce and maintain a large num-
ber of non-conflicting rules to cover a wide range of
grammatical errors. Moreover, ESL writers of differ-
ent first-language backgrounds and skill levels may
make different errors, and thus different sets of rules
may be required. Worse still, it is hard to write rules
for some grammatical errors, for example, detecting
errors concerning the articles and singular plural us-
age (Nagata et al, 2006).
Instead of asking experts to write hand-crafted
rules, statistical approaches (Chodorow and Lea-
cock, 2000; Izumi et al, 2003; Brockett et al, 2006;
Nagata et al, 2006) build statistical models to iden-
tify sentences containing errors. However, existing
81
statistical approaches focus on some pre-defined er-
rors and the reported results are not attractive. More-
over, these approaches, e.g., (Izumi et al, 2003;
Brockett et al, 2006) usually need errors to be spec-
ified and tagged in the training sentences, which re-
quires expert help to be recruited and is time con-
suming and labor intensive.
Considering the limitations of the previous work,
in this paper we propose a novel approach that is
based on pattern discovery and supervised learn-
ing to successfully identify erroneous/correct sen-
tences. The basic idea of our approach is to build
a machine learning model to automatically classify
each sentence into one of the two classes, ?erro-
neous? and ?correct.? To build the learning model,
we automatically extract labeled sequential patterns
(LSPs) from both erroneous sentences and correct
sentences, and use them as input features for classi-
fication models. Our main contributions are:
? We mine labeled sequential patterns(LSPs)
from the preprocessed training data to build
leaning models. Note that LSPs are also very
different from N-gram language models that
only consider continuous sequences.
? We also enrich the LSP features with other auto-
matically computed linguistic features, includ-
ing lexical collocation, language model, syn-
tactic score, and function word density. In con-
trast with previous work focusing on (a spe-
cific type of) grammatical errors, our model can
handle a wide range of errors, including gram-
mar, sentence structure, and lexical choice.
? We empirically evaluate our methods on two
datasets consisting of sentences written by
Japanese and Chinese, respectively. Experi-
mental results show that labeled sequential pat-
terns are highly useful for the classification
results, and greatly outperform other features.
Our method outperforms Microsoft Word03
and ALEK (Chodorow and Leacock, 2000)
from Educational Testing Service (ETS) in
some cases. We also apply our learning model
to machine translation (MT) data as a comple-
mentary measure to evaluate MT results.
The rest of this paper is organized as follows.
The next section discusses related work. Section 3
presents the proposed technique. We evaluate our
proposed technique in Section 4. Section 5 con-
cludes this paper and discusses future work.
2 Related Work
Research on detecting erroneous sentences can be
classified into two categories. The first category
makes use of hand-crafted rules, e.g., template
rules (Heidorn, 2000) and mal-rules in context-free
grammars (Michaud et al, 2000; Bender et al,
2004). As discussed in Section 1, manual rule based
methods have some shortcomings.
The second category uses statistical techniques
to detect erroneous sentences. An unsupervised
method (Chodorow and Leacock, 2000) is em-
ployed to detect grammatical errors by inferring
negative evidence from TOEFL administrated by
ETS. The method (Izumi et al, 2003) aims to de-
tect omission-type and replacement-type errors and
transformation-based leaning is employed in (Shi
and Zhou, 2005) to learn rules to detect errors for
speech recognition outputs. They also require spec-
ifying error tags that can tell the specific errors
and their corrections in the training corpus. The
phrasal Statistical Machine Translation (SMT) tech-
nique is employed to identify and correct writing er-
rors (Brockett et al, 2006). This method must col-
lect a large number of parallel corpora (pairs of er-
roneous sentences and their corrections) and perfor-
mance depends on SMT techniques that are not yet
mature. The work in (Nagata et al, 2006) focuses
on a type of error, namely mass vs. count nouns.
In contrast to existing statistical methods, our tech-
nique needs neither errors tagged nor parallel cor-
pora, and is not limited to a specific type of gram-
matical error.
There are also studies on automatic essay scoring
at document-level. For example, E-rater (Burstein
et al, 1998), developed by the ETS, and Intelligent
Essay Assessor (Foltz et al, 1999). The evaluation
criteria for documents are different from those for
sentences. A document is evaluated mainly by its or-
ganization, topic, diversity of vocabulary, and gram-
mar while a sentence is done by grammar, sentence
structure, and lexical choice.
Another related work is Machine Translation (MT)
evaluation. Classification models are employed
in (Corston-Oliver et al, 2001; Gamon et al, 2005)
82
to evaluate the well-formedness of machine transla-
tion outputs. The writers of ESL and MT normally
make different mistakes: in general, ESL writers can
write overall grammatically correct sentences with
some local mistakes while MT outputs normally pro-
duce locally well-formed phrases with overall gram-
matically wrong sentences. Hence, the manual fea-
tures designed for MT evaluation are not applicable
to detect erroneous sentences from ESL learners.
LSPs differ from the traditional sequential pat-
terns, e.g., (Agrawal and Srikant, 1995; Pei et al,
2001) in that LSPs are attached with class labels and
we prefer those with discriminating ability to build
classification model. In our other work (Sun et al,
2007), labeled sequential patterns, together with la-
beled tree patterns, are used to build pattern-based
classifier to detect erroneous sentences. The clas-
sification method in (Sun et al, 2007) is different
from those used in this paper. Moreover, instead of
labeled sequential patterns, in (Sun et al, 2007) the
most significant k labeled sequential patterns with
constraints for each training sentence are mined to
build classifiers. Another related work is (Jindal and
Liu, 2006), where sequential patterns with labels are
used to identify comparative sentences.
3 Proposed Technique
This section first gives our problem statement and
then presents our proposed technique to build learn-
ing models.
3.1 Problem Statement
In this paper we study the problem of identifying
erroneous/correct sentences. A set of training data
containing correct and erroneous sentences is given.
Unlike some previous work, our technique requires
neither that the erroneous sentences are tagged with
detailed errors, nor that the training data consist of
parallel pairs of sentences (an error sentence and its
correction). The erroneous sentence contains a wide
range of errors on grammar, sentence structure, and
lexical choice. We do not consider spelling errors in
this paper.
We address the problem by building classifica-
tion models. The main challenge is to automatically
extract representative features for both correct and
erroneous sentences to build effective classification
models. We illustrate the challenge with an exam-
ple. Consider an erroneous sentence, ?If Maggie will
go to supermarket, she will buy a bag for you.? It is
difficult for previous methods using statistical tech-
niques to capture such an error. For example, N-
gram language model is considered to be effective
in writing evaluation (Burstein et al, 1998; Corston-
Oliver et al, 2001). However, it becomes very ex-
pensive if N > 3 and N-grams only consider contin-
uous sequence of words, which is unable to detect
the above error ?if...will...will?.
We propose labeled sequential patterns to effec-
tively characterize the features of correct and er-
roneous sentences (Section 3.2), and design some
complementary features ( Section 3.3).
3.2 Mining Labeled Sequential Patterns ( LSP )
Labeled Sequential Patterns (LSP). A labeled se-
quential pattern, p, is in the form of LHS? c, where
LHS is a sequence and c is a class label. Let I be a
set of items and L be a set of class labels. Let D be a
sequence database in which each tuple is composed
of a list of items in I and a class label in L. We say
that a sequence s1 =< a1, ..., am > is contained in
a sequence s2 =< b1, ..., bn > if there exist integers
i1, ...im such that 1 ? i1 < i2 < ... < im ? n and
aj = bij for all j ? 1, ...,m. Similarly, we say that
a LSP p1 is contained by p2 if the sequence p1.LHS
is contained by p2.LHS and p1.c = p2.c. Note that
it is not required that s1 appears continuously in s2.
We will further refine the definition of ?contain? by
imposing some constraints (to be explained soon).
A LSP p is attached with two measures, support and
confidence. The support of p, denoted by sup(p),
is the percentage of tuples in database D that con-
tain the LSP p. The probability of the LSP p being
true is referred to as ?the confidence of p ?, denoted
by conf(p), and is computed as sup(p)sup(p.LHS) . The
support is to measure the generality of the pattern p
and minimum confidence is a statement of predictive
ability of p.
Example 1: Consider a sequence database contain-
ing three tuples t1 = (< a, d, e, f >,E), t2 = (<
a, f, e, f >,E) and t3 = (< d, a, f >,C). One
example LSP p1 = < a, e, f >? E, which is con-
tained in tuples t1 and t2. Its support is 66.7% and
its confidence is 100%. As another example, LSP p2
83
= < a, f >? E with support 66.7% and confidence
66.7%. p1 is a better indication of class E than p2.
2
Generating Sequence Database. We generate the
database by applying Part-Of-Speech (POS) tagger
to tag each training sentence while keeping func-
tion words1 and time words2. After the process-
ing, each sentence together with its label becomes
a database tuple. The function words and POS tags
play important roles in both grammars and sentence
structures. In addition, the time words are key
clues in detecting errors of tense usage. The com-
bination of them allows us to capture representative
features for correct/erroneous sentences by mining
LSPs. Some example LSPs include ?<a, NNS> ?
Error?(singular determiner preceding plural noun),
and ?<yesterday, is>?Error?. Note that the con-
fidences of these LSPs are not necessary 100%.
First, we use MXPOST-Maximum Entropy Part of
Speech Tagger Toolkit3 for POS tags. The MXPOST
tagger can provide fine-grained tag information. For
example, noun can be tagged with ?NN?(singular
noun) and ?NNS?(plural noun); verb can be tagged
with ?VB?, ?VBG?, ?VBN?, ?VBP?, ?VBD? and
?VBZ?. Second, the function words and time words
that we use form a key word list. If a word in a
training sentence is not contained in the key word
list, then the word will be replaced by its POS. The
processed sentence consists of POS and the words of
key word list. For example, after the processing, the
sentence ?In the past, John was kind to his sister? is
converted into ?In the past, NNP was JJ to his NN?,
where the words ?in?, ?the?, ?was?, ?to? and ?his?
are function words, the word ?past? is time word,
and ?NNP?, ?JJ?, and ?NN? are POS tags.
Mining LSPs. The length of the discovered LSPs
is flexible and they can be composed of contiguous
or distant words/tags. Existing frequent sequential
pattern mining algorithms (e.g. (Pei et al, 2001))
use minimum support threshold to mine frequent se-
quential patterns whose support is larger than the
threshold. These algorithms are not sufficient for our
problem of mining LSPs. In order to ensure that all
our discovered LSPs are discriminating and are capa-
1http://www.marlodge.supanet.com/museum/funcword.html
2http://www.wjh.harvard.edu/%7Einquirer/Time%40.html
3http://www.cogsci.ed.ac.uk/?jamesc/taggers/MXPOST.html
ble of predicting correct or erroneous sentences, we
impose another constraint minimum confidence. Re-
call that the higher the confidence of a pattern is, the
better it can distinguish between correct sentences
and erroneous sentences. In our experiments, we
empirically set minimum support at 0.1% and mini-
mum confidence at 75%.
Mining LSPs is nontrivial since its search space
is exponential, althought there have been a host of
algorithms for mining frequent sequential patterns.
We adapt the frequent sequence mining algorithm
in (Pei et al, 2001) for mining LSPs with constraints.
Converting LSPs to Features. Each discovered LSP
forms a binary feature as the input for classification
model. If a sentence includes a LSP, the correspond-
ing feature is set at 1.
The LSPs can characterize the correct/erroneous
sentence structure and grammar. We give some ex-
amples of the discovered LSPs. (1) LSPs for erro-
neous sentences. For example, ?<this, NNS>?(e.g.
contained in ?this books is stolen.?), ?<past,
is>?(e.g. contained in ?in the past, John is kind to
his sister.?), ?<one, of, NN>?(e.g. contained in ?it is
one of important working language?, ?<although,
but>?(e.g. contained in ?although he likes it, but
he can?t buy it.?), and ?<only, if, I, am>?(e.g. con-
tained in ?only if my teacher has given permission,
I am allowed to enter this room?). (2) LSPs for cor-
rect sentences. For instance, ?<would, VB>?(e.g.
contained in ?he would buy it.?), and ?<VBD,
yeserday>?(e.g. contained in ?I bought this book
yesterday.?).
3.3 Other Linguistic Features
We use some linguistic features that can be com-
puted automatically as complementary features.
Lexical Collocation (LC) Lexical collocation er-
ror (Yukio et al, 2001; Gui and Yang, 2003) is com-
mon in the writing of ESL learners, such as ?strong
tea? but not ?powerful tea.? Our LSP features can-
not capture all LCs since we replace some words
with POS tags in mining LSPs. We collect five types
of collocations: verb-object, adjective-noun, verb-
adverb, subject-verb, and preposition-object from a
general English corpus4. Correct LCs are collected
4The general English corpus consists of about 4.4 million
native sentences.
84
by extracting collocations of high frequency from
the general English corpus. Erroneous LC candi-
dates are generated by replacing the word in correct
collocations with its confusion words, obtained from
WordNet, including synonyms and words with sim-
ilar spelling or pronunciation. Experts are consulted
to see if a candidate is a true erroneous collocation.
We compute three statistical features for each sen-
tence below. (1) The first feature is computed by
m?
i=1
p(coi)/n, where m is the number of CLs, n is
the number of collocations in each sentence, and
probability p(coi) of each CL coi is calculated us-
ing the method (Lu? and Zhou, 2004). (2) The sec-
ond feature is computed by the ratio of the number
of unknown collocations (neither correct LCs nor er-
roneous LCs) to the number of collocations in each
sentence. (3) The last feature is computed by the ra-
tio of the number of erroneous LCs to the number of
collocations in each sentence.
Perplexity from Language Model (PLM) Perplex-
ity measures are extracted from a trigram language
model trained on a general English corpus using
the SRILM-SRI Language Modeling Toolkit (Stolcke,
2002). We calculate two values for each sentence:
lexicalized trigram perplexity and part of speech
(POS) trigram perplexity. The erroneous sentences
would have higher perplexity.
Syntactic Score (SC) Some erroneous sentences of-
ten contain words and concepts that are locally cor-
rect but cannot form coherent sentences (Liu and
Gildea, 2005). To measure the coherence of sen-
tences, we use a statistical parser Toolkit (Collins,
1997) to assign each sentence a parser?s score that
is the related log probability of parsing. We assume
that erroneous sentences with undesirable sentence
structures are more likely to receive lower scores.
Function Word Density (FWD) We consider the
density of function words (Corston-Oliver et al,
2001), i.e. the ratio of function words to content
words. This is inspired by the work (Corston-Oliver
et al, 2001) showing that function word density can
be effective in distinguishing between human refer-
ences and machine outputs. In this paper, we calcu-
late the densities of seven kinds of function words 5
5including determiners/quantifiers, all pronouns, different
pronoun types: Wh, 1st, 2nd, and 3rd person pronouns, prepo-
Dataset Type Source Number
JC
(+) the Japan Times newspaperand Model English Essay 16,857
(-)
HEL (Hiroshima English
Learners? Corpus) and JLE
(Japanese Learners of En-
glish Corpus)
17,301
CC (+) the 21st Century newspaper 3,200
(-)
CLEC (Chinese Learner Er-
ror Corpus) 3,199
Table 1: Corpora ((+): correct; (-): erroneous)
respectively as 7 features.
4 Experimental Evaluation
We evaluated the performance of our techniques
with support vector machine (SVM) and Naive
Bayesian (NB) classification models. We also com-
pared the effectiveness of various features. In ad-
dition, we compared our technique with two other
methods of checking errors, Microsoft Word03 and
ALEK method (Chodorow and Leacock, 2000). Fi-
nally, we also applied our technique to evaluate the
Machine Translation outputs.
4.1 Experimental Setup
Classification Models. We used two classification
models, SVM6 and NB classification model.
Data. We collected two datasets from different do-
mains, Japanese Corpus (JC) and Chinese Corpus
(CC). Table 1 gives the details of our corpora. In
the learner?s corpora, all of the sentences are erro-
neous. Note that our data does not consist of parallel
pairs of sentences (one error sentence and its correc-
tion). The erroneous sentences includes grammar,
sentence structure and lexical choice errors, but not
spelling errors.
For each sentence, we generated five kinds of fea-
tures as presented in Section 3. For a non-binary
feature X , its value x is normalized by z-score,
norm(x) = x?mean(X)?var(X) , where mean(x) is the em-
pirical mean of X and var(X) is the variance of X .
Thus each sentence is represented by a vector.
Metrics We calculated the precision, recall,
and F-score for correct and erroneous sentences,
respectively, and also report the overall accuracy.
sitions and adverbs, auxiliary verbs, and conjunctions.
6http://svmlight.joachims.org/
85
All the experimental results are obtained thorough
10-fold cross-validation.
4.2 Experimental Results
The Effectiveness of Various Features. The exper-
iment is to evaluate the contribution of each feature
to the classification. The results of SVM are given in
Table 2. We can see that the performance of labeled
sequential patterns (LSP) feature consistently out-
performs those of all the other individual features. It
also performs better even if we use all the other fea-
tures together. This is because other features only
provide some relatively abstract and simple linguis-
tic information, whereas the discovered LSP s char-
acterize significant linguistic features as discussed
before. We also found that the results of NB are a
little worse than those of SVM. However, all the fea-
tures perform consistently on the two classification
models and we can observe the same trend. Due to
space limitation, we do not give results of NB.
In addition, the discovered LSPs themselves are
intuitive and meaningful since they are intuitive fea-
tures that can distinguish correct sentences from er-
roneous sentences. We discovered 6309 LSPs in
JC data and 3742 LSPs in CC data. Some exam-
ple LSPs discovered from erroneous sentences are
<a, NNS> (support:0.39%, confidence:85.71%),
<to, VBD> (support:0.11%, confidence:84.21%),
and <the, more, the, JJ> (support:0.19%, confi-
dence:0.93%) 7; Similarly, we also give some exam-
ple LSPs mined from correct sentences: <NN, VBZ>
(support:2.29%, confidence:75.23%), and <have,
VBN, since> (support:0.11%, confidence:85.71%)
8. However, other features are abstract and it is hard
to derive some intuitive knowledge from the opaque
statistical values of these features.
As shown in Table 2, our technique achieves
the highest accuracy, e.g. 81.75% on the Japanese
dataset, when we use all the features. However, we
also notice that the improvement is not very signif-
icant compared with using LSP feature individually
(e.g. 79.63% on the Japanese dataset). The similar
results are observed when we combined the features
PLM, SC, FWD, and LC. This could be explained
7a + plural noun; to + past tense format; the more + the +
base form of adjective
8singular or mass noun + the 3rd person singular present
format; have + past participle format + since
by two reasons: (1) A sentence may contain sev-
eral kinds of errors. A sentence detected to be er-
roneous by one feature may also be detected by an-
other feature; and (2) Various features give conflict-
ing results. The two aspects suggest the directions
of our future efforts to improve the performance of
our models.
Comparing with Other Methods. It is difficult
to find benchmark methods to compare with our
technique because, as discussed in Section 2, exist-
ing methods often require error tagged corpora or
parallel corpora, or focus on a specific type of er-
rors. In this paper, we compare our technique with
the grammar checker of Microsoft Word03 and the
ALEK (Chodorow and Leacock, 2000) method used
by ETS. ALEK is used to detect inappropriate usage
of specific vocabulary words. Note that we do not
consider spelling errors. Due to space limitation, we
only report the precision, recall, F-score
for erroneous sentences, and the overall accuracy.
As can be seen from Table 3, our method out-
performs the other two methods in terms of over-
all accuracy, F-score, and recall, while the three
methods achieve comparable precision. We realize
that the grammar checker of Word is a general tool
and the performance of ALEK (Chodorow and Lea-
cock, 2000) can be improved if larger training data is
used. We found that Word and ALEK usually cannot
find sentence structure and lexical collocation errors,
e.g., ?The more you listen to English, the easy it be-
comes.? contains the discovered LSP <the, more, the,
JJ>? Error.
Cross-domain Results. To study the performance
of our method on cross-domain data from writers
of the same first-language background, we collected
two datasets from Japanese writers, one is composed
of 694 parallel sentences (+:347, -:347), and the
other 1,671 non-parallel sentences (+:795, -:876).
The two datasets are used as test data while we use
JC dataset for training. Note that the test sentences
come from different domains from the JC data. The
results are given in the first two rows of Table 4. This
experiment shows that our leaning model trained for
one domain can be effectively applied to indepen-
dent data in the other domains from the writes of the
same first-language background, no matter whether
the test data is parallel or not. We also noticed that
86
Dataset Feature A (-)F (-)R (-)P (+)F (+)R (+)P
JC
LSP 79.63 80.65 85.56 76.29 78.49 73.79 83.85
LC 69.55 71.72 77.87 66.47 67.02 61.36 73.82
PLM 61.60 55.46 50.81 64.91 62 70.28 58.43
SC 53.66 57.29 68.40 56.12 34.18 39.04 32.22
FWD 68.01 72.82 86.37 62.95 61.14 49.94 78.82
LC + PLM + SC + FWD 71.64 73.52 79.38 68.46 69.48 64.03 75.94
LSP + LC + PLM + SC + FWD 81.75 81.60 81.46 81.74 81.90 82.04 81.76
CC
LSP 78.19 76.40 70.64 83.20 79.71 85.72 74.50
LC 63.82 62.36 60.12 64.77 65.17 67.49 63.01
PLM 55.46 64.41 80.72 53.61 40.41 30.22 61.30
SC 50.52 62.58 87.31 50.64 13.75 14.33 13.22
FWD 61.36 60.80 60.70 60.90 61.90 61.99 61.80
LC + PLM + SC + FWD 67.69 67.62 67.51 67.77 67.74 67.87 67.64
LSP + LC + PLM + SC + FWD 79.81 78.33 72.76 84.84 81.10 86.92 76.02
Table 2: The Experimental Results (A: overall accuracy; (-): erroneous sentences; (+): correct sentences; F:
F-score; R: recall; P: precision)
Dataset Model A (-)F (-)R (-)P
JC
Ours 81.39 81.25 81.24 81.28
Word 58.87 33.67 21.03 84.73
ALEK 54.69 20.33 11.67 78.95
CC
Ours 79.14 77.81 73.17 83.09
Word 58.47 32.02 19.81 84.22
ALEK 55.21 22.83 13.42 76.36
Table 3: The Comparison Results
LSPs play dominating role in achieving the results.
Due to space limitation, no details are reported.
To further see the performance of our method
on data written by writers with different first-
language backgrounds, we conducted two experi-
ments. (1) We merge the JC dataset and CC dataset.
The 10-fold cross-validation results on the merged
dataset are given in the third row of Table 4. The
results demonstrate that our models work well when
the training data and test data contain sentences from
different first-language backgrounds. (2) We use the
JC dataset (resp. CC dataset) for training while the
CC dataset (resp. JC dataset) is used as test data. As
shown in the fourth (resp. fifth) row of Table 4, the
results are worse than their corresponding results of
Word given in Table 3. The reason is that the mis-
takes made by Japanese and Chinese are different,
thus the learning model trained on one data does not
work well on the other data. Note that our method is
not designed to work in this scenario.
Application to Machine Translation Evaluation.
Our learning models could be used to evaluate the
MT results as an complementary measure. This is
based on the assumption that if the MT results can
be accurately distinguished from human references
Dataset A (-)F (-)R (-)P
JC(Train)+nonparallel(Test) 72.49 68.55 57.51 84.84
JC(Train)+parallel(Test) 71.33 69.53 65.42 74.18
JC + CC 79.98 79.72 79.24 80.23
JC(Train)+ CC(Test) 55.62 41.71 31.32 62.40
CC(Train)+ JC(Test) 57.57 23.64 16.94 39.11
Table 4: The Cross-domain Results of our Method
by our technique, the MT results are not natural and
may contain errors as well.
The experiment was conducted using 10-fold
cross validation on two LDC data, low-ranked and
high-ranked data9. The results using SVM as classi-
fication model are given in Table 5. As expected, the
classification accuracy on low-ranked data is higher
than that on high-ranked data since low-ranked MT
results are more different from human references
than high-ranked MT results. We also found that
LSPs are the most effective features. In addition, our
discovered LSPs could indicate the common errors
made by the MT systems and provide some sugges-
tions for improving machine translation results.
As a summary, the mined LSPs are indeed effec-
tive for the classification models and our proposed
technique is effective.
5 Conclusions and Future Work
This paper proposed a new approach to identifying
erroneous/correct sentences. Empirical evaluating
using diverse data demonstrated the effectiveness of
9One LDC data contains 14,604 low ranked (score 1-3) ma-
chine translations and the corresponding human references; the
other LDC data contains 808 high ranked (score 3-5) machine
translations and the corresponding human references
87
Data Feature A (-)F (-)R (-)P (+)F (+)R (+)P
Low-ranked data (1-3 score) LSP 84.20 83.95 82.19 85.82 84.44 86.25 82.73
LSP+LC+PLM+SC+FWD 86.60 86.84 88.96 84.83 86.35 84.27 88.56
High-ranked data (3-5 score) LSP 71.74 73.01 79.56 67.59 70.23 64.47 77.40
LSP+LC+PLM+SC+FWD 72.87 73.68 68.95 69.20 71.92 67.22 77.60
Table 5: The Results on Machine Translation Data
our techniques. Moreover, we proposed to mine
LSPs as the input of classification models from a set
of data containing correct and erroneous sentences.
The LSPs were shown to be much more effective than
the other linguistic features although the other fea-
tures were also beneficial.
We will investigate the following problems in the
future: (1) to make use of the discovered LSPs to pro-
vide detailed feedback for ESL learners, e.g. the er-
rors in a sentence and suggested corrections; (2) to
integrate the features effectively to achieve better re-
sults; (3) to further investigate the application of our
techniques for MT evaluation.
References
Rakesh Agrawal and Ramakrishnan Srikant. 1995. Mining se-
quential patterns. In ICDE.
Emily M. Bender, Dan Flickinger, Stephan Oepen, Annemarie
Walsh, and Timothy Baldwin. 2004. Arboretum: Using a
precision grammar for grammmar checking in call. In Proc.
InSTIL/ICALL Symposium on Computer Assisted Learning.
Chris Brockett, William Dolan, and Michael Gamon. 2006.
Correcting esl errors using phrasal smt techniques. In ACL.
Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, Martin
Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998.
Automated scoring using a hybrid feature identification tech-
nique. In Proc. ACL.
Martin Chodorow and Claudia Leacock. 2000. An unsuper-
vised method for detecting grammatical errors. In NAACL.
Michael Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. ACL.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proc. ACL.
P.W. Foltz, D. Laham, and T.K. Landauer. 1999. Automated
essay scoring: Application to educational technology. In Ed-
Media ?99.
Michael Gamon, Anthony Aue, and Martine Smets. 2005.
Sentence-level mt evaluation without reference translations:
Beyond language modeling. In Proc. EAMT.
Shicun Gui and Huizhong Yang. 2003. Zhongguo Xuexizhe
Yingyu Yuliaohu. (Chinese Learner English Corpus). Shang-
hai: Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).
George E. Heidorn. 2000. Intelligent Writing Assistance.
Handbook of Natural Language Processing. Robert Dale,
Hermann Moisi and Harold Somers (ed.). Marcel Dekker.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Sup-
nithi, and Hitoshi Isahara. 2003. Automatic error detection
in the japanese learners? english spoken data. In Proc. ACL.
Nitin Jindal and Bing Liu. 2006. Identifying comparative sen-
tences in text documents. In SIGIR.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In Proc. ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization.
Yajuan Lu? and Ming Zhou. 2004. Collocation translation ac-
quisition using monolingual corpora. In Proc. ACL.
Lisa N. Michaud, Kathleen F. McCoy, and Christopher A. Pen-
nington. 2000. An intelligent tutoring system for deaf learn-
ers of written english. In Proc. 4th International ACM Con-
ference on Assistive Technologies.
Ryo Nagata, Atsuo Kawai, Koichiro Morihiro, and Naoki Isu.
2006. A feedback-augmented method for detecting errors in
the writing of learners of english. In Proc. ACL.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Richard Du-
rand. 1999. Cross-language information retrieval based on
parallel texts and automatic mining of parallel texts from the
web. In SIGIR, pages 74?81.
Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, and Helen Pinto.
2001. Prefixspan: Mining sequential patterns efficiently by
prefix-projected pattern growth. In Proc. ICDE.
Yongmei Shi and Lina Zhou. 2005. Error detection using lin-
guistic features. In HLT/EMNLP.
Andreas Stolcke. 2002. Srilm-an extensible language modeling
toolkit. In Proc. ICSLP.
Guihua Sun, Gao Cong, Xiaohua Liu, Chin-Yew Lin, and Ming
Zhou. 2007. Mining sequential patterns and tree patterns to
detect erroneous sentences. In AAAI.
Tono Yukio, T. Kaneko, H. Isahara, T. Saiga, and E. Izumi.
2001. The standard speaking test corpus: A 1 million-word
spoken corpus of japanese learners of english and its impli-
cations for l2 lexicography. In ASIALEX: Asian Bilingualism
and the Dictionary.
88
Proceedings of ACL-08: HLT, pages 710?718,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Conditional Random Fields to Extract Contexts and Answers of
Questions from Online Forums
Shilin Ding ? ? Gao Cong? ? Chin-Yew Lin? Xiaoyan Zhu?
?Department of Computer Science and Technology, Tsinghua University, Beijing, China
?Department of Computer Science, Aalborg University, Denmark
?Microsoft Research Asia, Beijing, China
dingsl@gmail.com gaocong@cs.aau.dk
cyl@microsoft.com zxy-dcs@tsinghua.edu.cn
Abstract
Online forum discussions often contain vast
amounts of questions that are the focuses of
discussions. Extracting contexts and answers
together with the questions will yield not only
a coherent forum summary but also a valu-
able QA knowledge base. In this paper, we
propose a general framework based on Con-
ditional Random Fields (CRFs) to detect the
contexts and answers of questions from forum
threads. We improve the basic framework by
Skip-chain CRFs and 2D CRFs to better ac-
commodate the features of forums for better
performance. Experimental results show that
our techniques are very promising.
1 Introduction
Forums are web virtual spaces where people can ask
questions, answer questions and participate in dis-
cussions. The availability of vast amounts of thread
discussions in forums has promoted increasing in-
terests in knowledge acquisition and summarization
for forum threads. Forum thread usually consists
of an initiating post and a number of reply posts.
The initiating post usually contains several ques-
tions and the reply posts usually contain answers to
the questions and perhaps new questions. Forum
participants are not physically co-present, and thus
reply may not happen immediately after questions
are posted. The asynchronous nature and multi-
participants make multiple questions and answers
?This work was done when Shilin Ding was a visiting stu-
dent at the Microsoft Research Asia
?This work was done when Gao Cong worked as a re-
searcher at the Microsoft Research Asia.
<context id=1>S1: Hi I am looking for a pet friendly
hotel in Hong Kong because all of my family is go-
ing there for vacation. S2: my family has 2 sons
and a dog.</context> <question id=1>S3: Is there
any recommended hotel near Sheung Wan or Tsing
Sha Tsui?</question><context id=2,3>S4: We also
plan to go shopping in Causeway Bay.</context>
<question id=2>S5: What?s the traffic situa-
tion around those commercial areas?</question>
<question id=3>S6: Is it necessary to take a
taxi?</question>. S7: Any information would be ap-
preciated.
<answer qid=1>S8: The Comfort Lodge near
Kowloon Park allows pet as I know, and usually fits
well within normal budget. S9: It is also conve-
niently located, nearby the Kowloon railway station
and subway.</answer>
<answer qid=2,3> S10: It?s very crowd in those ar-
eas, so I recommend MTR in Causeway Bay because
it is cheap to take you around </answer>
Figure 1: An example thread with question-context-
answer annotated
interweaved together, which makes it more difficult
to summarize.
In this paper, we address the problem of detecting
the contexts and answers from forum threads for the
questions identified in the same threads. Figure 1
gives an example of a forum thread with questions,
contexts and answers annotated. It contains three
question sentences, S3, S5 and S6. Sentences S1
and S2 are contexts of question 1 (S3). Sentence S4
is the context of questions 2 and 3, but not 1. Sen-
tence S8 is the answer to question 3. (S4-S5-S10) is
one example of question-context-answer triple that
we want to detect in the thread. As shown in the ex-
ample, a forum question usually requires contextual
information to provide background or constraints.
710
Moreover, it sometimes needs contextual informa-
tion to provide explicit link to its answers. For
example, S8 is an answer of question 1, but they
cannot be linked with any common word. Instead,
S8 shares word pet with S1, which is a context of
question 1, and thus S8 could be linked with ques-
tion 1 through S1. We call contextual information
the context of a question in this paper.
A summary of forum threads in the form of
question-context-answer can not only highlight the
main content, but also provide a user-friendly orga-
nization of threads, which will make the access to
forum information easier.
Another motivation of detecting contexts and an-
swers of the questions in forum threads is that it
could be used to enrich the knowledge base of
community-based question and answering (CQA)
services such as Live QnA and Yahoo! Answers,
where context is comparable with the question de-
scription while question corresponds to the question
title. For example, there were about 700,000 ques-
tions in the Yahoo! Answers travel category as of
January 2008. We extracted about 3,000,000 travel
related questions from six online travel forums. One
would expect that a CQA service with large QA data
will attract more users to the service. To enrich the
knowledge base, not only the answers, but also the
contexts are critical; otherwise the answer to a ques-
tion such as How much is the taxi would be useless
without context in the database.
However, it is challenging to detecting contexts
and answers for questions in forum threads. We as-
sume the questions have been identified in a forum
thread using the approach in (Cong et al, 2008).
Although identifying questions in a forum thread is
also nontrivial, it is beyond the focus of this paper.
First, detecting contexts of a question is important
and non-trivial. We found that 74% of questions in
our corpus, which contain 1,064 questions from 579
forum threads about travel, need contexts. However,
relative position information is far from adequate to
solve the problem. For example, in our corpus 63%
of sentences preceding questions are contexts and
they only represent 34% of all correct contexts. To
effectively detect contexts, the dependency between
sentences is important. For example in Figure 1,
both S1 and S2 are contexts of question 1. S1 could
be labeled as context based on word similarity, but it
is not easy to link S2 with the question directly. S1
and S2 are linked by the common word family, and
thus S2 can be linked with question 1 through S1.
The challenge here is how to model and utilize the
dependency for context detection.
Second, it is difficult to link answers with ques-
tions. In forums, multiple questions and answers
can be discussed in parallel and are interweaved to-
gether while the reply relationship between posts is
usually unavailable. To detect answers, we need to
handle two kinds of dependencies. One is the depen-
dency relationship between contexts and answers,
which should be leveraged especially when ques-
tions alone do not provide sufficient information to
find answers; the other is the dependency between
answer candidates (similar to sentence dependency
described above). The challenge is how to model
and utilize these two kinds of dependencies.
In this paper we propose a novel approach for de-
tecting contexts and answers of the questions in fo-
rum threads. To our knowledge this is the first work
on this.We make the following contributions:
First, we employ Linear Conditional Random
Fields (CRFs) to identify contexts and answers,
which can capture the relationships between con-
tiguous sentences.
Second, we also found that context is very im-
portant for answer detection. To capture the depen-
dency between contexts and answers, we introduce
Skip-chain CRF model for answer detection. We
also extend the basic model to 2D CRFs to model
dependency between contiguous questions in a fo-
rum thread for context and answer identification.
Finally, we conducted experiments on forum data.
Experimental results show that 1) Linear CRFs out-
perform SVM and decision tree in both context
and answer detection; 2) Skip-chain CRFs outper-
form Linear CRFs for answer finding, which demon-
strates that context improves answer finding; 3)
2D CRF model improves the performance of Linear
CRFs and the combination of 2D CRFs and Skip-
chain CRFs achieves better performance for context
detection.
The rest of this paper is organized as follows:
The next section discusses related work. Section 3
presents the proposed techniques. We evaluate our
techniques in Section 4. Section 5 concludes this
paper and discusses future work.
711
2 Related Work
There is some research on summarizing discussion
threads and emails. Zhou and Hovy (2005) seg-
mented internet relay chat, clustered segments into
subtopics, and identified responding segments of
the first segment in each sub-topic by assuming
the first segment to be focus. In (Nenkova and
Bagga, 2003; Wan and McKeown, 2004; Rambow
et al, 2004), email summaries were organized by
extracting overview sentences as discussion issues.
Carenini et al(2007) leveraged both quotation re-
lation and clue words for email summarization. In
contrast, given a forum thread, we extract questions,
their contexts, and their answers as summaries.
Shrestha and McKeown (2004)?s work on email
summarization is closer to our work. They used
RIPPER as a classifier to detect interrogative ques-
tions and their answers and used the resulting ques-
tion and answer pairs as summaries. However, it did
not consider contexts of questions and dependency
between answer sentences.
We also note the existing work on extracting
knowledge from discussion threads. Huang et
al.(2007) used SVM to extract input-reply pairs from
forums for chatbot knowledge. Feng et al (2006a)
used cosine similarity to match students? query with
reply posts for discussion-bot. Feng et al (2006b)
identified the most important message in online
classroom discussion board. Our problem is quite
different from the above work.
Detecting context for question in forums is related
to the context detection problem raised in the QA
roadmap paper commissioned by ARDA (Burger et
al., 2006). To our knowledge, none of the previous
work addresses the problem of context detection.
The method of finding follow-up questions (Yang
et al, 2006) from TREC context track could be
adapted for context detection. However, the follow-
up relationship is limited between questions while
context is not. In our other work (Cong et al, 2008),
we proposed a supervised approach for question de-
tection and an unsupervised approach for answer de-
tection without considering context detection.
Extensive research has been done in question-
answering, e.g. (Berger et al, 2000; Jeon et al,
2005; Cui et al, 2005; Harabagiu and Hickl, 2006;
Dang et al, 2007). They mainly focus on con-
structing answer for certain types of question from a
large document collection, and usually apply sophis-
ticated linguistic analysis to both questions and the
documents in the collection. Soricut and Brill (2006)
used statistical translation model to find the appro-
priate answers from their QA pair collections from
FAQ pages for the posted question. In our scenario,
we not only need to find answers for various types
of questions in forum threads but also their contexts.
3 Context and Answer Detection
A question is a linguistic expression used by a ques-
tioner to request information in the form of an an-
swer. The sentence containing request focus is
called question. Context are the sentences contain-
ing constraints or background information to the
question, while answer are that provide solutions. In
this paper, we use sentences as the detection segment
though it is applicable to other kinds of segments.
Given a thread and a set of m detected questions
{Qi}mi=1, our task is to find the contexts and an-
swers for each question. We first discuss using Lin-
ear CRFs for context and answer detection, and then
extend the basic framework to Skip-chain CRFs and
2D CRFs to better model our problem. Finally, we
will briefly introduce CRF models and the features
that we used for CRF model.
3.1 Using Linear CRFs
For ease of presentation, we focus on detecting con-
texts using Linear CRFs. The model could be easily
extended to answer detection.
Context detection. As discussed in Introduction
that context detection cannot be trivially solved by
position information (See Section 4.2 for details),
and dependency between sentences is important for
context detection. Recall that in Figure 1, S2 could
be labeled as context of Q1 if we consider the de-
pendency between S2 and S1, and that between S1
and Q1, while it is difficult to establish connection
between S2 and Q1 without S1. Table 1 shows that
the correlation between the labels of contiguous sen-
tences is significant. In other words, when a sen-
tence Yt?s previous Yt?1 is not a context (Yt?1 6= C)
then it is very likely that Yt (i.e. Yt 6= C) is also not a
context. It is clear that the candidate contexts are not
independent and there are strong dependency rela-
712
Contiguous sentences yt = C yt 6= C
yt?1 = C 901 1,081
yt?1 6= C 1,081 47,190
Table 1: Contingency table(?2 = 9,386,p-value<0.001)
tionships between contiguous sentences in a thread.
Therefore, a desirable model should be able to cap-
ture the dependency.
The context detection can be modeled as a clas-
sification problem. Traditional classification tools,
e.g. SVM, can be employed, where each pair of
question and candidate context will be treated as an
instance. However, they cannot capture the depen-
dency relationship between sentences.
To this end, we proposed a general framework to
detect contexts and answers based on Conditional
Random Fields (Lafferty et al, 2001) (CRFs) which
are able to model the sequential dependencies be-
tween contiguous nodes. A CRF is an undirected
graphical model G of the conditional distribution
P (Y|X). Y are the random variables over the la-
bels of the nodes that are globally conditioned on X,
which are the random variables of the observations.
(See Section 3.4 for more about CRFs)
Linear CRF model has been successfully applied
in NLP and text mining tasks (McCallum and Li,
2003; Sha and Pereira, 2003). However, our prob-
lem cannot be modeled with Linear CRFs in the
same way as other NLP tasks, where one node has a
unique label. In our problem, each node (sentence)
might have multiple labels since one sentence could
be the context of multiple questions in a thread.
Thus, it is difficult to find a solution to tag context
sentences for all questions in a thread in single pass.
Here we assume that questions in a given thread
are independent and are found, and then we can
label a thread with m questions one-by-one in m-
passes. In each pass, one question Qi is selected
as focus and each other sentence in the thread will
be labeled as context C of Qi or not using Linear
CRF model. The graphical representations of Lin-
ear CRFs is shown in Figure2(a). The linear-chain
edges can capture the dependency between two con-
tiguous nodes. The observation sequence x = <x1,
x2,...,xt>, where t is the number of sentences in a
thread, represents predictors (to be described in Sec-
tion 3.5), and the tag sequence y=<y1,...,yt>, where
yi ? {C,P}, determines whether a sentence is plain
text P or context C of question Qi.
Answer detection. Answers usually appear in the
posts after the post containing the question. There
are also strong dependencies between contiguous
answer segments. Thus, position and similarity in-
formation alone are not adequate here. To cope
with the dependency between contiguous answer
segments, Linear CRFs model are employed as in
context detection.
3.2 Leveraging Context for Answer Detection
Using Skip-chain CRFs
We observed in our corpus 74% questions lack con-
straints or background information which are very
useful to link question and answers as discussed in
Introduction. Therefore, contexts should be lever-
aged to detect answers. The Linear CRF model can
capture the dependency between contiguous sen-
tences. However, it cannot capture the long distance
dependency between contexts and answers.
One straightforward method of leveraging context
is to detect contexts and answers in two phases, i.e.
to first identify contexts, and then label answers us-
ing both the context and question information (e.g.
the similarity between context and answer can be
used as features in CRFs). The two-phase proce-
dure, however, still cannot capture the non-local de-
pendency between contexts and answers in a thread.
To model the long distance dependency between
contexts and answers, we will use Skip-chain CRF
model to detect context and answer together. Skip-
chain CRF model is applied for entity extraction
and meeting summarization (Sutton and McCallum,
2006; Galley, 2006). The graphical representation
of a Skip-chain CRF given in Figure2(b) consists
of two types of edges: linear-chain (yt?1 to yt) and
skip-chain edges (yi to yj).
Ideally, the skip-chain edges will establish the
connection between candidate pairs with high prob-
ability of being context and answer of a question.
To introduce skip-chain edges between any pairs of
non-contiguous sentences will be computationally
expensive, and also introduce noise. To make the
cardinality and number of cliques in the graph man-
ageable and also eliminate noisy edges, we would
like to generate edges only for sentence pairs with
high possibility of being context and answer. This is
713
(a) Linear CRFs (b) Skip-chain CRFs (c) 2D CRFs
Figure 2: CRF Models
Skip-Chain yv = A yv 6= A
yu = C 4,105 5,314
yu 6= C 3,744 9,740
Table 2: Contingence table(?2=615.8,p-value < 0.001)
achieved as follows. Given a question Qi in post Pj
of a thread with n posts, its contexts usually occur
within post Pj or before Pj while answers appear in
the posts after Pj . We will establish an edge between
each candidate answer v and one condidate context
in {Pk}jk=1 such that they have the highest possibil-
ity of being a context-answer pair of question Qi:
u = argmax
u?{Pk}jk=1
sim(xu, Qi).sim(xv, {xu, Qi})
here, we use the product of sim(xu, Qi) and
sim(xv, {xu, Qi} to estimate the possibility of be-
ing a context-answer pair for (u, v) , where sim(?, ?)
is the semantic similarity calculated on WordNet as
described in Section 3.5. Table 2 shows that yu and
yv in the skip chain generated by our heuristics in-
fluence each other significantly.
Skip-chain CRFs improve the performance of
answer detection due to the introduced skip-chain
edges that represent the joint probability conditioned
on the question, which is exploited by skip-chain
feature function: f(yu, yv, Qi,x).
3.3 Using 2D CRF Model
Both Linear CRFs and Skip-chain CRFs label the
contexts and answers for each question in separate
passes by assuming that questions in a thread are in-
dependent. Actually the assumption does not hold
in many cases. Let us look at an example. As in Fig-
ure 1, sentence S10 is an answer for both question
Q2 and Q3. S10 could be recognized as the answer
of Q2 due to the shared word areas and Causeway
bay (in Q2?s context, S4), but there is no direct re-
lation between Q3 and S10. To label S10, we need
consider the dependency relation between Q2 and
Q3. In other words, the question-answer relation be-
tween Q3 and S10 can be captured by a joint mod-
eling of the dependency among S10, Q2 and Q3.
The labels of the same sentence for two contigu-
ous questions in a thread would be conditioned on
the dependency relationship between the questions.
Such a dependency cannot be captured by both Lin-
ear CRFs and Skip-chain CRFs.
To capture the dependency between the contigu-
ous questions, we employ 2D CRFs to help context
and answer detection. 2D CRF model is used in
(Zhu et al, 2005) to model the neighborhood de-
pendency in blocks within a web page. As shown
in Figure2(c), 2D CRF models the labeling task for
all questions in a thread. For each thread, there are
m rows in the grid, where the ith row corresponds
to one pass of Linear CRF model (or Skip-chain
model) which labels contexts and answers for ques-
tion Qi. The vertical edges in the figure represent
the joint probability conditioned on the contiguous
questions, which will be exploited by 2D feature
function: f(yi,j , yi+1,j , Qi, Qi+1,x). Thus, the in-
formation generated in single CRF chain could be
propagated over the whole grid. In this way, context
and answer detection for all questions in the thread
could be modeled together.
3.4 Conditional Random Fields (CRFs)
The Linear, Skip-Chain and 2D CRFs can be gen-
eralized as pairwise CRFs, which have two kinds of
cliques in graph G: 1) node yt and 2) edge (yu, yv).
The joint probability is defined as:
p(y|x)= 1Z(x) exp
{?
k,t
?kfk(yt,x)+
?
k,t
?kgk(yu, yv,x)
}
714
where Z(x) is the normalization factor, fk is the
feature on nodes, gk is on edges between u and v,
and ?k and ?k are parameters.
Linear CRFs are based on the first order Markov
assumption that the contiguous nodes are dependent.
The pairwise edges in Skip-chain CRFs represent
the long distance dependency between the skipped
nodes, while the ones in 2D CRFs represent the de-
pendency between the neighboring nodes.
Inference and Parameter Estimation. For Linear
CRFs, dynamic programming is used to compute the
maximum a posteriori (MAP) of y given x. How-
ever, for more complicated graphs with cycles, ex-
act inference needs the junction tree representation
of the original graph and the algorithm is exponen-
tial to the treewidth. For fast inference, loopy Belief
Propagation (Pearl, 1988) is implemented.
Given the training Data D = {x(i),y(i)}ni=1, the
parameter estimation is to determine the parame-
ters based on maximizing the log-likelihood L? =?n
i=1 log p(y(i)|x(i)). In Linear CRF model, dy-
namic programming and L-BFGS (limited memory
Broyden-Fletcher-Goldfarb-Shanno) can be used to
optimize objective function L?, while for compli-
cated CRFs, Loopy BP are used instead to calculate
the marginal probability.
3.5 Features used in CRF models
The main features used in Linear CRF models for
context detection are listed in Table 3.
The similarity feature is to capture the word sim-
ilarity and semantic similarity between candidate
contexts and answers. The word similarity is based
on cosine similarity of TF/IDF weighted vectors.
The semantic similarity between words is computed
based on Wu and Palmer?s measure (Wu and Palmer,
1994) using WordNet (Fellbaum, 1998).1 The simi-
larity between contiguous sentences will be used to
capture the dependency for CRFs. In addition, to
bridge the lexical gaps between question and con-
text, we learned top-3 context terms for each ques-
tion term from 300,000 question-description pairs
obtained from Yahoo! Answers using mutual infor-
mation (Berger et al, 2000) ( question description
in Yahoo! Answers is comparable to contexts in fo-
1The semantic similarity between sentences is calculated as
in (Yang et al, 2006).
Similarity features:
? Cosine similarity with the question
? Similarity with the question using WordNet
? Cosine similarity between contiguous sentences
? Similarity between contiguous sentences using WordNet
? Cosine similarity with the expanded question using the lexical
matching words
Structural features:
? The relative position to current question
? Is its author the same with that of the question?
? Is it in the same paragraph with its previous sentence?
Discourse and lexical features:
? The number of Pronouns in the question
? The presence of fillers, fluency devices (e.g. ?uh?, ?ok?)
? The presence of acknowledgment tokens
? The number of non-stopwords
? Whether the question has a noun or not?
? Whether the question has a verb or not?
Table 3: Features for Linear CRFs. Unless otherwise
mentioned, we refer to features of the sentence whose la-
bel to be predicted
rums), and then use them to expand question and
compute cosine similarity.
The structural features of forums provide strong
clues for contexts. For example, contexts of a ques-
tion usually occur in the post containing the question
or preceding posts.
We extracted the discourse features from a ques-
tion, such as the number of pronouns in the question.
A more useful feature would be to find the entity in
surrounding sentences referred by a pronoun. We
tried GATE (Cunningham et al, 2002) for anaphora
resolution of the pronouns in questions, but the per-
formance became worse with the feature, which is
probably due to the difficulty of anaphora resolution
in forum discourse. We also observed that questions
often need context if the question do not contain a
noun or a verb.
In addition, we use similarity features between
skip-chain sentences for Skip-chain CRFs and simi-
larity features between questions for 2D CRFs.
4 Experiments
4.1 Experimental setup
Corpus. We obtained about 1 million threads
from TripAdvisor forum; we randomly selected 591
threads and removed 22 threads which has more than
40 sentences and 6 questions; the remaining 579 fo-
rum threads form our corpus 2. Each thread in our
2TripAdvisor (http://www.tripadvisor.com/ForumHome) is
one of the most popular travel forums; the list of 579 urls is
715
Model Prec(%) Rec(%) F1(%)
Context Detection
SVM 75.27 68.80 71.32
C4.5 70.16 64.30 67.21
L-CRF 75.75 72.84 74.45
Answer Detection
SVM 73.31 47.35 57.52
C4.5 65.36 46.55 54.37
L-CRF 63.92 58.74 61.22
Table 4: Context and Answer Detection
corpus contains at least two posts and on average
each thread consists of 3.87 posts. Two annotators
were asked to tag questions, their contexts, and an-
swers in each thread. The kappa statistic for identi-
fying question is 0.96, for linking context and ques-
tion given a question is 0.75, and for linking answer
and question given a question is 0.69. We conducted
experiments on both the union and intersection of
the two annotated data. The experimental results on
both data are qualitatively comparable. We only re-
port results on union data due to space limitation.
The union data contains 1,064 questions, 1,458 con-
texts and 3,534 answers.
Metrics. We calculated precision, recall,
and F1-score for all tasks. All the experimental
results are obtained through the average of 5 trials
of 5-fold cross validation.
4.2 Experimental results
Linear CRFs for Context and Answer Detection.
This experiment is to evaluate Linear CRF model
(Section 3.1) for context and answer detection by
comparing with SVM and C4.5(Quinlan, 1993). For
SVM, we use SVMlight(Joachims, 1999). We tried
linear, polynomial and RBF kernels and report the
results on polynomial kernel using default param-
eters since it performs the best in the experiment.
SVM and C4.5 use the same set of features as Lin-
ear CRFs. As shown in Table 4, Linear CRF model
outperforms SVM and C4.5 for both context and an-
swer detection. The main reason for the improve-
ment is that CRF models can capture the sequen-
tial dependency between segments in forums as dis-
cussed in Section 3.1.
given in http://homepages.inf.ed.ac.uk/gcong/acl08/; Removing
the 22 long threads can greatly reduce the training and test time.
position Prec(%) Rec(%) F1(%)
Context Detection
Previous One 63.69 34.29 44.58
Previous All 43.48 76.41 55.42
Anwer Detection
Following One 66.48 19.98 30.72
Following All 31.99 100 48.48
Table 5: Using position information for detection
Context Prec(%) Rec(%) F1(%)
No context 63.92 58.74 61.22
Prev. sentence 61.41 62.50 61.84
Real context 63.54 66.40 64.94
L-CRF+context 65.51 63.13 64.06
Table 6: Contextual Information for Answer Detection.
Prev. sentence uses one previous sentence of the current
question as context. RealContext uses the context anno-
tated by experts. L-CRF+context uses the context found
by Linear CRFs
We next report a baseline of context detection
using previous sentences in the same post with its
question since contexts often occur in the question
post or preceding posts. Similarly, we report a base-
line of answer detecting using following segments of
a question as answers. The results given in Table 5
show that location information is far from adequate
to detect contexts and answers.
The usefulness of contexts. This experiment is to
evaluate the usefulness of contexts in answer de-
tection, by adding the similarity between the con-
text (obtained with different methods) and candi-
date answer as an extra feature for CRFs. Table 6
shows the impact of context on answer detection
using Linear CRFs. Linear CRFs with contextual
information perform better than those without con-
text. L-CRF+context is close to that using real con-
text, while it is better than CRFs using the previous
sentence as context. The results clearly shows that
contextual information greatly improves the perfor-
mance of answer detection.
Improved Models. This experiment is to evaluate
the effectiveness of Skip-Chain CRFs (Section 3.2)
and 2D CRFs (Section 3.3) for our tasks. The results
are given in Table 7 and Table 8.
In context detection, Skip-Chain CRFs have simi-
716
Model Prec(%) Rec(%) F1(%)
L-CRF+Context 75.75 72.84 74.45
Skip-chain 74.18 74.90 74.42
2D 75.92 76.54 76.41
2D+Skip-chain 76.27 78.25 77.34
Table 7: Skip-chain and 2D CRFs for context detection
lar results as Linear CRFs, i.e. the inter-dependency
captured by the skip chains generated using the
heuristics in Section 3.2 does not improve the con-
text detection. The performance of Linear CRFs is
improved in 2D CRFs (by 2%) and 2D+Skip-chain
CRFs (by 3%) since they capture the dependency be-
tween contiguous questions.
In answer detection, as expected, Skip-chain
CRFs outperform L-CRF+context since Skip-chain
CRFs can model the inter-dependency between con-
texts and answers while in L-CRF+context the con-
text can only be reflected by the features on the ob-
servations. We also observed that 2D CRFs improve
the performance of L-CRF+context due to the de-
pendency between contiguous questions. In contrast
with our expectation, the 2D+Skip-chain CRFs does
not improve Skip-chain CRFs in terms of answer de-
tection. The possible reason could be that the struc-
ture of the graph is very complicated and too many
parameters need to be learned on our training data.
Evaluating Features. We also evaluated the con-
tributions of each category of features in Table 3
to context detection. We found that similarity fea-
tures are the most important and structural feature
the next. We also observed the same trend for an-
swer detection. We omit the details here due to space
limitation.
As a summary, 1) our CRF model outperforms
SVM and C4.5 for both context and answer detec-
tions; 2) context is very useful in answer detection;
3) the Skip-chain CRF method is effective in lever-
aging context for answer detection; and 4) 2D CRF
model improves the performance of Linear CRFs for
both context and answer detection.
5 Discussions and Conclusions
We presented a new approach to detecting contexts
and answers for questions in forums with good per-
formance. We next discuss our experience not cov-
ered by the experiments, and future work.
Model Prec(%) Rec(%) F1(%)
L-CRF+context 65.51 63.13 64.06
Skip-chain 67.59 71.06 69.40
2D 65.77 68.17 67.34
2D+Skip-chain 66.90 70.56 68.89
Table 8: Skip-chain and 2D CRFs for answer detection
Since contexts of questions are largely unexplored
in previous work, we analyze the contexts in our
corpus and classify them into three categories: 1)
context contains the main content of question while
question contains no constraint, e.g. ?i will visit NY at
Oct, looking for a cheap hotel but convenient. Any good
suggestion? ?; 2) contexts explain or clarify part of
the question, such as a definite noun phrase, e.g. ?We
are going on the Taste of Paris. Does anyone know if it is
advisable to take a suitcase with us on the tour., where
the first sentence is to describe the tour; and 3) con-
texts provide constraint or background for question
that is syntactically complete, e.g. ?We are inter-
ested in visiting the Great Wall(and flying from London).
Can anyone recommend a tour operator.? In our corpus,
about 26% questions do not need context, 12% ques-
tions need Type 1 context, 32% need Type 2 context
and 30% Type 3. We found that our techniques often
do not perform well on Type 3 questions.
We observed that factoid questions, one of fo-
cuses in the TREC QA community, take less than
10% question in our corpus. It would be interesting
to revisit QA techniques to process forum data.
Other future work includes: 1) to summarize mul-
tiple threads using the triples extracted from indi-
vidual threads. This could be done by clustering
question-context-answer triples; 2) to use the tradi-
tional text summarization techniques to summarize
the multiple answer segments; 3) to integrate the
Question Answering techniques as features of our
framework to further improve answer finding; 4) to
reformulate questions using its context to generate
more user-friendly questions for CQA services; and
5) to evaluate our techniques on more online forums
in various domains.
Acknowledgments
We thank the anonymous reviewers for their detailed
comments, and Ming Zhou and Young-In Song for
their valuable suggestions in preparing the paper.
717
References
A. Berger, R. Caruana, D. Cohn, D. Freitag, and V. Mit-
tal. 2000. Bridging the lexical chasm: statistical ap-
proaches to answer-finding. In Proceedings of SIGIR.
J. Burger, C. Cardie, V. Chaudhri, R. Gaizauskas,
S. Harabagiu, D. Israel, C. Jacquemin, C. Lin,
S. Maiorano, G. Miller, D. Moldovan, B. Ogden,
J. Prager, E. Riloff, A. Singhal, R. Shrihari, T. Strza-
lkowski16, E. Voorhees, and R. Weishedel. 2006. Is-
sues, tasks and program structures to roadmap research
in question and answering (qna). ARAD: Advanced
Research and Development Activity (US).
G. Carenini, R. Ng, and X. Zhou. 2007. Summarizing
email conversations with clue words. In Proceedings
of WWW.
G. Cong, L. Wang, C.Y. Lin, Y.I. Song, and Y. Sun. 2008.
Finding question-answer pairs from online forums. In
Proceedings of SIGIR.
H. Cui, R. Sun, K. Li, M. Kan, and T. Chua. 2005. Ques-
tion answering passage retrieval using dependency re-
lations. In Proceedings of SIGIR.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. Gate: A framework and graphical
development environment for robust nlp tools and ap-
plications. In Proceedings of ACL.
H. Dang, J. Lin, and D. Kelly. 2007. Overview of the
trec 2007 question answering track. In Proceedings of
TREC.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database (Language, Speech, and Communica-
tion). The MIT Press, May.
D. Feng, E. Shaw, J. Kim, and E. Hovy. 2006a. An intel-
ligent discussion-bot for answering student queries in
threaded discussions. In Proceedings of IUI.
D. Feng, E. Shaw, J. Kim, and E. Hovy. 2006b. Learning
to detect conversation focus of threaded discussions.
In Proceedings of HLT-NAACL.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Pro-
ceedings of EMNLP.
S. Harabagiu and A. Hickl. 2006. Methods for using tex-
tual entailment in open-domain question answering.
In Proceedings of ACL.
J. Huang, M. Zhou, and D. Yang. 2007. Extracting chat-
bot knowledge from online discussion forums. In Pro-
ceedings of IJCAI.
J. Jeon, W. Croft, and J. Lee. 2005. Finding similar
questions in large question and answer archives. In
Proceedings of CIKM.
T. Joachims. 1999. Making large-scale support vector
machine learning practical. MIT Press, Cambridge,
MA, USA.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. In Pro-
ceedings of CoNLL-2003.
A. Nenkova and A. Bagga. 2003. Facilitating email
thread access by extractive summary generation. In
Proceedings of RANLP.
J. Pearl. 1988. Probabilistic reasoning in intelligent sys-
tems: networks of plausible inference. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
J. Quinlan. 1993. C4.5: programs for machine learn-
ing. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004. Summarizing email threads. In Proceedings of
HLT-NAACL.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In HLT-NAACL.
L. Shrestha and K. McKeown. 2004. Detection of
question-answer pairs in email conversations. In Pro-
ceedings of COLING.
R. Soricut and E. Brill. 2006. Automatic question an-
swering using the web: Beyond the Factoid. Informa-
tion Retrieval, 9(2):191?206.
C. Sutton and A. McCallum. 2006. An introduction to
conditional random fields for relational learning. In
Lise Getoor and Ben Taskar, editors, Introduction to
Statistical Relational Learning. MIT Press. To appear.
S. Wan and K. McKeown. 2004. Generating overview
summaries of ongoing email thread discussions. In
Proceedings of COLING.
Z. Wu and M. S. Palmer. 1994. Verb semantics and lexi-
cal selection. In Proceedings of ACL.
F. Yang, J. Feng, and G. Fabbrizio. 2006. A data
driven approach to relevancy recognition for contex-
tual question answering. In Proceedings of the Inter-
active Question Answering Workshop at HLT-NAACL
2006.
L. Zhou and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet relay
chats. In Proceedings of ACL.
J. Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma. 2005. 2d
conditional random fields for web information extrac-
tion. In Proceedings of ICML.
718
