Proceedings of NAACL HLT 2007, Companion Volume, pages 77?80,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ILR-Based MT Comprehension Test with Multi-Level Questions 
 
Douglas Jones, Martha Herzog, Hussny Ibrahim, Arvind Jairam, Wade Shen,  
Edward Gibson and Michael Emonts 
 MIT Lincoln Laboratory 
Lexington, MA 02420 
{DAJ,Arvind,SWade}@LL.MIT.EDU 
MHerzog2005@comcast.net 
DLI Foreign Language Center
Monterey, CA 93944 
{Hussny.Ibrahim,Michael.Emonts}
@monterey.army.mil  
MIT Brain and Cognitive 
Sciences Department 
Cambridge MA, 02139 
EGibson@MIT.EDU 
Abstract 
We present results from a new Interagency 
Language Roundtable (ILR) based compre-
hension test. This new test design presents 
questions at multiple ILR difficulty levels 
within each document. We incorporated 
Arabic machine translation (MT) output 
from three independent research sites, arbi-
trarily merging these materials into one MT 
condition.  We contrast the MT condition, 
for both text and audio data types, with high 
quality human reference Gold Standard 
(GS) translations.  Overall, subjects 
achieved 95% comprehension for GS and 
74% for MT, across 4 genres and 3 diffi-
culty levels. Surprisingly, comprehension 
rates do not correlate highly with translation 
error rates, suggesting that we are measur-
ing an additional dimension of MT quality.   
We observed that it takes 15% more time 
overall to read MT than GS.  
1 Introduction 
The official Defense Language Proficiency Test 
(DLPT) is constructed according to rigorous and 
well-established principles that have been devel-
oped to measure the foreign language proficiency 
of human language learners in U.S. Department of 
Defense settings.  In 2004, a variant of that test 
type was constructed, following the general DLPT 
design principles, but modified to measure the 
quality of machine translation.  This test, known as 
the DLPTstar (Jones et al 2005),  was based on 
authentic Arabic materials at ILR  text difficulty 
levels 1, 2, and 3, accompanied by constructed-
response questions at matching levels.  The ILR 
level descriptors, used throughout the U.S. gov-
ernment, can be found at the website cited in the 
list of references. The text documents were pre-
sented in two conditions in English translation: (1) 
professionally translated into English, and (2) ma-
chine translated with state-of-the art MT systems, 
often quite garbled.  Results showed that native 
readers of English could generally pass the Levels 
1 and 2 questions on the test, but not those at Level 
3.  Also, Level 1 comprehension was less than ex-
pected, given the low level of the original material.  
It was not known whether the weak Level 1 per-
formance was due to systematic deficits in MT 
performance at Level 1, or whether the materials 
were simply mismatched to the MT capabilities. 
In this paper, we present a new variant of the 
test, using materials specifically created to test the 
capabilities of the MT systems.  To guarantee that 
the MT systems were up to the task of processing 
the documents, we used the DARPA GALE 2006 
evaluation data sets, against which several research 
sites were testing MT algorithms.  We arbitrarily 
merged the MT output from three sites. The ILR 
difficulty of the documents ranged from Level 2 to 
Level 3, but the test did not contain any true Level 
1 documents.  To compensate for this lack, we 
constructed questions about Level 1 elements (e.g., 
personal and place names) in Level 2 and 3 docu-
ments.  A standard DLPT would have more varia-
tion at Level 1.  
2 Related and Previous Work 
Earlier work in MT evaluation incorporated an in-
formativeness measure, based on comprehension 
test answers, in addition to fluency, a measure of 
output readability without reference to a gold stan-
dard, and adequacy, a measure of accuracy with 
reference to a gold standard translation (White and 
O'Connell, 1994).  Later MT evaluation found flu-
ency and adequacy to correlate well enough with 
automatic measures (BLEU), and since compre-
hension tests are relatively more expensive to cre-
ate, the informativeness test was not used in later 
77
MT evaluations, such as the ones performed by 
NIST from 2001-2006.  In other work, task-based 
evaluation has been used for MT evaluation (Voss 
and Tate, 2006), which measures human perform-
ance on exhaustively extracting ?who?, ?when?, and 
?where? type elements in MT output. The DLPT-
star also uses this type of factual question, particu-
larly for Level 2 documents, but not exhaustively.  
Instead, the test focuses on text elements most 
characteristic of the levels as defined in the ILR 
scale.  At Level 3, for example, questions may 
concern abstract concepts or hypotheses found in 
the documents.  Applying the ILR construct pro-
vides Defense Department decision makers with 
test scores that are readily interpretable. 
3 Test Construction and Administration 
In this paper, we present a new test, based entirely 
on the DARPA GALE 2006 evaluation data, se-
lecting approximately half of the material for our 
test. We selected twenty-four test documents, with 
balanced coverage across four genres: newswire, 
newsgroups, broadcast news and talk radio.  Our 
target was to have at least 2500 words for each 
genre, which we exceeded slightly with approxi-
mately 12,200 words in total for the test.  We be-
gan with a random selection of documents and 
adjusted it for better topic coverage.  We con-
structed an exhaustive set of questions for each 
document, approximately 200 questions in total.  
The questions ranged in ILR difficulty, from "0+, 
1,1+, 2, 2+ and 3, with Levels 0+, 1 and 1+ com-
bined to a pseudo-level we called L1~, providing 
four levels of difficulty to be measured.  We di-
vided the questions into two sets, and each indi-
vidual subject answered questions for one of the 
sets. The test itself was constructed by a DLPT 
testing expert and a senior native-speaking Arabic 
language instructor, using only the original Arabic 
documents and the Gold Standard translations.  
They had no access to any machine translation 
output during the test construction or scoring. 
In August 2006, we administered the test at MIT 
to 49 test subjects who responded to announce-
ments for paid experimental subjects.  The subjects 
read the documents in a Latin square design, mean-
ing that each subject saw each document, but only 
in one of the two conditions, randomly assigned.  
Subjects were allowed 5 hours to complete the test.  
Since the questions were divided into two sets for 
each document, the actual set of 49 subjects 
yielded approximately 25 ?virtual subjects? read-
ing the full list of 228 questions.  The mean time 
spent on testing, not counting breaks or subject 
orientation, was 2.5 hours; fastest was 1.1 hours, 
slowest was 3.4 hours. 
The subject responses were hand-graded by the 
two testing experts, following the pre-established 
answers in the test protocol.  There was no pre-
assessment of whether information was preserved 
or garbled in the MT when designing questions or 
responses in the test protocol.  The testing experts 
were provided the reference translations and the 
original Arabic documents, but not the MT during 
scoring.  Moreover, test conditions were masked in 
order to provide a blind assessment.  The two test-
ing experts provided both preliminary and final 
scores; multiple passes provided an opportunity to 
clarify the correct answers and to normalize scor-
ing.  The scoring agreement rate was 96% for the 
final scores. 
4 Overall Results 
The overall result for comprehension accuracy was 
95% for subjects reading the Gold Standard trans-
lation and 74% for reading Machine Translation, 
across each of the genres and difficulty levels. The 
comprehension accuracy for each genre is shown 
in Figure 1. The two text genres score better than 
the audio genres, which is to be expected because 
the audio MT condition has more opportunities for 
error.  Within each modality, the more standard, 
more structured genre fares better: newswire re-
sults are better than newsgroup results, and the 
more structured genre of broadcast news scores 
better than the less constrained, less structured 
conversations present in the talk radio shows. 
 
 
Figure 1. Comprehension Accuracy per Genre  
97% 93% 94% 94%
80% 77% 
72% 66%
0%
20%
40%
60%
80%
100%
Newswire Broadcast News Talk Radio 
GS
MT
Newsgroups 
Overall Comprehension Accuracy 
78
The break-down by ILR level of difficulty for each 
question is shown in Figure 2.  The general trend is 
consistent with what has been observed previously 
(Jones et al 2005).  The best results are at Level 2; 
Level 1 does well but not as well as expected.  
Thus the test has provided a key finding, which is 
that MT systems perform more poorly on Level 1, 
even when the data is matched to their capabilities. 
Level 3 is very challenging for the MT condition, 
and also more difficult in the GS condition.  Using 
a standard 70 percent passing threshold, responses 
to questions on all MT documents, except for 
Level 3, received a passing grade. 
 
Figure 2. Comprehension Accuracy per Level. 
To provide a snapshot of the ILR levels: L1 in-
dicates sentence-level comprehensibility, and may 
include factual local announcements, etc.; L2 indi-
cates paragraph-level comprehensibility; factual/ 
concrete, covering a wide spectrum of topics (poli-
tics, economy, society, culture, security, science); 
L3 involves extended discourse comprehensibility; 
the ability to understand hypotheses, supported 
opinion, implications, and abstract linguistic for-
mulations, etc. 
It was not possible to balance Level 3 documents 
across genres within the GALE evaluation data; 
except for those taken from Talk Radio, most 
documents did not reach that level of complexity.  
Hence, genre and difficulty level were not com-
pletely independent in this test. 
5 Comprehension and Translation Error 
We expect to see a relationship between compre-
hension rates and translation error.  In an idealized 
case, we may expect a precise inverse correlation.  
We then compared comprehension rates with Hu-
man Translation Error Rate (HTER), an error 
measure for machine translation that counts the 
number of human edits required to change system 
MT output so that it contains all and only the in-
formation present in a Gold Standard reference 
(NIST, 2006).  The linear regression line in Figure 
3 shows the kind of inverse correlation we might 
expect.  Subjects lose about 12% in comprehension 
for every 10% of translation error. The R2 value is 
33%.  The low correlation suggests that the com-
prehension results are measuring a somewhat inde-
pendent aspect of MT quality, which we feel is 
important.  HTER does not directly address the 
facts that not all MT errors are equally important 
and that the texts contain inherent redundancy that 
the readers use to answer the questions.  For ex-
ploratory purposes, we divide the graph of Figure 3 
into four quadrants.  Quadrant I and IV contain 
expected behavior: 122 data points of good transla-
tions and good comprehension results versus 43 
points of bad translations and poor comprehension.  
Q-II has 24 robust points: the translations have 
high error, but somehow managed to contain 
enough well-translated words that people can an-
swer the questions.  Q-III has 28 fragile points: the 
few translation errors impaired comprehension. 
 
Figure 3. Comprehension vs. Translation Error. 
We point out that there is a 1-to-1 mapping be-
tween comprehension questions and individual 
sub-passages of the documents in the data.  Each 
point in Figure 3 plots the HTER of a single seg-
ment versus the average comprehension score on 
the corresponding question. The good and bad 
items are essentially a sanity-check on the experi-
mental design.  We expect to see good comprehen-
sion when translations are good, and we expect to 
see poor comprehension when translations are bad.  
Next we will examine the two other types: fragile 
and robust translations. 
Overall Comprehension Accuracy 
97% 96% 91% 88%
77% 82% 76% 
51%
0% 
20% 
40% 
60% 
80% 
100% 
L1~ L2 L2+ L3
GS
MT
  Q-I (Good)                        Q-II (Robust) 
122 points (57%)               24 points (10%)                          
(All Levels and Genres)
0%
20%
40%
60%
80%
100%
0% 20% 40% 60% 80% 100%
x = Translation Error (HTER) 
y = Comprehension (DLPT*)
Q-III (Fragile)                      Q-IV (Bad) 
28 points (13%)                 43 points (20%)                          
79
A fragile translation is one that has a good 
HTER score but a bad comprehension score.  A 
sample fragile translation is one from a broadcast 
news which asks for a particular name:  the HTER 
was a respectable 24%, but the MT comprehension 
accuracy was a flat 0%, since the name was miss-
ing.  Everyone reading GS answered correctly. 
A robust translation is one that has a bad HTER 
score but still manages to get a good comprehen-
sion score.  A sample robust translation is one 
drawn from a posting providing instructions for 
foot massage.  The text was quite garbled, with an 
HTER score of 48%, but the MT comprehension 
accuracy was a perfect 100%. Everyone reading 
the GS condition also answered the question cor-
rectly, which was that one should start a foot mas-
sage with oil. We note in passing that the highest 
error rate for a question with 100% comprehension 
is about 50%, shown with the up-arrow in Figure 
3.  We should be surprised to see any items with 
100% comprehension for HTER rates above 50%, 
considering Shannon?s estimate that written Eng-
lish is about 50% redundant. We expect that MT 
readers are making use of their general world 
knowledge to interpret the garbled MT output.  A 
challenge is to identify robust translations, which 
are useful despite their high translation error rate. 
6 Detailed Discussion 
In this section we will discuss several aspects of 
the test in more detail: the scoring methodology, 
including a discussion of partial credit and inter-
rater agreement; timing information; questions 
about personal names. 
Each correct answer was assigned a score of 1, 
and each incorrect answer was assigned a score of 
0.  Partial credit was assigned on an ad-hoc basis, 
but normalized for scoring by assigning all non-
integer scores to 0.5.  This method yielded scores 
that were generally at the midpoint between binary 
scoring, in which non-integer scored were uni-
formly mapped either harshly to 0 or leniently to 1, 
the average difference between harsh and lenient 
scoring being approximately 11%.  Inter-rater 
agreement was 96%. 
The testing infrastructure we used recorded the 
amount of time spent on each document.  The gen-
eral trend is that people spend longer on MT than 
on GS.  The mean percentage of time spent on MT 
compared with GS is 115% per item, meaning that 
it takes 15% more time to read MT than GS. The 
standard error was 4%.  The median is 111%; 
minimum is 89% and maximum is 159%.  In future 
analysis and experimentation we will conduct more 
fine-grained temporal estimates.    
As we have seen in previous experiments, the 
performance for personal names is lower than for 
non-names.  We observed that the name questions 
have 71% comprehension accuracy, compared with 
the 83% for questions about things other than per-
sonal names.  
7 Conclusions and Future Work 
We have long felt that Level 2 is the natural and 
successful level for machine translation.  The abil-
ity to present concrete factual information that can 
be retrieved by the reader, without requirements 
for understanding the style, tone, or organizational 
pattern used by the writer seemed to be present in 
the previous work. It is worth pointing out that 
though we have many Level 1 questions, we are 
still not really testing Level 1 because the test does 
not contain true Level 1 documents. In future tests 
we wish to include Level 1 documents and ques-
tions.  
Continuing along these lines, we are currently 
creating two new tests. We are constructing a new 
Arabic DLPT-star test, tailoring the document se-
lection more specifically for comprehension testing 
and ensuring texts and tasks are at the intended 
ILR levels. We are also constructing a Mandarin 
Chinese test with similar design specifications.  
We intend for both of these tests to be available for 
a public machine translation evaluation to be con-
ducted in 2007. 
References 
Doddington, G. 2002. Automatic Evaluation of Machine 
Translation Quality Using N-gram Co-Occurrence 
Statistics. Proceedings of HLT 2002. 
NIST 2006. GALE Go/No-Go Eval Plan; www.nist.gov/ 
speech/tests/gale/2006/doc/GALE06_evalplan.v2.pdf 
Jones, D. A., W. Shen, et al 2005a. Measuring Transla-
tion Quality by Testing English Speakers with a New 
DLPT for Arabic. Int?l Conf. on Intel. Analysis. 
Interagency Language Roundtable Website. 2005. ILR 
Skill Level Descriptions: http://www.govtilr.org 
Voss, Clare and Calandra Tate. 2006. Task-based 
Evaluation of MT Engines. European Association for 
Machine Translation conference. 
White, JS and TA O'Connell. 1994. Evaluation in the 
ARPA machine translation program: 1993 method-
ology. Proceedings of the HLT workshop. 
80
Toward a Scoring Function for Quality-Driven Machine Translation 
Douglas A. Jones ~ Gregory M. Rusk 
Department of Defense RABA Technologies 
9800 Savage Road, Suite 6514 10500 Little Patuxent Parkway 
Fort Meade, MD 20755-6514 Colulnbia, MD 21044 
Abstract 
We describe how we constructed an automatic scoring function for machine translation quality; 
this function makes use of arbitrarily many pieces of natural anguage processing software that 
has been designed to process English language text. By machine-learning values of fnnctions 
available inside the software and by constructing functions that yield values based upon the 
software output, we are able to achieve preliminary, positive results in machine-learning the 
difference between human-produced English and machine-translation E glish. We suggest 
how the scoring ftmction may be used for MT system development. 
Introduction to the MT Plateau 
We believe it is fair to say that the field of 
machine translation has been on a plateau for at 
least the past decade. 2 Traditional, band-built 
MT systems held up very well in the ARPA 
MT evaluation (White and O'Connell 1994). 
These systems are relatively expensive to build 
and generally require a trained staff working 
for several years to produce a mature system. 
This is the current commercial state of the art: 
hand-building specialized lexicons and 
translation rules. A completely different ype of 
system was competitive in this evaluation, 
namely, the purely statistical CANDIDE 
system built at IBM. It was generally felt that 
this system had also reached a plateau in that 
more data and more training was not likely to 
improve the quality of the output. 
Low Density Machine Translation 
However, in the case of "Low Density Machine 
Translation" (see Nirenburg and Raskin 1998, 
Jones and Havrilla 1998) commercial market 
forces are not likely to provide significant 
incentives for machine translation systems for 
Low Density (Non-Major) languages any time 
soon. Two noteworthy efforts to break past the 
data and labor bottlenecks for high-quality 
machine translation development are the 
following. The NSF Summer Workshop on 
i Douglas Jones is now at National Institute of 
Standards & Technology, Gaithersburg, MD 20899, 
Douglas.Jones @NIST.gov 
a A sensible, plateau-fi'iendly strategy may be to 
accumulate translation memory to improve both the 
long-term efficiency of human translators and the 
quality of machine translation systems. If we 
imagine that the plateau is really a kind of 
logarithmic function tending ever upwards, we need 
only be patient. 
Statistical Machine Translation held at Johns 
Hopkins University summer 1999 developed a
public-domain version intended as a platform 
for further development of a CANDIDE-style 
MT system. Part of the goal here is to improve 
the trauslation by adding levels of linguistic 
analysis beyond the word N-gram. An effort 
addressing the labor bottleneck is the 
Expedition Project at New Mexico State 
University where a preliminary elicitation 
environlnent for a computational field 
linguistics ystem has been developed (the Boas 
interface; see Nirenburg and Raskin 1998) 
A Scoring Function for MT quality 
Our contribution toward working beyond this 
plateau is to look for a way to define a scoring 
function for the quality of the English output 
such that we can use it to machine-learn a good 
translation grammar. The novelty of our idea 
for this function is that we do not have to define 
the internals of it ourselves per se. We are able 
to define a successful function for two reasons. 
First, there is a growing body of software 
worldwide that has been designed to consume 
English; all we need is for each piece of 
software to provide a metric as to how English- 
like its input is. Second, we can tell whether the 
software had trouble with the input, either by 
system-internal diagnosis or by diagnosing the 
software's output. A good illustration is the 
facility in current word-processing software to 
put red squiggly lines underneath text it thinks 
should be revised. We know fi'om experience 
that this feature is often only annoying. 
Nevertheless, imagine that it is correct some 
percentage of the time, and that each piece of 
software we use for this purpose is correct solne 
percentage of the time. Our strategy is to 
376 
extract or create nurneric wflues fl'om each 
piece of software that corresponds to the degree 
to which the software was happy with the input. 
That array of numbers is tile heart of our 
scorim, function for En~lishness ~- we are 
calling these numeric values "indicators" of 
Englishness. We then use that array of 
indicators to drive the machine translation 
development. In this paper we will report on 
how we have constructed a prototype of this 
function; in separate work we discuss how to 
insert this function into a machine-learning 
regimen designed to maximize the overall 
quality of the rnachine translation output. 
A Reverse Turing Test 
People can generally tell the difference between 
human-produced English and machine 
translation Englisll~ assuming all tile obvious 
constraints uch as that tile reader and writer 
have command of the language. Whether or 
not a machine can tell the difference depends of 
course, on how good tim MT system is. Can we 
get a machine to tell tile difference? Of course 
it depends on how good the MT system is: if it 
were perfecL neither we nor the machines 
ought to be able to distinguish them. MT 
quality being what it is, that is not a problem 
for us now. An essential first step toward 
Q1)MT is what we are calling a "Reverse 
Turing Test". In the ordinary Turing Test, we 
want to fool a person into thinking the machine 
is a person. Here, we are turning that on its 
head. We want to define a function that can tell 
tile difference between English that a human 
being has produced versus English that the 
machine has produced) To construct he test, 
we use a bilingual parallel aligned corpus: we 
take tile foreign language side and send that 
through the MT system; then we see if we can 
define a scoring function that can distinguish 
the two w:rsions (original English and MT 
English). With our current indicators and 
corpus, we can machiue-leam a function that 
behaves as follows: if you hand it a human 
sentence, it conectly classifies it as human 74% 
of the time. If you hand it a machine sentence, 
it correctly classifies it as a machine sentence 
57% of the time. In tile remainder of the paper, 
we will step through the details of tile 
experiment; we will also discuss why we 
3Obviously the end goal here is to fail this Reverse 
Turing Test for a "perfect" machine translation 
system. We are very far away from this, but we 
would like to use this function to drive the process 
toward that eventual alld ti)rtunate failure. 
neither expect nor require 100% accuracy for 
this function. Our boundary tests behave as 
expected and are shown ill the final section -- 
we use tile same test to distinguish between 
English and (a) English word salad, (b) English 
alphabet soup, (c) Japanese, and (d) the identity 
case of more human-produced English. 
Case Study: Japanese-English 
In this paper, we report on results using a small 
corpus of 2,340 sentences drawn from the 
Kenkyusha New Japanese-English Dictionary. 
It was important in this particular experiment to 
use a very clean corpus (perfectly aligned and 
minimally formatted). This case study is 
situated in a broader context: we have 
conducted exploratory experiments on samples 
from several corpora, for example the ARPA 
MT Evaluation corpus, samples from European 
Corpus Initiative Data corpus (ECI-I) and 
others. Since we found that the scoring 
function was quite sensitive to forrnatting 
problems (for example, the presence of tables 
and sentence segmentation enors cause 
problems) we are examining a small corpus that 
is free flom these issues. The sentences are on 
average relatively short (7.0 words per 
sentence; 37.6 characters/sentence), this makes 
our task both easier and harder. It is easier 
because we have overcome tile forlnatting 
problems. It is harder because the MT system 
is able to perform much better on the shorter, 
cleaner sentences than it was on longer 
sentences with formatting problems. Since the 
output is better, it is more difficult to define a 
function that can tell the difference between the 
original English and the machine translation 
English. On balance, this corpus is a good one 
to illustrate our technique. 
i(l) #208 .~j~0)  ~z:~: {j~: {.a ff~:3;\]~ b j3';'\]-2 ~ x _3 \]-7_o 
}tie beauty ballled descnptu n
MT It described he, beauty and the abno,mal 
play applied 
, She was radiant with happiness 
MT she had shone happily 
In terror the child seized his father's\] 
/ a l l l l .  
! MT !Becoming fearful, the child , \] 
I : ",c a,m fa!h e'- I 
lFigure 1. Subjective Quality Ranking \] 
377 
Figure 1 shows a range of output quality. (1) is 
the worst -- it is obviously MT output. For us 
this output is only partially intelligible. (2) is 
not so bad, but it is still not perfect English. But 
(3)is nearly perfect. We want to design a 
system that can tell the difference. We will 
now walk through our suite of indicators; the 
goal is to get the machine to see what we see in 
terms of quality. 
Suite of Indicators 
We have defined a suite of functions that 
operate at various levels of linguistic analysis: 
syntactic, semantic, and phonological 
(orthographic). For each of these levels, we 
have integrated at least one tool for which we 
construct an indicator function. The task is to 
use these indicators to generate an array of 
values which we can use to capture the 
subjective quality we see wheu we read the 
sentences. We will step through these indicator 
functions one by one. In some cases, in order 
to get numbers, we take what amounts to 
debugging information from the tool (lnany of 
the tools have very nice API's that give access 
to a variety of information about how it 
processed input). In other cases, we define a 
function that yields an output based oil the 
output of the tool (for example, we defined a 
function that indicated the degree to which a 
parse tree was balanced; it turned out that a 
balanced tree was a negative indicator of 
Englishness, probably because English is right- 
branching). 
Syntactic Indicators 
Two sources of local syntactic information are 
(a) parse trees and (b) N-grams. Within tile 
parsers, we looked at internal processing 
information as well as output structures. For 
example, we measured the probability of a 
parse and number of edges in the parse from the 
Collins parser. The Apple Pie Parser provided 
various weights which we used. The Appendix 
lists all of the indicator functions that we used. 
N-Gram Language Model (Cross-Perplexity) 
An easy number to calculate is the cross- 
perplexity of a given text, as calculated using 
an N-gram language model. 4
4 We used the Cambridge/CMU language modeling 
toolkit, trained on the Wall Street Journal (4/1990 
through 3/1992), (hn parameters: n=4, Good-Turing 
smoothing) 
- C I 'OSS-  
perplexity 
__ (  1 ) 2439 It described her beauty and the 
_ abnormal play applied 
(2) ~ 2185 She had shone happily 
(3~ 1836 Becoming fearful, the child 
grasped the arm of the father 
tightly 
Figure 2. Cross-Perplexity Indicator 
Notice that the subjective order is mirrored by 
the cross-perl?lexity scores in Figure 2. 
Collins Parser 
The task here is to write functions that process 
the parse trees and return a number. We have 
experimented with lnore elaborate functions 
that indicate how balanced the parse tree is and 
less complicated functions uch as the level of 
embedding, number of parentheses, and so oil. 
Interestingly, the number of parentheses in the 
parse was a helpful indicator in conjunction 
with other indicators. 
Indicators of Semantic Cohesiveness 
For the semantic indicators, we want some 
indication as to how nmch the words in a text 
are related to each other by virtue of their 
meaning. Which words belong together, 
regardless of exactly how they are used in the 
seutence? Two resources we have begun to 
integrate for this purpose are WordNet and the 
Trigger Toolkit (measuring mutual 
information). The overall experimental design 
is roughly the same in both cases. Our method 
was to remove stop words, lemmatize the text, 
and then take a measurement of pairwise 
semantic cohesiveness of the iemmatized 
words 5. For WordNet, we are counting how 
many ways two words are related by the 
hyponylny relation (future indicators will be 
snore sophisticated). For the Trigger Toolkit, 
we weighted the connections (by mutual 
information). 
Orthographic 
We had two motivations for an orthographic 
level: one was methodological (we wanted to 
look at each of tile traditional levels of 
linguistic analysis). The other was driven by 
our experience in looking at the MT output. 
Some MT systems leave untranslated words 
5The following parameters were used to build and 
calculate mutual information using the Trigger 
Toolkit: (1) All uppercase l tters were converted 1o 
lowercase (2) All numbers were converted to a 
"NUMBER" token (3) Punctuation stripped (4) 
Stopwords removed (5) Words lcnunatized. 
378 
alone, or transliterate them, or insert a dummy 
synlbol, such as "X". These cities were 
adequate to give us apl)ropriate hints as to 
whether the text was produced by human or by 
machine. But some of our tools missed these 
clues because of how they were designed. 
Robust parsers often treat uukllowu words as 
UOUlIS,; SO if we got au uut raus la ted  tel i l l  or  an 
"X", the parser simply treats it as a noun. Five 
X's in a row might be a noun phrase followed 
by a verb. a Smoothed N-gram models of words 
usually treat any string of letters as a possible 
word. 
MT 
output 
worst 
(1) 
mid 
(2) 
best 
(3) 
Word N- Num. 
gram Edges 
Cross Per-i 
plexity 
2439 i152 
2185 27i 
1836 1654 
Pie 
Parser 
~core 
247 
139 
302 
A1;plo Sumof \[Char 
= 
mutual !N-gram 
infer- !cross 
ina/ ion per- 
!plexily 
0 '8.1 
0 16.3 
1.7E-4 9;3 
Figure 3. Subjective and Objective Ranidngs 
Because the parsers and N-gram models were 
designed to be very robust, they are not 
necessarily sensitive to these obvious clues. In 
order to get at these hints, we built a character- 
based N-gram model of English. Although 
these indicators were not very informative on 
their own for distinguishing htunan froln 
machine English, they boosted l)erforlnancc in 
conjunction with the syntactic aud semantic 
indicators. 
Combined Indicators 
Let's come back to the three sentences t'rom 
Figure 1: we want to capture our subjective 
ranking of tile sentences with appropriate 
indicator willies. In other words, we want the 
machine to be able to see differences which a 
human might see. 
For these three examples, some scores correhtte 
well with our subjective ranking of Englishuess 
(e.g. cross-perplexity, Edges). However, the 
other scores on their own only partially 
correlate. The expectation is that an indicator 
on its own will not be sutTicient o score tile 
Englishness. It is the combined effect of all 
indicators which ultimately decides the 
6We found that we cot, ld often guess the "del'ault" 
behavior that a parser used and we have begun to 
design indicators that can tell when a parser has 
defaulled to these. 
Englishness. Now we have enough raw data to 
begin machine-learning a way to distinguish 
these kinds of sentences. 
Simple Machine Learning Regimen 
We have started out with very simple memory- 
based machine learning techniques. Since we 
are del'ining a range of functions, we wanted to 
keep things relatively simple for debugging and 
didactic purposes. 
KNN 
One of the simplest methods we can use for 
classification is to collect values of the N 
indicators for a set of training cases and for the 
test cases, to find tile K nearest raining cases 
(using Euclidean distance in N-dimensional 
space). For K, we used 5 for our general 
CXl)el'iments (but see below fol" sonic 
variations). For a concrete example in two 
dimensions, imagine that wc use the cross- 
perplexity of an N-granl language model for the 
Y-axis and the probability of a parse from the 
Collins parser for tile X-axis. Human sentences 
tended to have bettor (lower) cross-perplexity 
numbers and better (higher) parse probabilities. 
If the 5 nearest neighbors to a data point were 
(h,h,h,h,m) four human sentences and olle 
machine our KNN function guesses that it is a 
human sentence. 
Figure 4 lists some of the parameters we used 
for KNN. The vahles for cross perplexity 
ranged fronl around 100 to 10,000 and the 
Collins parse probability (log) ranged from 
around -1000 to 0. These wlhles were 
n0rmalizcd torange fi'om 0-1. 
All columns were scaled between 0 and 1. 
- Value for K in KNN was set to 5. 
- Value for L in KNN was set to 0 (L is the 
i minimum number of positive neighbors 
I required for a confident classification 
i.e. L=5 means all neighbors must be of 
i one class) 
i- Distance calculation is Euclidean 
'- We used 10-fold cross-validation and 
i calculated the average classification 
l accuracy for the overall score. 
t:Figure 4. KNN l~arametel's 
To get an indication of how much guessing 
figured into tile classification, we wwied L fl'om 
3 to 5, keeping K at 5. We found that we get the 
same overall shape for tile classification, with 
fewer guesses made. Of course the penalty for 
not guessing as nmch is that more cases are left 
unclassified. When we reduced guessing by 
setting L to 4, we correctly classified 47% of 
the human sentences as human and incorrectly 
chlssified 9% of the human sentences as 
379 
machine (the remaining 44% were not 
classified). By setting L to 5 (eliminating 
guessing) these numbers dropped to 18% and 
2% respectively. When we varied K (for 
example, trying K of 101) we found that we can 
increase the performance of the human 
classifier to nearly 90%. Performance of KNN 
tended to top out at around 74% with the 
parameters in Figure 4. 
Indicator Monotonicity 
There is no guarantee that classification will 
perform better with more dimensions in KNN. 
However, we found that we generally got a 
monotonically increasing performance in 
classification when we added indicator 
functions. A helpful analogy might be to 
consider the blind men and the elephant. In our 
case, "English" is the elephant, and each of our 
indicator functions is one blind man grasping at 
the elephant. One is grasping at semantics, one 
at syntax, and so on. Figure 5 shows how 
classification improves with more indicators 
(the back of the elephant, so to speak). 
Benchmarks 
To calibrate' the indicator functions we have 
used to classify text into human- or machine- 
produced, we tested our method with some 
boundary cases, shown in Figure 6. The most 
extreme case was to learn the difference 
l Bu~in  I( MT 
o 
/ 
\ ]r ,oB~ 
Top: Truth is human; machine guesses human 
Bottom: Truth is MT; machine guesses MT 
Figure 5 
between Japanese text (in native character 
encoding) and English. 
Truth is: 
Machine 
Guesses: 
human machine 
Japanese 99.6 99.6 
A!phabet Soup 99.4 99.2 
Word Salad 95\[95.4.4 91.1 
MT Output \[74.0 56.1 
Identity Case 52.3 49.4 
Figure 6. Baselines 
In other words, we have come up with a very 
computationally expensive method for 
Language Identification. Next less extreme was 
what we called "Alphabet Soup"; we took 
English sentences from the English side of the 
Kenkyusha corpus: for each alphabetic 
character, we substituted a randomly-selected 
alphabetic character, preserving case and 
punctuation. 7 For "Word Salad", we took the 
English sentences from the Kenkyusha corpus 
and scrambled their order. MT Output is the 
case we discussed in detail above. The Identity 
Case is to divide the English sentences from the 
corpus into two piles and then try to tell them 
apart. As Figure 6 shows, the pathological 
baseline cases all work out very well: our 
machine can ahnost always tell that Japanese, 
Alphabet Soup, and Word Salad are not 
English. Nor can it distinguish between 
two arbitrarily divided piles of human 
English. 
Other Classification Algorithms 
We have performed some initial 
experiments with Support Vector 
Machines (SVM) as a classification 
method. SVM attempts to divide up an n- 
dimensional space using a set of support 
vectors defining a hyperplane. The basic 
approach of the SVM algorithm is 
different from KNN in that it actually 
deduces a classification model from the 
training data. KNN is a memory-based 
learning algorithm wherein the model is 
essentially a replica of the training 
examples. 
The initial trials using SVM are yielding 
classification accuracies of correctly 
classifying 83% of the human sentences 
and 64% of the machine sentences (single 
7We found that it was often easy to crash some 
of the software when we fed it arbitrary binary 
data, so we used "Alphabet Soup" instead of 
arbitrary binary data. 
380 
randonl sample of 10% withheld -- no n-fold 
cross-validation). These accuracies represent 
iml~rovenaents of 11% for truman test sentences 
and 14% for tile machine test sentences. 
Further tests on this and other classification 
methods will be investigated to maximize 
performance ill terms of accuracy and 
execution time. 
Next Steps 
There are two general areas we are cominuiug 
to work on: (a) to increase the scope and 
reliability of our indicators and (b) to insert tile 
scoring function into a machine-learning 
regimen for producing translation grammars. In 
the first area, we have begun to explore tile 
degree to which we might recapitulate tile 
ARPA MT Evaluation. The data from these 
evaluations are freely available, a Of course if 
all we did was recapitulate the data in some 
non-explanatory way, we would be doing 
something analogous to using the Chicago 
Bears to predict the stock market. The real 
work here is to map the objective scoring 
function numbers back to reliable subjective 
evaluation of the machine-produced texts. A 
crucial task t'or us here is to get a deeper 
understanding of how each of the pieces of 
software behaves with various types of input 
text. We are cnrrently at a quite preliminary 
stage in terms of the number of indicators we 
are using and the degree to which each is fine- 
tuned to out" \]mrpose. For machine-learning a 
translation gramnmr, we have begun to explore 
using our scoring function to drive the 
construction of a prototype Low Density 
machine translation grammar compatible with a 
previous ystem built by hand. We have found 
that the scoring function is sensitive to the word 
order difference between the target English 
translation and the glosses for the source 
language. We would like to re-create a 
compatible knowledge base of the English half 
of the translation grammar using only the 
glosses as input. Such a technique would 
reduce the labor requirements in constructing a 
translation knowledge base. 
Reverse Turing Scores for Machine 
Learning Grammars 
To illustrate how we can use tile Reverse 
Turing scores to machine learn a grammar, let 
us consider a simple case of learning lexical 
features for a unification-based phrase structure 
grammar of the sort discussed ill Jones & 
SFrom ursula.gcorgctown.cdu/mt_wcl~. 
Havrilla 1998. The working assumption there is 
that an adequate translation grammar can be 
created that conforms to the constraiut that the 
only reordering that is allowed is the 
permutation of nodes in a biuary-branching tree 
(as in Wu 1995, among others). How might we 
learn that postpositions and verbs generally 
trigger inversion? Consider the following 
example as shown ill Figure 7 fronl Jones & 
Havrilla 1998; let us use +T to indicate that the 
lexical item triggers inversion; -T means that it 
does not. Let the initial state of the lexicon 
mark all lexical items as "-T". 
Shobl)a~ karate-men \]baiThii hal 
~OS N N O IV V 
! -T -T +?' I+T ,, \[+T 
I 'Sh?bha l the_room-in \],,.sitting _ J,is 
~igurc 7. Shobha is sitting in tile room. 
Ore" machine learning process marks lexical 
items as "+T" when the Reverse Tnring 
classification score for the bilingual corpus 
improves. 
Conclusion 
We are capitalizing on two historical accideuts: 
(1) that English is a major world language and 
(2) that we want to translate into English. In 
addition to a variety of modern, standard NLP 
techniques and ideas, we have drawn fi'om two 
unlikely sources of intellectual capital: (l) 
philosophy of language and (2) the current 
ubiquity of hmguagc cnginecring software. 
What we have taken from (1) is that we have 
assumed that lhere is such a thing as "English". 
That might not seem like much of an 
assmnption, but we are treading near some very 
thorny problems in the philosophy of language. 
We can no nlore point to English than we can 
point to tile perfect riangle. And like the blind 
men grasping at tile elephant, how we 
characterize it depends on how we are exploring 
it. What is ilnportant is the helpful aggregate of 
numeric values that we use for the scoring. 
What does this mean for machine translation? 
We want to "Begin with the End in Mind"; in 
other words, we want the machine translation 
system to create output hat scores well on our 
indicators of Englishness. The rest would be 
details, so to speak. 
Acknowledgments 
This project was funded ill part by the 
Advanced Research and Development Activity. 
We would like to thank our colleagues at DoD 
for very helpful discussions and insights. 
381 
Appendix 
List of current Indicators 
1. Word N-Granl (CM U/Cambridge Language Tk) 
2. Ntunber of edges in parse (Collins Parser) 
3. Log probability (Collins Parser) 
4. 1,2xecution time (Collins Parser) 
5. Paren count (Collins Parse,') 
6. Mean leaf node size of parse live (Collins Parser) 
7. Mean NN sequence l ngth (Collins Parser) 
8. Overall scorn (Apple Pie Parser) 
9. Word level score (Apple Pie Parser) 
I 0. Node coun! (Apple Pie Parser) 
11. User execution lime (Apple Pie Parser) 
12. CD node count (Apple Pie Parser) 
13. Mean CD sequence l ngth (Apple Pie Parser) 
14. Mean leading spaces in outline tree (fi'om Collins Parse) 
15. Tree balance ratio (fl'om Collins Parse) 
16. Tree depth (fi'om Collins Parse) 
17. Average minimtun hypernym path length in WordNet 
18. Average number hypernym paths in WordNel 
19. Path found ratio in WordNel 
20. Percent words with sense in WordNet 
21. Sum of count of relations (Trigger Toolkit) 
22. Mean of eotinl of 1elations (T,igger Toolkit) 
23. Sum of mutual information (Trigger Toolkil) 
24. Mean of mutual information (Trigger Toolkil) 
25. Pairs with mutual information (Trigger Toolkit) 
26. Weighled pair sum of mutual information (Trigger Toolkit) 
27. Number of target paired words (Trigger Toolkil 
27. .  N-Gram Cross-perplexity (Cambridge/CMU Lang Tk.) 
Tools 
TiMBL: Tilburg Memory Based Learner 2.0. ILK 
Research Group. http:/lilk.kub.nl/software.html. 
PCKIMMO 2.0. Summer Institute of Linguistics. 
MXTERMINATOR. Adwait Ratnaparkhi. 
WEKA 3.0. University of Waikato. 
flp://ftp.cs.waikato.ac.nz/pubhnl/weka-3-O.jar 
Collins Parser 98. 
Brill Tagger 1.14 
R Statistical Package 0.65.0. http://cran.r- 
project.org/ 
Apple Pie Parser 5.8. New York University. 
http://cs.nyu.edu/cs/projects/proteus/app 
WordNet 1.6. ftp://ftp.cogsci.princeton.edu/-wn/ 
Trigger Toolkit 1.0. CMU. 
http://www.cs.cmu.edu/~aberger/software. 
References 
Afifi, A.A., Virginia Clark. 1996. Computer-Aided 
Multivariate Analysis, 3rd ed.. New York, NY: 
Chapman and Hall. 
Brill, Eric. 1995. Transformation-Based Error- 
Driven Learning and Natural Language 
Processing: A Case Study in Part Of Speech 
Tagging. (ACL). 
Chambers, John M., William S. Cleveland, Beat 
Kleiner, and Paul A. Tukey. 1983. Graphical 
Methods for Data Analysis. Boston, MA: Duxbury 
Press. 
Clarkson, Philip, Ronald Rosenfeld. 1997. 
Statistical Language Modeling Using the CMU- 
Cambridge Toolkit. Eurospeech97. Rhodes, 
Greece 
Collins, Michael. 1997. Three Generative, 
Lexicalised Models for Statistical Parsing. 
Proceedings of the 35th Annual Meeting of the 
ACL/EACL), Madrid. 
Daelemans, Walter, Jakub Zavrel and Ko van der 
Sleet. 1998. TiMBL: Tilbm'g Memory Based 
Learner, version 2.0, Reference Guide. Available 
l'ronl http://ilk.kub.nl/software.html. 
Everitt, Brian S. and Graham Dunn. 1992. Applied 
Multivariate Data Analysis. New York, NY: 
Oxford University Press. 
Fellbaum, Christiane (ed.). 1998. WordNet: An 
Electronic Lexical )atabase. Cambridge, MA: The 
MIT Press. 
Fhu'y, Bernhard and Hans Riedwyl. 1988. 
Multivariate Statistics. New York, NY: Chapman 
and Hall. 
Hornik, Kurt. 1999. "The R FAQ". Available at 
http://www.ci.tuwien.ac.al/Miornik/R/. 
Jones, Doug and Rick Haw'ilia. 1998. Twisted Pair 
Grammar: Sul~port for Rapid Development of 
Machine Translation for Low Density Languages. 
AMTA-98. Langhorn, PA. 
Knight, Kevin, Ishwar Chandler, Matthew Haines, 
Vasilcios, Hatzivassigloglou, Eduard Hovy, 
Masayo Ida, Steve Luk, Richard, Whimey, and 
Kenji Yamada. 1994. Integrating Knowledge 
Bases and Statistics in MT (AMTA-94) 
Manning, Christopher D. and Hinrich Schutze. 1999. 
Foundations of Statistical Natural Language 
Processing. Cambridge, MA: The MIT Press. 
Masuda, Koh (ed). 1974. Kcnkyusha's New 
Japanese-English Dictionary, 4th Ed. Tokyo: 
Kenkyusha. 
Michalski, Ryszard S., Ivan Bratko, and Miroslav 
Kubat (cds.). 1998. Machine Learning and Data 
Mining. John Wiley & Son. 
Mitchell, Tom M. 1997. Machine Learning. Boston, 
MA: McGraw-Hill. 
Nirenburg, S. and V. Raskin. 1998. Universal 
Grammar and Lcxis for Quick l~,amp-Ui ~ of MT 
Systems. Proceedings of ACL/COL1NG "98. 
Montrdal: University el' Montreal (in press). 
Reynar, Jeffrey C. and Adwait Ratnaparkhi. 1997. A 
Maximum Entropy Approach to ideutifying 
Sentence Boundaries. ANLP-97. Washington, 
D.C. 
Rosenfeld, Ronald. 1996. A Maximum Entropy 
Approach to Adaptive Statistical Language 
Modeling. Computer, Speech and Language. 
Witten, Ian H. and Eibe Frank. 1999. Data Mining: 
Practical Machine Learning Tools and Techniques 
with Java Implementations. Morgan Kaufmann. 
White, J. and T.A. O'Connell. 1994. The ARPA MT 
Evahmtion Methodologies: Evolution, Lessons, 
and Future Approaches. Proceedings of AMTA- 
94 
Wu, Dekai and Xuanyin Xia. 1995. Large-Scale 
Automatic Extraction of an English-Chinese 
Translation Lexicon. Machine Tranlsation. 9:3, 1- 
28. 
382 
