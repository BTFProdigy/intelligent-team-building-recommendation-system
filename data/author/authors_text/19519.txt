Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 514?522,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
HTM: A Topic Model for Hypertexts
Congkai Sun?
Department of Computer Science
Shanghai Jiaotong University
Shanghai, P. R. China
martinsck@hotmail.com
Bin Gao
Microsoft Research Asia
No.49 Zhichun Road
Beijing, P. R. China
bingao@microsoft.com
Zhenfu Cao
Department of Computer Science
Shanghai Jiaotong University
Shanghai, P. R. China
zfcao@cs.sjtu.edu.cn
Hang Li
Microsoft Research Asia
No.49 Zhichun Road
Beijing, P. R. China
hangli@microsoft.com
Abstract
Previously topic models such as PLSI (Prob-
abilistic Latent Semantic Indexing) and LDA
(Latent Dirichlet Allocation) were developed
for modeling the contents of plain texts. Re-
cently, topic models for processing hyper-
texts such as web pages were also proposed.
The proposed hypertext models are generative
models giving rise to both words and hyper-
links. This paper points out that to better rep-
resent the contents of hypertexts it is more es-
sential to assume that the hyperlinks are fixed
and to define the topic model as that of gen-
erating words only. The paper then proposes
a new topic model for hypertext processing,
referred to as Hypertext Topic Model (HTM).
HTM defines the distribution of words in a
document (i.e., the content of the document)
as a mixture over latent topics in the document
itself and latent topics in the documents which
the document cites. The topics are further
characterized as distributions of words, as in
the conventional topic models. This paper fur-
ther proposes a method for learning the HTM
model. Experimental results show that HTM
outperforms the baselines on topic discovery
and document classification in three datasets.
1 Introduction
Topic models are probabilistic and generative mod-
els representing contents of documents. Examples
of topic models include PLSI (Hofmann, 1999) and
LDA (Blei et al, 2003). The key idea in topic mod-
eling is to represent topics as distributions of words
* This work was conducted when the first author visited
Microsoft Research Asia as an intern.
and define the distribution of words in document
(i.e., the content of document) as a mixture over hid-
den topics. Topic modeling technologies have been
applied to natural language processing, text min-
ing, and information retrieval, and their effective-
ness have been verified.
In this paper, we study the problem of topic mod-
eling for hypertexts. There is no doubt that this is
an important research issue, given the fact that more
and more documents are available as hypertexts cur-
rently (such as web pages). Traditional work mainly
focused on development of topic models for plain
texts. It is only recently several topic models for pro-
cessing hypertexts were proposed, including Link-
LDA and Link-PLSA-LDA (Cohn and Hofmann,
2001; Erosheva et al, 2004; Nallapati and Cohen,
2008).
We point out that existing models for hypertexts
may not be suitable for characterizing contents of
hypertext documents. This is because all the models
are assumed to generate both words and hyperlinks
(outlinks) of documents. The generation of the latter
type of data, however, may not be necessary for the
tasks related to contents of documents.
In this paper, we propose a new topic model for
hypertexts called HTM (Hypertext Topic Model),
within the Bayesian learning approach (it is simi-
lar to LDA in that sense). In HTM, the hyperlinks
of hypertext documents are supposed to be given.
Each document is associated with one topic distribu-
tion. The word distribution of a document is defined
as a mixture of latent topics of the document itself
and latent topics of documents which the document
cites. The topics are further defined as distributions
514
of words. That means the content (topic distribu-
tions for words) of a hypertext document is not only
determined by the topics of itself but also the top-
ics of documents it cites. It is easy to see that HTM
contains LDA as a special case. Although the idea of
HTM is simple and straightforward, it appears that
this is the first work which studies the model.
We further provide methods for learning and in-
ference of HTM. Our experimental results on three
web datasets show that HTM outperforms the base-
line models of LDA, Link-LDA, and Link-PLSA-
LDA, in the tasks of topic discovery and document
classification.
The rest of the paper is organized as follows. Sec-
tion 2 introduces related work. Section 3 describes
the proposed HTM model and its learning and infer-
ence methods. Experimental results are presented in
Section 4. Conclusions are made in the last section.
2 Related Work
There has been much work on topic modeling. Many
models have been proposed including PLSI (Hof-
mann, 1999), LDA (Blei et al, 2003), and their
extensions (Griffiths et al, 2005; Blei and Lafferty,
2006; Chemudugunta et al, 2007). Inference and
learning methods have been developed, such as vari-
ational inference (Jordan et al, 1999; Wainwright
and Jordan, 2003), expectation propagation (Minka
and Lafferty, 2002), and Gibbs sampling (Griffiths
and Steyvers, 2004). Topic models have been uti-
lized in topic discovery (Blei et al, 2003), document
retrieval (Xing Wei and Bruce Croft, 2006), docu-
ment classification (Blei et al, 2003), citation analy-
sis (Dietz et al, 2007), social network analysis (Mei
et al, 2008), and so on. Most of the existing models
are for processing plain texts. There are also models
for processing hypertexts, for example, (Cohn and
Hofmann, 2001; Nallapati and Cohen, 2008; Gru-
ber et al, 2008; Dietz et al, 2007), which are most
relevant to our work.
Cohn and Hofmann (2001) introduced a topic
model for hypertexts within the framework of PLSI.
The model, which is a combination of PLSI and
PHITS (Cohn and Chang, 2000), gives rise to both
the words and hyperlinks (outlinks) of the document
in the generative process. The model is useful when
the goal is to understand the distribution of links
as well as the distribution of words. Erosheva et
al (2004) modified the model by replacing PLSI with
LDA. We refer to the modified mode as Link-LDA
and take it as a baseline in this paper. Note that the
above two models do not directly associate the top-
ics of the citing document with the topics of the cited
documents.
Nallapati and Cohn (2008) proposed an extension
of Link-LDA called Link-PLSA-LDA, which is an-
other baseline in this paper. Assuming that the cit-
ing and cited documents share similar topics, they
explicitly model the information flow from the cit-
ing documents to the cited documents. In Link-
PLSA-LDA, the link graph is converted into a bi-
partite graph in which links are connected from cit-
ing documents to cited documents. If a document
has both inlinks and outlinks, it will be duplicated
on both sides of the bipartite graph. The generative
process for the citing documents is similar to that of
Link-LDA, while the cited documents have a differ-
ent generative process.
Dietz et al(2007) proposed a topic model for ci-
tation analysis. Their goal is to find topical influ-
ence of publications in research communities. They
convert the citation graph (created from the publica-
tions) into a bipartite graph as in Link-PLSA-LDA.
The content of a citing document is assumed to be
generated by a mixture over the topic distribution
of the citing document and the topic distributions of
the cited documents. The differences between the
topic distributions of citing and cited documents are
measured, and the cited documents which have the
strongest influence on the citing document are iden-
tified.
Note that in most existing models described above
the hyperlinks are assumed to be generated and link
prediction is an important task, while in the HTM
model in this paper, the hyperlinks are assumed to
be given in advance, and the key task is topic iden-
tification. In the existing models for hypertexts, the
content of a document (the word distribution of the
document) are not decided by the other documents.
In contrast, in HTM, the content of a document is
determined by itself as well as its cited documents.
Furthermore, HTM is a generative model which can
generate the contents of all the hypertexts in a col-
lection, given the link structure of the collection.
Therefore, if the goal is to accurately learn and pre-
515
Table 1: Notations and explanations.
T Number of topics
D Documents in corpus
D Number of documents
?? , ?? Hyperparameters for ? and ?
? Hyperparameter to control the weight between
the citing document and the cited documents
? Topic distributions for all documents
? Word distribution for topic
b, c, z Hidden variables for generating word
d document (index)
wd Word sequence in document d
Nd Number of words in document d
Ld Number of documents cited by document d
Id Set of cited documents for document d
idl Index of lth cited document of document d
?d Distribution on cited documents of document d
?d Topic distribution associated with document d
bdn Decision on way of generating nth word in doc-
ument d
cdn Cited document that generates nth word in doc-
ument d
zdn Topic of nth word in document d
dict contents of documents, the use of HTM seems
more reasonable.
3 Hypertext Topic Model
3.1 Model
In topic modeling, a probability distribution of
words is employed for a given document. Specifi-
cally, the probability distribution is defined as a mix-
ture over latent topics, while each topic is future
characterized by a distribution of words (Hofmann,
1999; Blei et al, 2003). In this paper, we introduce
an extension of LDA model for hypertexts. Table 1
gives the major notations and their explanations.
The graphic representation of conventional LDA
is given in Figure 1(a). The generative process of
LDA has three steps. Specifically, in each document
a topic distribution is sampled from a prior distribu-
tion defined as Dirichlet distribution. Next, a topic is
sampled from the topic distribution of the document,
which is a multinominal distribution. Finally, a word
is sampled according to the word distribution of the
topic, which also forms a multinormal distribution.
The graphic representation of HTM is given in
Figure 1(b). The generative process of HTM is de-
scribed in Algorithm 1. First, a topic distribution
is sampled for each document according to Dirich-
let distribution. Next, for generating a word in a
document, it is decided whether to use the current
Algorithm 1 Generative Process of HTM
for each document d do
Draw ?d ? Dir(??).
end for
for each word wdn do
if Ld > 0 then
Draw bdn ? Ber(?)
Draw cdn ? Uni(?d)
if bdn = 1 then
Draw zdn ? Multi(?d)
else
Draw zdn ? Multi(?Idcdn )end if
else
Draw a topic zdn ? Multi(?d)
end if
Draw a word wdn ? P (wdn | zdn, ?)
end for
document or documents which the document cites.
(The weight between the citing document and cited
documents is controlled by an adjustable hyper-
parameter ?.) It is also determined which cited doc-
ument to use (if it is to use cited documents). Then, a
topic is sampled from the topic distribution of the se-
lected document. Finally, a word is sampled accord-
ing to the word distribution of the topic. HTM natu-
rally mimics the process of writing a hypertext docu-
ment by humans (repeating the processes of writing
native texts and anchor texts).
The formal definition of HTM is given be-
low. Hypertext document d has Nd words
wd = wd1 ? ? ?wdNd and Ld cited documents Id =
{id1, . . . , idLd}. The topic distribution of d is ?d
and topic distributions of the cited documents are
?i, i ? Id. Given ?, ?, and ?, the conditional proba-
bility distribution of wd is defined as:
p(wd|?, ?, ?) =
Nd?
n=1
?
bdn
p(bdn|?)
?
cdn
p(cdn|?d)
?
zdn
p(zdn|?d)bdnp(zdn|?idcdn )
1?bdnp(wdn|zdn, ?).
Here ?d, bdn, cdn, and zdn are hidden vari-
ables. When generating a word wdn, bdn determines
whether it is from the citing document or the cited
documents. cdn determines which cited document it
516
is when bdn = 0. In this paper, for simplicity we as-
sume that the cited documents are equally likely to
be selected, i.e., ?di = 1Ld .Note that ? represents the topic distributions of
all the documents. For any d, its word distribution
is affected by both ?d and ?i, i ? Id. There is a
propagation of topics from the cited documents to
the citing document through the use of ?i, i ? Id.
For a hypertext document d that does not have
cited documents. The conditional probability dis-
tribution degenerates to LDA:
p(wd|?d, ?) =
Nd?
n=1
?
zdn
p(zdn|?d)p(wdn|zdn, ?).
By taking the product of the marginal probabil-
ities of hypertext documents, we obtain the condi-
tional probability of the corpus D given the hyper-
parameters ?, ??, ?,
p(D|?, ??, ?) =
? D?
d=1
p(?d|??)
Nd?
n=1
?
bdn
p(bdn|?)
?
cdn
p(cdn|?d)
?
zdn
p(zdn|?d)bdnp(zdn|?Idcdn )
1?bdn
p(wdn|zdn, ?)d?. (1)
Note that the probability function (1) also covers the
special cases in which documents do not have cited
documents.
In HTM, the content of a document is decided by
the topics of the document as well as the topics of
the documents which the document cites. As a result
contents of documents can be ?propagated? along the
hyperlinks. For example, suppose web page A cites
page B and page B cites page C, then the content of
page A is influenced by that of page B, and the con-
tent of page B is further influenced by the content
of page C. Therefore, HTM is able to more accu-
rately represent the contents of hypertexts, and thus
is more useful for text processing such as topic dis-
covery and document classification.
3.2 Inference and Learning
An exact inference of the posterior probability of
HTM may be intractable, we employ the mean field
variational inference method (Wainwright and Jor-
dan, 2003; Jordan et al, 1999) to conduct approxi-
mation. Let I[?] be an indicator function. We first
define the following factorized variational posterior
distribution q with respect to the corpus:
q =
D?
d=1
q(?d|?d)
Nd?
n=1
(
q(xdn|?dn)(q(cdn|?dn)
)I[Ld>0]
q(zdn|?dn) ,
where ?, ?, ?, and ? denote free variational parame-
ters. Parameter ? is the posterior Dirichlet parameter
corresponding to the representations of documents
in the topic simplex. Parameters ?, ?, and ? cor-
respond to the posterior distributions of their asso-
ciated random variables. We then minimize the KL
divergence between q and the true posterior proba-
bility of the corpus by taking derivatives of the loss
function with respect to variational parameters. The
solution is listed as below.
Let ?iv be p(wvdn = 1|zi = 1) for the word v. If
Ld > 0, we have
E-step:
?di = ??i +
Nd?
n=1
?dn?dni +
D?
d?=1
Ld??
l=1
I [id?l = d]
Nd??
n=1
(1? ?d?n)?d?nl?d?ni .
?dni ? ?iv exp
{?dnEq [log (?di) |?d]
+ (1? ?dn)
Ld?
l=1
?dnlEq [log (?Idli) |?Idl ]
} .
?dn =
(
1 +
(
exp{
k?
i=1
((?dniEq[log(?di)|?d]
?
Ld?
l=1
?dnl?dniEq[log(?Idli)|?Idl ]
)
+ log ?? log (1? ?)}
)?1)?1
.
517
zw?
???
??
T Nd D
(a) LDA
w
??
?? ??
D T
d
z
D
Nd
?
c
?
b
Id
(b) HTM
z
w?
?
??
??
T Nd
D
z
d? ?
Ld
(c) Link-LDA
z
w
?
?
??
??
T
Nd
z
?
Ld
pi
z
wd?
N M
Cited Documents Citing Documents
d
(d) Link-PLSA-LDA
Figure 1: Graphical model representations
?dnl ? ?dl exp{(1? ?dn)
k?
i=1
?dniEq[log(?Idli)|?Idl ]}.
Otherwise,
?di = ??i +
Nd?
n=1
?dni +
D?
d?=1
Ld??
l=1
I [id?l = d]
Nd??
n=1
(1? ?d?n)?d?nl?d?ni .
?dni ? ?iv exp
{Eq [log (?di) |?d]
}.
From the first two equations we can see that the
cited documents and the citing document jointly af-
fect the distribution of the words in the citing docu-
ment.
M-step:
?ij ?
D?
d=1
Nd?
n=1
?dniwjdn.
In order to cope with the data sparseness problem
due to large vocabulary, we employ the same tech-
nique as that in (Blei et al, 2003). To be specific,
we treat ? as a K ?V random matrix, with each row
being independently drawn from a Dirichlet distri-
bution ?i ? Dir(??) . Variational inference is
modified appropriately.
4 Experimental Results
We compared the performances of HTM and three
baseline models: LDA, Link-LDA, and Link-PLSA-
LDA in topic discovery and document classification.
Note that LDA does not consider the use of link in-
formation; we included it here for reference.
4.1 Datasets
We made use of three datasets. The documents in the
datasets were processed by using the Lemur Took
kit (http://www.lemurproject.org), and the low fre-
quency words in the datasets were removed.
The first dataset WebKB (available at
http://www.cs.cmu.edu/?webkb) contains six
subjects (categories). There are 3,921 documents
and 7,359 links. The vocabulary size is 5,019.
518
The second dataset Wikipedia (available at
http://www.mpi-inf.mpg.de/?angelova) contains
four subjects (categories): Biology, Physics, Chem-
istry, and Mathematics. There are 2,970 documents
and 45,818 links. The vocabulary size is 3,287.
The third dataset is ODP composed of homepages
of researchers and their first level outlinked pages
(cited documents). We randomly selected five sub-
jects from the ODP archive. They are Cognitive
Science (CogSci), Theory, NeuralNetwork (NN),
Robotics, and Statistics. There are 3,679 pages and
2,872 links. The vocabulary size is 3,529.
WebKB and Wikipedia are public datasets widely
used in topic model studies. ODP was collected by
us in this work.
4.2 Topic Discovery
We created four topic models HTM, LDA, Link-
LDA, and Link-PLSA-LDA using all the data in
each of the three datasets, and evaluated the top-
ics obtained in the models. We heuristically set the
numbers of topics as 10 for ODP, 12 for WebKB,
and 8 for Wikipedia (i.e., two times of the number
of true subjects). We found that overall HTM can
construct more understandable topics than the other
models. Figure 2 shows the topics related to the
subjects created by the four models from the ODP
dataset. HTM model can more accurately extract
the three topics: Theory, Statistic, and NN than the
other models. Both LDA and Link-LDA had mixed
topics, labeled as ?Mixed? in Figure 2. Link-PLSA-
LDA missed the topic of Statistics. Interestingly, all
the four models split Cognitive Science into two top-
ics (showed as CogSci-1 and CogSci-2), probably
because the topic itself is diverse.
4.3 Document Classification
We applied the four models in the three datasets to
document classification. Specifically, we used the
word distributions of documents created by the mod-
els as feature vectors of the documents and used the
subjects in the datasets as categories. We further
randomly divided each dataset into three parts (train-
ing, validation, and test) and conducted 3-fold cross-
validation experiments. In each trial, we trained an
SVM classifier with the training data, chose param-
eters with the validation data, and conducted evalu-
ation on classification with the test data. For HTM,
Table 2: Classification accuracies in 3-fold cross-
validation.
LDA HTM Link-LDA Link-PLSA-LDA
ODP 0.640 0.698 0.535 0.581
WebKB 0.786 0.795 0.775 0.774
Wikipedia 0.845 0.866 0.853 0.855
Table 3: Sign-test results between HTM and the three
baseline models.
LDA Link-LDA Link-PLSA-LDA
ODP 0.0237 2.15e-05 0.000287
WebKB 0.0235 0.0114 0.00903
Wikipedia 1.79e-05 0.00341 0.00424
we chose the best ? value with the validation set in
each trial. Table 2 shows the classification accura-
cies. We can see that HTM performs better than the
other models in all three datasets.
We conducted sign-tests on all the results of the
datasets. In most cases HTM performs statistically
significantly better than LDA, Link-LDA, and Link-
PLSA-LDA (p-value < 0.05). The test results are
shown in Table 3.
4.4 Discussion
We conducted analysis on the results to see why
HTM can work better. Figure 3 shows an example
homepage from the ODP dataset, where superscripts
denote the indexes of outlinked pages. The home-
page contains several topics, including Theory, Neu-
ral network, Statistics, and others, while the cited
pages contain detailed information about the topics.
Table 4 shows the topics identified by the four mod-
els for the homepage. We can see that HTM can
really more accurately identify topics than the other
models.
The major reason for the better performance by
HTM seems to be that it can fully leverage the infor-
Table 4: Comparison of topics identified by the four mod-
els for the example homepage. Only topics with proba-
bilities > 0.1 and related to the subjects are shown.
Model Topics Probabilities
LDA Mixed 0.537
HTM Theory 0.229
NN 0.278
Statistics 0.241
Link-LDA Statistics 0.281
Link-PLSA-LDA Theory 0.527
CogSci-2 0.175
519
(a) LDA
Mixed NN Robot CogSci-1 CogSci-2
statistic learn robot visual conscious
compute conference project model psychology
algorithm system file experiment language
theory neural software change cognitive
complex network code function experience
mathematics model program response brain
model international data process theory
science compute motor data philosophy
computation ieee read move science
problem proceedings start observe online
random process build perception mind
analysis computation comment effect concept
paper machine post figure physical
method science line temporal problem
journal artificial include sensory content
(b) HTM
Theory Statistics NN Robot CogSci-1 CogSci-2
compute model learn robot conscious memory
science statistic system project visual psychology
algorithm data network software experience language
theory experiment neural motor change science
complex sample conference sensor perception cognitive
computation process model code move brain
mathematics method compute program theory human
paper analysis ieee build online neuroscience
problem response international line physical journal
lecture figure proceedings board concept society
random result machine read problem trauma
journal temporal process power philosophy press
bound probable computation type object learn
graph observe artificial comment content abuse
proceedings test intelligence post view associate
(c) Link-LDA
Statistics Mixed Robot CogSci-1 CogSci-2
statistic compute robot visual conscious
model conference project model psychology
data system software experiment cognitive
analysis learn file change language
method network motor function brain
learn computation robotics vision science
sample proceedings informatik process memory
algorithm neural program perception theory
process ieee build move philosophy
bayesian algorithm board response press
application international sensor temporal online
random science power object neuroscience
distribution complex code observe journal
simulate theory format sensory human
mathematics journal control figure mind
(d) Link-PLSA-LDA
Theory NN Robot CogSci-1 CogSci-2
compute conference robot conscious model
algorithm learn code experience process
computation science project language visual
theory international typeof book data
complex system motor change experiment
science compute control make function
mathematics network system problem learn
network artificial serve brain neural
paper ieee power world system
journal intelligence program read perception
proceedings robot software case represent
random technology file than vision
system proceedings build mind response
problem machine pagetracker theory object
lecture neural robotics content abstract
Figure 2: Topics identified by four models
Radford M.Neal
Professor, Dept. of Statistics and Dept. of Computer Science, University of Toronto
I?m currently highlighting the following :
? A new R function for performing univariate slice sampling.1
? A workshop paper on Computing Likelihood Functions for High-Energy Physics
Experiments when Distributions are Defined by Simulators with Nuisance Parameters.2
? Slides from a talk at the Third Workshop on Monte Carlo Methods on
?Short-Cut MCMC: An Alternative to Adaptation?, May 2007: Postscript, PDF.
Courses I?m teaching in Fall 2008 :
? STA 437: Methods for Multivariate Data3
? STA 3000: Advanced Theory of Statistics4
You can also find information on courses I?ve taught in the past.5
You can also get to information on :
? Research interests6 (with pointers to publications)
? Current and former graduate students7
? Current and former postdocs8
? Curriculum Vitae: PostScript, or PDF.
? Full publications list9
? How to contact me10
? Links to various places11
If you know what you want already,you may wish to go directly to :
? Software available on-line12
? Papers available on-line13
? Slides from talks14
? Miscellaneous other stuff15
Information in this hierarchy was last updated 2008-06-20.
Figure 3: An example homepage: http://www.cs.utoronto.ca/? radford/
520
Table 5: Word assignment in the example homepage.
Word bdn cdn Topic Probability
mcmc 0.544 2 Stat 0.949
experiment 0.546 2 Stat 0.956
neal 0.547 8 NN 0.985
likelihood 0.550 2 Stat 0.905
sample 0.557 2 Stat 0.946
statistic 0.559 2 Stat 0.888
parameter 0.563 2 Stat 0.917
perform 0.565 2 Stat 0.908
carlo 0.568 2 Stat 0.813
monte 0.570 2 Stat 0.802
toronto 0.572 8 NN 0.969
distribution 0.578 2 Stat 0.888
slice 0.581 2 Stat 0.957
energy 0.581 13 NN 0.866
adaptation 0.591 7 Stat 0.541
teach 0.999 11 Other 0.612
current 0.999 11 Other 0.646
curriculum 0.999 11 Other 0.698
want 0.999 11 Other 0.706
highlight 0.999 10 Other 0.786
professor 0.999 11 Other 0.764
academic 0.999 11 Other 0.810
student 0.999 11 Other 0.817
contact 0.999 11 Other 0.887
graduate 0.999 11 Other 0.901
Table 6: Most salient topics in cited pages.
URL Topic Probability
2 Stat 0.690
7 Stat 0.467
8 NN 0.786
13 NN 0.776
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5
ODP
Webkb
Wiki
A
c
c
u
r
a
c
y
?
Figure 4: Classification accuracies on three datasets with
different ? values. The cross marks on the curves cor-
respond to the average values of ? in the 3-fold cross-
validation experiments.
mation from the cited documents. We can see that
the content of the example homepage is diverse and
not very rich. It might be hard for the other base-
line models to identify topics accurately. In con-
trast, HTM can accurately learn topics by the help
of the cited documents. Specifically, if the content of
a document is diverse, then words in the document
are likely to be assigned into wrong topics by the
existing approaches. In contrast, in HTM with prop-
agation of topic distributions from cited documents,
the words of a document can be more accurately as-
signed into topics. Table 5 shows the first 15 words
and the last 10 words for the homepage given by
HTM, in ascending order of bdn, which measures
the degree of influence from the cited documents on
the words (the smaller the stronger). The table also
gives the values of cdn, indicating which cited docu-
ments have the strongest influence. Furthermore, the
topics having the largest posterior probabilities for
the words are also shown. We can see that the words
?experiment?, ?sample?, ?parameter?, ?perform?, and
?energy? are accurately classified. Table 6 gives the
most salient topics of cited documents. It also shows
the probabilities of the topics given by HTM. We can
see that there is a large agreement between the most
salient topics in the cited documents and the topics
which are affected the most in the citing document.
Parameter ? is the only parameter in HTM which
needs to be tuned. We found that the performance of
HTM is not very sensitive to the values of ?, which
reflects the degree of influence from the cited doc-
uments to the citing document. HTM can perform
well with different ? values. Figure 4 shows the clas-
sification accuracies of HTM with respect to differ-
ent ? values for the three datasets. We can see that
HTM works better than the other models in most of
the cases (cf., Table 2).
5 Conclusion
In this paper, we have proposed a novel topic
model for hypertexts called HTM. Existing models
for processing hypertexts were developed based on
the assumption that both words and hyperlinks are
stochastically generated by the model. The gener-
ation of latter type of data is actually unnecessary
for representing contents of hypertexts. In the HTM
model, it is assumed that the hyperlinks of hyper-
521
texts are given and only the words of the hypertexts
are stochastically generated. Furthermore, the word
distribution of a document is determined not only
by the topics of the document in question but also
from the topics of the documents which the doc-
ument cites. It can be regarded as ?propagation?
of topics reversely along hyperlinks in hypertexts,
which can lead to more accurate representations than
the existing models. HTM can naturally mimic hu-
man?s process of creating a document (i.e., by con-
sidering using the topics of the document and at the
same time the topics of the documents it cites). We
also developed methods for learning and inferring
an HTM model within the same framework as LDA
(Latent Dirichlet Allocation). Experimental results
show that the proposed HTM model outperforms
the existing models of LDA, Link-LDA, and Link-
PLSA-LDA on three datasets for topic discovery and
document classification.
As future work, we plan to compare the HTM
model with other existing models, to develop learn-
ing and inference methods for handling extremely
large-scale data sets, and to combine the current
method with a keyphrase extraction method for ex-
tracting keyphrases from web pages.
6 Acknowledgement
We thank Eric Xing for his valuable comments on
this work.
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. ACM Press / Addison-
Wesley.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of machine Learning
Research, 3:993?1022.
David Blei and John Lafferty. 2005. Correlated Topic
Models. In Advances in Neural Information Process-
ing Systems 12.
David Blei and John Lafferty. 2006. Dynamic topic
models. In Proceedings of the 23rd international con-
ference on Machine learning.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling General and Specific As-
pects of Documents with a Probabilistic Topic Model.
In Advances in Neural Information Processing Sys-
tems 19.
David Cohn and Huan Chang. 2000. Learning to Proba-
bilistically Identify Authoritative Documents. In Pro-
ceedings of the 17rd international conference on Ma-
chine learning.
David Cohn and Thomas Hofmann. 2001. The missing
link - a probabilistic model of document content and
hypertext connectivity. In Neural Information Pro-
cessing Systems 13.
Laura Dietz, Steffen Bickel and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In Pro-
ceedings of the 24th international conference on Ma-
chine learning.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. In Proceedings of the National Academy of
Sciences, 101:5220?5227.
Thomas Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. In Proceedings of the National
Academy of Sciences, 101 (suppl. 1) .
Thomas Griffiths, Mark Steyvers, David Blei, and Joshua
Tenenbaum. 2005. Integrating Topics and Syntax. In
Advances in Neural Information Processing Systems,
17.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2008.
Latent Topic Models for Hypertext. In Proceedings of
the 24th Conference on Uncertainty in Artificial Intel-
ligence.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Analysis. In Proceedings of the 15th Conference on
Uncertainty in Artificial Intelligence.
Michael Jordan, Zoubin Ghahramani, Tommy Jaakkola,
and Lawrence Saul. 1999. An Introduction to Varia-
tional Methods for Graphical Models. Machine Learn-
ing, 37(2):183?233.
QiaoZhu Mei, Deng Cai, Duo Zhang, and ChengXiang
Zhai. 2008. Topic Modeling with Network Regular-
ization. In Proceeding of the 17th international con-
ference on World Wide Web.
Thomas Minka and John Lafferty. 2002. Expectation-
Propagation for the Generative Aspect Model. In Pro-
ceedings of the 18th Conference in Uncertainty in Ar-
tificial Intelligence.
Ramesh Nallapati and William Cohen. 2008. Link-
PLSA-LDA: A new unsupervised model for topics and
influence of blogs. In International Conference for
Webblogs and Social Media.
Martin Wainwright, and Michael Jordan. 2003. Graph-
ical models, exponential families, and variational in-
ference. In UC Berkeley, Dept. of Statistics, Technical
Report, 2003.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of the 29th
annual international ACM SIGIR conference on Re-
search and development in information retrieval.
522
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 141?150, Dublin, Ireland, August 23-29 2014.
Co-learning of Word Representations and Morpheme Representations
Siyu Qiu
Nankai University
Tianjin, 300071, China
ppqq2356@gmail.com
Qing Cui
Tsinghua University
Beijing, 100084, China
cuiq12@mails.tsinghua.edu.cn
Jiang Bian
Microsoft Research
Beijing, 100080, China
jibian@microsoft.com
Bin Gao
Microsoft Research
Beijing, 100080, China
bingao@microsoft.com
Tie-Yan Liu
Microsoft Research
Beijing, 100080, China
tyliu@microsoft.com
Abstract
The techniques of using neural networks to learn distributed word representations (i.e., word
embeddings) have been used to solve a variety of natural language processing tasks. The re-
cently proposed methods, such as CBOW and Skip-gram, have demonstrated their effectiveness
in learning word embeddings based on context information such that the obtained word embed-
dings can capture both semantic and syntactic relationships between words. However, it is quite
challenging to produce high-quality word representations for rare or unknown words due to their
insufficient context information. In this paper, we propose to leverage morphological knowledge
to address this problem. Particularly, we introduce the morphological knowledge as both ad-
ditional input representation and auxiliary supervision to the neural network framework. As a
result, beyond word representations, the proposed neural network model will produce morpheme
representations, which can be further employed to infer the representations of rare or unknown
words based on their morphological structure. Experiments on an analogical reasoning task and
several word similarity tasks have demonstrated the effectiveness of our method in producing
high-quality words embeddings compared with the state-of-the-art methods.
1 Introduction
Word representation is a key factor for many natural language processing (NLP) applications. In the
conventional solutions to the NLP tasks, discrete word representations are often adopted, such as the
1-of-v representations, where v is the size of the entire vocabulary and each word in the vocabulary
is represented as a long vector with only one non-zero element. However, using discrete word vectors
cannot indicate any relationships between different words, even though they may yield high semantic
or syntactic correlations. For example, while careful and carefully have quite similar semantics, their
corresponding 1-of-v representations trigger different indexes to be the hot values, and it is not explicit
that careful is much closer to carefully than other words using 1-of-v representations.
To deal with the problem, neural network models have been widely applied to obtain word repre-
sentations. In particular, they usually take the 1-of-v representations as the word input vectors in the
neural networks, and learn new distributed word representations in a low-dimensional continuous em-
bedding space. The principle of these models is that words that are highly correlated in terms of either
semantics or syntactics should be close to each other in the embedding space. Representative works in
this field include feed-forward neural network language model (NNLM) (Bengio et al., 2003), recurrent
neural network language model (RNNLM) (Mikolov et al., 2010), and the recently proposed continues
bag-of-words (CBOW) model and continues skip-gram (Skip-gram) model (Mikolov et al., 2013a).
However, there are still challenges for using neural network models to achieve high-quality word
embeddings. First, it is difficult to obtain word embeddings for emerging words as they are not included
in the vocabulary of the training data. Some previous studies (Mikolov, 2012) used one or more default
indexes to represent all the unknown words, but such solution will lose information for the new words.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
141
Second, the embeddings for rare words are often of low quality due to the insufficient context information
in the training data.
Fortunately, semantically or syntactically similar words often share some common morphemes such
as roots, affixes, and syllables. For example, probably and probability share the same root, i.e., probab,
as well as the same syllables, i.e., pro and ba. Therefore, morphological information can provide valu-
able knowledge to bridge the gap between rare or unknown words and well-known words in learning
word representations. In this paper, we propose a novel neural network architecture that can leverage
morphological knowledge to obtaining high-quality word embeddings. Specifically, we first segment the
words in the training data into morphemes, and then employ the 1-of-v representations of both the words
and their morphemes as the input to the neural network models. In addition, we propose to use mor-
phological information as auxiliary supervision. Particularly, in the output layer of the neural network
architecture, we predict both the words and their corresponding morphemes simultaneously. Moreover,
we introduce extra coefficients into the network to balance the weights between word embeddings and
morpheme embeddings. Therefore, in the back propagation stage, we will update the word embeddings,
the morpheme embeddings, and the balancing coefficients simultaneously.
Our proposed neural network model yields two major advantages: on one hand, it can leverage three
types of co-occurrence information, including co-occurrence between word and word (conventional),
co-occurrence between word and morpheme (newly added), and co-occurrence between morpheme and
morpheme (newly added); on the other hand, this new model allows to learn word embeddings and
morpheme embeddings simultaneously, so that it is convenient to build the representations for unknown
words from morpheme embeddings and enhance the representations for rare words. Experiments on
large-scale public datasets demonstrate that our proposed approach can help produce improved word
representations on an analogical reasoning task and several word similarity tasks compared with the
state-of-the-art methods.
The rest of the paper is organized as follows. We briefly review the related work on word embedding
using neural networks in Section 2. In Section 3, we describe the proposed methods to leverage mor-
phological knowledge in word embedding using neural network models. The experimental results are
reported in Section 4. The paper is concluded in Section 5.
2 Related Work
Neural Language Models (NLMs) (Bengio et al., 2003) have been applied in a number of NLP tasks (Col-
lobert and Weston, 2008) (Glorot et al., 2011) (Mikolov et al., 2013a) (Mikolov et al., 2013b) (Socher
et al., 2011) (Turney, 2013) (Turney and Pantel, 2010) (Weston et al., ) (Deng et al., 2013) (Collobert
et al., 2011) (Mnih and Hinton, 2008) (Turian et al., 2010). In general, they learn distributed word rep-
resentations in a continuous embedding space. For example, Mikolov et al. proposed the continuous
bag-of-words model (CBOW) and the continuous skip-gram model (Skip-gram) (Mikolov et al., 2013a).
Both of them assume that words co-occurring with the same context should be similar. Collobert et
al. (Collobert et al., 2011) fed their neural networks with extra features such as the capital letter feature
and the part-of-speech (POS) feature, but they still met the challenge of producing high-quality word
embeddings for rare words.
Besides using neural network, many different types of models were proposed for estimating continuous
representations of words, such as the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet
Allocation (LDA). However, Mikolov et al. (Mikolov et al., 2013c) have shown that words learned by
neural networks are signicantly better than LSA for preserving linear regularities while LDA becomes
computationally expensive on large datasets.
There were a lot of previous attempts to include morphology in continuous models, especially in
the speech recognition field. Represent works include Letter n-gram (Sperr et al., 2013) and feature-
rich DNN-LMs (Mousa et al., 2013). The first work improves the letter-based word representation by
replacing the 1-of-v word input of restricted Boltzman machine with a vector indicating all n-grams of
order n and smaller that occur in the word. Additional information such as capitalization is added as well.
In the model of feature-rich DNN-LMs, the authors expand the inputs of the network to be a mixture of
142
selected full words and morphemes together with their features such as morphological tags. Both of
these works intend to capture more morphological information so as to better generalize to unknown or
rare words and to lower the out-of-vocabulary rate.
There are some other related works that consider morphological knowledge when learning the word
embeddings, such as factored NLMs (Alexandrescu and Kirchhoff, 2006) and csmRNN (Luong et al.,
2013), both of which are designed to handle rare words. In factored NLMs, each word is viewed as a
vector of shape features (e.g., affixed, capitalization, hyphenation, and classes) and a word is predicted
based on several previous vectors of factors. Although they made use of the co-occurrence of morphemes
and words, the context information is lost after chopping the words and feeding the neural network with
morphemes. In our model, we also utilize the co-occurrence information between morphemes, which has
not been investigated before. In csmRNN, Luong et al proposed a hierarchical model considering the
knowledge of both morphological constitutionality and context. The hierarchical structure looks more
sophisticated, but the relatedness of words with morphological similarity are weaken by layers when
combining morphemes into words. In addition, the noise accumulated in the hierarchical structure in
building a word might be propagated to the context layer. In our model, the morphological and contextual
knowledge are combined in parallel, and their contributions to the input vector are decided by a pair of
learned tradeoff coefficients.
3 The Morpheme powered CBOW Models
In this section, we introduce the architecture of our proposed neural network model based on the CBOW
model. In CBOW (see Figure 1), a sliding window is employed on the train text stream to obtain the train-
ing samples. In each sliding window, the model aims to predict the central word using the surrounding
words as the input. Specifically, the input words are represented in the 1-of-v format. In the feed-forward
process, these input words are first mapped into the embedding space by the same weight matrix M , and
then the embedding vectors are summed up to a combined embedding vector. After that, the combined
embedding vector is mapped back to the 1-of-v space by another weight matrix M
?
, and the resulting
vector is used to predict the central word after conducting softmax on it. In the back-propagation process,
the prediction errors are propagated back to the network to update the two weight matrices. After the
training process converges, the weight matrix M is regarded as the learned word representations.
SUM
?1
???
??1
??
??
??
? ??
?0
Embedding Matrix
1-of-? representation
Embedding Space
(?-dimension)
Vocabulary Space
(?-dimension)
Vocabulary Space
(?-dimension)
Projection Matrix
Figure 1: The CBOW model.
143
In our proposed model, we address the challenge of producing high-quality word embeddings for rare
words and unknown words by leveraging the three types of co-occurrence information between words
and morphemes.
On the input side, we segment the words into morphemes and put both the words and the morphemes
as input. That is, the vocabulary for the 1-of-v representation contains both words and morphemes.
As shown in Figure 2, the surrounding words in the sliding window are w
?s
, ? ? ? , w
?1
, w
1
, ? ? ? , w
s
and
their corresponding morphemes arem
?s,1
,m
?s,2
, ? ? ? ,m
?s,t
?s
; ? ? ? ;m
?1,1
,m
?1,2
, ? ? ? ,m
?1,t
?1
;m
1,1
,
m
1,2
, ? ? ? , m
1,t
1
; ? ? ? ;m
s,1
, m
s,2
, ? ? ? , m
s,t
s
, where 2s is the number of the surrounding words and t
i
is
the number of morphemes for w
i
(i = ?s, ? ? ? ,?1, 1, ? ? ? , s). Note that t
i
depends on the formation of
w
i
so that it may vary from word to word. If a word is also a morpheme, there will be two embedding
vectors which are tagged differently. We use v
w
i
and v
m
i,j
to represent the 1-of-v vectors of word w
i
and
morpheme m
i,j
respectively. On the input side, both the words and their morphemes are mapped into
the embedding space by the same weight matrix M , and then the weighted sum v
I
of the combination of
word embeddings and the combination of morpheme embeddings is calculate as below,
v
I
= ?
w
?
s
?
i=?s
i 6=0
v
w
i
+ ?
m
?
s
?
i=?s
i 6=0
t
i
?
j=1
v
m
i,j
,
where ?
w
and ?
m
are the tradeoff coefficients between the combination of word embeddings and the
combination of morpheme embeddings.
On the output side, we map the combined embedding vector v
I
back to the 1-of-v space by another
weight matrix M
?
to do the prediction. We have four settings of the structure. In the first setting, we only
predict the central wordw
0
, and we name the model under this setting as MorphemeCBOW. In the second
setting, we predict both the central word w
0
and its morphemes m
0,1
,m
0,2
, ? ? ? ,m
0,t
0
, and we name this
setting as MorphemeCBOW+. In the above two settings, the tradeoff weights ?
w
and ?
m
are fixed. If
we update the two weights in the learning process of MorphemeCBOW, we will get the third setting and
we name it as MorphemeCBOW*, while updating the two weights in MorphemeCBOW+ yields the forth
setting named MorphemeCBOW++ .
Take MorphemeCBOW+ as example, the objective is to maximize the following conditional co-
occurrence probability,
log(P (w
0
| {w
i
}, {m
i,j
})) + log(
t
0
?
j=1
P (m
0,j
| {w
i
}, {m
i,j
})), (1)
where {w
i
}, {m
i,j
} represent the bag of words and bag of morphemes separately. The conditional prob-
ability in the above formula is defined using the softmax function,
P (w
0
| {w
i
}, {m
i,j
}) =
exp(v
?T
w
0
? v
I
)
?
v
?
?V
O
exp(v
?T
? v
I
)
, P (m
0,j
| {w
i
}, {m
i,j
}) =
exp(v
?T
m
0,j
? v
I
)
?
v
?
?V
O
exp(v
?T
? v
I
)
, (2)
where V
O
is the set of the output representations for the whole vocabulary; v
?
is used to differentiate with
input representations; and v
?
w
0
, v
?
m
0,j
represent the output embedding vectors ofw
0
andm
0,j
respectively.
Usually, the computation cost for Formula (2) is expensive since it is proportional to the vocabulary
size. In our model, we use negative sampling discussed in (Mikolov et al., 2013b) to speed up the
computation. Particularly, we random select k negative samples u
1
, u
2
, ? ? ? , u
k
for each prediction target
(word or morpheme). By using this technique, Formula (1) can be equally written as,
G(v
I
) ? log ?(v
?T
w
0
? v
I
) +
t
0
?
j=1
log ?(v
?T
m
0,j
? v
I
) +
k
?
i=1
u
i
6=w
0
u
i
6=?m
0,j
E
u
i
?P
n
(u)
[log ?(?v
?T
u
i
? v
I
)],
144
where ? denotes the logistic function, and P
n
(u) is the vocabulary distribution used to select the negative
samples. P
n
(u) is set as the 3/4rd power of the unigram distribution U(u)
1
. The negative samples should
not be the same as any of the prediction targetsw
0
andm
0,j
(j = 1, ? ? ? , t
0
). By using negative sampling,
the training time spent on summing up the whole vocabulary in Formula (2) is greatly reduced so that it
becomes linear with the number of the negative samples. Thus, we can calculate the gradient of G(v
I
)
as below,
?G(v
I
)
?v
I
=(1? ?(v
?T
w
0
? v
I
)) ?
?(v
?T
w
0
? v
I
)
?v
I
+
t
0
?
j=1
(1? ?(v
?T
m
0,j
? v
I
)) ?
?(v
?T
m
0,j
? v
I
)
?v
I
?
k
?
i=1
u
i
6=w
0
u
i
6=?m
0,j
[?(v
?T
u
i
? v
I
) ?
?(v
?T
u
i
? v
I
)
?v
I
].
In the back-propagation process, the weights in the matricesM andM
?
are updated. When the training
process converges, we take the matrix M as the learned word embeddings and morpheme embeddings.
????
?? ???
??????????
? ???
??
????????????????????
?????
?1
??????????
???
??1
??
??
??
??
?? ????1??? ????
?0 0??1??? 0???
Projection Matrix
Embedding Matrix
Embedding Space
(?-dimension)
Vocabulary Space
(?-dimension) 
1-of-? representation
Word + Morphemes
Vocabulary Space
(?-dimension)
Bag of Morphemes
Bag of Words
Figure 2: The proposed neural network model.
4 Experimental Evaluation
In this section we test the effectiveness of our model in generating high-quality word embeddings. We
first introduce the experimental settings, and then we report the results on one analogical reasoning task
and several word similarity tasks.
4.1 Datasets
We used two datasets for training: enwiki9
2
and wiki2010
3
.
1
http://www.cs.bgu.ac.il/
?
yoavg/publications/negative-sampling.pdf
2
http://mattmahoney.ent/dc/enwik9.zip
3
http://www.psych.ualberta.ca/
?
westburylab/downloads/westburylab.wikicorp.
download.html
145
? The enwiki9 dataset contains about 123.4 million words. We used Matt Mahoney?s text pre-
processing script
4
to process the corpus. Thus, we removed all non-Roman characters and mapped
all digits to English words. In addition, words occurred less than 5 times in the training corpus were
discarded. We used the learned word embeddings from enwiki9 to test an analogical reasoning task
described in (Mikolov et al., 2013a).
? The wiki2010 dataset contains about 990 million words. The learned embeddings from this dataset
were used on word similarity tasks as it was convenient to compare with the csmRNN model (Luong
et al., 2013). We did the same data pre-processing as csmRNN did. That is, we removed all non-
Roman characters and mapped all digits to zero.
4.2 Settings
In the analogical reasoning task, we used the CBOW model as the baseline. In both CBOW and our
proposed model, we set the context window size to be 5, and generated three dimension sizes (100, 200,
and 300) of word embeddings. We used negative sampling (Mikolov et al., 2013b) in the output layer
and the number of negative samples is chosen as 3.
In the word similarity tasks, we used the csmRNN model as the baseline. The context window size of
our model was set to be 5. To make a fair comparison with the csmRNN model, we conducted the same
settings in our experiments as csmRNN. First, as csmRNN used the Morfessor (Creutz and Lagus, 2007)
method to segment words into morphemes, we also used Morfessor as one of our word segmentation
methods to avoid the influence caused by the segmentation methods. Second, as csmRNN used two
existing embeddings C&W
5
(Collobert et al., 2011) and HSMN
6
(Huang et al., 2012) to initialize the
training process, we also used the two embeddings as the initial weights of M in our experiments. Third,
we set the dimension of the embedding space to 50 as csmRNN did.
In our model, we employed three methods to segment a word into morphemes. The first method is
called Morfessor, which is a public tool implemented based on the minimum descriptions length algo-
rithm (Creutz and Lagus, 2007). The second method is called Root, which segments a word into roots
and affixes according to a predefined list in Longman Dictionaries. The third method is called Syllable,
which is implemented based on the hyphenation tool proposed by Liang (Liang, 1983). Besides, the ar-
chitecture of the proposed model can be specified into four types: MorphemeCBOW, MorphemeCBOW*,
MorphemeCBOW+, and MorphemeCBOW++. For the model MorphemeCBOW and MorphemeCBOW+
with fixed tradeoff coefficients, we set the weights ?
w
and ?
m
to be 0.8 and 0.2 respectively; while for
the other two models with updated tradeoff weights, the weights ?
w
and ?
m
are initialized as 1. These
weight settings are chosen empirically.
4.3 Evaluation Tasks
4.3.1 Analogical reasoning task
The analogical reasoning task was introduced by Mikolov et al (Mikolov et al., 2013a). All the questions
are in the form ?a is to b is as c is to ??, denoted as a : b? c : ?. The task consists of 19,544 questions
involving semantic analogies (e.g., England: London ? China: Beijing) and syntactic analogies (e.g.,
amazing: amazingly? unfortunate: unfortunately). Suppose that the corresponding vectors are
??
a ,
??
b ,
and
??
c , we will answer the question by finding the word with the representation having the maximum
cosine similarity to vector
??
b ?
??
a +
??
c , i.e,
max
x?V,x 6=b,x 6=c
(
??
b ?
??
a +
??
c )
T
??
x
where V is the vocabulary. Only when the computed word is exactly the answer word in evaluation set
can the question be regarded as answered correctly.
4
http://mattmahoney.net/dc/textdata.html
5
http://ronan.collobert.com/senna/
6
http://ai.stanford.edu/
?
ehhuang/
146
4.3.2 Word similarity task
The word similarity task was tested on five evaluation sets: WS353 (Finkelstein et al., 2002),
SCWS* (Huang et al., 2012), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965)
and RW (Luong et al., 2013), which contain 353, 1,762, 30, 65 and 2,034 pairs of words respectively.
Table 1 shows some statistics about the datasets. Furthermore, the words in WS353, MC, RG are mostly
frequent words, while SCWS* and RW have much more rare words and unknown words (i.e., unseen
words in the training corpus) than the first three sets. The word distributions of these datasets are shown
in Figure 3, from which we can see that RW contains the largest number of rare and unknown words.
For the unknown words, we segmented them into morphemes, and calculated their word embeddings by
summing up their corresponding morpheme embeddings. Each word pair in these datasets is associated
with several human judgments on similarity and relatedness on a scale from 0 to 10 or 0 to 4. For ex-
ample, (cup, drink) received an average score of 7.25, while (cup, substance) received an average score
of 1.92. To evaluate the quality of the learned word embeddings, we computed Spearman?s ? correlation
between the similarity scores calculated on the learned word embeddings and the human judgments.
Figure 3: Word distribution by frequency. Distinct words in each test dataset are grouped according
to frequencies. The figure shows the percentage of words in each bin.
Table 1: Statistics on the word similarity evaluation sets.
Dataset Number of pairs Number of words Percentage of multi-segments words by Morfessor
WS353 353 437 28.15%
SCWS* 1726 1703 34.00%
RW 2034 2951 69.06%
4.4 Experimental Results
4.4.1 Results on analogical reasoning task
The experimental results on the analogical reasoning task are shown in Table 2, including semantic
accuracy, syntactic accuracy, and total accuracy of all competition settings. Semantic/syntactic accuracy
refers to the number of correct answers over the total number of all semantic/syntactic questions. From
the results, we have the following observations:
? In MorphemeCBOW, we used the surrounding words and their morphemes to predict the central
word. The total accuracies are all improved compared with baseline using the three word segmen-
tation methods across three different dimensions of the embedding space. Generally, the improve-
ments on semantic accuracies are less than those on syntactic accuracies. The reason is that the
morphological information favors more for the syntactic tasks than the semantic tasks. Further-
147
more, the Root method achieved the best among the three segmentation methods, showing that the
roots and affixes from the dictionary can help produce a high-quality morpheme segmentation tool.
? In MorphemeCBOW*, we predicted the central word, and updated the tradeoff coefficients in
the learning process. We can see that the results are comparable or slightly better than Morphe-
meCBOW using the three word segmentation methods across three different dimensions of the
embedding space, showing that updating the tradeoff coefficients may further boost the model per-
formance under some specific settings.
? In MorphemeCBOW+, we predicted both the central word and its morphemes. MorphemeCBOW+
can provide slightly better results compared with MorphemeCBOW and MorphemeCBOW*, indi-
cating that putting morphemes (especially roots) in the output layer can do extra help in generating
high-quality word embeddings.
? In MorphemeCBOW++, we predicted the central word and its morphemes, and updated the trade-
off coefficients in the learning process. The performance under all of the three word segmentation
methods got further improved compared with MorphemeCBOW+. It tells that the contributions
from words and morphemes are different to the analogical reasoning task. According to our obser-
vations, the weight for words is usually higher than that for morphemes.
? By comparing MorphemeCBOW with MorphemeCBOW* as well as MorphemeCBOW+ with Mor-
phemeCBOW++, we can observe that updating the weights of tradeoff coefficients seem to essen-
tially boost syntactic accuracy by trading off a bit of semantic accuracy. As introduced in Section
4.2, in the fixed weight model the ratio of weight of morphemes to the weight of word is 0.25; while
our experiment records show that the averaged ratio are 0.43 if the two weights are updated, mean-
ing that the weight of the combination of morphemes increases and the contribution of the original
word to the final combined embedding decreased. As a result, the syntactic accuracy which largely
reflected in the morphological structure of a word increased, but the semantic accuracy hurts a little.
4.4.2 Results on word similarity task
Experimental results on the word similarity tasks are shown in Table 3
7
,where the labels of C&W + csm-
RNN and HSMN + csmRNN mean that using C&W and HSMN to initialize csmRNN model as what had
been introduced in the paper of Luong et al. In our experiments, the architecture of MorphemeCBOW*
performs the best, so we only show the results related to MorphemeCBOW* in the table. We have the
following observations from the results:
? On WS353, MC, RG, and SCWS*, MorphemeCBOW* performs consistently better than the csm-
RNN model, showing that our model can achieve better representations for common words.
7
csmRNN embeddings are available on http://www-nlp.stanford.edu/
?
lmthang/morphoNLM/, Perfor-
mances are tested based on the two embeddings.
Table 2: Performance of leveraging morphological information on the analogical reasoning task.
(a) Baseline
Dimension (%) CBOW
100 Total 26.49
Semantic 17.51
Syntactic 33.96
200 Total 30.50
Semantic 19.71
Syntactic 39.46
300 Total 29.04
Semantic 17.58
Syntactic 38.56
(b) MorphemeCBOW
Morfessor Syllable Root
31.99 31.28 32.49
19.44 18.76 21.77
42.42 41.68 41.40
34.04 34.71 36.29
19.10 19.13 22.45
46.45 47.65 47.79
31.27 32.45 36.12
15.45 15.63 20.79
44.41 46.44 48.86
(c) MorphemeCBOW*
Morfessor Syllable Root
33.07 31.16 34.04
15.20 15.68 17.87
47.92 44.02 47.48
34.69 33.13 36.50
11.53 15.91 18.92
53.92 47.44 51.10
31.21 32.16 35.63
8.85 12.54 15.75
49.79 48.47 52.14
(d) MorphemeCBOW+
Morfessor Syllable Root
33.26 31.12 32.77
22.82 20.80 22.79
41.93 39.70 41.07
38.28 39.32 39.53
25.94 27.99 28.29
48.52 48.74 48.86
38.01 39.56 39.70
25.11 26.94 27.80
48.72 50.05 49.58
(e) MorphemeCBOW++
Morfessor Syllable Root
38.86 34.42 35.78
21.12 22.58 22.43
53.59 44.26 46.87
40.32 41.79 43.29
24.20 24.05 25.04
53.72 56.53 58.45
37.65 41.64 41.96
13.97 26.64 25.82
57.32 54.10 55.36
148
Table 3: Performance of leveraging morphological information on the word similarity task.
Model WS353 (%) SCWS* (%) MC(%) RG(%) RW(%)
C&W 49.73 48.45 57.33 48.22 21.93
C&W + csmRNN 58.27 49.09 60.22 58.92 31.77
C&W + MorphemeCBOW* 63.81 53.30 74.33 61.22 31.14
HSMN 62.58 32.09 66.18 64.51 1.97
HSMN + csmRNN 64.58 44.08 71.88 65.15 22.31
HSMN + MorphemeCBOW* 65.19 53.40 81.62 67.41 32.13
MorphemeCBOW* 63.45 53.40 77.40 63.78 32.88
? On RW, MorphemeCBOW* performs better than the csmRNN model when using the HSMN em-
beddings as the initialization. When using the C&W embeddings as the initialization, the perfor-
mance of MorphemeCBOW* is also comparable with that of csmRNN. In particular, if we do not
use any pre-trained embeddings to initialize our mode, it performed the best (32.88%), and it even
beats the best performance of csmRNN with initializations (31.77%)
8
. The initialization is very im-
portant to a neural network. Suitable initialization will help increase the embedding quality which
works like training with multi-epochs. However, as there are two matrix M and M
?
in our network
structure, the initialization of both of them are more sensible. Furthermore, considering that the
recursive structure of csmRNN will bring higher computation complexity, we can conclude that our
model has excellent ability in learning the embeddings of rare words from pure scratch.
? The improvement on RW is more significant than those on the other four datasets. Considering that
RW contains more rare and unknown words (See Figure 3), we verified our idea that leveraging
morphological information will especially benefit the embedding of low-frequency words. More
specifically, without sufficient context information for the rare words in the training data, building
connections between words using morphemes will provide additional evidence for the model to
generate effective embeddings for these rare words; and, by combining the high-quality morpheme
embeddings to obtain the representations of the unknown words, the model does a good job in
dealing with the new emerging words.
5 Conclusions and Future Work
We proposed a novel neural network model to learn word representations from text. The model can lever-
age several types of morphological information to produce high-quality word embeddings, especially for
rare words and unknown words. Empirical experiments on an analogical reasoning task and several word
similarity tasks have shown that the proposed model can generate better word representations compared
with several state-of-the-art approaches.
For the future work, we plan to separate words and morphemes into several buckets according to their
frequencies. Different buckets will be associated with different coefficients, so that we can tune the
coefficients to approach even better word embeddings. We also plan to run our model on more training
corpus to obtain the embedding vectors for rare words, especially those new words invented out recently.
These emerging new words usually do not exist in standard training corpus such as Wikipedia, but exists
in some noisy data such as news articles and web pages. How well our model performs on these new
training corpus is an interesting question to explore.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2006. Factored neural language models. In Proceedings of the Human
Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 1?4, New York City,
USA, June. Association for Computational Linguistics.
8
34.36% in the paper of Luong et al; 32.06% in their project website, see note7
149
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. J. Mach. Learn. Res., 3:1137?1155, March.
R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks
with multitask learning. In ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing
(almost) from scratch. JMLR, 12.
M. Creutz and K. Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning.
TSLP.
L. Deng, X. He, and J. Gao. 2013. Deep stacking networks for information retrieval. In ICASSP, pages 3153?
3157.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search
in context: The concept revisited. In ACM Transactions on Information Systems.
X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep
learning approach. In ICML.
Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Prototypes. In Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
F. M. Liang. 1983. Word hy-phen-a-tion by com-put-er. Technical report.
M.-T. Luong, R. Socher, and C. D. Manning. 2013. Better word representations with recursive neural networks
for morphology. CoNLL-2013, 104.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan Cernock?y, and Sanjeev Khudanpur. 2010. Recurrent neural
network based language model. In INTERSPEECH, pages 1045?1048.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a. Efficient estimation of word representations in vector space.
ICLR ?13.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013b. Distributed representations of words and
phrases and their compositionality. In NIPS, pages 3111?3119.
T. Mikolov, W.-T. Yih, and G. Zweig. 2013c. Linguistic regularities in continuous space word representations. In
In NAACL-HLT, pages 746?751.
T. Mikolov. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Brno University of
Technology.
G.A. Miller and W.G. Charles. 1991. Contextual correlates of semantic similarity. 6(1):1?28.
A. Mnih and G. E. Hinton. 2008. A scalable hierarchical distributed language model. In NIPS, pages 1081?1088.
Amr El-Desoky Mousa, Hong-Kwang Jeff Kuo, Lidia Mangu, and Hagen Soltau. 2013. Morpheme-based feature-
rich language models using deep neural networks for lvcsr of egyptian arabic. In ICASSP, pages 8435?8439.
Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning. 2011. Parsing natural scenes and natural language with
recursive neural networks. In ICML.
Henning Sperr, Jan Niehues, and Alex Waibel. 2013. Letter n-gram-based input encoding for continuous space
language models. In Proceedings of the Workshop on Continuous Vector Space Models and their Composition-
ality, pages 30?39, Sofia, Bulgaria, August. Association for Computational Linguistics.
J. P. Turian, L.-A. Ratinov, and Y. Bengio. 2010. Word representations: A simple and general method for semi-
supervised learning. In ACL, pages 384?394.
P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of
Artificial Intelligence Research, 37:141?188.
P. D. Turney. 2013. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.
TACL, pages 353?366.
J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large vocabulary image annotation. In IJCAI.
150
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 151?160, Dublin, Ireland, August 23-29 2014.
A Probabilistic Model for Learning Multi-Prototype Word Embeddings
Fei Tian
?
, Hanjun Dai
?
, Jiang Bian
?
, Bin Gao
?
,
Rui Zhang
?
, Enhong Chen
?
, Tie-Yan Liu
?
?
University of Science and Technology of China, Hefei, P.R.China
?
Fudan University, Shanghai, P.R.China
?
Microsoft Research, Building 2, No. 5 Danling Street, Beijing, P.R.China
?
Sun Yat-Sen University, Guangzhou, P.R.China
?
tianfei@mail.ustc.edu.cn,
?
cheneh@ustc.edu.cn,
?
daihanjun@gmail.com,
?
{jibian, bingao, tyliu}@microsoft.com,
?
rayz0620@hotmail.com
Abstract
Distributed word representations have been widely used and proven to be useful in quite a few
natural language processing and text mining tasks. Most of existing word embedding models aim
at generating only one embedding vector for each individual word, which, however, limits their
effectiveness because huge amounts of words are polysemous (such as bank and star). To address
this problem, it is necessary to build multi embedding vectors to represent different meanings of
a word respectively. Some recent studies attempted to train multi-prototype word embeddings
through clustering context window features of the word. However, due to a large number of
parameters to train, these methods yield limited scalability and are inefficient to be trained with
big data. In this paper, we introduce a much more efficient method for learning multi embedding
vectors for polysemous words. In particular, we first propose to model word polysemy from a
probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model.
Under this framework, we design an Expectation-Maximization algorithm to learn the word?s
multi embedding vectors. With much less parameters to train, our model can achieve comparable
or even better results on word-similarity tasks compared with conventional methods.
1 Introduction
Distributed word representations usually refer to low dimensional and dense real value vectors (a.k.a.
word embeddings) to represent words, which are assumed to convey semantic information contained in
words. With the exploding text data on the Web and fast development of deep neural network technolo-
gies, distributed word embeddings have been effectively trained and widely used in a lot of text mining
tasks (Bengio et al., 2003) (Morin and Bengio, 2005) (Mnih and Hinton, 2007) (Collobert et al., 2011)
(Mikolov et al., 2010) (Mikolov et al., 2013b).
While word embedding plays an increasingly important role in many tasks, most of word embedding
models, which assume one embedding vector for each individual word, suffer from a critical limitation
for modeling tremendous polysemous words (e.g. bank, left, doctor). Using the same embedding vec-
tor to represent the different meanings (we will call prototype of a word in the rest of the paper) of a
polysemous word is somehow unreasonable and sometimes it even hurts the model?s expression ability.
To address this problem, some recent efforts, such as (Reisinger and Mooney, 2010) (Huang et al.,
2012), have investigated how to obtain multi embedding vectors for the respective different prototypes
of a polysemous word. Specifically, these works usually take a two-step approach: they first train single
prototype word representations through a multi-layer neural network with the assumption that one word
only yields single word embedding; then, they identify multi word embeddings for each polysemous
word by clustering all its context window features, which are usually computed as the average of single
prototype embeddings of its neighboring words in the context window.
Compared with traditional single prototype model, these models have demonstrated significant im-
provements in many semantic natural language processing (NLP) tasks. However, they suffer from a
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
151
crucial restriction in terms of scalability when facing exploding training text corpus, mainly due to the
deep layers and huge amounts of parameters in the neural networks in these models. Moreover, the
performance of these multi-prototype models is quite sensitive to the clustering algorithm and requires
much effort in clustering implementation and parameter tuning. The lack of probabilistic explanation
also refrains clustering based methods from being applied to many text mining tasks, such as language
modeling.
To address these challenges, in this work, we propose a new probabilistic multi-prototype model and
integrate it into a highly efficient continuous Skip-Gram model, which was recently introduced in the
well-known Word2Vec toolkit (Mikolov et al., 2013b). Compared with conventional neural network
language models which usually set up a multi-layer neural network, Word2Vec merely leverages a three-
layer neural network to learn word embeddings, resulting in greatly decreased number of parameters and
largely increased scalability. However, similar to most of existing word embedding models, Word2Vec
also assumes one embedding for one word. We break this limitation by introducing a new probabilistic
framework which employs hidden variables to indicate which prototype each word belongs to in the con-
text. In this framework, the conditional probability of observing word w
O
conditioned on the presence
of neighboring word w
I
(i.e. P(w
O
|w
I
)) can be formulated as a mixture model, where mixtures corre-
sponds to w
I
?s different prototypes. This is a more natural way to define P(w
O
|w
I
), since it has taken the
polysemy of word w
I
into consideration. After defining the model, we design an efficient Expectation-
Maximization (EM) algorithm to learn various word embedding vectors corresponding to each of w
I
?s
prototypes. Evaluations on widely used word similarity tasks demonstrate that our algorithm produces
comparable or even better word embeddings compared with either clustering-based multi-prototype mod-
els or the original Skip-Gram model. Furthermore, as a unified way to obtain multi word embeddings,
our proposed method can effectively avoid the sensitivity to the clustering algorithm applied by previous
multi-prototype word embedding approach.
The following of the paper is organized as follows: we introduce related work in Section 2. Then,
Section 3 describes our new model and algorithm in details and conducts a comparison in terms of
complexity between our algorithm and the previous method. We present our experimental results in
Section 4. The paper is concluded in Section 5.
2 Related Work
Since the initial work (Bengio et al., 2003), there have been quite a lot of neural network based models
to obtain distributed word representations (Morin and Bengio, 2005) (Mnih and Hinton, 2007) (Mikolov
et al., 2010) (Collobert et al., 2011) (Mikolov et al., 2013b). Most of these models assume that one
word has only one embedding, except the work of Eric Huang (Huang et al., 2012), in which the authors
propose to leverage global context information and multi-prototype embeddings to achieve performance
gains in word similarity task. To obtain multi-prototype word embeddings, this work conducts clustering
on a word?s all context words? features in the corpus. The features are the embedding vectors trained
previously via a three-layer neural network. Each cluster?s centroid is regarded as the embedding vector
for each prototype. Their reported experimental results verify the importance of considering multi-
prototype models.
Note that (Reisinger and Mooney, 2010) also proposes to deal with the word polysemy problem by
assigning to each prototype a real value vector. However their embedding vectors are obtained through
a tf-idf counting model, which is usually called as distributional representations (Turian et al., 2010),
rather than through a neural network. Therefore, we do not regard their paper as very related to our
work. The similar statement holds for other works on vector model for word meaning in context such as
(Erk and Pad?o, 2008) (Thater et al., 2011) (Reddy et al., 2011) (Van de Cruys et al., 2011).
Our model is mainly based on the recent proposed Word2Vec model, more concretely, the continuous
Skip-Gram model (Mikolov et al., 2013a) (Mikolov et al., 2013b). The continuous Skip-Gram model
specifies the probability of observing the context words conditioned on the central word w
I
in the win-
dow via a three-layer neural network. With less parameters to train (thus higher scalability), Word2Vec
discovers interesting analogical semantic relations between words like Japan - Tokyo = France - Paris.
152
3 Model Description
In this section, we introduce our algorithm for learning multi-prototype embeddings in details. In partic-
ular, since our new model is based on the continuous Skip-Gram model, we first make a brief introduction
to the Skip-Gram model. Then, we present our new multi-prototype algorithm and how we integrate it
into the Skip-Gram model. After that, we propose an EM algorithm to conduct the training process. We
also conduct a comparison on the number of parameters between the new EM algorithm and the state-
of-the-art multi-prototype model proposed in (Huang et al., 2012), which can illustrate the efficiency
superior of our algorithm.
3.1 Multi-Prototype Skip-Gram Model
In contrast to the conventional ways of using context words to predict the next word or the central
word, the Skip-Gram model (Mikolov et al., 2013b) aims to leverage the central word to predict its
context words. Specifically, assuming that the central word is w
I
and one of its neighboring word is w
O
,
P(w
O
|w
I
) is modeled in the following way:
P(w
O
|w
I
) =
exp(V
T
w
I
U
w
O
)
?
w?W
exp(V
T
w
I
U
w
)
, (1)
where W denotes the dictionary consisting of all words, U
w
?R
d
and V
w
?R
d
represent the d-dimensional
?output? and ?input? embedding vectors of word w, respectively. Note that all the parameters to be learned
are the input and output embedding vectors of all words, i.e. U = {U
w
|w ?W} and V = {V
w
|w ?W}.
This corresponds to a three-layer neural network, in which U and V denote the two parameter matrices of
the neural network. Compared with the conventional neural networks employed in the literature which
yield at least four layers (including the look-up table layer), the Skip-Gram model greatly reduces the
number of parameters and thus gives rise to a significant improvement in terms of training efficiency.
Our proposed Multi-Prototype Skip-Gram model is similar to the original Skip-Gram model in that it
also aims to model P(w
O
|w
I
) and uses two matrices (the input and output embedding matrices) as the
parameters. The difference lies in that given word w
I
, the occurrence of word w
O
is described as a finite
mixture model, in which each mixture corresponds to a prototype of word w
I
. To be specific, suppose
that word w has N
w
prototypes and it appears in its h
w
-th prototype, i.e., h
w
? {1, ? ? ? ,N
w
} is the index of
w?s prototype. Then P(w
O
|w
I
) is expanded as:
p(w
O
|w
I
) =
N
w
I
?
i=1
P(w
O
|h
w
I
= i,w
I
)P(h
w
I
= i|w
I
) (2)
=
N
w
I
?
i=1
exp(U
T
w
O
V
w
I
,i
)
?
w?W
exp(U
T
w
V
w
I
,i
)
P(h
w
I
= i|w
I
), (3)
where V
w
I
,i
? R
d
refers to the embedding vector of w
I
?s i-th prototype. This equation states that P(w
O
|w
I
)
is a weighted average of the probabilities of observing w
O
conditioned on the appearance of w
I
?s every
prototype. The probability P(w
O
|h
w
I
= i,w
I
) takes the similar softmax form to equation (1) and the
weight is specified as a prior probability of word w
I
falls in its every prototype.
The general idea behind the Multi-Prototype Skip-Gram model is very intuitive: the surrounding words
under different prototypes of the same word are usually different. For example, when the word bank
refers to the side of a river, it is very possible to observe the corresponding context words such as
river, water, and slope; however, when bank falls into the meaning of the financial organization, the
surrounding word set is likely to be comprised of quite different words, such as money, account, and
investment.
The probability formulation in (3) brings much computation cost because of the linear dependency of
|W | in the denominator
?
w?W
exp(U
T
w
V
w
I
,i
). To address this issue, several efficient methods have been
proposed such as Hierarchical Softmax Tree (Morin and Bengio, 2005) (Mnih and Kavukcuoglu, 2013)
and Negative Sampling (Mnih and Kavukcuoglu, 2013) (Mikolov et al., 2013b). Taking Hierarchical
153
Softmax Tree as an example, through a binary tree in which every word is a leaf node, word w
O
is
associated with a binary vector b
(w
O
)
? {?1,+1}
L
w
O
specifying a path from the root of the tree to leaf
w
O
, where L
w
O
is the length of vector b
(w
O
)
. Then the conditional probability is described as
P(w
O
|h
w
I
= i,w
I
) =
L
w
O
?
t=1
P
(
b
(w
O
)
t
|w
I
,h
w
I
= i
)
=
L
w
O
?
t=1
?
(
b
(w
O
)
t
U
T
w
O
,t
V
w
I
,i
)
, (4)
where ?(x) = 1/(1+ exp(?x)) is the sigmoid function, and U
w
O
,t
specifies the d-dimensional parameter
vector associated with the t-th node in the path from the root to the leaf node w
O
. Substituting (4) into
(2) to replace the large softmax operator in (3) leads to a much more efficient probability form.
3.2 EM Algorithm
In this section, we describe the EM algorithm adopted to train the Multi-Prototype Skip-Gram model.
Without loss of generality, we will focus on obtaining multi embeddings for a specified word w ?W
with N
w
prototypes. Word w?s embedding vectors are denoted as V
w
? R
d?N
w
. Suppose there are M
word pairs for training: {(w
1
,w),(w
2
,w), ? ? ? ,(w
M
,w)}, where all the inputs words (i.e., word w) are the
same, and the set of output words to be predicted are denoted as X= {w
1
,w
2
, ? ? ? ,w
M
}. That is, X are M
surrounding words of w in the training corpus.
For ease of reference and without loss of generality, we make some changes to the notations in Section
3.1. We will use h
m
as the index of w?s prototype in the pair (w
m
,w), m ? {1,2, ? ? ? ,M}. Besides,
some new notations are introduced: P(h
w
= i|w
I
) is simplified as pi
i
, and ?
m,k
, where m ? {1,2, ? ? ? ,M},
k ? {1,2, ? ? ?N
w
}, are the hidden binary variables indicating whether the m-th presence of word w is in
its k-th prototype, i.e. ?
m,k
= 1
h
m
=k
, where 1 is the indicator function. Other notations are the same as
before: V
w,i
? R
d
is the embedding vector for word w?s i-th prototype, U
w,t
? R
d
is the embedding vector
for the t-th node on the path from the tree root to the leaf node representing word w, and b
(w)
t
? {?1,1}
is the t-th bit of the binary coding vector of word w along its corresponding path on the Hierarchical
Softmax Tree.
Then the parameter set we aim to learn is ? = {pi
1
, ? ? ? ,pi
N
w
;U ;V
w
}. The hidden variable set is ? =
{?
m,k
|m ? (1,2, ? ? ? ,M),k ? (1,2, ? ? ? ,N
w
)}. Considering equation (2) and (4), we have the log likelihood
of X as below:
logP(X,?|?) =
M
?
m=1
N
w
?
k=1
?
m,k
(
logpi
k
+ logP(w
m
|h
m
= k,w)
)
=
M
?
m=1
N
w
?
k=1
?
m,k
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
.
(5)
With equation (5), the E-Step and M-Step are:
E-Step:
The conditional expectation of hidden variable ?
m,k
, denoted as
?
?
m,k
, is:
?
?
m,k
= P(?
m,k
= 1|X,?) =
pi
k
P(w
m
|h
m
= k,w)
?
N
w
i=1
pi
i
P(w
m
|h
m
= i,w)
.
(6)
The Q function w.r.t. the parameters at the i-th iteration ?
(i)
is written as:
Q(? ,?
(i)
) =
N
w
?
k=1
M
?
m=1
?
?
m,k
(logpi
k
+ logP(w
m
|h
m
= k,w))
=
M
?
m=1
N
w
?
k=1
?
?
m,k
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
.
(7)
M-Step:
pi can be updated by
154
pik
=
?
M
m=1
?
?
m,k
M
, k = 1,2 ? ? ? ,N
w
. (8)
We leave the detailed derivations for equation (6), (7), and (8) to the appendix of the paper. Then we
discuss how we obtain the update of the embedding parameters U
w
m
,t
and V
w,k
. Note that the optimization
problem is non-convex, and it is hard to compute the exact solution of
?Q
?U
w
m
,t
= 0 and
?Q
?V
w,k
= 0. Therefore,
we use gradient ascent to optimize in the M-step. The gradients of Q function w.r.t. embedding vectors
are given by:
?Q
?U
w
m
,t
=
N
w
?
k=1
?
?
m,k
b
(w
m
)
t
(
1? ?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
V
w,k
,
(9)
?Q
?V
w,k
=
M
?
m=1
?
?
m,k
L
w
m
?
t=1
b
(w
m
)
t
(
1? ?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
U
w
m
,t
.
(10)
Iterating between E-Step and M-Step till the convergence of the value of function Q makes the EM
algorithm complete.
In order to enhance the scalability of our approach, we propose a fast computing method to boost
the implementation of the EM algorithm. Note that the most expensive computing operations in both
the E-Step and M-Step are the inner product of the input and output embedding vectors, as well as the
sigmoid function. However, if we take the Hierarchical Softmax Tree form as shown in Equation (4) to
model P(w
m
|h
m
= i,w), and perform only one step gradient ascent in M-Step, the aforementioned two
expensive operations in M-Step will be avoided by leveraging the pre-computed results in the E-Step.
Specifically, since the gradient of the function f (x) = log?(x) is given by f
?
(x) = 1? ?(x), the sigmoid
values computed in the E-Step to obtain P(w
m
|h
m
= i,w) (i.e. the term ?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
) in equation (5),
(9), and (10)) can be re-used to derive the gradients in the M-Step.
However, such enhanced computation method cannot benefit the second order optimization methods
in the M-Step such as L-BFGS and Conjugate Gradient, since they usually rely on multiple iterations to
converge. In fact, we tried these two optimization methods in our experiments but they have brought no
improvement compared with simple one-step gradient ascent method.
3.3 Model Comparison
To show that our model is more scalable than the former multi-prototype model in (Huang et al., 2012)
(We denote it as EHModel in the rest of the paper), we conduct a comparison on the number of parameters
with respect to each of these two models in this subsection.
We use n
embedding
and n
window
to denote the numbers of all word embedding vectors and context win-
dow words, respectively. It is clear that n
embeddings
=
?
w?W
N
w
. EHModel aims to compute two scores,
i.e., the local score and the global score, both with hidden layer node activations. We denote the hidden
layer node number as h
l
and h
g
for these two scores. The parameter numbers are listed in Table 1.
Model EHModel Our Model
#parameters dn
words
+dn
embeddings
+(dn
window
+1)h
l
+(2d +1)h
g
dn
words
+dn
embeddings
Table 1: Comparison of parameter numbers of two models
Note that d in Table 1 denotes the embedding vector size. It can be observed that EHModel has
(dn
window
+1)h
l
+(2d +1)h
g
more parameters than our model, which is mainly because EHModel has
one more layer in the neural network and it considers global context. In previous study (Huang et al.,
2012), d, n
window
, h
l
, and h
g
are set to be 50, 10, 100, 100, respectively, which greatly increases the gap
of parameter numbers between the two models.
155
4 Experiments
In this section, we will present our experimental settings and results. Particularly, we first describe the
data collection and the training configuration we used in the experiments; then, we conduct a qualitative
case study followed by quantitative evaluation results on a public word similarity task to demonstrate the
performance of our proposed model.
4.1 Experimental Setup
Dataset: To make a fair comparison with the state-of-the-art methods, we employ a publicly available
dataset, which is used in (Huang et al., 2012), to train word embeddings in our experiments. Particularly,
this training corpus is a snapshot of Wikipedia at April, 2010 (Shaoul, 2010), which contains about 990
million tokens. We removed the infrequent words from this corpus and kept a dictionary of about 1
million most frequent words. Similar to Word2Vec, we removed pure digit words such as 2014 as well
as about 100 stop words like how, for, and we.
Training Configuration: In order to boost the training speed, we take advantage of the Hierarchical
Softmax Tree structure. More concretely, we use the Huffman tree structure, as introduced in Word2Vec,
to further increase the training speed. All the embedding size, including both word embedding vectors
and the Huffman tree node embedding vectors, are set to be 50, which is the same as the size used in
(Huang et al., 2012). To train word embedding, we set the context window size as 10, i.e., for a word w,
10 of the closest neighboring words to w are regarded as ws contexts. For the numbers of word prototypes,
i.e., N
w
introduced in Section 3.2, we set the top 7 thousand frequent words as multi-prototype words by
experience, with all of them having 10 prototypes (i.e. N
w
= 10).
During the training process, we used the same strategy to set the learning rate as what Word2Vec did.
Specifically, we set the initial learning rate to 0.025 and diminished the value linearly along with the
increasing number of training words. Our experimental results illustrate that this learning rate strategy
can lead to the best results for our algorithm.
For the hyper parameters of the EM algorithm, we set the batch size to 1, i.e. M = 1 in Section 3.2,
since our experimental results reveal that smaller batch size can result in better experimental results. The
reason is explained as the following. Our optimization problem is highly non-convex. Smaller batch size
yields more frequent updates of parameters, and thus avoids trapping in local optima, while larger batch
size, associated with more infrequent parameter updating, may cause higher probability to encounter
local optima. In our experiments, we observe that only one iteration of E-Step and M-Step can reach the
embedding vectors with good enough performance on the word similarity task, whereas increasing the
iteration number just leads to slight performance improvement with much longer training time. Under
the above configuration, our model runs about three times faster than EHModel.
4.2 Case Study
This section gives some qualitative evaluations of our model by demonstrating how our model can ef-
fectively identify multi-prototype word embeddings on some specific cases. In Table 2, we list several
polysemous words. For each word, we pick some of their prototypes learned by our model, including
the prototype prior probability (i.e. pi
i
introduced in Section 3.2) and three of the most similar words
with each prototype, respectively. The similarity is calculated by the cosine similarity score between the
embedding vectors.
From the table we can observe some interesting results of the multi-prototype embedding vectors
produced by our model:
? For a polysemous word, its different embedding vectors represent its different semantic meanings.
For example, the first embedding vector of the word apple corresponds to its sense as a kind of fruit,
whereas the second one represents its meaning as an IT company.
? The prior probability reflects the likelihood of the occurrence of various prototypes to some extent.
For example, the word cell is more likely to represent the meaning of the smallest part of living
structure (with probability 0.81), than to be used as the meaning of cellphone (with probability
156
Word Prior Probability Most Similar Words
apple 1 0.82 strawberry, cherry, blueberry
apple 2 0.17 iphone, macintosh, microsoft
bank 1 0.15 river, canal, waterway
bank 2 0.6 citibank, jpmorgan, bancorp
bank 3 0.25 stock, exchange, banking
cell 1 0.09 phones, cellphones, mobile
cell 2 0.81 protein, tissues, lysis
cell 3 0.01 locked, escape, handcuffed
Table 2: Most similar words with different prototypes of the same word
0.09) or prisoned (with probability 0.01). Note that the three prior probability scores of cell do not
sum to 1. The reason is that there are some other embeddings not presented in the table which are
found to have high similarities with the three embeddings. We do not present them due to the space
limitation.
? By setting the prototype number to a fairly large value (e.g. N
w
= 10), the model tends to learn
more fine-grained separations of the word?s different meanings. For example, we can observe from
Table 2 that the second and the third prototypes of the word bank seem similar to each other as both
of them denote a financial concept. However, there are subtle differences between them: the second
prototype represents concrete banks, such as citibank and jpmorgan, whereas the third one denotes
what is done in the banks, since it is most similar to the words stock, exchange, and banking. We
believe that such a fine-grained separation will bring more expressiveness to the multi-prototype
word embeddings learned by our model.
4.3 Results on Word Similarity in Context Dataset
In this subsection, we give quantitative comparison of our method with conventional word embedding
models, including Word2Vec and EHModel (Huang et al., 2012).
The task we perform is the word similarity evaluation introduced in (Huang et al., 2012). Word simi-
larity tasks evaluate a model?s performance by calculating the Spearman?s rank correlation between the
ranking of ground truth similarity scores (given by human labeling) and the ranking based on the simi-
larity scores produced by the model. Traditional word similarity tasks such as WordSim353 (Finkelstein
et al., 2001) and RG (Rubenstein and Goodenough, 1965) are not suitable for evaluating multi-prototype
models since there is neither enough number of polysemous words in these datasets nor context infor-
mation to infer the prototype index. To address this issue, a new word similarity benchmark dataset
including context information was released in (Huang et al., 2012). Following (Luong et al., 2013), we
use SCWS to denote this dataset. Similar to WordSim353, SCWS contains some word pairs (concretely,
2003 pairs), together with human labeled similarity scores for these word pairs. What makes SCWS
different from WS353 is that the words in SCWS are contained in sentences, i.e., there are 2003 pairs of
sentences containing these words, while words in WS353 are not associated with sentences. Therefore,
the human labeled scores are based on the meanings of the words in the context. Given the presence
of the context, the word similarity scores, especially those scores depending on polysemous words, are
much more convincing for evaluating different models? performance in our experiments.
Then, we propose a method to compute the similarity score for a pair of words {w
1
,w
2
} in the context
based on our model. Suppose that the context of a word w is defined as all its neighboring words in a
T +1 sized window, where w is the central word in the window. We use Context
1
= {c
1
1
,c
1
2
, ? ? ? ,c
1
T
} and
Context
2
= {c
2
1
,c
2
2
, ? ? ? ,c
2
T
} to separately denote the context of w
1
and w
2
, where c
1
t
and c
2
t
are the t-th
context word of w
1
and w
2
, respectively. According to Bayesian rule, we have that for i? {1,2, ? ? ? ,N
w
1
}:
P(h
w
1
= i|Context
1
,w
1
) ? P(Context
1
|h
w
1
= i,w
1
)P(h
w
1
= i|w
1
)
=
T
?
t=1
P(c
1
t
|h
w
1
= i,w
1
)P(h
w
1
= i|w
1
),
(11)
157
where P(c
1
t
|h
w
1
= i,w
1
) can be calculated by equation (4) and P(h
w
1
= i|w
1
) is the prior probability
we learned in the EM algorithm (equation (8)). The similar equation holds for word w
2
as well. Here
we make an assumption that the context words are independent with each other given the central word.
Furthermore, suppose that the most likely prototype index for w
1
given Context
1
is
?
h
w
1
, i.e., we de-
note
?
h
w
1
= argmax
i?{1,2,??? ,N
w
1
}
P(h
w
1
= i|Context
1
,w
1
). Similarly,
?
h
w
2
is denoted as the corresponding
meaning for w
2
.
We calculate two similarity scores base on equation (11), i.e., MaxSim Score and WeightedSim Score:
MaxSim(w
1
,w
2
) =Cosine(V
w
1
,
?
h
w
1
,V
w
2
,
?
h
w
2
), (12)
WeightedSim(w
1
,w
2
) =
N
w
1
?
i=1
N
w
2
?
j=1
P(h
w
1
= i|Context
1
,w
1
)P(h
w
2
= j|Context
2
,w
2
)Cosine(V
w
1
,i
,V
w
2
, j
).
(13)
In the above similarity scores, Cosine(x,y) denotes the cosine similarity score of vector x and y, and
V
w,i
? R
d
is the embedding vector for the word w?s i-th prototype.
The detailed experimental results are listed in Table 3, where ? refers to the Spearman?s rank cor-
relation. The higher value of ? indicates the better performance. The performance score of EHModel
is borrowed from its original paper (Huang et al., 2012). For Word2Vec model, we use Hierarchical
Huffman Tree rather than Negative Sampling to do the acceleration. Our Model M uses the MaxSim
score in testing and our Model W uses the WeightedSim score. All of these models are run on the same
aforementioned Wikipedia corpus, with the dimension of the embedding space to be 50.
From the table, we can observe that our Model W (65.4%) outperforms the original Word2Vec model
(61.7%), and achieves almost the same performance with the state-of-the-art EHModel (65.7%). Among
the two similarity measures used in testing, the WeightedSim score performs better (65.4%) than the
MaxSim score (63.6%), indicating that the overall consideration of all prototype probabilities are more
effective.
Model ??100
Word2Vec 61.7
EHModel 65.7
Model M 63.6
Model W 65.4
Table 3: Spearman?s rank correlations on SCWS dataset.
5 Conclusion
In this paper, we introduce a fast and probabilistic method to generate multiple embedding vectors for
polysemous words, based on the continuous Skip-Gram model. On one hand, our method addresses
the drawbacks of the original Word2Vec model by leveraging multi-prototype word embeddings; on the
other hand, our model yields much less complexity without performance loss compared with the former
clustering based multi-prototype algorithms. In addition, the probabilistic framework of our method
avoids the extra efforts to perform clustering besides training word embeddings.
For the future work, we plan to apply the proposed probabilistic framework to other neural network
language models. Moreover, we would like to apply the multi-prototype embeddings to more real world
text mining tasks, such as information retrieval and knowledge mining, with the expectation that the
multi-prototype embeddings produced by our model will benefit these tasks.
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. In Journal of Machine Learning Research, pages 1137?1155.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
158
Katrin Erk and Sebastian Pad?o. 2008. A structured vector space model for word meaning in context. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 897?906,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on
World Wide Web, pages 406?414. ACM.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-Volume 1, pages 873?882. Association for Computational
Linguistics.
Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recur-
sive neural networks for morphology. CoNLL-2013, 104.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural
network based language model. In INTERSPEECH, pages 1045?1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013b. Distributed representations
of words and phrases and their compositionality. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111?3119.
Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In
Proceedings of the 24th international conference on Machine learning, pages 641?648. ACM.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive esti-
mation. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in
Neural Information Processing Systems 26, pages 2265?2273.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceed-
ings of the international workshop on artificial intelligence and statistics, pages 246?252.
Siva Reddy, Ioannis P Klapaftis, Diana McCarthy, and Suresh Manandhar. 2011. Dynamic and static prototype
vectors for semantic composition. In IJCNLP, pages 705?713.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 109?117. Association for Computational Linguistics.
Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Westbury C Shaoul, C. 2010. The westbury lab wikipedia corpus.
Stefan Thater, Hagen F?urstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective
vector model. In IJCNLP, pages 1134?1143.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394. Association for Computational Linguistics.
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2011. Latent vector weighting for word meaning in
context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP
?11, pages 1012?1022, Stroudsburg, PA, USA. Association for Computational Linguistics.
6 Appendix
6.1 Derivations for the EM Algorithm
We give detailed derivations for the updating rules used in the EM algorithms in Section 3.2., i.e., the
derivations for equation (6), (7), and (8).
159
According to the properties of conditional probability, we have
?
?
m,k
= P(?
m,k
= 1|X,?) =
P(?
m,k
= 1,X|?)
?
N
w
i=1
P(?
m,i
= 1,X|?)
=
P(?
m,k
= 1|?)P(X|?
m,k
= 1,?)
?
N
w
i=1
P(?
m,i
= 1|?)P(X|?
m,i
= 1,?)
=
pi
k
P(w
m
|h
m
= k,w)
?
N
w
i=1
pi
i
P(w
m
|h
m
= i,w)
.
(14)
From equation (7), the Q function is calculated as:
Q(? ,?
(i)
) = E[logP(X,?|?)|?
(i)
]
=
N
w
?
k=1
M
?
m=1
E[?
m,k
|?
(i)
]
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
=
N
w
?
k=1
M
?
m=1
?
?
m,k
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
=
M
?
m=1
N
w
?
k=1
?
?
m,k
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
.
(15)
Then we give the derivations for pi?s updating rule, i.e., equation (8). Note that for parameters pi
k
,
k = {1,2, ? ? ? ,N
w
}, they need to satisfy the condition that
?
N
w
k=1
pi
k
= 1. From equation (7) (or equivalently
equation (15)), the loss with regard to pi is:
L
[pi]
=
M
?
m=1
N
w
?
k=1
?
?
m,k
logpi
k
+? (
N
w
?
k=1
pi
k
?1), (16)
where ? is the Language multiplier. Letting
?L
[pi]
?pi
= 0, we obtain:
pi
k
?
M
?
m=1
?
?
m,k
. (17)
Further considering the fact that
?
N
w
k=1
?
M
m=1
?
?
m,k
= M, we have pi
k
=
?
M
m=1
?
?
m,k
M
.
160
