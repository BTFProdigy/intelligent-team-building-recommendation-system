Proceedings of NAACL-HLT 2013, pages 617?626,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Down-stream effects of tree-to-dependency conversions
Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi?,
Hector Martinez, Anders S?gaard
Center for Language Technology, University of Copenhagen
?Institute for Informatics, University of Oslo
Abstract
Dependency analysis relies on morphosyntac-
tic evidence, as well as semantic evidence.
In some cases, however, morphosyntactic ev-
idence seems to be in conflict with seman-
tic evidence. For this reason dependency
grammar theories, annotation guidelines and
tree-to-dependency conversion schemes often
differ in how they analyze various syntactic
constructions. Most experiments for which
constituent-based treebanks such as the Penn
Treebank are converted into dependency tree-
banks rely blindly on one of four-five widely
used tree-to-dependency conversion schemes.
This paper evaluates the down-stream effect of
choice of conversion scheme, showing that it
has dramatic impact on end results.
1 Introduction
Annotation guidelines used in modern depen-
dency treebanks and tree-to-dependency conversion
schemes for converting constituent-based treebanks
into dependency treebanks are typically based on
a specific dependency grammar theory, such as the
Prague School?s Functional Generative Description,
Meaning-Text Theory, or Hudson?s Word Grammar.
In practice most parsers constrain dependency struc-
tures to be tree-like structures such that each word
has a single syntactic head, limiting diversity be-
tween annotation a bit; but while many dependency
treebanks taking this format agree on how to an-
alyze many syntactic constructions, there are still
many constructions these treebanks analyze differ-
ently. See Figure 1 for a standard overview of clear
and more difficult cases.
The difficult cases in Figure 1 are difficult for
the following reason. In the easy cases morphosyn-
tactic and semantic evidence cohere. Verbs gov-
ern subjects morpho-syntactically and seem seman-
tically more important. In the difficult cases, how-
ever, morpho-syntactic evidence is in conflict with
the semantic evidence. While auxiliary verbs have
the same distribution as finite verbs in head position
and share morpho-syntactic properties with them,
and govern the infinite main verbs, main verbs seem
semantically superior, expressing the main predi-
cate. There may be distributional evidence that com-
plementizers head verbs syntactically, but the verbs
seem more important from a semantic point of view.
Tree-to-dependency conversion schemes used
to convert constituent-based treebanks into
dependency-based ones also take different stands on
the difficult cases. In this paper we consider four dif-
ferent conversion schemes: the Yamada-Matsumoto
conversion scheme yamada,1 the CoNLL 2007
format conll07,2 the conversion scheme ewt used in
the English Web Treebank (Petrov and McDonald,
2012),3 and the lth conversion scheme (Johansson
1The Yamada-Matsumoto scheme can be
replicated by running penn2malt.jar available at
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html. We
used Malt dependency labels (see website). The Yamada-
Matsumoto scheme is an elaboration of the Collins scheme
(Collins, 1999), which is not included in our experiments.
2The CoNLL 2007 conversion scheme can be
obtained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/with the
?conll07? flag set.
3The EWT conversion scheme can be repli-
cated using the Stanford converter available at
http://nlp.stanford.edu/software/stanford-dependencies.shtml
617
Clear cases Difficult cases
Head Dependent ? ?
Verb Subject Auxiliary Main verb
Verb Object Complementizer Verb
Noun Attribute Coordinator Conjuncts
Verb Adverbial Preposition Nominal
Punctuation
Figure 1: Clear and difficult cases in dependency annotation.
and Nugues, 2007).4 We list the differences in
Figure 2. An example of differences in analysis is
presented in Figure 3.
In order to access the impact of these conversion
schemes on down-stream performance, we need ex-
trinsic rather than intrinsic evaluation. In general
it is important to remember that while researchers
developing learning algorithms for part-of-speech
(POS) tagging and dependency parsing seem ob-
sessed with accuracies, POS sequences or depen-
dency structures have no interest on their own. The
accuracies reported in the literature are only inter-
esting insofar they correlate with the usefulness of
the structures predicted by our systems. Fortunately,
POS sequences and dependency structures are use-
ful in many applications. When we consider tree-to-
dependency conversion schemes, down-stream eval-
uation becomes particularly important since some
schemes are more fine-grained than others, leading
to lower performance as measured by intrinsic eval-
uation metrics.
Approach in this work
In our experiments below we apply a state-of-the-art
parser to five different natural language processing
(NLP) tasks where syntactic features are known to
be effective: negation resolution, semantic role la-
beling (SRL), statistical machine translation (SMT),
sentence compression and perspective classification.
In all five tasks we use the four tree-to-dependency
conversion schemes mentioned above and evaluate
them in terms of down-stream performance. We also
compare our systems to baseline systems not rely-
4The LTH conversion scheme can be ob-
tained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/ with the
?oldLTH? flag set.
ing on syntactic features, when possible, and to re-
sults in the literature, when comparable results exist.
Note that negation resolution and SRL are not end
applications. It is not easy to generalize across five
very different tasks, but the tasks will serve to show
that the choice of conversion scheme has significant
impact on down-stream performance.
We used the most recent release of the Mate parser
first described in Bohnet (2010),5 trained on Sec-
tions 2?21 of the Wall Street Journal section of the
English Treebank (Marcus et al, 1993). The graph-
based parser is similar to, except much faster, and
performs slightly better than the MSTParser (Mc-
Donald et al, 2005), which is known to perform
well on long-distance dependencies often important
for down-stream applications (McDonald and Nivre,
2007; Galley and Manning, 2009; Bender et al,
2011). This choice may of course have an effect on
what conversion schemes seem superior (Johansson
and Nugues, 2007). Sentence splitting was done us-
ing splitta,6, and the sentences were then tokenized
using PTB-style tokenization7 and tagged using the
in-built Mate POS tagger.
Previous work
There has been considerable work on down-stream
evaluation of syntactic parsers in the literature, but
most previous work has focused on evaluating pars-
ing models rather than linguistic theories. No one
has, to the best of our knowledge, compared the
impact of choice of tree-to-dependency conversion
scheme across several NLP tasks.
Johansson and Nugues (2007) compare the im-
pact of yamada and lth on semantic role labeling
5http://code.google.com/p/mate-tools/
6http://code.google.com/p/splitta/
7http://www.cis.upenn.edu/?treebank/tokenizer.sed
618
FORM1 FORM2 yamada conll07 ewt lth
Auxiliary Main verb 1 1 2 2
Complementizer Verb 1 2 2 2
Coordinator Conjuncts 2 1 2 2
Preposition Nominal 1 1 1 2
Figure 2: Head decisions in conversions. Note: yamada also differ from CoNLL 2007 in proper names.
Figure 3: CoNLL 2007 (blue) and LTH (red) dependency conversions.
performance, showing that lth leads to superior per-
formance.
Miyao et al (2008) measure the impact of syntac-
tic parsers in an information extraction system iden-
tifying protein-protein interactions in biomedical re-
search articles. They evaluate dependency parsers,
constituent-based parsers and deep parsers.
Miwa et al (2010) evaluate down-stream per-
formance of linguistic representations and parsing
models in biomedical event extraction, but do not
evaluate linguistic representations directly, evaluat-
ing representations and models jointly.
Bender et al (2011) compare several parsers
across linguistic representations on a carefully de-
signed evaluation set of hard, but relatively frequent
syntactic constructions. They compare dependency
parsers, constituent-based parsers and deep parsers.
The authors argue in favor of evaluating parsers on
diverse and richly annotated data. Others have dis-
cussed various ways of evaluating across annotation
guidelines or translating structures to a common for-
mat (Schwartz et al, 2011; Tsarfaty et al, 2012).
Hall et al (2011) discuss optimizing parsers for
specific down-stream applications, but consider only
a single annotation scheme.
Yuret et al (2012) present an overview of the
SemEval-2010 Evaluation Exercises on Semantic
Evaluation track on recognition textual entailment
using dependency parsing. They also compare sev-
eral parsers using the heuristics of the winning sys-
tem for inference. While the shared task is an
example of down-stream evaluation of dependency
parsers, the evaluation examples only cover a subset
of the textual entailments relevant for practical ap-
plications, and the heuristics used in the experiments
assume a fixed set of dependency labels (ewt labels).
Finally, Schwartz et al (2012) compare the
above conversion schemes and several combinations
thereof in terms of learnability. This is very different
from what is done here. While learnability may be
a theoretically motivated parameter, our results indi-
cate that learnability and downstream performance
do not correlate well.
2 Applications
Dependency parsing has proven useful for a wide
range of NLP applications, including statistical ma-
chine translation (Galley and Manning, 2009; Xu et
al., 2009; Elming and Haulrich, 2011) and sentiment
analysis (Joshi and Penstein-Rose, 2009; Johansson
and Moschitti, 2010). This section describes the ap-
plications and experimental set-ups included in this
study.
In the five applications considered below we
619
use syntactic features in slightly different ways.
While our statistical machine translation and sen-
tence compression systems use dependency rela-
tions as additional information about words and on
a par with POS, our negation resolution system uses
dependency paths, conditioning decisions on both
dependency arcs and labels. In perspective classifi-
cation, we use dependency triples (e.g. SUBJ(John,
snore)) as features, while the semantic role labeling
system conditions on a lot of information, including
the word form of the head, the dependent and the ar-
gument candidates, the concatenation of the depen-
dency labels of the predicate, and the labeled depen-
dency relations between predicate and its head, its
arguments, dependents or siblings.
2.1 Negation resolution
Negation resolution (NR) is the task of finding nega-
tion cues, e.g. the word not, and determining their
scope, i.e. the tokens they affect. NR has recently
seen considerable interest in the NLP community
(Morante and Sporleder, 2012; Velldal et al, 2012)
and was the topic of the 2012 *SEM shared task
(Morante and Blanco, 2012).
The data set used in this work, the Conan Doyle
corpus (CD),8 was released in conjunction with the
*SEM shared task. The annotations in CD extend
on cues and scopes by introducing annotations for
in-scope events that are negated in factual contexts.
The following is an example from the corpus show-
ing the annotations for cues (bold), scopes (under-
lined) and negated events (italicized):
(1) Since we have been so
unfortunate as to miss him [. . . ]
CD-style scopes can be discontinuous and overlap-
ping. Events are a portion of the scope that is se-
mantically negated, with its truth value reversed by
the negation cue.
The NR system used in this work (Lapponi et al,
2012), one of the best performing systems in the
*SEM shared task, is a CRF model for scope resolu-
tion that relies heavily on features extracted from de-
pendency graphs. The feature model contains token
distance, direction, n-grams of word forms, lemmas,
POS and combinations thereof, as well as the syntac-
tic features presented in Figure 4. The results in our
8http://www.clips.ua.ac.be/sem2012-st-neg/data.html
Syntactic
constituent
dependency relation
parent head POS
grand parent head POS
word form+dependency relation
POS+dependency relation
Cue-dependent
directed dependency distance
bidirectional dependency distance
dependency path
lexicalized dependency path
Figure 4: Features used to train the conditional random
field models
experiments are obtained from configurations that
differ only in terms of tree-to-dependency conver-
sions, and are trained on the training set and tested
on the development set of CD. Since the negation
cue classification component of the system does not
rely on dependency features at all, the models are
tested using gold cues.
Table 1 shows F1 scores for scopes, events and
full negations, where a true positive correctly as-
signs both scope tokens and events to the rightful
cue. The scores are produced using the evaluation
script provided by the *SEM organizers.
2.2 Semantic role labeling
Semantic role labeling (SRL) is the attempt to de-
termine semantic predicates in running text and la-
bel their arguments with semantic roles. In our
experiments we have reproduced the second best-
performing system in the CoNLL 2008 shared task
in syntactic and semantic parsing (Johansson and
Nugues, 2008).9
The English training data for the CoNLL 2008
shared task were obtained from PropBank and
NomBank. For licensing reasons, we used
OntoNotes 4.0, which includes PropBank, but not
NomBank. This means that our system is only
trained to classify verbal predicates. We used
the Clearparser conversion tool10 to convert the
OntoNotes 4.0 and subsequently supplied syntac-
tic dependency trees using our different conversion
schemes. We rely on gold standard argument identi-
fication and focus solely on the performance metric
semantic labeled F1.
9http://nlp.cs.lth.se/software/semantic parsing: propbank
nombank frames
10http://code.google.com/p/clearparser/
620
2.3 Statistical machine translation
The effect of the different conversion schemes was
also evaluated on SMT. We used the reordering
by parsing framework described by Elming and
Haulrich (2011). This approach integrates a syn-
tactically informed reordering model into a phrase-
based SMT system. The model learns to predict the
word order of the translation based on source sen-
tence information such as syntactic dependency re-
lations. Syntax-informed SMT is known to be use-
ful for translating between languages with different
word orders (Galley and Manning, 2009; Xu et al,
2009), e.g. English and German.
The baseline SMT system is created as described
in the guidelines from the original shared task.11
Only modifications are that we use truecasing in-
stead of lowercasing and recasing, and allow train-
ing sentences of up to 80 words. We used data
from the English-German restricted task: ?3M par-
allel words of news, ?46M parallel words of Eu-
roparl, and ?309M words of monolingual Europarl
and news. We use newstest2008 for tuning, new-
stest2009 for development, and newstest2010 for
testing. Distortion limit was set to 10, which is
also where the baseline system performed best. The
phrase table and the lexical reordering model is
trained on the union of all parallel data with a max
phrase length of 7, and the 5-gram language model
is trained on the entire monolingual data set.
We test four different experimental systems that
only differ with the baseline in the addition of a syn-
tactically informed reordering model. The baseline
system was one of the tied best performing system
in the WMT 2011 shared task on this dataset. The
four experimental systems have reordering models
that are trained on the first 25,000 sentences of the
parallel news data that have been parsed with each
of the tree-to-dependency conversion schemes. The
reordering models condition reordering on the word
forms, POS, and syntactic dependency relations of
the words to be reordered, as described in Elming
and Haulrich (2011). The paper shows that while
reordering by parsing leads to significant improve-
ments in standard metrics such as BLEU (Papineni
et al, 2002) and METEOR (Lavie and Agarwal,
2007), improvements are more spelled out with hu-
11 http://www.statmt.org/wmt11/translation-task.html
man judgements. All SMT results reported below
are averages based on 5 MERT runs following Clark
et al (2011).
2.4 Sentence compression
Sentence compression is a restricted form of sen-
tence simplification with numerous usages, includ-
ing text simplification, summarization and recogniz-
ing textual entailment. The most commonly used
dataset in the literature is the Ziff-Davis corpus.12 A
widely used baseline for sentence compression ex-
periments is Knight and Marcu (2002), who intro-
duce two models: the noisy-channel model and a de-
cision tree-based model. Both are tree-based meth-
ods that find the most likely compressed syntactic
tree and outputs the yield of this tree. McDonald et
al. (2006) instead use syntactic features to directly
find the most likely compressed sentence.
Here we learn a discriminative HMM model
(Collins, 2002) of sentence compression using
MIRA (Crammer and Singer, 2003), comparable to
previously explored models of noun phrase chunk-
ing. Our model is thus neither tree-based nor
sentence-based. Instead we think of sentence com-
pression as a sequence labeling problem. We com-
pare a model informed by word forms and predicted
POS with models also informed by predicted depen-
dency labels. The baseline feature model conditions
emission probabilities on word forms and POS us-
ing a ?2 window and combinations thereoff. The
augmented syntactic feature model simply adds de-
pendency labels within the same window.
2.5 Perspective classification
Finally, we include a document classification dataset
from Lin and Hauptmann (2006).13 The dataset con-
sists of blog posts posted at bitterlemons.org by Is-
raelis and Palestinians. The bitterlemons.org web-
site is set up to ?contribute to mutual understanding
through the open exchange of ideas.? In the dataset,
each blog post is labeled as either Israeli or Pales-
tinian. Our baseline model is just a standard bag-
of-words model, and the system adds dependency
triplets to the bag-of-words model in a way similar
to Joshi and Penstein-Rose (2009). We do not re-
move stop words, since perspective classification is
12LDC Catalog No.: LDC93T3A.
13https://sites.google.com/site/weihaolinatcmu/data
621
bl yamada conll07 ewt lth
DEPRELS - 12 21 47 41
PTB-23 (LAS) - 88.99 88.52 81.36? 87.52
PTB-23 (UAS) - 90.21 90.12 84.22? 90.29
Neg: scope F1 - 81.27 80.43 78.70 79.57
Neg: event F1 - 76.19 72.90 73.15 76.24
Neg: full negation F1 - 67.94 63.24 61.60 64.31
SentComp F1 68.47 72.07 64.29 71.56 71.56
SMT-dev-Meteor 35.80 36.06 36.06 36.16 36.08
SMT-test-Meteor 37.25 37.48 37.50 37.58 37.51
SMT-dev-BLEU 13.66 14.14 14.09 14.04 14.06
SMT-test-BLEU 14.67 15.04 15.04 14.96 15.11
SRL-22-gold - 81.35 83.22 84.72 84.01
SRL-23-gold - 79.09 80.85 80.39 82.01
SRL-22-pred - 74.41 76.22 78.29 66.32
SRL-23-pred - 73.42 74.34 75.80 64.06
bitterlemons.org 96.08 97.06 95.58 96.08 96.57
Table 1: Results. ?: Low parsing results on PTB-23 using ewt are explained by changes between the PTB-III and the
Ontonotes 4.0 release of the English Treebank.
similar to authorship attribution, where stop words
are known to be informative. We evaluate perfor-
mance doing cross-validation over the official train-
ing data, setting the parameters of our learning algo-
rithm for each fold doing cross-validation over the
actual training data. We used soft-margin support
vector machine learning (Cortes and Vapnik, 1995),
tuning the kernel (linear or polynomial with degree
3) and C = {0.1, 1, 5, 10}.
3 Results and discussion
Our results are presented in Table 1. The parsing
results are obtained relying on predicted POS rather
than, as often done in the dependency parsing liter-
ature, relying on gold-standard POS. Note that they
comply with the result in Schwartz et al (2012) that
Yamada-Matsumoto-style annotation is more easily
learnable.
The negation resolution results are significantly
better using syntactic features in yamada annota-
tion. It is not surprising that a syntactically ori-
ented conversion scheme performs well in this task.
Since Lapponi et al (2012) used Maltparser (Nivre
et al, 2007) with the freely available pre-trained
parsing model for English,14 we decided to also
run that parser with the gold-standard cues, in ad-
14http://www.maltparser.org/mco/english parser/engmalt.html
dition to Mate. The pre-trained model was trained
on Sections 2?21 of the Wall Street Journal sec-
tion of the English Treebank (Marcus et al, 1993),
augmented with 4000 sentences from the Question-
Bank,15 which was converted using the Stanford
converter and thus similar to the ewt annotations
used here. The results were better than using ewt
with Mate trained on Sections 2?21 alone, but worse
than the results obtained here with yamada conver-
sion scheme. F1 score on full negation was 66.92%.
The case-sensitive BLEU evaluation of the
SMT systems indicates that choice of conversion
scheme has no significant impact on overall perfor-
mance. The difference to the baseline system is
significant (p < 0.01), showing that the reorder-
ing model leads to improvement using any of the
schemes. However, the conversion schemes lead to
very different translations. This can be seen, for
example, by the fact that the relative tree edit dis-
tance between translations of different syntactically
informed SMT systems is 12% higher than within
each system (across different MERT optimizations).
The reordering approach puts a lot of weight on
the syntactic dependency relations. As a conse-
quence, the number of relation types used in the
conversion schemes proves important. Consider the
15http://www.computing.dcu.ie/?jjudge/qtreebank/
622
REFERENCE: Zum Glu?ck kam ich beim Strassenbahnfahren an die richtige Stelle .
SOURCE: Luckily , on the way to the tram , I found the right place .
yamada: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
conll07: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
ewt: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
lth: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
BASELINE: Zum Glu?ck hat auf dem Weg zur S-Bahn , ich fand den richtigen Platz .
Figure 5: Examples of SMT output.
ORIGINAL: * 68000 sweden ab of uppsala , sweden , introduced the teleserve , an integrated answering
machine and voice-message handler that links a macintosh to touch-tone phones .
BASELINE: 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
yamada 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
conll07 68000 sweden ab sweden introduced the teleserve integrated answering
machine and voice-message handler .
ewt 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
lth 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
HUMAN: 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
Figure 6: Examples of sentence compression output.
example in Figure 5. German requires the verb in
second position, which is obeyed in the much bet-
ter translations produced by the ewt and lth sys-
tems. Interestingly, the four schemes produce virtu-
ally identical structures for the source sentence, but
they differ in their labeling. Where conll07 and ya-
mada use the same relation for the first two con-
stituents (ADV and vMOD, respectively), ewt and
lth distinguish between them (ADVMOD/PREP and
ADV/LOC). This distinction may be what enables
the better translation, since the model may learn to
move the verb after the sentence adverbial. In the
other schemes, sentence adverbials are not distin-
guished from locational adverbials. Generally, ewt
and lth have more than twice as many relation types
as the other schemes.
The schemes ewt and lth lead to better SRL
performance than conll07 and yamada when re-
lying on gold-standard syntactic dependency trees.
This supports the claims put forward in Johansson
and Nugues (2007). These annotations also hap-
pen to use a larger set of dependency labels, how-
ever, and syntactic structures may be harder to re-
construct, as reflected by labeled attachment scores
(LAS) in syntactic parsing. The biggest drop in
SRL performance going from gold-standard to pre-
dicted syntactic trees is clearly for the lth scheme,
at an average 17.8% absolute loss (yamada 5.8%;
conll07 6.8%; ewt 5.5%; lth 17.8%).
The ewt scheme resembles lth in most respects,
but in preposition-noun dependencies it marks the
preposition as the head rather than the noun. This
is an important difference for SRL, because seman-
tic arguments are often nouns embedded in preposi-
tional phrases, like agents in passive constructions.
It may also be that the difference in performance is
simply explained by the syntactic analysis of prepo-
sitional phrases being easier to reconstruct.
The sentence compression results are generally
much better than the models proposed in Knight and
Marcu (2002). Their noisy channel model obtains
an F1 compression score of 14.58%, whereas the
decision tree-based model obtains an F1 compres-
sion score of 31.71%. While F1 scores should be
complemented by human judgements, as there are
typically many good sentence compressions of any
source sentence, we believe that error reductions of
more than 50% indicate that the models used here
623
ADV AMOD CC COORDDEP EXP GAP IOBJ LGS NMOD OBJ P PMOD PRD PRN PRT ROOT SBJ VC VMOD
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
L
a
b
e
l
s
srl
neg
Figure 7: Distributions of dependency labels in the
Yamada-Matsumoto scheme
(though previously unexplored in the literature) are
fully competitive with state-of-the-art models.
We also see that the models using syntactic fea-
tures perform better than our baseline model, except
for the model using conll07 dependency annotation.
This may be surprising to some, since distributional
information is often considered important in sen-
tence compression (Knight and Marcu, 2002). Some
output examples are presented in Figure 6. Un-
surprisingly, it is seen that the baseline model pro-
duces grammatically incorrect output, and that most
of our syntactic models correct the error leading to
ungrammaticality. The model using ewt annotation
is an exception. We also see that conll07 introduces
another error. We believe that this is due to the way
the conll07 tree-to-dependency conversion scheme
handles coordination. While the word Sweden is not
coordinated, it occurs in a context, surrounded by
commas, that is very similar to coordinated items.
In perspective classification we see that syntactic
features based on yamada and lth annotations lead
to improvements, with yamada leading to slightly
better results than lth. The fact that a syntactically
oriented conversion scheme leads to the best results
may reflect that perspective classification, like au-
thorship attribution, is less about content than stylis-
tics.
While lth seems to lead to the overall best re-
sults, we stress the fact that the five tasks considered
here are incommensurable. What is more interest-
ing is that, task to task, results are so different. The
semantically oriented conversion schemes, ewt and
lth, lead to the best results in SRL, but with a signif-
icant drop for lth when relying on predicted parses,
while the yamada scheme is competitive in the other
four tasks. This may be because distributional infor-
mation is more important in these tasks than in SRL.
The distribution of dependency labels seems rel-
atively stable across applications, but differences in
data may of course also affect the usefulness of dif-
ferent annotations. Note that conll07 leads to very
good results for negation resolution, but bad results
for SRL. See Figure 7 for the distribution of labels
in the conll07 conversion scheme on the SRL and
negation scope resolution data. Many differences
relate to differences in sentence length. The nega-
tion resolution data is literary text with shorter sen-
tences, which therefore uses more punctuation and
has more root dependencies than newspaper articles.
On the other hand we do see very few predicate de-
pendencies in the SRL data. This may affect down-
stream results when classifying verbal predicates in
SRL. We also note that the number of dependency
labels have less impact on results in general than we
would have expected. The number of dependency
labels and the lack of support for some of them may
explain the drop with predicted syntactic parses in
our SRL results, but generally we obtain our best re-
sults with yamada and lth annotations, which have
12 and 41 dependency labels, respectively.
4 Conclusions
We evaluated four different tree-to-dependency con-
version schemes, putting more or less emphasis on
syntactic or semantic evidence, in five down-stream
applications, including SMT and negation resolu-
tion. Our results show why it is important to be
precise about exactly what tree-to-dependency con-
version scheme is used. Tools like pennconverter.jar
gives us a wide range of options when converting
constituent-based treebanks, and even small differ-
ences may have significant impact on down-stream
performance. The small differences are also impor-
tant for more linguistic comparisons that also tend to
gloss over exactly what conversion scheme is used,
e.g. Ivanova et al (2012).
Acknowledgements
Hector Martinez is funded by the ERC grant
CLARA No. 238405, and Anders S?gaard is
funded by the ERC Starting Grant LOWLANDS
No. 313695.
624
References
Emily Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local dependencies in a large corpus. In EMNLP.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In COLING.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: controlling for optimizer instabil-
ity. In ACL.
Mike Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models. In EMNLP.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive algorithms for multiclass problems. In JMLR.
Jakob Elming and Martin Haulrich. 2011. Reordering
by parsing. In Proceedings of International Workshop
on Using Linguistic Information for Hybrid Machine
Translation (LIHMT-2011).
Michel Galley and Christopher Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
EMNLP.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom? a con-
trastive study of syntactico-semantic dependencies. In
LAW.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In CoNLL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In CoNLL.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Emanuele Lapponi, Erik Velldal, Lilja ?vrelid, and
Jonathon Read. 2012. UiO2: Sequence-labeling nega-
tion using dependency features. In *SEM.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspectives?
In COLING-ACL.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsers. In
EMNLP-CoNLL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
2005, pages 523?530, Vancouver, British Columbia.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In EACL.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In COLING.
Yusuke Miyao, Rune S? tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
ACL.
Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: Resolving the scope and focus of nega-
tion. In *SEM.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Computational linguistics, 38(2):223?260.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: a language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Roy Schwartz, and Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency pars-
ing evaluation. In ACL.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
COLING.
625
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical pars-
ing. In EACL.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of synta. Computational linguis-
tics, 38(2):369?410.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In NAACL-
HLT, Boulder, Colorado.
Deniz Yuret, Laura Rimell, and Aydin Han. 2012. Parser
evaluation using textual entailments. Language Re-
sources and Evaluation, Published online 31 October
2012.
626
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 319?327,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UiO2: Sequence-Labeling Negation Using Dependency Features
Emanuele Lapponi Erik Velldal Lilja ?vrelid Jonathon Read
University of Oslo, Department of Informatics
{emanuel,erikve,liljao,jread}@ifi.uio.no
Abstract
This paper describes the second of two sys-
tems submitted from the University of Oslo
(UiO) to the 2012 *SEM Shared Task on re-
solving negation. The system combines SVM
cue classification with CRF sequence labeling
of events and scopes. Models for scopes and
events are created using lexical and syntactic
features, together with a fine-grained set of la-
bels that capture the scopal behavior of certain
tokens. Following labeling, negated tokens are
assigned to their respective cues using simple
post-processing heuristics. The system was
ranked first in the open track and third in the
closed track, and was one of the top perform-
ers in the scope resolution sub-task overall.
1 Introduction
Negation Resolution (NR) is the task of determin-
ing, for a given sentence, which tokens are affected
by a negation cue. The data set most prominently
used for the development of systems for automatic
NR is the BioScope Corpus (Vincze et al, 2008), a
collection of clinical reports and papers in the bio-
medical domain annotated with negation and specu-
lation cues and their scopes. The data sets released
in conjunction with the 2012 shared task on NR
hosted by The First Joint Conference on Lexical and
Computational Semantics (*SEM 2012) are com-
prised of the following negation annotated stories of
Conan Doyle (CD): a training set of 3644 sentences
drawn from The Hound of the Baskervilles (CDT), a
development set of 787 sentences taken from Wis-
teria Lodge (CDD; we will refer to the combina-
tion of CDT and CDD as CDTD), and a held-out
test set of 1089 sentences from The Cardboard Box
and The Red Circle (CDE). In these sets, the con-
cept of negation scope extends on the one adopted
in the BioScope corpus in several aspects: Nega-
tion cues are not part of the scope, morphological
(affixal) cues are annotated and scopes can be dis-
continuous. Moreover, in-scope states or events are
marked as negated if they are factual and presented
as events that did not happen (Morante and Daele-
mans, 2012). Examples (1) and (2) below are exam-
ples of affixal negation and discontinuous scope re-
spectively: The cues are bold, the tokens contained
within their scopes are underlined and the negated
event is italicized.
(1) Since we have been so unfortunate as to miss him [. . . ]
(2) If he was in the hospital and yet not on the staff he could
only have been a house-surgeon or a house-physician: lit-
tle more than a senior student.
Example (2) has no negated events because the sen-
tence is non-factual.
The *SEM shared task thus comprises three sub-
tasks: cue identification, scope resolution and event
detection. It is furthermore divided into two separate
tracks: one closed track, where only the data sup-
plied by the organizers (word form, lemma, PoS-tag
and syntactic constituent for each token) may be em-
ployed, and an open track, where participants may
employ any additional tools or resources.
Pragmatically speaking, a token can be either out
of scope or assigned to one or more of the three re-
maining classes: negation cue, in scope and negated
event. Additionally, in-scope tokens and negated
events are paired to the cues they are negated by.
319
Our system achieves this by remodeling the task as a
sequence labeling task. With annotations converted
to sequences of labels, we train a Conditional Ran-
dom Field (CRF) classifier with a range of different
feature types, including features defined over depen-
dency graphs. This article presents two submissions
for the *SEM shared task, differing only with re-
spect to how these dependency graphs were derived.
For our open track submission, the dependency rep-
resentations are produced by a state-of-the-art de-
pendency parser, whereas the closed track submis-
sion employs dependencies derived from the con-
stituent analyses supplied with the shared task data
sets through a process of constituent-to-dependency
conversion. In both systems, labeling of test data is
performed in two stages. First, cues are detected us-
ing a token classifier,1 and secondly, scope and event
resolution is achieved by post-processing the output
of the sequence labeler.
The two systems described in this paper have been
developed using CDT for training and CDD for test-
ing, and differ only with regard to the source of syn-
tactic information. All reported scores are generated
using an evaluation script provided by the task or-
ganizers. In addition to providing a full end-to-end
evaluation, the script breaks down results with re-
spect to identification of cues, events, scope tokens,
and two variants of scope-level exact match; one re-
quiring exact match also of cues and another only
partial cue match. For our system these two scope-
level scores are identical and so are not duplicated
in our reporting. Additionally we chose not to opti-
mize for the scope tokens measure, and hence this is
also not reported as a development result.
Note also that the official evaluation actually in-
cludes two different variants of the metrics men-
tioned above; a set of primary measures with pre-
cision computed as P=TP/(TP+FP) and a set of B
measures where precision is rather computed as
P=TP/SYS, where SYS is the total number of pre-
dictions made by the system. The reason why SYS is
not identical with TP+FP is that partial matches are
1Note that the cue classifier applied in the current paper is
the same as that used in the other shared task submission from
the University of Oslo (Read et al, 2012), and the two system
descriptions will therefore have much overlap on this particular
point. For all other components the architectures of the two
system are completely different, however.
only counted as FNs (and not FPs) in order to avoid
double penalties. We do not report the B measures
for development testing as they were introduced for
the final evaluation and hence were not considered
in our system optimization. We note though, that the
relative-ranking of participating systems for the pri-
mary and B measures is identical, and that the cor-
relation between the paired lists of scores is nearly
perfect (r=0.997).
The rest of the paper is structured as follows.
First, the cue classifier, its features and results are
described in Section 2. Section 3 presents the sys-
tem for scope and event resolution and details differ-
ent features, the model-internal representation used
for sequence-labeling, as well as the post-processing
component. Error analyses for the cue, scope and
event components are provided in the respective sec-
tions. Section 4 and 5 provide developmental and
held-out results, respectively. Finally, we provide
conclusions and some reflections regarding future
work in Section 6.
2 Cue detection
Identification of negation cues is based on the light-
weight classification scheme presented by Velldal et
al. (2012). By treating the set of cue words as a
closed class, Velldal et al (2012) showed that one
could greatly reduce the number of examples pre-
sented to the learner, and correspondingly the num-
ber of features, while at the same time improving
performance. This means that the classifier only at-
tempts to ?disambiguate? known cue words while
ignoring any words not observed as cues in the train-
ing data.
The classifier applied in the current submission
is extended to also handle affixal negation cues,
such as the prefix cue in impatience, the infix in
carelessness, and the suffix of colourless. The types
of negation affixes observed in CDTD are; the pre-
fixes un, dis, ir, im, and in; the infix less (we inter-
nally treat this as the suffixes lessly and lessness);
and the suffix less. Of the total number of 1157 cues
in the training and development set, 192 are affixal.
There are, however, a total of 1127 tokens matching
one of the affix patterns above, and while we main-
tain the closed class assumption also for the affixes,
the classifier will need to consider its status as a cue
320
or non-cue when attaching to any such token, like
for instance image, recklessness, and bless.
2.1 Features
In the initial formulation of Velldal (2011), an SVM
classifier was trained using simple n-gram features
over words, both full forms and lemmas, to the left
and right of the candidate cues. In addition to these
token-level features, the classifier we apply here in-
cludes some features specifically targeting morpho-
logical or affixal cues. The first such feature records
character n-grams from both the beginning and end
of the base that an affix attaches to (up to five po-
sitions). For a context like impossible we would
record n-grams such {possi, poss, pos, . . .} and
{sible, ible, ble, . . .}, and combine this with infor-
mation about the affix itself (im) and the token part-
of-speech (?JJ?).
For the second feature type targeting affix cues
we try to emulate the effect of a lexicon look-up
of the remaining substring that an affix attaches to,
checking its status as an independent base form and
its part-of-speech. In order to take advantage of
such information while staying within the confines
of the closed track, we automatically generate a lex-
icon from the training data, counting the instances
of each PoS tagged lemma in addition to n-grams
of word-initial characters (again recording up to five
positions). For a given match of an affix pattern, a
feature will then record the counts from this lexicon
for the substring it attaches to. The rationale for this
feature is that the occurrence of a substring such as
un in a token such as underlying should be consid-
ered more unlikely to be a cue given that the first
part of the remaining string (e.g., derly) would be an
unlikely way to begin a word.
Note that, it is also possible for a negation cue
to span multiple tokens, such as the (discontinuous)
pair neither / nor or fixed expressions like on the
contrary. There are, however, only 16 instances of
such multiword cues (MWCs) in the entire CDTD.
Rather than letting the classifier be sensitive to these
corner cases, we cover such MWC patterns using
a small set of simple post-processing heuristics. A
small stop-list is used for filtering out the relevant
words from the examples presented to the classifier
(on, the, etc.).
Data set Model Prec Prec F1
CDD
Baseline 90.68 84.39 87.42
Classifier 93.75 95.38 94.56
CDE
Baseline 87.10 92.05 89.51
Classifier 89.17 93.56 91.31
Table 1: Cue classification results for the final classifier
and the majority-usage baseline, showing test scores for
the development set (training on CDT) and the final held-
out set (training on CDTD).
2.2 Results
Table 1 presents results for the cue classifier. While
the classifier configuration was optimized against
CDD, the model used for the final held-out testing
is trained on the entire CDTD, which (given our
closed-class treatment of cues) provides a total of
1162 positive and 1100 negative training examples.
As an informed baseline, we also tried classifying
each word based on its most frequent use as cue
or non-cue in the training data. (Affixal cue oc-
currences are counted by looking at both the affix-
pattern and the base it attaches to, basically treating
the entire token as a cue. Tokens that end up be-
ing classified as cues are then matched against the
affix patterns observed during training in order to
correctly delimit the annotation of the cue.) This
simple majority-usage approach actually provides a
fairly strong baseline, yielding an F1 of 87.42 on
CDD (P=90.68, R=84.39). Compare this to the F1 of
94.56 obtained by the classifier on the same data set
(P=93.75, R=95.38). However, when applying the
models to the held-out set, with models estimated
over the entire CDTD, the baseline seems to able
to make good use of the additional data and proves
to be even more competitive: While our final cue
classifier achieves F1=91.31, the baseline achieves
F1=89.51, almost two percentage points higher than
its score on the development data, and even outper-
forms four of the ten cue detection systems submit-
ted for the shared task (three of the 12 shared task
submissions use the same classifier).
When inspecting the predictions of our final cue
classifier on CDD, comprising a total of 173 gold
annotated cues, we find that our system mislabels
11 false positives (FPs) and 7 false negatives (FNs).
321
Of the FPs, we find five so-called false negation cues
(Morante et al, 2011), including three instances of
none in the fixed expression none the less. The
others are affixal cues, of which two are clearly
wrong (underworked, universal) while others might
arguably be due to annotation errors (insuperable,
unhappily, endless, listlessly). Among the FNs, two
are due to MWCs not covered by our heuristics (e.g.,
no more), while the remaining errors concern af-
fixes, including one in an interesting context of dou-
ble negation; not dissatisfied.
3 Scope and event resolution
In this work, we model negation scope resolution
as a special instance of the classical IOB (Inside,
Outside, Begin) sequence labeling problem, where
negation cues are labeled to be sequence starters and
scopes and events as two different kinds of chunks.
CRFs allow the computation of p(X|Y), whereX is
a sequence of labels andY is a sequence of observa-
tions, and have already been shown to be efficient in
similar, albeit less involved, tasks of negation scope
resolution (Morante and Daelemans, 2009; Councill
et al, 2010). We employ the CRF implementation in
the Wapiti toolkit, using default settings (Lavergne
et al, 2010). A number of features were used to
create the models. In addition to the information
provided for each token in the CD corpus (lemma,
part of speech and constituent), we extracted both
left and right token distance to the closest negation
cue. Features were expanded to include forward and
backward bigrams and trigrams on both token and
PoS level, as well as lexicalized PoS unigrams and
bigrams2. Table 2 presents a complete list of fea-
tures. The more intricate, dependency-based fea-
tures are presented in Section 3.1, while the labeling
of both scopes and events is detailed in Section 3.2.
3.1 Dependency-based features
For the system submitted to the closed track, the syn-
tactic representations were converted to dependency
representations using the Stanford dependency con-
verter, which comes with the Stanford parser (de
Marneffe et al, 2006).3 These dependency represen-
2By lexicalized PoS we mean an instance of a PoS-Tag in
conjunction with the sentence token.
3Note that the converter was applied directly to the phrase-
structure trees supplied with the negation data sets, and the
General features
Token
Lemma
PoS unigram
Forward token bigram and trigram
Backward token bigram and trigram
Forward PoS trigram
Backward PoS trigram
Lexicalized PoS
Forward Lexicalized PoS bigram
Backward Lexicalized PoS bigram
Constituent
Dependency relation
First order head PoS
Second order head PoS
Lexicalized dependency relation
PoS-disambiguated dependency relation
Cue-dependent features
Token distance
Directed dependency distance
Bidirectional dependency distance
Dependency path
Lexicalized dependency path
Table 2: List of features used to train the CRF models.
tations result from a conversion of Penn Treebank-
style phrase structure trees, combining ?classic? head
finding rules with rules that target specific linguistic
constructions, such as passives or attributive adjec-
tives. The so-called basic format provides a depen-
dency graph which is a directed tree, see Figure 1
for an example.
For the open track submission we used Maltparser
(Nivre et al, 2006) with its pre-trained parse model
for English.4 The parse model has been trained on a
conversion of sections 2-21 of the Wall Street Jour-
nal section of the Penn Treebank to Stanford depen-
dencies, augmented with data from Question Bank.
The parser was applied to the negation data, using
the word tokens and supplied parts-of-speech as in-
put to the parser.
The features extracted via the dependency graphs
aim at modeling the syntactic relationship between
each token and the closest negation cue. Token dis-
tance was therefore complemented with two variants
of dependency distance from each token to the lexi-
Stanford parser was not used to parse the data.
4The pre-trained model is available from maltparser.org
322
we   have  never  gone  out  without  keeping  a  sharp  watch  ,  and  no  one  could  have  escaped  our  notice  .  "
nsubj
aux
neg
conj
cc
punct
prep
part
pcomp
dobj
det
amod
dep
nsubj
aux
aux
punct
punct
dobj
poss
root
ann. 1:
ann. 2:
ann. 3:
cue
cue
cue
labels: CUE CUE CUEN N E E
N N
N N E N N N NS O S O
N
Figure 1: A sentence from the CD corpus showing a dependency graph and the annotation-to-label conversion.
cally closest cue, Directed Distance (DD) and Bidi-
rectional Distance (BD). DD is extracted by follow-
ing the reversed, directed edges from token X to the
cue. If there is no such path, the value of the feature
is -1. BD uses the Dijkstra shortest path algorithm
on an undirected representation of the graph. The
latter feature proved to be more effective than the
former when not used together; using them in con-
junction seemed to confuse the model, thus the fi-
nal model utilizes only BD. We furthermore use the
Dependency Graph Path (DGP) as a feature. This
feature was inspired by the Parse Tree Path feature
presented in Gildea and Jurafsky (2002) in the con-
text of Semantic Role Labeling. It represents the
path traversed from each token to the cue, encod-
ing both the dependency relations and the direction
of the arc that is traversed: for instance, the rela-
tion between our and no in Figure 1 is described as
 poss  dobj  nsubj  det. Like Councill et
al. (2010), we also encode the PoS of the first and
second order syntactic head of each token. For the
token no in Figure 1, for instance, we record the PoS
of one and escaped, respectively.
3.2 Model-internal representation
The token-wise annotations in the CD corpus con-
tain multiple layers of information. Tokens may or
may not be negation cues and they can be either in
or out of scope; in-scope tokens may or may not
be negated events, and are associated with each of
the cues they are negated by. Moreover, scopes may
be (partially) overlapping, as in Figure 1, where the
PoS # S PoS # MCUE PoS # CUE
punctuation 1492 JJ 268 RB 1026
CC 52 RB 28 DT 296
IN + TO 46 NN 16 NN 146
RB 38 NN 4 UH 118
PRP 32 IN 2 IN 64
rest 118 rest ? rest 38
Table 3: Frequency distribution of parts of speech over
the S, MCUE and CUE labels in CDTD.
scope of without is contained within the scope of
never. We convert this representation internally by
assigning one of six labels to each token: O, CUE,
MCUE, N, E and S, for out-of-scope, cue, mor-
phological (affixal) cue, in-scope, event and nega-
tion stop respectively. The CUE, O, N and E la-
bels parallel the IOB chunking paradigm and are
eventually translated in the final annotations by our
post-processing component. MCUE and S extend
the label set to account for the specific behavior of
the tokens they are associated with. The rationale
behind the separation of cues in two classes is the
pronounced differences between the PoS frequency
distributions of standard versus morphological cues.
Table 3 presents the frequency distribution of PoS-
tags over the different cue types in CDTD and shows
that, unsurprisingly, the majority class for morpho-
logical cues is adjectives, which typically generate
different scope patterns compared to the majority
class for standard cues. The S label, a special in-
stance of an out-of-scope token, is defined as the
323
first non-cue, out-of-scope token to the right of one
labeled with N, and targets mostly punctuation.
After some experimentation with joint labeling of
scopes and events, we opted for separation of the
two models, hence training separate models for the
two tasks of scope resolution and event detection.
In the model for scopes, all E labels are switched
to N; conversely, Ns become Os in the event model.
Given the nature of the annotations, the predictions
provided by the model for events serve a double pur-
pose: finding the negated token in a sentence and
deciding whether a sentence is factual or not. The
outputs of the two classifiers are merged during post-
processing.
3.3 Post-processing
A simple, heuristics-based algorithm was applied
to the output of the labelers in order to pair each
in-scope token to its negation cue(s) and determine
overlaps. Our algorithm works by first determining
the overlaps among negation cues. Cue A negates
cue B if the following conditions are met:
? B is to the right of A.
? There are no tokens labeled with S between A
and B.
? Token distance between A and B does not ex-
ceed 10.
In the example in Figure 1, the overlapping condi-
tion holds for never and without but not for without
and no, because of the punctuation between them.
The token distance threshold of 10 was determined
empirically on CDT. In order to assign in-scope to-
kens to their respective cue, tokens labeled with N
are treated as follows:
? Assign each token T to the closest negation cue
A with no S-labeled tokens or punctuation sep-
arating it from T.
? If A was found to be negated by cue B, assign
T to B as well.
? If T is labeled with E by the event classifier,
mark it as an event.
F1
Configuration Closed Open
(A) O, N, CUE, MCUE, E, S 64.85 66.41Dependency Features
(B) O, N, CUE, MCUE, E, S 59.35 59.35No Dependency Features
(C) O, N, CUE, E 62.69 63.24Dependency Features
(D) O, N, CUE, E 56.44 56.44No Dependency Features
Table 4: Full negation results on CDD with gold cues.
This algorithm yields the correct annotations for
the example in Figure 1; when applied to label se-
quences originating from the gold scopes in CDD,
the reported F1 is 95%. We note that this loss of in-
formation could have been avoided by presenting the
CRF with a version of a sentence for each negation
cue. Then, when labeling new sentences, the model
could be applied repeatedly (based on the number of
cues provided by the cue detection system). How-
ever, training with multiple instances of the same
sentence could result in a dilution of the evidence
needed for scope labeling; this remains to be inves-
tigated in future work.
4 Development results
To investigate the effects of the augmented set of la-
bels and that of dependency features comparatively,
we present four different configurations of our sys-
tem in Table 4, using F1 for the stricter score that
counts perfect-match negation resolution for each
negation cue. Comparing (B) and (D), we observe
that explicitly encoding significant tokens with extra
labels does improve the performance of the classi-
fier. Comparing (A) to (B) and (C) to (B) shows the
effect of the dependency features with and without
the augmented set of labels. With (A) being our top
performing system and (D) a kind of internal base-
line, we observe that the combined effects of the la-
bels and dependency features is beneficial, with a
margin of about 8 and 10 percentage points for our
closed and open track systems respectively.
Table 5 presents the results for scope resolution on
CDD with gold cues. Interestingly, the constituent
324
Closed Open
Prec Rec F1 Prec Rec F1
Scopes 100.00 70.24 82.52 100.00 66.67 80.00
Scope Tokens 94.69 82.16 87.98 90.64 81.36 85.75
Negated 82.47 72.07 76.92 83.65 77.68 80.55
Full negation 100.00 47.98 64.85 100.00 49.71 66.41
Table 5: Results for scope resolution on CDD with gold cues.
trees converted to Stanford dependencies used in the
closed track outperform the open system employing
Maltparser on scopes, while for negated events the
latter is over 5 percentage points better than the for-
mer, as shown in Table 5.
4.1 Error analysis
We performed a manual error analysis of the scope
resolution on the development data using gold cue
information. Since our system does not deal specifi-
cally with discontinuous scopes, and seeing that we
are employing a sequence classifier with a fairly lo-
cal window, we are not surprised to find that a sub-
stantial portion of the errors are caused by discon-
tinuous scopes. In fact, in our closed track system,
these errors amount to 34% of the total number of
errors. Discontinuous scopes, as in (3) below, ac-
count for 9.3% of all scopes in CDD and the closed
task system does not analyze any of these correctly,
whereas the open system correctly analyzes one dis-
continuous scope.
(3) I therefore spent the day at my club and did not
return to Baker Street until evening.
A similar analysis with respect to event detection
on gold scope information indicated that errors are
mostly due to either predicting an event for a non-
factual context (false positive) or not predicting an
event for a factual context (false negative), i.e., there
are relatively few instances of predicting the wrong
token for a factual context (which result in both a
false negative and a false positive). This suggests
that the CRF has learned what tokens should be la-
beled as an event for a negation, but has not learned
so well how to determine whether the negation is
factual or non-factual. In this respect it may be that
incorporating information from a separate and dedi-
cated component for factuality detection ? as in the
system of Read et al (2012) ? could yield improve-
ments for the CRF event model.
5 Held-out evaluation
Final results on held-out data for both closed and
open track submissions are reported in Table 6. For
the final run, we trained our systems on CDTD. We
observe a similar relative performance to our devel-
opment results, with the open track system outper-
forming the closed track one, albeit by a smaller
margin than what we saw in development. We are
also surprised to see that despite not addressing dis-
continuous scopes directly, our system obtained the
best score on scope resolution (according to the met-
ric dubbed ?Scopes (cue match)?).
6 Conclusions and future work
This paper has provided an overview of our system
submissions for the *SEM 2012 shared task on re-
solving negation. This involves the subtasks of iden-
tifying negations cues, identifying the in-sentence
scope of these cues, as well as identifying negated
(and factual) events. While a simple SVM token
classifier is applied for the cue detection task, we ap-
ply CRF sequence classifiers for predicting scopes
and events. For the CRF models we experimented
with a fine-grained set of labels and a wide range of
feature types, drawing heavily on information from
dependency structures. We have detailed two dif-
ferent system configurations ? one submitted for
the open track and another for the closed track ?
and the two configurations only differ with respect
to the source used for the dependency parses: For
the closed track submission we simply converted
the constituent structures provided in the shared task
data to Stanford dependencies, while for the open
track we apply the Maltparser. For the end-to-end
evaluation, our submission was ranked first in the
open track and third in the closed track. The system
also had the best performance for each individual
sub-task in the open track, as well as being among
325
Closed Open
Prec Rec F1 Prec Rec F1
Cues 89.17 93.56 91.31 89.17 93.56 91.31
Scopes 85.71 62.65 72.39 85.71 62.65 72.39
Scope Tokens 86.03 81.55 83.73 82.25 82.16 82.20
Negated 68.18 52.63 59.40 66.90 57.40 61.79
Full negation 78.26 40.91 53.73 78.72 42.05 54.82
Cues B 86.97 93.56 90.14 86.97 93.56 90.14
Scopes B 59.32 62.65 60.94 59.54 62.65 61.06
Negated B 67.16 52.63 59.01 63.82 57.40 60.44
Full negation B 38.03 40.91 39.42 39.08 42.05 40.51
Table 6: End-to-end results on the held-out data.
the top-performers on the scope resolution sub-task
across both tracks.
Due to time constraints we were not able to di-
rectly address discontinuous scopes in our system.
For future work we plan on looking for ways to
tackle this problem by taking advantage of syntac-
tic information, both in the classification and in the
post-processing steps. We are also interested in de-
veloping the CRF-internal label-set to include more
informative labels. We also want to test the sys-
tem design developed for this task on other corpora
annotated for negation (or other related phenom-
ena such as speculation), as well as perform extrin-
sic evaluation of our system as a sub-component to
other NLP tasks such as sentiment analysis or opin-
ion mining. Lastly, we would like to try training
separate classifiers for affixal and token-level cues,
given that largely separate sets of features are effec-
tive for the two cases.
Acknowledgements
We thank colleagues at the University of Oslo, and
in particular Johan Benum Evensberget and Arne
Skj?rholt for fruitful discussions and suggestions.
We also thank the anonymous reviewers for their
helpful feedback.
References
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: Learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop On
Negation and Speculation in Natural Language Pro-
cessing, pages 51?59.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):245?288.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513. As-
sociation for Computational Linguistics.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In CoNLL ?09: Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning, pages 21?29. Association for Com-
putational Linguistics.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation, Istanbul.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope: Guidelines v1.0. Technical report, Univer-
sity of Antwerp. CLIPS: Computational Linguistics
& Psycholinguistics technical report series.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation,
pages 2216?2219.
Jonathon Read, Erik Velldal, Lilja ?vrelid, and Stephan
Oepen. 2012. UiO1: Constituent-based discrimina-
tive ranking for negation resolution. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics, Montreal. Submission under review.
326
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers and the role of syntax. Computational Lin-
guistics, 38(2).
Erik Velldal. 2011. Predicting speculation: A simple dis-
ambiguation approach to hedge detection in biomedi-
cal literature. Journal of Biomedical Semantics, 2(5).
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: Biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 (Suppl. 11).
327
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 56?60,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Predicting Party Affiliations from European Parliament Debates
Bj?rn H?yland
Department of Political Science
University of Oslo
bjorn.hoyland@stv.uio.no
Jean-Franc?ois Godbout
Department of Political Science
University of Montreal
godboutj@umontreal.ca
Emanuele Lapponi
Department of Informatics
University of Oslo
emanuel@ifi.uio.no
Erik Velldal
Department of Informatics
University of Oslo
erikve@ifi.uio.no
Abstract
This paper documents an ongoing effort
to assess whether party group affiliation
of participants in European Parliament de-
bates can be automatically predicted on
the basis of the content of their speeches,
using a support vector machine multi-class
model. The work represents a joint effort
between researchers within Political Sci-
ence and Language Technology.
1 Introduction
The European Parliament (EP) is the directly
elected parliamentary institution of the European
Union (EU), elected once every five years by vot-
ers from across all 28 member states. An im-
portant arena for the political activity in the EP
is the plenary sittings, the forum where all (cur-
rently 766) elected members of the European Par-
liament (MEPs) from all member states participate
in plenary debates (in all represented languages,
simultaneously translated). Our current work in-
vestigates to what extent the party affiliation of
the legislators in the plenary debates can be pre-
dicted on the basis of their speeches. More specif-
ically, the goal is to predict the party affiliation of
plenary speakers in the 6
th
European Parliament
(July 2004 ? July 2009) on the basis of the party
affiliations of plenary speakers in the 5
th
Euro-
pean Parliament (July 1999 ? July 2004).
1
One
1
The data have been collected from the official website of
the European Parliament, where verbatim reports from each
plenary sitting are published:
www.europarl.europa.eu/plenary/en/debates.html
premise for the success of such an approach is that
differences in ideology and belief systems are re-
flected in differences in choice of words in ple-
nary debates. Another premise is that a shared be-
lief system translates to the same choice of party
group. As discussed below, systematic differences
in prediction performance in the data can be used
to reveal interesting differences in the extent to
which these premises hold for various subgroups
of MEPs. While this is further discussed in Sec-
tion 4, we first describe the data sets in some more
detail in Section 2 and present some preliminary
results in Section 3.
2 Data sets and experimental set-up
The debates from the 5
th
EP are used for train-
ing an SVM(Cortes and Vapnik, 1995) multi-
class classifier
2
which we then apply for predict-
ing party affiliations in the 6
th
EP. We do 5-fold
cross validation experiments on the 5
th
term for
model tuning. Data points in the model cor-
respond to speakers; participants in the debates
in the EP labeled with their political party. All
recorded speeches for a given speaker are con-
flated in a single vector. Although we can so far
only report results for models using fairly basic
feature types ? various bag-of-words configura-
tions based on lemmas, stems or full word-forms ?
combined with more linguistically informed ones,
2
We use the freely available SVM
multiclass
toolkit, im-
plementing the multi-class formulation of support vector ma-
chines described by Crammer and Singer (2001) with very
fast optimization for linear kernels based on the algorithm
for estimating Structural SVMs described by Tsochantaridis
et al. (2004). For more information see; www.cs.cornell.
edu/people/tj/svm_light/svm_multiclass.html
56
Party MEPs
PSE 211
EPP-ED 272
ELDR 65
GUE/NGL 48
V/ALE 55
UEN 38
Total 689
Table 1: Distribution of members across the var-
ious political parties in the training data from the
5
th
European Parliament plenary debates.
like part-of-speech (PoS) tags and dependency re-
lations; work is already in progress with respect
to assessing the usefulness of e.g. class-based fea-
tures drawn from unsupervised word clustering
and modeling semantic phenomena such as nega-
tion, speculation and sentiment. Large-scale ex-
perimentation with different features sets and hy-
perparameters is made possible by running ex-
periments on a large high-performance computing
(HPC) cluster.
The main political groups in the European Par-
liament during these terms were the Christian
Democratic / Conservative (EPP-ED), the Social-
Democratic (PES), the Liberal (ELDR), the Green
(V/ALE), the Socialists (GUE/NGL), and Right
(UEN). Note that experiments only focus on the
six largest political parties, excluding the smaller
and more marginal ones which are often more un-
stable or ad-hoc configurations, including inde-
pendent MEPs with various forms of Anti-EU ide-
ologies. To give an idea of class distribution, the
number of MEPs for all parties in our training set
is listed in Table 1. Our 5
th
EP term training set
comprises a total of 689 MEPs while our 6
th
term
test set comprises 818. It is worth pointing out that
in the 6
th
EP, roughly 75% of the data corresponds
to MEPs from the old member states while 25%
test are MEPs from the new member states. Of
the members from the old member states, roughly
53% of the MEPs are incumbents while the re-
mainders are freshmen (we return to this property
in Section 4 below).
In order to facilitate reproducibility of reported
results and foster further research, all data sets are
available for download, including party labels and
Baseline stem dep/stem
Acc 0.394 0.476 0.492
Prec 0.065 0.439 0.458
Rec 0.166 0.399 0.393
F
1
0.094 0.418 0.423
Table 3: Results of the 5-fold cross-validation ex-
periments on the training data for the majority-
class baseline, a model trained on stems and one
enriched with dependency-disambiguated stems.
all (automatic) linguistic annotations.
3
3 Preliminary results
In addition to reporting results for the SVM classi-
fier, we also include figures for a simple majority-
class baseline approach, i.e., simply assigning all
MEPs in the test set to the largest political party,
EPP-ED. For evaluating the various approaches
we will be reporting precision (Prec), recall (Rec)
and F
1
for each individual class/party, in addi-
tion to the corresponding macro-averaged scores
across all parties. Note that for one-of classifi-
cation problems like the one we are dealing with
here, micro-averaging would simply correspond to
accuracy (with Prec = Rec = F
1
, since the number
of false positives and false negatives would be the
same). While we also report accuracy, it is worth
bearing in mind that accuracy will overemphasize
performance on the large classes when working
with skewed class distributions like we do here.
In order to study the effects of different surface
features and classifier-tuning, we conduct a num-
ber of 5-fold cross-validation experiments on the
training data using different feature combinations,
for each empirically determining the best value
for the C-parameter (i.e., the regularization param-
eter of the SVM classifier, governing the trade-
off between training error and margin size). In
this initial experiments we trained different mod-
els with various configurations of non-normalized
tokens (i.e., observed word forms), stems, lemmas
and PoS- and dependency-disambiguated tokens
and stems. The best performing configuration so
far turns out to be the dependency disambiguated
stems with the observed optimal C-value of 0.8,
with F
1
over two percentage points higher than
the model trained on stems alone at the same C-
3
Downloadable at http://emanuel.at.ifi.uio.
no/debates_data.tar.gz
57
PSE EPP-ED ELDR GUE/NGL V/ALE UEN total
PSE 111 36 15 9 6 1 178
EPP-ED 123 286 77 13 12 31 542
ELDR 3 3 7 0 1 0 14
GUE/NGL 2 0 1 18 1 0 22
V/ALE 3 2 2 5 25 1 38
UEN 7 9 3 1 8 4 32
total 249 336 105 46 53 37 826
Acc 0.445 0.851 0.066 0.391 0.471 0.108 0.551
Prec 0.623 0.527 0.500 0.818 0.657 0.166 0.549
Rec 0.445 0.851 0.066 0.391 0.555 0.108 0.403
F
1
0.519 0.651 0.117 0.529 0.602 0.131 0.464
Table 2: Confusion matrix showing predicted (horizontal) and true (vertical) party affiliations, together
with accuracy, precision, recall and F
1
scores for system predictions. Overall accuracy and macro-
averaged precision, recall and F
1
(presented in bold text) can be compared to majority-class baseline
results of Acc=0.410, Prec=0.068, Rec=0.166 and F
1
=0.097.
value point (see Table 3 for details). This indicates
that linguistically informed features do provide the
model with relevant information.
Results obtained by applying the best-
performing configuration from our development
experiments to the test data are presented in
Table 2, together with a confusion matrix for
the classifier assignments. Party-wise F
1
and
accuracy scores in addition to overall accuracy
and macro-averaged precision, recall and F
1
are shown in the bottom four rows; compare
values in bold text to majority-class baseline
results of Acc=0.407, Prec=0.069, Rec=0.166 and
F
1
=0.097. There are two groups with compara-
tively poor prediction scores, the Liberal (ELDR)
and the Right (UEN). In the case of the former,
there are two key factors that may account for
this: (1) Ideological compositions of the group
and, (2) coalition-formation in the EP. Firstly,
ELDR consists of delegations from national
parties that tend to locate themselves between
the Social-Democratic (PES) and the Christian-
Democratic / Conservative parties (EPP-ED) at
the national arena. Due to differences in the
ideological landscape across EU member states,
some members of the ELDR may find themselves
holding views that are closer to those held by
PES or EPP-ED than ELDR representatives from
some countries. Secondly, in the period under
investigation, ELDR tended to form coalitions
with the EPP-ED on some policy areas and with
the PES on others. As MEPs mainly speak on
policies related to the Committees they serve
on, misclassifications as PES or EPP-ED may
be a reflection of the coaltion-formation on the
committees they served on (Hix and H?yland,
2013). When it comes to UEN, misclassification
as EPP-ED may be explained in terms of shared
ideology. In some cases, the membership of UEN
rather than EPP-ED is due to historical events
rather than ideological considerations (McElroy
and Benoit, 2012).
4 Research questions
This section briefly outlines some of the questions
we will be focusing on in the ongoing work of an-
alyzing the predictions of the SVM classifier in
more detail. In most cases this analysis will con-
sist of comparing prediction performance across
various relevant subsets of MEPs while looking
for systematic differences.
Contribution of linguistic features Much of
the work done to date in ?text-as-data? approaches
in social sciences has been based on relatively sim-
ple and surface oriented features, typically bag-
of-words models, perhaps combined with term
weighting and stemming for word normalization
(for an overview of what is currently considered
best practice, see Grimmer and Stewart (2013)).
Much of the methodology can be seen as imports
from the fields of information retrieval and data
mining rather than natural language processing.
58
A relevant question for the current work is the
extent to which more linguistically informed fea-
tures can contribute to the task of predicting po-
litical affiliation, compared to ?surface features?
based solely on directly observable lexical infor-
mation. One of our goals is to asses whether more
accurate models can be built by including richer
feature sets with more linguistically informed fea-
tures, like part-of-speech (PoS) tags, dependency
relations, class-based features drawn from unsu-
pervised word clustering, negation scope analysis,
and more. The preliminary results already demon-
strates that linguistically motivated features can
be useful for the current task, but there are still
many more feature types and combinations to be
explored.
Differences between new and old member
states Ten countries joined the European Union
in 2004. This offered a rare opportunity for the ex-
isting party groups to substantively increase their
share of the seats in the European Parliament by
recruiting national party delegations from the new
member states. As most of the new member states
have a relative short history of competitive mul-
tiparty system, there were weaker ties between
parties in new and old member states when com-
pared to previous rounds of enlargement. Since
the allocation of office spoils in the EP is fairly
proportional among party groups it was assumed
that national parties from the new member states
? less ideologically committed to any of the be-
lief systems held by the traditional Western Euro-
pean party families ? would shift the allocation of
some offices in the EP by opting to join certain
party group who controlled a larger share of min-
isterial portfolios. If there are large differences
in classifier performance between members from
new and old members states, this can provide sup-
port for the hypothesis that national party delega-
tions from new member states joined the existing
party groups for other reasons than simply shared
ideological beliefs and goals. H?yland and God-
bout (2008) presented similar preliminary results
that already hint at this tendency. The ongoing
collaboration will further explore this question by
targeting it in new ways.
Differences between incumbents and freshmen
MEPs This point is tightly connected to the pre-
vious. Given that our training and testing data are
correspond to distinct consecutive terms of parlia-
ment, one should determine whether any differ-
ences in prediction performance for MEPs from
new and old member states can be explained sim-
ply by the fact that the latter will include many
MEPs that appear both in the training and the test
data (i.e., speakers participating in the debates in
both the 5
th
and 6
th
term). In order to factor out
any such ?incumbency effect?, we will also in-
vestigate whether any differences can be found in
prediction performance between incumbents and
?freshmen? (members who joined the EP after
the 2004 elections) originating from old member
states only.
Differences between political domains An-
other effect we would like to measure is whether
there are any systematic differences in prediction
performance across different political topics or do-
mains. Among other things this could indicate that
the language use is more politicized or ideologi-
cally charged in debates concerning certain politi-
cal issues. Much of the work in the European Par-
liament is carried out in specialized committees
that prepare reports that will later be debated and
voted on in the plenary. By coupling the debates
with information about which legislative standing
committee has handled each particular case, we
would be able to automatically break down our
results according to political domain. This could
be achieved using a resource like that described
by H?yland et al. (2009). Examples of commit-
tee domains include Foreign Affairs, International
Trade, Legal Affairs, Regional Development, Eco-
nomic and Monetary Affairs, and Internal Market
and Consumer Protection, to name a few. Another
possibility here would be to train separate special-
ized classifiers for debates falling within the do-
main of each specialized committee directly.
5 Summary
This paper has outlined an interdisciplinary effort
to explore whether the recorded speeches from the
plenary debates of the European Parliament can be
utilized by an SVM classifier to correctly predict
the party affiliations of the participants. Prelim-
inary experimental results already demonstrates
that such predictions can indeed be made ? also
demonstrating the contribution of linguistically in-
formed features ? and the paper has outlined a
number of related research questions currently be-
ing pursued in ongoing work.
59
References
[Cortes and Vapnik1995] Corinna Cortes and Vladimir
Vapnik. 1995. Support-vector networks. Machine
Learning, 20(3):273?297, September.
[Crammer and Singer2001] Koby Crammer and Yoram
Singer. 2001. On the algorithmic implementation of
multiclass kernel-based vector machines. Journal of
Machine Learning Research, 2:265?292, December.
[Grimmer and Stewart2013] Justin Grimmer and Bran-
don M. Stewart. 2013. Text as data: The promise
and pitfalls of automatic content analysis methods
for political texts. Political Analysis, 21(3):267?
297.
[Hix and H?yland2013] Simon Hix and Bj?rn H?yland.
2013. The empowerment of the european parlia-
ment. Annual Review of Political Science, 16:171?
189.
[H?yland and Godbout2008] Bj?rn H?yland and Jean-
Franc?ois Godbout. 2008. Lost in translation? pre-
dicting party group affiliation from european parlia-
ment debates. In On-line Proceedings of the Fourth
Pan-European Conference on EU Politics.
[H?yland et al.2009] Bj?rn H?yland, Indraneel Sircar,
and Simon Hix. 2009. Forum section: an auto-
mated database of the european parliament. Euro-
pean Union Politics, 10(1):143?152.
[McElroy and Benoit2012] Gail McElroy and Kenneth
Benoit. 2012. Policy positioning in the european
parliament. European Union Politics, 13(1):150?
167.
[Tsochantaridis et al.2004] Ioannis Tsochantaridis,
Thomas Hofmann, Thorsten Joachims, and Yasemin
Altun. 2004. Support vector machine learning for
interdependent and structured output spaces. In
Proceedings of the 21st International Conference
on Machine Learning.
60
