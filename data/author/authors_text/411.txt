Proceedings of the 43rd Annual Meeting of the ACL, pages 255?262,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Scaling Phrase-Based Statistical Machine Translation
to Larger Corpora and Longer Phrases
Chris Callison-Burch Colin Bannard
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
{chris,colin}@linearb.co.uk
Josh Schroeder
Linear B Ltd.
39 B Cumberland Street
Edinburgh EH3 6RA
josh@linearb.co.uk
Abstract
In this paper we describe a novel data
structure for phrase-based statistical ma-
chine translation which allows for the re-
trieval of arbitrarily long phrases while si-
multaneously using less memory than is
required by current decoder implementa-
tions. We detail the computational com-
plexity and average retrieval times for
looking up phrase translations in our suf-
fix array-based data structure. We show
how sampling can be used to reduce the
retrieval time by orders of magnitude with
no loss in translation quality.
1 Introduction
Statistical machine translation (SMT) has an advan-
tage over many other statistical natural language
processing applications in that training data is reg-
ularly produced by other human activity. For some
language pairs very large sets of training data are
now available. The publications of the European
Union and United Nations provide gigbytes of data
between various language pairs which can be eas-
ily mined using a web crawler. The Linguistics
Data Consortium provides an excellent set of off
the shelf Arabic-English and Chinese-English paral-
lel corpora for the annual NIST machine translation
evaluation exercises.
The size of the NIST training data presents a prob-
lem for phrase-based statistical machine translation.
Decoders such as Pharaoh (Koehn, 2004) primarily
use lookup tables for the storage of phrases and their
translations. Since retrieving longer segments of hu-
man translated text generally leads to better trans-
lation quality, participants in the evaluation exer-
cise try to maximize the length of phrases that are
stored in lookup tables. The combination of large
corpora and long phrases means that the table size
can quickly become unwieldy.
A number of groups in the 2004 evaluation exer-
cise indicated problems dealing with the data. Cop-
ing strategies included limiting the length of phrases
to something small, not using the entire training data
set, computing phrases probabilities on disk, and fil-
tering the phrase table down to a manageable size
after the testing set was distributed. We present a
data structure that is easily capable of handling the
largest data sets currently available, and show that it
can be scaled to much larger data sets.
In this paper we:
? Motivate the problem with storing enumerated
phrases in a table by examining the memory re-
quirements of the method for the NIST data set
? Detail the advantages of using long phrases in
SMT, and examine their potential coverage
? Describe a suffix array-based data structure
which allows for the retrieval of translations
of arbitrarily long phrases, and show that it re-
quires far less memory than a table
? Calculate the computational complexity and
average time for retrieving phrases and show
how this can be sped up by orders of magnitude
with no loss in translation accuracy
2 Related Work
Koehn et al (2003) compare a number of differ-
ent approaches to phrase-based statistical machine
255
length num uniq
(mil)
average #
translations
avg trans
length
1 .88 8.322 1.37
2 16.5 1.733 2.35
3 42.6 1.182 3.44
4 58.7 1.065 4.58
5 65.0 1.035 5.75
6 66.4 1.022 6.91
7 65.8 1.015 8.07
8 64.3 1.012 9.23
9 62.2 1.010 10.4
10 59.9 1.010 11.6
Table 1: Statistics about Arabic phrases in the NIST-
2004 large data track.
translation including the joint probability phrase-
based model (Marcu and Wong, 2002) and a vari-
ant on the alignment template approach (Och and
Ney, 2004), and contrast them to the performance of
the word-based IBM Model 4 (Brown et al, 1993).
Most relevant for the work presented in this paper,
they compare the effect on translation quality of us-
ing various lengths of phrases, and the size of the
resulting phrase probability tables.
Tillmann (2003) further examines the relationship
between maximum phrase length, size of the trans-
lation table, and accuracy of translation when in-
ducing block-based phrases from word-level align-
ments. Venugopal et al (2003) and Vogel et al
(2003) present methods for achieving better transla-
tion quality by growing incrementally larger phrases
by combining smaller phrases with overlapping seg-
ments.
3 Scaling to Long Phrases
Table 1 gives statistics about the Arabic-English par-
allel corpus used in the NIST large data track. The
corpus contains 3.75 million sentence pairs, and has
127 million words in English, and 106 million words
in Arabic. The table shows the number of unique
Arabic phrases, and gives the average number of
translations into English and their average length.
Table 2 gives estimates of the size of the lookup
tables needed to store phrases of various lengths,
based on the statistics in Table 1. The number of
unique entries is calculated as the number unique
length entries
(mil)
words
(mil)
memory
(gigs)
including
alignments
1 7.3 10 .1 .11
2 36 111 .68 .82
3 86 412 2.18 2.64
4 149 933 4.59 5.59
5 216 1,645 7.74 9.46
6 284 2,513 11.48 14.07
7 351 3,513 15.70 19.30
8 416 4,628 20.34 25.05
9 479 5,841 25.33 31.26
10 539 7,140 30.62 37.85
Table 2: Estimated size of lookup tables for the
NIST-2004 Arabic-English data
length coverage length coverage
1 93.5% 6 4.70%
2 73.3% 7 2.95%
3 37.1% 8 2.14%
4 15.5% 9 1.99%
5 8.05% 10 1.49%
Table 3: Lengths of phrases from the training data
that occur in the NIST-2004 test set
phrases times the average number of translations.
The number of words in the table is calculated as the
number of unique phrases times the phrase length
plus the number of entries times the average transla-
tion length. The memory is calculated assuming that
each word is represented with a 4 byte integer, that
each entry stores its probability as an 8 byte double
and that each word alignment is stored as a 2 byte
short. Note that the size of the table will vary de-
pending on the phrase extraction technique.
Table 3 gives the percent of the 35,313 word long
test set which can be covered using only phrases of
the specified length or greater. The table shows the
efficacy of using phrases of different lengths. The ta-
ble shows that while the rate of falloff is rapid, there
are still multiple matches of phrases of length 10.
The longest matching phrase was one of length 18.
There is little generalization in current SMT imple-
mentations, and consequently longer phrases gener-
ally lead to better translation quality.
256
3.1 Why use phrases?
Statistical machine translation made considerable
advances in translation quality with the introduction
of phrase-based translation. By increasing the size
of the basic unit of translation, phrase-based ma-
chine translation does away with many of the prob-
lems associated with the original word-based for-
mulation of statistical machine translation (Brown
et al, 1993), in particular:
? The Brown et al (1993) formulation doesn?t
have a direct way of translating phrases; instead
they specify a fertility parameter which is used
to replicate words and translate them individu-
ally.
? With units as small as words, a lot of reordering
has to happen between languages with different
word orders. But the distortion parameter is a
poor explanation of word order.
Phrase-based SMT overcomes the first of these
problems by eliminating the fertility parameter
and directly handling word-to-phrase and phrase-to-
phrase mappings. The second problem is alleviated
through the use of multi-word units which reduce
the dependency on the distortion parameter. Less
word re-ordering need occur since local dependen-
cies are frequently captured. For example, common
adjective-noun alternations are memorized. How-
ever, since this linguistic information is not encoded
in the model, unseen adjective noun pairs may still
be handled incorrectly.
By increasing the length of phrases beyond a
few words, we might hope to capture additional
non-local linguistic phenomena. For example, by
memorizing longer phrases we may correctly learn
case information for nouns commonly selected by
frequently occurring verbs; we may properly han-
dle discontinuous phrases (such as French negation,
some German verb forms, and English verb particle
constructions) that are neglected by current phrase-
based models; and we may by chance capture some
agreement information in coordinated structures.
3.2 Deciding what length of phrase to store
Despite the potential gains from memorizing longer
phrases, the fact remains that as phrases get longer
length coverage length coverage
1 96.3% 6 21.9%
2 94.9% 7 11.2%
3 86.1% 8 6.16%
4 65.6% 9 3.95%
5 40.9% 10 2.90%
Table 4: Coverage using only repeated phrases of
the specified length
there is a decreasing likelihood that they will be re-
peated. Because of the amount of memory required
to store a phrase table, in current implementations a
choice is made as to the maximum length of phrase
to store.
Based on their analysis of the relationship be-
tween translation quality and phrase length, Koehn
et al (2003) suggest limiting phrase length to three
words or less. This is entirely a practical sugges-
tion for keeping the phrase table to a reasonable
size, since they measure minor but incremental im-
provement in translation quality up to their maxi-
mum tested phrase length of seven words.1
Table 4 gives statistics about phrases which oc-
cur more than once in the English section of the Eu-
roparl corpus (Koehn, 2002) which was used in the
Koehn et al (2003) experiments. It shows that the
percentage of words in the corpus that can be cov-
ered by repeated phrases falls off rapidly at length
6, but that even phrases up to length 10 are able to
cover a non-trivial portion of the corpus. This draws
into question the desirability of limiting phrase re-
trieval to length three.
The decision concerning what length of phrases
to store in the phrase table seems to boil down to
a practical consideration: one must weigh the like-
lihood of retrieval against the memory needed to
store longer phrases. We present a data structure
where this is not a consideration. Our suffix array-
based data structure allows the retrieval of arbitrar-
ily long phrases, while simultaneously requiring far
less memory than the standard table-based represen-
tation.
1While the improvements to translation quality reported in
Koehn et al (2003) are minor, their evaluation metric may not
have been especially sensitive to adding longer phrases. They
used the Bleu evaluation metric (Papineni et al, 2002), but
capped the n-gram precision at 4-grams.
257
01
2
3
4
5
6
7
8
9
spain declined to confirm that spain declined to aid morocco
declined to confirm that spain declined to aid morocco
to confirm that spain declined to aid morocco
confirm that spain declined to aid morocco
that spain declined to aid morocco
spain declined to aid morocco
declined to aid morocco
to aid morocco
aid morocco
morocco
spain declined to confirm that spain declined aidto morocco
0 1 2 3 4 5 6 87 9
s[0]
s[1]
s[2]
s[3]
s[4]
s[5]
s[6]
s[7]
s[8]
s[9]
Initialized, unsorted
Suffix Array
Suffixes denoted by s[i]
Corpus
Index of
words:
Figure 1: An initialized, unsorted suffix array for a
very small corpus
4 Suffix Arrays
The suffix array data structure (Manber and Myers,
1990) was introduced as a space-economical way of
creating an index for string searches. The suffix ar-
ray data structure makes it convenient to compute
the frequency and location of any substring or n-
gram in a large corpus. Abstractly, a suffix array is
an alphabetically-sorted list of all suffixes in a cor-
pus, where a suffix is a substring running from each
position in the text to the end. However, rather than
actually storing all suffixes, a suffix array can be
constructed by creating a list of references to each
of the suffixes in a corpus. Figure 1 shows how a
suffix array is initialized for a corpus with one sen-
tence. Each index of a word in the corpus has a cor-
responding place in the suffix array, which is identi-
cal in length to the corpus. Figure 2 shows the final
state of the suffix array, which is as a list of the in-
dices of words in the corpus that corresponds to an
alphabetically sorted list of the suffixes.
The advantages of this representation are that it is
compact and easily searchable. The total size of the
suffix array is a constant amount of memory. Typ-
ically it is stored as an array of integers where the
array is the same length as the corpus. Because it is
organized alphabetically, any phrase can be quickly
located within it using a binary search algorithm.
Yamamoto and Church (2001) show how to use
suffix arrays to calculate a number of statistics that
are interesting in natural language processing appli-
cations. They demonstrate how to calculate term fre-
8
3
6
1
9
5
0
4
7
2
to aid morocco
to confirm that spain declined to aid morocco
morocco
spain declined to aid morocco
declined to confirm that spain declined to aid morocco
declined to aid morocco
confirm that spain declined to aid morocco
aid morocco
that spain declined to aid morocco
spain declined to confirm that spain declined to aid morocco
Sorted
Suffix Array
Suffixes denoted by s[i]
s[0]
s[1]
s[2]
s[3]
s[4]
s[5]
s[6]
s[7]
s[8]
s[9]
Figure 2: A sorted suffix array and its corresponding
suffixes
quency / inverse document frequency (tf / idf) for all
n-grams in very large corpora, as well as how to use
these frequencies to calculate n-grams with high mu-
tual information and residual inverse document fre-
quency. Here we show how to apply suffix arrays to
parallel corpora to calculate phrase translation prob-
abilities.
4.1 Applied to parallel corpora
In order to adapt suffix arrays to be useful for sta-
tistical machine translation we need a data structure
with the following elements:
? A suffix array created from the source language
portion of the corpus, and another created from
the target language portion of the corpus,
? An index that tells us the correspondence be-
tween sentence numbers and positions in the
source and target language corpora,
? An alignment a for each sentence pair in the
parallel corpus, where a is defined as a subset
of the Cartesian product of the word positions
in a sentence e of length I and a sentence f of
length J :
a ? {(i, j) : i = 1...I; j = 1...J}
? A method for extracting the translationally
equivalent phrase for a subphrase given an
aligned sentence pair containing that sub-
phrase.
The total memory usage of the data structure is
thus the size of the source and target corpora, plus
the size of the suffix arrays (identical in length to the
258
corpora), plus the size of the two indexes that cor-
relate sentence positions with word positions, plus
the size of the alignments. Assuming we use ints
to represent words and indices, and shorts to repre-
sent word alignments, we get the following memory
usage:
2 ? num words in source corpus ? sizeof(int)+
2 ? num words in target corpus ? sizeof(int)+
2 ? number sentence pairs ? sizeof(int)+
number of word alignments ? sizeof(short)
The total amount of memory required to store the
NIST Arabic-English data using this data structure
is
2 ? 105,994,774 ? sizeof(int)+
2 ? 127,450,473 ? sizeof(int)+
2 ? 3,758,904 ? sizeof(int)+
92,975,229 ? sizeof(short)
Or just over 2 Gigabytes.
4.2 Calculating phrase translation
probabilities
In order to produce a set of phrase translation prob-
abilities, we need to examine the ways in which
they are calculated. We consider two common ways
of calculating the translation probability: using the
maximum likelihood estimator (MLE) and smooth-
ing the MLE using lexical weighting.
The maximum likelihood estimator for the proba-
bility of a phrase is defined as
p(f? |e?) =
count(f? , e?)
?
f? count(f? , e?)
(1)
Where count(f? , e?) gives the total number of times
the phrase f? was aligned with the phrase e? in the
parallel corpus. We define phrase alignments as fol-
lows. A substring e? consisting of the words at po-
sitions l...m is aligned with the phrase f? by way of
the subalignment
s = a ? {(i, j) : i = l...m, j = 1...J}
The aligned phrase f? is the subphrase in f which
spans from min(j) to max(j) for j|(i, j) ? s.
The procedure for generating the counts that are
used to calculate the MLE probability using our suf-
fix array-based data structures is:
1. Locate all the suffixes in the English suffix ar-
ray which begin with the phrase e?. Since the
suffix array is sorted alphabetically we can eas-
ily find the first occurrence s[k] and the last oc-
currence s[l]. The length of the span in the suf-
fix array l?k+1 indicates the number of occur-
rences of e? in the corpus. Thus the denominator
?
f? count(f? , e?) can be calculated as l ? k + 1.
2. For each of the matching phrases s[i] in the
span s[k]...s[l], look up the value of s[i] which
is the word index w of the suffix in the English
corpus. Look up the sentence number that in-
cludes w, and retrieve the corresponding sen-
tences e and f , and their alignment a.
3. Use a to extract the target phrase f? that aligns
with the phrase e? that we are searching for. In-
crement the count for < f?, e? >.
4. Calculate the probability for each unique
matching phrase f? using the formula in Equa-
tion 1.
A common alternative formulation of the phrase
translation probability is to lexically weight it as fol-
lows:
plw(f? |e?, s) =
n?
i=1
1
|{i|(i, j) ? s}|
?
?(i,j)?s
p(fj |ei)
(2)
Where n is the length of e?.
In order to use lexical weighting we would need
to repeat steps 1-4 above for each word ei in e?. This
would give us the values for p(fj |ei). We would fur-
ther need to retain the subphrase alignment s in or-
der to know the correspondence between the words
(i, j) ? s in the aligned phrases, and the total num-
ber of foreign words that each ei is aligned with
(|{i|(i, j) ? s}|). Since a phrase alignment < f?, e? >
may have multiple possible word-level alignments,
we retain a set of alignments S and take the maxi-
mum:
259
p(f? |e?, S) = p(f? |e?) ? argmax
s?S
plw(f? |e?, s) (3)
Thus our suffix array-based data structure can be
used straightforwardly to look up all aligned trans-
lations for a given phrase and calculate the proba-
bilities on-the-fly. In the next section we turn to
the computational complexity of constructing phrase
translation probabilities in this way.
5 Computational Complexity
Computational complexity is relevant because there
is a speed-memory tradeoff when adopting our data
structure. What we gained in memory efficiency
may be rendered useless if the time it takes to cal-
culate phrase translation probabilities is unreason-
ably long. The computational complexity of looking
up items in a hash table, as is done in current table-
based data structures, is extremely fast. Looking up
a single phrase can be done in unit time, O(1).
The computational complexity of our method has
the following components:
? The complexity of finding all occurrences of
the phrase in the suffix array
? The complexity of retrieving the associated
aligned sentence pairs given the positions of the
phrase in the corpus
? The complexity of extracting all aligned
phrases using our phrase extraction algorithm
? The complexity of calculating the probabilities
given the aligned phrases
The methods we use to execute each of these, and
their complexities are as follow:
? Since the array is sorted, finding all occur-
rences of the English phrase is extremely fast.
We can do two binary searches: one to find the
first occurrence of the phrase and a second to
find the last. The computational complexity is
therefore bounded by O(2 log(n)) where n is
the length of the corpus.
? We use a similar method to look up the sen-
tences ei and fi and word-level alignment ai
phrase freq O time (ms)
respect for the
dead
3 80 24
since the end of
the cold war
19 240 136
the parliament 1291 4391 1117
of the 290921 682550 218369
Table 5: Examples of O and calculation times for
phrases of different frequencies
that are associated with the position wi in the
corpus of each phrase occurrence e?i. The com-
plexity is O(k ? 2 log(m)) where k is the num-
ber of occurrences of e? and m is the number of
sentence pairs in the parallel corpus.
? The complexity of extracting the aligned phrase
for a single occurrence of e?i is O(2 log(|ai|) to
get the subphrase alignment si, since we store
the alignments in a sorted array. The complex-
ity of then getting f?i from si is O(length(f?i)).
? The complexity of summing over all aligned
phrases and simultaneously calculating their
probabilities is O(k).
Thus we have a total complexity of:
O(2 log(n) + k ? 2 log(m) (4)
+
e?1...e?k?
ai,f?i|e?i
(2 log(|ai|) + length(f?i)) + k) (5)
for the MLE estimation of the translation probabil-
ities for a single phrase. The complexity is domi-
nated by the k terms in the equation, when the num-
ber of occurrences of the phrase in the corpus is
high. Phrases with high frequency may cause exces-
sively long retrieval time. This problem is exacer-
bated when we shift to a lexically weighted calcula-
tion of the phrase translation probability. The com-
plexity will be multiplied across each of the compo-
nent words in the phrase, and the component words
themselves will be more frequent than the phrase.
Table 5 shows example times for calculating the
translation probabilities for a number of phrases. For
frequent phrases like of the these times get unaccept-
ably long. While our data structure is perfect for
260
overcoming the problems associated with storing the
translations of long, infrequently occurring phrases,
it in a way introduces the converse problem. It has
a clear disadvantage in the amount of time it takes
to retrieve commonly occurring phrases. In the next
section we examine the use of sampling to speed up
the calculation of translation probabilities for very
frequent phrases.
6 Sampling
Rather than compute the phrase translation proba-
bilities by examining the hundreds of thousands of
occurrences of common phrases, we instead sam-
ple from a small subset of the occurrences. It is
unlikely that we need to extract the translations of
all occurrences of a high frequency phrase in order
to get a good approximation of their probabilities.
We instead cap the number of occurrences that we
consider, and thus give a maximum bound on k in
Equation 5.
In order to determine the effect of different lev-
els of sampling, we compare the translation quality
against cumulative retrieval time for calculating the
phrase translation probabilities for all subphrases in
an evaluation set. We translated a held out set of
430 German sentences with 50 words or less into
English. The test sentences were drawn from the
01/17/00 proceedings of the Europarl corpus. The
remainder of the corpus (1 million sentences) was
used as training data to calculate the phrase trans-
lation probabilities. We calculated the translation
quality using Bleu?s modified n-gram precision met-
ric (Papineni et al, 2002) for n-grams of up to length
four. The framework that we used to calculate the
translation probabilities was similar to that detailed
in Koehn et al (2003). That is:
e? = argmax
eI1
p(eI1|f
I
1) (6)
= argmax
eI1
pLM (e
I
1) ? (7)
I?
i=1
p(f?i|e?i)d(ai ? bi?1)plw(f?i|e?i,a) (8)
Where pLM is a language model probability and d is
a distortion probability which penalizes movement.
Table 6 gives a comparison of the translation qual-
ity under different levels of sampling. While the ac-
sample size time quality
unlimited 6279 sec .290
50000 1051 sec .289
10000 336 sec .291
5000 201 sec .289
1000 60 sec .288
500 35 sec .288
100 10 sec .288
Table 6: A comparison of retrieval times and trans-
lation quality when the number of translations is
capped at various sample sizes
curacy fluctuates very slightly it essentially remains
uniformly high for all levels of sampling. There are
a number of possible reasons for the fact that the
quality does not decrease:
? The probability estimates under sampling are
sufficiently good that the most probable trans-
lations remain unchanged,
? The interaction with the language model prob-
ability rules out the few misestimated probabil-
ities, or
? The decoder tends to select longer or less fre-
quent phrases which are not affected by the
sampling.
While the translation quality remains essentially
unchanged, the cumulative time that it takes to cal-
culate the translation probabilities for all subphrases
in the 430 sentence test set decreases radically. The
total time drops by orders of magnitude from an hour
and a half without sampling down to a mere 10 sec-
onds with a cavalier amount of sampling. This sug-
gests that the data structure is suitable for deployed
SMT systems and that no additional caching need
be done to compensate for the structure?s computa-
tional complexity.
7 Discussion
The paper has presented a super-efficient data struc-
ture for phrase-based statistical machine translation.
We have shown that current table-based methods are
unwieldily when used in conjunction with large data
sets and long phrases. We have contrasted this with
our suffix array-based data structure which provides
261
a very compact way of storing large data sets while
simultaneously allowing the retrieval of arbitrarily
long phrases.
For the NIST-2004 Arabic-English data set,
which is among the largest currently assembled for
statistical machine translation, our representation
uses a very manageable 2 gigabytes of memory. This
is less than is needed to store a table containing
phrases with a maximum of three words, and is ten
times less than the memory required to store a table
with phrases of length eight.
We have further demonstrated that while compu-
tational complexity can make the retrieval of trans-
lation of frequent phrases slow, the use of sampling
is an extremely effective countermeasure to this.
We demonstrated that calculating phrase translation
probabilities from sets of 100 occurrences or less re-
sults in nearly no decrease in translation quality.
The implications of the data structure presented
in this paper are significant. The compact rep-
resentation will allow us to easily scale to paral-
lel corpora consisting of billions of words of text,
and the retrieval of arbitrarily long phrases will al-
low experiments with alternative decoding strate-
gies. These facts in combination allow for an even
greater exploitation of training data in statistical ma-
chine translation.
References
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished
Draft.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Udi Manber and Gene Myers. 1990. Suffix arrays:
A new method for on-line string searches. In The
First Annual ACM-SIAM Symposium on Dicrete Algo-
rithms, pages 319?327.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?450, De-
cember.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proceed-
ings of EMNLP.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings of MT Summit 9.
Mikio Yamamoto and Kenneth Church. 2001. Using suf-
fix arrays to compute term frequency and document
frequency for all substrings in a corpus. Compuata-
tional Linguistics, 27(1):1?30.
262
Proceedings of the 43rd Annual Meeting of the ACL, pages 597?604,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Paraphrasing with Bilingual Parallel Corpora
Colin Bannard Chris Callison-Burch
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
{c.j.bannard, callison-burch}@ed.ac.uk
Abstract
Previous work has used monolingual par-
allel corpora to extract and generate para-
phrases. We show that this task can be
done using bilingual parallel corpora, a
much more commonly available resource.
Using alignment techniques from phrase-
based statistical machine translation, we
show how paraphrases in one language
can be identified using a phrase in another
language as a pivot. We define a para-
phrase probability that allows paraphrases
extracted from a bilingual parallel corpus
to be ranked using translation probabili-
ties, and show how it can be refined to
take contextual information into account.
We evaluate our paraphrase extraction and
ranking methods using a set of manual
word alignments, and contrast the qual-
ity with paraphrases extracted from auto-
matic alignments.
1 Introduction
Paraphrases are alternative ways of conveying the
same information. Paraphrases are useful in a num-
ber of NLP applications. In natural language gen-
eration the production of paraphrases allows for the
creation of more varied and fluent text (Iordanskaja
et al, 1991). In multidocument summarization the
identification of paraphrases allows information re-
peated across documents to be condensed (McKe-
own et al, 2002). In the automatic evaluation of
machine translation, paraphrases may help to alle-
viate problems presented by the fact that there are
often alternative and equally valid ways of translat-
ing a text (Pang et al, 2003). In question answering,
discovering paraphrased answers may provide addi-
tional evidence that an answer is correct (Ibrahim et
al., 2003).
In this paper we introduce a novel method for ex-
tracting paraphrases that uses bilingual parallel cor-
pora. Past work (Barzilay and McKeown, 2001;
Barzilay and Lee, 2003; Pang et al, 2003; Ibrahim et
al., 2003) has examined the use of monolingual par-
allel corpora for paraphrase extraction. Examples
of monolingual parallel corpora that have been used
are multiple translations of classical French novels
into English, and data created for machine transla-
tion evaluation methods such as Bleu (Papineni et
al., 2002) which use multiple reference translations.
While the results reported for these methods are
impressive, their usefulness is limited by the scarcity
of monolingual parallel corpora. Small data sets
mean a limited number of paraphrases can be ex-
tracted. Furthermore, the narrow range of text gen-
res available for monolingual parallel corpora limits
the range of contexts in which the paraphrases can
be used.
Instead of relying on scarce monolingual parallel
data, our method utilizes the abundance of bilingual
parallel data that is available. This allows us to cre-
ate a much larger inventory of phrases that is appli-
cable to a wider range of texts.
Our method for identifying paraphrases is an
extension of recent work in phrase-based statisti-
cal machine translation (Koehn et al, 2003). The
essence of our method is to align phrases in a bilin-
gual parallel corpus, and equate different English
phrases that are aligned with the same phrase in the
other language. This assumption of similar mean-
597
Emma burst into tears and he tried to comfort
her, saying things to make her smile.
Emma cried, and he tried to console her, adorn-
ing his words with puns.
Figure 1: Using a monolingal parallel corpus to ex-
tract paraphrases
ing when multiple phrases map onto a single for-
eign language phrase is the converse of the assump-
tion made in the word sense disambiguation work of
Diab and Resnik (2002) which posits different word
senses when a single English word maps onto differ-
ent words in the foreign language (we return to this
point in Section 4.4).
The remainder of this paper is as follows: Section
2 contrasts our method for extracting paraphrases
with the monolingual case, and describes how we
rank the extracted paraphrases with a probability
assignment. Section 3 describes our experimental
setup and includes information about how phrases
were selected, how we manually aligned parts of the
bilingual corpus, and how we evaluated the para-
phrases. Section 4 gives the results of our evalua-
tion and gives a number of example paraphrases ex-
tracted with our technique. Section 5 reviews related
work, and Section 6 discusses future directions.
2 Extracting paraphrases
Much previous work on extracting paraphrases
(Barzilay and McKeown, 2001; Barzilay and Lee,
2003; Pang et al, 2003) has focused on finding iden-
tifying contexts within aligned monolingual sen-
tences from which divergent text can be extracted,
and treated as paraphrases. Barzilay and McKeown
(2001) gives the example shown in Figure 1 of how
identical surrounding substrings can be used to ex-
tract the paraphrases of burst into tears as cried and
comfort as console.
While monolingual parallel corpora often have
identical contexts that can be used for identifying
paraphrases, bilingual parallel corpora do not. In-
stead, we use phrases in the other language as piv-
ots: we look at what foreign language phrases the
English translates to, find all occurrences of those
foreign phrases, and then look back at what other
English phrases they translate to. We treat the other
English phrases as potential paraphrases. Figure 2 il-
lustrates how a German phrase can be used as a point
of identification for English paraphrases in this way.
Section 2.1 explains which statistical machine trans-
lation techniques are used to align phrases within
sentence pairs in a bilingual corpus.
A significant difference between the present work
and that employing monolingual parallel corpora, is
that our method frequently extracts more than one
possible paraphrase for each phrase. We assign a
probability to each of the possible paraphrases. This
is a mechanism for ranking paraphrases, which can
be utilized when we come to select the correct para-
phrase for a given context . Section 2.2 explains how
we calculate the probability of a paraphrase.
2.1 Aligning phrase pairs
We use phrase alignments in a parallel corpus as
pivots between English paraphrases. We find these
alignments using recent phrase-based approaches to
statistical machine translation.
The original formulation of statistical machine
translation (Brown et al, 1993) was defined as a
word-based operation. The probability that a foreign
sentence is the translation of an English sentence is
calculated by summing over the probabilities of all
possible word-level alignments, a, between the sen-
tences:
p(f |e) =
?
a
p(f ,a|e)
Thus Brown et al decompose the problem of de-
termining whether a sentence is a good translation
of another into the problem of determining whether
there is a sensible mapping between the words in the
sentences.
More recent approaches to statistical translation
calculate the translation probability using larger
blocks of aligned text. Koehn (2004), Tillmann
(2003), and Vogel et al (2003) describe various
heuristics for extracting phrase alignments from the
Viterbi word-level alignments that are estimated us-
ing Brown et al (1993) models. We use the heuris-
tic for phrase alignment described in Och and Ney
(2003) which aligns phrases by incrementally build-
ing longer phrases from words and phrases which
have adjacent alignment points.1
1Note that while we induce the translations of phrases from
598
what is more, the relevant cost dynamic is completely under control
im ?brigen ist die diesbez?gliche kostenentwicklung v?llig  unter kontrolle
we owe it to the taxpayers to keep in checkthe costs
wir sind es den steuerzahlern die kosten zu habenschuldig  unter kontrolle
Figure 2: Using a bilingual parallel corpus to extract paraphrases
2.2 Assigning probabilities
We define a paraphrase probability p(e2|e1) in terms
of the translation model probabilities p(f |e1), that
the original English phrase e1 translates as a partic-
ular phrase f in the other language, and p(e2|f), that
the candidate paraphrase e2 translates as the foreign
language phrase. Since e1 can translate as multiple
foreign language phrases, we sum over f :
e?2 = arg max
e2 6=e1
p(e2|e1) (1)
= arg max
e2 6=e1
?
f
p(f |e1)p(e2|f) (2)
The translation model probabilities can be com-
puted using any standard formulation from phrase-
based machine translation. For example, p(e|f)
can be calculated straightforwardly using maximum
likelihood estimation by counting how often the
phrases e and f were aligned in the parallel corpus:
p(e|f) =
count(e, f)
?
e count(e, f)
(3)
Note that the paraphrase probability defined in
Equation 2 returns the single best paraphrase, e?2, ir-
respective of the context in which e1 appears. Since
the best paraphrase may vary depending on informa-
tion about the sentence that e1 appears in, we extend
the paraphrase probability to include that sentence
S:
e?2 = arg max
e2 6=e1
p(e2|e1, S) (4)
word-level alignments in this paper, direct estimation of phrasal
translations (Marcu and Wong, 2002) would also suffice for ex-
tracting paraphrases from bilingual corpora.
a million, as far as possible, at work, big business,
carbon dioxide, central america, close to, concen-
trate on, crystal clear, do justice to, driving force,
first half, for the first time, global warming, great
care, green light, hard core, horn of africa, last re-
sort, long ago, long run, military action, military
force, moment of truth, new world, noise pollution,
not to mention, nuclear power, on average, only too,
other than, pick up, president clinton, public trans-
port, quest for, red cross, red tape, socialist party,
sooner or later, step up, task force, turn to, under
control, vocational training, western sahara, world
bank
Table 1: Phrases that were selected to paraphrase
S allows us to re-rank the candidate paraphrases
based on additional contextual information. The ex-
periments in this paper employ one variety of con-
textual information. We include a simple language
model probability, which would additionally rank
e2 based on the probability of the sentence formed
by substiuting e2 for e1 in S. A possible extension
which we do not evaluate might be permitting only
paraphrases that are the same syntactic type as the
original phrase, which we could do by extending the
translation model probabilities to count only phrase
occurrences of that type.
3 Experimental Design
We extracted 46 English phrases to paraphrase
(shown in Table 1), randomly selected from those
multi-word phrases in WordNet which also occured
multiple times in the first 50,000 sentences of our
bilingual corpus. The bilingual corpus that we used
599
Alignment Tool
.
kontrolle
unter
v?llig
kostenentwickl...
diesbez?gliche
die
ist
?brigen
im
.c
o
n
t
r
o
l
u
n
d
e
r
c
o
m
p
l
e
t
e
l
y
i
s
d
y
n
a
m
i
c
c
o
s
t
r
e
l
e
v
a
n
t
t
h
e
,m
o
r
e
i
s
w
h
a
t
(a) Aligning the English phrase to be paraphrased
haben
zu
kontrolle
unter
kosten
die
schuldig
steuerzahlern
den
es
sind
wir
.c
h
e
c
k
i
n
c
o
s
t
s
t
h
e
k
e
e
p
t
o
t
a
x
p
a
y
e
r
s
t
h
e
t
o
i
t
o
w
e
w
e
Alignment Tool
(b) Aligning occurrences of its German translation
Figure 3: Phrases highlighted for manual alignment
was the German-English section of the Europarl cor-
pus, version 2 (Koehn, 2002). We produced auto-
matic alignments for it with the Giza++ toolkit (Och
and Ney, 2003). Because we wanted to test our
method independently of the quality of word align-
ment algorithms, we also developed a gold standard
of word alignments for the set of phrases that we
wanted to paraphrase.
3.1 Manual alignment
The gold standard alignments were created by high-
lighting all occurrences of the English phrase to
paraphrase and manually aligning it with its Ger-
man equivalent by correcting the automatic align-
ment, as shown in Figure 3a. All occurrences of
its German equivalents were then highlighted, and
aligned with their English translations (Figure 3b).
The other words in the sentences were left with their
automatic alignments.
3.2 Paraphrase evaluation
We evaluated the accuracy of each of the para-
phrases that was extracted from the manually
aligned data, as well as the top ranked paraphrases
from the experimental conditions detailed below in
Section 3.3. Because the acccuracy of paraphrases
can vary depending on context, we substituted each
Under control
This situation is in check in terms of security.
This situation is checked in terms of security.
This situation is curbed in terms of security.
This situation is curb in terms of security.
This situation is limit in terms of security.
This situation is slow down in terms of security.
Figure 4: Paraphrases substituted in for the original
phrase
set of candidate paraphrases into between 2?10 sen-
tences which contained the original phrase. Figure 4
shows the paraphrases for under control substituted
into one of the sentences in which it occurred. We
created a total of 289 such evaluation sets, with a
total of 1366 unique sentences created through sub-
stitution.
We had two native English speakers produce
judgments as to whether the new sentences pre-
served the meaning of the original phrase and as to
whether they remained grammatical. Paraphrases
that were judged to preserve both meaning and
grammaticality were considered to be correct, and
examples which failed on either judgment were con-
sidered to be incorrect.
In Figure 4 in check, checked, and curbed were
600
under control checked, curb, curbed, in check, limit, slow down
sooner or later at some point, eventually
military force armed forces, defence, force, forces, military forces, peace-keeping personnel
long ago a little time ago, a long time, a long time ago, a lot of time, a while ago, a while back,
far, for a long time, for some time, for such a long time, long, long period of time, long
term, long time, long while, overdue, some time, some time ago
green light approval, call, go-ahead, indication, message, sign, signal, signals, formal go-ahead
great care a careful approach, greater emphasis, particular attention, special attention, specific
attention, very careful
first half first six months
crystal clear absolutely clear, all clarity, clear, clearly, in great detail, no mistake, no uncertain,
obvious, obviously, particularly clear, perfectly clear, quite clear, quite clearly, quite
explicitly, quite openly, very clear, very clear and comprehensive, very clearly, very
sure, very unclear, very well
carbon dioxide co2
at work at the workplace, employment, held, holding, in the work sphere, operate, organised,
taken place, took place, working
Table 2: Paraphrases extracted from a manually word-aligned parallel corpus
judged to be correct and curb, limit and slow down
were judged to be incorrect. The inter-annotator
agreement for these judgements was measured at
? = 0.605, which is conventionally interpreted as
?good? agreement.
3.3 Experiments
We evaluated the accuracy of top ranked paraphrases
when the paraphrase probability was calculated us-
ing:
1. The manual alignments,
2. The automatic alignments,
3. Automatic alignments produced over multiple
corpora in different languages,
4. All of the above with language model re-
ranking.
5. All of the above with the candidate paraphrases
limited to the same sense as the original phrase.
4 Results
We report the percentage of correct translations (ac-
curacy) for each of these experimental conditions. A
summary of these can be seen in Table 3. This sec-
tion will describe each of the set-ups and the score
reported in more detail.
4.1 Manual alignments
Table 2 gives a set of example paraphrases extracted
from the gold standard alignments. The italicized
paraphrases are those that were assigned the highest
probability by Equation 2, which chooses a single
best paraphrase without regard for context. The 289
sentences created by substituting the italicized para-
phrases in for the original phrase were judged to be
correct an average of 74.9% of the time.
Ignoring the constraint that the new sentences re-
main grammatically correct, these paraphrases were
judged to have the correct meaning 84.7% of the
time. This suggests that the context plays a more
important role with respect to the grammaticality
of substituted paraphrases than with respect to their
meaning.
In order to allow the surrounding words in the sen-
tence to have an influence on which paraphrase was
selected, we re-ranked the paraphrase probabilities
based on a trigram language model trained on the
entire English portion of the Europarl corpus. Para-
phrases were selected from among all those in Table
2, and not constrained to the italicized phrases. In
the case of the paraphrases extracted from the man-
ual word alignments, the language model re-ranking
had virtually no influence, and resulted in a slight
dip in accuracy to 71.7%
601
Paraphrase Prob Paraphrase Prob & LM Correct Meaning
Manual Alignments 74.9 71.7 84.7
Automatic Alignments 48.9 55.3 64.5
Using Multiple Corpora 55.0 57.4 65.4
Word Sense Controlled 57.0 61.9 70.4
Table 3: Paraphrase accuracy and correct meaning for the different data conditions
4.2 Automatic alignments
In this experimental condition paraphrases were ex-
tracted from a set of automatic alignments produced
by running Giza++ over a set of 1,036,000 German-
English sentence pairs (roughly 28,000,000 words in
each language). When the single best paraphrase (ir-
respective of context) was used in place of the orig-
inal phrase in the evaluation sentence the accuracy
reached 48.9% which is quite low compared to the
74.9% of the manually aligned set.
As with the manual alignments it seems that we
are selecting phrases which have the correct mean-
ing but are not grammatical in context. Indeed our
judges thought the meaning of the paraphrases to
be correct in 64.5% of cases. Using a language
model to select the best paraphrase given the con-
text reduces the number of ungrammatical examples
and gives an improvement in quality from 48.9% to
55.3% correct.
These results suggest two things: that improving
the quality of automatic alignments would lead to
more accurate paraphrases, and that there is room
for improvement in limiting the paraphrases by their
context. We address these points below.
4.3 Using multiple corpora
Work in statistical machine translation suggests that,
like many other machine learning problems, perfor-
mance increases as the amount of training data in-
creases. Och and Ney (2003) show that the accuracy
of alignments produced by Giza++ improve as the
size of the training corpus increases.
Since we used the whole of the German-English
section of the Europarl corpus, we could not try
improving the alignments by simply adding more
German-English training data. However, there is
nothing that limits our paraphrase extraction method
to drawing on candidate paraphrases from a sin-
gle target language. We therefore re-formulated the
paraphrase probability to include multiple corpora,
as follows:
e?2 = arg max
e2 6=e1
?
C
?
f in C
p(f |e1)p(e2|f) (5)
where C is a parallel corpus from a set of parallel
corpora.
For this condition we used Giza++ to align
the French-English, Spanish-English, and Italian-
English portions of the Europarl corpus in addition
to the German-English portion, for a total of around
4,000,000 sentence pairs in the training data.
The accuracy of paraphrases extracted over mul-
tiple corpora increased to 55%, and further to 57.4%
when the language model re-ranking was included.
4.4 Controlling for word sense
As mentioned in Section 1, the way that we extract
paraphrases is the converse of the methodology em-
ployed in word sense disambiguation work that uses
parallel corpora (Diab and Resnik, 2002). The as-
sumption made in the word sense disambiguation
work is that if a source language word aligns with
different target language words then those words
may represent different word senses. This can be
observed in the paraphrases for at work in Table 2.
The paraphrases at the workplace, employment, and
in the work sphere are a different sense of the phrase
than operate, held, and holding, and they are aligned
with different German phrases.
When we calculate the paraphrase probability we
sum over different target language phrases. There-
fore the English phrases that are aligned with the dif-
ferent German phrases (which themselves maybe in-
dicative of different word senses) are mingled. Per-
formance may be degraded since paraphrases that
reflect different senses of the original phrase, and
which therefore have a different meaning, are in-
cluded in the same candidate set.
602
We therefore performed an experiment to see
whether improvement could be had by limiting the
candidate paraphrases to be the same sense as the
original phrase in each test sentence. To do this,
we used the fact that our test sentences were drawn
from a parallel corpus. We limited phrases to the
same word sense by constraining the candidate para-
phrases to those that aligned with the same target
language phrase. Our basic paraphrase calculation
was therefore:
p(e2|e1, f) = p(f |e1)p(e2|f) (6)
Using the foreign language phrase to identify the
word sense is obviously not applicable in monolin-
gual settings, but acts as a convenient stand-in for a
proper word sense disambiguation algorithm here.
When word sense is controlled in this way, the
accuracy of the paraphrases extracted from the au-
tomatic alignments raises dramatically from 48.9%
to 57% without language model re-ranking, and fur-
ther to 61.9% when language model re-ranking was
included.
5 Related Work
Barzilay and McKeown (2001) extract both single-
and multiple-word paraphrases from a monolingual
parallel corpus. They co-train a classifier to iden-
tify whether two phrases were paraphrases of each
other based on their surrounding context. Two dis-
advantages of this method are that it requires iden-
tical bounding substrings, and has bias towards sin-
gle words. For an evaluation set of 500 paraphrases,
they report an average precision of 86% at identi-
fying paraphrases out of context, and of 91% when
the paraphrases are substituted into the original con-
text of the aligned sentence. The results of our sys-
tems are not directly comparable, since Barzilay and
McKeown (2001) evaluated their paraphrases with a
different set of criteria (they asked judges whether
to judge paraphrases based on ?approximate con-
ceptual equivalence?). Furthermore, their evaluation
was carried out only by substituting the paraphrase
in for the phrase with the identical context, and not
in for arbitrary occurrences of the original phrase, as
we have done.
Lin and Pantel (2001) use a standard (non-
parallel) monolingual corpus to generate para-
phrases, based on dependancy graphs and distribu-
tional similarity. One strong disadvantage of this
method is that their paraphrases can also have op-
posite meanings.
Ibrahim et al (2003) combine the two approaches:
aligned monolingual corpora and parsing. They
evaluated their system with human judges who were
asked whether the paraphrases were ?roughly inter-
changeable given the genre?, scored an average of
41% on a set of 130 paraphrases, with the judges
all agreeing 75% of the time, and a correlation of
0.66. The shortcomings of this method are that it is
dependent upon parse quality, and is limited by the
rareness of the data.
Pang et al (2003) use parse trees over sentences in
monolingual parallel corpus to identify paraphrases
by grouping similar syntactic constituents. They
use heuristics such as keyword checking to limit
the over-application of this method. Our alignment
method might be an improvement of their heuris-
tics for choosing which constituents ought to be
grouped.
6 Discussion and Future Work
In this paper we have introduced a novel method for
extracting paraphrases, which we believe greatly in-
creases the usefulness of paraphrasing in NLP ap-
plications. The advantages of our method are that
it:
? Produces a ranked list of high quality para-
phrases with associated probabilities, from
which the best paraphrase can be chosen ac-
cording to the target context. We have shown
how a language model can be used to select the
best paraphrase for a particular context from
this list.
? Straightforwardly handles multi-word units.
Whereas for previous approaches the evalua-
tion has been performed over mostly single
word paraphrases, our results are reported ex-
clusively over units of between 2 and 4 words.
? Because we use a much more abundant source
of data, our method can be used for a much
wider range of text genres than previous ap-
proaches, namely any for which parallel data
is available.
603
One crucial thing to note is that we have demon-
strated our paraphrases to be of higher quality when
the alignments used to produce them are improved.
This means that our method will reap the benefits
of research that improvements to automatic align-
ment techniques (Callison-Burch et al, 2004), and
will further improve as more parallel data becomes
available.
In the future we plan to:
? Investigate whether our re-ranking can be fur-
ther improved by using a syntax-based lan-
guage model.
? Formulate a paraphrase probability for senten-
tial paraphrases, and use this to try to identify
paraphrases across documents in order to con-
dense information for multi-document summa-
rization.
? See whether paraphrases can be used to in-
crease coverage for statistical machine trans-
lation when translating into ?low-density? lan-
guages which have small parallel corpora.
Acknowledgments
The authors would like to thank Beatrice Alex,
Marco Kuhlmann, and Josh Schroeder for their valu-
able input as well as their time spent annotating and
contributing to the software.
References
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Chris Callison-Burch, David Talbot, and Miles Osborne.
2004. Statistical machine translation with word- and
sentence-aligned parallel corpora. In Proceedings of
ACL.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora. In Proceedings of the Second International
Workshop on Paraphrasing (ACL 2003).
Lidija Iordanskaja, Richard Kittredge, and Alain Polge?re.
1991. Lexical selection and paraphrase in a meaning-
text generation model. In Ce?cile L. Paris, William R.
Swartout, and William C. Mann, editors, Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics. Kluwer Academic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished
Draft.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia?s Newsblaster. In Pro-
ceedings of the Human Language Technology Confer-
ence.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proceed-
ings of EMNLP.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings of MT Summit 9.
604
A Statistical Approach to the Semantics of Verb-Particles
Colin Bannard
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
c.j.bannard@ed.ac.uk
Timothy Baldwin
CSLI
Stanford University
210 Panama Street
Stanford CA 94305, USA
tbaldwin@csli.stanford.edu
Alex Lascarides
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
alex@inf.ed.ac.uk
Abstract
This paper describes a distributional ap-
proach to the semantics of verb-particle
constructions (e.g. put up, make off ). We
report first on a framework for implement-
ing and evaluating such models. We then go
on to report on the implementation of some
techniques for using statistical models ac-
quired from corpus data to infer the mean-
ing of verb-particle constructions.
1 Introduction
The semantic representation of multiword expres-
sions (MWEs) has recently become the target of re-
newed attention, notably in the area of hand-written
grammar development (Sag et al, 2002; Villavicen-
cio and Copestake, 2002). Such items cause con-
siderable problems for any semantically-grounded
NLP application (including applications where se-
mantic information is implicit, such as information
retrieval) because their meaning is often not sim-
ply a function of the meaning of the constituent
parts. However, corpus-based or empirical NLP has
shown limited interest in the problem. While there
has been some work on statistical approaches to
the semantics of compositional compound nominals
(e.g. Lauer (1995), Barker and Szpakowicz (1998),
Rosario and Hearst (2001)), the more idiosyncratic
items have been largely ignored beyond attempts at
identification (Melamed, 1997; Lin, 1999; Schone and
Jurafsky, 2001). And yet the identification of non-
compositional phrases, while valuable in itself, would
by no means be the end of the matter. The unique
challenge posed by MWEs for empirical NLP is pre-
cisely that they do not fall cleanly into the binary
classes of compositional and non-compositional ex-
pressions, but populate a continuum between the two
extremes.
Part of the reason for the lack of interest by com-
putational linguists in the semantics of MWEs is that
there is no established gold standard data from which
to construct or evaluate models. Evaluation to date has
tended to be fairly ad hoc. Another key problem is the
lack of any firm empirical foundations for the notion
of compositionality. Given this background, this pa-
per has two aims. The first is to put the treatment of
non-compositionality in corpus-based NLP on a firm
empirical footing. As such it describes the develop-
ment of a resource for implementing and evaluating
statistical models of MWE meaning, based on non-
expert human judgements. The second is to demon-
strate the usefulness of such approaches by imple-
menting and evaluating a handful of approaches.
The remainder of this paper is structured as follows.
We outline the linguistic foundations of this research
in Section 2 before describing the process of resource
building in Section 3. Section 4 summarises previ-
ous work on the subject and Section 5 details our pro-
posed models of compositionality. Section 6 lays out
the evaluation of those models over the gold standard
data, and we conclude the paper in Section 7.
2 Verb Particle Constructions
We selected the English verb-particle construction as
our test case MWE in this paper. Verb-particle con-
structions (hereafter referred to as VPCs) consist of a
head verb and one or more obligatory particles, in the
form of intransitive prepositions (e.g. hand in), ad-
jectives (e.g. cut short) or verbs (e.g. let go) . Here,
we focus exclusively on prepositional particles due to
their high productivity and variable compositionality.
Examples of prepositional VPCs are put up, finish up,
gun down and make out as used in the following sen-
tences:
(1) Peter put the picture up
(2) Susan finished up her paper
(3) Philip gunned down the intruder
(4) Barbara and Simon made out
VPCs cause significant problems for NLP sys-
tems. Semantically, they often cannot be understood
through the simple composition of their independent
parts. Compare, for example, sentences (1) and (4).
In (1), the meaning seems to be that Peter put the pic-
ture somewhere and that as a consequence the picture
was up. That is, the verb and the particle make in-
dependent contributions to the sentence. A (partial)
Parsons-style semantic analysis of this might be as
follows:
put(e1, x, y) ? peter(x) ? picture(y) ? up(e1, y)
Sentence (4), on the other hand requires a rather dif-
ferent analysis. Neither Barbara nor Simon can be
said to have made or to be out. The semantic anal-
ysis we would want then might be something like the
following:
make out(e1, e2) ? and(e2, x, y) ? barbara(x) ? simon(y)
How are we to identify whether the first or the second
kind of semantic representation is appropriate for any
given item? If we look at the other two sentences we
can see that the problem is even more complicated.
In (2) it is the case that the paper is finished, but it
would be hard to claim that anything or anyone is up.
Only the verb then seems to be contributing its sim-
plex meaning, and the semantic analysis is (roughly):
finish(e1, x, y) ? susan(x) ? paper(y)
In (3), by contrast, it is the particle that contributes its
simplex meaning and not the verb. As a consequence
of Philip?s action the intruder is down, but since there
is no simplex verb to gun, we would not say that any-
one gunned or was gunned The semantic analysis is
consequently as follows:
gun down(e1, x, y) ? philip(x) ? intruder(y) ? down(e1, y)
In the linguistic literature, the semantics of VPCs
is frequently viewed in rather more complicated terms
than we are suggesting here, with particles often seen
as making significant construction-specific contribu-
tions in terms of aspect (e.g. Brinton (1985)). How-
ever no such existing linguistic account is completely
robust, and for practical NLP purposes we are forced
to adopt a rather straightforward definition of compo-
sitionality as meaning that the overall semantics of the
MWE can be composed from the simplex semantics
of its parts, as described (explicitly or implicitly) in a
finite lexicon.
3 Building the Resource
Rather than attempting to model compositionality by
anchoring word semantics to a given lexicon, our ap-
proach in this work is to defer to an empirical refer-
ence based on human judgements. We define MWE
compositionality to be an entailment relationship be-
tween the whole and its various parts, and solicit en-
tailment judgements based on a handful of example
sentences.
Entailment is conventionally defined for logical
propositions, where a proposition P entails a proposi-
tion Q iff there is no conceivable state of affairs that
could make P true and Q false. This can be gener-
alised to refer to the relationship between two verbs
V1 and V2 that holds when the sentence Someone V1s
entails the sentence Someone V2s (see, e.g., the treat-
ment of verbs in the WordNet hierarchy (Miller et al,
1990)). According to this generalisation we would
then say that the verb run entails the verb move be-
cause the sentence He runs entails the sentence He
moves. The same idea can be generalised to the rela-
tionship between simplex verbs (e.g. walk) and VPCs
(e.g. walk off ). For example, sentence (1) can be said
to entail that Peter put the picture somewhere and so
we can say that put up entails put. The same might be
said of finish up and finish in (2). However, (3) and
(4) produce a rather different result. (4) does not en-
tail that Simon and Barbara made something, and (3)
cannot entail that Philip gunned the intruder because
there is no simplex verb to gun. This is a very useful
way of testing whether the simplex verb contributes to
the meaning of the construction.
We can approach the relationship between VPCs
and particles in this same way. For (1), while it is
not true that Peter was up, it is true that The picture
was up. We can therefore say that the VPC entails the
particle here. For (2), it is not true that either Susan
or the paper were up, and the VPC therefore does not
entail the particle. In the case of (3), while it is not
true that Philip was down it is true that The intruder
was down, and the VPC therefore entails the particle.
Finally, for (4), it is not true that Barbara and Simon
were out, and the VPC therefore does not entail the
particle.
We make the assumption that these relationships
between the component words of the VPC and the
whole are intuitive to non-experts, and aim to use
their ?entailment? judgements accordingly. This
use of entailment in exploring the semantics of
verb and preposition combinations was first pro-
posed by Hawkins (2000), and applied to VPCs by
Lohse et al (in preparation).
3.1 Experimental Materials
In an attempt to normalise the annotators? entailment
judgements, we decided upon an experimental setup
where the subject is, for each VPC type, presented
with a fixed selection of sentential contexts for that
VPC. So as to avoid introducing any bias into the
experiment through artificially-generated sentences,
we chose to extract the sentences from naturally-
occurring text, namely the written component of the
British National Corpus (BNC, Burnard (2000)).
Extraction of the VPCs was based on the method
of Baldwin and Villavicencio (2002). First, we used a
POS tagger and chunker (both built using fnTBL 1.0
(Ngai and Florian, 2001)) to (re)tag the BNC. This al-
lowed us to extract VPC tokens through use of: (a)
the particle POS in the POS tagged output, for each
instance of which we simply then look for the right-
most verb within a fixed window to the left of the
particle, and (b) the particle chunk tag in the chunker
output, where we similarly locate the rightmost verb
associated with each particle chunk occurrence. Fi-
nally, we ran a stochastic chunk-based grammar over
the chunker output to extend extraction coverage to
include mistagged particles and also more reliably de-
termine the valence of the VPC. The token output of
these three methods was amalgamated by weighted
voting.
The above method extracted 461 distinct VPC types
occurring at least 50 times, attested in a total of
110,199 sentences. After partitioning the sentence
data by type, we randomly selected 5 sentences for
each VPC type. We then randomly selected 40 VPC
types (with 5 sentences each) to use in the entailment
experiment. That is, all results described in this paper
are over 40 VPC types.
3.2 Participants
28 participants took part in our initial experiment.
They were all native speakers of English, recruited
by advertisements posted to newsgroups and mailing
lists.
3.3 Experimental Method
Each participant was presented with 40 sets of 5 sen-
tences, where each of the five sentences contained a
particular VPC. The VPC in question was indicated at
the top of the screen, and they were asked two ques-
tions: (1) whether the VPC implies the verb, and (2)
whether the VPC implies the particle. If the VPC
was round up, e.g., the subject would be asked ?Does
round up imply round?? and ?Does round up im-
ply up??, respectively. They were given the option of
three responses: ?Yes?, ?No? or ?Don?t Know?. Once
they had indicated their answer and pressed next, they
advanced to the next VPC and set of 5 sentences. They
were unable to move on until a choice had been indi-
cated.
As with any corpus-based approach to lexical se-
mantics, our study of VPCs is hampered by poly-
semy, e.g. carry outTRANSin the execute and transport
out (from a location) senses.1 Rather than intervene
to customise example sentences to a prescribed sense,
we accepted whatever composition of senses random
sampling produced. Participants were advised that if
they felt more that one meaning was present in the set
of five sentences, they should base their decision on
the sense that had the greatest number of occurrences
in the set.
1The effects of polysemy were compounded by not having any
reliable method for determining valence. We consider that sim-
ply partitioning VPC items into intransitive and transitive usages
would reduce polysemy significantly.
VPC Component word Yes No Don?t Know
get 19 5 2get down down 14 10 2
move 14 12 0
move off
off 19 7 0
throw 20 6 0throw out
out 15 10 1
pay 11 12 3pay off
off 16 8 2
lift 25 1 0lift out
out 26 0 0
roll 13 9 4
roll back back 14 12 0
dig 21 5 0dig up
up 18 7 1
lie 24 2 0lie down down 25 1 0
wear 6 19 1
wear on
on 3 22 1
fall 23 3 0fall off
off 25 1 0
move 22 4 0
move out
out 26 0 0
hand 15 9 2hand out
out 19 7 0
seek 13 13 0
seek out
out 15 11 0
sell 14 12 0sell off
off 16 9 1
trail 8 18 0trail off
off 10 16 0
stay 20 5 1stay up
up 21 5 0
go 18 7 1go down down 22 3 1
hang 22 4 0hang out
out 25 1 0
get 20 6 0get back back 19 6 1
throw 15 9 2throw in in 13 12 1
put 8 17 1put off
off 5 19 2
shake 12 14 0shake off
off 15 11 0
step 25 1 0step off
off 26 0 0
give 12 12 2give off
off 21 5 0
carry 7 17 2carry away
away 6 18 2
throw 18 7 1throw back back 21 4 1
pull 13 10 3pull off
off 13 6 7
carry 0 25 1carry out
out 0 25 1
brighten 9 16 1brighten up
up 16 10 0
map 9 17 0
map out
out 10 16 0
slow 11 14 1slow down down 19 7 0
sort 6 19 1
sort out
out 11 15 0
bite 15 10 1bite off
off 16 8 2
add 12 14 0add up
up 19 6 1
mark 13 13 0
mark out
out 14 12 0
lay 11 14 1lay out
out 10 14 2
catch 6 20 0catch up
up 7 18 1
run 12 13 1
run up
up 13 10 3
stick 20 6 0
stick out
out 15 11 0
play 10 15 1play down down 6 20 0
Table 1: Participant entailment judgements
Overall Verbs only Particles only
Agreement .677 .703 .650
Kappa (?) .376 .372 .352
% Yes .575 .655 .495
% No .393 .319 .467
% Don?t Know .032 .026 .038
Table 2: Summary of judgements for all VPCs
The experiment was conducted remotely over the
Web, using the experimental software package Web-
Exp (Corley et al, 2000). Experimental sessions
lasted approximately 20 minutes and were self-paced.
The order in which the forty sets of sentences were
presented was randomised by the software.
3.4 Annotator agreement
We performed a pairwise analysis of the agreement
between our 28 participants. The overall mean agree-
ment was .655, with a kappa (?) score (Carletta, 1996)
of .329. An initial analysis showed that two partic-
ipants strongly disagreed with the other, achieving a
mean pairwise ? score of less than .1. We decided
therefore to remove these from the set before pro-
ceeding. The overall results for the remaining 26 par-
ticipants can be seen in Table 2. The ? score over
these 26 participants (.376) is classed as fair (0.2?
0.4) and approaching moderate (0.4?0.6) according to
Altman (1991).
As mentioned above, a major problem with lexi-
cal semantic studies is that items tend to occur with
more than one meaning. In order to test the effects of
polysemy in the example sentences on inter-annotator
agreement, we analysed the agreement obtained over
those VPCs which have only one meaning accord-
ing to WordNet (Miller et al, 1990). There was a
total of 14 such items, giving 28 entailment judge-
ments (one for the verb and one for the particle in
each item). For these items, mean agreement and the
? score were .700 and .387, respectively. These are
only very slightly higher than the overall scores, sug-
gesting, although by no means proving, that polysemy
was not a significant confounding factor.
The results for each VPC type can be seen in Ta-
ble 1, broken down into the verb and particle entail-
ment judgements and based on the 26 participants. We
took two approaches to deriving a single judgement
for each test. First, we took the majority judgement
to be the correct one (majority). Second, we identi-
fied the participant who achieved the highest overall ?
score with the other participants, and took their judge-
ments to be correct (centroid annotator). Both sets
of results will be referred to in evaluating our models.
It is interesting to look at the way in which the re-
sults for component entailment are distributed across
the VPCs. According to the majority view, there are
21 fully-compositional items, 10 items where neither
the verb nor the particle is entailed, 9 items where only
the particle is entailed, and 0 items where the verb
alone is entailed. According to the judgements of the
centroid annotator, there are 10 fully-compositional
items, 12 items where neither the verb nor the parti-
cle is entailed, 15 where only the verb is entailed, and
3 where only the particle is entailed. It is surprising
to notice that the majority view holds there to be no
items in which the verb alone is contributing mean-
ing. It could be the case that items where only the
verb contributes meaning are rare, or that they are not
represented in our dataset. Another possible, and to
our minds more likely, conclusion is that the contribu-
tion of the head verb strongly affects the way in which
participants view the whole item. Thus if a verb is
considered to be contributing simplex semantics, the
participant is likely to assume that the VPC is com-
pletely compositional, and conversely if a verb is con-
sidered to not be contributing simplex semantics, the
participant is more likely to assume the VPC to be
non-compositional.
4 Related Work
We devote this section to a description of statistical
NLP work on the non-compositionality of MWEs.
Perhaps the singularly most influential work on
MWE non-compositionality is that of Lin (1999). We
describe Lin?s method in some detail here as it forms
the basis of one of the methods tested in this re-
search. Lin?s method is based on the premise that non-
compositional items have markedly different distribu-
tional characteristics to expressions derived through
synonym substitution over the original word compo-
sition. Lin took his multiword items from a colloca-
tion database (Lin, 1998b). For each collocation, he
substituted each of the component words with a word
with a similar meaning. The list of similar meanings
was obtained by taking the 10 most similar words ac-
cording to a corpus-derived thesaurus, the construc-
tion of which is described in Lin (1998a). The mutual
information value was then found for each item pro-
duced by this substitution by taking a collocation to
consist of three events: the type of dependency rela-
tionship, the head lexical item, and the modifier. A
phrase ? was then said to be non-compositional iff
there exists no phrase ? where: (a) ? can be pro-
duced by substitution of the components of ? as de-
scribed above, and (b) there is an overlap between
the 95% confidence interval of the mutual informa-
tion values of ? and ?. These judgements were eval-
uated by comparison with a dictionary of idioms. If
an item was in the dictionary then it was said to be
non-compositional. Scores of 15.7% for precision and
13.7% for recall are reported.
There are, to our minds, significant problems with
the underlying assumptions of Lin?s method. The
theoretical basis of the technique is that composi-
tional items should have a similar distribution to items
formed by replacing components words with seman-
tically similar ones. The idea presumably is that if an
item is the result of the free combination of words, or
a fully productive lexical rule, then word-substituted
variants should be distributed similarly. This seems a
reasonable basis for modelling productivity but not
compositionality, as Lin claims. There are many ex-
amples in natural language of phrases that are not at
all productive but are still compositional (e.g. frying
pan); we term the process by which these expressions
arise institutionalisation . Similar work to Lin?s has
been done in the area of collocation extraction (e.g.
Pearce (2002)), to pick up on this alternate concept of
institutionalisation.
Schone and Jurafsky (2001) employed Latent Se-
mantic Analysis (LSA, Deerwester et al (1990)) in an
effort to improve on existing techniques for extract-
ing MWEs from corpora. One property they try and
pick up on in doing so is non-compositionality. They
measure the cosine between the vector representation
for the candidate MWE and a weighted vector sum
of its component words, suggesting that a small co-
sine would indicate compositionality. They evaluate
this by comparing the extracted items with those listed
in existing dictionaries, and report that it offers no
improvement in extracting MWEs over existing tech-
niques. The assumption that non-compositionality is
requisite for the presence of a MWE in a dictionary,
while interesting, is not well-founded, and hence it
does not seem to us that the poor results reflect a fail-
ure of the LSA approach in measuring compositional-
ity.
Bannard (2002) used a combination of hand-built
thesauri and corpus statistics to explore the compo-
sitionality of VPCs. The task was to predict whether
the verb and/or the particle were contributing meaning
to a given item, using statistical analysis of a set of
VPCs extracted from the Wall Street Journal section
of the Penn Treebank (Marcus et al, 1993). Two tech-
niques were used. The first of these loosely followed
Lin in measuring the extent to which the component
verb or particle of any VPC could be replaced with
items of a similar semantic class to form a corpus-
attested VPC; WordNet (Miller et al, 1990) was used
as the source for verb substitution candidates, and a
hand-build semantic taxonomy for particles. The sec-
ond technique explored the semantic similarity of a
VPC to its component verb by comparing their subcat-
egorisation preferences, assuming that semantic sim-
ilarity between a VPC and its component verb indi-
cates compositionality. Poor results were put down
to data-sparseness, and the lexical resources not being
well suited to the task. We use a larger corpus and an
automatically-derived thesaurus for the research de-
scribed in this paper, with the hope of overcoming
these problems.
McCarthy et al (2003) carry out research close in
spirit to that described here, in taking VPC tokens
automatically extracted from the BNC and using an
automatically acquired thesaurus to classify their rel-
ative compositionality. One significant divergence
from our research is that they consider composition-
ality to be an indivisible property of the overall VPC,
and not the individual parts. Gold-standard data was
generated by asking human annotators to describe the
compositionality of a given VPC according to a 11-
point scale, based upon which the VPCs were ranked
in order of compositionality. Similarly to this re-
search, McCarthy et al in part used the similarity
measure of Lin (1998a) to model compositionality,
e.g., in taking the top N similar words to each VPC
and looking at overlap with the top N similar words to
the head verb. They also examine the use of statistical
tests such as mutual information in modelling com-
positionality, and find the similarity-based methods to
correlate more highly with the human judgements.
Baldwin et al (2003) use LSA as a technique for
analysing the compositionality (or decomposabil-
ity) of a given MWE. LSA is suggested to be
a construction-inspecific test for compositionality,
which is illustrated by testing its effectivity over both
English noun-noun compounds and VPCs. Baldwin
et al used LSA to calculate the distributional similar-
ity between an MWE and its head word, and demon-
strate a correlation between similarity and composi-
tionality (modelled in terms of endocentricity) by way
of items with higher similarity being more composi-
tional. They do not go as far as to classify MWEs as
being compositional or non-compositional, however.
5 Building a classifier
Having created our gold-standard data, we imple-
mented some statistical techniques for automatic anal-
ysis. In this, we use the VPC tokens with sentential
contexts extracted from the BNC as reported in Sec-
tion 3, i.e. a superset of the data used to annotate the
VPCs. We mapped the gold-standard data onto four
binary (yes/no) classification tasks over VPC items:
TASK 1: The item is completely compositional.
TASK 2: The item includes at least one item that is
compositional.
TASK 3: The verb in the item contributes its simplex
meaning.
TASK 4: The particle in the item contributes its sim-
plex meaning.
Note the partial conditional chaining between these
tests, e.g. an item for which the verb and particle con-
tribute their simplex meaning (i.e. positive exemplars
for TASKS 3 and 4) is completely compositional (i.e.
a positive exemplar for TASK 1).
The following sections describe four methods for
modelling VPC compositionality, each of which is
tested over the 4 individual compositionality classi-
fication tasks. The results for each method are given
in Table 4, in which the baseline for each task is the
score obtained when we assign the most frequent label
to all items. Each method is evaluated in terms of pre-
cision (Prec), Recall (Rec) and F-score (? = 1, FB1),
and all values which exceed the baseline are indicated
in boldface.
5.1 Method 1
We decided to gain a sense of the start-of-the-art on
the task by reimplementing the technique described
in Lin (1999) over VPCs. In our implementation we
replaced Lin?s collocations with our VPCs, treating
the relationship between a verb and a particle as a
kind of grammatical relation. In addition to the binary
compositional/non-compositional judgement that Lin
offers (which seems to be equivalent to TASK 1), we
tested the method over the other three tasks. Ac-
knowledging, as we must, that items can be partially
compositional (i.e. have one component item con-
tributing a conventional meaning), it would seem to
be the case, according to the assumptions made by
the technique, that the substitutability of each item
will give us some insight into its semantic contribu-
tion. The thesaurus used by Lin has been generously
made available online. However this is not adequate
for our purposes since it includes only verbs, nouns
and adjectives/adverbs. We therefore replicated the
approach described in Lin (1998a) to build the the-
saurus, using BNC data and including prepositions.
5.2 Method 2
Method 2 is very similar to Method 1, except that
instead of using a thesaurus based on Lin?s method,
we took a knowledge-free approach to obtaining syn-
onyms. Our technique is very similar to the approach
taken to building a ?context space? by Schu?tze (1998).
We measured the frequency of co-occurrence of our
target words (the 20,000 most frequent words, includ-
ing all of our VPCs2 and all of their component verbs
and prepositions) with a set of 1000 ?content-bearing?
words (we used the 51st to the 1050th most frequent
words, the 50 most frequent being taken to have ex-
tremely low infomation content). A target word was
said to co-occur with a content word if that content
word occurred within a window of 5 words to either
side of it. These co-occurrence figures were stored as
feature vectors. In order to overcome data sparseness,
we used techniques borrowed from Latent Seman-
tic Indexing (LSI, Deerwester et al (1990)). LSI is
an information retrieval technique based on Singular
Value Decomposition (SVD), and works by project-
ing a term-document matrix onto a lower-dimensional
subspace, in which relationships might more easily be
2Concatenated into a single-word item
Majority Centroid 60% Agreement
1.29 4.09 0.48All (p =.255) (p=.043) (p=.488)
2.19 0.01 5.56Monosemous (p=.137) (p=.924) (p =.018)
Table 3: Logistic regression for Method 4
observed between terms which are related but do not
co-occur. We used this technique to reduce the feature
space for our target words from 1000 to 100, allow-
ing relations to be discovered between target words
even if there is not direct match between their context
words. We used the various tools in the GTP software
package, created at the University of Tennessee3 to
build these matrices from the co-occurrence data, and
to perform SVD analysis.
We calculated the similarity between two terms by
finding the cosine of the angle between their vec-
tors. We performed a pairwise comparison between
all verbs and all particles. For each term we then
sorted all of the other items of the same part-of-speech
in descending order of similarity, which gave us the
thesaurus for use in substitution. As with the Lin
method, we performed substitutions by taking the 10
most similar items for the head verb and particle of
each VPC.
5.3 Method 3
We noted in Section 4 that a significant problem
with the substitution approach is that it is sensitive to
institutionalisation rather than non-compositionality.
Method 3 attempts to adapt substitution to more ac-
curately reflect non-compositionality by removing the
assumption that an item formed by substitution should
have the same distributional characteristics as the
original item. Rather than basing the composition-
ality judgement on the relative mutual information
scores of the original items and the items resulting
from substitution, we instead base it on the corpus-
based semantic similarity between the original ex-
pression and word-substituted derivative expressions.
The same method of substitution is used, with each
component being replaced by each of its 10 nearest
neighbours according to the knowledge-free similar-
ity measure described above. We judge a VPC item
to be compositional if an expression formed by sub-
stitution occurs among the nearest 100 verb-particle
items to the original, and failing this, we judge it to be
non-compositional. We experimented with a number
of cut-off points for identifying semantically similar
items, and found that a value of 100 gave the best re-
sults.
5.4 Method 4
While Method 3 softens the reliance upon productiv-
ity as a test for compositionality, it still confuses insti-
3http://www.cs.utk.edu/?lsi/soft.html
TASK 1 TASK 2
(mean agreement = .693) (mean agreement = .750)
Majority Centroid annotator Majority Centroid annotator
Prec Rec FB1 Prec Rec FB1 Prec Rec FB1 Prec Rec FB1
Baseline .525 1.000 .680 .250 1.000 .400 .750 1.000 .860 .700 1.000 .820
Method 1 .577 .714 .638 .269 .700 .389 .731 .633 .678 .731 .679 .704
Method 2 .575 .714 .638 .308 .800 .447 .769 .667 .717 .769 .714 .739
Method 3 .558 .905 .690 .235 .800 .360 .765 .866 .810 .735 .892 .810
Method 4 .514 .857 .642 .200 .700 .280 .771 .900 .830 .714 .893 .794
TASK 3 TASK 4
(mean agreement = .729) (mean agreement = .688)
Majority Centroid annotator Majority Centroid annotator
Prec Rec FB1 Prec Rec FB1 Prec Rec FB1 Prec Rec FB1
Baseline .525 1.000 .690 .625 1.000 .770 .750 1.000 .857 .670 1.000 .800
Method 1 .474 .429 .450 .632 .480 .546 .818 .300 .442 .454 .385 .417
Method 2 .608 .666 .639 .782 .720 .749 .818 .300 .442 .454 .385 .417
Method 3 .531 .810 .641 .625 .800 .717 .769 .333 .480 .308 .308 .308
Method 4 .666 .286 .400 .666 .240 .353 .758 .833 .793 .303 .769 .435
Table 4: Results for the four methods over the different compositionality classification tasks
tutionalisation with non-compositionality somewhat
in its reliance upon substitution. We now suggest an-
other technique which we claim is based on sounder
principles. The underlying intuition is that identify-
ing the degree of semantic similarity between a VPC
and its component verb and/or particle will indicate
whether that component part contributes independent
semantics. This is similar to the assumption made
in Schone and Jurafsky (2001), except that we make
a distinction between the contribution of the different
component parts. We again used the knowledge-free
semantic similarity measure. We performed a pair-
wise comparison of all VPCs with all verbs and all
particles, obtaining cosine similarity scores for each
pair.
In order to measure the usefulness of this score,
we performed a logistic regression of the similarity
scores and the human judgements as to whether the
given verb or particle is entailed by the VPC. We did
this for the majority human judgements, and also the
centroid annotator scores. We also did the same us-
ing the majority scores but rejecting those items on
which there was less than 60% agreement. In ad-
dition to performing a regression for all items (All),
we also performed a regression for only those items
which have only one meaning according to WordNet
(Monosemous). The results for all of these are shown
in Table 3. The figures shown are chi-squared scores,
with their associated significance values. We observed
significant correlations for a number of the regres-
sions (notably all items vs. the centroid annotator, and
monosemous items vs. 60% agreement). While the
results are far from stable, such variation is perhaps to
be expected on a test like this since the nature of con-
text space models means that rogue items sometimes
get extremely high similarity scores, and we are per-
forming the regression over only 40 VPCs (80 VPC-
component pairs).
In order to build a classifier for making composi-
tionality decisions, we again used a neighbour-based
approach with a cut-off. We said that a verb was
contributing meaning to a VPC if it occurred in the
20 most similar items to the VPC. For particles, we
said that the item was contributing meaning if it was
among the 10 nearest neighbours. We tried out a range
of different cut-offs for each item and found that these
gave the best results.
6 Results
The results in Table 4 show that on all tasks (for the
majority-view based data and three out of four for the
centroid data), at least one of the four statistical meth-
ods offers an improvement in precision over the base-
line, and that there is an improvement in F-score for
TASK 1 on both sets of data. There are swings in the
relative scores obtained over the majority as compared
to centroid annotator data for a given task. In terms
of relative performance, the semantic similarity based
approach of Methods 3 and 4 outperform the distribu-
tion based approach of Methods 1 and 2 in terms of
F-score, on 6 of the 8 sets of results reported.
In order to get a reliable sense for how good these
scores are, we compare them with the level of agree-
ment across human judges. We calculated pairwise
agreement across all participants on the four classifi-
cation tasks, resulting in the figures given in Table 4.
These agreement scores give us an upper bound for
classification accuracy on each task, from which it is
possible to benchmark the classification accuracy of
the classifiers on that same task. On TASK 1, three of
the four classifiers achieved a classification accuracy
of .575. On TASK 2, the highest-performing classi-
fier (Method 4), achieved a classification accuracy of
.725. On TASK 3, Method 2 achieved the highest clas-
sification accuracy at .600, and on TASK 4, Method 4
achieved a classification accuracy of .675. We can see
then that the best classifiers perform only marginally
below the upper bound on at least two of the tasks.
While these results may appear at first glance to be
less than conclusive, we must bear in mind that we are
working with limited amounts of data and relatively
simplistic models of a cognitively intensive task. We
interpret them as very positive indicators of the via-
bility of using empirical methods to analyse VPC se-
mantics.
7 Conclusion
This paper has described the implementation and eval-
uation of four corpus-based approaches to the seman-
tics of verb-particle constructions. We created a set of
gold-standard data, based on non-expert judgements
acquired via a web-based experiment. We then imple-
mented four different techniques and showed that they
offer a significant improvement over a naive approach.
Acknowledgements
We would like to thank Ann Copestake, Maria Lapata, Diana Mc-
Carthy, Aline Villavicencio, Tom Wasow, Dominic Widdows and
the three anonymous reviewers for their valuable input on this
research. Timothy Baldwin is supported by the National Science
Foundation under Grant No. BCS-0094638 and also the Research
Collaboration between NTT Communication Science Laborato-
ries, Nippon Telegraph and Telephone Corporation and CSLI,
Stanford University. Colin Bannard is supported by ESRC Grant
PTA-030-2002-01740
References
Douglas G. Altman. 1991. Practical Statistics for Medical Re-
search. Chapman and Hall.
Timothy Baldwin and Aline Villavicencio. 2002. Extracting the
unextractable: A case study on verb-particles. In Proc. of
the 6th Conference on Natural Language Learning (CoNLL-
2002), Taipei, Taiwan.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic
Widdows. 2003. An empirical model of multiword expres-
sion decomposability. In Proc. of the ACL-2003 Workshop on
Multiword Expressions: Analysis, Acquisition and Treatment.
(this volume).
Colin Bannard. 2002. Statistical techniques for automatically
inferring the semantics of verb-particle constructions. LinGO
Working Paper No. 2002-06.
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic recog-
nition of noun modifier relationships. In Proc. of the 36th An-
nual Meeting of the ACL and 17th International Conference on
Computational Linguistics (COLING/ACL-98), pages 96?102,
Montreal, Canada.
Laurel Brinton. 1985. Verb particles in English: Aspect or ak-
tionsart. Studia Linguistica, 39:157?68.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Jean Carletta. 1996. Assessing agreement on classification tasks:
the kappa statistic. Computational Linguistics, 22(2):249?
254.
Martin Corley, Frank Keller, and Christoph Scheepers. 2000.
Conducting psychological experiments over the world wide
web. Unpublished manuscript, University of Edinburgh and
Saarland University.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990. Indexing
by latent semantic analysis. Journal of the American Society
of Information Science, 41(6).
John A. Hawkins. 2000. The relative order of preposition phrases
in English: Going beyond manner ? place ? time. Language
Variation and Change, 11:231?266.
Mark Lauer. 1995. Designing Statistical Language Learners:
Experiments on Noun Compounds. Ph.D. thesis, Macquarie
University.
Dekang Lin. 1998a. Automatic retrieval and clustering of similar
words. In Proc. of the 36th Annual Meeting of the ACL and
17th International Conference on Computational Linguistics
(COLING/ACL-98), Montreal, Canada.
Dekang Lin. 1998b. Extracting collocations from text corpora.
In First Workshop on Computational Terminology.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual Meeting
of the ACL, pages 317?24, College Park, USA.
Barbara Lohse, John A. Hawkins, and Tom Wasow. in prepara-
tion. Domain minimization in English verb-particle construc-
tions.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguistics,
19(2):313?30.
Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting
a continuum of compositionality in phrasal verbs. In Proc. of
the ACL-2003 Workshop on Multiword Expressions: Analysis,
Acquisition and Treatment. (this volume).
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proceedings of
2nd Conference on Empirical Methods in Natural Language
Processing.
G.A. Miller, R. Beckwith, C. Fellbaum, D Gross, and K.J. Miller.
1990. Introduction to WordNet: an on-line lexical database.
International Journal of Lexicography, 3(4):235?44.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting of
the North American Chapter of Association for Computational
Linguistics (NAACL2001), pages 40?7, Pittsburgh, USA.
Darren Pearce. 2002. A comparative evaluation of collocation
extraction techniques. In Proc. of the 3rd International Con-
ference on Language Resources and Evaluation (LREC 2002),
Las Palmas, Canary Islands.
Barbara Rosario and Marti Hearst. 2001. Classifying the seman-
tic relations in noun compounds via a domain-specific lexical
hierarchy. In Proc. of the 6th Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2001), Pitts-
burgh, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A pain in
the neck for NLP. In Proc. of the 3rd International Conference
on Intelligent Text Processing and Computational Linguistics
(CICLing-2002), pages 1?15, Mexico City, Mexico.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-free in-
duction of multiword unit dictionary headwords a solved prob-
lem? In Proc. of the 6th Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001), pages 100?108.
Hinrich Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
Aline Villavicencio and Ann Copestake. 2002. Phrasal verbs and
the LinGO-ERG. LinGO Working Paper No. 2002-01.
An Empirical Model of Multiword Expression Decomposability
Timothy Baldwin?, Colin Bannard?, Takaaki Tanaka? and Dominic Widdows?
? CSLI
Stanford University
Stanford CA 94305, USA
{tbaldwin,dwiddows}@csli.stanford.edu
? School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
c.j.bannard@ed.ac.uk
? Communication Science
Labs
NTT Corporation
Kyoto, Japan
takaaki@cslab.kecl.ntt.co.jp
Abstract
This paper presents a construction-
inspecific model of multiword expression
decomposability based on latent semantic
analysis. We use latent semantic analysis
to determine the similarity between a
multiword expression and its constituent
words, and claim that higher similarities
indicate greater decomposability. We
test the model over English noun-noun
compounds and verb-particles, and eval-
uate its correlation with similarities and
hyponymy values in WordNet. Based on
mean hyponymy over partitions of data
ranked on similarity, we furnish evidence
for the calculated similarities being corre-
lated with the semantic relational content
of WordNet.
1 Introduction
This paper is concerned with an empirical model of
multiword expression decomposability. Multiword
expressions (MWEs) are defined to be cohesive lex-
emes that cross word boundaries (Sag et al, 2002;
Copestake et al, 2002; Calzolari et al, 2002). They
occur in a wide variety of syntactic configurations
in different languages (e.g. in the case of English,
compound nouns: post office, verbal idioms: pull
strings, verb-particle constructions: push on, etc.).
Decomposability is a description of the degree to
which the semantics of an MWE can be ascribed
to those of its parts (Riehemann, 2001; Sag et al,
2002). Analysis of the semantic correlation between
the constituent parts and whole of an MWE is per-
haps more commonly discussed under the banner of
compositionality (Nunberg et al, 1994; Lin, 1999).
Our claim here is that the semantics of the MWE are
deconstructed and the parts coerced into often id-
iosyncratic interpretations to attain semantic align-
ment, rather than the other way around. One id-
iom which illustrates this process is spill the beans,
where the semantics of reveal?(secret?) are de-
composed such that spill is coerced into the idiosyn-
cratic interpretation of reveal? and beans into the
idiosyncratic interpretation of secret?. Given that
these senses for spill and beans are not readily avail-
able at the simplex level other than in the context
of this particular MWE, it seems fallacious to talk
about them composing together to form the seman-
tics of the idiom.
Ideally, we would like to be able to differ-
entiate between three classes of MWEs: non-
decomposable, idiosyncratically decomposable and
simple decomposable (derived from Nunberg et al?s
sub-classification of idioms (1994)). With non-
decomposable MWEs (e.g. kick the bucket, shoot
the breeze, hot dog), no decompositional anal-
ysis is possible, and the MWE is semantically
impenetrable. The only syntactic variation that
non-decomposable MWEs undergo is verbal in-
flection (e.g. kicked the bucket, kicks the bucket)
and pronominal reflexivisation (e.g. wet oneself ,
wet themselves). Idiosyncratically decomposable
MWEs (e.g. spill the beans, let the cat out of the
bag, radar footprint) are decomposable but co-
erce their parts into taking semantics unavailable
outside the MWE. They undergo a certain degree
of syntactic variation (e.g. the cat was let out of
the bag). Finally, simple decomposable MWEs
(also known as ?institutionalised? MWEs, e.g. kin-
dle excitement, traffic light) decompose into simplex
senses and generally display high syntactic variabil-
ity. What makes simple decomposable expressions
true MWEs rather than productive word combina-
tions is that they tend to block compositional al-
ternates with the expected semantics (termed anti-
collocations by Pearce (2001b)). For example, mo-
tor car cannot be rephrased as *engine car or *mo-
tor automobile. Note that the existence of anti-
collocations is also a test for non-decomposable and
idiosyncratically decomposable MWEs (e.g. hot dog
vs. #warm dog or #hot canine).
Our particular interest in decomposability stems
from ongoing work on grammatical means for cap-
turing MWEs. Nunberg et al (1994) observed that
idiosyncratically decomposable MWEs (in particu-
lar idioms) undergo much greater syntactic variation
than non-decomposable MWEs, and that the vari-
ability can be partially predicted from the decompo-
sitional analysis. We thus aim to capture the decom-
posability of MWEs in the grammar and use this to
constrain the syntax of MWEs in parsing and gen-
eration. Note that it is arguable whether simple de-
composable MWEs belong in the grammar proper,
or should be described instead as lexical affinities
between particular word combinations.
As the first step down the path toward an empir-
ical model of decomposability, we focus on demar-
cating simple decomposable MWEs from idiosyn-
cratically decomposable and non-decomposable
MWEs. This is largely equivalent to classifying
MWEs as being endocentric (i.e., a hyponym of
their head) or exocentric (i.e., not a hyponym of
their head: Haspelmath (2002)).
We attempt to achieve this by looking at the se-
mantic similarity between an MWE and its con-
stituent words, and hypothesising that where the
similarity between the constituents of an MWE and
the whole is sufficiently high, the MWE must be of
simple decomposable type.
The particular similarity method we adopt is la-
tent semantic analysis, or LSA (Deerwester et al,
1990). LSA allows us to calculate the similarity
between an arbitrary word pair, offering the advan-
tage of being able to measure the similarity between
the MWE and each of its constituent words. For
MWEs such as house boat, therefore, we can expect
to capture the fact that the MWE is highly similar in
meaning to both constituent words (i.e. the modifier
house and head noun boat). More importantly, LSA
makes no assumptions about the lexical or syntac-
tic composition of the inputs, and thus constitutes a
fully construction- and language-inspecific method
of modelling decomposability. This has clear advan-
tages over a more conventional supervised classifier-
style approach, where training data would have to be
customised to a particular language and construction
type.
Evaluation is inevitably a difficulty when it comes
to the analysis of MWEs, due to the lack of con-
cise consistency checks on what MWEs should and
should not be incorporated into dictionaries. While
recognising the dangers associated with dictionary-
based evaluation, we commit ourselves to this
paradigm and focus on searching for appropriate
means of demonstrating the correlation between
dictionary- and corpus-based similarities.
The remainder of this paper is structured as fol-
lows. Section 2 describes past research on MWE
compositionality of relevance to this effort. Sec-
tion 3 provides a basic outline of the resources used
in this research, LSA, the MWE extraction methods,
and measures used to evaluate our method. Section 4
then provides evaluation of the proposed method,
and the paper is concluded with a brief discussion
in Section 5.
2 Past research
Although there has been some useful work on com-
positionality in statistical machine translation (e.g.
Melamed (1997)), there has been little work on de-
tecting ?non-compositional? (i.e. non-decomposable
and idiosyncratically decomposable) items of vari-
able syntactic type in monolingual corpora. One in-
teresting exception is Lin (1999), whose approach is
explained as follows:
The intuitive idea behind the method is
that the metaphorical usage of a non-
compositional expression causes it to
have a different distributional characteris-
tic than expressions that are similar to its
literal meaning.
The expressions he uses are taken from a colloca-
tion database (Lin, 1998b). These ?expressions that
are similar to [their] literal meaning? are found by
substituting each of the words in the expression with
the 10 most similar words according to a corpus de-
rived thesaurus (Lin, 1998a). Lin models the dis-
tributional difference as a significant difference in
mutual information. Significance here is defined as
the absence of overlap between the 95% confidence
interval of the mutual information scores. Lin pro-
vides some examples that suggest he has identified
a successful measure of ?compositionality?. He of-
fers an evaluation where an item is said to be non-
compositional if it occurs in a dictionary of idioms.
This produces the unconvincing scores of 15.7% for
precision and 13.7% for recall.
We claim that substitution-based tests are use-
ful in demarcating MWEs from productive word
combinations (as attested by Pearce (2001a) in a
MWE detection task), but not in distinguishing the
different classes of decomposability. As observed
above, simple decomposable MWEs such as mo-
tor car fail the substitution test not because of non-
decomposability, but because the expression is in-
stitutionalised to the point of blocking alternates.
Thus, we expect Lin?s method to return a wide ar-
ray of both decomposable and non-decomposable
MWEs.
Bannard (2002) focused on distributional tech-
niques for describing the meaning of verb-particle
constructions at the level of logical form. The
semantic similarity between a multiword expres-
sion and its head was used as an indicator of
decomposability. The assumption was that if a
verb-particle was sufficiently similar to its head
verb, then the verb contributed its simplex mean-
ing. It gave empirical backing to this assump-
tion by showing that annotator judgements for verb-
particle decomposability correlate significantly with
non-expert human judgements on the similarity be-
tween a verb-particle construction and its head verb.
Bannard et al (2003) extended this research in look-
ing explicitly at the task of classifying verb-particles
as being compositional or not. They successfully
combined statistical and distributional techniques
(including LSA) with a substitution test in analysing
compositionality. McCarthy et al (2003) also tar-
geted verb-particles for a study on compositionality,
and judged compositionality according to the degree
of overlap in the N most similar words to the verb-
particle and head verb, e.g., to determine composi-
tionality.
We are not the first to consider applying LSA to
MWEs. Schone and Jurafsky (2001) applied LSA to
the analysis of MWEs in the task of MWE discov-
ery, by way of rescoring MWEs extracted from a
corpus. The major point of divergence from this re-
search is that Schone and Jurafsky focused specifi-
cally on MWE extraction, whereas we are interested
in the downstream task of semantically classifying
attested MWEs.
3 Resources and Techniques
In this section, we outline the resources used in eval-
uation, give an informal introduction to the LSA
model, sketch how we extracted the MWEs from
corpus data, and describe a number of methods
for modelling decomposability within a hierarchical
lexicon.
3.1 Resources and target MWEs
The particular reference lexicon we use to eval-
uate our technique is WordNet 1.7 (Miller et
al., 1990), due to its public availability, hier-
archical structure and wide coverage. Indeed,
Schone and Jurafsky (2001) provide evidence that
suggests that WordNet is as effective an evaluation
resource as the web for MWE detection methods,
despite its inherent size limitations and static nature.
Two MWE types that are particularly well repre-
sented in WordNet are compound nouns (47,000 en-
tries) and multiword verbs (2,600 entries). Of these,
we chose to specifically target two types of MWE:
noun-noun (NN) compounds (e.g. computer net-
work, work force) and verb-particles (e.g. look on,
eat up) due to their frequent occurrence in both de-
composable and non-decomposable configurations,
and also their disparate syntactic behaviours.
We extracted the NN compounds from the 1996
Wall Street Journal data (WSJ, 31m words), and
the verb-particles from the British National Corpus
(BNC, 90m words: Burnard (2000)). The WSJ data
is more tightly domain-constrained, and thus a more
suitable source for NN compounds if we are to ex-
pect sentential context to reliably predict the seman-
tics of the compound. The BNC data, on the other
hand, contains more colloquial and prosaic texts and
is thus a richer source of verb-particles.
3.2 Description of the LSA model
Our goal was to compare the distribution of differ-
ent compound terms with their constituent words, to
see if this indicated similarity of meaning. For this
purpose, we used latent semantic analysis (LSA) to
build a vector space model in which term-term sim-
ilarities could be measured.
LSA is a method for representing words as points
in a vector space, whereby words which are related
in meaning should be represented by points which
are near to one another, first developed as a method
for improving the vector model for information re-
trieval (Deerwester et al, 1990). As a technique for
measuring similarity between words, LSA has been
shown to capture semantic properties, and has been
used successfully for recognising synonymy (Lan-
dauer and Dumais, 1997), word-sense disambigua-
tion (Schu?tze, 1998) and for finding correct transla-
tions of individual terms (Widdows et al, 2002).
The LSA model we built is similar to that de-
scribed in (Schu?tze, 1998). First, 1000 frequent con-
tent words (i.e. not on the stoplist)1 were chosen
as ?content-bearing words?. Using these content-
bearing words as column labels, the 50,000 most
frequent terms in the corpus were assigned row
vectors by counting the number of times they oc-
1A ?stoplist? is a list of frequent words which have little
independent semantic content, such as prepositions and deter-
miners (Baeza-Yates and Ribiero-Neto, 1999, p167).
curred within the same sentence as a content-bearing
word. Singular-value decomposition (Deerwester et
al., 1990) was then used to reduce the number of
dimensions from 1000 to 100. Similarity between
two vectors (points) was measured using the cosine
of the angle between them, in the same way as the
similarity between a query and a document is often
measured in information retrieval (Baeza-Yates and
Ribiero-Neto, 1999, p28). Effectively, we could use
LSA to measure the extent to which two words or
MWEs x and y usually occur in similar contexts.
Since the corpora had been tagged with parts-of-
speech, we could build syntactic distinctions into the
LSA models ? instead of just giving a vector for
the string test we were able to build separate vec-
tors for the nouns, verbs and adjectives test. This
combination of technologies was also used to good
effect by Widdows (2003): an example of the con-
tribution of part-of-speech information to extracting
semantic neighbours of the word fire is shown in
Table 1. As can be seen, the noun fire (as in the
substance/element) and the verb fire (mainly used
to mean firing some sort of weapon) are related to
quite different areas of meaning. Building a single
vector for the string fire confuses this distinction ?
the neighbours of fire treated just as a string include
words related to both the meaning of fire as a noun
(more frequent in the BNC) and as a verb. The ap-
propriate granularity of syntactic classifications is an
open question for this kind of research: treating all
the possible verbs categories as different (e.g. dis-
tinguishing infinitive from finite from gerund forms)
led to data sparseness, and instead we considered
?verb? as a single part-of-speech type.
3.3 MWE extraction methods
NN compounds were extracted from the WSJ by
first tagging the data with fnTBL 1.0 (Ngai and Flo-
rian, 2001) and then simply taking noun bigrams
(adjoined on both sides by non-nouns to assure the
bigram is not part of a larger compound nominal).
Out of these, we selected those compounds that are
listed in WordNet, resulting in 5,405 NN compound
types (208,000 tokens).
Extraction of the verb-particles was consider-
ably more involved, and drew on the method of
Baldwin and Villavicencio (2002). Essentially, we
used a POS tagger and chunker (both built using
fnTBL 1.0 (Ngai and Florian, 2001)) to first (re)tag
the BNC. This allowed us to extract verb-particle to-
kens through use of the particle POS and chunk tags
returned by the two systems. This produces high-
precision, but relatively low-recall results, so we
performed the additional step of running a chunk-
based grammar over the chunker output to detect
candidate mistagged particles. In the case that a
noun phrase followed the particle candidate, we per-
formed attachment disambiguation to determine the
transitivity of the particle candidate. These three
methods produced three distinct sets of verb-particle
tokens, which we carried out weighted voting over
to determine the final set of verb-particle tokens. A
total of 461 verb-particles attested in WordNet were
extracted (160,765 tokens).
For both the NN compound and verb-particle
data, we replaced each token occurrence with a
single-word POS-tagged token to feed into the LSA
model.
3.4 Techniques for evaluating correlation with
WordNet
In order to evaluate our approach, we employed the
lexical relations as defined in the WordNet lexical
hierarchy (Miller et al, 1990). WordNet groups
words into sets with similar meaning (known as
?synsets?), e.g. {car, auto, automobile, machine,
motorcar } . These are organised into a hierarchy
employing multiple inheritance. The hierarchy is
structured according to different principles for each
of nouns, verbs, adjectives and adverbs. The nouns
are arranged according to hyponymy or ISA rela-
tions, e.g. a car is a kind of automobile. The verbs
are arranged according to troponym or ?manner-of?
relations, where murder is a manner of killing, so
kill immediately dominates murder in the hierarchy.
We used WordNet for evaluation by way of look-
ing at: (a) hyponymy, and (b) semantic distance.
Hyponymy provides the most immediate way of
evaluating decomposability. With simple decompos-
able MWEs, we can expect the constituents (and
particularly the head) to be hypernyms (ancestor
nodes) or synonyms of the MWE. That is, simple
decomposable MWEs are generally endocentric, al-
though there are some exceptions to this generali-
sation such as vice president arguably not being a
hyponym of president. No hyponymy relation holds
with non-decomposable or idiosyncratically decom-
posable MWEs (i.e., they are exocentric), as even if
the semantics of the head noun can be determined
through decomposition, by definition this will not
correspond to a simplex sense of the word.
We deal with polysemy of the constituent words
and/or MWE by simply looking for the exis-
tence of a sense of the constituent words which
fire (string only) fire nn1 fire vvi
fire 1.000000 fire nn1 1.000000 fire vvi 1.000000
flames 0.709939 flames nn2 0.700575 guns nn2 0.663820
smoke 0.680601 smoke nn1 0.696028 firing vvg 0.537778
blaze 0.668504 brigade nn1 0.589625 cannon nn0 0.523442
firemen 0.627065 fires nn2 0.584643 gun nn1 0.484106
fires 0.617494 firemen nn2 0.567170 fired vvd 0.478572
explosion 0.572138 explosion nn1 0.551594 detectors nn2 0.477025
burning 0.559897 destroyed vvn 0.547631 artillery nn1 0.469173
destroyed 0.558699 burning aj0 0.533586 attack vvb 0.468767
brigade 0.532248 blaze nn1 0.529126 firing nn1 0.459000
arson 0.528909 arson nn1 0.522844 volley nn1 0.458717
accidental 0.519310 alarms nn2 0.512332 trained vvn 0.447797
chimney 0.489577 destroyed vvd 0.512130 enemy nn1 0.445523
blast 0.488617 burning vvg 0.502052 alert aj0 0.443610
guns 0.487226 burnt vvn 0.500864 shoot vvi 0.443308
damaged 0.484897 blast nn1 0.498635 defenders nn2 0.438886
Table 1: Semantic neighbours of fire with different parts-of-speech. The scores are cosine similarities
subsumes a sense of the MWE. The function
hyponym(word i,mwe) thus returns a value of 1 if
some sense of word i subsumes a sense of mwe , and
a value of 0 otherwise.
A more proactive means of utilising the WordNet
hierarchy is to derive a semantic distance based on
analysis of the relative location of senses in Word-
Net. Budanitsky and Hirst (2001) evaluated the per-
formance of five different methods that measure
the semantic distance between words in the Word-
Net Hierarchy, which Patwardhan et al (2003) have
then implemented and made available for general
use as the Perl package distance-0.11.2 We fo-
cused in particular on the following three measures,
the first two of which are based on information the-
oretic principles, and the third on sense topology:
? Resnik (1995) combined WordNet with corpus
statistics. He defines the similarity between
two words as the information content of the
lowest superordinate in the hierarchy, defining
the information content of a concept c (where
a concept is the WordNet class containing the
word) to be the negative of its log likelihood.
This is calculated over a corpus of text.
? Lin (1998c) also employs the idea of corpus-
derived information content, and defines the
similarity between two concepts in the follow-
ing way:
sim(C1, C2) =
2 log P (C0)
log P (C1) + log P (C2)
(1)
where C0 is the lowest class in the hierarchy
that subsumes both classes.
2http://www.d.umn.edu/?tpederse/
distance.html
? Hirst and St-Onge (1998) use a system of ?re-
lations? of different strength to determine the
similarity of word senses, conditioned on the
type, direction and relative distance of edges
separating them.
The Patwardhan et al (2003) implementation that
we used calculates the information values from
SemCor, a semantically tagged subset of the Brown
corpus. Note that the first two similarity measures
operate over nouns only, while the last can be ap-
plied to any word class.
The similarity measures described above calcu-
late the similarity between a pair of senses. In the
case that a given constituent word and/or MWE oc-
cur with more than one sense, we calculate a similar-
ity for sense pairing between them, and average over
them to produce a consolidated similarity value.
4 Evaluation
LSA was used to build models in which MWEs
could be compared with their constituent words.
Two models were built, one from the WSJ corpus
(indexing NN compounds) and one from the BNC
(indexing verb-particles). After removing stop-
words, the 50,000 most frequent terms were indexed
in each model. From the WSJ, these 50,000 terms
included 1,710 NN compounds (with corpus fre-
quency of at least 13) and from the BNC, 461 verb-
particles (with corpus frequency of at least 49).
We used these models to compare different words,
and to find their neighbours. For example, the neigh-
bours of the simplex verb cut and the verb-particles
cut out and cut off (from the BNC model) are shown
in Table 2. As can be seen, several of the neighbours
of cut out are from similar semantic areas as those
of cut, whereas those of cut off are quite different.
cut (verb) cut out (verb) cut off (verb)
cut verb 1.000000 cut out verb 1.000000 cut off verb 1.000000
trim verb 0.529886 fondant nn 0.516956 knot nn 0.448871
slash verb 0.522370 fondant jj 0.501266 choke verb 0.440587
cut nns 0.520345 strip nns 0.475293 vigorously rb 0.438071
cut nn 0.502100 piece nns 0.449555 suck verb 0.413003
reduce verb 0.465364 roll nnp 0.440769 crush verb 0.412301
cut out verb 0.433465 stick jj 0.434082 ministry nn 0.408702
pull verb 0.431929 cut verb 0.433465 glycerol nn 0.395148
fall verb 0.426111 icing nn 0.432307 tap verb 0.383932
hook verb 0.419564 piece nn 0.418780 shake verb 0.381581
recycle verb 0.413206 paste nn 0.416581 jerk verb 0.381284
project verb 0.401246 tip nn 0.413603 put down verb 0.380368
recycled jj 0.396315 hole nns 0.412813 circumference nn 0.378097
prune verb 0.395656 straw nn 0.411617 jn nnp 0.375634
pare verb 0.394991 hook nn 0.402947 pump verb 0.373984
tie verb 0.392964 strip nn 0.399974 nell nnp 0.373768
Table 2: Semantic neighbours of the verbs cut, cut out, and cut off .
Construction Method Pearson R2
Resnik .108 .012
NN compound Lin .101 .010
HSO .072 .005
verb-particle HSO .255 .065
Table 3: Correlation between LSA and WordNet
similarities
This reflects the fact that in most of its instances the
verb cut off is used to mean ?forcibly isolate?.
In order to measure this effect quantitatively, we
can simply take the cosine similarities between these
verbs, finding that sim(cut, cut out) = 0.433 and
sim(cut, cut off) = 0.183 from which we infer di-
rectly that, relative to the sense of cut, cut out is a
clearer case of a simple decomposable MWE than
cut off .
4.1 Statistical analysis
In order to get an initial feel for how well
the LSA-based similarities for MWEs and their
head words correlate with the WordNet-based
similarities over those same word pairs, we
did a linear regression and Pearson?s correla-
tion analysis of the paired data (i.e. the pair-
ing ?simLSA(word i,mwe), simWN(word i,mwe)?
for each WordNet similarity measure simWN). For
both tests, values closer to 0 indicate random distri-
bution of the data, whereas values closer to 1 indi-
cate a strong correlation. The correlation results for
NN compounds and verb-particles are presented in
Table 3, where R2 refers to the output of the linear
regression test and HSO refers to Hirst and St-Onge
similarity measure. In the case of NN compounds,
the correlation with LSA is very low for all tests,
that is LSA is unable to reproduce the relative sim-
ilarity values derived from WordNet with any reli-
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3
M
ea
n
 H
yp
on
ym
y
Partition No.
VPC(head)          
VPC(head)       ALL
HIGH
NN(mod)       
NN(head)       ALL
ALL
NN(head)         LOW
NN(head)          HIGH
VPC(head)         LOW
Figure 1: Hyponymy correlation
ability. With verb-particles, correlation is notably
higher than for NN compounds,3 but still at a low
level.
Based on these results, LSA would appear to
correlate poorly with WordNet-based similarities.
However, our main interest is not in similarity per
se, but how reflective LSA similarities are of the de-
composability of the MWE in question. While tak-
ing note of the low correlation with WordNet simi-
larities, therefore, we move straight on to look at the
hyponymy test.
4.2 Hyponymy-based analysis
We next turn to analysis of correlation between LSA
similarities and hyponymy values. Our expectation
is that for constituent word?MWE pairs with higher
LSA similarities, there is a greater likelihood of the
MWE being a hyponym of the constituent word. We
test this hypothesis by ranking the constituent word?
MWE pairs in decreasing order of LSA similarity,
3Recall that HSO is the only similarity measure which oper-
ates over verbs.
and partitioning the ranking up into m partitions of
equal size. We then calculate the average number of
hyponyms per partition. If our hypothesis is correct,
the earlier partitions (with higher LSA similarities)
will have higher occurrences of hyponyms than the
latter partitions.
Figure 1 presents the mean hyponymy values
across partitions of the NN compound data and verb-
particle data, with m set to 3 in each case. For the
NN compounds, we derive two separate rankings,
based on the similarity between the head noun and
NN compound (NN(head)) and the modifier noun
and the NN compound (NN(mod)). In the case of
the verb-particle data, WordNet has no classification
of prepositions or particles, so we can only calcu-
late the similarity between the head verb and verb-
particle (VPC(head)). Looking to the curves for
these three rankings, we see that they are all fairly
flat, nondescript curves. If we partition the data up
into low- and high-frequency MWEs, as defined by a
threshold of 100 corpus occurrences, we find that the
graphs for the low-frequency data (NN(head)LOW
and VPC(head)LOW) are both monotonically de-
creasing, whereas those for high-frequency data
(NN(head)HIGH and VPC(head)HIGH) are more hap-
hazard in nature. Our hypothesis of lesser instances
of hyponymy for lower similarities is thus supported
for low-frequency items but not for high-frequency
items, suggesting that LSA similarities are more
brittle over high-frequency items for this particu-
lar task. The results for the low-frequency items
are particularly encouraging given that the LSA-
based similarities were found to correlate poorly
with WordNet-derived similarities. The results for
NN(mod) are more erratic for both low- and high-
frequency terms, that is the modifier noun is not as
strong a predictor of decomposability as the head
noun. This is partially supported by the statistics on
the relative occurrence of NN compounds in Word-
Net subsumed by their head noun (71.4%) as com-
pared to NN compounds subsumed by their modifier
(13.7%).
In an ideal world, we would hope that the val-
ues for mean hyponymy were nearly 1 for the first
partition and nearly 0 for the last. Naturally, this
presumes perfect correlation of the LSA similarities
with decomposability, but classificational inconsis-
tencies in WordNet alo work against us. For ex-
ample, vice chairman is an immediate hyponym of
both chairman and president, but vice president is
not a hyponym of president. According to LSA,
however, sim(chairman, vice chairman) = .508 and
sim(president, vice president) = .551.
It remains to be determined why LSA should per-
form better over low-frequency items, although the
higher polysemy of high-frequency items is one po-
tential cause. We intend to further investigate this
matter in future research.
5 Discussion
While evaluation pointed to a moderate correlation
between LSA similarities and occurrences of hy-
ponymy, we have yet to answer the question of
exactly where the cutoffs between simple decom-
posable, idiosyncratically decomposable and non-
decomposable MWEs lie. While it would be pos-
sible to set arbitrary thresholds to artificially parti-
tion up the space of MWEs based on LSA similarity
(or alternatively use statistical tests to derive confi-
dence intervals for similarity values), we feel that
more work needs to be done in establishing exactly
what different LSA similarities for different MWE?
constituent word combinations mean.
One area in which we plan to extend this research
is the analysis of MWEs in languages other than
English. Because of LSA?s independence from lin-
guistic constraints, it is equally applicable to all lan-
guages, assuming there is some way of segmenting
inputs into constituent words.
To summarise, we have proposed a construction-
inspecific empirical model of MWE decomposabil-
ity, based on latent semantic analysis. We evaluated
the method over English NN compounds and verb-
particles, and showed it to correlate moderately with
WordNet-based hyponymy values.
Acknowledgements
This material is partly based upon work supported by the Na-
tional Science Foundation under Grant No. BCS-0094638 and
also the Research Collaboration between NTT Communication
Science Laboratories, Nippon Telegraph and Telephone Corpo-
ration and CSLI, Stanford University. We would like to thank
the anonymous reviewers for their valuable input on this re-
search.
References
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999. Modern
Information Retrieval. Addison Wesley / ACM press.
Timothy Baldwin and Aline Villavicencio. 2002. Extracting
the unextractable: A case study on verb-particles. In Proc. of
the 6th Conference on Natural Language Learning (CoNLL-
2002), Taipei, Taiwan.
Colin Bannard, Timothy Baldwin, and Alex Lascarides. 2003.
A statistical approach to the semantics of verb-particles. In
Proc. of the ACL-2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment. (this volume).
Colin Bannard. 2002. Statistical techniques for automati-
cally inferring the semantics of verb-particle constructions.
LinGO Working Paper No. 2002-06.
Alexander Budanitsky and Graeme Hirst. 2001. Semantic dis-
tance in WordNet: An experimental, application-oriented
evaluation of five measures. In Workshop on Wordnet and
Other Lexical Resources, Second meeting of the NAACL,
Pittsburgh, USA.
Lou Burnard. 2000. User Reference Guide for the British Na-
tional Corpus. Technical report, Oxford University Comput-
ing Services.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman, Nancy
Ide, Alessandro Lenci, Catherine MacLeod, and Antonio
Zampolli. 2002. Towards best practice for multiword ex-
pressions in computational lexicons. In Proceedings of the
Third International Conference on Language Resources and
Evaluation (LREC 2002), pages 1934?40, Las Palmas, Ca-
nary Islands.
Ann Copestake, Fabre Lambeau, Aline Villavicencio, Francis
Bond, Timothy Baldwin, Ivan A. Sag, and Dan Flickinger.
2002. Multiword expressions: Linguistic precision and
reusability. In Proc. of the 3rd International Conference
on Language Resources and Evaluation (LREC 2002), pages
1941?7, Las Palmas, Canary Islands.
Scott Deerwester, Susan Dumais, George Furnas, Thomas Lan-
dauer, and Richard Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society for In-
formation Science, 41(6):391?407.
Martin Haspelmath. 2002. Understanding Morphology.
Arnold Publishers.
Graeme Hirst and David St-Onge. 1998. Lexical chains as
representations of context for the detection and correction
of malapropism. In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 305?32. MIT Press,
Cambridge, USA.
Thomas Landauer and Susan Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis theory of ac-
quisition. Psychological Review, 104(2):211?240.
Dekang Lin. 1998a. Automatic retrieval and clustering of simi-
lar words. In Proceedings of the 36th Annual Meeting of the
ACL and 17th International Conference on Computational
Linguistics (COLING/ACL-98).
Dekang Lin. 1998b. Extracting collocations from text corpora.
In First Workshop on Computational Terminology.
Dekang Lin. 1998c. An information-theoretic definition of
similarity. In Proceedings of the 15th International Confer-
ence on Machine Learning.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of the 37th Annual Meeting
of the ACL, pages 317?24, College Park, USA.
Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting
a continuum of compositionality in phrasal verbs. In Proc. of
the ACL-2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment. (this volume).
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc. of the
2nd Conference on Empirical Methods in Natural Language
Processing (EMNLP-97), Providence, USA.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduction
to WordNet: an on-line lexical database. International Jour-
nal of Lexicography, 3(4):235?44.
Grace Ngai and Radu Florian. 2001. Transformation-based
learning in the fast lane. In Proc. of the 2nd Annual Meeting
of the North American Chapter of Association for Compu-
tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,
USA.
Geoffrey Nunberg, Ivan A. Sag, and Tom Wasow. 1994. Id-
ioms. Language, 70:491?538.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen.
2003. Using measures of semantic relatedness for word
sense disambiguation. In Proc. of the 4th International Con-
ference on Intelligent Text Processing and Computational
Linguistics (CICLing-2003), Mexico City, Mexico.
Darren Pearce. 2001a. Synonymy in collocation extraction. In
Proc. of the NAACL 2001 Workshop on WordNet and Other
Lexical Resources: Applications, Extensions and Customiza-
tions, Pittsburgh, USA.
Darren Pearce. 2001b. Using conceptual similarity for collo-
cation extraction. In Proc. of the 4th UK Special Interest
Group for Computational Linguistics (CLUK4).
Philip Resnik. 1995. Using information content to evaluate
semantic similarity. In Proceedings of the 14th International
Joint Conference on Artificial Intelligence.
Susanne Riehemann. 2001. A Constructional Approach to Id-
ioms and Word Formation. Ph.D. thesis, Stanford.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake,
and Dan Flickinger. 2002. Multiword expressions: A pain in
the neck for NLP. In Proc. of the 3rd International Confer-
ence on Intelligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1?15, Mexico City, Mexico.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-free
induction of multiword unit dictionary headwords a solved
problem? In Proc. of the 6th Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2001), pages
100?108.
Hinrich Sch u?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
Dominic Widdows, Beate Dorow, and Chiu-Ki Chan. 2002.
Using parallel corpora to enrich multilingual lexical re-
sources. In Third International Conference on Language Re-
sources and Evaluation, pages 240?245, Las Palmas, Spain,
May.
Dominic Widdows. 2003. Unsupervised methods for develop-
ing taxonomies by combining syntactic and statistical infor-
mation. In Proc. of the 3rd International Conference on Hu-
man Language Technology Research and 4th Annual Meet-
ing of the NAACL (HLT-NAACL 2003). (to appear).
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 1?8,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Measure of Syntactic Flexibility for Automatically Identifying Multiword
Expressions in Corpora
Colin Bannard
Department of Developmental and Comparative Psychology
Max Planck Institute for Evolutionary Anthropology
Deutscher Platz 6
D-04103 Leipzig
colin.bannard@eva.mpg.de
Abstract
Natural languages contain many multi-word
sequences that do not display the variety of
syntactic processes we would expect given
their phrase type, and consequently must be
included in the lexicon as multiword units.
This paper describes a method for identify-
ing such items in corpora, focussing on En-
glish verb-noun combinations. In an eval-
uation using a set of dictionary-published
MWEs we show that our method achieves
greater accuracy than existing MWE extrac-
tion methods based on lexical association.
1 Introduction
A multi-word expression (henceforth MWE) is usu-
ally taken to be any word combination (adjacent or
otherwise) that has some feature (syntactic, semantic
or purely statistical) that cannot be predicted on the
basis of its component words and/or the combinato-
rial processes of the language. Such units need to be
included in any language description that hopes to
account for actual usage. Lexicographers (for both
printed dictionaries and NLP systems) therefore re-
quire well-motivated ways of automatically identi-
fying units of interest. The work described in this
paper is a contribution to this task.
Many linguists have offered classification
schemes for MWEs. While these accounts vary in
their terminology, they mostly focus on three differ-
ent phenomena: collocation, non-compositionality
and syntactic fixedness. In computational linguis-
tics, a great deal of work has been done on the
extraction of collocations in the last decade and
a half (see Pecina (2005) for a survey). There
have also been a number of papers focusing on the
detection of semantic non-compositional items in
recent years beginning with the work of Schone
and Jurafsky (2001). The task of identifying
syntactically-fixed phrases, however, has been much
less explored. This third variety is the focus of
the present paper. Languages contain many word
combinations that do not allow the variation we
would expect based solely on their grammatical
form. In the most extreme case there are many
phrases which seem to allow no syntactic variation
whatsoever. These include phrases such as by
and large and in short, which do not allow any
morphological variation (*in shortest) or internal
modification (*by and pretty large). We focus here
on phrases that allow some syntactic variation, but
do not allow other kinds.
The small amount of previous work on the iden-
tification of syntactic fixedness (Wermter and Hahn
(2004), Fazly and Stevenson (2006)) has either fo-
cused on a single variation variety, or has only been
evaluated for combinations of a small preselected
list of words, presumably due to noise. In this pa-
per we employ a syntactic parser, thus allowing us
to include a wider range of syntactic features in our
model. Furthermore we describe a statistical mea-
sure of variation that is robust enough to be freely
evaluated over the full set of possible word combi-
nations found in the corpus.
The remainder of our paper will be structured as
follows. Section 2 will discuss the kinds of fixedness
that we observe in our target phrase variety. Sec-
1
tion 3 will describe our model. Section 4 will eval-
uate the performance of the method and compare it
to some other methods that have been described in
the literature. Section 5 will describe some previous
work on the problem, and section 6 will review our
findings.
2 Syntactic Fixedness in English Verb
Phrases
The experiments described here deal with one par-
ticular variety of phrase: English verb phrases of the
form verb plus noun (e.g. walk the dog, pull teeth,
take a leaflet). In a survey of the idiomatic phrases
listed in the Collins Cobuild Dictionary of Idioms,
Villavicencio and Copestake (2002) found this kind
of idiom to account for more of the entries than any
other. Riehemann (2001) performed a manual cor-
pus analysis of verb and noun phrase idioms found
in the Collins Cobuild Dictionary of Idioms. She
found considerable fixedness with some phrases al-
lowing no variation at all.
Based on this literature we identified three im-
portant kinds of non-morphological variation that
such phrases can undergo, and which crucially have
been observed to be restricted for particular combi-
nations. These are as follows:
? Variation, addition or dropping of a determiner
so that, for example, run the show becomes run
their show, make waves becomes make more
waves, or strike a chord becomes strike chord
respectively.
? Modification of the noun phrase so that, for ex-
ample, break the ice becomes break the diplo-
matic ice. We refer to this as internal modifica-
tion.
? The verb phrase passivises so that, for example,
call the shots is realised as the shots were called
by.
3 Our Model
We use the written component of the BNC to make
observations about the extent to which these varia-
tions are permitted by particular verb-noun combi-
nations. In order to do this we need some way to
a) identify such combinations, and b) identify when
they are displaying a syntactic variation. In order to
do both of these we utilise a syntactic parser.
We parse our corpus using the RASP system
(Briscoe and Carroll, 2002). The system contains
a LR probabilistic parser, based on a tag-sequence
grammar. It is particularly suited to this task be-
cause unlike many contemporary parsers, it makes
use of no significant information about the probabil-
ity of seeing relationships between particular lexical
items. Since we are looking here for cases where
the syntactic behaviour of particular word combina-
tions deviates from general grammatical patterns, it
is desirable that the analysis we use has not already
factored in lexical information. Example output can
be seen in figure 1. We extract all verb and nouns
pairs connected by an object relation in the parsed
corpus. We are interested here in the object relation-
ship between buy and apartment, and we can use the
output to identify the variations that this phrase dis-
plays.
The first thing to note is that the phrase is pas-
sivised. Apartment is described as an object of buy
by the ?obj? relation that appears at the end of the
line. Because of the passivisation, apartment is also
described as a non-clausal subject of buy by the ?nc-
mod? relation that appears at the beginning of the
line. This presence of a semantic object that appears
as a surface subject tells us that we are a dealing
with a passive. The ?ncmod? relation tells us that
the adjective largest is a modifier of apartment. And
finally, the ?detmod? relation tells us that the is a de-
terminer attached to apartment. We make a count
over the whole corpus of the number of times each
verb-object pair occurs, and the number of times it
occurs with each relation of interest.
For passivisation and internal modification, a vari-
ation is simply the presence of a particular grammat-
ical relation. The addition, dropping or variation of
a determiner is not so straightforward. We are inter-
ested in the frequency with which each phrase varies
from its dominant determiner status. We need there-
fore to determine what this dominant status is for
each item. A verb and noun object pair where the
noun has no determiner relation is recorded as hav-
ing no determiner. This is one potential determiner
status. The other varieties of status are defined by
the kind of determiner that is appended. The RASP
parser uses the very rich CLAWS-2 tagset. We con-
2
(|ncsubj| |buy+ed:6_VVN| |apartment:3_NN1| |obj|)
(|arg_mod| |by:7_II| |buy+ed:6_VVN| |couple:10_NN1| |subj|)
(|ncmod| _ |apartment:3_NN1| |largest:2_JJT|)
(|detmod| _ |apartment:3_NN1| |The:1_AT|)
(|ncmod| _ |couple:10_NN1| |Swedish:9_JJ|)
(|detmod| _ |couple:10_NN1| |a:8_AT1|)
(|mod| _ |buy+ed:6_VVN| |immediately:5_RR|)
(|aux| _ |buy+ed:6_VVN| |be+ed:4_VBDZ|)
Figure 1: RASP parse of sentence The largest apartment was immediately bought by a Swedish couple.
sider each of these tags as a different determiner sta-
tus. Once the determiner status of all occurrences
has been recorded, the dominant status for each item
is taken to be the status that occurs most frequently.
The number of variations is taken to be the number
of times that the phrase occurs with any other status.
3.1 Quantifying variation
We are interested here in measuring the degree of
syntactic variation allowed by each verb-object pair
found in our corpus. Firstly we use the counts that
we extracted above to estimate the probability of
each variation for each combination, employing a
Laplace estimator to deal with zero counts.
A straightforward product of these probabilities
would give us the probability of free variation for a
given verb-object pair. We need, however, to con-
sider the fact that each phrase has a prior probability
of variation derived from the probability of variation
of the component words. Take passivisation for ex-
ample. Some verbs are more prone to passivisation
than others. The degree of passivisation of a phrase
will therefore depend to a large extent upon the pas-
sivisation habits of the component verb.
What we want is an estimate of the extent to
which the probability of variation for that combi-
nation deviates from the variation we would expect
based on the variation we observe for its component
words. For this we use conditional pointwise mu-
tual information. Each kind of variation is associ-
ated with a single component word. Passivisation is
associated with the verb. Internal modification and
determiner variation are associated with the object.
We calculate the mutual information of the syntactic
variation x and the word y given the word z, as seen
in equation 1. In the case of passivisation z will be
the verb and y will be the object. In the case of inter-
nal modification and determiner variation z will be
the object.
I(x; y|z) = H(x|z) ? H(x|y, z) (1)
= ? log2 p(x|z) ? [? log2 p(x|y, z)]
= ? log2 p(x|z) + log2 p(x|y, z)
= log2
p(x|y, z)
p(x|z)
Conditional pointwise mutual information tells us
the amount of information in bits that y provides
about x (and vice versa) given z (see e.g. MacKay
(2003)). If a variation occurs for a given word pair
with greater likelihood than we would expect based
on the frequency of seeing that same variation with
the relevant component word, then the mutual infor-
mation will be high. We want to find the informa-
tion that is gained about all the syntactic variations
by a particular verb and object combination. We
therefore calculate the information gained about all
the verb-relevant syntactic variations (passivisation)
by the addition of the object, and the information
gained about all the object relevant variations (inter-
nal modification and determiner dropping, variation
or addition) by the addition of the verb. Summing
these, as in equation 2 then gives us the total infor-
mation gained about syntactic variation for the word
pair W, and we take this as our measure of the degree
of syntactic flexibility for this pair.
SynV ar(W )=
n?
i
I(V erbV ari;Obj|V erb) (2)
+
n?
j
I(ObjV arj ;V erb|Obj)
3
4 Evaluation
This paper aims to provide a method for highlight-
ing those verb plus noun phrases that are syntacti-
cally fixed and consequently need to be included in
the lexicon. This is intended as a tool for lexicog-
raphers. We hypothesize that in a list that has been
inversely ranked with the variability measure valid
MWEs will occur at the top.
The evaluation procedure used here (first sug-
gested by Evert and Krenn (2001) for evaluating
measures of lexical association) involves producing
and evaluating just such a ranking. The RASP parser
identifies 979,156 unique verb-noun pairs in the
BNC. The measure of syntactic flexibility was used
to inverse rank these items (the most fixed first).1
This ranking was then evaluated using a list of id-
ioms taken from published dictionaries, by observ-
ing how many of the gold standard items were found
in each top n, and calculating the accuracy score.
2 By reason of the diverse nature of MWEs, these
lists can be expected to contain manyMWEs that are
not syntactically fixed, giving us a very low upper
bound. However this seems to us the evaluation that
best reflects the application for which the measure is
designed. The list of gold standard idioms we used
were taken from the Longman Dictionary of English
idioms (Long and Summers, 1979) and the SAID
Syntactically Annotated Idiom Dataset (Kuiper et
al., 2003). Combining the two dictionaries gave us
a list of 1109 unique verb-noun pairs, 914 of which
were identified in the BNC.
In order to evaluate the performance of our tech-
nique it will be useful to compare its results with the
ranks of scores that can be obtained by other means.
A simple method of sorting items available to the
corpus lexicographer that might expected to give
reasonable performance is item frequency. We take
this as our baseline. In the introduction we referred
to multiple varieties of MWE. One such variety is
the collocation. Although the collocation is a dif-
ferent variety of MWE, any dictionary will contain
collocations as well as syntactically fixed phrases.
1Any ties were dealt with by generating a random number
for each item and ranking the drawn items using this.
2Note that because the number of candidate items in each
sample is fixed, the relative performance of any two methods
will be the same for recall as it is for precision. In such circum-
stances the term accuracy is preferred.
The collocation has received more attention than
any other variety of MWE and it will therefore be
useful to compare our measure with these methods
as state-of-the-art extraction techniques. We report
the performance obtained when we rank our candi-
date items using all four collocation extraction tech-
niques described in Manning and Schutze (1999) :
t-score, mutual information, log likelihood and ?2.
4.1 Results
Figure 2 provides a plot of the accuracy score each
sample obtains when evaluated using the superset of
the two dictionaries for all samples from n = 1 to n
= 5,000.
Included in figure 2 are the scores obtained when
we inverse ranked using the variation score for each
individual feature, calculated with equation 1. There
is notable divergence in the performance of the dif-
ferent features. The best performing feature is pas-
sivisation, followed by internal modification. Deter-
miner variation performs notably worse for all val-
ues of n.
We next wanted to look at combinations of these
features using equation 2. We saw that the various
syntactic variations achieved very different scores
when used in isolation, and it was by no means cer-
tain that combining all features would the best ap-
proach. Nonetheless we found that the best scores
were achieved by combining all three - an accuracy
of 18%, 14.2 and 5.86% for n of 100, 1000 and 5000
respectively. This can be see in figure 2. The results
achieved with frequency ranking can also be seen in
the plot.
The accuracy achieved by the four collocation
measures can be seen plotted in figure 3. The best
performers are the t-score and the log-likelihood ra-
tio, with MI and ?-squared performing much worse.
The best score for low values of n is t-score, with
log-likelihood overtaking for larger values. The best
performing collocation measures often give a perfor-
mance that is only equal to and often worse than raw
frequency. This is consistent with results reported
by Evert and Krenn (2001). Our best syntactic vari-
ation method outperforms all the collocation extrac-
tion techniques.
We can see, then, that our method is outperform-
ing frequency ranking and the various collocation
measures in terms of accuracy. A major claim we
4
 
0
 
5
 
10
 
15
 
20
 
25  
0
 
100
0
 
200
0
 
300
0
 
400
0
 
500
0
ACCURACY
SAM
PLE
 SIZ
E
"PA
SSI
VE,
 INT
ERN
AL &
 DE
TER
MIN
ER"
"FR
EQU
ENC
Y"
"PA
SSI
VE"
"INT
ERN
AL M
ODI
FIER
"
"DE
TER
MIN
ER 
VAR
IAT
ION
"
Figure 2: Accuracy by sample size for syntactic variation measures
are making for the method however is that it ex-
tracts a different kind of phrase. A close examina-
tion tells us that this is the case. Table 1 lists the
top 25 verb-noun combinations extracted using our
best performing combination of features, and those
extracted using frequency ranking. As can be see
there is no overlap between these lists. In the top 50
items there is an overlap of 3 between the two lists.
Over the top 100 items of the two lists there is only
an overlap of 6 items and over the top 1000 there is
an overlap of only 98.
This small overlap compares favourably with that
found for the collocation scores. While they pro-
duce ranks that are different from pure frequency,
the collocation measures are still based on relative
frequencies. The two high-performing collocation
measures, t-score and log-likelihood have overlap
with frequency of 795 and 624 out of 1000 respec-
tively. This tells us that the collocation measures
are significantly duplicating the information avail-
able from frequency ranking. The item overlap be-
tween t-score items and those extracted using the
the best-performing syntactic variation measure is
116. The overlap between syntactic variation and
log-likelihood items is 108. This small overlap tells
us that our measure is extracting very different items
from the collocation measures.
Given that our measure appears to be pinpoint-
ing a different selection of items from those high-
lighted by frequency ranking or lexical association,
we next want looked at combining the two sources
of information. We test this by ranking our candi-
date list using frequency and using the most consis-
tently well-performing syntactic variation measure
in two separate runs, and then adding together the
two ranks achieved using the two methods for each
item. The items are then reranked using the result-
ing sums. When this ranking is evaluated against the
dictionaries it gives the scores plotted in figure 3 - a
clearly better performance than syntactic fixedness
or frequency alone for samples of 1000 and above.
Having reported all scores we now want to mea-
sure whether any of them are beating frequency
ranking at a level that is statistically significant.
5
 
0
 
5
 
10
 
15
 
20
 
25  
0
 
200
0
 
400
0
 
600
0
 
800
0
 
100
00
ACCURACY
SAM
PLE
 SIZ
E
"PA
SSI
VE,
 INT
ERN
AL, 
DET
ERM
INE
R &
 FR
EQU
ENC
Y"
"PA
SSI
VE,
 INT
ERN
AL &
 DE
TER
MIN
ER"
"FR
EQU
ENC
Y"
"TS
COR
E"
"LO
G L
IKE
LIHO
OD"
"CH
I SQ
UAR
ED" "MI"
Figure 3: Accuracy by sample size for lexical association measures
In order to do this we pick three values of n
(100,1000 and 5000) and examine whether the ac-
curacy achieved by our method are greater than
those achieved with frequency ranking at a level
that is significantly greater than chance. Conven-
tional significance testing is problematic for this
task. Rather than using a significance test that relies
upon an assumed distribution, then, we will use a
computationally-intensive randomization test of sig-
nificance called stratified shuffling. This technique
works by estimating the difference that might occur
between scores by chance through a simulation (see
(Cohen, 1995) for details). As is standard we per-
form 10,000 shuffle iterations.
The results for our three chosen values of n can
be seen in table 2. We accept any result of p < 0.05
as significant, and scores that achieve this level of
significance are shown in bold. As an additional
check on performance we also extend our evalua-
tion. In any evaluation against a gold standard re-
source, there is a risk that the performance of a tech-
nique is particular to the lexical resource used and
will not generalise. For this reason we will here re-
port results achieved using not only the combined set
but also each dictionary in isolation. If the technique
is effective then we would expect it to perform well
for both resources.
We can see that our syntactic variation measures
perform equal to or better than frequency over both
dictionaries in isolation for samples of 1000 and
5000. The good performance against two data sets
tells us that the performance does generalise beyond
a single resource. For the Longman dictionary, the
accuracy achieved by the syntactic variation mea-
sure employing the three best performing features
(?P, I and D?) is significantly higher (at a level of p
< 0.05) than that achieved when ranking with fre-
quency for sample sizes of 1000 and 5000. The
ranking achieved using the combination of syntac-
tic fixedness and frequency information produces a
result that is significant over all items for samples of
1000 and 5000. By contrast, none of the collocation
scores perform significantly better than frequency. 3
3As very low frequency items have been observed to cause
6
Syntactic Variation Collocation
DICTIONARY Freq P,I &D P,I,D &Freq t MI LLR ?2
Top 100 items
LONGMANS 14 21 15 16 0 13 0
SAID 21 17 17 23 0 17 0
BOTH 28 18 25 32 0 25 0
Top 1000 items
LONGMANS 6.6 10.4 10.2 6.3 0 6.5 0.3
SAID 9.1 9 9.9 9 0 8.1 0.2
BOTH 12.2 14.2 15.2 12 0 11.4 0.4
Top 5000 items
LONGMANS 3.24 4.28 4.84 3.12 0.06 3.44 0.58
SAID 3.86 3.56 4.54 3.68 0.04 3.86 0.54
BOTH 5.56 5.86 7.68 5.34 0.04 5.66 0.88
Table 2: Accuracy for top 100, 1000 and 5000 items (scores beating frequency at p < 0.05 are in bold)
An important issue for future research is how
much the performance of our measure is affected
by the technology used. In an evalutaion of RASP,
Preiss (2003) reports an precision of 85.83 and recall
of 78.48 for the direct object relation, 69.45/57.72
for the ?ncmod? relation, and 91.15/98.77 for the
?detmod? relation. There is clearly some variance
here, but it is not easy to see any straightforward re-
lationship with our results. The highest performance
relation (?detmod?) was our least informative fea-
ture. Meanwhile our other two features both rely on
the ?ncmod? relation. One way to address this issue
in future research will be to replicate using multiple
parsers.
5 Previous work
Wermter and Hahn (2004) explore one kind of
syntactic fixedness: the (non-)modifiability of
preposition-noun-verb combinations in German.
They extract all preposition-noun-verb combina-
tions from a corpus of German news text, and iden-
tify all the supplementary lexical information that
occurs between the preposition and the verb. For
each phrase they calculate the probability of seeing
each piece of supplementary material, and take this
as its degree of fixedness. A final score is then cal-
culated by taking the product of this score and the
problems for collocation measures, we experimented with vari-
ous cutoffs up to an occurence rate of 5. We found that this did
not lead to any significant difference from frequency.
probability of occurrence of the phrase. They then
manually evaluated how many true MWEs occurred
in the top n items at various values of n. Like us
they report that their measure outperformed t-score,
log likelihood ratio and frequency.
Fazly and Stevenson (2006) propose a measure
for detecting the syntactic fixedness of English verb
phrases of the same variety as us. They use a set of
regular patterns to identify, for particular word com-
binations (including one of a chosen set of 28 fre-
quent ?basic? verbs), the probability of occurrence
in passive voice, with particular determiners and in
plural form. They then calculate the relative en-
tropy of this probability distribution for the particu-
lar word pair and the probabilities observed over all
the word combinations. As we pointed out in section
3.1 a comparison with all verbs is problematic as
each verb will have its own probability of variation,
and this perhaps explains their focus on a small set
of verbs. They use a development set to establish a
threshold on what constitutes relative fixedness and
calculate the accuracy. This threshhold gives over
the set of 200 items, half of which were found in
a dictionary and hence considered MWEs and half
weren?t. They report an accuracy of 70%, against a
50% baseline. While this is promising, their use of a
small selection of items of a particular kind in their
evaluation makes it somewhat difficult to assess.
7
FREQUENCY P,I & D
1 take place follow suit
2 have effect draw level
3 shake head give rise
4 have time part company
5 take part see chapter
6 do thing give moment
7 make decision open fire
8 have idea run counter
9 play role take refuge
10 play part clear throat
11 open door speak volume
12 do job please contact
13 do work leave net
14 make sense give way
15 have chance see page
16 make use catch sight
17 ask question cite argument
18 spend time see table
19 take care check watch
20 have problem list engagement
21 take step go bust
22 take time change subject
23 take action change hand
24 find way keep pace
25 have power see paragraph
Table 1: Top 25 phrases
6 Discussion
Any lexicon must contain multiword units as well
as individual words. The linguistic literature con-
tains claims for the inclusion of multiword items
in the lexicon on the basis of a number of linguis-
tic dimensions. One of these is syntactic fixedness.
This paper has shown that by quantifying the syntac-
tic fixedness of verb-noun phrases we can identify a
gold standard set of dictionary MWEs with a greater
accuracy than the lexical association measures that
have hitherto dominated the literature, and that, per-
haps more crucially, we can identify a different set of
expressions, not available using existing techniques.
Acknowledgements
Thanks to Tim Baldwin, Francis Bond, Ted Briscoe,
Chris Callison-Burch, Mirella Lapata, Alex Las-
carides, Andrew Smith, Takaaki Tanaka and two
anonymous reviewers for helpful ideas and com-
ments.
References
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of LREC-2003.
P. Cohen. 1995. Empirical Methods for Artificial Intelli-
gence. MIT Press.
Stefan Evert and Brigitte Krenn. 2001. Methods for the
qualitative evaluation of lexical association measures.
In Proceedings of ACL-2001.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of EACL-2006.
Koenraad Kuiper, Heather McCann, and Heidi Quinn.
2003. A syntactically annotated idiom database (said),
v,1.
Thomas H. Long and Della Summers. 1979. Longman
Dictionary of English Idioms. Longman Dictionaries.
David J.C. MacKay. 2003. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
C. Manning and H. Schutze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press,
Cambridge, USA.
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of the
ACL-2005 Student Research Workshop.
Judita Preiss. 2003. Using grammatical relations to com-
pare parsers. In Proceedings of EACL-03.
Suzanne Riehemann. 2001. A Constructional Approach
to Idioms and Word Formation. Ph.D. thesis, Stanford
University.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-
free induction of multiword unit dictionary headwords
a solved problem? In Proceedings of EMNLP-2001.
Aline Villavicencio and Ann Copestake. 2002. On the
nature of idioms. LinGO Working Paper No. 2002-04.
Joachim Wermter and Udo Hahn. 2004. Collocation ex-
traction based on modifiability statistics. In Proceed-
ings of COLING-2004.
8
