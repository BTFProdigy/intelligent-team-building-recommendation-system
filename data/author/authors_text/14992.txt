Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 16?19,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
TransAhead: A Writing Assistant for CAT and CALL 
 
*Chung-chi Huang  ++Ping-che Yang  *Mei-hua Chen *Hung-ting Hsieh  +Ting-hui Kao 
 
   
+Jason S. Chang 
*ISA, NTHU, HsinChu, Taiwan, R.O.C.  
++III, Taipei, Taiwan, R.O.C. +CS, NTHU, HsinChu, Taiwan, R.O.C. 
{u901571,maciaclark,chen.meihua,vincent732,maxis1718,jason.jschang}gmail.com 
 
Abstract 
We introduce a method for learning to 
predict the following grammar and text 
of the ongoing translation given a source 
text. In our approach, predictions are 
offered aimed at reducing users? burden 
on lexical and grammar choices, and 
improving productivity. The method 
involves learning syntactic phraseology 
and translation equivalents. At run-time, 
the source and its translation prefix are 
sliced into ngrams to generate subsequent 
grammar and translation predictions. We 
present a prototype writing assistant, 
TransAhead1, that applies the method to 
where computer-assisted translation and 
language learning meet. The preliminary 
results show that the method has great 
potentials in CAT and CALL (significant 
boost in translation quality is observed). 
1.  Introduction 
More and more language learners use the MT 
systems on the Web for language understanding 
or learning. However, web translation systems 
typically suggest a, usually far from perfect, one-
best translation and hardly interact with the user. 
Language learning/sentence translation could 
be achieved more interactively and appropriately 
if a system recognized translation as a 
collaborative sequence of the user?s learning and 
choosing from the machine-generated predictions 
of the next-in-line grammar and text and the 
machine?s adapting to the user?s accepting 
/overriding the suggestions. 
Consider the source sentence ????????
?????????? (We play an important role 
in closing this deal). The best learning 
environment is probably not the one solely 
                                                          
1Available at http://140.114.214.80/theSite/TransAhead/ 
which, for the time being, only supports Chrome browsers. 
providing the automated translation. A good 
learning environment might comprise a writing 
assistant that gives the user direct control over 
the target text and offers text and grammar 
predictions following the ongoing translations. 
We present a new system, TransAhead, that 
automatically learns to predict/suggest the 
grammatical constructs and lexical translations 
expected to immediately follow the current 
translation given a source text, and adapts to the 
user?s choices. Example TransAhead responses 
to the source ?????????????????? 
and the ongoing translation ?we? and ?we play 
an important role? are shown in Figure 12(a) and 
(b) respectively. TransAhead has determined the 
probable subsequent grammatical constructions 
with constituents lexically translated, shown in 
pop-up menus (e.g., Figure 1(b) shows a 
prediction ?IN[in] VBG[close, end, ?]? due to 
the history ?play role? where lexical items in 
square brackets are lemmas of potential 
translations). TransAhead learns these constructs 
and translations during training. 
At run-time, TransAhead starts with a source 
sentence, and iteratively collaborates with the 
user: by making predictions on the successive 
grammar patterns and lexical translations, and by 
adapting to the user?s translation choices to 
reduce source ambiguities (e.g., word 
segmentation and senses). In our prototype, 
TransAhead mediates between users and 
automatic modules to boost users? writing/ 
translation performance (e.g., productivity). 
2.  Related Work 
CAT has been an area of active research. Our 
work addresses an aspect of CAT focusing on 
language learning. Specifically, our goal is to 
build a human-computer collaborative writing 
assistant: helping the language learner with in- 
text  grammar  and  translation  and  at  the  same 
                                                          
2
 Note that grammatical constituents (in all-capitalized 
words) are represented using Penn parts-of-speech and the 
history based on the user input is shown in shades. 
16
 Figure 1. Example TransAhead responses to a source text under the translation (a) ?we? and (b) ?we play an important role?. Note 
that the grammar/text predictions of (a) and (b) are not placed directly under the current input focus for space limit. (c) and (d) 
depict predominant grammar constructs which follow and (e) summarizes the translations for the source?s character-based ngrams. 
 
time updating the system?s segmentation 
/translation options through the user?s word 
choices. Our intended users are different from 
those of the previous research focusing on what 
professional translator can bring for MT systems 
(e.g., Brown and Nirenburg, 1990). 
More recently, interactive MT (IMT) systems 
have begun to shift the user?s role from analyses 
of the source text to the formation of the target 
translation. TransType project (Foster et al 2002) 
describes such pioneering system that supports 
next word predictions. Koehn (2009) develops 
caitra which displays one phrase translation at a 
time and offers alternative translation options. 
Both systems are similar in spirit to our work. 
The main difference is that we do not expect the 
user to be a professional translator and we 
provide translation hints along with grammar 
predictions to avoid the generalization issue 
facing phrase-based system. 
Recent work has been done on using fully-
fledged statistical MT systems to produce target 
hypotheses completing user-validated translation 
prefix in IMT paradigm. Barrachina et al(2008) 
investigate the applicability of different MT 
kernels within IMT framework. Nepveu et al
(2004) and Ortiz-Martinez et al(2011) further 
exploit user feedbacks for better IMT systems 
and user experience. Instead of trigged by user 
correction, our method is triggered by word 
delimiter and assists in target language learning. 
In contrast to the previous CAT research, we 
present a writing assistant that suggests 
subsequent grammar constructs with translations 
and interactively collaborates with learners, in 
view of reducing users? burden on grammar and 
word choice and enhancing their writing quality. 
3.  The TransAhead System 
3.1 Problem Statement 
For CAT and CALL, we focus on predicting a 
set of grammar patterns with lexical translations 
likely to follow the current target translation 
given a source text. The predictions will be 
examined by a human user directly. Not to 
overwhelm the user, our goal is to return a 
reasonable-sized set of predictions that contain 
suitable word choices and correct grammar to 
choose and learn from. Formally speaking, 
Problem Statement: We are given a target-
language reference corpus Ct, a parallel corpus 
Cst, a source-language text S, and its target 
translation prefix Tp. Our goal is to provide a set 
of predictions based on Ct and Cst likely to 
further translate S in terms of grammar and text. 
For this, we transform S and Tp into sets of 
ngrams such that the predominant grammar 
constructs with suitable translation options 
following Tp are likely to be acquired. 
3.2  Learning to Find Pattern and Translation 
We attempt to find syntax-based phraseology and 
translation equivalents beforehand (four-staged) 
so that a real-time system is achievable. 
Firstly, we syntactically analyze the corpus Ct. 
In light of the phrases in grammar book (e.g., 
one?s in ?make up one?s mind?), we resort to 
parts-of-speech for syntactic generalization. 
Secondly, we build up inverted files of the words 
in Ct for the next stage (i.e., pattern grammar 
generation). Apart from sentence and position 
information, a word?s lemma and part-of-speech 
(POS) are also recorded. 
(b) 
Source text: 
???????????????? 
(a) 
Pop-up predictions/suggestions: 
we MD VB[play, act, ..] , ? 
we VBP[play, act, ..] DT , ? 
we VBD[play, act, ..] DT , ? 
Pop-up predictions/suggestions: 
play role IN[in] VBG[close, end, ..] , ? 
important role IN[in] VBG[close, end, ..] , ? 
role IN[in] VBG[close, end, ..] , ? 
(c) 
(d) 
(e) 
Patterns for ?we?: 
we MD VB , ?, 
we VBP DT , ?, 
we VBD DT , ? 
Patterns for ?we play an important role?: 
play role IN[in] DT , 
play role IN[in] VBG , ?, 
important role IN[in] VBG , ?, 
role IN[in] VBG , ? 
Translations for the source text: 
????: we, ?; ????: close, end, ?;  ?; ????: 
play, ?; ????: critical, ?; ?; ???: act, ?; ?; 
???: heavy, ?; ???: will, wish, ?; ???: cents, ?; 
???: outstanding, ? 
Input your source text and start to interact with TransAhead! 
17
We then leverage the procedure in Figure 2 to 
generate grammar patterns for any given 
sequence of words (e.g., contiguous or not). 
 
Figure 2. Automatically generating pattern grammar. 
 
The algorithm first identifies the sentences 
containing the given sequence of words, query. 
Iteratively, Step (3) performs an AND operation 
on the inverted file, InvList, of the current word 
wi and interInvList, a previous intersected results. 
Afterwards, we analyze query?s syntax-based 
phraseology (Step (5)). For each element of the 
form ([wordPosi(w1),?,wordPosi(wn)], sentence 
number) denoting the positions of query?s words 
in the sentence, we generate grammar pattern 
involving replacing words with POS tags and 
words in wordPosi(wi) with lemmas, and 
extracting fixed-window3  segments surrounding 
query from the transformed sentence. The result 
is a set of grammatical, contextual patterns. 
The procedure finally returns top N 
predominant syntactic patterns associated with 
the query. Such patterns characterizing the 
query?s word usages follow the notion of pattern 
grammar in (Hunston and Francis, 2000) and are 
collected across the target language. 
In the fourth and final stage, we exploit Cst for 
bilingual phrase acquisition, rather than a manual 
dictionary, to achieve better translation coverage 
and variety. We obtain phrase pairs through 
leveraging IBM models to word-align the bitexts, 
?smoothing? the directional word alignments via 
grow-diagonal-final, and extracting translation 
equivalents using (Koehn et al 2003). 
3.3  Run-Time Grammar and Text Prediction 
Once translation equivalents and phraseological 
tendencies are learned, TransAhead then 
predicts/suggests the following grammar and text 
of a translation prefix given the source text using 
the procedure in Figure 3. 
We first slice the source text S and its 
translation prefix Tp into character-level and 
                                                          
3
 Inspired by (Gamon and Leacock, 2010). 
word-level ngrams respectively. Step (3) and (4) 
retrieve the translations and patterns learned 
from Section 3.2. Step (3) acquires the active 
target-language vocabulary that may be used to 
translate the source text. To alleviate the word 
boundary issue in MT raised by Ma et al(2007), 
TransAhead non-deterministically segments the 
source text using character ngrams and proceeds 
with collaborations with the user to obtain the 
segmentation for MT and to complete the 
translation. Note that a user vocabulary of 
preference (due to users? domain of knowledge 
or errors of the system) may be exploited for 
better system performance. On the other hand, 
Step (4) extracts patterns preceding with the 
history ngrams of {tj}. 
 
Figure 3. Predicting pattern grammar and translations. 
 
In Step (5), we first evaluate and rank the 
translation candidates using linear combination: 
( ) ( )( ) ( )1 1 1 2 2   i i pP t s P s t P t T? ?? + + ?  
where ?i is combination weight, P1 and P2 are 
translation and language model respectively, and 
t is one of the translation candidates under S and 
Tp. Subsequently, we incorporate the lemmatized 
translation candidates into grammar constituents 
in GramOptions. For example, we would include 
?close? in pattern ?play role IN[in] VBG? as 
?play role IN[in] VBG[close]?. 
At last, the algorithm returns the 
representative grammar patterns with confident 
translations expected to follow the ongoing 
translation and further translate the source. This 
algorithm will be triggered by word delimiter to 
provide an interactive environment where CAT 
and CALL meet. 
4.  Preliminary Results 
To train TransAhead, we used British National 
Corpus and Hong Kong Parallel Text and 
deployed GENIA tagger for POS analyses. 
To evaluate TransAhead in CAT and CALL, 
we introduced it to a class of 34 (Chinese) first-
year college students learning English as foreign 
language. Designed to be intuitive to the general 
public, esp. language learners, presentational 
tutorial lasted only for a minute. After the tutorial, 
the participants were asked to translate 15 
procedure PatternFinding(query,N,Ct) (1)  interInvList=findInvertedFile(w1 of query) 
for each word wi in query except for w1 (2)     InvList=findInvertedFile(wi) (3a)   newInterInvList= ? ; i=1; j=1 
(3b)   while i<=length(interInvList) and j<=lengh(InvList) 
(3c)      if interInvList[i].SentNo==InvList[j].SentNo 
(3d)         Insert(newInterInvList, interInvList[i],InvList[j]) 
else 
(3e)         Move i,j accordingly 
(3f)    interInvList=newInterInvList 
(4) Usage= ?  
for each element in interInvList 
(5)     Usage+={PatternGrammarGeneration(element,Ct)} (6) Sort patterns in Usage in descending order of frequency (7) return the N patterns in Usage with highest frequency 
procedure MakePrediction(S,Tp) 
(1) Assign sliceNgram(S) to {si} (2) Assign sliceNgram(Tp) to {tj} (3) TransOptions=findTranslation({si},Tp) (4) GramOptions=findPattern({tj}) (5) Evaluate translation options in TransOptions 
           and incorporate them into GramOptions (6) Return GramOptions 
18
Chinese texts from (Huang et al 2011a) one by 
one (half with TransAhead assistance, and the 
other without). Encouragingly, the experimental 
group (i.e., with the help of our system) achieved 
much better translation quality than the control 
group in BLEU (Papineni et al 2002) (i.e., 
35.49 vs. 26.46) and significantly reduced the 
performance gap between language learners and 
automatic decoder of Google Translate (44.82).  
We noticed that, for the source ?????????
?????????, 90% of the participants in the 
experimental group finished with more 
grammatical and fluent translations (see Figure 4) 
than (less interactive) Google Translate (?We 
conclude this transaction plays an important 
role?). In comparison, 50% of the translations of 
the source from the control group were erroneous. 
 
Figure 4. Example translations with TransAhead assistance. 
 
Post-experiment surveys indicate that a) the 
participants found TransAhead intuitive enough 
to collaborate with in writing/translation; b) the 
participants found TransAhead suggestions 
satisfying, accepted, and learned from them; c) 
interactivity made translation and language 
learning more fun and the participants found 
TransAhead very recommendable and would like 
to use the system again in future translation tasks. 
5.  Future Work and Summary 
Many avenues exist for future research and 
improvement. For example, in the linear 
combination, the patterns? frequencies could be 
considered and the feature weight could be better 
tuned. Furthermore, interesting directions to 
explore include leveraging user input such as 
(Nepveu et al 2004) and (Ortiz-Martinez et al 
2010) and serially combining a grammar checker 
(Huang et al 2011b). Yet another direction 
would be to investigate the possibility of using 
human-computer collaborated translation pairs to 
re-train word boundaries suitable for MT. 
In summary, we have introduced a method for 
learning to offer grammar and text predictions 
expected to assist the user in translation and 
writing (or even language learning). We have 
implemented and evaluated the method. The 
preliminary results are encouragingly promising, 
prompting us to further qualitatively and 
quantitatively evaluate our system in the near 
future (i.e., learners? productivity, typing speed 
and keystroke ratios of ?del? and ?backspace? 
(possibly hesitating on the grammar and lexical 
choices), and human-computer interaction, 
among others). 
Acknowledgement 
This study is conducted under the ?Project 
Digital Convergence Service Open Platform? of 
the Institute for Information Industry which is 
subsidized by the Ministry of Economy Affairs 
of the Republic of China. 
References  
S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. 
Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tomas, E. 
Vidal, and J.-M. Vilar. 2008. Statistical approaches to 
computer-assisted translation. Computer Linguistics, 
35(1): 3-28. 
R. D. Brown and S. Nirenburg. 1990. Human-computer 
interaction for semantic disambiguation. In Proceedings 
of COLING, pages 42-47. 
G. Foster, P. Langlais, E. Macklovitch, and G. Lapalme. 
2002. TransType: text prediction for translators. In 
Proceedings of ACL Demonstrations, pages 93-94. 
M. Gamon and C. Leacock. 2010. Search right and thou 
shalt find ? using web queries for learner error 
detection. In Proceedings of the NAACL Workshop on 
Innovative Use of NLP for Building Educational 
Applications, pages 37-44. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, H.-C. Liou, and J. 
S. Chang. 2011a. GRASP: grammar- and syntax-based 
pattern-finder in CALL. In Proceedings of ACL. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, and J. S. Chang. 
2011b. EdIt: a broad-coverage grammar checker using 
pattern grammar. In Proceedings of ACL. 
S. Hunston and G. Francis. 2000. Pattern Grammar: A 
Corpus-Driven Approach to the Lexical Grammar of 
English. Amsterdam: John Benjamins. 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proceedings of NAACL. 
P. Koehn. 2009. A web-based interactive computer aided 
translation tool. In Proceedings of ACL. 
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrapping word 
alignment via word packing. In Proceedings of ACL. 
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004. 
Adaptive language and translation models for interactive 
machine translation. In Proceedings of EMNLP. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1):19-51. 
D. Ortiz-Martinez, L. A. Leiva, V. Alabau, I. Garcia-Varea, 
and F. Casacuberta. 2011. An interactive machine 
translation system with online learning. In Proceedings 
of ACL System Demonstrations, pages 68-73. 
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. Bleu: a 
method for automatic evaluation of machine translation. 
In Proceedings of ACL, pages 311-318. 
1. we play(ed) a critical role in closing/sealing this/the deal. 
2. we play(ed) an important role in ending/closing this/the deal. 
19
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 352?356,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
TransAhead: A Computer-Assisted Translation and Writing Tool 
G
G
*Chung-chi Huang      +Ping-che Yang **Keh-jiann Chen       ++Jason S. Chang 
  
*ISA, NTHU, HsinChu, Taiwan, R.O.C. **IIS, Academia Sinica, Taipei, Taiwan, R.O.C. 
+III, Taipei, Taiwan, R.O.C. ++CS, NTHU, HsinChu, Taiwan, R.O.C. 
{*u901571,+maciaclark,++jason.jschang}@gmail.com; **kchen@iis.sinica.edu.tw 
  
 
Abstract 
We introduce a method for learning to predict 
text completion given a source text and partial 
translation. In our approach, predictions are 
offered aimed at alleviating users? burden on 
lexical and grammar choices, and improving 
productivity. The method involves learning 
syntax-based phraseology and translation 
equivalents. At run-time, the source and its 
translation prefix are sliced into ngrams to 
generate and rank completion candidates, 
which are then displayed to users. We present 
a prototype writing assistant, TransAhead, that 
applies the method to computer-assisted 
translation and language learning. The 
preliminary results show that the method has 
great potentials in CAT and CALL with 
significant improvement in translation quality 
across users. 
1 Introduction 
More and more language workers and learners use 
the MT systems on the Web for information 
gathering and language learning. However, web 
translation systems typically offer top-1 
translations (which are usually far from perfect) 
and hardly interact with the user. 
Text translation could be achieved more 
interactively and effectively if a system considered 
translation as a collaborative between the machine 
generating suggestions and the user accepting or 
overriding on those suggestions, with the system 
adapting to the user?s action. 
Consider the source sentence ?????????
????????? (We play an important role in 
closing this deal). The best man-machine 
interaction is probably not the one used by typical 
existing MT systems. A good working 
environment might be a translation assistant that 
offers suggestions and gives the user direct control 
over the target text. 
We present a system, TransAhead1, that learns 
to predict and suggest lexical translations (and 
their grammatical patterns) likely to follow the 
ongoing translation of a source text, and adapts to 
the user?s choices. Example responses of 
TransAhead to the source sentence ????????
?????????? and two partial translations  
are shown in Figure 1. The responses include text 
and grammatical patterns (in all-cap labels 
representing parts-of-speech). TransAhead 
determines and displays the probable subsequent 
grammatical constructions and partial translations 
in the form of parts-of-speech and words (e.g., 
?IN[in] VBG[close,?]? for keywords ?play role? 
where lexical items in square brackets are lemmas 
of potential translations) in a pop-up. TransAhead 
learns these constructs and translations during 
training. 
At run-time, TransAhead starts with a source 
sentence, and iterates with the user, making 
predictions on the grammar patterns and lexical 
translations, while adapting to the user?s 
translation choices to resolve ambiguities in the 
source sentence related to word segmentation and 
word sense. In our prototype, TransAhead 
mediates between users and suggestion modules to 
translation quality and  productivity. 
2 Related Work 
Computer Assisted Translation (CAT) has been an 
area of active research. We focus on offering 
suggestions during the  translation process with  an 
                                                           
1
 http://140.114.214.80/theSite/TransAhead/ (Chrome only) 
352
 Figure 1. Example TransAhead responses to a source text under the translation (a) ?we? and (b) ?we play an 
important role?. Note that the grammar/text predictions of (a) and (b) are not placed directly under the caret (current 
input focus) for space limit. (c) and (d) depict predominant grammar constructs which follow and (e) summarizes 
the confident translations of the source?s character-based ngrams. The frequency of grammar pattern is shown in 
round brackets while the history (i.e., keyword) based on the user input is shown in shades. 
 
emphasis on language learning. Specifically, our 
goal is to build a translation assistant to help 
translator (or learner-translator) with inline 
grammar help and translation. Unlike recent 
research focusing on professional (e.g., Brown and 
Nirenburg, 1990), we target on both professional 
and student translators. 
More recently, interactive MT (IMT) systems 
have begun to shift the user?s role from post-
editing machine output to collaborating with the 
machine to produce the target text. Foster et al
(2000) describe TransType, a pioneering system 
that supports next word predictions. Along the 
similar line, Koehn (2009) develops caitra which 
predicts and displays phrasal translation 
suggestions one phrase at a time. The main 
difference between their systems and TransAhead 
is that we also display grammar patterns to provide 
the general patterns of predicted translations so a 
student translator can learn and become more 
proficient. 
Recent work has been done on using fully-
fledged statistical MT systems to produce target 
hypotheses completing user-validated translation 
prefix in IMT paradigm. Barrachina et al (2008) 
investigate the applicability of different MT 
kernels within IMT framework. Nepveu et al 
(2004) and Ortiz-Martinez et al (2011) further 
exploit user feedbacks for better IMT systems and 
user experience. Instead of triggered by user 
correction, our method is triggered by word 
delimiter and assists both translation and learning 
the target language. 
In contrast to the previous CAT research, we 
present a writing assistant that suggests grammar 
constructs as well as lexical translations following 
users? partial translation, aiming to provide users 
with choice to ease mental burden and enhance 
performance. 
3 The TransAhead System 
3.1 Problem Statement 
We focus on predicting a set of grammar patterns 
with lexical translations likely to follow the current 
partial target translation of a source text. The 
predictions will be examined by a human user 
directly. Not to overwhelm the user, our goal is to 
return a reasonable-sized set of predictions that 
contain suitable word choices and grammatical 
patterns to choose and learn from. Formally, 
Problem Statement: We are given a target-
language reference corpus Ct, a parallel corpus Cst, 
a source-language text S, and its translation prefix 
Tp. Our goal is to provide a set of predictions based 
on Ct and Cst likely to further translate S in terms of 
grammar and text. For this, we transform S and Tp 
into sets of ngrams such that the predominant 
grammar constructs with suitable translation 
options following Tp are likely to be acquired. 
(b) 
Source text: ???????????????? 
(a) 
Pop-up predictions/suggestions: 
we MD VB[play, act, ..]  (41369), ? 
we VBP[play, act, ..] DT  (13138), ? 
we VBD[play, act, ..] DT  (8139), ? 
Pop-up predictions/suggestions: 
play role IN[in] VBG[close, end, ..] (397), ? 
important role IN[in] VBG[close, end, ..]  (110), ? 
role IN[in] VBG[close, end, ..] (854), ? 
(c) 
(d) 
(e) 
Patterns for ?we?: 
we MD VB (41369), ?, 
we VBP DT (13138), ?, 
we VBD DT (8139), ? 
Patterns for ?we play an important role?: 
play role IN[in] DT (599), 
play role IN[in] VBG (397), ?, 
important role IN[in] VBG (110), ?, 
role IN[in] VBG (854), ? 
Translations for the source text: 
????: we, ?; ????: close, end, ?;  ?; ????: 
play, ?; ????: critical, ?; ?; ???: act, ?; ?; 
???: heavy, ?; ???: will, wish, ?; ???: cents, ?; 
???: outstanding, ? 
Input your source text and start to interact with TransAhead! 
353
3.2 Learning to Find Pattern and Translation 
In the training stage, we find and store syntax-
based phraseological tendencies and translation 
pairs. These patterns and translations are intended 
to be used in a real-time system to respond to user 
input speedily. 
First, we part of speech tag sentences in Ct. 
Using common phrase patterns (e.g., the 
possessive noun one?s in ?make up one?s mind?) 
seen in grammar books, we resort to parts-of-
speech (POS) for syntactic generalization. Then, 
we build up inverted files of the words in Ct for the 
next stage (i.e., pattern grammar generation). Apart 
from sentence and position information, a word?s 
lemma and POS are also recorded. 
Subsequently, we use the procedure in Figure 2 
to generate grammar patterns following any given 
sequence of words, either contiguous or skipped. 
 
 
Figure 2. Automatically generating pattern grammar. 
 
The algorithm first identifies the sentences 
containing the given sequence of words, query. 
Iteratively, Step (3) performs an AND operation on 
the inverted file, InvList, of the current word wi and 
interInvList, a previous intersected results. 
After that, we analyze query?s syntax-based 
phraseology (Step (5)). For each element of the 
form ([wordPosi(w1),?, wordPosi(wn)], sentence 
number) denoting the positions of query?s words in 
the sentence, we generate grammar pattern 
involving replacing words in the sentence with 
POS tags and words in wordPosi(wi) with lemmas, 
and extracting fixed-window 2  segments 
surrounding query from the transformed sentence. 
The result is a set of grammatical patterns (i.e., 
syntax-based phraseology) for the query. The 
procedure finally returns top N predominant 
                                                           
2
 Inspired by (Gamon and Leacock, 2010). 
syntactic patterns of the query. Such patterns 
characterizing the query?s word usages in the spirit 
of pattern grammar in (Hunston and Francis, 2000) 
and are collected across the target language. 
In the fourth and final stage, we exploit Cst for 
bilingual phrase acquisition, rather than a manual 
dictionary, to achieve better translation coverage 
and variety. We obtain phrase pairs through a 
number of steps, namely, leveraging IBM models 
for bidirectional word alignments, grow-diagonal-
final heuristics to extract phrasal equivalences 
(Koehn et al, 2003). 
3.3 Run-Time Grammar and Text Prediction 
Once translation equivalents and phraseological 
tendencies are learned, they are stored for run-time 
reference. TransAhead then predicts/suggests the 
following grammar and text of a translation prefix 
given the source text using the procedure in Figure 
3. 
 
 
Figure 3. Predicting pattern grammar and 
translations at run-time. 
 
We first slice the source text S into character-
level ngrams, represented by {si}. We also find the 
word-level ngrams of the translation prefix Tp. But 
this time we concentrate on the ngrams, may 
skipped, ending with the last word of Tp (i.e., 
pivoted on the last word) since these ngrams are 
most related to the subsequent grammar patterns. 
Step (3) and (4) retrieve translations and patterns 
learned from Section 3.2. Step (3) acquires the 
target-language active vocabulary that may be used 
to translate the source. To alleviate the word 
boundary issue in MT (Ma et al (2007)), the word 
boundary in our system is loosely decided. Initially, 
TransAhead non-deterministically segments the 
source text using character ngrams for translations 
and proceeds with collaborations with the user to 
obtain the segmentation for MT and to complete 
the translation. Note that Tp may reflect some 
translated segments, reducing the size of the active 
vocabulary, and that a user vocabulary of 
preference (due to users? domain knowledge or 
procedure PatternFinding(query,N,Ct) (1)  interInvList=findInvertedFile(w1 of query) 
for each word wi in query except for w1 (2)     InvList=findInvertedFile(wi) (3a)   newInterInvList= ? ; i=1; j=1 
(3b)   while i<=length(interInvList) and j<=lengh(InvList) 
(3c)      if interInvList[i].SentNo==InvList[j].SentNo 
(3d)         Insert(newInterInvList, interInvList[i],InvList[j]) 
else 
(3e)         Move i,j accordingly 
(3f)    interInvList=newInterInvList 
(4) Usage= ?  
for each element in interInvList 
(5)     Usage+={PatternGrammarGeneration(element,Ct)} (6) Sort patterns in Usage in descending order of frequency 
(7) return the N patterns in Usage with highest frequency 
procedure MakePrediction(S,Tp) 
(1) Assign sliceNgram(S) to {si} (2) Assign sliceNgramWithPivot(Tp) to {tj} (3) TransOptions=findTranslation({si},Tp) (4) GramOptions=findPattern({tj}) (5) Evaluate translation options in TransOptions 
           and incorporate them into GramOptions (6) Return GramOptions 
354
errors of the system) may be exploited for better 
system performance. In addition, Step (4) extracts 
patterns preceding with the history ngrams of {tj}. 
In Step (5), we first evaluate and rank the 
translation candidates using linear combination: 
( ) ( )( ) ( )1 1 1 2 2   i i pP t s P s t P t T? ?? + + ?  
where ?i is combination weight, P1 and P2 are 
translation and language model respectively, and t 
is one of the translation candidates under S and Tp. 
Subsequently, we incorporate the lemmatized 
translation candidates according to their ranks into 
suitable grammar constituents in GramOptions. 
For example, we would include ?close? in pattern 
?play role IN[in] VBG? as ?play role IN[in] 
VBG[close]?. 
At last, the algorithm returns the representative 
grammar patterns with confident translations 
expected to follow the ongoing translation and 
further translate the source. This algorithm will be 
triggered by word delimiter to provide an 
interactive CAT and CALL environment. Figure 1 
shows example responses of our working prototype. 
4 Preliminary Results 
In developing TransAhead, we used British 
National Corpus and Hong Kong Parallel Text as 
target-language reference corpus and parallel 
training corpus respectively, and deployed GENIA 
tagger for lemma and POS analyses. 
To evaluate TransAhead in CAT and CALL, we 
introduced it to a class of 34 (Chinese) college 
freshmen learning English as foreign language. We 
designed TransAhead to be accessible and intuitive, 
so the user training tutorial took only one minute. 
After the tutorial, the participants were asked to 
translate 15 Chinese texts from (Huang et al, 2011) 
(half with TransAhead assistance called experi-
mental group, and the other without any system 
help whatsoever called control group). The 
evaluation results show that the experimental 
group achieved much better translation quality than 
the control group with an average BLEU score 
(Papineni et al, 2002) of 35.49 vs. 26.46. 
Admittedly, the MT system Google Translate 
produced translations with a higher BLEU score of 
44.82. 
Google Translate obviously has much more 
parallel training data and bilingual translation 
knowledge. No previous work in CAT uses Google 
Translate for comparison. Although there is a 
difference in average translation quality between 
the experimental TransAhead group and the 
Google Translate, it is not hard for us to notice the 
source sentences were better translated by 
language learners with the help of TransAhead. 
Take the sentence  ????????????????
?? for example. A total of 90% of the participants 
in the experimental group produced more 
grammatical and fluent translations (see Figure 4) 
than that (?We conclude this transaction plays an 
important role?) by Google Translate. 
 
 
Figure 4. Example translations with 
TransAhead assistance. 
 
Post-experiment surveys indicate that (a) the 
participants found Google Translate lack human-
computer interaction while TransAhead is intuitive 
to collaborate with in translation/writing; (b) the 
participants found TransAhead grammar and 
translation predictions useful for their immediate 
task and for learning; (c) interactivity made the 
translation and language learning a fun process 
(like image tagging game of (von Ahn and Dabbish, 
2004)) and the participants found TransAhead very 
recommendable and would like to use it again in 
future translation tasks. 
5 Summary 
We have introduced a method for learning to offer 
grammar and text predictions expected to assist the 
user in translation and writing. We have 
implemented and evaluated the method. The 
preliminary results are encouragingly promising. 
As for the further work, we intend to evaluate and 
improve our system further in learner productivity 
in terms of output quality, typing speed, and the 
amount of using certain keys such as delete and 
backspace. 
Acknowledgement 
This study is conducted under the ?Project Digital 
Convergence Service Open Platform? of the 
Institute for Information Industry which is 
subsidized by the Ministry of Economy Affairs of 
the Republic of China. 
1. we play(ed) a critical role in closing this/the deal. 
2. we play(ed) a critical role in sealing this/the deal. 
3. we play(ed) an important role in ending this/the deal. 
4. we play(ed) an important role in closing this/the deal. 
355
References 
S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. 
Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tomas, E. 
Vidal, and J.-M. Vilar. 2008. Statistical approaches 
to computer-assisted translation. Computational 
Linguistics, 35(1): 3-28. 
R. D. Brown and S. Nirenburg. 1990. Human-computer 
interaction for semantic disambiguation. In 
Proceedings of COLING, pages 42-47. 
G. Foster, P. Langlais, E. Macklovitch, and G. Lapalme. 
2002. TransType: text prediction for translators. In 
Proceedings of ACL Demonstrations, pages 93-94. 
M. Gamon and C. Leacock. 2010. Search right and thou 
shalt find ? using web queries for learner error 
detection. In Proceedings of the NAACL Workshop. 
C.-C. Huang, M.-H. Chen, S.-T. Huang, H.-C. Liou, and 
J. S. Chang. 2011. GRASP: grammar- and syntax-
based pattern-finder in CALL. In Proceedings of 
ACL Workshop. 
S. Hunston and G. Francis. 2000. Pattern Grammar: A 
Corpus-Driven Approach to the Lexical Grammar of 
English. Amsterdam: John Benjamins. 
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical 
phrase-based translation. In Proceedings of NAACL. 
P. Koehn. 2009. A web-based interactive computer 
aided translation tool. In Proceedings of ACL. 
Y. Ma, N. Stroppa, and A. Way. 2007. Bootstrapping 
word alignment via word packing. In Proceedings of 
ACL. 
L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 
2004. Adaptive language and translation models for 
interactive machine translation. In Proceedings of 
EMNLP. 
D. Ortiz-Martinez, L. A. Leiva, V. Alabau, I. Garcia-
Varea, and F. Casacuberta. 2011. An interactive 
machine translation system with online learning. In 
Proceedings of ACL System Demonstrations, pages 
68-73. 
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2002. Bleu: 
a method for automatic evaluation of machine 
translation. In Proceedings of ACL, pages 311-318. 
L. von Ahn and L. Dabbish. 2004. Labeling images with 
a computer game. In Proceedings of CHI. 
356
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 26?31,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar
Chung-Chi Huang Mei-Hua Chen Shih-Ting Huang Jason S. Chang
Institute of Information Systems and Department of Computer Science,
Applications, National Tsing Hua University, National Tsing Hua University,
HsinChu, Taiwan, R.O.C. 300 HsinChu, Taiwan, R.O.C. 300
{u901571,chen.meihua,koromiko1104,Jason.jschang}@gmail.com
Abstract
We introduce a new method for learning to 
detect grammatical errors in learner?s writ-
ing and provide suggestions. The method 
involves parsing a reference corpus and 
inferring grammar patterns in the form of a 
sequence of content  words, function words, 
and parts-of-speech (e.g., ?play ~ role in 
Ving? and ?look forward to  Ving?). At run-
time, the given passage submitted by the 
learner is matched using an extended 
Levenshtein algorithm against  the set  of 
pattern rules in order to detect  errors and 
provide suggestions. We present a proto-
type implementation of the proposed 
method, EdIt, that  can handle a broad range 
of errors. Promising results are illustrated 
with three common types of errors in non-
native writing.
1 Introduction
Recently, an increasing number of research has 
targeted language learners? need in editorial assis-
tance including detecting and correcting grammar 
and usage errors in texts written in a second lan-
guage. For example, Microsoft  Research has de-
veloped the ESL Assistant, which provides such a 
service to ESL and EFL learners.
Much of the research in this area depends on 
hand-crafted rules and focuses on certain error 
types. Very little research provides a general 
framework for detecting and correcting all types of 
errors. However, in the sentences of ESL writing, 
there may be more than one errors and one error 
may affect the performance of handling other er-
rors. Erroneous sentences could be more efficiently 
identified and corrected if a grammar checker han-
dles all errors at  once, using a set of pattern rules 
that reflect the predominant usage of the English 
language.
Consider the sentences, ?He play an important 
roles to close this deals.? and ?He looks forward to 
hear you.? The first  sentence contains inaccurate 
word forms (i.e., play, roles, and deals), and rare 
usage (i.e., ?role to close?), while the second sen-
tence use the incorrect verb form of ?hear?. Good 
responses to these writing errors might  be (a) Use 
?played? instead of ?play.? (b) Use ?role? instead 
of ?roles?,  (c) Use ?in closing? instead of ?to 
close? (d) Use ?to hearing? instead of ?to hear?, 
and (e) insert  ?from? between ?hear? and ?you.? 
These suggestions can be offered by learning the 
patterns rules related to ?play ~ role? and ?look 
forward? based on analysis of ngrams and collo-
cations in a very large-scale reference corpus. With 
corpus statistics, we could learn the needed phra-
seological tendency in the form of pattern rules 
such as ?play ~ role in  V-ing) and ?look forward 
to V-ing.? The use of such pattern rules is in line 
with the recent  theory of Pattern Grammar put 
forward by Hunston and Francis (2000).
We present  a system, EdIt, that automatically 
learns to provide suggestions for rare/wrong usages 
in non-native writing. Example EdIt  responses to a 
26
text are shown in Figure 1. EdIt has retrieved the 
related pattern grammar of some ngram and collo-
cation sequences given the input  (e.g., ?play ~ role 
in V-ing
1
?, and ?look forward to V-ing?). EdIt 
learns these patterns during pattern extraction 
process by syntactically analyzing a collection of 
well-formed, published texts.
At run-time, EdIt first processes the input  pas-
sages in the article (e.g., ?He play an important 
roles to close ?) submitted by the L2 learner. And 
EdIt  tag the passage with part  of speech informa-
tion, and compares the tagged sentence against  the 
pattern rules anchored at  certain collocations (e.g., 
?play ~ role? and ?look forward?). Finally, EdIt 
finds the minimum-edit-cost  patterns matching the 
passages using an extended Levenshtein?s algo-
rithm (Levenshtein, 1966). The system then high-
lights the edits and displays the pattern rules as 
suggestions for correction. In our prototype, EdIt 
returns the preferred word form and preposition 
usages to the user directly (see Figure 1); alterna-
tively, the actual surface words (e.g., ?closing? and 
?deal?) could be provided.
Input:
Related pattern rules
play ~ role in Noun
play ~ role in V-ing
he plays DET
he played DET
look forward to V-ing
hear from PRON ...
Suggestion:
He played an important role in closing this deal. He looks 
forward to hearing from you.
He play an important roles to close this 
deals. He looks forward to hear you.
Figure 1. Example responses to the non-native writing.
2 Related Work
Grammar checking has been an area of active re-
search. Many methods, rule-oriented or data-
driven, have been proposed to tackle the problem 
of detecting and correcting incorrect grammatical 
and usage errors in learner texts. It is at  times no 
easy to distinguish these errors. But Fraser and 
Hodson (1978) shows the distinction between these 
two kinds of errors.
For some specific error types (e.g., article and 
preposition error), a number of interesting rule-
based systems have been proposed. For example, 
Uria et al (2009) and Lee et  al. (2009) leverage 
heuristic rules for detecting Basque determiner and 
Korean particle errors, respectively. Gamon et  al. 
(2009) bases some of the modules in ESL Assistant 
on rules derived from manually inspecting learner 
data. Our pattern rules, however, are automatically 
derived from readily available well-formed data, 
but nevertheless very helpful for correcting errors 
in non-native writing.
More recently, statistical approaches to develop-
ing grammar checkers have prevailed. Among un-
supervised checkers, Chodorow and Leacock 
(2000) exploits negative evidence from edited tex-
tual corpora achieving high precision but low re-
call, while Tsao and Wible (2009) uses general 
corpus only. Additionally, Hermet et al (2008) and 
Gamon and Leacock (2010) both use Web as a 
corpus to detect  errors in non-native writing. On 
the other hand, supervised models, typically treat-
ing error detection/correction as a classification 
problem, may train on well-formed texts as in the 
methods by De Felice and Pulman (2008) and Te-
treault et  al. (2010), or with additional learner texts 
as in the method proposed by Brockett et al 
(2006). Sun et  al. (2007) describes a method for 
constructing a supervised detection system trained 
on raw well-formed and learner texts without error 
annotation. 
Recent work has been done on incorporating 
word class information into grammar checkers. For 
example, Chodorow and Leacock (2000) exploit 
bigrams and trigrams of function words and part-
of-speech (PoS) tags, while Sun et al (2007) use 
labeled sequential patterns of function, time ex-
pression, and part-of-speech tags. In an approach 
similar to our work, Tsao and Wible (2009) use a 
combined ngrams of words forms, lemmas, and 
part-of-speech tags for research into constructional 
phenomena. The main differences are that  we an-
chored each pattern rule in lexical collocation so 
as to avoid deriving rules that  is may have two 
1
 In the pattern rules, we translate the part-of-speech tag to labels that are commonly used in learner dictionaries. For 
instance, we use V-ing for the tag VBG denoting the progressive verb form, and Pron and Pron$ denotes a pronoun 
and a possessive pronoun respectively.
27
consecutive part-of-speech tags (e.g, ?V Pron$ 
socks off?). The pattern rules we have derived are 
more specific and can be effectively used in detect-
ing and correcting errors.
In contrast  to the previous research, we intro-
duce a broad-coverage grammar checker that ac-
commodates edits such as substitution, insertion 
and deletion, as well as replacing word forms or 
prepositions using pattern rules automatically de-
rived from very large-scale corpora of well-formed 
texts.
3 The EdIt System
Using supervised training on a learner corpus is not 
very feasible due to the limited availability of 
large-scale annotated non-native writing. Existing 
systems trained on learner data tend to offer high 
precision but low recall. Broad coverage grammar 
checkers may be developed using readily available 
large-scale corpora. To detect  and correct errors in 
non-native writing, a promising approach is to 
automatically extract  lexico-syntactical pattern 
rules that  are expected to distinguish correct and in 
correct sentences.
3.1 Problem Statement
We focus on correcting grammatical and usage 
errors by exploiting pattern rules of specific collo-
cation (elastic or rigid such as ?play ~ rule? or 
?look forward?). For simplification, we assume 
that there is no spelling errors. EdIt provides sug-
gestions to common writing errors
2
 of the follow-
ing correlated with essay scores
3
.
(1)  wrong word form
(A) singular determiner preceding plural noun
(B) wrong verb form: concerning modal verbs (e.g., 
?would said?), subject-verb agreement, auxiliary 
(e.g., ?should have tell the truth?), gerund and in-
finitive usage (e.g., ?look forward to see you? and 
?in an attempt to helping you?)
(2) wrong preposition (or infinitive-to)
(A) wrong preposition (e.g., ?to depends of it?)
(B) wrong preposition and verb form (e.g., ?to play 
an important role to close this deal?)
(3) transitivity errors
(A) transitive verb (e.g., ?to discuss about the mat-
ter? and ?to affect to his decision?)
(B) intransitive verb (e.g., ?to listens the music?)
The system is designed to find pattern rules related 
to the errors and return suggestionst. We now for-
mally state the problem that we are addressing.
Problem  Statement: We are given a reference 
corpus C and a non-native passage T. Our goal is 
to detect  grammatical and usage errors in T and 
provide suggestions for correction. For this, we 
extract a set of pattern rules, u
1
,?, u
m
 from C 
such that the rules reflect the predominant usage 
and are likely to distinguish most errors in non-
native writing. 
In the rest  of this section, we describe our solu-
tion to this problem. First, we define a strategy for 
identifying predominant  phraseology of frequent 
ngrams and collocations in Section 3.2. Afer that, 
we show how EdIt proposes grammar correc-
tionsedits to non-native writing at  run-time in Sec-
tion 3.3.
3.2 Deriving Pattern Rules
We attempt  to derive patterns (e.g., ?play ~ role in 
V-ing?) from C expected to represent the immedi-
ate context  of collocations (e.g., ?play ~ role? or 
?look forward?). Our derivation process consists of 
the following four-stage:
Stage 1. Lemmatizing, POS Tagging and Phrase 
chunking. In the first  stage, we lemmatize and tag 
sentences in C. Lemmatization and POS tagging 
both help to produce more general pattern rules 
from ngrams or collocations. The based phrases are 
used to extract collocations.
Stage 2. Ngrams and Collocations. In the second 
stage of the training process, we calculate ngrams 
and collocations in C, and pass the frequent 
ngrams and collocations to Stage 4.
We employ a number of steps to acquire statisti-
cally significant collocations--determining the pair 
of head words in adjacent base phrases, calculating 
their pair-wise mutual information values, and fil-
tering out candidates with low MI values. 
Stage 3. onstructing Inverted Files. In the third 
stage in the training procedure, we build up in-
verted files for the lemmas in C for quick access in 
Stage 4. For each word lemma we store surface 
words, POS tags, pointers to sentences with base 
phrases marked.
2
 See (Nicholls, 1999) for common errors.
3
 See (Leacock and Chodorow, 2003) and (Burstein et al, 2004) for correlation.
28
procedure GrammarChecking(T,PatternGrammarBank)
(1) Suggestions=??//candidate suggestions
(2) sentences=sentenceSplitting(T)
for each sentence in sentences
(3)   userProposedUsages=extractUsage(sentence)
for each userUsage in userProposedUsages
(4)     patGram=findPatternGrammar(userUsage.lexemes, 
PatternGrammarBank)
(5)     minEditedCost=SystemMax; minEditedSug=??
for each pattern in patGram
(6)        cost=extendedLevenshtein(userUsage,pattern)
if cost<minEditedCost
(7)            minEditedCost=cost; minEditedSug=pattern
if minEditedCost>0
(8)       append (userUsage,minEditedSug) to Suggestions
(9) Return Suggestions
Figure 2. Grammar suggestion/correction at run-time
Stage 4. Deriving pattern rules. In the fourth and 
final stage, we use the method described in a pre-
vious work (Chen et al, 2011) and use the inverted 
files to find all sentences containing a give word 
and collocation. Words surrounding a collocation 
are identified and generalized based on their corre-
sponding POS tags. These sentences are then trans-
formed into a set  of n-gram of words and POS 
tags, which are subsequently counted and ranked to 
produce  pattern rules with high frequencies. 
3.3 Run-Time Error Correction
Once the patterns rules are derived from a corpus 
of well-formed texts, EdIt utilizes them to check 
grammaticality and provide suggestions for a given 
text via the procedure in Figure 2.
In Step (1) of the procedure, we initiate a set 
Suggestions to collect grammar suggestions to the 
user text T according to the bank of pattern gram-
mar PatternGrammarBank. Since EdIt  system fo-
cuses on grammar checking at  sentence level, T is 
heuristically split (Step (2)).
For each sentence, we extract ngram and POS 
tag sequences userUsage in T. For the example of 
?He play an important  roles. He looks forword to 
hear you?,  we extract ngram such as he V DET, 
play an JJ NNS, play ~ roles to V, this NNS, look 
forward to VB, and hear Pron. 
For each userUsage, we first access the pattern 
rules related to the word and collocation within 
(e.g., play-role  patterns for ?play ~ role to close?) 
Step (4). And then we compare userUsage against 
these rules (from Step (5) to (7)). We use the ex-
tended Levenshtein?s algorithm shown in Figure 3 
to compare userUsage and pattern rules.
Figure 3. Algorithm for identifying errors
If only partial matches are found for userUsage, 
that could mean we have found a potential errors. 
We use minEditedCost and minEditedSug to con-
train the patterns rules found for error suggestions 
(Step (5)). In the following, we describe how to 
find minimal-distance edits.
In Step (1) of the algorithm in Figure 3 we allo-
cate and initialize costArray to gather the dynamic 
programming based cost  to transform userUsage 
into a specific contextual rule pattern. Afterwards, 
the algorithm defines the cost of performing substi-
tution (Step (2)), deletion (Step (3)) and insertion 
(Step (4)) at  i-indexed userUsage and j-indexed 
pattern. If the entries userUsage[i] and pattern[j] 
are equal literally (e.g., ?VB? and ?VB?) or gram-
matically (e.g., ?DT? and ?Pron$?), no edit  is 
needed, hence, no cost  (Step (2a)). On the other 
hand, since learners tend to select wrong word 
form and preposition, we set a lower cost  for  sub-
stitution among different word forms of the same 
lemma or lemmas with the same POS tag (e.g., 
replacing V with V-ing or replacing to with in?. In 
addition to the conventional deletion and insertion 
(Step (3b) and (4b) respectively), we look ahead to 
the elements userUsage[i+1] and pattern[j+1] con-
sidering the fact that  ?with or without preposition? 
and ?transitive or intransitive verb? often puzzles 
EFL learners (Step (3a) and (4a)). Only a small 
edit cost is counted if the next  elements in use-
rUsage and Pattern are ?equal?. In Step (6) the 
extended Levenshtein?s algorithm returns the 
minimum edit  cost of revising userUsage using 
pattern.
Once we obtain the costs to transform the use-
rUsage into a similar, frequent pattern rules, we 
propose the minimum-cost rules as suggestions  for 
procedure extendedLevenshtein(userUsage,pattern)
(1) allocate and initialize costArray
for i in range(len(userUsage))
for j in range(len(pattern))
if equal(userUsage[i],pattern[j]) //substitution
(2a)       substiCost=costArray[i-1,j-1]+0
elseif sameWordGroup(userUsage[i],pattern[j])
(2b)       substiCost=costArray[i-1,j-1]+0.5
(2c)     else substiCost=costArray[i-1,j-1]+1
if equal(userUsage[i+1],pattern[j+1]) //deletion
(3a)       delCost=costArray[i-1,j]+smallCost
(3b)     else delCost=costArray[i-1,j]+1
if equal(userUsage[i+1],pattern[j+1]) //insertion
(4a)        insCost=costArray[i,j-1]+smallCost
(4b)      else insCost=costArray[i,j-1]+1
(5)        costArray[i,j]=min(substiCost,delCost,insCost)
(6) Return costArray[len(userUsage),len(pattern)]
29
correction (e.g., ?play ~ role in V-ing? for revising 
?play ~ role to V?) (Step (8) in Figure 2), if its 
minimum edit  cost  is greater than zero. Otherwise, 
the usage is considered valid. Finally, the Sugges-
tions accumulated for T are returned to users (Step 
(9)). Example input  and editorial suggestions re-
turned to the user are shown in Figure 1. Note that 
pattern rules involved flexible collocations are de-
signed to take care of long distance dependencies 
that might be always possible to cover with limited 
ngram (for n less than 6). In addition, the long pat-
ter rules can be useful even when it  is not  clear 
whether there is an error when looking at a very 
narrow context. For example, ?hear? can be either 
be transitive or intransitive depending on context. 
In the context of ?look forward to? and person 
noun object, it is should be intransitive and  require 
the preposition ?from? as suggested in the results 
provided by EdIt (see Figure 1).
In existing grammar checkers, there are typically 
many modules examining different types of errors 
and different module may have different  priority 
and conflict  with one another. Let  us note that this 
general framework for error detection and correc-
tion is an original contribution of our work. In ad-
dition, we incorporate probabilities conditioned on 
word positions in order to weigh edit  costs. For 
example, the conditional probability of V to imme-
diately follow ?look forward to? is virtually  0, 
while the probability of V-ing  to do so is approxi-
mates 0.3. Those probabilistic values are used to 
weigh different edits. 
4 Experimental Results
In this section, we first present the experimental 
setting in EdIt (Section 4.1). Since our goal is to 
provide to learners a means to efficient  broad-
coverage grammar checking, EdIt  is web-based 
and the acquisition of the pattern grammar in use is 
offline. Then, we illustrate three common types of 
errors, scores correlated, EdIt
4
 capable of handling.
4.1 Experimental Setting
We used British National Corpus (BNC) as our 
underlying general corpus C. It is a 100 million 
British English word collection from a wide range 
of sources. We exploited GENIA tagger to obtain 
the lemmas, PoS tags and shallow parsing results 
of C?s sentences, which were all used in construct-
ing inverted files and used as examples for GRASP 
to infer lexicalized pattern grammar.
Inspired by (Chen et al, 2011) indicating EFL 
learners tend to choose incorrect  prepositions and 
following word forms following a VN collocation, 
and (Gamon and Leacock, 2010) showing fixed-
length and fixed-window lexical items are the best 
evidence for correction, we equipped EdIt with 
pattern grammar rules consisting of fixed-length 
(from one- to five-gram) lexical sequences or VN 
collocations and their fixed-window usages (e.g., 
?IN(in) VBG? after ?play ~ role?, for window 2).
4.2 Results
We examined three types of errors and the mixture 
of them for our correction system (see Table 1). In 
this table, results of ESL Assistant  are shown for 
comparison, and grammatical suggestions are un-
derscored. As suggested, lexical and PoS informa-
tion in learner texts is useful for a grammar 
checker, pattern grammar EdIt  uses is easily acces-
sible and effective in both grammaticality and us-
age check, and a weighted extension to Leven-
shtein?s algorithm in EdIt  accommodates substitu-
tion, deletion and insertion edits to learners? fre-
quent mistakes in writing.
5 Future Work and Summary 
Many avenues exist  for future research and im-
provement. For example, we could augment  pat-
tern grammar with lexemes? PoS information in 
that the contexts of a word of different PoS tags 
vary. Take discuss for instance. The present tense 
verb discuss is often followed by determiners and 
nouns while the passive is by the preposition in  as 
in ?? is discussed in Chapter one.? Additionally, 
an interesting direction to explore is enriching pat-
tern grammar with semantic role labels (Chen et 
al., 2011) for simple semantic check.
In summary, we have introduced a method for 
correcting errors in learner text based on its lexical 
and PoS evidence. We have implemented the 
method and shown that the pattern grammar and 
extended Levenshtein algorithm in this method are 
promising in grammar checking. Concerning EdIt?s 
broad coverage over different  error types, simplic-
ity in design, and short  response time, we plan to 
evaluate it more fully: with or without  conditional 
probability using majority voting or not.
4
 At http://140.114.214.80/theSite/EdIt_demo2/
30
Erroneous sentence EdIt suggestion ESL Assistant suggestion
Incorrect word form
? a sunny days ? a sunny N a sunny day
every days, I ? every N every day
I would said to ? would V would say
he play a ? he V-ed none
? should have tell the truth should have V-en should have to tell
? look forward to see you look forward to V-ing none
? in an attempt to seeing you an attempt to V none
? be able to solved this problem able to V none
Incorrect preposition
he plays an important role to close ? play ~ role in none
he has a vital effect at her. have ~ effect on effect on her
it has an effect on reducing ? have ~ effect of V-ing none
? depend of the scholarship depend on depend on
Confusion between intransitive and transitive verb
he listens the music. missing ?to? after ?listens? missing ?to? after ?listens?
it affects to his decision. unnecessary ?to? unnecessary ?to?
I understand about the situation. unnecessary ?about? unnecessary ?about?
we would like to discuss about this matter. unnecessary ?about? unnecessary ?about?
Mixed
she play an important roles to close this deals. she V-ed; an Adj N;
play ~ role in V-ing; this N
play an important role;
close this deal
I look forward to hear you. look forward to V-ing;
missing ?from? after ?hear?
none
Table 1. Three common score-related error types and their examples with suggestions from EdIt and ESL Assistant.
References
C. Brockett, W. Dolan, and M. Gamon. 2006. Correcting ESL 
errors using phrasal SMT techniques. In Proceedings of the 
ACL.
J. Burstein, M. Chodorow, and C. Leacock. 2004. Automated 
essay evaluation: the criterion online writing  service. AI 
Magazine, 25(3):27-36.
M. H. Chen, C. C. Huang, S. T. Huang, H. C. Liou, and J. S. 
Chang. 2011. A cross-lingual pattern retrieval framework. 
In Proceedings of the CICLing.
M. Chodorow and C. Leacock. 2000. An unsupervised method 
for detecting grammatical  errors. In Proceedings of the 
NAACL, pages 140-147.
R. De Felice and S. Pulman. 2008. A classifer-based approach 
to  preposition and determiner error correction in  L2 Eng-
lish. In COLING.
I. S. Fraser and L. M. Hodson. 1978. Twenty-one kicks at the 
grammar horse. English Journal.
M. Gamon, C. Leacock, C. Brockett, W. B. Bolan, J. F. Gao, 
D. Belenko, and A. Klementiev.  Using statistical tech-
niques and web search to correct ESL errors. CALICO, 
26(3): 491-511.
M. Gamon and C. Leacock. 2010. Search right and thou shalt 
find ? using web queries for learner error detection. In 
Proceedings of the NAACL.
M. Hermet, A. Desilets, S. Szpakowicz. 2008. Using the web 
as a linguistic resource to automatically correct lexico-
syntatic errors. In LREC, pages 874-878.
S. Hunston and G. Francis. 2000. Pattern grammar: a corpus-
driven approach to the lexical grammar of English.
C. M. Lee, S. J. Eom, and M. Dickinson. 2009. Toward ana-
lyzing Korean learner particles. In CALICO.
V. I. Levenshtein. 1966. Binary codes capable of correcting 
deletions, insertions and reversals. Soviet Physics Doklady, 
10:707-710.
C. Leacock and M. Chodorow. 2003. Automated grammatical 
error detection.
D. Nicholls. 1999. The Cambridge Learner Corpus ? error 
coding and analysis for writing dictionaries and other 
books for English Learners.
G. H. Sun, X. H. Liu, G. Cong, M. Zhou, Z. Y. Xiong, J. Lee, 
and C. Y. Lin. 2007. Detecting erroneous sentences using 
automatically mined sequential patterns. In ACL.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using parse 
features for prepositions selection and error detection. In 
Proceedings of the ACL, pages 353-358.
N. L. Tsao and D. Wible. 2009. A method for unsupervised 
broad-coverage lexical error detection and correction. In 
NAACL Workshop, pages 51-54.
31
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1?6,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
Cross-Lingual Information to the Rescue in Keyword Extraction 
 
1Chung-Chi Huang 2Maxine Eskenazi 3Jaime Carbonell 4Lun-Wei Ku 5Ping-Che Yang 
1,2,3Language Technologies Institute, CMU, United States 
4Institute of Information Science, Academia Sinica, Taiwan 
5Institute for Information Industry, Taipei, Taiwan 
{1u901571,4lunwei.jennifer.ku}@gmail.com 
{2max+,3jgc}@cs.cmu.edu 5maciaclark@iii.org.tw 
 
  
  
Abstract 
We introduce a method that extracts keywords 
in a language with the help of the other. In our 
approach, we bridge and fuse conventionally 
irrelevant word statistics in languages. The 
method involves estimating preferences for 
keywords w.r.t. domain topics and generating 
cross-lingual bridges for word statistics 
integration. At run-time, we transform parallel 
articles into word graphs, build cross-lingual 
edges, and exploit PageRank with word 
keyness information for keyword extraction. 
We present the system, BiKEA, that applies 
the method to keyword analysis. Experiments 
show that keyword extraction benefits from 
PageRank, globally learned keyword 
preferences, and cross-lingual word statistics 
interaction which respects language diversity. 
1 Introduction 
Recently, an increasing number of Web services 
target extracting keywords in articles for content 
understanding, event tracking, or opinion mining. 
Existing keyword extraction algorithm (KEA) 
typically looks at articles monolingually and 
calculate word significance in certain language. 
However, the calculation in another language 
may tell the story differently since languages 
differ in grammar, phrase structure, and word 
usage, thus word statistics on keyword analysis. 
Consider the English article in Figure 1. Based 
on the English content alone, monolingual KEA 
may not derive the best keyword set. A better set 
might be obtained by referring to the article and 
its counterpart in another language (e.g., 
Chinese). Different word statistics in articles of 
different languages may help, due to language 
divergence such as phrasal structure (i.e., word 
order) and word usage and repetition (resulting 
from word translation or word sense) and so on. 
For example, bilingual phrases ?social 
reintegration? and ?????? in Figure 1 have 
inverse word orders (?social? translates into ??
? ? and ?reintegration? into ? ? ? ?), both 
?prosthesis? and ?artificial limbs? translate into 
????, and ?physical? can be associated with ??
? ? and ??? ? in ?physical therapist? and 
?physical rehabilitation? respectively. Intuitively, 
using cross-lingual statistics (implicitly 
leveraging language divergence) can help look at 
articles from different perspectives and extract 
keywords more accurately. 
We present a system, BiKEA, that learns to 
identify keywords in a language with the help of 
the other. The cross-language information is 
expected to reinforce language similarities and 
value language dissimilarities, and better 
understand articles in terms of keywords. An 
example keyword analysis of an English article 
is shown in Figure 1. BiKEA has aligned the 
parallel articles at word level and determined the 
scores of topical keyword preferences for words. 
BiKEA learns these topic-related scores during 
training by analyzing a collection of articles. We 
will describe the BiKEA training process in more 
detail in Section 3. 
At run-time, BiKEA transforms an article in a 
language (e.g., English) into PageRank word 
graph where vertices are words in the article and 
edges between vertices indicate the words? co-
occurrences. To hear another side of the story, 
BiKEA also constructs graph from its counterpart 
in another language (e.g., Chinese). These two 
independent graphs are then bridged over nodes 
1
  
 
 
 
 
 
 
 
 
 
Figure 1. An example BiKEA keyword analysis for an article.
that are bilingually equivalent or aligned. The 
bridging is to take language divergence into 
account and to allow for language-wise 
interaction over word statistics. BiKEA, then in 
bilingual context, iterates with learned word 
keyness scores to find keywords. In our 
prototype, BiKEA returns keyword candidates of 
the article for keyword evaluation (see Figure 1); 
alternatively, the keywords returned by BiKEA 
can be used as candidates for social tagging the 
article or used as input to an article 
recommendation system. 
2 Related Work 
Keyword extraction has been an area of active 
research and applied to NLP tasks such as 
document categorization (Manning and Schutze, 
2000), indexing (Li et al., 2004), and text mining 
on social networking services ((Li et al., 2010); 
(Zhao et al., 2011); (Wu et al., 2010)). 
The body of KEA focuses on learning word 
statistics in document collection. Approaches 
such as tfidf and entropy, using local document 
and/or across-document information, pose strong 
baselines. On the other hand, Mihalcea and 
Tarau (2004) apply PageRank, connecting words 
locally, to extract essential words. In our work, 
we leverage globally learned keyword 
preferences in PageRank to identify keywords. 
Recent work has been done on incorporating 
semantics into PageRank. For example, Liu et al. 
(2010) construct PageRank synonym graph to 
accommodate words with similar meaning. And 
Huang and Ku (2013) weigh PageRank edges 
based on nodes? degrees of reference. In contrast, 
we bridge PageRank graphs of parallel articles to 
facilitate statistics re-distribution or interaction 
between the involved languages. 
In studies more closely related to our work, 
Liu et al. (2010) and Zhao et al. (2011) present 
PageRank algorithms leveraging article topic 
information for keyword identification. The main 
differences from our current work are that the 
article topics we exploit are specified by humans 
not by automated systems, and that our 
PageRank graphs are built and connected 
bilingually. 
In contrast to the previous research in keyword 
extraction, we present a system that 
automatically learns topical keyword preferences 
and constructs and inter-connects PageRank 
graphs in bilingual context, expected to yield 
better and more accurate keyword lists for 
articles. To the best of our knowledge, we are the 
first to exploit cross-lingual information and take 
advantage of language divergence in keyword 
extraction. 
3 The BiKEA System 
Submitting natural language articles to keyword 
extraction systems may not work very well. 
Keyword extractors typically look at articles 
from monolingual points of view. Unfortunately, 
word statistics derived based on a language may 
The English Article: 
I've been in Afghanistan for 21 years. I work for the Red Cross and I'm a physical therapist. My job is to 
make arms and legs -- well it's not completely true. We do more than that. We provide the patients, the 
Afghan disabled, first with the physical rehabilitation then with the social reintegration. It's a very logical 
plan, but it was not always like this. For many years, we were just providing them with artificial limbs. It 
took quite many years for the program to become what it is now. ? 
 
Its Chinese Counterpart: ??????? 21 ?? ????????? ?????????? ??????????? -- ?????????? ?????????? ???????? ???????? ???????, ??????? ???????????? ?????????? ??????????? ????? ???????? ?????????????? 
 
Word Alignment Information: 
physical (??), therapist (???), social (??), reintegration (??), physical (??), rehabilitation  (??), prosthesis (??), ? 
 
Scores of Topical Keyword Preferences for Words: 
(English)    prosthesis: 0.32; artificial leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; ? 
(Chinese)   ??: 0.41; ?????: 0.15; ??:0.10; ???: 0.08, ? 
 
English Keywords from Bilingual Perspectives: 
prosthesis, artificial, leg, rehabilitation, orthopedic, ? 
2
be biased due to the language?s grammar, phrase 
structure, word usage and repetition and so on. 
To identify keyword lists from natural language 
articles, a promising approach is to automatically 
bridge the original monolingual framework with 
bilingual parallel information expected to respect 
language similarities and diversities at the same 
time.  
3.1 Problem Statement 
We focus on the first step of the article 
recommendation process: identifying a set of 
words likely to be essential to a given article. 
These keyword candidates are then returned as 
the output of the system. The returned keyword 
list can be examined by human users directly, or 
passed on to article recommendation systems for 
article retrieval (in terms of the extracted 
keywords). Thus, it is crucial that keywords be 
present in the candidate list and that the list not 
be too large to overwhelm users or the 
subsequent (typically computationally expensive) 
article recommendation systems. Therefore, our 
goal is to return reasonable-sized set of keyword 
candidates that, at the same time, must contain 
essential terms in the article. We now formally 
state the problem that we are addressing. 
Problem Statement: We are given a bilingual 
parallel article collection of various topics from 
social media (e.g., TED), an article ARTe in 
language e, and its counterpart ARTc in language 
c. Our goal is to determine a set of words that are 
likely to contain important words of ARTe. For 
this, we bridge language-specific statistics of 
ARTe and ARTc via bilingual information (e.g., 
word alignments) and consider word keyness 
w.r.t. ARTe?s topic such that cross-lingual 
diversities are valued in extracting keywords in e. 
In the rest of this section, we describe our 
solution to this problem. First, we define 
strategies for estimating keyword preferences for 
words under different article topics (Section 3.2). 
These strategies rely on a set of article-topic 
pairs collected from the Web (Section 4.1), and 
are monolingual, language-dependent 
estimations. Finally, we show how BiKEA 
generates keyword lists for articles leveraging 
PageRank algorithm with word keyness and 
cross-lingual information (Section 3.3). 
3.2 Topical Keyword Preferences 
We attempt to estimate keyword preferences 
with respect to a wide range of article topics. 
Basically, the estimation is to calculate word 
significance in a domain topic. Our learning 
process is shown in Figure 2. 
 
 
 
 
 
 
Figure 2. Outline of the process used 
to train BiKEA. 
In the first two stages of the learning process, we 
generate two sets of article and word information. 
The input to these stages is a set of articles and 
their domain topics. The output is a set of pairs 
of article ID and word in the article, e.g., 
(ARTe=1, we=?prosthesis?) in language e or 
(ARTc=1, wc=????) in language c, and a set of 
pairs of article topic and word in the article, e.g., 
(tpe=?disability?, we=?prosthesis?) in e and 
(tpe=?disability?, wc=????) in c. Note that the 
topic information is shared between the involved 
languages, and that we confine the calculation of 
such word statistics in their specific language to 
respect language diversities and the language-
specific word statistics will later interact in 
PageRank at run-time (See Section 3.3). 
The third stage estimates keyword preferences 
for words across articles and domain topics using 
aforementioned (ART,w) and (tp,w) sets. In our 
paper, two popular estimation strategies in 
Information Retrieval are explored. They are as 
follows. 
tfidf. tfidf(w)=freq(ART,w)/appr(ART?,w) where 
term frequency in an article is divided by its 
appearance in the article collection to distinguish 
important words from common words. 
ent. entropy(w)= -?
tp?
Pr(tp?|w)?log(Pr(tp?|w)) 
where  a word?s uncertainty in topics is used to 
estimate its associations with domain topics. 
These strategies take global information (i.e., 
article collection) into account, and will be used 
as keyword preference models, bilingually 
intertwined, in PageRank at run-time which 
locally connects words (i.e., within articles). 
3.3 Run-Time Keyword Extraction 
Once language-specific keyword preference 
scores for words are automatically learned, they 
are stored for run-time reference. BiKEA then 
uses the procedure in Figure 3 to fuse the 
originally language-independent word statistics 
(1) Generate article-word pairs in training data 
(2) Generate topic-word pairs in training data 
(3) Estimate keyword preferences for words w.r.t.  
      article topic based on various strategies 
(4) Output word-and-keyword-preference-score  
      pairs for various strategies 
3
to determine keyword list for a given article. In 
this procedure a machine translation technique 
(i.e., IBM word aligner) is exploited to glue 
statistics in the involved languages and make 
bilingually motivated random-walk algorithm 
(i.e., PageRank) possible. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Extracting keywords at run-time. 
 
Once language-specific keyword preference 
scores for words are automatically learned, they 
are stored for run-time reference. BiKEA then 
uses the procedure in Figure 3 to fuse the 
originally language-independent word statistics 
to determine keyword list for a given article. In 
this procedure a machine translation technique 
(i.e., IBM word aligner) is exploited to glue 
statistics in the involved languages and make 
bilingually motivated random-walk algorithm 
(i.e., PageRank) possible.  
In Steps (1) and (2) we construct PageRank 
word graphs for the article ARTe in language e 
and its counterpart ARTc in language c. They are 
built individually to respect language properties 
(such as subject-verb-object or subject-object-
verb structure). Figure 4 shows the algorithm. In 
this algorithm, EW stores normalized edge 
weights for word wi and wj (Step (2)). And EW 
is a v by v matrix where v is the vocabulary size 
of ARTe and ARTc. Note that the graph is directed 
(from words to words that follow) and edge 
weights are words? co-occurrences within 
window size WS. Additionally we incorporate 
edge weight multiplier m>1 to propagate more 
PageRank scores to content words, with the 
intuition that content words are more likely to be 
keywords (Step (2)). 
 
 
 
 
 
 
 
Figure 4. Constructing PageRank word graph. 
Step (3) in Figure 3 linearly combines word 
graphs EWe and EWc using ?. We use ? to 
balance language properties or statistics, and 
BiKEA backs off to monolingual KEA if ? is one. 
In Step (4) of Figure 3 for each word 
alignment (wic, wje), we construct a link between 
the word nodes with the weight BiWeight. The 
inter-language link is to reinforce language 
similarities and respect language divergence 
while the weight aims to elevate the cross-
language statistics interaction. Word alignments 
are derived using IBM models 1-5 (Och and Ney, 
2003). The inter-language link is directed from 
wi
c
 to wj
e
, basically from language c to e based on 
the directional word-aligning entry (wic, wje). The 
bridging is expected to help keyword extraction 
in language e with the statistics in language c. 
Although alternative approach can be used for 
bridging, our approach is intuitive, and most 
importantly in compliance with the directional 
spirit of PageRank. 
Step (6) sets KP of keyword preference model 
using topical preference scores learned from 
Section 3.2, while Step (7) initializes KN of 
PageRank scores or, in our case, word keyness 
scores. Then we distribute keyness scores until 
the number of iteration or the average score 
differences of two consecutive iterations reach 
their respective limits. In each iteration, a word?s 
keyness score is the linear combination of its 
keyword preference score and the sum of the 
propagation of its inbound words? previous 
PageRank scores. For the word wje in ARTe, any 
edge (wie,wje) in ARTe, and any edge (wkc,wje) in 
WA, its new PageRank score is computed as 
below. 
procedure PredictKW(ARTe,ARTc,KeyPrefs,WA,?,N) 
//Construct language-specific word graph for PageRank 
(1)  EWe=constructPRwordGraph(ARTe) 
(2)  EWc=constructPRwordGraph(ARTc) 
//Construct inter-language bridges 
(3)  EW=?? EWe+(1-?) ? EWc 
       for each word alignment (wic, wje) in WA 
         if IsContWord(wic) and IsContWord(wje) 
(4a)      EW[i,j]+=1? BiWeightcont 
         else 
(4b)      EW[i,j]+=1? BiWeightnoncont 
(5)  normalize each row of EW to sum to 1 
//Iterate for PageRank 
(6)  set KP1 ?v to 
             [KeyPrefs(w1), KeyPrefs(w2), ?,KeyPrefs(wv)] 
(7)  initialize KN1 ?v to [1/v,1/ v, ?,1/v] 
       repeat 
(8a)  KN?=?? KN? EW+(1-?) ? KP 
(8b)  normalize KN? to sum to 1 
(8c)  update KN with KN? after the check of KN and KN? 
       until maxIter or avgDifference(KN,KN?) ? smallDiff 
(9)  rankedKeywords=Sort words in decreasing order of KN 
       return the N rankedKeywords in e with highest 
scores 
procedure constructPRwordGraph(ART) 
(1) EWv ?v=0v ?v 
      for each sentence st in ART 
         for each word wi in st 
            for each word wj in st where i<j and j-i ? WS 
         if not IsContWord(wi) and IsContWord(wj) 
(2a)            EW[i,j]+=1? m 
               elif not IsContWord(wi) and not IsContWord(wj) 
(2b)            EW[i,j]+=1 ? (1/m) 
               elif IsContWord(wi) and not IsContWord(wj) 
(2c)            EW[i,j]+=1? (1/m) 
               elif IsContWord(wi) and IsContWord(wj) 
(2d)            EW[i,j]+=1 ? m 
       return EW 
4
???[1, ?] =? ?
??
? ? ????[1, ?] ? ???[?, ?] +???(1 ? ?) ????[1, ?] ? ??[?, ?]??? ??
?
+ (1 ??) ? ??[1, ?] 
 
Once the iterative process stops, we rank 
words according to their final keyness scores and 
return top N ranked words in language e as 
keyword candidates of the given article ARTe. An 
example keyword analysis for an English article 
on our working prototype is shown in Figure 1. 
Note that language similarities and dissimilarities 
lead to different word statistics in articles of 
difference languages, and combining such word 
statistics helps to generate more promising 
keyword lists. 
4 Experiments 
BiKEA was designed to identify words of 
importance in an article that are likely to cover 
the keywords of the article. As such, BiKEA will 
be trained and evaluated over articles. 
Furthermore, since the goal of BiKEA is to 
determine a good (representative) set of 
keywords with the help of cross-lingual 
information, we evaluate BiKEA on bilingual 
parallel articles. In this section, we first present 
the data sets for training BiKEA (Section 4.1). 
Then, Section 4.2 reports the experimental 
results under different system settings. 
4.1 Data Sets 
We collected approximately 1,500 English 
transcripts (3.8M word tokens and 63K word 
types) along with their Chinese counterparts 
(3.4M and 73K) from TED (www.ted.com) for 
our experiments. The GENIA tagger (Tsuruoka 
and Tsujii, 2005) was used to lemmatize and 
part-of-speech tag the English transcripts while 
the CKIP segmenter (Ma and Chen, 2003) 
segment the Chinese. 
30 parallel articles were randomly chosen and 
manually annotated for keywords on the English 
side to examine the effectiveness of BiKEA in 
English keyword extraction with the help of 
Chinese. 
4.2 Experimental Results 
Table 1 summarizes the performance of the 
baseline tfidf and our best systems on the test set. 
The evaluation metrics are nDCG (Jarvelin and 
Kekalainen, 2002), precision, and mean 
reciprocal rank. 
(a) @N=5 nDCG P MRR 
tfidf .509 .213 .469 
PR+tfidf .676 .400 .621 
BiKEA+tfidf .703 .406 .655 
 
(b) @N=7 nDCG P MRR 
tfidf .517 .180 .475 
PR+tfidf .688 .323 .626 
BiKEA+tfidf .720 .338 .660 
 
(c) @N=10 nDCG P MRR 
tfidf .527 .133 .479 
PR+tfidf .686 .273 .626 
BiKEA+tfidf .717 .304 .663 
Table 1. System performance at 
(a) N=5 (b) N=7 (c) N=10. 
As we can see, monolingual PageRank (i.e., 
PR) and bilingual PageRank (BiKEA), using 
global information tfidf, outperform tfidf. They 
relatively boost nDCG by 32% and P by 87%. 
The MRR scores also indicate their superiority: 
their top-two candidates are often keywords vs. 
the 2nd place candidates from tfidf. 
Encouragingly, BiKEA+tfidf achieves better 
performance than the strong monolingual 
PR+tfidf across N?s. Specifically, it further 
improves nDCG relatively by 4.6% and MRR 
relatively by 5.4%. 
Overall, the topical keyword preferences, and 
the inter-language bridging and the bilingual 
score propagation in PageRank are simple yet 
effective. And respecting language statistics and 
properties helps keyword extraction. 
5 Summary 
We have introduced a method for extracting 
keywords in bilingual context. The method 
involves estimating keyword preferences, word-
aligning parallel articles, and bridging language-
specific word statistics using PageRank. 
Evaluation has shown that the method can 
identify more keywords and rank them higher in 
the candidate list than monolingual KEAs. As for 
future work, we would like to explore the 
possibility of incorporating the articles? reader 
feedback into keyword extraction. We would 
also like to examine the proposed methodology 
in a multi-lingual setting.  
5
Acknowledgement 
This study is conducted under the ?Online and 
Offline integrated Smart Commerce Platform 
(1/4)? of the Institute for Information Industry 
which is subsidized by the Ministry of Economy 
Affairs of the Republic of China. 
References  
Scott A. Golder and Bernardo A. Huberman. 
2006. Usage patterns of collaborative tagging 
systems. Information Science, 32(2): 198-208. 
Harry Halpin, Valentin Robu, and Hana 
Shepherd. 2007. The complex dynamics of 
collaborative tagging. In Proceedings of the 
WWW, pages 211-220. 
Chung-chi Huang and Lun-wei Ku. 2013. 
Interest analysis using semantic PageRank and 
social interaction content. In Proceedings of 
the ICDM Workshop on Sentiment Elicitation 
from Natural Text for Information Retrieval 
and Extraction, pages 929-936. 
Kalervo Jarvelin and Jaana Kekalainen. 2002. 
Cumulated gain-based evaluation of IR 
technologies. ACM Transactions on 
Information Systems, 20(4): 422-446. 
Philipp Koehn, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase-based 
translation. In Proceedings of the North 
American Chapter of the Association for 
Computational Linguistics, pages 48-54. 
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin 
Chen. 2004. Incorporating document 
keyphrases in search results. In Proceedings of 
the Americas Conference on Information 
Systems. 
Zhenhui Li, Ging Zhou, Yun-Fang Juan, and 
Jiawei Han. 2010. Keyword extraction for 
social snippets. In Proceedings of the WWW, 
pages 1143-1144. 
Marina Litvak and Mark Last. 2008. Graph-
based keyword extraction for single-document 
summarization. In Proceedings of the ACL 
Workshop on Multi-Source Multilingual 
Information Extraction and Summarization, 
pages 17-24. 
Zhengyang Liu, Jianyi Liu, Wenbin Yao, Cong 
Wang. 2010. Keyword extraction using 
PageRank on synonym networks. In 
Proceedings of the ICEEE, pages 1-4. 
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and 
Maosong Sun. 2010. Automatic keyphrase 
extraction via topic decomposition. In 
Proceedings of the EMNLP, pages 366-376. 
Wei-Yun Ma and Keh-Jiann Chen. 2003. 
Introduction to CKIP Chinese word 
segmentation system for the first international 
Chinese word segmentation bakeoff. In 
Proceedings of the ACL Workshop on Chinese 
Language Processing. 
Chris D. Manning and Hinrich Schutze. 2000. 
Foundations of statistical natural language 
processing. MIT Press. 
Rada Mihalcea and Paul Tarau. 2004. TextRank: 
Bringing orders into texts. In Proceedings of 
the EMNLP, pages 404-411. 
Franz Josef Och and Hermann Ney. 2003. A 
systematic comparison of various statistical 
alignment models. Computational Linguistics, 
29(1): 19-51. 
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. 
Bidirectional inference with the easiest-first 
strategy for tagging sequence data. In 
Proceedings of the EMNLP, pages 467-474. 
Peter D. Turney. 2000. Learning algorithms for 
keyphrase extraction. Information Retrieval, 
2(4): 303-336. 
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. 
Automatic generation of personalized 
annotation tags for Twitter users. In 
Proceedings of the NAACL, pages 689-692. 
Wayne Xin Zhao, Jing Jiang, Jing He, Yang 
Song, Palakorn Achananuparp, Ee-Peng Lim, 
and Xiaoming Li. 2011. Topical keyword 
extraction from Twitter. In Proceedings of the 
ACL, pages 379-388. 
 
6
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 96?104,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
GRASP: Grammar- and Syntax-based Pattern-Finder in CALL 
 
 
Chung-Chi Huang*  Mei-Hua Chen* Shih-Ting Huang+  Hsien-Chin Liou**  Jason S. Chang+ 
  
* Institute of Information Systems and Applications, NTHU, HsinChu, Taiwan, R.O.C. 300 
+ Department of Computer Science, NTHU, HsinChu, Taiwan, R.O.C. 300 
**  Department of Foreign Languages and Literature, NTHU, HsinChu, Taiwan, R.O.C. 300 
{u901571,chen.meihua,koromiko1104,hsienchin,jason.jschang}gmail.com 
 
 
 
 
 
 
Abstract 
We introduce a method for learning to 
describe the attendant contexts of a given 
query for language learning. In our 
approach, we display phraseological 
information in the form of a summary of 
general patterns as well as lexical bundles 
anchored at the query. The method 
involves syntactical analyses and inverted 
file construction. At run-time, grammatical 
constructions and their lexical 
instantiations characterizing the usage of 
the given query are generated and 
displayed, aimed at improving learners? 
deep vocabulary knowledge. We present a 
prototype system, GRASP, that applies the 
proposed method for enhanced collocation 
learning. Preliminary experiments show 
that language learners benefit more from 
GRASP than conventional dictionary 
lookup. In addition, the information 
produced by GRASP is potentially useful 
information for automatic or manual 
editing process. 
1 Introduction 
Many learners submit word or phrase queries (e.g., 
?role?) to language learning sites on the Web to 
get usage information every day, and an increasing 
number of services on the Web specifically target 
such queries. Language learning tools such as 
concordancers typically accept single-word queries 
and respond with example sentences containing the 
words. There are also collocation reference tools 
such as Sketch Engine and TANGO that provide 
co-occurring words for the query word. Another 
collocation tool, JustTheWord further organizes 
and displays collocation clusters. 
Learners may want to submit phrase queries 
(fixed or rigid collocaions) to learn further how to 
use the phrase in context, or in other words, to 
acquire the knowledge on the attendant 
phraseology of the query. These queries could be 
answered more appropriately if the tool accepted 
long queries and returned a concise summary of 
their surrounding contexts. 
Consider the query ?play role?. The best 
responses for this query are probably not just 
example sentences, but rather the phraseological 
tendencies described grammatically or lexically. A 
good response of such a summary might contain  
patterns such as ?play Det Adj role? (as in ?play an 
important role?) and ?play ~ role in V-ing? (as in 
?play ~ role in shaping ??). Intuitively, by 
exploiting simple part-of-speech analysis, we can 
derive such patterns, inspired by the grammatical 
theory of Pattern Grammar 1  in order to provide 
more information on demand beyond what is given 
in a grammar book. 
We present a system, GRASP, that provide a 
usage summary of the contexts of the query in the 
form of patterns and frequent lexical bundles. Such 
rich information is expected to help learners and 
lexicographers grasp the essence of word usages. 
An example GRASP response for the query ?play 
                                                           
1
 Please refer to (Hunston and Francis, 2000). 
96
role? is shown in Figure 1. GRASP has retrieved 
the sentences containing the query in a reference 
corpus. GRASP constructs these query-to-sentence 
index in the preparation stage (Section 3). 
 
Figure 1. An example GRASP search for ?play role?. 
 
At run-time, GRASP starts with a search query 
(e.g., ?play role?) submitted by the user. GRASP 
then retrieves example sentences and generates a 
summary of representative contexts, using patterns 
(e.g., ?play ~ role in V-ing?) and lexical bundles 
(e.g., ?play ~ role in shaping. In our 
implementation, GRASP also returns the 
translations and the example sentences of the 
lexical instances, so the learner can use their 
knowledge of native language to enhance the 
learning process. 
2 Related Work 
Computer-assisted language learning (CALL) has 
been an area of active research. Recently, more and 
more research based on natural language 
processing techniques has been done to help 
language learners. In our work, we introduce a 
language learning environment, where summarized 
usage information are provided, including how 
function words and verb forms are used in 
combination with the query. These usage notes 
often help contrast the common sources of error in 
learners? writing (Nicholls, 1999). In our pilot 
teaching experiment, we found learners have 
problems using articles and prepositions correctly 
in sentence composition (as high as 80% of the 
articles and 60% of the prepositions were used 
incorrectly), and GRASP is exactly aimed at 
helping ESL or EFL learners in that area. 
Until recently, collocations and usage 
information are compiled mostly manually 
(Benson et al, 1986). With the accessibility to 
large-scale corpora and powerful computers, it has 
become common place to compile a list of 
collocations automatically (Smadja, 1993). In 
addition, there are many collocation checkers 
developed to help non-native language learners 
(Chang et al, 2008), or learners of English for 
academic purposes (Durrant, 2009). 
Recently, automatic generation of collocations 
for computational lexicography and online 
language learning has drawn much attention. 
Sketch Engine (Kilgarriff et al, 2004) summarizes 
a word?s grammatical and collocation behavior, 
while JustTheWord clusters the co-occurring 
words of single-word queries and TANGO (Jian et 
al., 2004) accommodates cross-lingual collocation 
searches. Moreover, Cheng et al (2006) describe 
how to retrieve mutually expected words using 
concgrams. In contrast, GRASP, going one step 
further, automatically computes and displays the 
information that reveals the regularities of the 
contexts of user queries in terms of grammar 
patterns. 
Recent work has been done on incorporating 
word class information into the analyses of 
phraseological tendencies. Stubbs (2004) 
introduces phrase-frames, which are based on 
lexical ngrams with variable slots, while Wible et 
al. (2010) describe a database called StringNet, 
with lexico-syntactic patterns. Their methods of 
using word class information are similar in spirit to 
our work. The main differences are that our 
patterns is anchored with query words directly and 
generalizes query?s contexts via parts-of-speech, 
and that we present the query?s usage summary in 
Search query: 
Mapping query words to (position, sentence) pairs: 
?play? occurs in (10,77), (4,90), (6,102), ?, and so on. 
?role? occurs in (7,90), (12,122), (6,167), ?, and so on. 
A. In-between pattern grammar: 
   Distance 3 (1624): 
play DT JJ role (1364): 
e.g., ?play an important role? (259), ?play a major role? (168), ? 
play DT VBG role (123): 
e.g., ?play a leading role? (75), ?play a supporting role? (5), ? 
play DT JJR role (40): 
e.g., ?play a greater role? (17), ?play a larger role? (8), ? 
   Distance 2 (480): 
play DT role (63): 
e.g., ?play a role? (197), ?play the role? (123), ? 
play JJ role (63): 
e.g., ?play important role? (15), ?play different role? (6), ? 
   Distance 1 (6): 
play role (6) 
B. Subsequent pattern grammar: 
play ~ role IN(in) DT (707): 
e.g., ?play ~ role in the? (520), ?play ~ role in this? (24), ? 
play ~ role IN(in) VBG (407): 
e.g., ?play ~ role in shaping? (22), ? 
play ~ role IN(in) NN (166): 
e.g., ?play ~ role in society? (7), ?play ~ role in relation? (5), ? 
C. Precedent pattern grammar: 
NN MD play ~ role (83): 
e.g., ?communication will play ~ role ? (2), ? 
JJ NNS play ~ role (69): 
e.g., ?voluntary groups play ~ role? (2), ? 
Type your search query, and push GRASP! 
97
terms of function words as well as content word 
form (e.g., ?play ~ role in V-ing?), as well as 
elastic lexical bundles (e.g., ?play ~ role in 
shaping?). Additionally, we also use semantic 
codes (e.g., PERSON) to provide more information 
in a way similar what is provided in learner 
dictionaries. 
3 The GRASP System 
3.1 Problem Statement 
We focus on constructing a usage summary likely 
to explain the contexts of a given linguistic search. 
The usage summary, consisting of the query?s 
predominant attendant phraseology ranging from 
pattern grammar to lexical phrases, is then returned 
as the output of the system. The returned summary, 
or a set of patterns pivoted with both content and 
function words, can be used for learners? benefits 
directly, or passed on to an error detection and 
correction system (e.g., (Tsao and Wible, 2009) 
and some modules in (Gamon et al, 2009) as rules. 
Therefore, our goal is to return a reasonable-sized 
set of lexical and grammatical patterns 
characterizing the contexts of the query. We now 
formally state the problem that we are addressing. 
Problem Statement: We are given a reference 
corpus C from a wide range of sources and a 
learner search query Q. Our goal is to construct a 
summary of word usages based on C that is likely 
to represent the lexical or grammatical preferences 
on Q?s contexts. For this, we transform the words 
in Q into sets of (word position, sentence record) 
pairs such that the context information, whether 
lexically- or grammatical-oriented, of the querying 
words is likely to be acquired efficiently. 
In the rest of this section, we describe our 
solution to this problem. First, we define a strategy 
for preprocessing our reference corpus (Section 
3.2). Then, we show how GRASP generates 
contextual patterns, comprising the usage summary, 
at run-time (Section 3.3). 
3.2 Corpus Preprocessing 
We attempt to find the word-to-sentence mappings 
and the syntactic counterparts of the L1 sentences 
expected to speed up run-time pattern generation. 
Our preprocessing procedure has two stages. 
Lemmatizing and PoS Tagging. In the first stage, 
we lemmatize each sentence in the reference 
corpus C and generate its most probable POS tag 
sequence. The goal of lemmatization is to reduce 
the impact of morphology on statistical analyses 
while that of POS tagging is to provide a way to 
grammatically describe and generalize the 
contexts/usages of a linguistic query. Actually, 
using POS tags is quite natural: they are often used 
for general description in grammar books, such as 
one?s (i.e., possessive pronoun) in the phrase 
?make up one?s mind?, oneself (i.e., reflexive 
pronoun) in ?enjoy oneself very much?, 
superlative_adjective in ?the most 
superlative_adjective?, NN (i.e., noun) and VB (i.e., 
base form of a verb) in ?insist/suggest/demand that 
NN VB? and so on. 
Constructing Inverted Files. In the second stage, 
we build up inverted files of the lemmas in C for 
quick run-time search. For each lemma, we record 
the sentences and positions in which it occurs. 
Additionally, its corresponding surface word and 
POS tag are kept for run-time pattern grammar 
generation. 
 
Figure 2. Generating pattern grammar and usage 
summary at run-time. 
procedure GRASPusageSummaryBuilding(query,proximity,N,C) 
(1)  queries=queryReformulation(query) 
(2)  GRASPresponses= ?  
for each query in queries 
(3)    interInvList=findInvertedFile(w1 in query) 
for each lemma wi in query except for w1 
(4)      InvList=findInvertedFile(wi) 
//AND operation on interInvList and InvList 
(5a)    newInterInvList= ? ; i=1; j=1 
(5b)    while i<=length(interInvList) and j<=lengh(InvList) 
(5c)       if interInvList[i].SentNo==InvList[j].SentNo 
(5d)         if withinProximity(interInvList[i]. 
wordPosi,InvList[j].wordPosi,proximity) 
(5e)   Insert(newInterInvList, interInvList[i],InvList[j]) 
else if interInvList[i].wordPosi<InvList[j].wordPosi 
(5f)   i++ 
else //interInvList[i].wordPosi>InvList[j].wordPosi 
(5g)   j++ 
else if interInvList[i].SentNo<InvList[j].SentNo 
(5h)          i++ 
else //interInvList[i].SentNo>InvList[j].SentNo 
(5i)           j++ 
(5j)     interInvList=newInterInvList 
//construction of GRASP usage summary for this query 
(6)    Usage= ?  
for each element in interInvList 
(7)       Usage+={PatternGrammarGeneration(query,element,C)} 
(8a)  Sort patterns and their instances in Usage in descending order 
of frequency 
(8b)  GRASPresponse=the N patterns and instances in Usage with 
highest frequency 
(9)    append GRASPresponse to GRASPresponses 
(10) return GRASPresponses 
98
3.3 Run-Time Usage Summary Construction 
Once the word-to-sentence mappings and syntactic 
analyses are obtained, GRASP generates the usage 
summary of a query using the procedure in Figure 
2. 
In Step (1) we reformulate the user query into 
new ones, queries, if necessary. The first type of 
query reformulation concerns the language used in 
query. If it is not in the same language as C, we 
translate query and append the translations to 
queries as if they were submitted by the user. The 
second concerns the length of the query. Since 
single words may be ambiguous in senses and 
contexts or grammar patterns are closely associated 
with words? meanings (Hunston and Francis, 2000), 
we transform single-word queries into their 
collocations, particularly focusing on one word 
sense (Yarowsky, 1995), as stepping stones to 
GRASP patterns. Notice that, in implementation, 
users may be allowed to choose their own 
interested translation or collocation of the query 
for usage learning. The prototypes for first-
language (i.e., Chinese) queries and English 
queries of any length are at A2 and B3 respectively. 
The goal of cross-lingual GRASP is to assist EFL 
users even when they do not know the words of 
their searches and to avoid incorrect queries 
largely because of miscollocation, misapplication, 
and misgeneralization. 
Afterwards, we initialize GRASPresponses to 
collect usage summaries for queries (Step (2)) and 
leverage inverted files to extract and generate each 
query?s syntax-based contexts. In Step (3) we prep 
interInvList for the intersected inverted files of the 
lemmas in query. For each lemma wi within, we 
first obtain its inverted file, InvList (Step (4)) and 
perform an AND operation on interInvList 
(intersected results from previous iteration) and 
InvList (Step (5a) to (5j)4), defined as follows. 
First, we enumerate the inverted lists (Step (5b)) 
after the initialization of their indices i and j and 
temporary resulting intersection newInterInvList 
(Step (5a)). Second, we incorporate a new instance 
of (position, sentence), based on interInvList[i] and 
InvList[j], into newInterInvList (Step (5e)) if the 
sentence records of the indexed list elements are 
the same (Step (5c)) and the distance between their 
                                                           
2
 http://140.114.214.80/theSite/bGRASP_v552/ 
3
 http://140.114.214.80/theSite/GRASP_v552/ 
4
 These steps only hold for sorted inverted files. 
words are within proximity (Step (5d)). Otherwise, 
i and j are moved accordingly. To accommodate 
the contexts of queries? positional variants (e.g., 
?role to play? and ?role ~ play by? for the query 
?play role?), Step (5d) considers the absolute 
distance. Finally, interInvList is set for the next 
AND iteration (Step (5j)). 
Once we obtain the sentences containing query, 
we construct its context summary as below. For 
each element, taking the form ([wordPosi(w1), ?, 
wordPosi(wn)], sentence record) denoting the 
positions of query?s lemmas in the sentence, we 
generate pattern grammar involving replacing 
words in the sentence with POS tags and words in 
wordPosi(wi) with lemmas, and extracting fixed-
window 5  segments surrounding query from the 
transformed sentence. The result is a set of 
grammatical patterns with counts. Their lexical 
realizations also retrieved and displayed. 
The procedure finally generates top N 
predominant syntactic patterns and their N most 
frequent lexical phrases as output (Step (8)). The 
usage summaries GRASP returns are aimed to 
accelerate EFL learners? language understanding 
and learning and lexicographers? word usage 
navigation. To acquire more semantic-oriented 
patterns, we further exploit WordNet and majority 
voting to categorize words, deriving the patterns 
like ?provide PERSON with.? 
4 Experimental Results 
GRASP was designed to generate usage 
summarization of a query for language learning. 
As such, GRASP will be evaluated over CALL. In 
this section, we first present the setting of GRASP 
(Section 4.1) and report the results of different 
consulting systems on language learning in Section 
4.2. 
4.1 Experimental Setting 
We used British National Corpus (BNC) as our 
underlying reference corpus C. It is a British 
English text collection. We exploited GENIA 
tagger to obtain the lemmas and POS tags of C?s 
sentences. After lemmatizing and syntactic 
analyses, all sentences in BNC were used to build 
up inverted files and used as examples for 
grammar pattern extraction. 
                                                           
5
 Inspired by (Gamon and Leacock, 2010). 
99
English (E) sentence with corresponding Chinese (C) translation answer to 1st blank  answer to 2nd blank 
C: ????????????? 
E: Environmental protection has ___ impact ___. 
a profound on the Earth 
C: ????????????? 
E: The real estate agent ___ record profit ___. 
made a on house selling 
C: ?????????????? 
E: They plan to release their new album in ___ future 
the near none 
C: ???????????? 
E: He waited for her for a long time in ___ attempt ___ again. 
an to see her 
 
4.2 Results of Constrained Experiments 
In our experiments, we showed GRASP6  to two 
classes of Chinese EFL (first-year) college students. 
32 and 86 students participated, and were trained 
to use GRASP and instructed to perform a sentence 
translation/composition task, made up of pretest 
and posttest. In (30-minute) pretest, participants 
were to complete 15 English sentences with 
Chinese translations as hints, while, in (20-minute) 
posttest, after spending 20 minutes familiarizing 
word usages of the test candidates from us by 
consulting traditional tools or GRASP, participants 
were also asked to complete the same English 
sentences. We refer to the experiments as 
constrained ones since the test items in pre- and 
post-test are the same except for their order. A 
more sophisticated testing environment, however, 
are to be designed. 
Each test item contains one to two blanks as 
shown in the above table. In the table, the first item 
is supposed to test learners? knowledge on the 
adjective and prepositional collocate of ?have 
impact? while the second test the verb collocate 
make, subsequent preposition on, and preceding 
article a of ?record profit?. On the other hand, the 
third tests the ability to produce the adjective 
enrichment of ?in future?, and the fourth the in-
between article a or possessive his and the 
following infinitive of ?in attempt?. Note that as 
existing collocation reference tools retrieve and 
display collocates, they typically ignore function 
words like articles and determiners, which happen 
to be closely related to frequent errors made by the 
learners (Nicholls, 1999), and fail to provide an 
overall picture of word usages. In contrast, GRASP 
attempts to show the overall picture with 
appropriate function words and word forms. 
We selected 20 collocations and phrases 7 
manually from 100 most frequent collocations in 
                                                           
6
 http://koromiko.cs.nthu.edu.tw/grasp/ 
7
 Include the 15 test items. 
BNC whose MI values exceed 2.2 and used them 
as the target for learning between the pretest and 
posttest. To evaluate GRASP, half of the 
participants were instructed to use GRASP for 
learning and the other half used traditional tools 
such as online dictionaries or machine translation 
systems (i.e., Google Translate and Yahoo! Babel 
Fish). We summarize the performance of our 
participants on pre- and post-test in Table 1 where 
GRASP denotes the experimental group and TRAD 
the control group. 
 
 class 1 class 2 combined 
 pretest posttest  pretest  posttest  pretest posttest 
GRASP 26.4 41.9 43.6 58.4 38.9 53.9 
TRAD 27.1 32.7 43.8 53.4 39.9 48.6 
Table 1. The performance (%) on pre- and post-test. 
 
We observe in Table 1 that (1) the partition of 
the classes was quite random (the difference 
between GRASP and TRAD was insignificant 
under pretest); (2) GRASP summaries of words? 
contexts were more helpful in language learning 
(across class 1, class 2 and combined). Specifically, 
under the column of the 1st class, GRASP helped to 
boost students? achievements by 15.5%, almost 
tripled (15.5 vs. 5.6) compared to the gain using 
TRAD; (3) the effectiveness of GRASP in language 
learning do not confine to students at a certain 
level. Encouragingly, both high- and low-
achieving students benefited from GRASP if we 
think of students in class 2 and those in class 1 as 
the high and the low respectively (due to the 
performance difference on pretests). 
We have analyzed some participants? answers 
and found that GRASP helped to reduce learners? 
article and preposition errors by 28% and 8%, 
comparing to much smaller error reduction rate 7% 
and 2% observed in TRAD group. Additionally, an 
experiment where Chinese EFL students were 
asked to perform the same task but using GRASP 
as well as GRASP with translation information8 
                                                           
8
 http://koromiko.cs.nthu.edu.tw/grasp/ch 
100
was conducted. We observed that with Chinese 
translation there was an additional 5% increase in 
students? test performance. This suggests to some 
extent learners still depend on their first languages 
in learning and first-language information may 
serve as another quick navigation index even when 
English GRASP is presented. 
Overall, we are modest to say that (in the 
constrained experiments) GRASP summarized 
general-to-specific usages, contexts, or phrase-
ologies of words are quite effective in assisting 
learners in collocation and phrase learning. 
5 Applying GRASP to Error Correction 
To demonstrate the viability of GRASP-retrieved 
lexicalized grammar patterns (e.g., ?play ~ role In 
V-ING? and ?look forward to V-ING?) in error 
detection and correction, we incorporate them into 
an extended Levenshtein algorithm (1966) to 
provide broad-coverage sentence-level grammat-
ical edits (involving substitution, deletion, and 
insertion) to inappropriate word usages in learner 
text. 
Previously, a number of interesting rule-based 
error detection/correction systems have been 
proposed for some specific error types such as 
article and preposition error (e.g., (Uria et al, 
2009), (Lee et al, 2009), and some modules in 
(Gamon et al, 2009)). Statistical approaches, 
supervised or unsupervised, to grammar checking 
have become the recent trend. For example, 
unsupervised systems of (Chodorow and Leacock, 
2000) and (Tsao and Wible, 2009) leverage word 
distributions in general and/or word-specific 
corpus for detecting erroneous usages while 
(Hermet et al, 2008) and (Gamon and Leacock, 
2010) use Web as a corpus. On the other hand, 
supervised models, typically treating error 
detection/correction as a classification problem, 
utilize the training of well-formed texts ((De Felice 
and Pulman, 2008) and (Tetreault et al, 2010)), 
learner texts, or both pairwisely (Brockett et al, 
2006). Moreover, (Sun et al, 2007) describes a 
way to construct a supervised error detection 
system trained on well-formed and learner texts 
neither pairwise nor error tagged. 
In contrast to the previous work in grammar 
checking, our pattern grammar rules are 
automatically inferred from a general corpus (as 
described in Section 3) and helpful for correcting 
errors resulting from the others (e.g., ?to close? in 
?play ~ role to close?), our pattern grammar 
lexicalizes on both content and function words and 
lexical items within may be contiguous (e.g., ?look 
forward to V-ING PRP?) or non-contiguous (e.g., 
?play ~ role In V-ING?), and, with word class 
(POS) information, error correction or grammatical 
suggestion is provided at sentence level. 
5.1 Error Correcting Process 
Figure 3 shows how we check grammaticality and 
provide suggestions for a given text with accurate 
spelling. 
 
 
Figure 3. Procedure of grammar suggestion/correction. 
 
In Step (1), we initiate a set Suggestions to 
collect grammar suggestions to the user text T 
according to a bank of patterns 
PatternGrammarBank, i.e., a collection of 
summaries of grammatical usages (e.g., ?play ~ 
role In V-ING?) of queries (e.g., ?play role?) 
submitted to GRASP. Since we focus on grammar 
checking at sentence level, T is heuristically split 
(Step (2)). 
For each sentence, we extract user-proposed 
word usages (Step (3)), that is, the user 
grammatical contexts of ngram and collocation 
sequences. Take for example the (ungrammatical) 
sentences and their corresponding POS sequences 
?he/PRP play/VBP an/DT important/JJ roles/NNS 
to/TO close/VB this/DT deals/NNS? and ?he/PRP 
looks/VBZ forward/RB to/TO hear/VB you/PRP?. 
Ngram contexts include ?he VBP DT?, ?play an JJ 
NNS?, ?this NNS? for the first sentence and ?look 
forward to VB PRP? and ?look forward to hear 
PRP? for the second. And collocation contexts for 
procedure GrammarChecking(T,PatternGrammarBank) 
(1) Suggestions=??//candidate suggestions 
(2) sentences=sentenceSplitting(T) 
for each sentence in sentences 
(3)   userProposedUsages=extractUsage(sentence) 
for each userUsage in userProposedUsages 
(4)     patGram=findPatternGrammar(userUsage.lexemes, 
PatternGrammarBank) 
(5)     minEditedCost=SystemMax; minEditedSug=?? 
for each pattern in patGram 
(6)        cost=extendedLevenshtein(userUsage,pattern) 
if cost<minEditedCost 
(7)            minEditedCost=cost; minEditedSug=pattern 
if minEditedCost>0 
(8)       append (userUsage,minEditedSug) to Suggestions 
(9) Return Suggestions 
101
the first sentence are ?play ~ role to VERB? and 
?close ~ deal .? 
For each userUsage in the sentence (e.g., ?play 
~ role TO VB? and ?look forward to hear PRP?), 
we first acquire the pattern grammar of its lexemes 
(e.g., ?play role? and ?look forward to hear?) such 
as ?play ~ role in V-ing? and ?look forward to 
hear from? in Step (4), and we compare the user-
proposed usage against the corresponding 
predominant, most likely more proper, ones (from 
Step (5) to (7)). We leverage an extended 
Levenshtein?s algorithm in Figure 4 for usage 
comparison, i.e. error detection and correction, 
after setting up minEditedCost and minEditedSug 
for the minimum-cost edit from alleged error usage 
into appropriate one (Step (5)). 
 
 
Figure 4. Extended Levenshtein algorithm for correction. 
 
In Step (1) of the algorithm in Figure 4 we 
allocate and initialize costArray to gather the 
dynamic programming based cost to transform 
userUsage into a specific pattern. Afterwards, the 
algorithm defines the cost of performing 
substitution (Step (2)), deletion (Step (3)) and 
insertion (Step (4)) at i-indexed userUsage and j-
indexed pattern. If the entries userUsage[i] and 
pattern[j] are equal literally (e.g., ?VB? and ?VB?) 
or grammatically (e.g., ?DT? and ?PRP$?9), no edit 
                                                           
9
 ONE?S denotes possessives. 
is needed, hence, no cost (Step (2a)). On the other 
hand, since learners tend to select wrong word 
form and preposition, we make less the cost of the 
substitution of the same word group, say from 
?VERB? to ?V-ing?, ?TO? to ?In? and ?In? to 
?IN(on)? (Step (2b)) compared to a total edit (Step 
(2c)). In addition to the conventional deletion and 
insertion (Step (3b) and (4b) respectively), we look 
ahead to the elements userUsage[i+1] and 
pattern[j+1] considering the fact that ?with or 
without preposition? and ?transitive or intransitive 
verb? often puzzles EFL learners (Step (3a) and 
(4a)). Only a small edit cost is applied if the next 
elements in userUsage and Pattern are ?equal?. In 
Step (6) the extended Levenshtein?s algorithm 
returns the minimum cost to edit userUsage based 
on pattern. 
Once we obtain the costs to transform the 
userUsage into its related frequent patterns, we 
propose the minimum-cost one as its grammatical 
suggestion (Step (8) in Figure 3), if its minimum 
edit cost is greater than zero. Otherwise, the usage 
is considered valid. At last, the gathered 
suggestions Suggestions to T are returned to users 
(Step (9)). Example edits to the user text ?he play 
an important roles to close this deals. he looks 
forward to hear you.? from our working prototype, 
EdIt10, is shown in Figure 5. Note that we exploit 
context checking of collocations to cover longer 
span than ngrams?, and longer ngrams like 
fourgrams and fivegrams to (more or less) help 
semantic checking (or word sense disambiguation). 
For example, ?hear? may be transitive or 
intransitive, but, in the context of ?look forward 
to?, there is strong tendency it is used intransitively 
and follows by ?from?, as EdIt would suggest (see 
Figure 5). 
There are two issues worth mentioning on the 
development of EdIt. First, grammar checkers 
typically have different modules examining 
different types of errors with different priority. In 
our unified framework, we set the priority of 
checking collocations? usages higher than that of 
ngrams?, set the priority of checking longer 
ngrams? usages higher than that of shorter, and we 
do not double check. Alternatively, one may first 
check usages of all sorts and employ majority 
voting to determine the grammaticality of a 
sentence. Second, we further incorporate
                                                           
10
 http://140.114.214.80/theSite/EdIt_demo2/ 
procedure extendedLevenshtein(userUsage,pattern) 
(1) allocate and initialize costArray 
for i in range(len(userUsage)) 
for j in range(len(pattern)) 
//substitution 
if equal(userUsage[i],pattern[j]) 
(2a)       substiCost=costArray[i-1,j-1]+0 
elseif sameWordGroup(userUsage[i],pattern[j]) 
(2b)       substiCost=costArray[i-1,j-1]+0.5 
else 
(2c)       substiCost=costArray[i-1,j-1]+1 
//deletion 
if equal(userUsage[i+1],pattern[j+1]) 
(3a)       delCost=costArray[i-1,j]+smallCost 
else 
(3b)       delCost=costArray[i-1,j]+1 
//insertion 
if equal(userUsage[i+1],pattern[j+1])  
(4a)        insCost=costArray[i,j-1]+smallCost 
else 
(4b)       insCost=costArray[i,j-1]+1 
(5)       costArray[i,j]=min(substiCost,delCost,insCost) 
(6) Return costArray[len(userUsage),len(pattern)] 
102
Erroneous sentence EdIt suggestion ESL Assistant suggestion 
Wrong word form 
? a sunny days ? a sunny NN a sunny day 
every days, I ? every NN every day 
I would said to ? would VB would say 
he play a ? he VBD none 
? should have tell the truth should have VBN should have to tell 
? look forward to see you look forward to VBG none 
? in an attempt to seeing you an attempt to VB none 
? be able to solved this problem able to VB none 
Wrong preposition 
he plays an important role to close ? play ~ role IN(in) none 
he has a vital effect at her. have ~ effect IN(on) effect on her 
it has an effect on reducing ? have ~ effect IN(of) VBG none 
? depend of the scholarship depend IN(on) depend on 
Confusion between intransitive and transitive verb 
he listens the music. missing ?to? after ?listens? missing ?to? after ?listens? 
it affects to his decision. unnecessary ?to? unnecessary ?to? 
I understand about the situation. unnecessary ?about? unnecessary ?about? 
we would like to discuss about this matter. unnecessary ?about? unnecessary ?about? 
Mixture 
she play an important roles to close this deals. she VBD; an JJ NN; 
play ~ role IN(in) VBG; this NN 
play an important role; 
close this deal 
I look forward to hear you. look forward to VBG; 
missing ?from? after ?hear? 
none 
Table 2. Three common score-related error types and their examples with suggestions from EdIt and ESL Assistant. 
 
 
Figure 5. Example EdIt responses to the ungrammatical. 
 
probabilities conditioned on word positions to 
weigh edit costs. For example, the conditional 
probability of ?VERB? being the immediate 
follower of ?look forward to? is virtually zero, but 
the probability of ?V-ing? is around 0.3. 
5.2 Preliminary Results in Error Correction 
We examined three common error types in learner 
text that are highly correlated with essay scores 
(Leacock and Chodorow, 2003; Burstein et al, 
2004), to evaluate EdIt, (see Table 2). In Table 2, 
the results of a state-of-the-art checker, ESL 
Assistant (www.eslassistant.com/), are shown for 
comparison, and information produced by both 
systems are underscored. As indicated, GRASP 
retrieves patterns which are potential useful if 
incorporated into an extension of Levenshtein?s 
algorithm to correct substitution, deletion, and 
insertion errors in learner. 
6 Summary 
We have introduced a new method for producing a 
general-to-specific usage summary of the contexts 
of a linguistic search query aimed at accelerating 
learners? grasp on word usages. We have 
implemented and evaluated the method as applied 
to collocation and phrase learning and grammar 
checking. In the preliminary evaluations we show 
that GRASP is more helpful than traditional 
language learning tools, and that the patterns and 
lexical bundles provided are promising in detecting 
and correcting common types of errors in learner 
writing. 
References 
Morton Benson, Evellyn Benson, and Robert Ilson. 
1986. The BBI Combinatory Dictionary of English: A 
Article: 
Related pattern grammar 
(a) of collocation sequences includes ?play ~ role 
IN(in) NN?, ?play ~ role IN(in) DT?, ?play ~ role 
IN(in) VBG? and so on. 
(b) of ngram sequences includes ?he VBD DT?, ?play 
an JJ NN?, ?this NN?, ?look forward to VBG PRP? 
and ?look forward to hear IN(from) PRP? and so on. 
Grammatical/Usage suggestion: 
For sentence 1: 
(a) use the VBD of ?play?, (b) use the NN of ?roles?, 
(c) use the preposition ?in? and VBG of ?close?, 
instead of ?to close?. (d) use the NN of ?deals? 
For sentence 2: 
(a) insert the preposition ?from? after ?hear?, (b) use 
the ?VBG? of ?hear? 
he play an important roles to close this deals. 
he looks forward to hear you. 
Type your article and push the buttom ?EdIt? ! 
103
guide to word combinations. Philadelphia: John 
Benjamins. 
Chris Brockett, William B. Dolan, and Michael Gamon. 
2006. Correcting ESL errors using phrasal SMT 
techniques. In Proceedings of the ACL, pages 249-
256. 
Jill Burstein, Martin Chodorow, and Claudia Leacock. 
2004. Automated essay evaluation: the criterion 
online writing service. AI Magazine, 25(3): 27-36. 
Yu-Chia Chang, Jason S. Chang, Hao-Jan Chen, and 
Hsien-Chin Liou. 2008. An automatic collocation 
writing assistant for Taiwanese EFL learners: a case 
of corpus-based NLP technology. CALL, 21(3): 283-
299. 
Winnie Cheng, Chris Greaves, and Martin Warren. 2006. 
From n-gram to skipgram to concgram. Corpus 
Linguistics, 11(4): 411-433. 
Martin Chodorow and Claudia Leacock. 2000. An 
unsupervised method for detecting grammatical 
errors. In Proceedings of the NAACL, pages 140-147. 
Rachele De Felice and Stephen G. Pulman. 2008. A 
classifer-based approach to preposition and 
determiner error correction in L2 English. In 
Proceedings of the COLING, pages 169-176. 
Philip Durrant. 2009. Investigating the viability of a 
collocation list for students of English for academic 
purposes. ESP, 28(3): 157-169. 
John R. Firth. 1957. Modes of meaning. In Papers in 
Linguistics. London: Oxford University Press, pages 
190-215. 
Michael Gamon, Claudia Leacock, Chris Brockett, 
William B. Dolan., Jianfeng Gao, Dmitriy Belenko, 
and Alexandre Klementiev. 2009. Using statistical 
techniques and web search to correct ESL errors. 
CALICO, 26(3): 491-511. 
Michael Gamon and Claudia Leacock. 2010. Search 
right and thou shalt find ? using web queries for 
learner error detection. In Proceedings of the NAACL. 
Matthieu Hermet, Alain Desilets, and Stan Szpakowicz. 
2008. Using the web as a linguistic resource to 
automatically correct lexico-syntatic errors. In 
Proceedings of the LREC, pages 874-878. 
Susan Hunston and Gill Francis. 2000. Pattern 
Grammar: A Corpus-Driven Approach to the Lexical 
Grammar of English. Amsterdam: John Benjamins. 
Jia-Yan Jian, Yu-Chia Chang, and Jason S. Chang. 2004. 
TANGO: Bilingual collocational concordancer. In 
ACL Poster. 
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David 
Tugwell. 2004. The sketch engine. In Proceedings of 
the EURALEX, pages 105-116. 
Chong Min Lee, Soojeong Eom, and Markus Dickinson. 
2009. Toward analyzing Korean learner particles. In 
CALICO Workshop. 
Claudia Leacock and Martin Chodorow. 2003. 
Automated grammatical error detection. In M.D. 
Shermis and J.C. Burstein, editors, Automated Essay 
Scoring: A Cross-Disciplinary Perspective, pages 
195-207. 
Vladimir I. Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. Soviet 
Physics Doklady, 10, page 707. 
Diane Nicholls. 1999. The Cambridge Learner Corpus ? 
error coding and analysis for writing dictionaries and 
other books for English Learners. 
John M. Sinclair. 1987. The nature of the evidence. In J. 
Sinclair (ed.) Looking Up. Collins: 150-159. 
Frank Smadja. 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1): 143-177. 
Michael Stubbs. 2004. At 
http://web.archive.org/web/20070828004603/http://www.u
ni-trier.de/uni/fb2/anglistik/Projekte/stubbs/icame-2004.htm. 
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, 
Zhongyang Xiong, John Lee, and Chin-Yew Lin. 
2007. Detecting erroneous sentences using 
automatically mined sequential patterns. In 
Proceedings of the ACL, pages 81-88. 
Joel Tetreault, Jennifer Foster, and Martin Chodorow. 
2010. Using parse features for prepositions selection 
and error detection. In Proceedings of the ACL, pages 
353-358. 
Nai-Lung Tsao and David Wible. 2009. A method for 
unsupervised broad-coverage lexical error detection 
and correction. In NAACL Workshop, pages 51-54. 
Larraitz Uria, Bertol Arrieta, Arantza D. De Ilarraza, 
Montse Maritxalar, and Maite Oronoz. 2009. 
Determiner errors in Basque: analysis and automatic 
detection. Procesamiento del Lenguaje Natural, 
pages 41-48. 
David Wible and Nai-Lung Tsao. 2010. StringNet as a 
computational resource for discovering and 
investigating linguistic constructions. In NAACL 
Workshop, pages 25-31. 
David Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the ACL, pages 189-196. 
104
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 80?85,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
 
 
PREFER: Using a Graph-Based Approach to Generate Paraphrases for 
Language Learning 
  
Mei-Hua Chen*, Shih-Ting Huang+, Chung-Chi Huang*, Hsien-Chin Liou**, Jason S. Chang+ 
  * Institute of Information Systems and Applications 
+ Department of Computer Science 
**  Department of Foreign Languages and Literature 
National Tsing Hua University  
HsinChu, Taiwan, R.O.C. 30013 
{chen.meihua,koromiko1104,u901571,hsienchin,jason.jschang}@gmail.com 
  
  
Abstract 
Paraphrasing is an important aspect of language 
competence; however, EFL learners have long 
had difficulty paraphrasing in their writing 
owing to their limited language proficiency. 
Therefore, automatic paraphrase suggestion 
systems can be useful for writers. In this paper, 
we present PREFER1, a paraphrase reference 
tool for helping language learners improve their 
writing skills. In this paper, we attempt to 
transform the paraphrase generation problem 
into a graphical problem in which the phrases 
are treated as nodes and translation similarities 
as edges. We adopt the PageRank algorithm to 
rank and filter the paraphrases generated by the 
pivot-based paraphrase generation method. We 
manually evaluate the performance of our 
method and assess the effectiveness of 
PREFER in language learning. The results 
show that our method successfully preserves 
both the semantic meaning and syntactic 
structure of the query phrase. Moreover, the 
students? writing performance improve most 
with the assistance of PREFER.  
1. Introduction 
Paraphrasing, or restating information using 
different words, is an essential part of productive 
language competence (Fuchs, 1980; Mel??uk, 1992; 
Martinot, 2003). However, EFL learners have 
difficulty paraphrasing in their writing partly 
                                                 
1 http://140.114.89.231/PREFER 
because of their insufficient lexical knowledge 
(Abasi et al 2006; Chandrasoma et al 2004). If 
they are provided with direct and substantial 
support while writing, they may be able to express 
their thoughts more fluently. Unfortunately, few 
paraphrase reference tools have been developed to 
provide instant assistance to learners in their 
writing process. In the light of the pressing need 
for paraphrase reference tools, we develop 
PREFER, a paraphrasing assistant system to help 
EFL learners vary their expression during writing.  
Over the past decade, paraphrasing techniques 
have played an important role in many areas of 
Natural Language Processing, such as machine 
translation, and question answering. However, very 
few studies have been conducted concerning the 
application of automatic paraphrase generation 
techniques in language learning and teaching.  
In this paper, we treat the paraphrase generation 
problem as a graph-related problem. We adopt the 
PageRank algorithm (Page et al, 1999) to generate 
paraphrases based on the assumption that a page 
with more incoming links is likely to receive a 
higher rank. Meanwhile, a page which is linked by 
a higher ranked page should transitively be ranked 
higher. We take advantage of transitivity of 
relevance to rank and filter the paraphrases 
generated by the pivot-based method (i.e., phrase 
are treated as paraphrases if they share the same 
translations) of Bannard and Callison-Burch 
(2005).  
The advantage of the pivot approach is that the 
generated paraphrases are exactly semantically 
equivalent to the query phrase. However, its 
80
  
quality of the paraphrases highly correlates with 
that of the techniques of bilingual alignment. To 
overcome such limitation, we use the PageRank 
algorithm to refine the generated paraphrases. In 
other words, we leverage the PageRank algorithm 
to find more relevant paraphrases that preserve 
both meaning and grammaticality for language 
learners. The results of a manual evaluation and a 
system assessment show that our approach and 
system perform well. 
2. Related Work 
A number of studies have investigated EFL leaners? 
paraphrase competence. For example, Campbell 
(1987) reveals that language proficiency 
significantly affects paraphrasing competence. 
McInnis (2009) reports that paraphrasing task is 
more difficult for L2 students than that for L1 
students. According to Milicevic (2011), L2 
learners propose less valid paraphrases than native 
speakers. These findings indicate that EFL students 
have problems in paraphrasing. In view of this, we 
develop PREFER, a paraphrase reference tool, for 
helping English learners with their writing. 
Paraphrase generation, on the other hand, has 
been an area of active research and the related 
work has been thoroughly surveyed in 
Androutsopoulos and Malakasiotis (2010) as well 
as in Madnani and Dorr (2010). In the rest of this 
section, we focus on reviewing the methods related 
to our work.  
One prominent approach to paraphrase 
generation is based on bilingual parallel corpora. 
For example, Bannard and Callison-Burch (2005) 
propose the pivot approach to generate phrasal 
paraphrases from an English-German parallel 
corpus. With the advantage of its parallel and 
bilingual natures of such a corpus, the output 
paraphrases do preserve semantic similarity. 
Callison-Burch (2008) further places syntactic 
constraints on generated paraphrases to improve 
the quality of the paraphrases. In this paper, we 
generate paraphrases adopting the pivot-based 
method proposed by Bannard and Callison-Burch 
(2005) in the first round. Then we use a 
graph-based approach to further ensure paraphrase 
candidates preserve both meaning and 
grammaticality. 
In a study more closely related to our work, 
Kok and Brockett (2010) take a graphical view of 
the pivot-based approach. They propose the Hitting 
Time Paraphrase algorithm (HTP) to measure 
similarities between phrases. The smaller the 
number of steps a random walker goes from one 
node to the other, the more likely these two nodes 
are paraphrases. The main difference between their 
work and ours lies in the definition of the graph. 
While they treat multilingual phrases as nodes, we 
treat only English phrases as nodes. Besides, we 
define the edges between nodes as semantic 
relation instead of bilingual alignment. 
In contrast to the previous work, we present a 
graph-based method for refining the paraphrases 
generated by the pivoting approach. Our goal is to 
consolidate the relation between paraphrases to 
provide learners with more and better paraphrases 
which are helpful in expanding their lexical 
knowledge. 
3. Graph-Based Paraphrase Generation 
In this section, we describe how we use the 
PageRank algorithm to rank and filter the 
paraphrases generated by the pivot-based method. 
3.1 Graph Construction 
We first exploit the pivot-based method proposed 
by Bannard and Callison-Burch (2005) to populate 
our graph G using of candidate paraphrases 
cP={             } from a bilingual parallel 
corpus B for a query phrase q. Each phrase in cP is 
also represented as a node in G. Note that the 
query phrase q is excluded from cP.  
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  A simple graph G. Note that the cp1 and 
   
  
 will be linked iff    
  
 is the paraphrase of q 
and is also the paraphrase of      
 
q 
cP 
        
 
   
    
      
81
  
Graph G only contains the paraphrases cpi 
whose probabilities are higher than a certain 
threshold ?2  as nodes. In addition, each cpi is 
linked to the query phrase q with edge e which is 
weighted by the probability  (   | ). Furthermore, 
we establish the edges among the phrases in cP. 
An example graph is shown in Figure 1. By 
repeating the previous steps, for each phrase cp1, 
cp2,... in cP, we find their corresponding 
paraphrases,         
 
  
        and 
         
 
  
       ?., and discard the 
paraphrases that are not in cP. Once the phrases are 
linked with their paraphrases, the graph G is 
created.  
In this paper, we also place a constraint that a 
paraphrase of a phrase q must neither be a 
substring nor a superstring of q. These strings are 
usually aligned with the same foreign language 
phrase while they are not paraphrases at all. For 
example, ?play an important? and ?play an 
important role in? are excluded for ?play an 
important role?. This has the effect of reducing 
some of the noise generated by the pivot-based 
method.   
3.2 Graph-Based Paraphrase Generation 
We then refine the generated paraphrases adopting 
the PageRank algorithm proposed by Page et al 
(1999). Consider a graph consisting of a set of 
webpages on the Web V and a set of hyperlinks E. 
The PageRank algorithm assigns a value PR to 
each webpage as their importance measurement. 
The PR value of a certain page u is defined 
iteratively as the following equation: 
  ( )   ?
  ( )
 ( )
                   ( )
    
 
where Bu is a set of pages linked to u and L(.) 
denotes the number of outbound links from a page 
v.  
Intuitively, by using formula (1) iteratively, we 
are able to calculate the PR values for all nodes 
and thus extract relatively important paraphrases. 
However, the original PageRank algorithm does 
not take the weight of each edge into consideration. 
That is, the PageRank algorithm treats all links 
equally when distributing rank scores. Treating all 
links equally in paraphrase generation task might 
                                                 
2 We set ? to be 0.01. 
lose some linguistic properties. For this, we 
consider the importance of edges of the nodes and 
weight the edges based on the paraphrase 
probability in the pivot-based approach using 
 (      )  ? ( |  ) ( |  )
 
      ( )  
Formula (2) represents the probability that the 
phrase u is the paraphrase of the phrase v. f refers 
to shared translations of v and u. Then for each 
iteration of the PageRank calculation, we reassign 
the PR value for all u in V to be PR?(u) as:  
   ( )   ?
 (   )  ( )
 ( )
           ( )
    
 
Instead of treating all edges equally, formula (3) 
integrates the weights of inbound link and 
outbound link edges (see Section 4 for the 
performance differences with and without 
weighting edges). 
4. Results 
In this section, we first present our experimental 
setting. Then evaluation results are reported. 
4.1 Experimental Setting 
In this paper, word alignments were produced by 
Giza++ toolkit (Och and Ney, 2003) over a set of 
Danish-English section (containing 1,236,427 
sentences) of the Europarl corpus, version 2 
(Koehn, 2002).  
We compared our graph-based approach with a 
strong baseline, the pivot-based method with 
syntactic constraint (SBP) (Callison-Burch, 2008) 
utilizing the same Danish-English corpus. We also 
investigate the contribution of adding the edge 
weights to the PageRank algorithm by building 
two models, PR representing the method of the 
PageRank algorithm without weights and PRw 
representing the method of the weighted PageRank 
algorithm, for comparison.   
To assess the performance of our method, we 
conducted a manual evaluation. We asked an 
experienced English lecturer to randomly select 
100 most commonly used and meaningful phrases 
from 30 research articles in the discipline of 
Computer-Assisted Language Learning (CALL). A 
total of 88 unique phrases were used as our test set 
for evaluation excluding 12 phrases not existing in 
the Europarl corpus. For each phrase, we extracted 
82
  
the corresponding candidate paraphrases and chose 
top 5 for evaluation. Two raters, provided with a 
simplified scoring standard used by Callison-Burch 
(2008), manually evaluate the accuracy of the top 
ranked paraphrases of each phrase by score 0, 1 
and 2. It is worth noting that the raters were asked 
to score each paraphrase candidate by considering 
its appropriateness in various contexts. In this 
evaluation, we strictly deemed a paraphrase to be 
correct if and only if both raters scored 2.  The 
inter-annotator agreement was 0.63.   
The coverage was measured by the number of 
correct answers within top 5 candidates. The 
precision was measured by the number of correct 
answers within the returned answers. 
On the other hand, to assess the effectiveness of 
PREFER in language learning, we carried out an 
experiment with 55 Chinese-speaking EFL college 
freshmen, who had at least six years of formal 
instruction from junior to senior high schools and 
were estimated to be at the intermediate level 
regarding their overall English competence. The 
students were randomly divided into three groups. 
They were asked to paraphrase seven short 
paragraphs in the pre-test with no system support, 
and then paraphrase another seven short 
paragraphs in the post-test using three different 
tools: PREFER (P), LONGMAN Dictionary of 
Contemporary English Online (L), and 
Thesaurus.com (T). A total of 22 default phrases 
(http://140.114.75.22/share/examples.htm) were 
embedded in the paragraphs in the pre- and 
post-tests, targeted at comparing the quality and 
quantity of students? paraphrasing performance. 
Students were not restricted to paraphrase these 
embedded phrases. Instead, they were encouraged 
to replace any possible phrases or even restructure 
sentences. We had two experienced native-speaker 
TESL (Teaching English as a Second Language) 
lecturers to score the students? paraphrasing 
performance. 
4.2 Experimental Results 
4.2.1 Manual Evaluation 
As shown in Table 1, PRw achieved both good 
precision and coverage. Moreover, PR and PRw 
performed better than SBP in both coverage and 
precision. Also, the result that the performance of 
PRw is better than that of PR implies that PRw is 
able to generate more semantically and 
syntactically correct paraphrases. However, the 
precision of 0.19 indicates that there is still room to 
improve the paraphrase generation model.  
 
 
 
  PR PRw SBP 
Coverage 0.17 0.18 0.07 
Precision 0.17 0.19 0.10 
Table 1: The measurement of paraphrases. 
 
Additionally, Mean Reciprocal Rank (MRR) is 
also reported. Here, MRR is defined as a measure 
of how much effort needed to locate the first 
appropriate paraphrase for the given phrase in the 
ranked list of paraphrases. The MRR score of PRw 
(0.53) outperformed PR (0.51) and SBP (0.47). It 
demonstrated that the PRw model facilitates the 
high ranking of good paraphrases (i.e., paraphrases 
with meaning and grammaticality preserved would 
be ranked high).  
4.2.2. Evaluation on Language Learning  
The second evaluation is to assess the effectiveness 
of PREFER applied to CALL. We used a 
comparison method to measure the extent to which 
EFL learners achieved good performance in 
paraphrasing.  
 
Table 2. Comparison of paraphrasing performance 
among students using three different reference tools.  
 
As seen in the first row of Table 2, the students? 
writing performance improved most with the 
assistance of PREFER (i.e., group P), compared 
with group L and group T. We further analyzed 
and compared the number of the rephrased phrases 
and the correct paraphrases, and the rate of 
    P L T 
improvement of paraphrasing 
task 
38.2% -31.6% -6.2% 
all 
paraphrasable 
phrases 
rephrased 38.4% -23.2% 9.5% 
correct 53.3% -17.5% 4.6% 
correctness rate  7.9% 4.9% -3.1% 
22 default 
phrases 
rephrased 68% -16% 28% 
correct 100% -5% 31% 
correctness rate 13.6% 7.9% 1.5% 
83
  
correctness students achieved using different 
reference tools among our testing paraphrase 
candidates (see the middle and bottom panels of 
Table 2). Obviously, the students consulting 
PREFER achieved substantial paraphrasing 
improvement in all three aspects of both all and 
default phrases. But the other two groups seemed 
unable to manage well the paraphrasing task with 
traditional way of phrase information. This limited 
information seems insufficient to enable students 
to familiarize themselves with proper usages of 
phrases which might lead to improper 
paraphrasing. 
In short, PREFER outperformed the other two 
reference tools in assisting EFL learners in their 
paraphrasing task. 
5. Conclusion and Future Work   
In this paper, we treat the paraphrase generation 
problem as a graphical problem. We utilize the 
PageRank algorithm to rank and filter the 
paraphrases generated using the pivot-based 
method. The results show that our method 
significantly produces better paraphrases in both 
precision and coverage compared with the 
syntactically-constrained pivot method of 
Callison-Burch (2008). Additionally, PREFER 
does benefit learners? writing performance. 
 In order to conduct a more comprehensive 
evaluation, we plan to adapt the in-context 
evaluation metric introduced by Callison-Burch et. 
al (2008). A larger test set would be generated 
manually to evaluate the performance of our 
paraphrase system. In addition, we will implement 
various kinds of baseline systems such as Kok and 
Brockett (2010) and Chan et al (2011) to provide a 
more competitive comparison. 
Many avenues exist for future research and 
improvement. For example, we would like to 
extend paraphrasing consecutive n-gram phrases to 
inconsecutive ones such as ones with incomplete 
transitive verbs (e.g., ?provide someone with 
something?). Besides, we are interested in 
weighting edges using syntactic and semantic 
relation in our graph-based method to further 
improve the quality of generated paraphrases. 
 
 
 
References  
Ali R. Abasi, Nahal Akbari, and Barbara Graves. 2006. 
Discourse appropriation, construction of identities, 
and the complex issue of plagiarism: ESL students 
writing in graduate school. Journal of Second 
Language Writing, 15, 102?117. 
Ion Androutsopoulos and Prodromos Malakasiotis. 2010. 
A Survey of Paraphrasing and Textual Entailment 
Methods. Journal of Artificial Intelligence Research 
38, 135?187. 
Colin Bannard and Chris Callison-Burch. 2005. 
Paraphrasing with bilingual parallel corpora. In 
Proceedings of ACL, pp. 597-604. 
Chris Callison-Burch. 2008. Syntactic constraints on 
paraphrases extracted from parallel corpora. In 
Proceedings of EMNLP, pp. 196?205. 
Chris Callison-Burch, Trevor Cohn, and Mirella Lapata. 
2008. ParaMetric: an automatic evaluation metric for 
paraphrasing. In Proceedings of the 22nd 
International Conference on Computational 
Linguistics, pp. 97-104. 
Cherry Campbell. 1990. Writing with others? words: 
Using background reading text in academic 
compositions. In B. Kroll (Ed.), Second language 
writing: Research insights for the classroom (pp. 
211-30). Cambridge, UK: Cambridge University 
Press. 
Tsz Ping Chan, Chris Callison-Burch and Benjamin Van 
Durme. 2011. Reranking Bilingually Extracted 
Paraphrases Using Monolingual Distributional 
Similarity. In Proceedings of the GEMS 2011 
Workshop on GEometrical Models of Natural 
Language Semantics, pp. 33-42.  
Ranamukalage Chandrasoma, Celia Thompson, and 
Alastair Pennycook. 2004. Beyond plagiarism: 
Transgressive and nontransgressive intertextuality. 
Journal of Language, Identity, and Education, 3(3), 
171?194. 
Catherine Fuchs. 1980. Paraphrase et th?orie du 
langage. Contribition ? une histoire des theories 
linguistiques contemporaines et ? la construction 
d?une th?orie ?nonciative de la paraphraase. Th?se 
de doctorat. Paris: Universit? Paris VII. 
Philipp Koehn. 2002. Europarl: A multilingual corpus 
for evaluation of machine translation. Unpublished, 
http://www.isi.edu/~koehn/europarl/. 
Stanley Kok and Chris Brockett. 2010. Hitting the right 
paraphrases in good time. In Proceedings of 
NAACL/HLT, pp. 145-153. 
84
  
Nitin Madnani and Bonnie J. Dorr. 2010. Generating 
phrasal and sentential paraphrases: A survey of 
data-driven methods. Computational Linguistics, 
36(3):341?388. 
Claire Martinot. 2003. Pour une linguistique de 
l?acquisition. La reformulation: du concept descriptif 
au concept explicatif. Langage et Soci?t?, 2(104); 
147-151. 
Lara McInnis. 2009. Analyzing English L1 and L2 
Paraphrasing Strategies through Concurrent Verbal 
Report and Stimulated Recall Protocols. MA Thesis. 
Toronto: University of Toronto. 
Igor Mel??uk. 1992. Paraphrase et lexique: la th?orie 
Sens-Texte et le Dictionnaire explicatif et 
combinatoire. In: Mel??uk, I. et al, Dictionnaire 
explicatif et combinatoire du fran?ais contemporain. 
Recherches lexico-s?mantiques III. Montr?al: Presses 
de l?Universit? de Montr?al; 9-59. 
Jasmina Mili?evi? and Alexandra Tsedryk. 2011. 
Assessing and Improving Paraphrasing Competence 
in FSL. In Proceedings of the 5th International 
Conference on Meaning- Text Theory, pp. 175-184. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1): 19-51. 
Lawrence Page, Sergey Brin, Rajeev Motwani, Terry 
Winograd. 1999. The PageRank Citation Ranking: 
Bringing Order to the Web. Technical Report. pp. 
1999-66, Stanford University InfoLab. 
 
85
