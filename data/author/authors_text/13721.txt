Coling 2010: Poster Volume, pages 605?613,
Beijing, August 2010
Best Topic Word Selection for Topic Labelling
Jey Han Lau,?? David Newman,?? Sarvnaz Karimi? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California
jhlau@csse.unimelb.edu.au, newman@uci.edu, skarimi@unimelb.edu.au, tb@ldwin.net
Abstract
This paper presents the novel task of best
topic word selection, that is the selection
of the topic word that is the best label for
a given topic, as a means of enhancing the
interpretation and visualisation of topic
models. We propose a number of features
intended to capture the best topic word,
and show that, in combination as inputs to
a reranking model, we are able to consis-
tently achieve results above the baseline of
simply selecting the highest-ranked topic
word. This is the case both when training
in-domain over other labelled topics for
that topic model, and cross-domain, us-
ing only labellings from independent topic
models learned over document collections
from different domains and genres.
1 Introduction
In the short time since its inception, topic mod-
elling (Blei et al, 2003) has become a main-
stream technique for tasks as diverse as multi-
document summarisation (Haghighi and Vander-
wende, 2009), word sense discrimination (Brody
and Lapata, 2009), sentiment analysis (Titov and
McDonald, 2008) and information retrieval (Wei
and Croft, 2006). For many of these tasks, the
multinomial topics learned by the topic model can
be interpreted natively as probabilities, or mapped
onto a pre-defined discrete class set. However,
for tasks where the learned topics are provided
to humans as a first-order output, e.g. for use in
document collection analysis/navigation, it can be
difficult for the end-user to interpret the rich sta-
tistical information encoded in the topics. This
research is concerned with making topics more
readily human interpretable, by selecting a single
term with which to label the topic.
Although topics are formally a multinomial dis-
tribution over terms, with every term having finite
probability in every topic, topics are usually dis-
played by printing the top-10 terms (i.e. the 10
most probable terms) in the topic. These top-10
terms typically account for about 30% of the topic
mass for reasonable setting of number of topics,
and usually provide sufficient information to de-
termine the subject area and interpretation of a
topic, and distinguish one topic from another.
Our research task can be illustrated via the top-
10 terms in the following topic, learned from a
book collection. Terms wi are presented in de-
scending order of P (wi|tj) for the topic tj :
trout fish fly fishing water angler stream rod
flies salmon
Clearly the topic relates to fishing, and indeed,
the fourth term fishing is an excellent label for the
topic. The task is thus termed best word or most
representative word selection, as we are selecting
the label from the closed set of the top-N topic
words in that topic.
Naturally, not all topics are equally coherent,
however, and the lower the topic coherence, the
more difficult the label selection task becomes.
For example:
oct sept nov aug dec july sun lite adv globe
appears to conflate months with newspaper
names, and no one of these topic words is able to
capture the topic accurately. As such, our method-
ology presupposes an automatic means of rating
topics for coherence. Fortunately, recent research
by Newman et al (2010) has shown that this is
achievable at levels approaching human perfor-
mance, meaning that this is not an unreasonable
assumption.
Labelling topics has applications across a di-
verse range of tasks. Our original interest in the
605
problem stems from work in document collection
visualisation/navigation, and the realisation that
presenting users with topics natively (e.g. as rep-
resented by the top-N terms) is ineffective, and
would be significantly enhanced if we could au-
tomatically predict succinct labels for each topic.
Another application area where labelling has been
shown to enhance the utility of topic models is se-
lectional preference learning via topic modelling
(Ritter et al, to appear). Here, topic labelling via
taxonomic classes (e.g. WordNet synsets) can lead
to better topic generalisation, in addition to better
human readability.
This paper is based around the assumption that
an appropriate label for a topic can be found
among the high-ranking (high probability) terms
in that topic. We assess the suitability of each term
by way of comparison with other high-ranking
terms in that same topic, using simple pointwise
mutual information and conditional probabilities.
We first experiment with a simple ranking method
based on the component scores, and then move
on to using those scores, along with features from
WordNet and from the original topic model, in a
ranking support vector regression (SVR) frame-
work. Our experiments demonstrate that we are
able to perform the task significantly better than
the baseline of selecting the topic word of high-
est marginal probability, including when training
the ranking model on labelled topics from other
document collections.
2 Related Work
Predictably, there has been significant work on in-
terpreting topics in the context of topic modelling.
Topic are conventionally interpreted via the top-
N words in each topic (Blei et al, 2003; Grif-
fiths and Steyvers, 2004), or alternatively by post-
hoc manual labelling of each topic based on do-
main knowledge and subjective interpretation of
each topic (Wang and McCallum, 2006; Mei et
al., 2006).
Mei et al (2007) proposed various approaches
for automatically suggesting phrasal labels for
topics, based on first extracting phrases from the
document collection, and subsequently ranking
the phrases based on KL divergence with a given
topic.
Magatti et al (2009) proposed a method for la-
belling topics induced by hierarchical topic mod-
elling, based on ontological alignment with the
Google Directory (gDir) hierarchy, and optionally
expanding topics based on a thesaurus or Word-
Net. Preliminary experiments suggest the method
has promise, but the method crucially relies on
both a hierarchical topic model and a pre-existing
ontology, so has limited applicability.
Over the general task of labelling a learned se-
mantic class, Pantel and Ravichandran (2004) pro-
posed the use of lexico-semantic patterns involv-
ing each member of that class to learn a (usu-
ally hypernym) label. The proposed method was
shown to perform well over the semantically ho-
mogeneous, fine-grained clusters learned by CBC
(Pantel and Lin, 2002), but for the coarse-grained,
heterogeneous topics learned by topic modelling,
it is questionable whether it would work as well.
The first works to report on human scoring of
topics were Chang et al (2009) and Newman et
al. (2010). The first study used a novel but syn-
thetic intruder detection task where humans eval-
uate both topics (that had an intruder word), and
assignment of topics to documents (that had an in-
truder topic). The second study had humans di-
rectly score topics learned by a topic model. This
latter work introduced the pointwise mutual infor-
mation (PMI) score to model human scoring. Fol-
lowing this work, we use PMI as features in the
ranking SVR model.
3 Methodology
Our task is to predict which words annotators
tend to select as most representative or best words
when presented with a list of ten words. Since
annotators are not generally unanimous in their
choice of best word, we formulate this as a rank-
ing task, and treat the top-1, 2 and 3 system-
ranked items as the best words, and compare that
to the top-1, 2 and 3 words chosen most frequently
by annotators. In this section, we describe the fea-
tures that may be useful for this ranking task. We
start with features motivated by word association.
An obvious idea is that the most representative
word should be readily evoked by other words
in the topic. For example, given a list of words
?space, earth, moon, nasa, mission?, which is a
606
Space Exploration topic, space could arguably be
the most representative word. This is because
it is natural to think about the word space after
seeing the words earth, moon and nasa individ-
ually. A good candidate for best word could be
the word that has high average conditional proba-
bility given each of the other words. To calculate
conditional probability, we use word counts from
the entire collection of English Wikipedia articles.
Conditional probability is defined as:
P (wi|wj) =
P (wi, wj)
P (wj)
,
where i 6= j and P (wi, wj) is the probability of
observing both wi and wj in the same sliding win-
dow, and P (wi) is the overall probability of word
wi in the corpus. In the above example, evoked by
means that space would fill the slot of wi. The av-
erage conditional probability for word wi is given
by:
avg-CP1(wi) = 19
?
j
P (wi|wj),
for j = 1 . . . 10, j 6= i (this range of indices ap-
plies to all following average quantities).
In other cases, we have the flip situation, where
the most representative word may evoke (rather
than be evoked by) other words in the list of ten
words. Imagine a NASCAR Racing topic, which
has a list of words ?race, car, nascar, driver, rac-
ing?. Given the word nascar, words from the list
such as race, car, racing and driver might come
to mind because nascar is heavily associated with
these words. Therefore, a good candidate, wi,
might also correlate with high P (wj |wi). As be-
fore, the average conditional probability (here de-
noted with CP2) for word wi is given by:
avg-CP2(wi) = 19
?
j
P (wj |wi).
Another approach to measuring word associa-
tion is by calculating pointwise mutual informa-
tion (PMI) between word pairs. Unlike condi-
tional probability, PMI is symmetric and thus the
order of words in a pair does not matter. We
calculate PMI using word counts from English
Wikipedia as follows:
PMI(wi, wj) = log P (wi, wj)P (wi)P (wj) .
The average PMI for word wi is given by:
avg-PMI(wi) = 19
?
j
PMI(wi, wj).
The topic model produces an ordered list of
words for each topic, and the ordering is given by
the marginal probability of each word given that
topic, P (wi|tj). The ranking of words based on
these probabilities indicates the importance of a
word in a topic, and it is also a feature that we use
for predicting the most representative word.
We also observe that sometimes the most repre-
sentative words are generalized concepts of other
words. As such, hypernym relations could be an-
other feature that may be relevant to predicting the
best word. To this end, we use WordNet to find
hypernym relations between pairs of words in a
topic and obtain a set of boolean-valued relation-
ships for each topic word.
Our last feature is the distributional similar-
ity scores of Pantel et al (2009), as trained over
Wikipedia.1 This takes the form of representing
the distributional similarity between each pairing
of terms sim(wi|wj); if wi is not in the top-200
most similar terms for a given wj , we assume it to
have a similarity of 0.
While the above features can be used alone
to get a ranking on the ten topic words, we can
also use various combinations of features in a
reranking model such as support vector regres-
sion (SVMrank: Joachims (2006)). Applying the
features described above ? conditional probabil-
ities, PMI, WordNet hypernym relations, the topic
model word rank, and Pantel?s distributional simi-
larity score ? as features for SVMrank, a ranking
of words is produced and candidates for the most
representative word are selected by choosing the
top-ranked words.
607
NEWS stock market investor fund trading investment firm exchange ...
police gun officer crime shooting death killed street victim ...
food restaurant chef recipe cooking meat meal kitchen eat...
patient doctor medical cancer hospital heart blood surgery ...
BOOKS loom cloth thread warp weaving machine wool cotton yarn ...
god worship religion sacred ancient image temple sun earth ...
crop land wheat corn cattle acre grain farmer manure plough ...
sentence verb noun adjective grammar speech pronoun ...
Figure 1: Selected topics from the two collections
(each line is one topic, with fewer than ten topic
words displayed because of limited space)
4 Datasets
We used two collections of text documents from
different genres for our experiments. The first col-
lection (NEWS) was created by selecting 55,000
news articles from the LDC Gigaword corpus.
The second collection (BOOKS) was 12,000 En-
glish language books selected from the Inter-
net Archive American Libraries collection. The
NEWS and BOOKS collections provide a diverse
range of content for topic modeling. In the first
case ? news articles from the past decade written
by journalists ? each article usually attempts to
clearly and concisely convey information to the
reader, and hence the learned topics tend to be
fairly interpretable. For BOOKS (with publication
dates spanning more than a century), the writing
style often uses lengthy and descriptive prose, so
one sees a different style to the learned topics.
The input to the topic model is a bag-of-words
representation of the collection of text documents,
where word counts are preserved, but word order
is lost. After performing fairly standard tokeniza-
tion and limited lemmatisation, and creating a vo-
cabulary of terms that occurred at least ten times,
each corpus was converted into its bag-of-words
representation. We learned topic models for the
two collections, choosing a setting of T = 200
topics for NEWS and T = 400 topics for BOOKS.
After computing the PMI-score for each topic (ac-
cording to Newman et al (2010)), we selected 60
topics with high PMI-score, and 60 topics with
low PMI-score, from both corpora, resulting in a
total of 240 topics for human evaluation.
The 240 topics selected for human scoring were
1Accessed from http://demo.patrickpantel.
com/Content/LexSem/thesaurus.htm.
Features Description
PMI Pointwise mutual information
CP1 Conditional probability P (wi|?)
CP2 Conditional probability P (?|wi)
TM Rank Original topic model word rank
Hypernym WordNet hypernym relationships
PDS Pantel distributional similarity score
Table 1: Description of feature sets
each evaluated by between 10 and 20 users. For
the two topic models, we used the conventional
approach of displaying each topic with its top-10
terms. In a typical survey, a user was asked to
evaluate anywhere from 60 to 120 topics. The in-
structions asked the user to perform the following
tasks, for each topic in the survey: (a) score the
topic for ?usefulness? or ?coherence? on a scale
of 1 to 3; and (b) select the single best word that
exemplifies the topic (when score=3).
From both NEWS and BOOKS, the 40 topics
with the highest average human scores had rela-
tively complete data for the ?best word? selection
task (i.e. every time a user gave a topics score=3,
they also selected a ?best word?). The remain-
der of this paper is concerned with the 40 NEWS
topics and 40 BOOKS topics where we had ?best
word? data from the annotators. Sample topics
from these two sets are given in Figure 1.
To measure presentational bias (i.e. the extent
to which annotators tend to choose a word seen
earlier rather than later, particularly when armed
with the knowledge that words are presented in or-
der of probability), we reissued a survey using the
40 NEWS topics to ten additional annotators, but
this time the top-10 topic words were presented
in random order. Again, these ten new annotators
were asked to select the best word.
5 Experiments
We used average PMI and conditional probabili-
ties, CP1 and CP2, to rank the ten words in each
topic. Candidates for the best words were selected
by choosing the top-1, 2 and 3 ranked words.
We used the following weighted scoring func-
tion for evaluation:
Best-N score =
?N
i=1 n(wrevi)?N
i=1 n(wi)
608
Features Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
PMI 0.25 0.38 0.49
CP1 0.30 0.42 0.51
CP2 0.15 0.27 0.45
Upper bound 0.48 ? ?
Table 2: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for NEWS
Features Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
PMI 0.25 0.38 0.49
CP1 0.30 0.38 0.47
CP2 0.15 0.30 0.49
Upper bound 0.64 ? ?
Table 3: Best-1,2,3 scores for ranking with single
feature sets (PMI and both conditional probabili-
ties) for BOOKS
where wrevi is the ith term ranked by the system
and wi is the ith most popular term selected by
annotators; revi gives the index of the word wi
in the annotator?s list; and n(w) is the number of
votes given by annotators for word w.
The baseline is obtained using the original word
rank produced by the topic model based on topic
word probabilities P (wi|tj). An upperbound is
calculated by evaluating the decision of an annota-
tor against others for each topic. This upperbound
signifies the maximum accuracy for human anno-
tators on average; since the annotators were asked
to pick a single best word in the survey, only the
Best-1 upperbound can be obtained.
The Best-1/2/3 results are summarized in Ta-
ble 2 for NEWS and Table 3 for BOOKS. These
Best-N scores are computed just using the single
feature of PMI, CP1 and CP2 (each in turn) to rank
the words in each topic. None of these features
alone produces a result that exceeds baseline per-
formance.
To make better use of all the features described
in Section 3, namely the PMI score, conditional
probabilities (both directions), topic model word
rank, WordNet Hypernym relationships and Pan-
tel?s distributional similarity score, we build a
ranking classifier using SVMrank and evaluating
Feature Set Best-1 Best-2 Best-3
Baseline 0.35 0.50 0.59
All Features 0.43 0.56 0.62
?PMI 0.45 (+0.02) 0.52 (?0.04) 0.62 (?0.00)
?CP1 0.35 (?0.08) 0.49 (?0.07) 0.57 (?0.05)
?CP2 0.40 (?0.03) 0.50 (?0.06) 0.61 (?0.01)
?TM Rank 0.40 (?0.03) 0.52 (?0.04) 0.57 (?0.05)
?Hypernym 0.43 (?0.00) 0.57 (+0.01) 0.62 (?0.00)
?PDS 0.43 (?0.00) 0.53 (?0.03) 0.62 (?0.00)
Upper bound 0.48 ? ?
Table 4: SVR-based best topic word results for
NEWS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
Feature Set Best-1 Best-2 Best-3
Baseline 0.38 0.48 0.60
All Features 0.40 0.51 0.62
?PMI 0.38 (?0.02) 0.51 (?0.00) 0.63 (+0.01)
?CP1 0.33 (?0.07) 0.47 (?0.04) 0.56 (?0.06)
?CP2 0.40 (?0.00) 0.50 (?0.01) 0.64 (+0.02)
?TM Rank 0.35 (?0.05) 0.49 (?0.02) 0.63 (+0.01)
?Hypernym 0.40 (?0.00) 0.50 (?0.01) 0.61 (?0.01)
?PDS 0.45 (+0.05) 0.48 (?0.03) 0.67 (+0.05)
Upper bound 0.64 ? ?
Table 5: SVR-based best topic word results for
BOOKS for all six feature types, and feature abla-
tion over each (numbers in brackets show the rel-
ative change over the full feature set)
using 10-fold cross validation. Our first approach
is to use the entire set of features to train the clas-
sifier. Following this, we also measure the effect
of each feature by ablating (removing) one fea-
ture at a time. The drop in Best-N score indicates
which features are the strongest predictors of the
best words (a larger drop in score indicates that
feature is more important). The results for Best-1,
Best-2 and Best-3 scores are summarized in Ta-
ble 4 for NEWS, and Table 5 for BOOKS (averaged
across the 10 iterations of cross validation).
We then produced a condensed set of features,
consisting of the conditional probabilities, the
original topic model word rank and the WordNet
hypernym relationships. This ?best? set of fea-
tures is used to make predictions of best words.
Results are improved in most cases, and are sum-
marized in Table 6 for both NEWS and BOOKS.
609
Dataset Best-1 Best-2 Best-3
NEWS
Baseline 0.35 0.50 0.59
Best Feat. Set 0.45 0.50 0.65
Upper bound 0.48 ? ?
BOOKS
Baseline 0.38 0.48 0.60
Best Feat. Set 0.48 0.56 0.66
Upper bound 0.64 ? ?
Table 6: Results with the best feature set com-
pared to the baseline
Dataset Best-1 Best-2 Best-3
NEWS baseline 0.35 0.50 0.59
BOOKS ? NEWS 0.38 0.56 0.62
NEWS upper bound 0.48 ? ?
BOOKS baseline 0.38 0.48 0.60
NEWS ? BOOKS 0.48 0.56 0.65
BOOKS upper bound 0.64 ? ?
Table 7: Results for cross-domain learning
We also tested whether the SVM classifier
could be trained using data from one domain, and
run on data from another domain. Using our two
datasets as these different domains, we trained a
model using BOOKS data and made predictions
for NEWS, and then we trained a model using
NEWS data and made predictions for BOOKS.
The results, shown in Table 7, indicate that
we are still able to outperform the baseline, even
when the ranking classifier is trained on a differ-
ent domain. In fact, when we trained a model
using NEWS, we saw almost no drop in perfor-
mance for predicting best words for BOOKS, and
improvement is seen for Best-2 score from NEWS.
This implies that the SVM classifier generalizes
well across domains and suggests the possibility
of having a fixed training model to predict best
words for any data.
In these experiments, topic words are presented
in the original order that the topic model produces,
i.e. in descending order of probability of a word
under a topic P (wi|tj). We noticed that the first
words of the topics are frequently selected as the
best words by annotators, and suspected that this
was introducing a bias towards the first word. As
our baseline scores are derived from this topic
word ordering, such a bias could give rise to an
artificially high baseline.
To investigate this effect, we ran a second anno-
Word Order Best-1 Best-2 Best-3
Original 0.35 0.50 0.59
Randomized 0.23 0.33 0.46
Table 8: Reduction of baseline scores for NEWS
when words are presented in random order to an-
notators.
2 4 6 8 10
0.
0
0.
1
0.
2
0.
3
0.
4
Rank
Fr
ac
tio
n 
of
 h
um
an
 s
el
ec
te
d 
be
st
 w
or
d
ordered
random
Figure 2: Bias for humans selecting the best word,
when the topic words are presented in their origi-
nal ordering (ordered) or randomised (random)
tation exercise over the same set of topics (but dif-
ferent annotators), to obtain a new set of best word
annotations for NEWS, with the topic words pre-
sented in random order. In Figure 2, we plot the
cumulative proportion of words selected as best
word by the annotators across the topics, in the
case of the random topic word order, mapping the
topic words back onto their original ranks in the
topic model. A slight drop can be observed in the
proportion of first- and second-ranked topic words
being selected when we randomise the topic word
order. When we recalculate the baseline accuracy
for NEWS on the basis of the new set of annota-
tions, we observe an appreciable drop in the scores
(see Table 8).
6 Discussion
From the experiments in Section 5, perhaps the
first thing to observe is: (a) the high performance
of the baseline, and (b) the relatively low (Best-
1) upper bound accuracy for the task. The first is
perhaps unsurprising, given that it represents the
610
topic model?s own interpretation of the word(s)
which are most representative of that topic. In this
sense, we have set our sights high in attempting to
better the baseline. The upper bound accuracy is
a reflection of both the inter-annotator agreement,
and the best that we can meaningfully expect to
do for the task. That is, any result higher than this
would paradoxically suggest that we are able to do
better at a task than humans, where we are evalu-
ating ourselves relative to the labellings of those
humans. The upper bound for NEWS was slightly
less than 0.5, indicating that humans agree on the
best topic word only 50% of the time. To better
understand what is happening here, consider the
following topic from Figure 1:
health drug patient medical doctor hospital
care cancer treatment disease
This is clearly a coherent topic, but at least two
topic words suggest themselves as labels: health
and medical. By way of having between 10 and 20
annotators (uniquely) label a given topic, and in-
terpreting the multiple labellings probabilistically,
we are side-stepping the inter-annotator agree-
ment issue, but ultimately, for the Best-1 evalu-
ation, we are forced to select one term only, and
consider any alternative to be wrong. Because an-
notators selected only one best topic word, we un-
fortunately have no way of performing Best-2 or
Best-3 upper bound evaluation and deal with top-
ics such as this, but would expect the numbers to
rise appreciably.
Looking at the original feature rankings in Ta-
bles 2 and 3, no clear picture emerges as to which
of the three methods (PMI, CP1 and CP2) was
most successful, but there were certainly clear dif-
ferences in the relative numbers for each, point-
ing to possible complementarity in the scoring.
This expectation was born out in the results for
the reranking model in Tables 4 and 5, where the
combined feature set surpassed the baseline in all
cases, and feature ablation tended to lead to a drop
in results, with the single most effective feature set
being CP1 (P (wi|?)), followed by CP2 (P (?|wi))
and topic model rank. The lexical semantic fea-
tures of WordNet hypernymy and PDS (Pantel?s
distributional similarity) were the worst perform-
ers, often having no or negative impact on the re-
sults.
Comparing the best results for the SVR-based
reranking model and the upper bound Best-1
score, we approach the upper bound performance
for NEWS, but are still quite a way off with
BOOKS when training in-domain. This is encour-
aging, but a slightly artificial result in terms of the
broader applicability of this research, as what it
means in practical terms is that if we can access
multi-annotator best word labelling for the ma-
jority of topics in a given topic model, we can
use those annotations to predict the best word for
the remainder of the topics with reasonably suc-
cess. When we look to the cross-domain results,
however, we see that we almost perfectly replicate
the best-achieved Best-1, Best-2 and Best-3 in-
domain results for BOOKS by training on NEWS
(making no use of the annotations for BOOKS).
Applying the annotations for BOOKS to NEWS is
less successful in terms of Best-1 accuracy, but we
actually achieve higher Best-2, and largely mir-
ror the Best-3 results as compared to the best of
the in-domain results in Table 6. This leads to
the much more compelling conclusion that we can
take annotations from an independent topic model
(based on a completely unrelated document col-
lection), and apply them to successfully model the
best topic word for a new topic model, without
requiring any additional annotation. As we now
have two sets of topics multiply-annotated for best
words, this result suggests that we can perform the
best topic word selection task with high success
over novel topic models.
We carried out manual analysis of topics where
the model did particularly poorly, to get a sense
for how and where our model is being led astray.
One such example is the topic:
race car nascar driver racing cup winston team
gordon season
where the following topic words were selected by
our annotators: nascar (8 people), race (2 peo-
ple), and racing (2 people). First, we observe the
split between race and racing, where more judi-
cious lemmatisation/stemming would make both
the annotation easier and the evaluation cleaner.
The SVR model tends to select more common,
general terms, so in this case chose race as the
best word, and ranked nascar third. This is one
611
instance were nascar evokes all of the other words
effectively, but not conversely (racing is asso-
ciated with many events/sports beyond nascar,
e.g.).
Another topic where our model had difficulty
was:
window nave aisle transept chapel tower arch
pointed arches roof
where our best model selected nave, while the hu-
man annotators selected chapel (6 people), arch
(2 people), nave, roof , tower and transept (1 per-
son each). Clearly, our annotators struggled to
come up with a best word here, despite the topic
again being coherent. This is an obvious candi-
date for labelling with a hypernym/holonym of
the topic words (e.g. church or church architec-
ture), and points to the limitations of best word la-
belling ? there are certainly many topics where
best word labelling works, as our upper bound
analysis demonstrated, but there are equally many
topics where the most natural label is not found
in the top-ranked topic words. While this points
to slight naivety in the current task set up ? we
are forcing annotators to label words with topic
words, where we know that this is sub-optimal
for a significant number of topics ? we contend
that our numbers suggest that: (a) consistent best
topic word labelling is possible at least 50% of
the time; and (b) we have developed a method
which is highly adept at labelling these topics. As
a way forward, we intend to relax the constraint
on the topic label needing to be based on a topic
word, and explore the possibility of predicting
which topics are best labelled with topic words,
and which require independent labels. For topics
which can be labelled with topic words, we can
use the methodology developed here, and for top-
ics where this is predicted to be sub-optimal, we
intend to build on the work of Mei et al (2007),
Pantel and Ravichandran (2004) and others in se-
lecting phrasal/hypernym labels for topics. We are
also interested in applying the methodology pro-
posed herein to the closely-related task of intruder
word, or worst topic word, detection, as proposed
by Chang et al (2009).
Finally, looking to the question of the impact of
the presentation order of the topic words on best
word selection, it would appear that our baseline
is possibly an over-estimate (based on Table 8).
Having said that, the flipside of the bias is that it
leads to more consistency in the annotations, and
tends to help in tie-breaking of examples such as
race and racing from above, for example. In sup-
port of this claim, the upper bound Best-1 accu-
racy of the randomised annotations, relative to the
original gold-standard is 0.44, slightly below the
original upper bound for NEWS. More work is
needed to determine the real impact of this bias
on the overall task setup and evaluation.
7 Conclusion
This paper has presented the novel task of best
topic word selection, that is the selection of the
topic word that is the best label for a given topic.
We proposed a number of features intended to
capture the best topic word, and demonstrated
that, while they were relatively unsuccessful in
isolation, in combination as inputs to a rerank-
ing model, we were able to consistently achieve
results above the baseline of simply selecting the
highest-ranked topic word, both when training in-
domain over other labelled topics for that topic
model, and cross-domain, using only labellings
from independent topic models learned over docu-
ment collections from different domains and gen-
res.
Acknowledgements
NICTA is funded by the Australian government as repre-
sented by Department of Broadband, Communication and
Digital Economy, and the Australian Research Council
through the ICT centre of Excellence programme. DN has
also been supported by a grant from the Institute of Museum
and Library Services, and a Google Research Award.
References
Blei, D.M., A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Brody, S. and M. Lapata. 2009. Bayesian word sense
induction. In Proceedings of the 12th Conference
of the EACL (EACL 2009), pages 103?111, Athens,
Greece.
Chang, J., J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Proceedings of the 23rd
612
Annual Conference on Neural Information Process-
ing Systems (NIPS 2009), pages 288?296, Vancou-
ver, Canada.
Griffiths, T. and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy
of Sciences, 101:5228?5235.
Haghighi, A. and L. Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics ?
Human Language Technologies 2009 (NAACL HLT
2009), pages 362?370, Boulder, USA.
Joachims, T. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
pages 217?226, Philadelphia, USA.
Magatti, D., S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In Proceedings of the
International Conference on Intelligent Systems De-
sign and Applications, pages 1227?1232, Pisa, Italy.
Mei, Q., C. Liu, H. Su, and C. Zhai. 2006. A prob-
abilistic approach to spatiotemporal theme pattern
mining on weblogs. In Proceedings of the 15th
International World Wide Web Conference (WWW
2006), pages 533?542.
Mei, Q., X. Shen, and C. Zhai. 2007. Automatic la-
beling of multinomial topic models. In Proceedings
of the 13th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2007), pages 490?499, San Jose, USA.
Newman, D., J.H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL HLT 2010), pages 100?108, Los
Angeles, USA.
Pantel, P. and D. Lin. 2002. Discovering word
senses from text. In Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 613?619, Ed-
monton, Canada.
Pantel, P. and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
the 4th International Conference on Human Lan-
guage Technology Research and 5th Annual Meet-
ing of the NAACL (HLT-NAACL 2004), pages 321?
328, Boston, USA.
Pantel, P., E. Crestan, A. Borkovsky, A-M. Popescu,
and V. Vyas. 2009. Web-scale distributional sim-
ilarity and entity set expansion. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2009), pages
938?947, Singapore.
Ritter, A, Mausam, and O Etzioni. to appear. A la-
tent Dirichlet alocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meeting
of the ACL (ACL 2010), Uppsala, Sweden.
Titov, I. and R. McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th International World Wide Web
Conference (WWW 2008), pages 111?120, Beijing,
China.
Wang, X. and A. McCallum. 2006. Topics over time:
A non-Markov continuous-time model of topical
trends. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD 2006), pages 424?433,
Philadelphia, USA.
Wei, S. and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of 29th
International ACM-SIGIR Conference on Research
and Development in Information Retrieval (SIGIR
2006), pages 178?185, Seattle, USA.
613
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1624?1635, Dublin, Ireland, August 23-29 2014.
Novel Word-sense Identification
Paul Cook
?
, Jey Han Lau
?
, Diana McCarthy
?
and Timothy Baldwin
?
? Department of Computing and Information Systems, The University of Melbourne
? Department of Philosophy, King?s College London
? University of Cambridge
paulcook@unimelb.edu.au, jeyhan.lau@gmail.com,
diana@dianamccarthy.co.uk, tb@ldwin.net
Abstract
Automatic lexical acquisition has been an active area of research in computational linguistics
for over two decades, but the automatic identification of new word-senses has received attention
only very recently. Previous work on this topic has been limited by the availability of appropriate
evaluation resources. In this paper we present the largest corpus-based dataset of diachronic
sense differences to date, which we believe will encourage further work in this area. We then
describe several extensions to a state-of-the-art topic modelling approach for identifying new
word-senses. This adapted method shows superior performance on our dataset of two different
corpus pairs to that of the original method for both: (a) types having taken on a novel sense over
time; and (b) the token instances of such novel senses.
1 Novel word-senses
The meanings of words change over time with, in particular, established words taking on new senses. For
example, the usages of drop, wall, and blow up in the following sentences correspond to relatively-recent
senses of these words that appear to be quite common in text related to popular culture, but are not listed
in many dictionaries; for example, they are all missing from WordNet 3.0 (Fellbaum, 1998).
1. The reissue album drops March 27 and is an extension of Perry?s huge 2010 Teenage Dream. [drops
= ?comes out?, ?is released? ]
2. On Facebook, you can plainly see much of the data the site has on you, because it?s posted to your
wall. [wall = ?Facebook wall?, ?personal electronic noticeboard? ]
3. Why would I give him my number so he can blow up my phone the way he does my inbox. [blow up
= ?overwhelm with messages? ]
Computational lexicons are an essential component of systems for a variety of natural language process-
ing (NLP) tasks. The success of such systems, therefore, depends on the quality of the lexicons they use,
and (semi-)automatic techniques for identifying new word-senses could benefit applied NLP by helping
to keep lexicons up-to-date. In revising dictionaries, lexicographers must identify new word-senses, in
addition to new words themselves; methods which identify new word-senses could therefore also help to
keep dictionaries current.
In this paper, because of the need for lexicon maintenance, we focus on relatively-new word-senses.
Specifically, we consider the identification of word-senses that are not attested in a reference corpus,
taken to represent standard usage, but that are attested in a focus corpus of newer texts.
Lau et al. (2012) introduced the task of novel sense identification. They presented a method for
identifying novel word-senses ? described here in Section 4 ? and evaluated this method on a very
small dataset consisting of just five lemmas having a novel sense in a single corpus pair. Cook et al.
(2013) extended the method of Lau et al. to incorporate knowledge of the expected domains of new word-
senses, but did not conduct a rigorous empirical evaluation. The remainder of this paper is structured
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1624
as follows. After discussing related work in Section 2, we present a substantially-expanded evaluation
dataset in Section 3, that is based on a second corpus pair and consists of many more lemmas with a
novel sense. We describe the models used by Lau et al. and Cook et al., and our new extensions to
them, in Section 4. In Section 5 we analyse the results of novel sense identification, and consider a new
baseline for this task. We demonstrate that the extended methods give an improvement over the original
method of Lau et al. We conclude by discussing some previously-unexplored variations on novel sense
identification, and limitations of the approaches considered.
The primary contributions of this paper are: (1) development of a novel sense detection dataset much
larger than has been used in research to date; (2) development and evaluation of a new baseline for
novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only
the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4)
extension of the novel sense detection method of Cook et al. to automatically acquire information about
the expected domain(s) of novel senses.
2 Related work
Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently
in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to
identify specific types of semantic change ? widening and narrowing, and amelioration and pejoration,
respectively ? based on specific properties of these phenomena. Gulordava and Baroni (2011) identify
diachronic sense change in an n-gram database, but using a model that is not restricted to any particular
type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for
identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able
to identify words that have undergone a change in meaning, but not the token instances which give rise
to these sense differences.
Bamman and Crane (2011) use a parallel Latin?English corpus to induce word senses and build a
WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al.
(2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based
approaches there is a clear connection between (induced) word-senses and tokens, making it possible to
identify usages of a specific (new) sense.
Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010)
consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either
marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant word-
senses in corpora, including differences between domains. However, this approach does not identify new
senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domain-
specific parallel corpus with novel translations.
The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel word-
senses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a
natural account of polysemy and not only identifies word types that have a novel sense, but identifies
the token instances of the hypothesized novel senses, without reliance on parallel text or a pre-existing
sense inventory. We therefore adopt this method for evaluation on our new dataset, and propose further
extensions to this method.
3 Datasets
Evaluating approaches to identifying semantic change is a challenge due to the lack of appropriate evalu-
ation resources (i.e., corpora for the appropriate time periods, known to exhibit particular sense changes);
indeed, most previous approaches have used very small datasets (e.g., Sagi et al., 2009; Cook and Steven-
son, 2010; Bamman and Crane, 2011). In this study we consider two datasets of relatively newly-coined
word-senses: (1) an extended version of the dataset based on the BNC (Burnard, 2000) and ukWaC (Fer-
raresi et al., 2008) used by Lau et al. (2012); and (2) a new dataset based on the SiBol/Port Corpus.
1
This
1
http://www3.lingue.unibo.it/blog/clb/?page_id=8
1625
is the largest dataset for evaluating approaches to identifying diachronic semantic change constructed
from corpus evidence to be presented to date.
3.1 BNC?ukWaC
Lau et al. (2012) take the written portion of the BNC (approximately 87 million words of British English
from the late 20th century) as the reference corpus, and a similarly-sized random sample of documents
from the ukWaC (a Web corpus built from the .uk domain in 2007) as the focus corpus. They used
TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora.
A set of words that has acquired a new sense between the late 20th and early 21st centuries ? the time
periods of the reference and focus corpora ? is required. The Concise Oxford English Dictionary aims
to document contemporary usage, and has been published in numerous editions including Thompson
(1995, COD95) and Soanes and Stevenson (2005, COD08), enabling the identification of new senses
amongst the entries in COD08 relative to COD95. Manually searching these dictionaries for new senses
would be time intensive, but new words often correspond to concepts that are culturally salient (Ayto,
2006), and one can leverage this observation to speed up the process of finding some candidate words
with novel senses.
2
Between the time periods of the reference and focus corpora, computers and the Internet have become
much more mainstream in society. Lau et al. therefore extracted all headwords in COD08 whose entries
contain the word computing. They then carefully annotated these lemmas to identify those that indeed
exhibit the novel sense indicated in the dictionary in the corpora. Here, we expand Lau et al.?s dataset by
extracting all headwords including any of the following words code, computer, internet, network, online,
program, web, and website. We then follow a similar annotation process to Lau et al.
An annotator read the entries for the selected lexical items in COD95 and COD08, and identified those
which have a clear sense related to computers or the Internet in COD08 that is not present in COD95;
such senses are referred to as novel senses. This process, along with all the annotation in this section
(including Section 3.2), is carried out by native English-speaking authors of this paper and graduate
students in computational linguistics.
To ensure that the words identified from the dictionaries do in fact have a new sense in the ukWaC
sample compared to the BNC, we examine word sketches (Kilgarriff et al., 2004)
3
for each of these
lemmas in the BNC and ukWaC for collocates that likely correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel sense in the BNC, or fail to find evidence of the novel
sense in the ukWaC sample.
4
We further examine the usage of these words in the corpora. We extract a random sample of 100
usages of each lemma from the BNC and ukWaC sample, and annotate these usages as to whether they
correspond to the novel sense or not. This binary distinction is easier than fine-grained sense annotation,
and since we do not use these annotations for formal evaluation ? only for selecting items for our dataset
? we do not carry out an inter-annotator agreement study here. We eliminate any lemma for which we
find evidence of the novel sense in the usages from the BNC, or for which we do not find evidence of the
novel sense in the ukWaC sample usages.
5
This process resulted in the identification of two lemmas not in the dataset of Lau et al., with frequency
greater than 1000 in the ukWaC sample, and having a novel sense in the ukWaC compared to the BNC
(feed (n) and visit (v)). Combining these new lemmas with the dataset of Lau et al. gives an expanded
dataset consisting of seven lemmas. For both of the two new lemmas, a second annotator annotated
the sample of 100 usages from the ukWaC. The observed agreement and unweighted Kappa for this
annotation task for all seven lemmas is 97.4% and 0.93, respectively, indicating that this is indeed a
relatively easy annotation task. The annotators discussed the small number of disagreements to reach
2
We access the dictionaries in the same way as Lau et al., namely we use COD08 online via http://oxfordreference.
com, and the paper version of COD95.
3
http://www.sketchengine.co.uk/
4
We examine word sketches for the full ukWaC because this version of the corpus is available through the Sketch Engine.
5
We use the IMS Open Corpus Workbench (http://cwb.sourceforge.net/) to extract usages of our target lemmas
from the corpora. This extraction process fails in a number of cases, and so we also eliminate such items from our dataset.
1626
BNC?ukWaC
Lemma Frequency Novel sense definition
domain (n) 41 Internet domain
export (v) 28 export data
feed (n) 23 data feed
mirror (n) 10 mirror website
poster (n) 4 one who posts online
visit (v) 28 access a website
worm (n) 30 malicious program
SiBol/Port
Lemma Frequency Novel sense definition
cloud (n) 9 Internet-based computational resources
drag (v) 1 move on a computer screen using a mouse
follower (n) 34 Twitter follower
help (n) 1 displayed instructions, e.g., help menu
hit (n) 2 search hit
platform (n) 22 computing platform
poster (n) 5 one who posts online
reader (n) 3 e-reader
rip (v) 1 copy music
site (n) 39 website
text (n) 39 text message
visit (v) 7 access a website
wall (n) 2 Facebook wall
Table 1: Lemmas in the BNC?ukWaC and SiBol/Port datasets. For each lemma, the frequency of its
novel sense in the annotated sample of usages from the focus corpus, and a definition of its novel sense,
are shown.
consensus. The seven lemmas in this dataset are shown in Table 1, along with definitions of their novel
senses, and the frequencies of their novel senses in the focus corpus.
Lau et al. compared the novelty of the lemmas with a novel sense to that of a same-size set of distractor
lemmas not having a novel sense. Here we consider a much larger set of 50 distractors ? 25 nouns and
25 verbs ? randomly sampled from a similar frequency range as the items with a novel sense.
One shortcoming of this dataset (and indeed the subset of it used by Lau et al.) is that text types are
represented to different extents in the BNC and ukWaC, with, for example, texts related to the Internet
being much more common in the ukWaC. Such differences in corpus composition are a noted challenge
for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010). In the
following subsection we therefore consider the creation of a new dataset from more-comparable corpora.
3.2 SiBol/Port
The SiBol/Port Corpus consists of texts from several British newspapers for the years 1993, 2005, and
2010; we use the 1993 and 2010 portions of this corpus ? referred to as SP1993 and SP2010 ? as
our reference and focus corpora, respectively. SP1993 and SP2010 contain approximately 93M and 99M
words, respectively. In contrast to BNC?ukWaC, our reference and focus corpora are now comparable,
in that they both consist of texts from British newspapers but they differ with respect to the specific year.
The novel word-senses in the BNC?ukWaC dataset are all related to computers and the Internet, but
there has been recent lexical semantic change unrelated to technology as well (e.g., sick can be used to
mean ?excellent?). In an effort to include such non-technical novel senses in this new dataset, we obtain
a list of headwords for which a sense was added to the Macmillan English Dictionary for Advanced
1627
Learners (MEDAL)
6
since its first edition (Rundell and Fox, 2002), courtesy of Macmillan Dictionaries.
Beginning with these candidates from MEDAL, and the items extracted from COD from Section 3.1, we
discard any lemma whose frequency is less than 1000 in SP1993 or SP2010.
As for the BNC?ukWaC dataset, an annotator examined word sketches for these lemmas. However, it
is possible that the novel sense for a lemma is present in a corpus, but that we fail to find evidence for it in
that lemma?s word sketch. We therefore also obtain judgements from two annotators as to whether each
novel sense is expected to be very infrequent (or unattested) in SP2010. To reduce subsequent annotation
effort, we discard any lemma for which its novel sense is believed to be infrequent in SP2010 by both
judges, and is not found in the word sketch from SP2010.
Annotators then annotate a random sample of 100 usages of each lemma in the reference and focus
corpora as before, and again eliminate any lemma for which we find evidence of its novel sense in the
reference corpus, or fail to find evidence of that sense in the focus corpus. We identify thirteen lemmas
having a novel sense in SP2010 relative to SP1993. These lemmas are also shown in Table 1.
We obtain a second set of annotations for the usages of these lemmas in the sample from SP2010,
with each lemma being annotated by a different annotator than before. The observed agreement and
unweighted Kappa between the two sets of annotations is 96.2% and 0.81, respectively. In cases of
disagreement, a final annotation is again reached through discussion.
We randomly select 164 lemmas (116 nouns and 48 verbs) from a similar frequency range as the
lemmas having a novel sense, to serve as distractors.
Both the BNC?ukWaC and SiBol/Port datasets have been made available.
7
4 The WSI-based approach to novel word-sense detection
In this section we describe the WSI-based method of Lau et al. (2012) for detecting novel senses, and an
extension of this method from Cook et al. (2013). We then present new extensions of this method.
The Lau et al. (2012) WSI model is based on a Hierarchical Dirichlet Process (HDP, Teh et al., 2006),
which is a non-parametric variant of a topic model that, like the commonly-used Latent Dirichlet Allo-
cation (LDA, Blei et al., 2003), learns topics (in the form of multinomial probability distributions over
words) and per-document topic assignments (in the form of multinomial probability distributions over
topics) for a collection of documents; unlike LDA, however, it also optimises the number of topics in an
unsupervised data-driven manner. In the context of WSI, by creating ?documents? that consist of sen-
tences containing a target word, we can view the topics learnt by topic models as the sense representation
of the target word. Indeed, topic models have been previously applied to WSI (e.g., Brody and Lapata,
2009; Yao and Van Durme, 2011).
To generate the input for the topic model, the documents are tokenised (in this case, a ?document? is
a short context, typically 1?3 sentences, containing a target word) into a bag of words. All words are
lemmatised, and stopwords and low frequency terms are removed. Positional word features ? commonly
used in WSI ? for each of the three words to the left and right of the target word are also included.
To induce the senses of a target word w from a given set of usages of w, HDP is run on those usages
(represented according to the features described above) to induce topics; these topics are then interpreted
as representing the senses of w (one topic per sense). To determine the sense assigned to each instance,
the system aggregates over the topic assignments for each word in the context of w, and selects the topic
with the highest aggregated probability, i.e., argmax
z
P (t = z|d), where d is a document and t is a topic.
Recently, Lau et al. (2013a,b) found this method to give the overall best performance on two WSI
shared tasks (Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013), demonstrating that the method
is competitive with the state-of-the-art in WSI, and appropriate as the basis for a method for identifying
novel word-senses.
6
http://www.macmillandictionary.com/
7
http://www.csse.unimelb.edu.au/
~
tim/etc/novel-sense-dataset.tgz
1628
4.1 Novel Sense Detection
Following Lau et al. (2012), to detect novel senses of a target word using this WSI method, we jointly
topic model two corpora: a reference corpus ? taken to represent standard usage ? and a focus corpus
of newer texts potentially containing novel senses. In other words, we extract usages of a target word w
from both corpora, and then topic model the pooled instances of w. Under this approach, the discovered
topics are applicable to both corpora, so there is no need to reconcile two different sets of topics. For the
experiments in this paper, we extract three sentences of context for each usage, one sentence to either
side of the usage of the target word.
As each usage is given a sense assignment, we can identify novel senses ? senses present in the focus
corpus, but unattested in the reference corpus ? based on differences in the sense distribution for a given
word between the two corpora. Lau et al. present a Novelty score which is proportional to the following:
Novelty
Ratio
(s) =
p
f
(s)
p
r
(s)
(1)
where p
f
(s) and p
r
(s) are the proportion of usages of a given word corresponding to sense s in the focus
corpus and reference corpus, respectively, calculated using smoothed maximum likelihood estimates.
The score for a given lemma is the maximum score for any of its induced senses. We refer to the novel
sense for a lemma as the induced sense corresponding to this maximum.
4.2 Alternative Formulations of Novelty
The WSI system underlying the approach of Lau et al. labels each usage of a target lemma with an
induced sense. Therefore, any approach to identifying keywords ? words that are substantially more
frequent in one corpus than another ? can potentially be applied to identify novel senses, by viewing
?words? as (word,sense) tuples. We consider a version of Novelty based on the difference in relative
frequency of an induced sense in the focus and reference corpora, as below:
Novelty
Diff
(s) = p
f
(s)? p
r
(s) (2)
We consider a further new variant of Novelty based on the log-likelihood ratio of an induced sense in the
two corpora, referred to as Novelty
LLR
.
4.3 Incorporating knowledge of expected topics of novel senses
Cook et al. (2013) extended Lau et al.?s method by incorporating the observation that many neologisms
are related to topics that are culturally salient (e.g., Ayto, 2006); nowadays we see many neologisms
related to computing and the Internet. Indeed this observation was used to construct the gold-standard
dataset for this study. Cook et al. identified a set of words, W , related to computing and the Inter-
net, based on manual analysis of keywords for the corpora they considered. They then formulated the
Relevance of an induced sense s for a given word as follows:
Relevance
Manual
(s) =
?
w?W
p(w|s) (3)
For a given lemma, Relevance
Manual
is the maximum of this score for any of its induced senses, similar
to Novelty.
Following Cook et al., we calculate Relevance and Novelty for each induced sense of each lemma,
and then rank all the induced senses by these measures independently. We then compute the rank sum
of each induced sense of each lemma under these two rankings. The final score for a given lemma is
then the rank sum of its highest-ranked sense, and this sense is taken as that lemma?s novel sense. We
refer to this new method as ?Rank Sum?. Cook et al. only considered Novelty and Rank Sum; here we
additionally consider Relevance on its own.
For the keywords, we manually construct a set of words related to computing and the Internet, the
topics for which we expect to observe many novel senses in both of our datasets, in a similar way to
Cook et al. In order to minimize annotation effort, we concentrate on words that are more-frequent in the
1629
focus corpus than the reference corpus. For a given corpus pair, we begin by computing the keywords
for those corpora using Kilgarriff?s (2009) method.
8
Two annotators ? both computational linguists
and not authors of this paper ? independently scanned the top-1000 keywords for the focus corpus, and
selected those that were, based on their intuition, related to computing and the Internet. We then took
the topically-relevant words for a given corpus pair to be those in the intersection of the sets of words
selected by the two annotators. For BNC?ukWaC and SiBol/Port this gives 102 and 30 topically-relevant
words, respectively. This annotation required, on average, 23 minutes per annotator per corpus pair to
complete. Examples of the keywords selected for SiBol/Port include broadband, click, device, online,
and tweet.
4.4 Automatically-extracting keywords
We propose a new fully-automated method for identifying a set of topically-relevant keywords. Because
of the differences in corpus composition, the BNC?ukWaC keywords are often related to computing and
the Internet. To automatically obtain topically-relevant words, we take the top-1000 keywords for the
ukWaC relative to the BNC (i.e., the same keywords annotated for the BNC?ukWaC in Section 4.3).
The keywords for SiBol/Port are less-clearly related to the topics of interest, so we therefore use the
topically-relevant keywords from BNC?ukWaC for both datasets.
5 Results
In the following subsections we consider results at the type and then token level.
5.1 Type-level results
In these experiments we rank all items ? lemmas with a novel sense, and distractors ? by the various
Novelty, Relevance and Rank Sum methods for the BNC?ukWaC and SiBol/Port datasets. When a
lemma takes on a new sense, it might also increase in frequency. We therefore also consider a baseline in
which we rank the lemmas by the ratio of their frequency in the focus corpus and the reference corpus.
This baseline has not been previously considered by Lau et al. (2012) or Cook et al. (2013).
To compare approaches, we examine precision?recall curves in Figures 1 and 2. In an applied setting,
we envision these ranked lists being manually examined; we are therefore primarily interested in the
highly-ranked items, i.e., the left portion of the precision?recall curves.
For BNC?ukWaC (Figure 1), Novelty
Diff
and Novelty
Ratio
perform much better than Novelty
LLR
, but
not better than the frequency ratio baseline, at least for the left-most portion of the precision?recall
curve. Surprisingly, for Relevance, Relevance
Auto
outperforms Relevance
Manual
. This could be because
the focus corpus exhibits a clear topical bias towards computing and the Internet (the expected domain
of many neologisms in the focus corpus), and therefore a larger set of potentially noisy keywords is
more informative than a smaller, hand-selected set. All of the measures including the baseline, except
for Novelty
LLR
, assign higher scores to lemmas with a gold-standard novel sense than the distractors,
according to a one-sided Wilcoxon rank sum test (p < 0.05 in each case).
Turning to SiBol/Port in Figure 2, the frequency ratio baseline is much less effective here; the fre-
quency of the gold-standard novel senses is much lower overall than for BNC?ukWaC. All of the Novelty
and Relevance methods outperform the baseline, and ? with the exception of Novelty
Ratio
? rank the
lemmas with a gold-standard novel sense higher than the distractors (again using a one-sided Wilcoxon
rank sum test and p < 0.05). Furthermore, in this case, Relevance
Manual
outperforms Relevance
Auto
, as
expected.
In terms of the three Novelty measures, only Novelty
Diff
ranked items with a novel sense higher than
the distractors for both datasets. We therefore also show results for the Rank Sum approach combin-
ing Novelty
Diff
and each of Relevance
Manual
and Relevance
Auto
, denoted Rank Sum
Diff,manual
and Rank
Sum
Diff,auto
, respectively, in Figures 1 and 2. For both BNC?ukWaC and SiBol/Port, Rank Sum
Diff,manual
8
Using this method, the keywordness score for a given word is simply the ratio of its frequency per million words, plus a
constant, in two corpora; we set the constant to 100, the value recommended by Kilgarriff.
1630
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Freq ratio
Nov: Diff
Nov: LLR
Nov: Ratio
Rel: Man
Rel: Auto
RS: Diff+Man
RS: Diff+Auto
Figure 1: Precision?recall curve for the BNC?ukWaC dataset.
gives the best performance, and is a clear improvement over either of the individual methods. As ex-
pected, the performance of Rank Sum
Diff,auto
is not as good, but is nevertheless an improvement over the
frequency ratio baseline for both datasets and provides an alternative to manual scrutiny of the keywords.
To further examine the potential of incorporating knowledge of the expected domains of novel senses
to improve novel sense identification, we consider the case of cloud (n) from the SiBol/Port dataset. The
highest-probability words for the topic with highest Novelty
Diff
are the following: ash, volcanic, flight,
@card@,
9
travel, airline, volcano, airport, air, cloud. This sense appears to be related to the eruption
of the Eyjafjallajo?kull volcano, a major event in 2010 (the year from which the SiBol/Port focus corpus
is taken). Such topical differences, which do not correspond to a novel sense, are a problem for any
approach to identifying lexical semantic differences between two corpora based on differences in the
lexical context of a target word, and indeed observations such as this motivated our use of the methods
incorporating Relevance. The highest probability words for the topic with highest Relevance
Auto
are
the following: cloud, @card@, company, service, business, computing, market, security, datum, need.
This topic appears to correspond to the expected novel sense of Internet-based computational resources,
demonstrating the potential to improve a system for identifying novel word-senses by incorporating
knowledge of the expected domains of neologisms. Moreover, incorporating Relevance is particularly
powerful for avoiding false positives. For example, the distractor clause (n) is the lemma with the
sixth-highest Novelty
Diff
for SiBol/Port. The highest probability words for the corresponding topic are
the following: contract, @card@, club, player, million, england, capello, manager, sign, deal. This
induced sense appears to be related to clauses in Fabio Capello?s contract as manager of the England
national football team, and is not a novel sense of clause. However, none of the induced senses of clause
have high Relevance
Auto
or Relevance
Manual
, and so incorporating information from Relevance can avoid
incorrectly identifying this lemma as having a novel sense.
9
A generic token signifying a cardinal number.
1631
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Freq ratio
Nov: Diff
Nov: LLR
Nov: Ratio
Rel: Man
Rel: Auto
RS: Diff+Man
RS: Diff+Auto
Figure 2: Precision?recall curve for the SiBol/Port dataset.
5.2 Token-level results
In this section, we consider the token-level identification of instances of the gold-standard novel senses.
We compare Novelty, Relevance, and Rank Sum to a baseline that assigns all usages of a lemma to a
single topic which is selected as the novel sense; in this case recall is 1, and precision is proportional to
the frequency of the novel sense. We further consider the theoretical upper-bound of a method which
selects a single topic as the novel sense, based on the output of the HDP-based WSI method; this oracle
selects the best topic in terms of F-score as the novel sense. Results are presented in Table 2.
Each variant of Novelty and Relevance is an improvement over the baseline, although the Relevance
measures don?t perform as well as the Novelty ones, despite this dataset only containing novel senses
related to computing (despite our efforts to include non-technical novel senses). For consistency with
the presentation of the type-level results, we again consider Rank Sum using Novelty
Diff
, even though it
doesn?t perform as well as Novelty
LLR
or Novelty
Ratio
on BNC?ukWaC. Using either automatically- or
manually-obtained keywords, the performance of Rank Sum on BNC?ukWaC is remarkably on par with
the upper-bound, although for SiBol/Port there is little or no improvement over Novelty
Diff
. Neverthe-
less, these findings are further indication that novel sense identification can be improved by incorporating
information about the topics for which we expect to see novel senses. However, this approach is par-
ticularly helpful at the type-level, where information about the expected topics of novel senses prevents
lemmas not having a novel sense (i.e., the distractors) from being assigned high novelty.
6 Discussion and conclusion
The methods considered in this paper could be applied to any corpus pair, and potentially to identify
lexical semantic differences between, for example, domains or language varieties. The focus of this
study is English; sufficiently-large comparable corpora of national varieties of English (e.g., British and
American English), are not readily-available, but could potentially be inexpensively constructed in the
future (Cook and Hirst, 2012). We conducted some preliminary experiments using domain-specific sports
1632
Method
F-score
BNC?ukWaC SiBol/Port
Novelty
Diff
0.57 0.29
Novelty
LLR
0.67 0.28
Novelty
Ratio
0.66 0.28
Relevance
Auto
0.48 0.24
Relevance
Manual
0.45 0.27
Rank Sum
Diff,auto
0.72 0.30
Rank Sum
Diff,manual
0.72 0.29
Upper-bound 0.72 0.42
Baseline 0.36 0.20
Table 2: Token-level F-score for the BNC?ukWaC and SiBol/Port datasets using variants of Novelty,
Relevance, and Rank Sum. The F-score of an oracle upper-bound and baseline are also shown.
and finance corpora (Koeling et al., 2005) and the BNC. However, in these experiments we observed
very high Novelty
Ratio
for many distractors (selected in a similar way to our other experiments). Unlike
the case of time difference, in corpora from different domains, an arbitrarily chosen word will tend to
cooccur with very different words in the corpora, and Novelty
Ratio
will consequently be high. To address
vocabulary differences between corpora, in their experiments on identifying lexical semantic differences
between Dutch dialects, Peirsman et al. (2010) restricted the context words used to represent a target word
to those with moderate frequency in each of the two corpora used. We considered a similar restriction in
experiments on SiBol/Port, but did not see an overall improvement in performance.
We demonstrated that the performance of a method for identifying novel word-senses can be improved
by incorporating information ? acquired manually or automatically ? about the expected topics of
novel senses, which tend to be related to culturally-salient concepts. In future work, we intend to consider
improved approaches for automatically identifying topically-relevant words by incorporating information
about the top keywords of a corpus harvested from the Web for the domain of interest (e.g., PVS et al.,
2012). We also believe that topic models could be useful for identifying emerging or changing domains
themselves given the reference and focus corpus, and related work in this area (e.g., Wang andMcCallum,
2006; Blei and Lafferty, 2007).
To conclude, we have presented the largest type- and token-level dataset of diachronic sense differ-
ences to date, drawing on two pairs of corpora, and have made this dataset available. We applied a
recently-proposed WSI-based method to the task of finding sense differences in this data. We demon-
strated that, while the method shows promise, on a type-based task it is comparable to a a simple fre-
quency baseline, which had not been previously considered for this task. We carried out the first empirical
evaluation of a recently-proposed extension of this method that incorporates manually-acquired knowl-
edge of the expected domains of new senses, and found it to have superior performance at both the type
and token level. We further proposed and evaluated an approach that only uses this domain knowledge,
and a method for automating its acquisition.
Acknowledgments
We thank Michael Rundell and Macmillan Dictionaries for providing the list of headwords added to
MEDAL since its first edition, and Charlotte Taylor for providing us with early access to SiBol/Port. We
also thank Richard Fothergill, Karl Grieser, and Andrew Mackinlay for their help in annotation. This
research was supported in part by funding from the Australian Research Council.
References
John Ayto. 2006. Movers and Shakers: A Chronology of Words that Shaped our Age. Oxford University
Press, Oxford.
1633
David Bamman and Gregory Crane. 2011. Measuring historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Digital Libraries (JCDL 2011), pages 1?10. Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research,
3:993?1022.
David M. Blei and John D. Lafferty. 2007. Latent dirichlet allocation. The Annals of Applied Statistics,
1(1):17?35.
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103?111. Athens, Greece.
Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford
University Computing Services.
Marine Carpuat, Hal Daume? III, Katharine Henry, Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your parallel data tie you to an old domain. In Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages
1435?1445. Sofia, Bulgaria.
Paul Cook and Graeme Hirst. 2011. Automatic identification of words with novel but infrequent
senses. In Proceedings of the 25th Pacific Asia Conference on Language Information and Compu-
tation (PACLIC 25), pages 265?274. Singapore.
Paul Cook and Graeme Hirst. 2012. Do Web corpora from top-level domains represent national varieties
of English? In Actes des 11es Journ
?
ees Internationales d?Analyse Statistique des Donn
?
ees Textuelles /
Proceedings of the 11th International Conference on Textual Data Statistical Analysis, pages 281?293.
Lie`ge, Belgium.
Paul Cook, Jey Han Lau, Michael Rundell, Diana McCarthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for detecting new word-senses. In Electronic lexicography
in the 21st century: thinking outside the paper. Proceedings of the eLex 2013 conference, pages 49?65.
Tallinn, Estonia.
Paul Cook and Suzanne Stevenson. 2010. Automatically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh International Conference on Language Resources and
Evaluation (LREC 2010), pages 28?34. Valletta, Malta.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evalu-
ating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47?54. Marrakech, Morocco.
Kristina Gulordava and Marco Baroni. 2011. A distributional similarity approach to the detection of
semantic change in the Google Books Ngram corpus. In Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Semantics, pages 67?71. Edinburgh, Scotland.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-2013 task 13: Word sense induction for graded
and non-graded senses. In Proceedings of the 7th International Workshop on Semantic Evaluation
(SemEval 2013), pages 290?299. Atlanta, USA.
Adam Kilgarriff. 2009. Simple maths for keywords. In Proceedings of the Corpus Linguistics Confer-
ence. Liverpool, UK.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004. The Sketch Engine. In Proceed-
ings of the Eleventh EURALEX International Congress (EURALEX 2004), pages 105?116. Lorient,
France.
Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predom-
inant sense acquisition. In Proceedings of Human Language Technology Conference and Conference
1634
on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), pages 419?426. Van-
couver, Canada.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a. unimelb: Topic modelling-based word sense
induction. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013),
pages 307?311. Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b. unimelb: Topic modelling-based word sense
induction for web snippet clustering. In Proceedings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013), pages 217?221. Atlanta, USA.
Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense
induction for novel sense detection. In Proceedings of the 13th Conference of the European Chapter
of the Association for Computational Linguistics (EACL 2012), pages 591?601. Avignon, France.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics, 33(4):553?590.
Roberto Navigli and Daniele Vannella. 2013. SemEval-2013 task 11: Word sense induction and dis-
ambiguation within an end-user application. In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?201. Atlanta, USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language Engineering, 16(4):469?491.
Avinesh PVS, Diana McCarthy, Dominic Glennon, and Jan Pomika?lek. 2012. Domain specific corpora
from the Web. In Proceedings of the 15th Euralex International Congress, pages 336?342. Oslo,
Norway.
Christian Rohrdantz, Annette Hautli, Thomas Mayer, Miriam Butt, Daniel A. Keim, and Frans Plank.
2011. Towards tracking semantic change by visual analytics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies (ACL 2011),
pages 305?310. Portland, USA.
Michael Rundell and Gwyneth Fox, editors. 2002. Macmillan English Dictionary for Advanced Learn-
ers. Macmillan Education, Oxford, UK.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009. Semantic density analysis: Comparing word
meaning across time and space. In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?111. Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the
International Conference on New Methods in Language Processing, pages 44?49. Manchester, UK.
Catherine Soanes and Angus Stevenson, editors. 2008. The Concise Oxford English Dictionary. Oxford
University Press, Oxford, UK, eleventh (revised) edition. Oxford Reference Online.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Association, 101:1566?1581.
Della Thompson, editor. 1995. The Concise Oxford Dictionary of Current English. Oxford University
Press, Oxford, UK, ninth edition.
Xuerei Wang and Andrew McCallum. 2006. Topics over time: A non-Markov continuous-time model of
topical trends. In Proceedings of the Eleventh International Conference on Knowledge Discovery and
Data Mining, pages 424?433. Philadelphia, USA.
Xuchen Yao and Benjamin Van Durme. 2011. Nonparametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing, pages 10?14.
Portland, USA.
1635
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 591?601,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Word Sense Induction for Novel Sense Detection
Jey Han Lau,?? Paul Cook,? Diana McCarthy, ?
David Newman,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California Irvine
? Lexical Computing
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, newman@uci.edu, tb@ldwin.net
Abstract
We apply topic modelling to automatically
induce word senses of a target word, and
demonstrate that our word sense induction
method can be used to automatically de-
tect words with emergent novel senses, as
well as token occurrences of those senses.
We start by exploring the utility of stan-
dard topic models for word sense induction
(WSI), with a pre-determined number of
topics (=senses). We next demonstrate that
a non-parametric formulation that learns an
appropriate number of senses per word ac-
tually performs better at the WSI task. We
go on to establish state-of-the-art results
over two WSI datasets, and apply the pro-
posed model to a novel sense detection task.
1 Introduction
Word sense induction (WSI) is the task of auto-
matically inducing the different senses of a given
word, generally in the form of an unsupervised
learning task with senses represented as clusters
of token instances. It contrasts with word sense
disambiguation (WSD), where a fixed sense in-
ventory is assumed to exist, and token instances
of a given word are disambiguated relative to the
sense inventory. While WSI is intuitively appeal-
ing as a task, there have been no real examples of
WSI being successfully deployed in end-user ap-
plications, other than work by Schutze (1998) and
Navigli and Crisafulli (2010) in an information re-
trieval context. A key contribution of this paper
is the successful application of WSI to the lexico-
graphical task of novel sense detection, i.e. identi-
fying words which have taken on new senses over
time.
One of the key challenges in WSI is learning
the appropriate sense granularity for a given word,
i.e. the number of senses that best captures the
token occurrences of that word. Building on the
work of Brody and Lapata (2009) and others, we
approach WSI via topic modelling ? using La-
tent Dirichlet Allocation (LDA: Blei et al(2003))
and derivative approaches ? and use the topic
model to determine the appropriate sense gran-
ularity. Topic modelling is an unsupervised ap-
proach to jointly learn topics ? in the form of
multinomial probability distributions over words
? and per-document topic assignments ? in the
form of multinomial probability distributions over
topics. LDA is appealing for WSI as it both as-
signs senses to words (in the form of topic alloca-
tion), and outputs a representation of each sense
as a weighted list of words. LDA offers a solu-
tion to the question of sense granularity determi-
nation via non-parametric formulations, such as
a Hierarchical Dirichlet Process (HDP: Teh et al
(2006), Yao and Durme (2011)).
Our contributions in this paper are as follows.
We first establish the effectiveness of HDP for
WSI over both the SemEval-2007 and SemEval-
2010WSI datasets (Agirre and Soroa, 2007; Man-
andhar et al 2010), and show that the non-
parametric formulation is superior to a standard
LDA formulation with oracle determination of
sense granularity for a given word. We next
demonstrate that our interpretation of HDP-based
WSI is superior to other topic model-based ap-
proaches to WSI, and indeed, better than the best-
published results for both SemEval datasets. Fi-
nally, we apply our method to the novel sense de-
tection task based on a dataset developed in this
research, and achieve highly encouraging results.
2 Methodology
In topic modelling, documents are assumed to ex-
hibit multiple topics, with each document having
591
its own distribution over topics. Words are gen-
erated in each document by first sampling a topic
from the document?s topic distribution, then sam-
pling a word from that topic. In this work we
use the topic models?s probabilistic assignment of
topics to words for the WSI task.
2.1 Data Representation and Pre-processing
In the context of WSI, topics form our sense rep-
resentation, and words in a sentence are gener-
ated conditioned on a particular sense of the target
word. The ?document? in the WSI case is a sin-
gle sentence or a short document fragment con-
taining the target word, as we would not expect
to be able to generate a full document from the
sense of a single target word.1 In the case of the
SemEval datasets, we use the word contexts pro-
vided in the dataset, while in our novel sense de-
tection experiments, we use a context window of
three sentences, one sentence to either side of the
token occurrence of the target word.
As our baseline representation, we use a bag of
words, where word frequency is kept but not word
order. All words are lemmatised, and stopwords
and low frequency terms are removed.
We also experiment with the addition of po-
sitional context word information, as commonly
used in WSI. That is, we introduce an additional
word feature for each of the three words to the left
and right of the target word.
Pado? and Lapata (2007) demonstrated the im-
portance of syntactic dependency relations in the
construction of semantic space models, e.g. for
WSD. Based on these findings, we include depen-
dency relations as additional features in our topic
models,2 but just for dependency relations that in-
volve the target word.
2.2 Topic Modelling
Topic models learn a probability distribution over
topics for each document, by simply aggregating
the distributions over topics for each word in the
document. In WSI terms, we take this distribu-
tion over topics for each target word (?instance?
in WSI parlance) as our distribution over senses
for that word.
1Notwithstanding the one sense per discourse heuristic
(Gale et al 1992).
2We use the Stanford Parser to do part of speech tagging
and to extract the dependency relations (Klein and Manning,
2003; De Marneffe et al 2006).
In our initial experiments, we use LDA topic
modelling, which requires us to set T , the num-
ber of topics to be learned by the model. The
LDA generative process is: (1) draw a latent
topic z from a document-specific topic distribu-
tion P (t = z|d) then; (2) draw a word w from
the chosen topic P (w|t = z). Thus, the probabil-
ity of producing a single copy of word w given a
document d is given by:
P (w|d) =
T
?
z=1
P (w|t = z)P (t = z|d).
In standard LDA, the user needs to specify the
number of topics T . In non-parametric variants of
LDA, the model dynamically learns the number of
topics as part of the topic modelling. The particu-
lar implementation of non-parametric topic model
we experiment with is Hierarchical Dirichlet Pro-
cess (HDP: Teh et al(2006)),3 where, for each
document, a distribution of mixture components
P (t|d) is sampled from a base distribution G0
as follows: (1) choose a base distribution G0 ?
DP (?,H); (2) for each document d, generate dis-
tribution P (t|d) ? DP (?0, G0); (3) draw a la-
tent topic z from the document?s mixture compo-
nent distribution P (t|d), in the same manner as
for LDA; and (4) draw a word w from the chosen
topic P (w|t = z).4
For both LDA and HDP, we individually topic
model each target word, and determine the sense
assignment z for a given instance by aggregating
over the topic assignments for each word in the
instance and selecting the sense with the highest
aggregated probability, argmaxz P (t = z|d).
3 SemEval Experiments
To facilitate comparison of our proposed method
for WSI with previous approaches, we use the
dataset from the SemEval-2007 and SemEval-
2010 word sense induction tasks (Agirre and
3We use the C++ implementation of HDP
(http://www.cs.princeton.edu/?blei/
topicmodeling.html) in our experiments.
4The two HDP parameters ? and ?0 control the variabil-
ity of senses in the documents. In particular, ? controls the
degree of sharing of topics across documents ? a high ?
value leads to more topics, as topics for different documents
are more dissimilar. ?0, on the other hand, controls the de-
gree of mixing of topics within a document? a high ?0 gen-
erates fewer topics, as topics are less homogeneous within a
document.
592
Soroa, 2007; Manandhar et al 2010). We first
experiment with the SemEval-2010 dataset, as it
includes explicit training and test data for each
target word and utilises a more robust evaluation
methodology. We then return to experiment with
the SemEval-2007 dataset, for comparison pur-
poses with other published results for topic mod-
elling approaches to WSI.
3.1 SemEval-2010
3.1.1 Dataset and Methodology
Our primary WSI evaluation is based on
the dataset provided by the SemEval-2010 WSI
shared task (Manandhar et al 2010). The dataset
contains 100 target words: 50 nouns and 50 verbs.
For each target word, a fixed set of training and
test instances are supplied, typically 1 to 3 sen-
tences in length, each containing the target word.
The default approach to evaluation for the
SemEval-2010 WSI task is in the form of WSD
over the test data, based on the senses that have
been automatically induced from the training
data. Because the induced senses will likely vary
in number and nature between systems, the WSD
evaluation has to incorporate a sense alignment
step, which it performs by splitting the test in-
stances into two sets: a mapping set and an eval-
uation set. The optimal mapping from induced
senses to gold-standard senses is learned from the
mapping set, and the resulting sense alignment is
used to map the predictions of the WSI system to
pre-defined senses for the evaluation set. The par-
ticular split we use to calculate WSD effective-
ness in this paper is 80%/20% (mapping/test), av-
eraged across 5 random splits.5
The SemEval-2010 training data consists of ap-
proximately 163K training instances for the 100
target words, all taken from the web. The test
data is approximately 9K instances taken from a
variety of news sources. Following the standard
approach used by the participating systems in the
SemEval-2010 task, we induce senses only from
the training instances, and use the learned model
to assign senses to the test instances.
5A 60%/40% split is also provided as part of the task
setup, but the results are almost identical to those for the
80%/20% split, and so are omitted from this paper. The orig-
inal task also made use of V-measure and Paired F-score to
evaluate the induced word sense clusters, but have degen-
erate behaviour in correlating strongly with the number of
senses induced by the method (Manandhar et al 2010), and
are hence omitted from this paper.
In our original experiments with LDA, we set
the number of topics (T ) for each target word to
the number of senses represented in the test data
for that word (varying T for each target word).
This is based on the unreasonable assumption that
we will have access to gold-standard information
on sense granularity for each target word, and is
done to establish an upper bound score for LDA.
We then relax the assumption, and use a fixed T
setting for each of sets of nouns (T = 7) and
verbs (T = 3), based on the average number of
senses from the test data in each case. Finally,
we introduce positional context features for LDA,
once again using the fixed T values for nouns and
verbs.
We next apply HDP to the WSI task, using
positional features, but learning the number of
senses automatically for each target word via the
model. Finally, we experiment with adding de-
pendency features to the model.
To summarise, we provide results for the fol-
lowing models:
1. LDA+Variable T : LDA with variable T
for each target word based on the number of
gold-standard senses.
2. LDA+Fixed T : LDA with fixed T for each
of nouns and verbs.
3. LDA+Fixed T+Position: LDA with fixed
T and extra positional word features.
4. HDP+Position: HDP (which automatically
learns T ), with extra positional word fea-
tures.
5. HDP+Position+Dependency: HDP with
both positional word and dependency fea-
tures.
We compare our models with two baselines
from the SemEval-2010 task: (1) Baseline Ran-
dom ? randomly assign each test instance to one
of four senses; (2) Baseline MFS ? most fre-
quent sense baseline, assigning all test instances
to one sense; and also a benchmark system
(UoY), in the form of the University of York sys-
tem (Korkontzelos and Manandhar, 2010), which
achieved the best overall WSD results in the orig-
inal SemEval-2010 task.
3.2 SemEval-2010 Results
The results of our experiments over the SemEval-
2010 dataset are summarised in Table 1.
593
System WSD (80%/20%)All Verbs Nouns
Baselines
Baseline Random 0.57 0.66 0.51
Baseline MFS 0.59 0.67 0.53
LDA
Variable T 0.64 0.69 0.60
Fixed T 0.63 0.68 0.59
Fixed T +Position 0.63 0.68 0.60
HDP
+Position 0.68 0.72 0.65
+Position+Dependency 0.68 0.72 0.65
Benchmark
UoY 0.62 0.67 0.59
Table 1: WSD F-score over the SemEval-2010 dataset
Looking first at the results for LDA, we see
that the first LDA approach (variable T ) is very
competitive, outperforming the benchmark sys-
tem. In this approach, however, we assume per-
fect knowledge of the number of gold senses of
each target word, meaning that the method isn?t
truly unsupervised. When we fixed T for each
of the nouns and verbs, we see a small drop in
F-score, but encouragingly the method still per-
forms above the benchmark. Adding positional
word features improves the results very slightly
for nouns.
When we relax the assumption on the number
of word senses in moving to HDP, we observe a
marked improvement in F-score over LDA. This
is highly encouraging and somewhat surprising,
as in hiding information about sense granularity
from the model, we have actually improved our
results. We return to discuss this effect below.
For the final feature, we add dependency features
to the HDP model (in addition to retaining the
positional word features), but see no movement
in the results.6 While the dependency features
didn?t reduce F-score, their utility is questionable
as the generation of the features from the Stanford
parser is computationally expensive.
To better understand these results, we present
the top-10 terms for each of the senses induced for
the word cheat in Table 2. These senses are learnt
using HDP with both positional word features
(e.g. husband #-1, indicating the lemma husband
to the immediate left of the target word) and de-
pendency features (e.g. cheat#prep on#wife). The
first observation to make is that senses 7, 8 and
9 are ?junk? senses, in that the top-10 terms do
6An identical result was observed for LDA.
not convey a coherent sense. These topics are an
artifact of HDP: they are learnt at a much later
stage of the iterative process of Gibbs sampling
and are often smaller than other topics (i.e. have
more zero-probability terms). We notice that they
are assigned as topics to instances very rarely (al-
though they are certainly used to assign topics to
non-target words in the instances), and as such,
they do not present a real issue when assigning
the sense to an instance, as they are likely to be
overshadowed by the dominant senses.7 This con-
clusion is born out when we experimented with
manually filtering out these topics when assign-
ing instance to senses: there was no perceptible
change in the results, reinforcing our suggestion
that these topics do not impact on target word
sense assignment.
Comparing the results for HDP back to those
for LDA, HDP tends to learn almost double the
number of senses per target word as are in the
gold-standard (and hence are used for the ?Vari-
able T ? version of LDA). Far from hurting our
WSD F-score, however, the extra topics are dom-
inated by junk topics, and boost WSD F-score for
the ?genuine? topics. Based on this insight, we
ran LDA once again with variable T (and posi-
tional and dependency features), but this time set-
ting T to the value learned by HDP, to give LDA
the facility to use junk topics. This resulted in an
F-score of 0.66 across all word classes (verbs =
0.71, nouns = 0.62), demonstrating that, surpris-
ingly, even for the same T setting, HDP achieves
superior results to LDA. I.e., not only does HDP
learn T automatically, but the topic model learned
for a given T is superior to that for LDA.
Looking at the other senses discovered for
cheat, we notice that the model has induced a
myriad of senses: the relationship sense of cheat
(senses 1, 3 and 4, e.g. husband cheats); the exam
usage of cheat (sense 2); the competition/game
usage of cheat (sense 5); and cheating in the po-
litical domain (sense 6). Although the senses are
possibly ?split? a little more than desirable (e.g.
senses 1, 3 and 4 arguably describe the same
sense), the overall quality of the produced senses
7In the WSD evaluation, the alignment of induced senses
to the gold senses is learnt automatically based on the map-
ping instances. E.g. if all instances that are assigned sense
a have gold sense x, then sense a is mapped to gold sense
x. Therefore, if the proportion of junk senses in the map-
ping instances is low, their influence on WSD results will be
negligible.
594
Sense Num Top-10 Terms
1 cheat think want ... love feel tell guy cheat#nsubj#include find
2 cheat student cheating test game school cheat#aux#to teacher exam study
3 husband wife cheat wife #1 tiger husband #-1 cheat#prep on#wife ... woman cheat#nsubj#husband
4 cheat woman relationship cheating partner reason cheat#nsubj#man woman #-1 cheat#aux#to spouse
5 cheat game play player cheating poker cheat#aux#to card cheated money
6 cheat exchange china chinese foreign cheat #-2 cheat #2 china #-1 cheat#aux#to team
7 tina bette kirk walk accuse mon pok symkyn nick star
8 fat jones ashley pen body taste weight expectation parent able
9 euro goal luck fair france irish single 2000 cheat#prep at#point complain
Table 2: The top-10 terms for each of the senses induced for the verb cheat by the HDP model (with positional
word and dependency features)
is encouraging. Also, we observe a spin-off ben-
efit of topic modelling approaches to WSI: the
high-ranking words in each topic can be used to
gist the sense, and anecdotally confirm the impact
of the different feature types (i.e. the positional
word and dependency features).
3.3 Comparison with other Topic Modelling
Approaches to WSI
The idea of applying topic modelling to WSI is
not entirely new. Brody and Lapata (2009) pro-
posed an LDA-based model which assigns differ-
ent weights to different feature sets (e.g. unigram
tokens vs. dependency relations), using a ?lay-
ered? feature representation. They carry out ex-
tensive parameter optimisation of both the (fixed)
number of senses, number of layers, and size of
the context window.
Separately, Yao and Durme (2011) proposed
the use of non-parametric topic models in WSI.
The authors preprocess the instances slightly dif-
ferently, opting to remove the target word from
each instance and stem the tokens. They also
tuned the hyperparameters of the topic model to
optimise the WSI effectiveness over the evalua-
tion set, and didn?t use positional or dependency
features.
Both of these papers were evaluated over
only the SemEval-2007 WSI dataset (Agirre and
Soroa, 2007), so we similarly apply our HDP
method to this dataset for direct comparability. In
the remainder of this section, we refer to Brody
and Lapata (2009) as BL, and Yao and Durme
(2011) as YVD.
The SemEval-2007 dataset consists of roughly
27K instances, for 65 target verbs and 35 target
nouns. BL report on results only over the noun
instances, so we similarly restrict our attention to
System F-Score
BL 0.855
YVD 0.857
SemEval Best (I2R) 0.868
Our method (default parameters) 0.842
Our method (tuned parameters) 0.869
Table 3: F-score for the SemEval-2007 WSI task, for
our HDP method with default and tuned parameter set-
tings, as compared to competitor topic modelling and
other approaches to WSI
the nouns in this paper. Training data was not pro-
vided as part of the original dataset, so we fol-
low the approach of BL and YVD in construct-
ing our own training dataset for each target word
from instances extracted from the British National
Corpus (BNC: Burnard (2000)).8 Both BL and
YVD separately report slightly higher in-domain
results from training on WSJ data (the SemEval-
2007 data was taken from the WSJ). For the pur-
poses of model comparison under identical train-
ing settings, however, it is appropriate to report on
results for only the BNC.
We experiment with both our original method
(with both positional word and dependency fea-
tures, and default parameter settings for HDP)
without any parameter tuning, and the same
method with the tuned parameter settings of
YVD, for direct comparability. We present the re-
sults in Table 3, including the results for the best-
performing system in the original SemEval-2007
task (I2R: Niu et al(2007)).
The results are enlightening: with default pa-
rameter settings, our methodology is slightly be-
low the results of the other three models. Bear
8In creating the training dataset, each instance is made
up of the sentence the target word occurs in, as we as one
sentence to either side of that sentence, i.e. 3 sentences in
total per instance.
595
in mind, however, that the two topic modelling-
based approaches were tuned extensively to the
dataset. When we use the tuned hyperparame-
ter settings of YVD, our results rise around 2.5%
to surpass both topic modelling approaches, and
marginally outperform the I2R system from the
original task. Recall that both BL and YVD report
higher results again using in-domain training data,
so we would expect to see further gains again over
the I2R system in following this path.
Overall, these results agree with our findings
over the SemEval-2010 dataset (Section 3.2), un-
derlining the viability of topic modelling to auto-
mated word sense induction.
3.4 Discussion
As part of our preprocessing, we remove all stop-
words (other than for the positional word and de-
pendency features), as described in Section 2.1.
We separately experimented with not removing
stopwords, based on the intuition that prepositions
such as to and on can be informative in determin-
ing word sense based on local context. The results
were markedly worse, however. We also tried ap-
pending part of speech information to each word
lemma, but the resulting data sparseness meant
that results dropped marginally.
When determining the sense for an instance, we
aggregate the sense assignments for each word in
the instance (not just the target word). An alter-
nate strategy is to use only the target word topic
assignment, but again, the results for this strategy
were inferior to the aggregate method.
In the SemEval-2007 experiments (Sec-
tion 3.3), we found that YVD?s hyperparameter
settings yielded better results than the default
settings. We experimented with parameter tuning
over the SemEval-2010 dataset (including YVD?s
optimal setting on the 2007 dataset), but found
that the default setting achieved the best overall
results: although the WSD F-score improved a
little for nouns, it worsened for verbs. This obser-
vation is not unexpected: as the hyperparameters
were optimised for nouns in their experiments,
the settings might not be appropriate for verbs.
This also suggests that their results may be due in
part to overfitting the SemEval-2007 data.
4 Identifying Novel Senses
Having established the effectiveness of our ap-
proach at WSI, we next turn to an application of
WSI, in identifying words which have taken on
novel senses over time, based on analysis of di-
achronic data. Our topic modelling approach is
particularly attractive for this task as, not only
does it jointly perform type-level WSI, and token-
level WSD based on the induced senses (in as-
signing topics to each instance), but it is possible
to gist the induced senses via the contents of the
topic (typically using the topic words with highest
marginal probability).
The meanings of words can change over time;
in particular, words can take on new senses. Con-
temporary examples of new word-senses include
the meanings of swag and tweet as used below:
1. We all know Frankie is adorable, but does he
have swag? [swag = ?style?]
2. The alleged victim gave a description of the
man on Twitter and tweeted that she thought
she could identify him. [tweet = ?send a mes-
sage on Twitter?]
These senses of swag and tweet are not included
in many dictionaries or computational lexicons ?
e.g., neither of these senses is listed in Wordnet
3.0 (Fellbaum, 1998) ? yet appear to be in regu-
lar usage, particularly in text related to pop culture
and online media.
The manual identification of such new word-
senses is a challenge in lexicography over and
above identifying new words themselves, and
is essential to keeping dictionaries up-to-date.
Moreover, lexicons that better reflect contempo-
rary usage could benefit NLP applications that use
sense inventories.
The challenge of identifying changes in word
sense has only recently been considered in com-
putational linguistics. For example, Sagi et al
(2009), Cook and Stevenson (2010), and Gulor-
dava and Baroni (2011) propose type-based mod-
els of semantic change. Such models do not
account for polysemy, and appear best-suited to
identifying changes in predominant sense. Bam-
man and Crane (2011) use a parallel Latin?
English corpus to induce word senses and build
a WSD system, which they then apply to study
diachronic variation in word senses. Crucially, in
this token-based approach there is a clear connec-
tion between word senses and tokens, making it
possible to identify usages of a specific sense.
Based on the findings in Section 3.2, here we
apply the HDP method for WSI to the task of
596
identifying new word-senses. In contrast to Bam-
man and Crane (2011) our token-based approach
does not require parallel text to induce senses.
4.1 Method
Given two corpora ? a reference corpus which
we take to represent standard usage, and a second
corpus of newer texts ? we identify senses that
are novel to the second corpus compared to the
reference corpus. For a given word w, we pool
all usages of w in the reference corpus and sec-
ond corpus, and run the HDP WSI method on this
super-corpus to induce the senses of w. We then
tag all usages of w in both corpora with their sin-
gle most-likely automatically-induced sense.
Intuitively, if a word w is used in some sense
s in the second corpus, and w is never used in
that sense in the reference corpus, then w has ac-
quired a new sense, namely s. We capture this
intuition into a novelty score (?Nov?) that indi-
cates whether a given word w has a new sense in
the second corpus, s, compared to the reference
corpus, r, as below:
Nov(w) = max
({
ps(ti)? pr(ti)
pr(ti)
: ti ? T
})
(1)
where ps(ti) and pr(ti) are the probability of
sense ti in the second corpus and reference cor-
pus, respectively, calculated using smoothed max-
imum likelihood estimates, and T is the set of
senses induced for w. Novelty is high if there is
some sense t that has much higher relative fre-
quency in s than r and that is also relatively infre-
quent in r.
4.2 Data
Because we are interested in the identification of
novel word-senses for applications such as lexi-
con maintenance, we focus on relatively newly-
coined word-senses. In particular, we take the
written portion of the BNC ? consisting primar-
ily of British English text from the late 20th cen-
tury ? as our reference corpus, and a similarly-
sized random sample of documents from the
ukWaC (Ferraresi et al 2008) ? a Web corpus
built from the .uk domain in 2007 which in-
cludes a wide range of text types ? as our sec-
ond corpus. Text genres are represented to dif-
ferent extents in these corpora with, for example,
text types related to the Internet being much more
common in the ukWaC. Such differences are a
noted challenge for approaches to identifying lex-
ical semantic differences between corpora (Peirs-
man et al 2010), but are difficult to avoid given
the corpora that are available. We use TreeTagger
(Schmid, 1994) to tokenise and lemmatise both
corpora.
Evaluating approaches to identifying seman-
tic change is a challenge, particularly due to the
lack of appropriate evaluation resources; indeed,
most previous approaches have used very small
datasets (Sagi et al 2009; Cook and Stevenson,
2010; Bamman and Crane, 2011). Because this
is a preliminary attempt at applying WSI tech-
niques to identifying new word-senses, our evalu-
ation will also be based on a rather small dataset.
We require a set of words that are known to
have acquired a new sense between the late 20th
and early 21st centuries. The Concise Oxford
English Dictionary aims to document contempo-
rary usage, and has been published in numerous
editions including Thompson (1995, COD95) and
Soanes and Stevenson (2008, COD08). Although
some of the entries have been substantially re-
vised between editions, many have not, enabling
us to easily identify new senses amongst the en-
tries in COD08 relative to COD95. A manual lin-
ear search through the entries in these dictionaries
would be very time consuming, but by exploit-
ing the observation that new words often corre-
spond to concepts that are culturally salient (Ayto,
2006), we can quickly identify some candidates
for words that have taken on a new sense.
Between the time periods of our two corpora,
computers and the Internet have become much
more mainstream in society. We therefore ex-
tracted all entries from COD08 containing the
word computing (which is often used as a topic la-
bel in this dictionary) that have a token frequency
of at least 1000 in the BNC. We then read the
entries for these 87 lexical items in COD95 and
COD08 and identified those which have a clear
computing sense in COD08 that was not present
in COD95. In total we found 22 such items. This
process, along with all the annotation in this sec-
tion, is carried out by a native English-speaking
author of this paper.
To ensure that the words identified from the
dictionaries do in fact have a new sense in the
ukWaC sample compared to the BNC, we exam-
ine the usage of these words in the corpora. We
extract a random sample of 100 usages of each
597
lemma from the BNC and ukWaC sample and
annotate these usages as to whether they corre-
spond to the novel sense or not. This binary dis-
tinction is easier than fine-grained sense annota-
tion, and since we do not use these annotations
for formal evaluation ? only for selecting items
for our dataset ? we do not carry out an inter-
annotator agreement study here. We eliminate any
lemma for which we find evidence of the novel
sense in the BNC, or for which we do not find
evidence of the novel sense in the ukWaC sam-
ple.9 We further check word sketches (Kilgarriff
and Tugwell, 2002)10 for each of these lemmas
in the BNC and ukWaC for collocates that likely
correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel
sense in the BNC, or fail to find evidence of the
novel sense in the ukWaC sample. At the end
of this process we have identified the following
5 lemmas that have the indicated novel senses in
the ukWaC compared to the BNC: domain (n) ?In-
ternet domain?; export (v) ?export data?; mirror
(n) ?mirror website?; poster (n) ?one who posts
online?; and worm (n) ?malicious program?. For
each of the 5 lemmas with novel senses, a sec-
ond annotator ? also a native English-speaking
author of this paper ? annotated the sample of
100 usages from the ukWaC. The observed agree-
ment and unweighted Kappa between the two an-
notators is 97.2% and 0.92, respectively, indicat-
ing that this is indeed a relatively easy annotation
task. The annotators discussed the small number
of disagreements to reach consensus.
For our dataset we also require items that have
not acquired a novel sense in the ukWaC sample.
For each of the above 5 lemmas we identified a
distractor lemma of the same part-of-speech that
has a similar frequency in the BNC, and that has
not undergone sense change between COD95 and
COD08. The 5 distractors are: cinema (n); guess
(v); symptom (n); founder (n); and racism (n).
4.3 Results
We compute novelty (?Nov?, Equation 1) for all
10 items in our dataset, based on the output of the
9We use the IMS Open Corpus Workbench (http://
cwb.sourceforge.net/) to extract the usages of our
target lemmas from the corpora. This extraction process fails
in some cases, and so we also eliminate such items from our
dataset.
10http://www.sketchengine.co.uk/
Lemma Novelty Freq. ratio Novel sense freq.
domain (n) 116.2 2.60 41
worm (n) 68.4 1.04 30
mirror (n) 38.4 0.53 10
guess (v) 16.5 0.93 ?
export (v) 13.8 0.88 28
founder (n) 11.0 1.20 ?
cinema (n) 9.7 1.30 ?
poster (n) 7.9 1.83 4
racism (n) 2.4 0.98 ?
symptom (n) 2.1 1.16 ?
Table 4: Novelty score (?Nov?), ratio of frequency in
the ukWaC sample and BNC, and frequency of the
novel sense in the manually-annotated 100 instances
from the ukWaC sample (where applicable), for all
lemmas in our dataset. Lemmas shown in boldface
have a novel sense in the ukWaC sample compared to
the BNC.
topic modelling. The results are shown in column
?Novelty? in Table 4. The lemmas with a novel
sense have higher novelty scores than the distrac-
tors according to a one-sided Wilcoxon rank sum
test (p < .05).
When a lemma takes on a new sense, it might
also increase in frequency. We therefore also con-
sider a baseline in which we rank the lemmas by
the ratio of their frequency in the second and ref-
erence corpora. These results are shown in col-
umn ?Freq. ratio? in Table 4. The difference be-
tween the frequency ratios for the lemmas with a
novel sense, and the distractors, is not significant
(p > .05).
Examining the frequency of the novel senses?
shown in column ?Novel sense freq.? in Table 4
? we see that the lowest-ranked lemma with a
novel sense, poster, is also the lemma with the
least-frequent novel sense. This result is unsur-
prising as our novelty score will be higher for
higher-frequency novel senses. The identification
of infrequent novel senses remains a challenge.
The top-ranked topic words for the sense cor-
responding to the maximum in Equation 1 for
the highest-ranked distractor, guess, are the fol-
lowing: @card@, post, ..., n?t, comment, think,
subject, forum, view, guess. This sense seems
to correspond to usages of guess in the context
of online forums, which are better represented
in the ukWaC sample than the BNC. Because of
the challenges posed by such differences between
corpora (discussed in Section 4.2) we are unsur-
prised to see such an error, but this could be ad-
dressed in the future by building comparable cor-
598
Lemma
Topic Selection Methodology
Nov Oracle (single topic) Oracle (multiple topics)
Precision Recall F-score Precision Recall F-score Precision Recall F-score
domain (n) 1.00 0.29 0.45 1.00 0.56 0.72 0.97 0.88 0.92
export (v) 0.93 0.96 0.95 0.93 0.96 0.95 0.90 1.00 0.95
mirror (n) 0.67 1.00 0.80 0.67 1.00 0.80 0.67 1.00 0.80
poster (n) 0.00 0.00 0.00 0.44 1.00 0.62 0.44 1.00 0.62
worm (n) 0.93 0.90 0.92 0.93 0.90 0.92 0.86 1.00 0.92
Table 5: Results for identifying the gold-standard novel senses based on the three topic selection methodologies
of: (1) Nov; (2) oracle selection of a single topic; and (3) oracle selection of multiple topics.
pora for use in this application.
Having demonstrated that our method for iden-
tifying novel senses can distinguish lemmas that
have a novel sense in one corpus compared to an-
other from those that do not, we now consider
whether this method can also automatically iden-
tify the usages of the induced novel sense.
For each lemma with a gold-standard novel
sense, we define the automatically-induced novel
sense to be the single sense corresponding to the
maximum in Equation 1. We then compute the
precision, recall, and F-score of this novel sense
with respect to the gold-standard novel sense,
based on the 100 annotated tokens for each of
the 5 lemmas with a novel sense. The results are
shown in the first three numeric columns of Ta-
ble 5.
In the case of export and worm the results are
remarkably good, with precision and recall both
over 0.90. For domain, the low recall is a result of
the majority of usages of the gold-standard novel
sense (?Internet domain?) being split across two
induced senses ? the top-two highest ranked in-
duced senses according to Equation 1. The poor
performance for poster is unsurprising due to the
very low frequency of this lemma?s gold-standard
novel sense.
These results are based on our novelty rank-
ing method (?Nov?), and the assumption that
the novel sense will be represented in a single
topic. To evaluate the theoretical upper-bound
for a topic-ranking method which uses our HDP-
based WSI method and selects a single topic to
capture the novel sense, we next evaluate an op-
timal topic selection approach. In the middle
three numeric columns of Table 5, we present re-
sults for an experimental setup in which the sin-
gle best induced sense ? in terms of F-score ?
is selected as the novel sense by an oracle. We
see big improvements in F-score for domain and
poster. This encouraging result suggests refining
the sense selection heuristic could theoretically
improve our method for identifying novel senses,
and that the topic modelling approach proposed
in this paper has considerable promise for auto-
matic novel sense detection. Of particular note is
the result for poster: although the gold-standard
novel sense of poster is rare, all of its usages are
grouped into a single topic.
Finally, we consider whether an oracle which
can select the best subset of induced senses ? in
terms of F-score ? as the novel sense could of-
fer further improvements. In this case ? results
shown in the final three columns of Table 5 ?
we again see an increase in F-score to 0.92 for
domain. For this lemma the gold-standard novel
sense usages were split across multiple induced
topics, and so we are unsurprised to find that a
method which is able to select multiple topics as
the novel sense performs well. Based on these
findings, in future work we plan to consider alter-
native formulations of novelty.
5 Conclusion
We propose the application of topic modelling
to the task of word sense induction (WSI), start-
ing with a simple LDA-based methodology with
a fixed number of senses, and culminating in
a nonparametric method based on a Hierarchi-
cal Dirichlet Process (HDP), which automatically
learns the number of senses for a given target
word. Our HDP-based method outperforms all
methods over the SemEval-2010WSI dataset, and
is also superior to other topic modelling-based
approaches to WSI based on the SemEval-2007
dataset. We applied the proposed WSI model to
the task of identifying words which have taken on
new senses, including identifying the token oc-
currences of the new word sense. Over a small
dataset developed in this research, we achieved
highly encouraging results.
599
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12, Prague, Czech Re-
public.
John Ayto. 2006. Movers and Shakers: A Chronology
of Words that Shaped our Age. Oxford University
Press, Oxford.
David Bamman and Gregory Crane. 2011. Measur-
ing historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Dig-
ital Libraries (JCDL 2011), pages 1?10, Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. pages 103?111, Athens, Greece.
Lou Burnard. 2000. The British National Corpus
Users Reference Guide. Oxford University Com-
puting Services.
Paul Cook and Suzanne Stevenson. 2010. Automat-
ically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2010), pages 28?34, Valletta,
Malta.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
Genoa, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47?54, Mar-
rakech, Morocco.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. pages
233?237.
Kristina Gulordava and Marco Baroni. 2011. A dis-
tributional similarity approach to the detection of
semantic change in the Google Books Ngram cor-
pus. In Proceedings of the GEMS 2011 Workshop
on GEometrical Models of Natural Language Se-
mantics, pages 67?71, Edinburgh, Scotland.
Adam Kilgarriff and David Tugwell. 2002. Sketch-
ing words. In Marie-He?le`ne Corre?ard, editor, Lex-
icography and Natural Language Processing: A
Festschrift in Honour of B. T. S. Atkins, pages 125?
137. Euralex, Grenoble, France.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), pages 3?
10, Whistler, Canada.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
Uoy: Graphs of unambiguous vertices for word
sense induction and disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 355?358, Uppsala, Sweden.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dli-
gach, and Sameer Pradhan. 2010. SemEval-2010
Task 14: Word sense induction & disambiguation.
In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68, Uppsala,
Sweden.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing word senses to improve web search result
clustering. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 116?126, Cambridge, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2007. I2R: Three systems for word sense discrimi-
nation, chinese word sense disambiguation, and en-
glish word sense disambiguation. In Proceedings
of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 177?182,
Prague, Czech Republic.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33:161?199.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Catherine Soanes and Angus Stevenson, editors. 2008.
The Concise Oxford English Dictionary. Oxford
University Press, eleventh (revised) edition. Oxford
Reference Online.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101:1566?
1581.
600
Della Thompson, editor. 1995. The Concise Oxford
Dictionary of Current English. Oxford University
Press, Oxford, ninth edition.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, Oregon.
601
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 530?539,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence
and Topic Model Quality
Jey Han Lau
Dept of Philosophy
King?s College London
jeyhan.lau@gmail.com
David Newman
Google
dnewman@google.com
Timothy Baldwin
Dept of Computing and
Information Systems
The University of Melbourne
tb@ldwin.net
Abstract
Topic models based on latent Dirichlet al-
location and related methods are used in a
range of user-focused tasks including doc-
ument navigation and trend analysis, but
evaluation of the intrinsic quality of the
topic model and topics remains an open
research area. In this work, we explore
the two tasks of automatic evaluation of
single topics and automatic evaluation of
whole topic models, and provide recom-
mendations on the best strategy for per-
forming the two tasks, in addition to pro-
viding an open-source toolkit for topic and
topic model evaluation.
1 Introduction
Topic modelling based on Latent Dirichlet Alloca-
tion (LDA: Blei et al. (2003)) and related methods
is increasingly being used in user-focused tasks, in
contexts such as the evaluation of scientific impact
(McCallum et al., 2006; Hall et al., 2008), trend
analysis (Bolelli et al., 2009; Lau et al., 2012a)
and document search (Wang et al., 2007). The
LDA model is based on the assumption that doc-
ument collections have latent topics, in the form
of a multinomial distribution of words, which is
typically presented to users via its top-N highest-
probability words. In NLP, topic models are gener-
ally used as a means of preprocessing a document
collection, and the topics and per-document topic
allocations are fed into downstream applications
such as document summarisation (Haghighi and
Vanderwende, 2009), novel word sense detection
methods (Lau et al., 2012b) and machine transla-
tion (Zhao and Xing, 2007). In fields such as the
digital humanities, on the other hand, human users
interact directly with the output of topic models. It
is this context of topic modelling for direct human
consumption that we target in this paper.
The topics produced by topic models have a
varying degree of human-interpretability. To il-
lustrate this, we present two topics automatically
learnt from a collection of news articles:
1. ?farmers, farm, food, rice, agriculture?
2. ?stories, undated, receive, scheduled, clients?
The first topic is clearly related to agriculture.
The subject of the second topic, however, is less
clear, and may confuse users if presented to them
as part of a larger topic model. Measuring the
human-interpretability of topics and the overall
topic model is the core topic of this paper.
Various methodologies have been proposed for
measuring the semantic interpretability of topics.
In Chang et al. (2009), the authors proposed an
indirect approach based on word intrusion, where
?intruder words? are randomly injected into topics
and human users are asked to identify the intruder
words. The word intrusion task builds on the as-
sumption that the intruder words are more iden-
tifiable in coherent topics than in incoherent top-
ics, and thus the interpretability of a topic can be
estimated by measuring how readily the intruder
words can be manually identified by annotators.
Since its inception, the method of Chang et
al. (2009) has been used variously as a means
of assessing topic models (Paul and Girju, 2010;
Reisinger et al., 2010; Hall et al., 2012). Despite
its wide acceptance, the method relies on manual
annotation and has never been automated. This is
one of the primary contributions of this work: the
demonstration that we can automate the method of
Chang et al. (2009) at near-human levels of accu-
racy, as a result of which we can perform auto-
matic evaluation of the human-interpretability of
topics, as well as topic models.
There has been prior work to directly estimate
the human-interpretability of topics through au-
tomatic means. For example, Newman et al.
530
(2010) introduced the notion of topic ?coher-
ence?, and proposed an automatic method for es-
timating topic coherence based on pairwise point-
wise mutual information (PMI) between the topic
words. Mimno et al. (2011) similarly introduced
a methodology for computing coherence, replac-
ing PMI with log conditional probability. Musat
et al. (2011) incorporated the WordNet hierarchy
to capture the relevance of topics, and in Aletras
and Stevenson (2013a), the authors proposed the
use of distributional similarity for computing the
pairwise association of the topic words. One ap-
plication of these methods has been to remove in-
coherent topics before generating labels for topics
(Lau et al., 2011; Aletras and Stevenson, 2013b).
Ultimately, all these methodologies, and also
the word intrusion approach, attempt to assess the
same quality: the human-interpretability of top-
ics. The relationship between these methodolo-
gies, however, is poorly understood, and there is
no consensus on what is the best approach for
computing the semantic interpretability of topic
models. This is a second contribution of this pa-
per: we perform a systematic empirical compar-
ison of the different methods and find apprecia-
ble differences between them. We further go on to
propose an improved formulation of Newman et
al. (2010) based on normalised PMI. Finally, we
release a toolkit which implements the topic inter-
pretability measures described in this paper.
2 Related Work
Chang et al. (2009) challenged the conventional
wisdom that held-out likelihood ? often com-
puted as the perplexity of test data or unseen doc-
uments ? is the only way to evaluate topic mod-
els. To measure the human-interpretability of top-
ics, the authors proposed a word intrusion task
and conducted experiments using three topic mod-
els: Latent Dirichlet Allocation (LDA: Blei et al.
(2003)), Probabilistic Latent Semantic Indexing
(PLSI: Hofmann (1999)) and the Correlated Topic
Model (CTM: Blei and Lafferty (2005)). Contrary
to expectation, they found that perplexity corre-
lates negatively with topic interpretability.
In the word intrusion task, each topic is pre-
sented as a list of six words ? the five most proba-
ble topic words and a randomly-selected ?intruder
word?, which has low probability in the topic of
interest, but high probability in other topics ?
and human users are asked to identify the intruder
word that does not belong to the topic in question.
Newman et al. (2010) capture topic inter-
pretability using a more direct approach, by asking
human users to rate topics (represented by their
top-10 topic words) on a 3-point scale based on
how coherent the topic words are (i.e. their ob-
served coherence). They proposed several ways of
automating the estimation of the observed coher-
ence, and ultimately found that a simple method
based on PMI term co-occurrence within a sliding
context window over English Wikipedia produces
the consistently best result, nearing levels of inter-
annotator agreement over topics learnt from two
distinct document collections.
Mimno et al. (2011) proposed a closely-related
method for evaluating semantic coherence, replac-
ing PMI with log conditional probability. Rather
than using Wikipedia for sampling the word co-
occurrence counts, Mimno et al. (2011) used the
topic-modelled documents, and found that their
measure correlates well with human judgements
of observed coherence (where topics were rated
in the same manner as Newman et al. (2010),
based on a 3-point ordinal scale). To incorpo-
rate the evaluation of semantic coherence into the
topic model, the authors proposed to record words
that co-occur together frequently, and update the
counts of all associated words before and after the
sampling of a new topic assignment in the Gibbs
sampler. This variant of topic model was shown to
produce more coherent topics than LDA based on
the log conditional probability coherence measure.
Aletras and Stevenson (2013a) introduced dis-
tributional semantic similarity methods for com-
puting coherence, calculating the distributional
similarity between semantic vectors for the top-N
topic words using a range of distributional similar-
ity measures such as cosine similarity and the Dice
coefficient. To construct the semantic vector space
for the topic words, they used English Wikipedia
as the reference corpus, and collected words that
co-occur in a window of ?5 words. They showed
that their method correlates well with the observed
coherence rated by human judges.
3 Dataset
As one of the primary foci of this paper is the au-
tomation of the intruder word task of Chang et
al. (2009), our primary dataset is that used in the
original paper by Chang et al. (2009), which pro-
vides topics and human annotations for a range of
531
domains and topic model types. In the dataset,
two text collections were used: (1) 10,000 articles
from English Wikipedia (WIKI); and (2) 8,447 arti-
cles from the New York Times dating from 1987 to
2007 (NEWS). For each document collection, top-
ics were generated by three topic modelling meth-
ods: LDA, PLSI and CTM (see Section 2). For
each topic model, three settings of T (the num-
ber of topics) were used: T = 50, T = 100
and T = 150. In total, there were 9 topic mod-
els (3 models ? 3 T ) and 900 topics (3 models ?
(50 + 100 + 150)) for each dataset.
1
For some of topic interpretability estimation
methods, we require a reference corpus to sam-
ple lexical probabilities. We use two reference
corpora: (1) NEWS-FULL, which contains 1.2 mil-
lion New York Times articles from 1994 to 2004
(from the English Gigaword); and (2) WIKI-FULL,
which contains 3.3 million English Wikipedia ar-
ticles (retrieved November 28th 2009).
2
The ratio-
nale for choosing the New York Times and English
Wikipedia as the reference corpora is to ensure do-
main consistency with the word intrusion dataset;
the full collections are used to more robustly esti-
mate lexical probabilities.
4 Human-Interpretability at the Model
Level
In this section, we evaluate measures for estimat-
ing human-interpretability at the model level. That
is, for a measure ? human-judged or automated
? we first aggregate its coherence/interpretability
scores for all topics from a given topic model to
obtain the topic model?s average coherence score.
We then calculate the Pearson correlation coeffi-
cients between the two measures using the topic
models? average coherence scores. In summary,
the correlation is computed over nine sets of top-
ics (3 topic modellers ? 3 settings of T ) for each
of WIKI and NEWS.
4.1 Indirect Approach: Word Intrusion
The word intrusion task measures topic inter-
pretability indirectly, by computing the fraction
of annotators who successfully identify the in-
truder word. A limitation of the word intrusion
1
In the WIKI topics there were corrupted symbols in the
topic words for 24 topics. We removed these topics, reducing
the total number of topics to 876.
2
For both corpora we perform tokenisation and POS tag-
ging using OpenNLP and lemmatisation using Morpha (Min-
nen et al., 2001).
task is that it requires human annotations, there-
fore preventing large-scale evaluation. We begin
by proposing a methodology to fully automate the
word intrusion task.
Lau et al. (2010) proposed a methodology that
learns the most representative or best topic word
that summarises the semantics of the topic. Ob-
serving that the word intrusion task ? the task
of detecting the least representative word ? is
the converse of the best topic word selection task,
we adapt their methodology to automatically iden-
tify the intruder word for the word intrusion task,
based on the knowledge that there is a unique in-
truder word per topic.
The methodology works as follows: given a set
of topics (including intruder words), we compute
the word association features for each of the top-
N topic words of a topic,
3
and combine the fea-
tures in a ranking support vector regression model
(SVM
rank
: Joachims (2006)) to learn the intruder
words. Following Lau et al. (2010), we use three
word association measures:
PMI(w
i
) =
N?1
?
j
log
P (w
i
, w
j
)
P (w
i
)P (w
j
)
CP1(w
i
) =
N?1
?
j
P (w
i
, w
j
)
P (w
j
)
CP2(w
i
) =
N?1
?
j
P (w
i
, w
j
)
P (w
i
)
We additionally experiment with normalised
pointwise mutual information (NPMI: Bouma
(2009)):
NPMI(w
i
) =
N?1
?
j
log
P (w
i
,w
j
)
P (w
i
)P (w
j
)
? logP (w
i
, w
j
)
In the dataset of Chang et al. (2009) (see Sec-
tion 3), each topic was presented to 8 annota-
tors, with small variations in the displayed topic
words (including the intruder word) for each an-
notator. That is, each topic has essentially 8 subtly
different representations. To measure topic inter-
pretability, the authors defined ?model precision?:
the relative success of human annotators at identi-
fying the intruder word, across all representations
of the different topics. The model precision scores
produced by human judges are henceforth referred
to as WI-Human, and the scores produced by our
3
N is the number of topic words displayed to the human
users in the word intrusion task, including the intruder word.
532
Topic Ref. Pearson?s r with WI-Human
Domain Corpus WI-Auto-PMI WI-Auto-NPMI
WIKI
WIKI-FULL 0.947 0.936
NEWS-FULL 0.801 0.835
NEWS
NEWS-FULL 0.913 0.831
WIKI-FULL 0.811 0.750
Table 1: Pearson correlation of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the model level.
automated method for the PMI and NPMI vari-
ants as WI-Auto-PMI and WI-Auto-NPMI respec-
tively.
4
The Pearson correlation coefficients between
WI-Human and WI-Auto-PMI/WI-Auto-NPMI at
the model level are presented in Table 1. Note
that our two reference corpora are used to inde-
pendently sample the lexical probabilities for the
word association features.
We see very strong correlation for in-domain
pairings (i.e. WIKI+WIKI-FULL and NEWS+NEWS-
FULL), achieving r > 0.9 in most cases for both
WI-Auto-PMI or WI-Auto-NPMI, demonstrating
the effectiveness of our methodology at automat-
ing the word intrusion task for estimating human-
interpretability at the model level. Overall, WI-
Auto-PMI outperforms WI-Auto-NPMI.
Note that although our proposed methodology
is supervised, as intruder words are synthetically
generated and no annotation is needed for the su-
pervised learning, the whole process of computing
topic coherence via word intrusion is fully auto-
matic, without the need for hand-labelled training
data.
4.2 Direct Approach: Observed Coherence
Newman et al. (2010) defined topic interpretabil-
ity based on a more direct approach, by asking hu-
man judges to rate topics based on the observed
coherence of the top-N topic words, and various
methodologies have since been proposed to auto-
mate the computation of the observed coherence.
In this section, we present all these methods and
compare them.
The word intrusion dataset is not annotated with
human ratings of observed coherence. To cre-
ate gold-standard coherence judgements, we used
Amazon Mechanical Turk:
5
we presented the top-
ics (with intruder words removed) to the Turkers
and asked them to rate the topics using on a 3-point
4
Note that both variants use CP1 and CP2 features, i.e.
WI-Auto-PMI uses PMI+CP1+C2 while WI-Auto-NPMI
uses NPMI+CP1+C2 features.
5
https://www.mturk.com/mturk/
ordinal scale, following Newman et al. (2010). In
total, we collected six to fourteen annotations per
topic (an average of 8.4 annotations per topic).
The observed coherence of a topic is computed
as the arithmetic mean of the annotators? ratings,
once again following Newman et al. (2010). The
human-judged observed topic coherence is hence-
forth referred to as OC-Human.
For the automated methods, we experimented
with the following methods for estimating the
human-interpretability of a topic t:
1. OC-Auto-PMI: Pairwise PMI of top-N
topic words (Newman et al., 2010):
OC-Auto-PMI(t) =
N
?
j=2
j?1
?
i=1
log
P (w
j
, w
i
)
P (w
i
)P (w
j
)
2. OC-Auto-NPMI: NPMI variant of OC-
Auto-PMI:
OC-Auto-NPMI(t) =
N
?
j=2
j?1
?
i=1
log
P (w
j
,w
i
)
P (w
i
)P (w
j
)
? logP (w
i
, w
j
)
3. OC-Auto-LCP: Pairwise log conditional
probability of top-N topic words (Mimno et
al., 2011):
6
OC-Auto-LCP(t) =
N
?
j=2
j?1
?
i=1
log
P (w
j
, w
i
)
P (w
i
)
4. OC-Auto-DS: Pairwise distributional simi-
larity of the top-N topic words, as described
in Aletras and Stevenson (2013a).
For OC-Auto-PMI, OC-Auto-NPMI and OC-
Auto-LCP, all topics are lemmatised and intruder
words are removed before coherence is com-
puted.
7
In-domain and cross-domain pairings of
6
Although the original method uses the topic-modelled
document collection and document co-occurrence for sam-
pling word counts, for a fairer comparison we use log condi-
tional probability only as a replacement to the PMI compo-
nent of the coherence computation (i.e. words are still sam-
pled using a reference corpus and a sliding window). For ad-
ditional evidence that the original method performs at a sub-
par level, see Lau et al. (2013) and Aletras and Stevenson
(2013a).
7
We once again use Morpha to do the lemmatisation, and
determine POS via the majority POS for a given word, aggre-
gated over all its occurrences in English Wikipedia.
533
Topic Ref. Pearson?s r with OC-Human
Domain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI
WIKI-FULL 0.490 0.903 0.959
0.859
NEWS-FULL 0.696 0.844 0.913
NEWS
NEWS-FULL 0.965 0.979 0.887
0.941
WIKI-FULL 0.931 0.964 0.872
Table 2: Pearson correlation of OC-Human and the automated methods ? OC-Auto-PMI, OC-Auto-
NPMI, OC-Auto-LCP and OC-Auto-DS ? at the model level.
the topic domain and reference corpus are experi-
mented with for these measures.
For OC-Auto-DS, all topics are lemmatised, in-
truder words are removed and English Wikipedia
is used to generate the vector space for the topic
words. The size of the context window is set to
?5 word (i.e. 5 words to either side of the tar-
get word). We use PMI to weight the vectors,
cosine similarity for measuring the distributional
similarity between the top-N topic words, and the
?Topic Word Space? approach to reduce the di-
mensionality of the vector space. A complete de-
scription of the parameters can be found in Aletras
and Stevenson (2013a). Note that cross-domain
pairings of the topic domain and reference corpus
are not tested: in line with the original paper, we
use only English Wikipedia to generate the vector
space before distributional similarity.
We present the Pearson correlation coefficient
of OC-Human and the four automated methods at
the model level in Table 2. For OC-Auto-NPMI,
OC-Auto-LCP and OC-Auto-DS, we see that they
correlate strongly with the human-judged coher-
ence. Overall, OC-Auto-NPMI has the best per-
formance among the methods, and in-domain pair-
ings generally produce the best results for OC-
Auto-NPMI and OC-Auto-LCP. The results are
comparable to those for the automated intruder
word detection method in Section 4.1.
The non-normalised variant OC-Auto-PMI cor-
relates well for NEWS but performs poorly for WIKI,
producing a correlation of only 0.490 for the in-
domain pairing. We revisit this in Section 6, and
provide a qualitative analysis to explain the dis-
crepancy in results between OC-Auto-PMI and
OC-Auto-NPMI.
4.3 Word Intrusion vs. Observed Coherence
In the previous sections, we showed for both the
direct and indirect approaches that the automated
methods correlate strongly with the manually-
annotated human-interpretability of topics at the
model level (with the exception of OC-Auto-PMI).
One question that remains unanswered, however,
is whether word intrusion measures topic inter-
pretability differently to observed coherence. This
is the focus of this section.
From the results in Table 3 for the intruder
word model vs. observed coherence, we see a
strong correlation between WI-Human and OC-
Human. This observation is insightful: it shows
that the topic interpretability estimated by the two
approaches is almost identical at the model level.
Between WI-Human and the observed coher-
ence methods automated methods, overall we see
a strong correlation for the OC-Auto-NPMI, OC-
Auto-LCP and OC-Auto-DS methods. OC-Auto-
PMI once again performs poorly over WIKI, but
this is unsurprising given its previous results (i.e.
its poor correlation with OC-Human). In-domain
pairings tend to perform better, and the per-
formance of OC-Auto-NPMI, OC-Auto-LCP and
OC-Auto-DS is comparable, with no one clearly
best method.
5 Human-Interpretability at the Topic
Level
In this section, we evaluate the various methods
at the topic level. We group together all topics
for each dataset (without distinguishing the topic
models that produce them) and calculate the cor-
relation of one measure against another. That is,
the correlation coefficient is computed for 900 top-
ics/data points in the case of each of WIKI and
NEWS.
5.1 Indirect Approach: Word Intrusion
In Section 4.1, we proposed a novel methodol-
ogy to automate the word intrusion task (WI-Auto-
PMI and WI-Auto-NPMI). We now evaluate its
performance at the topic level, and present its
correlation with the human gold standard (WI-
Human) in Table 4.
The correlation of WI-Human and WI-Auto-
PMI/WI-Auto-NPMI at the topic level is consid-
erably worse, compared to its results at the model
534
Topic Ref. Pearson?s r with WI-Human
Domain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI
WIKI-FULL
0.900
0.638 0.927 0.911
0.907
NEWS-FULL 0.614 0.757 0.821
NEWS
NEWS-FULL
0.915
0.865 0.866 0.867
0.925
WIKI-FULL 0.838 0.874 0.893
Table 3: Word intrusion vs. observed coherence: Pearson correlation coefficient at the model level.
Topic Ref. Pearson?s r with WI-Human Human
Domain Corpus WI-Auto-PMI WI-Auto-NPMI Agreement
WIKI
WIKI-FULL 0.554 0.573
0.735
NEWS-FULL 0.622 0.592
NEWS
NEWS-FULL 0.602 0.612
0.770
WIKI-FULL 0.638 0.648
Table 4: Pearson correlation coefficient of WI-Human and WI-Auto-PMI/WI-Auto-NPMI at the topic
level.
level (Table 1). The performance between WI-
Auto-PMI and WI-Auto-NPMI is not very differ-
ent, and the cross-domain pairing slightly outper-
forms the in-domain pairing.
To better understand the difficulty of the task,
we compute the agreement between human anno-
tators by calculating the Pearson correlation co-
efficient of model precisions produced by ran-
domised sub-group pairs in the topics.
8
That is, for
each topic, we randomly split the annotations into
two sub-groups, and compute the Pearson correla-
tion coefficient of the model precisions produced
by the first sub-group and that of the second sub-
group.
The original dataset has 8 annotations per topic.
Splitting the annotations into two sub-groups re-
duces the number of annotations to 4 per group,
which is not ideal for computing model precision.
We thus chose to expand the number of annota-
tions by sampling 300 random topics from each
domain (for a total of 600 topics) and following
the same process as Chang et al. (2009) to get in-
truder word annotations using Amazon Mechani-
cal Turk. On average, we obtained 11.7 additional
annotations per topic for these 600 topics. The hu-
man agreement scores (i.e. the Pearson correlation
coefficient of randomised sub-group pairs) for the
sampled 600 topics are presented in the last col-
umn of Table 4.
The sub-group correlation is around r = 0.75
for the topics from both datasets. As such, esti-
mating topic interpretability at the topic level is a
much harder task than model-level evaluation. Our
automated methods perform at a highly credible
8
To counter for the fact that annotators labelled varying
numbers of topics.
r = 0.6, but there is certainly room for improve-
ment. Note that the correlation values reported in
Newman et al. (2010) are markedly higher than
ours, as they evaluated based on Spearman rank
correlation, which isn?t attuned to the relative dif-
ferences in coherence values and returns higher
values for the task.
5.2 Direct Approach: Observed Coherence
We repeat the experiments of observed coherence
in Section 4.2, and evaluate the correlation of
the automated methods (OC-Auto-PMI, OC-Auto-
NPMI, OC-Auto-LCP and OC-Auto-DS) on the
human gold standard (OC-Human) at the topic
level. Results are summarised in Table 5.
OC-Auto-PMI performs poorly at the topic
level in the WIKI domain, similar to what was
seen at the model level in Section 4.2. Over-
all, both OC-Auto-NPMI and OC-Auto-DS are the
most consistent methods. OC-Auto-LCP performs
markedly worse than these two methods.
To get a better understanding of how well hu-
man annotators perform at the task, we compute
the one-vs-rest Pearson correlation coefficient us-
ing the gold standard annotations. That is, for
each topic, we single out each rating/annotation
and compare it to the average of all other rat-
ings/annotations. The one-vs-rest correlation re-
sult is displayed in the last column (titled ?Hu-
man Agreement?) in Table 5. The best auto-
mated methods surpass the single-annotator per-
formance, indicating that they are able to per-
form the task as well as human annotators (unlike
the topic-level results for the word intrusion task
where humans were markedly better at the task
than the automated methods).
535
Topic Ref. Pearson?s r with OC-Human Human
Domain Corpus OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS Agreement
WIKI
WIKI-FULL 0.533 0.638 0.579
0.682 0.624
NEWS-FULL 0.582 0.667 0.496
NEWS
NEWS-FULL 0.719 0.741 0.471
0.682 0.634
WIKI-FULL 0.671 0.722 0.452
Table 5: Pearson correlation of OC-Human and the automated methods at the topic level.
Topic Ref. Pearson?s r with WI-Human
Domain Corpus OC-Human OC-Auto-PMI OC-Auto-NPMI OC-Auto-LCP OC-Auto-DS
WIKI
WIKI-FULL
0.665
0.472 0.557 0.547
0.639
NEWS-FULL 0.504 0.571 0.455
NEWS
NEWS-FULL
0.641
0.629 0.634 0.407
0.649
WIKI-FULL 0.604 0.633 0.390
Table 6: Word intrusion vs. observed coherence: pearson correlation results at the topic level.
5.3 Word Intrusion vs. Observed Coherence
In this section, we bring together the indirect ap-
proach of word intrusion and the direct approach
of observed coherence, and evaluate them against
each other at the topic level. Results are sum-
marised in Table 6.
We see that the correlation between the human
ratings of intruder words and observed coherence
is only modest, implying that there are topic-level
differences in the output of the two approaches. In
Section 6, we provide a qualitative analysis and
explanation as to what constitutes the differences
between the approaches.
For the automated methods, OC-Auto-DS has
the best performance, with OC-Auto-NPMI per-
forming relatively well (in particularly in the NEWS
domain).
6 Discussion
Normalised PMI (NPMI) was first introduced by
Bouma (2009) as a means of reducing the bias for
PMI towards words of lower frequency, in addition
to providing a standardised range of [?1, 1] for the
calculated values.
We introduced NPMI to the automated meth-
ods of word intrusion (WI-Auto-NPMI) and ob-
served coherence (OC-Auto-NPMI) to explore its
suitability for the task. For the latter, we saw
that NPMI achieves markedly higher correlation
than OC-Human (in particular, at the model level).
To better understand the impact of normalisation,
we inspected a list of WIKI topics that have simi-
lar scores for OC-Human and OC-Auto-NPMI but
very different OC-Auto-PMI scores. A sample of
these topics is presented in Table 7. WIKI-FULL
is used as the reference corpus for computing the
scores. Note that the presented OC-Auto-NPMI*
and OC-Auto-PMI* scores are post-normalised to
the range [0, 1] for ease of interpretation. To give
a sense of how readily these topic words occur in
the reference corpus, we additionally display the
frequency of the first topic word in the reference
corpus (last column).
All topics presented have an OC-Human score
of 3.0 (i.e. these topics are rated as being very co-
herent by human judges) and similar OC-Auto-
NPMI values. Their OC-Auto-PMI scores, how-
ever, are very different between the top-3 and
bottom-3 topics. The bias of PMI towards lower
frequency words is clear: topic words that occur
frequently in the corpus receive a lower OC-Auto-
PMI score compared to those that occur less fre-
quently, even though the human-judged observed
coherence is the same. OC-Auto-NPMI on the
other hand, correctly estimates the coherence.
We observed, however, that the impact of nor-
malising PMI is less in the word intrusion task.
One possible explanation is that for the automated
methods WI-Auto-PMI and WI-Auto-NPMI, the
PMI/NPMI scores are used indirectly as a feature
to a machine learning framework, and the bias
could be reduced/compensated by other features.
On the subject of the difference between ob-
served coherence and word intrusion in estimat-
ing topic interpretability, we observed that WI-
Human and OC-Human correlate only moderately
(r ? 0.6) at the topic level (Table 6). To better
understand this effect, we manually analysed top-
ics that have differing WI-Human and OC-Human
scores. A sample of topics with high divergence
in estimated coherence score is given in Table 8.
As before, the presented the OC-Human* and WI-
536
Topic
OC- OC- OC- Word
Human Auto-NPMI* Auto-PMI* Count
cell hormone insulin muscle receptor 3.0 0.59 0.61 #(cell) = 1.1M
electron laser magnetic voltage wavelength 3.0 0.52 0.54 #(electron) = 0.3M
magnetic neutrino particle quantum universe 3.0 0.55 0.55 #(magnetic) = 0.4M
album band music release song 3.0 0.56 0.37 #(album) = 12.5M
college education school student university 3.0 0.57 0.38 #(college) = 9.8M
city county district population town 3.0 0.52 0.34 #(city) = 22.0M
Table 7: A list of WIKI topics to illustrate the impact of NPMI.
Topic # Topic OC-Human* WI-Human*
1 business company corporation cluster loch shareholder 0.94 0.25
2 song actor clown play role theatre 1.00 0.50
3 census ethnic female male population village 0.92 0.25
4 composer singer jazz music opera piano 1.00 0.63
5 choice count give i.e. simply unionist 0.14 1.00
6 digital clown friend love mother wife 0.17 1.00
Table 8: A list of WIKI topics to illustrate the difference between observed coherence and word intrusion.
Boxes denote human chosen intruder words, and boldface denotes true intruder words.
Human* scores in the table are post-normalised to
the range [0, 1] for ease of comparison.
In general, there are two reasons for topics to
have high OC-Human and low WI-Human scores.
First, if a topic has an outlier word that is mildly
related to the topic, users tend to choose this word
as the intruder word in the word intrusion task,
yielding a low WI-Human score. If they are asked
to rate the observed coherence, however, the single
outlier word often does not affect its overall coher-
ence, resulting in a high OC-Human score. This is
observed in topics 1 and 2 in Table 8, where loch
and clown are chosen by annotators in the word in-
trusion task, as they detract from the semantics of
the topic. This results in low WI-Human scores,
but high observed coherence scores (OC-Human).
The second reason is the random selection of
intruder words related to the original topic. We
see this in topics 3 and 4, where related intruder
words (village and singer) were selected.
For topics with low OC-Human and high WI-
Human scores, the true intruder words are often
very different to the domain/focus of other topic
words. As such, annotators are consistently able
to single them out to yield high WI-Human scores,
even though the topic as a whole is not coherent.
Topics 5 and 6 in Table 8 exhibit this.
All topic evaluation measures described in this
paper are implemented in an open-source toolkit.
9
9
https://github.com/jhlau/topic_
interpretability
7 Conclusion
In this paper, we examined various methodologies
that estimate the semantic interpretability of top-
ics, at two levels: the model level and the topic
level. We looked first at the word intrusion task
proposed by Chang et al. (2009), and proposed
a method that fully automates the task. Next we
turned to observed coherence, a more direct ap-
proach to estimate topic interpretability. At the
model level, results were very positive for both the
word intrusion and observed coherence methods.
At the topic level, however, the results were more
mixed. For observed coherence, our best methods
(OC-Auto-NPMI and OC-Auto-DS) were able to
emulate human performance. For word intrusion,
the automated methods were slightly below human
performance, with some room for improvement.
We finally observed that there are systematic dif-
ferences in the topic-level scores derived from the
two task formulations.
Acknowledgements
This work was supported in part by the Australian
Research Council, and for author JHL, also partly
funded by grant ES/J022969/1 from the Economic
and Social Research Council of the UK. The au-
thors acknowledge the generosity of Nikos Ale-
tras and Mark Stevenson in providing their code
for OC-Auto-DS, and Jordan Boyd-Graber in pro-
viding the data used in Chang et al. (2009).
537
References
N. Aletras and M. Stevenson. 2013a. Evaluating
topic coherence using distributional semantics. In
Proceedings of the Tenth International Workshop on
Computational Semantics (IWCS-10), pages 13?22,
Potsdam, Germany.
N. Aletras and M. Stevenson. 2013b. Representing
topics using images. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL HLT 2013), pages
158?167, Atlanta, USA.
D. Blei and J. Lafferty. 2005. Correlated topic mod-
els. In Advances in Neural Information Processing
Systems 17 (NIPS-05), pages 147?154, Vancouver,
Canada.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet allocation. Journal of Machine Learning
Research, 3:993?1022.
L. Bolelli, S?. Ertekin, and C.L. Giles. 2009. Topic
and trend detection in text collections using Latent
Dirichlet Allocation. In Proceedings of ECIR 2009,
pages 776?780, Toulouse, France.
G. Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of the Biennial GSCL Conference, pages 31?40,
Potsdam, Germany.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans
interpret topic models. In Advances in Neural In-
formation Processing Systems 21 (NIPS-09), pages
288?296, Vancouver, Canada.
A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics ? Human
Language Technologies 2009 (NAACL HLT 2009),
pages 362?370.
D. Hall, D. Jurafsky, and C.D. Manning. 2008. Study-
ing the history of ideas using topic models. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 363?371, Honolulu, USA.
M. Hall, P. Clough, and M. Stevenson. 2012. Evalu-
ating the use of clustering for automatically organ-
ising digital library collections. In Proceedings of
the Second International Conference on Theory and
Practice of Digital Libraries, pages 323?334, Pa-
phos, Cyprus.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In Proceedings of 22nd International ACM-
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR?99), pages 50?57,
Berkeley, USA.
T. Joachims. 2006. Training linear SVMs in linear
time. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD 2006), Philadelphia, USA.
J.H. Lau, D. Newman, S. Karimi, and T. Baldwin.
2010. Best topic word selection for topic labelling.
In Proceedings of the 23rd International Confer-
ence on Computational Linguistics (COLING 2010),
Posters Volume, pages 605?613, Beijing, China.
J.H. Lau, K. Grieser, D. Newman, and T. Baldwin.
2011. Automatic labelling of topic models. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL HLT 2011), pages 1536?
1545, Portland, USA.
J.H. Lau, N. Collier, and T. Baldwin. 2012a. On-
line trend analysis with topic models: #twitter
trends detection topic model online. In Proceedings
of the 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1519?
1534, Mumbai, India.
J.H. Lau, P. Cook, D. McCarthy, D. Newman, and
T. Baldwin. 2012b. Word sense induction for novel
sense detection. In Proceedings of the 13th Con-
ference of the EACL (EACL 2012), pages 591?601,
Avignon, France.
J.H. Lau, T. Baldwin, and D. Newman. 2013. On
collocations and topic models. ACM Transactions
on Speech and Language Processing, 10(3):10:1?
10:14.
A McCallum, G.S. Mann, and D Mimno. 2006. Bib-
liometric impact measures leveraging topic analysis.
In Proceedings of the 6th ACM/IEEE-CS Joint Con-
ference on Digital Libraries 2006 (JCDL?06), pages
65?74, Chapel Hill, USA.
D. Mimno, H. Wallach, E. Talley, M. Leenders, and
A. McCallum. 2011. Optimizing semantic coher-
ence in topic models. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2011), pages 262?272,
Edinburgh, UK.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207?223.
C. Musat, J. Velcin, S. Trausan-Matu, and M.A. Rizoiu.
2011. Improving topic evaluation using concep-
tual knowledge. In Proceedings of the 22nd Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-2011), pages 1866?1871, Barcelona, Spain.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin.
2010. Automatic evaluation of topic coherence.
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL HLT 2010), pages 100?108, Los
Angeles, USA.
538
M. Paul and R. Girju. 2010. A two-dimensional topic-
aspect model for discovering multi-faceted topics.
In Proceedings of the 24th Annual Conference on
Artificial Intelligence (AAAI-10), Atlanta, USA.
J. Reisinger, A. Waters, B. Silverthorn, and R.J.
Mooney. 2010. Spherical topic models. In Proceed-
ings of the 27th International Conference on Ma-
chine Learning (ICML 2010), pages 903?910, Haifa,
Israel.
X. Wang, A. McCallum, and X. Wei. 2007. Topical
n-grams: Phrase and topic discovery, with an ap-
plication to information retrieval. In Proceedings
of the Seventh IEEE International Conference on
Data Mining (ICDM 2007), pages 697?702, Omaha,
USA.
B. Zhao and E.P. Xing. 2007. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and transla-
tion. In Advances in Neural Information Processing
Systems (NIPS 2007), pages 1689?1696, Vancouver,
Canada.
539
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 100?108,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Topic Coherence
David Newman,?? Jey Han Lau,? Karl Grieser?, and Timothy Baldwin,??
? NICTA Victoria Research Laboratory, Australia
? Dept of Computer Science, University of California, Irvine
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? Dept of Information Systems, University of Melbourne, Australia
newman@uci.edu, depthchargex@gmail.com,
kgrieser@csse.unimelb.edu.au, tb@ldwin.net
Abstract
This paper introduces the novel task of topic
coherence evaluation, whereby a set of words,
as generated by a topic model, is rated for
coherence or interpretability. We apply a
range of topic scoring models to the evaluation
task, drawing on WordNet, Wikipedia and the
Google search engine, and existing research
on lexical similarity/relatedness. In compar-
ison with human scores for a set of learned
topics over two distinct datasets, we show a
simple co-occurrence measure based on point-
wise mutual information over Wikipedia data
is able to achieve results for the task at or
nearing the level of inter-annotator correla-
tion, and that other Wikipedia-based lexical
relatedness methods also achieve strong re-
sults. Google produces strong, if less consis-
tent, results, while our results over WordNet
are patchy at best.
1 Introduction
There has traditionally been strong interest within
computational linguistics in techniques for learning
sets of words (aka topics) which capture the latent
semantics of a document or document collection, in
the form of methods such as latent semantic analysis
(Deerwester et al, 1990), probabilistic latent seman-
tic analysis (Hofmann, 2001), random projection
(Widdows and Ferraro, 2008), and more recently, la-
tent Dirichlet alocation (Blei et al, 2003; Griffiths
and Steyvers, 2004). Such methods have been suc-
cessfully applied to a myriad of tasks including word
sense discrimination (Brody and Lapata, 2009), doc-
ument summarisation (Haghighi and Vanderwende,
2009), areal linguistic analysis (Daume III, 2009)
and text segmentation (Sun et al, 2008). In each
case, extrinsic evaluation has been used to demon-
strate the effectiveness of the learned topics in the
application domain, but standardly, no attempt has
been made to perform intrinsic evaluation of the top-
ics themselves, either qualitatively or quantitatively.
In machine learning, on the other hand, researchers
have modified and extended topic models in a vari-
ety of ways, and evaluated intrinsically in terms of
model perplexity (Wallach et al, 2009), but there has
been less effort on qualitative understanding of the
semantic nature of the learned topics.
This research seeks to fill the gap between topic
evaluation in computational linguistics and machine
learning, in developing techniques to perform intrin-
sic qualitative evaluation of learned topics. That
is, we develop methods for evaluating the qual-
ity of a given topic, in terms of its coherence to
a human. After learning topics from a collection
of news articles and a collection of books, we ask
humans to decide whether individual learned top-
ics are coherent, in terms of their interpretability
and association with a single over-arching seman-
tic concept. We then propose models to predict
topic coherence, based on resources such as Word-
Net, Wikipedia and the Google search engine, and
methods ranging from ontological similarity to link
overlap and term co-occurrence. Over topics learned
from two distinct datasets, we demonstrate that there
is remarkable inter-annotator agreement on what is
a coherent topic, and additionally that our methods
based on Wikipedia are able to achieve nearly perfect
agreement with humans over the evaluation of topic
coherence.
This research forms part of a larger research
agenda on the utility of topic modelling in gist-
ing and visualising document collections, and ulti-
mately enhancing search/discovery interfaces over
100
document collections (Newman et al, to appeara).
Evaluating topic coherence is a component of the
larger question of what are good topics, what char-
acteristics of a document collection make it more
amenable to topic modelling, and how can the po-
tential of topic modelling be harnessed for human
consumption (Newman et al, to appearb).
2 Related Work
Most earlier work on intrinsically evaluating learned
topics has been on the basis of perplexity results,
where a model is learned on a collection of train-
ing documents, then the log probability of the un-
seen test documents is computed using that learned
model. Usually perplexity is reported, which is the
inverse of the geometric mean per-word likelihood.
Perplexity is useful for model selection and adjust-
ing parameters (e.g. number of topics T ), and is
the standard way of demonstrating the advantage of
one model over another. Wallach et al (2009) pre-
sented efficient and unbiased methods for computing
perplexity and evaluating almost any type of topic
model.
While statistical evaluation of topic models is
reasonably well understood, there has been much
less work on evaluating the intrinsic semantic qual-
ity of topics learned by topic models, which could
have a far greater impact on the overall value of
topic modeling for end-user applications. Some re-
searchers have started to address this problem, in-
cluding Mei et al (2007) who presented approaches
for automatic labeling of topics (which is core to the
question of coherence and semantic interpretabil-
ity), and Griffiths and Steyvers (2006) who applied
topic models to word sense discrimination tasks.
Misra et al (2008) used topic modelling to identify
semantically incoherent documents within a docu-
ment collection (vs. coherent topics, as targeted in
this research). Chang et al (2009) presented the
first human-evaluation of topic models by creating
a task where humans were asked to identify which
word in a list of five topic words had been ran-
domly switched with a word from another topic.
This work showed some possibly counter-intuitive
results, where in some cases humans preferred mod-
els with higher perplexity. This type of result shows
the need for further exploring measures other than
perplexity for evaluating topic models. In earlier
work, we carried out preliminary experimentation
using pointwise mutual information and Google re-
sults to evaluate topic coherence over the same set
of topics as used in this research (Newman et al,
2009).
Part of this research takes inspiration from the
work on automatic evaluation in machine translation
(Papineni et al, 2002) and automatic summarisation
(Lin, 2004). Here, the development of automated
methods with high correlation with human subjects
has opened the door to large-scale automated evalua-
tion of system outputs, revolutionising the respective
fields. While our aspirations are more modest, the
basic aim is the same: to develop a fully-automated
method for evaluating a well-grounded task, which
achieves near-human correlation.
3 Topic Modelling
In order to evaluate topic modelling, we require a
topic model and set of topics for a given document
collection. While the evaluation methodology we
describe generalises to any method which gener-
ates sets of words, all of our experiments are based
on Latent Dirichlet Allocation (LDA, aka Discrete
Principal Component Analysis), on the grounds that
it is a state-of-the-art method for generating topics.
LDA is a Bayesian graphical model for text docu-
ment collections represented by bags-of-words (see
Blei et al (2003), Griffiths and Steyvers (2004),
Buntine and Jakulin (2004)). In a topic model, each
document in the collection of D documents is mod-
elled as a multinomial distribution over T topics,
where each topic is a multinomial distribution over
W words. Typically, only a small number of words
are important (have high likelihood) in each topic,
and only a small number of topics are present in each
document.
The collapsed Gibbs sampled topic model simul-
taneously learns the topics and the mixture of topics
in documents by iteratively sampling the topic as-
signment z to every word in every document, using
the Gibbs sampling update:
p(zid = t|xid = w, z?id) ?
N?idwt + ?
?
w N?idwt + W?
N?idtd + ?
?
t N?idtd + T?
101
where zid = t is the assignment of the ith word in
document d to topic t, xid = w indicates that the
current observed word is w, and z?id is the vector of
all topic assignments not including the current word.
Nwt represents integer count arrays (with the sub-
scripts denoting what is counted), and ? and ? are
Dirichlet priors.
The maximum a posterior (MAP) estimates of the
topics p(w|t), t = 1 . . . T are given by:
p(w|t) = Nwt + ??
w Nwt + W?
We will follow the convention of representing a
topic via its top-n words, ordered by p(w|t). Here,
we use the top-ten words, as they usually provide
sufficient detail to convey the subject of a topic,
and distinguish one topic from another. For the
remainder of this paper, we will refer to individ-
ual topics by its list of top-ten words, denoted by
w = (w1, . . . , w10).
4 Topic Evaluation Methods
We experiment with scoring methods based on
WordNet (Section 4.1), Wikipedia (Section 4.2) and
the Google search engine (Section 4.3). In the case
of Google, we query for the entire topic, but with
WordNet and Wikipedia, this takes the form of scor-
ing each word-pair in a given topic w based on the
component words (w1, . . . , w10). Given some (sym-
metric) word-similarity measure D(wi, wj), two
straightforward ways of producing a combined score
from the 45 (i.e.
(10
2
)
) word-pair scores are: (1) the
arithmetic mean, and (2) the median, as follows:
Mean-D-Score(w) =
mean{D(wi, wj), ij ? 1 . . . 10, i < j}
Median-D-Score(w) =
median{D(wi, wj), ij ? 1 . . . 10, i < j}
Intuitively, the median seems the more natural rep-
resentation, as it is less affected by outlier scores,
but we experiment with both, and fall back to empir-
ical verification of which is the better combination
method.
4.1 WordNet similarity
WordNet (Fellbaum, 1998) is a lexical ontology
that represents word sense via ?synsets?, which
are structured in a hypernym/hyponym hierarchy
(nouns) or hypernym/troponym hierarchy (verbs).
WordNet additionally links both synsets and words
via lexical relations including antonymy, morpho-
logical derivation and holonymy/meronym.
In parallel with the development of WordNet, a
number of computational methods for calculating
the semantic relatedness/similarity between synset
pairs (i.e. sense-specified word pairs) have been de-
veloped, as we outline below. These methods ap-
ply to synset rather than word pairs, so to generate a
single score for a given word pair, we look up each
word in WordNet and exhaustively generate scores
for each sense pairing defined by them, and calcu-
late their arithmetic mean.1
The majority of the methods (all methods other
than HSO, VECTOR and LESK) are restricted to op-
erating strictly over hierarchical links within a sin-
gle hierarchy. As the verb and noun hierarchies are
not connected (other than via derivational links), this
means that it is generally not possible to calculate
the similarity between noun and verb senses, for ex-
ample. In such cases, we simply drop the synset
pairing in question from our calculation of the mean.
The least common subsumer (LCS) is a common
feature to a number of the measures, and is defined
as the deepest node in the hierarchy that subsumes
both of the synsets under question.
For all our experiments over WordNet, we use the
WordNet::Similarity package.
Path distance (PATH)
The simplest of the WordNet-based measures is
to count the number of nodes visited while going
from one word to another via the hypernym hierar-
chy. The path distance between two nodes is de-
fined as the number of nodes that lie on the short-
est path between two words in the hierarchy. This
1We also experimented with the median, and trialled filter-
ing the set of senses in a variety of ways, e.g. using only the
first sense (the sense with the highest prior) for a given word,
or using only the word senses associated with the POS with the
highest prior. In all cases, the overall trend was for the correla-
tion with the human scores to drop relative to the mean, so we
only present the numbers for the mean in this paper.
102
count of nodes includes the beginning and ending
word nodes.
Leacock-Chodorow (LCH)
The measure of semantic similarity devised by
Leacock et al (1998) finds the shortest path between
two WordNet synsets (sp(c1, c2)) using hypernym
and synonym relationships. This path length is then
scaled by the maximum depth of WordNet (D), and
the log likelihood taken:
simlch(c1, c2) = ? log
sp(c1, c2)
2 ?D
Wu-Palmer (WUP)
Wu and Palmer (1994) proposed to scale the depth
of the two synset nodes (depthc1 and depthc2) by
the depth of their LCS (depth(lcsc1,c2)):
simwup(c1, c2) =
2 ? depth(lcsc1,c2)
depthc1 + depthc2 + 2 ? depth(lcsc1,c2)
The scaling means that specific terms (deeper in the
hierarchy) that are close together are more semanti-
cally similar than more general terms, which have a
short path distance between them. Only hypernym
relationships are used in this measure, as the LCS
is defined by the common member in the concepts?
hypernym path.
Hirst-St Onge (HSO)
Hirst and St-Onge (1998) define a measure of se-
mantic similarity based on length and tortuosity of
the path between nodes. Hirst and St-Onge attribute
directions (up, down and horizontal) to the larger set
of WordNet relationships, and identify the path from
one word to another utilising all of these relation-
ships. The relatedness score is then computed by
the weighted sum of the path length between the two
words (len(c1, c2)) and the number of turns the path
makes (turns(c1, c2)) to take this route:
relhso(c1, c2) =
C ? len(c1, c2)? k ? turns(c1, c2)
where C and k are constants. Additionally, a set of
restrictions is placed on the path so that it may not
be more than a certain length, may not contain more
than a set number of turns, and may only take turns
in certain directions.
Resnik Information Content (RES)
Resnik (1995) presents a method for weighting
edges in WordNet (avoiding the assumption that all
edges between nodes have equal importance), by
weighting edges between nodes by their frequency
of use in textual corpora.
Resnik found that the most effective measure of
comparison using this methodology was to measure
the Information Content (IC(c) = ? log p(c)) of
the subsumer with the greatest Information Content
from the set of all concepts that subsumed the two
initial concepts (S(c1, c2)) being compared:
simres(c1, c2) = max
c?S(c1,c2)
[? log p(c)]
Lin (LIN)
Lin (1998) expanded on the Information Theo-
retic approach presented by Resnik by scaling the
Information Content of each node by the informa-
tion content of their LCS:
simlin(c1, c2) =
2? log p(lcsc1,c2)
log p(c1) + log p(c2)
This measure contrasts the joint content of the two
concepts with the difference between them.
Jiang-Conrath (JCN)
Jiang and Conrath (1997) define a measure that
utilises the components of the information content
of the LCS in a different manner:
simjcn(c1, c2) =
1
IC(a) + IC(b)? 2? IC(lcsa,b)
Instead of defining commonality and difference as
with Lin?s measure, the key determinant is the speci-
ficity of the two nodes compared with their LCS.
Lesk (LESK)
Lesk (1986) proposed a significantly different ap-
proach to lexical similarity to that proposed in the
methods presented above, using the lexical over-
lap in dictionary definitions (or glosses) to disam-
biguate word sense. The sense definitions that con-
tain the most words in common indicate the most
likely sense of the word given its co-occurrence with
similar word senses. Banerjee and Pedersen (2002)
103
adapted this method to utilise WordNet sense glosses
rather than dictionary definitions, and expand the
dictionary definitions via ontological links, and it is
this method we experiment with in this paper.
Vector (VECTOR)
Schu?tze (1998) uses the words surrounding a term
in a piece of text to form a context vector that de-
scribes the context in which the word sense appears.
For a set of words associated with a target sense, a
context vector is computed as the centroid vector of
these words. The centroid context vectors each rep-
resent a word sense. To compare word senses, the
cosine similarity of the context vectors is used.
4.2 Wikipedia
In the last few years, there has been a surge of in-
terest in using Wikipedia to calculate semantic sim-
ilarity, using the Wikipedia article content, in-article
links and document categories (Stru?be and Ponzetto,
2006; Gabrilovich and Markovitch, 2007; Milne and
Witten, 2008). We present a selection of such meth-
ods below. There are a number of Wikipedia-based
scoring methods which we do not present results
for here (notably Stru?be and Ponzetto (2006) and
Gabrilovich and Markovitch (2007)), due to their
computational complexity and uncertainty about the
full implementation details of the methods.
As with WordNet, a given word will often have
multiple entries in Wikipedia, grouped in a disam-
biguation page. For MIW, RACO and DOCSIM,
we apply the same strategy as we did with Word-
Net, in exhaustively calculating the pairwise scores
between the sets of documents associated with each
term, and averaging across them.
Milne-Witten (MIW)
Milne and Witten (2008) adapted the Resnik
(1995) methodology to utilise the count of links
pointing to an article. As Wikipedia is self-
referential (articles link to related articles), no ex-
ternal data is needed to find the ?referred-to-edness?
of a concept. Milne and Witten use an adapted In-
formation Content measure that weights the number
of links from one article to another (c1 ? c2) by the
total number of links to the second article:
w(c1 ? c2) = |c1 ? c2| ? log
?
x?W
|W |
|c1, x)|
where x is an article in W , Wikipedia. This mea-
sure provides the similarity of one article to another,
however this is asymmetrical. The above metric is
used to find the weights of all outlinks from the two
articles being compared:
~c1 = (w(c1 ? l1), w(c1 ? l2), ? ? ? , w(c1 ? ln))
~c2 = (w(c2 ? l1), w(c2 ? l2), ? ? ? , w(c2 ? ln))
for the set of links l that is the union of the sets of
outlinks from both articles. The overall similarity
of the two articles is then calculated by taking the
cosine similarity of the two vectors.
Related Article Concept Overlap (RACO)
We also determine the category overlap of two
articles by examining the outlinks of both articles,
in the form of the Related Article Concept Overlap
(RACO) measure. The concept overlap of the sets
of respective outlinks is given by the union of the
two sets of categories from the outlinks from each
article:
overlap(c1, c1) =
?
?
(
?
l?ol(c1)
cat(l)
)
?
(
?
l?ol(c2)
cat(l)
)
?
?
where ol(c1) is the set of outlinks from article c1,
and cat(l) is the set of categories of which the arti-
cle at outlink l is a member. To account for article
size (and differing number of outlinks), the Jaccard
coefficient is used:
relraco(c1, c2) =
?
?
(
?
l?ol(c1) cat(l)
)
?
(
?
l?ol(c2) cat(l)
)
?
?
?
?
?
l?ol(c1) cat(l)
?
?+
?
?
?
l?ol(c2) cat(l)
?
?
Document Similarity (DOCSIM)
In addition to these two measures of semantic re-
latedness, we experiment with simple cosine simi-
larity of the text of Wikipedia articles as a measure
of semantic relatedness.
Term Co-occurrence (PMI)
Another variant is to treat Wikipedia as a single
meta-document and score word pairs using term co-
occurrence. Here, we calculate the pointwise mu-
tual information (PMI) of each word pair, estimated
104
Selected high-scoring topics (unanimous score=3):
[NEWS] space earth moon science scientist light nasa mission planet mars ...
[NEWS] health disease aids virus vaccine infection hiv cases infected asthma ...
[BOOKS] steam engine valve cylinder pressure piston boiler air pump pipe ...
[BOOKS] furniture chair table cabinet wood leg mahogany piece oak louis ...
Selected low-scoring topics (unanimous score=1):
[NEWS] king bond berry bill ray rate james treas byrd key ...
[NEWS] dog moment hand face love self eye turn young character ...
[BOOKS] soon short longer carried rest turned raised filled turn allowed ...
[BOOKS] act sense adv person ppr plant sax genus applied dis ...
Table 1: A selection of high-scoring and low-scoring topics
from the entire corpus of over two million English
Wikipedia articles (?1 billion words). PMI has been
studied variously in the context of collocation ex-
traction (Pecina, 2008), and is one measure of the
statistical independence of observing two words in
close proximity. Using a sliding window of 10-
words to identify co-occurrence, we computed the
PMI of all a given word pair (wi, wj) as, following
Newman et al (2009):
PMI(wi, wj) = log
p(wi, wj)
p(wi)p(wj)
4.3 Search engine-based similarity
Finally, we present two search engine-based scor-
ing methods, based on Newman et al (2009). In
this case the external data source is the entire World
Wide Web, via the Google search engine. Unlike
the methods presented above, here we query for the
topic in its entirety,2 meaning that we return a topic-
level score rather than scores for individual word or
word sense pairs. In each case, we mark each search
term with the advanced search option + to search
for the terms exactly as is and prevent Google from
using synonyms or lexical variants of the term. An
example query is: +space +earth +moon +science
+scientist +light +nasa +mission +planet +mars.
Google title matches (TITLES)
Firstly, we score topics by the relative occurrence
of their component words in the titles of documents
returned by Google:
Google-titles-match(w) = 1 [wi = vj ]
2All queries were run on 15/09/2009.
where i = 1, . . . , 10 and j = 1, . . . , |V |, vj are
all the unique terms mentioned in the titles from the
top-100 search results, and 1 is the indicator function
to count matches. For example, in the top-100 re-
sults for our query above, there are 194 matches with
the ten topic words, so Google-titles-match(w) =
194.
Google log hits matches (LOGHITS)
Second, we issue queries as above, but return the
log number of hits for our query:
Google-log-hits(w) =
log10(# results from search for w)
where w is the search string +w1 +w2 +w3 . . .
+w10. For example, our query above returns
171,000 results, so Google-log-hits(w) = 5.2. and
the URL titles from the top-100 results include a to-
tal of 194 matches with the ten topic words, so for
this topic Google-titles-match(w)=194.
5 Experimental Setup
We learned topics for two document collections: a
collection of news articles, and a collection of books.
These collections were chosen to produce sets of
topics that have more variable quality than one typi-
cally observes when topic modeling highly uniform
content. The collection of D = 55, 000 news arti-
cles was selected from English Gigaword, and the
collection of D = 12, 000 books was downloaded
from the Internet Archive. We refer to these collec-
tions as NEWS and BOOKS, respectively.
Standard procedures were used to tokenize each
collection and create the bags-of-words. We learned
105
Resource Method Median Mean
WordNet
HSO ?0.29 0.34
JCN 0.08 0.22
LCH ?0.18 ?0.07
LESK 0.38 0.37
LIN 0.18 0.25
PATH 0.19 0.11
RES ?0.10 0.13
VECTOR 0.07 0.20
WUP 0.03 0.10
Wikipedia
RACO 0.61 0.63
MIW 0.69 0.60
DOCSIM 0.45 0.50
PMI 0.78 0.77
Google TITLES 0.80LOGHITS 0.46
Gold-standard IAA 0.79 0.73
Table 2: Spearman rank correlation ? values for the
different scoring methods over the NEWS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
topic models of NEWS and BOOKS using T = 200
and T = 400 topics respectively. We randomly
selected a total of 237 topics from the two collec-
tions for user scoring. We asked N = 9 users to
score each of the 237 topics on a 3-point scale where
3=?useful? (coherent) and 1=?useless? (less coher-
ent).
We provided annotators with a rubric and guide-
lines on how to judge whether a topic was useful
or useless. In addition to showing several examples
of useful and useless topics, we instructed users to
decide whether the topic was to some extent coher-
ent, meaningful, interpretable, subject-heading-like,
and something-you-could-easily-label. For our pur-
poses, the usefulness of a topic can be thought of
as whether one could imagine using the topic in a
search interface to retrieve documents about a par-
ticular subject. One indicator of usefulness is the
ease by which one could think of a short label to de-
scribe a topic.
Table 1 shows a selection of high- and low-
scoring topics, as scored by the N = 9 users. The
first topic illustrates the notion of labelling coher-
ence, as space exploration, e.g., would be an obvi-
ous label for the topic. The low-scoring topics dis-
play little coherence, and one would not expect them
Resource Method Median Mean
WordNet
HSO 0.15 0.59
JCN ?0.20 0.19
LCH ?0.31 ?0.15
LESK 0.53 0.53
LIN 0.09 0.28
PATH 0.29 0.12
RES 0.57 0.66
VECTOR ?0.08 0.27
WUP 0.41 0.26
Wikipedia
RACO 0.62 0.69
MIW 0.68 0.70
DOCSIM 0.59 0.60
PMI 0.74 0.77
Google TITLES 0.51LOGHITS ?0.19
Gold-standard IAA 0.82 0.78
Table 3: Spearman rank correlation ? values for the dif-
ferent scoring methods over the BOOKS dataset (best-
performing method for each resource underlined; best-
performing method overall in boldface)
to be useful as categories or facets in a search inter-
face. Note that the useless topics from both collec-
tions are not chance artifacts produced by the mod-
els, but are in fact stable and robust statistical fea-
tures in the data sets.
6 Results
The results for the different topic scoring methods
over the NEWS and BOOKS collections are pre-
sented in Tables 2 and 3, respectively. In each ta-
ble, we separate out the scoring methods into those
based on WordNet (from Section 4.1), those based
on Wikipedia (from Section 4.2), and those based on
Google (from Section 4.3).
As stated in Section 4, we experiment with two
methods for combining the word-pair scores (for all
methods other than the two Google methods, which
operate natively over a word set), namely the arith-
metic mean and median. We present the numbers
for these two methods in each table. In each case,
we evaluate via Spearman rank correlation, revers-
ing the sign of the calculated ? value for PATH (as it
is the only instance of a distance metric, where the
gold-standard is made up of similarity values).
We include the inter-annotator agreement (IAA)
in the final row of each table, which we consider
106
to be the upper bound for the task. This is calcu-
lated as the average Spearman rank correlation be-
tween each annotator and the mean/median of the
remaining annotators for that topic. Encouragingly,
there is relatively little difference in the IAA be-
tween the two datasets; the median-based calcula-
tion produces slightly higher ? values and is empiri-
cally the method of choice.3
Of all the topic scoring methods tested, PMI
(term co-occurrence via simple pointwise mutual in-
formation) is the most consistent performer, achiev-
ing the best or near-best results over both datasets,
and approaching or surpassing the inter-annotator
agreement. This indicates both that the task of
topic evaluation as defined in this paper is com-
putationally tractable, and that word-pair based co-
occurrence is highly successful at modelling topic
coherence.
Comparing the different resources, Wikipedia is
far and away the most consistent performing, with
PMI producing the best results, followed by MIW
and RACO, and finally DOCSIM. There is rela-
tively little difference in results between NEWS and
BOOKS for the Wikipedia methods. Google achieves
the best results over NEWS, for TITLES (actually
slightly above the IAA), but the results fall away
sharply over BOOKS. The reason for this can be
seen in the sample topics in Table 1: the topics for
BOOKS tend to be more varied in word class than
for NEWS, and contain less proper names; also, the
genre of BOOKS is less well represented on the web.
We hypothesise that Wikipedia?s encyclopedic na-
ture means that it has good coverage over both do-
mains, and thus more robust.
Turning to WordNet, the overall results are
markedly better over BOOKS, again largely because
of the relative sparsity of proper names in the re-
source. The results for individual methods are some-
what surprising. Whereas JCN and LCH have been
shown to be two of the best-performing methods
over lexical similarity tasks (Budanitsky and Hirst,
2005; Agirre et al, 2009), they perform abysmally
at the topic scoring task. Indeed, the spread of re-
sults across the WordNet similarity methods (no-
3Note that the choice of mean or median for IAA is in-
dependent of that for the scoring methods, as they are com-
bining different things: annotator scores in the one hand, and
word/concept pair scores on the other.
tably HSO, JCN, LCH, LIN, RES and WUP) is
much greater than we had expected. The single most
consistent method is LESK, which is based on lexi-
cal overlap in definition sentences and makes rela-
tively modest use of the WordNet hierarchy. Supple-
mentary evaluation where we filtered out all proper
nouns from the topics (based on simple POS priors
for each word learned from an automatically-tagged
version of the British National Corpus) led to a slight
increase in results for the WordNet methods; the full
results are omitted for reasons of space. In future
work, we intend to carry out error analysis to deter-
mine why some of the methods performed so badly,
or inconsistently across the two datasets.
There is no clear answer to the question of
whether the mean or median is the best method for
combining the pair-wise scores.
7 Conclusions
We have proposed the novel task of topic coher-
ence evaluation as a form of intrinsic topic evalu-
ation with relevance in document search/discovery
and visualisation applications. We constructed
a gold-standard dataset of topic coherence scores
over the output of a topic model for two distinct
datasets, and evaluated a wide range of topic scor-
ing methods over this dataset, drawing on WordNet,
Wikipedia and the Google search engine. The sin-
gle best-performing method was term co-occurrence
within Wikipedia based on pointwise mutual infor-
mation, which achieve results very close to the inter-
annotator agreement for the task. Google was also
found to perform well over one of the two datasets,
while the results for the WordNet-based methods
were overall surprisingly low.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme. DN
has also been supported by a grant from the Institute of
Museum and Library Services, and a Google Research
Award.
References
E Agirre, E Alfonseca, K Hall, J Kravalova, M Pas?ca,
and A Soroa. 2009. A study on similarity and re-
107
latedness using distributional and WordNet-based ap-
proaches. In Proc. of HLT: NAACL 2009, pages 19?
27, Boulder, Colorado.
S Banerjee and T Pedersen. 2002. An adapted Lesk algo-
rithm for word sense disambiguation using WordNet.
Proc. of CICLing?02, pages 136?145.
DM Blei, AY Ng, and MI Jordan. 2003. Latent Dirich-
let alocation. Journal of Machine Learning Research,
3:993?1022.
S Brody and M Lapata. 2009. Bayesian word sense
induction. In Proc. of EACL 2009, pages 103?111,
Athens, Greece.
A Budanitsky and G Hirst. 2005. Evaluating WordNet-
based Measures of Lexical Sematic Relatedness.
Computational Linguistics, 32(1):13?47.
WL Buntine and A Jakulin. 2004. Applying discrete
PCA in data analysis. In Proc. of UAI 2004, pages
59?66.
J Chang, J Boyd-Graber, S Gerris, C Wang, and D Blei.
2009. Reading tea leaves: How humans interpret topic
models. In Proc. of NIPS 2009.
H Daume III. 2009. Non-parametric bayesian areal lin-
guistics. In Proc. of HLT: NAACL 2009, pages 593?
601, Boulder, USA.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society of Information Science, 41(6).
C Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, USA.
E Gabrilovich and S Markovitch. 2007. Computing se-
mantic relatedness using Wikipedia-based explicit se-
mantic analysis. In Proc. of IJCAI?07, pages 1606?
1611, Hyderabad, India.
T Griffiths and M Steyvers. 2004. Finding scientific top-
ics. In Proc. of the National Academy of Sciences, vol-
ume 101, pages 5228?5235.
T Griffiths and M Steyvers. 2006. Probabilistic topic
models. In Latent Semantic Analysis: A Road to
Meaning.
A Haghighi and L Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proc. of HLT: NAACL 2009, pages 362?370, Boulder,
USA.
G Hirst and D St-Onge. 1998. Lexical chains as repre-
sentations of context for the detection and correction
of malapropism. In Fellbaum (Fellbaum, 1998), pages
305?332.
T Hofmann. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177?196.
JJ Jiang and DW Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of COLING?97, pages 19?33, Taipei, Taiwan.
C Leacock, G A Miller, and M Chodorow. 1998. Using
corpus statistics and WordNet relations for sense iden-
tification. Computational Linguistics, 24(1):147?65.
M Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proc. of SIGDOC?86,
pages 24?26, Toronto, Canada.
D Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of COLING/ACL?98, pages 768?
774, Montreal, Canada.
C-Y Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In Proc. of the ACL 2004
Workshop on Text Summarization Branches Out (WAS
2004), pages 74?81, Barcelona, Spain.
Q Mei, X Shen, and CX Zhai. 2007. Automatic labeling
of multinomial topic models. In Proc. of KDD 2007,
pages 490?499.
D Milne and IH Witten. 2008. An effective, low-
cost measure of semantic relatedness obtained from
Wikipedia links. In Proc. of AAAI Workshop on
Wikipedia and Artificial Intelligence, pages 25?30,
Chicago, USA.
H Misra, O Cappe, and F Yvon. 2008. Using LDA to
detect semantically incoherent documents. In Proc. of
CoNLL 2008, pages 41?48, Manchester, England.
D Newman, S Karimi, and L Cavedon. 2009. External
evaluation of topic models. In Proc. of ADCS 2009,
pages 11?18, Sydney, Australia.
D Newman, T Baldwin, L Cavedon, S Karimi, D Mar-
tinez, and J Zobel. to appeara. Visualizing docu-
ment collections and search results using topic map-
ping. Journal of Web Semantics.
D Newman, Y Noh, E Talley, S Karimi, and T Bald-
win. to appearb. Evaluating topic models for digital
libraries. In Proc. of JCDL/ICADL 2010, Gold Coast,
Australia.
K Papineni, S Roukos, T Ward, and W-J Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL 2002, pages 311?318,
Philadelphia, USA.
P Pecina. 2008. Lexical Association Measures: Colloca-
tion Extraction. Ph.D. thesis, Charles University.
P Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proc. of IJ-
CAI?95, pages 448?453, Montreal, Canada.
H Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
M Stru?be and SP Ponzetto. 2006. WikiRelate! comput-
ing semantic relateness using Wikipedia. In Proc. of
AAAI?06, pages 1419?1424, Boston, USA.
Q Sun, R Li, D Luo, and X Wu. 2008. Text segmentation
with LDA-based Fisher kernel. In Proc. of ACL-08:
HLT, pages 269?272.
HM Wallach, I Murray, R Salakhutdinov, and
DM Mimno. 2009. Evaluation methods for
topic models. In Proc. of ICML 2009, page 139.
D Widdows and K Ferraro. 2008. Semantic Vectors:
A scalable open source package and online technol-
ogy management application. In Proc. of LREC 2008,
Marrakech, Morocco.
Z Wu and M Palmer. 1994. Verb selection and lexical
selection. In Proc. of ACL?94, pages 133?138, Las
Cruces, USA.
108
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1536?1545,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Labelling of Topic Models
Jey Han Lau,?? Karl Grieser,? David Newman,?? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California Irvine
jhlau@csse.unimelb.edu.au, kgrieser@csse.unimelb.edu.au, newman@uci.edu, tb@ldwin.net
Abstract
We propose a method for automatically la-
belling topics learned via LDA topic models.
We generate our label candidate set from the
top-ranking topic terms, titles of Wikipedia ar-
ticles containing the top-ranking topic terms,
and sub-phrases extracted from the Wikipedia
article titles. We rank the label candidates us-
ing a combination of association measures and
lexical features, optionally fed into a super-
vised ranking model. Our method is shown to
perform strongly over four independent sets of
topics, significantly better than a benchmark
method.
1 Introduction
Topic modelling is an increasingly popular frame-
work for simultaneously soft-clustering terms and
documents into a fixed number of ?topics?, which
take the form of a multinomial distribution over
terms in the document collection (Blei et al,
2003). It has been demonstrated to be highly ef-
fective in a wide range of tasks, including multi-
document summarisation (Haghighi and Vander-
wende, 2009), word sense discrimination (Brody
and Lapata, 2009), sentiment analysis (Titov and
McDonald, 2008), information retrieval (Wei and
Croft, 2006) and image labelling (Feng and Lapata,
2010).
One standard way of interpreting a topic is to use
the marginal probabilities p(wi|tj) associated with
each term wi in a given topic tj to extract out the 10
terms with highest marginal probability. This results
in term lists such as:1
stock market investor fund trading invest-
ment firm exchange companies share
1Here and throughout the paper, we will represent a topic tj
via its ranking of top-10 topic terms, based on p(wi|tj).
which are clearly associated with the domain of
stock market trading. The aim of this research is to
automatically generate topic labels which explicitly
identify the semantics of the topic, i.e. which take us
from a list of terms requiring interpretation to a sin-
gle label, such as STOCK MARKET TRADING in the
above case.
The approach proposed in this paper is to first
generate a topic label candidate set by: (1) sourc-
ing topic label candidates from Wikipedia by query-
ing with the top-N topic terms; (2) identifying the
top-ranked document titles; and (3) further post-
processing the document titles to extract sub-strings.
We translate each topic label into features extracted
from Wikipedia, lexical association with the topic
terms in Wikipedia documents, and also lexical fea-
tures for the component terms. This is used as the
basis of a support vector regression model, which
ranks each topic label candidate.
Our contributions in this work are: (1) the genera-
tion of a novel evaluation framework and dataset for
topic label evaluation; (2) the proposal of a method
for both generating and scoring topic label candi-
dates; and (3) strong in- and cross-domain results
across four independent document collections and
associated topic models, demonstrating the ability
of our method to automatically label topics with re-
markable success.
2 Related Work
Topics are conventionally interpreted via their top-
N terms, ranked based on the marginal probability
p(wi|tj) in that topic (Blei et al, 2003; Griffiths and
Steyvers, 2004). This entails a significant cognitive
load in interpretation, prone to subjectivity. Topics
are also sometimes presented with manual post-hoc
labelling for ease of interpretation in research pub-
lications (Wang and McCallum, 2006; Mei et al,
1536
2006). This has obvious disadvantages in terms of
subjectivity, and lack of reproducibility/automation.
The closest work to our method is that of Mei et
al. (2007), who proposed various unsupervised ap-
proaches for automatically labelling topics, based
on: (1) generating label candidates by extracting ei-
ther bigrams or noun chunks from the document col-
lection; and (2) ranking the label candidates based
on KL divergence with a given topic. Their proposed
methodology generates a generic list of label can-
didates for all topics using only the document col-
lection. The best method uses bigrams exclusively,
in the form of the top-1000 bigrams based on the
Student?s t-test. We reimplement their method and
present an empirical comparison in Section 5.3.
In other work, Magatti et al (2009) proposed a
method for labelling topics induced by a hierarchi-
cal topic model. Their label candidate set is the
Google Directory (gDir) hierarchy, and label selec-
tion takes the form of ontological alignment with
gDir. The experiments presented in the paper are
highly preliminary, although the results certainly
show promise. However, the method is only applica-
ble to a hierarchical topic model and crucially relies
on a pre-existing ontology and the class labels con-
tained therein.
Pantel and Ravichandran (2004) addressed the
more specific task of labelling a semantic class
by applying Hearst-style lexico-semantic patterns
to each member of that class. When presented
with semantically homogeneous, fine-grained near-
synonym clusters, the method appears to work well.
With topic modelling, however, the top-ranking
topic terms tended to be associated and not lexically
similar to one another. It is thus highly questionable
whether their method could be applied to topic mod-
els, but it would certainly be interesting to investi-
gate whether our model could conversely be applied
to the labelling of sets of near-synonyms.
In recent work, Lau et al (2010) proposed to ap-
proach topic labelling via best term selection, i.e.
selecting one of the top-10 topic terms to label the
overall topic. While it is often possible to label top-
ics with topic terms (as is the case with the stock
market topic above), there are also often cases where
topic terms are not appropriate as labels. We reuse
a selection of the features proposed by Lau et al
(2010), and return to discuss it in detail in Section 3.
While not directly related to topic labelling,
Chang et al (2009) were one of the first to propose
human labelling of topic models, in the form of syn-
thetic intruder word and topic detection tasks. In the
intruder word task, they include a term w with low
marginal probability p(w|t) for topic t into the top-
N topic terms, and evaluate how well both humans
and their model are able to detect the intruder.
The potential applications for automatic labelling
of topics are many and varied. In document col-
lection visualisation, e.g., the topic model can be
used as the basis for generating a two-dimensional
representation of the document collection (Newman
et al, 2010a). Regions where documents have a
high marginal probability p(di|tj) of being associ-
ated with a given topic can be explicitly labelled
with the learned label, rather than just presented
as an unlabelled region, or presented with a dense
?term cloud? from the original topic. In topic model-
based selectional preference learning (Ritter et al,
2010; O` Se?aghdha, 2010), the learned topics can
be translated into semantic class labels (e.g. DAYS
OF THE WEEK), and argument positions for individ-
ual predicates can be annotated with those labels for
greater interpretability/portability. In dynamic topic
models tracking the diachronic evolution of topics
in time-sequenced document collections (Blei and
Lafferty, 2006), labels can greatly enhance the inter-
pretation of what topics are ?trending? at any given
point in time.
3 Methodology
The task of automatic labelling of topics is a natural
progression from the best topic term selection task
of Lau et al (2010). In that work, the authors use
a reranking framework to produce a ranking of the
top-10 topic terms based on how well each term ? in
isolation ? represents a topic. For example, in our
stock market investor fund trading ... topic example,
the term trading could be considered as a more rep-
resentative term of the overall semantics of the topic
than the top-ranked topic term stock.
While the best term could be used as a topic la-
bel, topics are commonly ideas or concepts that are
better expressed with multiword terms (for example
STOCK MARKET TRADING), or terms that might not
be in the top-10 topic terms (for example, COLOURS
1537
would be a good label for a topic of the form red
green blue cyan ...).
In this paper, we propose a novel method for au-
tomatic topic labelling that first generates topic label
candidates using English Wikipedia, and then ranks
the candidates to select the best topic labels.
3.1 Candidate Generation
Given the size and diversity of English Wikipedia,
we posit that the vast majority of (coherent) topics
or concepts are encapsulated in a Wikipedia article.
By making this assumption, the difficult task of gen-
erating potential topic labels is transposed to find-
ing relevant Wikipedia articles, and using the title of
each article as a topic label candidate.
We first use the top-10 topic terms (based on the
marginal probabilities from the original topic model)
to query Wikipedia, using: (a) Wikipedia?s native
search API; and (b) a site-restricted Google search.
The combined set of top-8 article titles returned
from the two search engines for each topic consti-
tutes the initial set of primary candidates.
Next we chunk parse the primary candidates us-
ing the OpenNLP chunker,2 and extract out all noun
chunks. For each noun chunk, we generate all com-
ponent n-grams (including the full chunk), out of
which we remove all n-grams which are not in them-
selves article titles in English Wikipedia. For exam-
ple, if the Wikipedia document title were the single
noun chunk United States Constitution, we would
generate the bigrams United States and States Con-
stitution, and prune the latter; we would also gen-
erate the unigrams United, States and Constitution,
all of which exist as Wikipedia articles and are pre-
served.
In this way, an average of 30?40 secondary labels
are produced for each topic based on noun chunk n-
grams. A good portion of these labels are commonly
stopwords or unigrams that are only marginally re-
lated to the topic (an artifact of the n-gram gener-
ation process). To remove these outlier labels, we
use the RACO lexical association method of Grieser
et al (2011).
RACO (Related Article Conceptual Overlap) uses
Wikipedia?s link structure and category membership
to identify the strength of relationship between arti-
2http://opennlp.sourceforge.net/
cles via their category overlap. The set of categories
related to an article is defined as the union of the cat-
egory membership of all outlinks in that article. The
category overlap of two articles (a and b) is the in-
tersection of the related category sets of each article.
The formal definition of this measure is as follows:
|(?p?O(a)C(p)) ? (?p?O(b)C(p))|
where O(a) is the set of outlinks from article a, and
C(p) is the set of categories of which article p is a
member. This is then normalised using Dice?s co-
efficient to generate a similarity measure. In the in-
stance that a term maps onto multiple Wikipedia ar-
ticles via a disambiguation page, we return the best
RACO score across article pairings for a given term
pair. The final score for each secondary label can-
didate is calculated as the average RACO score with
each of the primary label candidates. All secondary
labels with an average RACO score of 0.1 and above
are added to the label candidate set.
Finally, we add the top-5 topic terms to the set of
candidates, based on the marginals from the origi-
nal topic model. Doing this ensures that there are
always label candidates for all topics (even if the
Wikipedia searches fail), and also allows the pos-
sibility of labeling a topic using its own topic terms,
which was demonstrated by Lau et al (2010) to be a
baseline source of topic label candidates.
3.2 Candidate Ranking
After obtaining the set of topic label candidates, the
next step is to rank the candidates to find the best la-
bel for each topic. We will first describe the features
that we use to represent label candidates.
3.2.1 Features
A good label should be strongly associated with
the topic terms. To learn the association of a label
candidate with the topic terms, we use several lexical
association measures: pointwise mutual information
(PMI), Student?s t-test, Dice?s coefficient, Pearson?s
?2 test, and the log likelihood ratio (Pecina, 2009).
We also include conditional probability and reverse
conditional probability measures, based on the work
of Lau et al (2010). To calculate the association
measures, we parse the full collection of English
Wikipedia articles using a sliding window of width
1538
20, and obtain term frequencies for the label candi-
dates and topic terms. To measure the association
between a label candidate and a list of topic terms,
we average the scores of the top-10 topic terms.
In addition to the association measures, we in-
clude two lexical properties of the candidate: the raw
number of terms, and the relative number of terms in
the label candidate that are top-10 topic terms.
We also include a search engine score for each
label candidate, which we generate by querying a
local copy of English Wikipedia with the top-10
topic terms, using the Zettair search engine (based
on BM25 term similarity).3 For a given label candi-
date, we return the average score for the Wikipedia
article(s) associated with it.
3.2.2 Unsupervised and Supervised Ranking
Each of the proposed features can be used as the
basis for an unsupervised model for label candidate
selection, by ranking the label candidates for a given
topic and selecting the top-N . Alternatively, they
can be combined in a supervised model, by training
over topics where we have gold-standard labelling
of the label candidates. For the supervised method,
we use a support vector regression (SVR) model
(Joachims, 2006) over all of the features.
4 Datasets
We conducted topic labelling experiments using
document collections constructed from four distinct
domains/genres, to test the domain/genre indepen-
dence of our method:
BLOGS : 120,000 blog articles dated from August
to October 2008 from the Spinn3r blog dataset4
BOOKS : 1,000 English language books from the
Internet Archive American Libraries collection
NEWS : 29,000 New York Times news articles
dated from July to September 1999, from the
English Gigaword corpus
PUBMED : 77,000 PubMed biomedical abstracts
published in June 2010
3http://www.seg.rmit.edu.au/zettair/
4http://www.icwsm.org/data/
The BLOGS dataset contains blog posts that cover
a diverse range of subjects, from product reviews
to casual, conversational messages. The BOOKS
topics, coming from public-domain out-of-copyright
books (with publication dates spanning more than
a century), relate to a wide range of topics includ-
ing furniture, home decoration, religion and art,
and have a more historic feel to them. The NEWS
topics reflect the types and range of subjects one
might expect in news articles such as health, finance,
entertainment, and politics. The PUBMED topics
frequently contain domain-specific terms and are
sharply differentiated from the topics for the other
corpora. We are particularly interested in the perfor-
mance of the method over PUBMED, as it is a highly
specialised domain where we may expect lower cov-
erage of appropriate topic labels within Wikipedia.
We took a standard approach to topic modelling
each of the four document collections: we tokenised,
lemmatised and stopped each document,5 and cre-
ated a vocabulary of terms that occurred at least
ten times. From this processed data, we created a
bag-of-words representation of each document, and
learned topic models with T = 100 topics in each
case.
To focus our experiments on topics that were rela-
tively more coherent and interpretable, we first used
the method of Newman et al (2010b) to calculate
the average PMI-score for each topic, and filtered
all topics that had an average PMI-score lower than
0.4. We additionally filtered any topics where less
than 5 of the top-10 topic terms are default nomi-
nal in Wikipedia.6 The filtering criteria resulted in
45 topics for BLOGS, 38 topics for BOOKS, 60 top-
ics for NEWS, and 85 topics for PUBMED. Man-
ual inspection of the discarded topics indicated that
they were predominantly hard-to-label junk topics or
mixed topics, with limited utility for document/term
clustering.
Applying our label candidate generation method-
ology to these 228 topics produced approximately
6000 labels ? an average of 27 labels per topic.
5OpenNLP is used for tokenization, Morpha for lemmatiza-
tion (Minnen et al, 2001).
6As determined by POS tagging English Wikipedia with
OpenNLP, and calculating the coarse-grained POS priors (noun,
verb, etc.) for each term.
1539
Figure 1: A screenshot of the topic label evaluation task on Amazon Mechanical Turk. This screen constitutes a
Human Intelligence Task (HIT); it contains a topic followed by 10 suggested topic labels, which are to be rated. Note
that been would be the stopword label in this example.
4.1 Topic Candidate Labelling
To evaluate our methods and train the supervised
method, we require gold-standard ratings for the la-
bel candidates. To this end, we used Amazon Me-
chanical Turk to collect annotations for our labels.
In our annotation task, each topic was presented
in the form of its top-10 terms, followed by 10 sug-
gested labels for the topic. This constitutes a Human
Intelligence Task (HIT); annotators are paid based
on the number of HITs they have completed. A
screenshot of a HIT seen by annotator is presented
in Figure 1.
In each HIT, annotators were asked to rate the la-
bels based on the following ordinal scale:
3: Very good label; a perfect description of the
topic.
2: Reasonable label, but does not completely cap-
ture the topic.
1: Label is semantically related to the topic, but
would not make a good topic label.
0: Label is completely inappropriate, and unrelated
to the topic.
To filter annotations from workers who did not
perform the task properly or from spammers, we ap-
1540
Domain Topic Terms Label Candidate AverageRating
BLOGS china chinese olympics gold olympic team win beijing medal sport 2008 summer olympics 2.60
BOOKS church arch wall building window gothic nave side vault tower gothic architecture 2.40
NEWS israel peace barak israeli minister palestinian agreement prime leader palestinians israeli-palestinian conflict 2.63
PUBMED cell response immune lymphocyte antigen cytokine t-cell induce receptor immunity immune system 2.36
Table 1: A sample of topics and topic labels, along with the average rating for each label candidate
plied a few heuristics to automatically detect these
workers. Additionally, we inserted a small num-
ber of stopwords as label candidates in each HIT
and recorded workers who gave high ratings to these
stopwords. Annotations from workers who failed to
passed these tests are removed from the final set of
gold ratings.
Each label candidate was rated in this way by at
least 10 annotators, and ratings from annotators who
passed the filter were combined by averaging them.
A sample of topics, label candidates, and the average
rating is presented in Table 1.7
Finally, we train the regression model over all
the described features, using the human rating-based
ranking.
5 Experiments
In this section we present our experimental results
for the topic labelling task, based on both the unsu-
pervised and supervised methods, and the methodol-
ogy of Mei et al (2007), which we denote MSZ for
the remainder of the paper.
5.1 Evaluation
We use two basic measures to evaluate the perfor-
mance of our predictions. Top-1 average rating is
the average annotator rating given to the top-ranked
system label, and has a maximum value of 3 (where
annotators unanimously rated all top-ranked system
labels with a 3). This is intended to give a sense of
the absolute utility of the top-ranked candidates.
The second measure is normalized discounted
cumulative gain (nDCG: Jarvelin and Kekalainen
(2002), Croft et al (2009)), computed for the top-1
(nDCG-1), top-3 (nDCG-3) and top-5 ranked sys-
tem labels (nDCG-5). For a given ordered list of
7The dataset is available for download from
http://www.csse.unimelb.edu.au/research/
lt/resources/acl2011-topic/.
scores, this measure is based on the difference be-
tween the original order, and the order when the list
is sorted by score. That is, if items are ranked op-
timally in descending order of score at position N ,
nDCG-N is equal to 1. nDCG is a normalised score,
and indicates how close the candidate label ranking
is to the optimal ranking within the set of annotated
candidates, noting that an nDCG-N score of 1 tells
us nothing about absolute values of the candidates.
This second evaluation measure is thus intended to
reflect the relative quality of the ranking, and com-
plements the top-1 average rating.
Note that conventional precision- and recall-based
evaluation is not appropriate for our task, as each
label candidate has a real-valued rating.
As a baseline for the task, we use the unsuper-
vised label candidate ranking method based on Pear-
son?s ?2 test, as it was overwhelmingly found to be
the pick of the features for candidate ranking.
5.2 Results for the Supervised Method
For the supervised model, we present both in-
domain results based on 10-fold cross-validation,
and cross-domain results where we learn a model
from the ratings for the topic model from a given
domain, and apply it to a second domain. In each
case, we learn an SVR model over the full set of fea-
tures described in Section 3.2.1. In practical terms,
in-domain results make the unreasonable assump-
tion that we have labelled 90% of labels in order
to be able to label the remaining 10%, and cross-
domain results are thus the more interesting data
point in terms of the expected results when apply-
ing our method to a novel topic model. It is valuable
to compare the two, however, to gauge the relative
impact of domain on the results.
We present the results for the supervised method
in Table 2, including the unsupervised baseline and
an upper bound estimate for comparison purposes.
The upper bound is calculated by ranking the candi-
1541
Test Domain Training Top-1 Average Rating nDCG-1 nDCG-3 nDCG-5All 1? 2? Top5
BLOGS
Baseline (unsupervised) 1.84 1.87 1.75 1.74 0.75 0.77 0.79
In-domain 1.98 1.94 1.95 1.77 0.81 0.82 0.83
Cross-domain: BOOKS 1.88 1.92 1.90 1.77 0.77 0.81 0.83
Cross-domain: NEWS 1.97 1.94 1.92 1.77 0.80 0.83 0.83
Cross-domain: PUBMED 1.95 1.95 1.93 1.82 0.80 0.82 0.83
Upper bound 2.45 2.26 2.29 2.18 1.00 1.00 1.00
BOOKS
Baseline (unsupervised) 1.75 1.76 1.70 1.72 0.77 0.77 0.79
In-domain 1.91 1.90 1.83 1.74 0.84 0.81 0.83
Cross-domain: BLOGS 1.82 1.88 1.79 1.71 0.79 0.81 0.82
Cross-domain: NEWS 1.82 1.87 1.80 1.75 0.79 0.81 0.83
Cross-domain: PUBMED 1.87 1.87 1.80 1.73 0.81 0.82 0.83
Upper bound 2.29 2.17 2.15 2.04 1.00 1.00 1.00
NEWS
Baseline (unsupervised) 1.96 1.76 1.87 1.70 0.80 0.79 0.78
In-domain 2.02 1.92 1.90 1.82 0.82 0.82 0.84
Cross-domain: BLOGS 2.03 1.92 1.89 1.85 0.83 0.82 0.84
Cross-domain: BOOKS 2.01 1.80 1.93 1.73 0.82 0.82 0.83
Cross-domain: PUBMED 2.01 1.93 1.94 1.80 0.82 0.82 0.83
Upper bound 2.45 2.31 2.33 2.12 1.00 1.00 1.00
PUBMED
Baseline (unsupervised) 1.73 1.74 1.68 1.63 0.75 0.77 0.79
In-domain 1.79 1.76 1.74 1.67 0.77 0.82 0.84
Cross-domain: BLOGS 1.80 1.77 1.73 1.69 0.78 0.82 0.84
Cross-domain: BOOKS 1.77 1.70 1.74 1.64 0.77 0.82 0.83
Cross-domain: NEWS 1.79 1.76 1.73 1.65 0.77 0.82 0.84
Upper bound 2.31 2.17 2.22 2.01 1.00 1.00 1.00
Table 2: Supervised results for all domains
dates based on the annotated human ratings. The up-
per bound for top-1 average rating is thus the high-
est average human rating of all label candidates for
a given topic, while the upper bound for the nDCG
measures will always be 1.
In addition to results for the combined candidate
set, we include results for each of the three candi-
date subsets, namely the primary Wikipedia labels
(?1??), the secondary Wikipedia labels (?2??) and
the top-5 topic terms (?Top5?); the nDCG results
are over the full candidate set only, as the numbers
aren?t directly comparable over the different subsets
(due to differences in the number of candidates and
the distribution of ratings).
Comparing the in-domain and cross-domain re-
sults, we observe that they are largely compara-
ble, with the exception of BOOKS, where there is
a noticeable drop in both top-1 average rating and
nDGC-1 when we use cross-domain training. We
see an appreciable drop in scores when we train
BOOKS against BLOGS (or vice versa), which we
analyse as being due to incompatibility in document
content and structure between these two domains.
Overall though, the results are very encouraging,
and point to the plausibility of using labelled topic
models from independent domains to learn the best
topic labels for a new domain.
Returning to the question of the suitability of la-
bel candidates for the highly specialised PUBMED
document collection, we first notice that the up-
per bound top-1 average rating is comparable to
the other domains, indicating that our method has
been able to extract equivalent-quality label can-
didates from Wikipedia. The top-1 average rat-
ings of the supervised method are lower than the
other domains. We hypothesise that the cause of
the drop is that the lexical association measures are
trained over highly diverse Wikipedia data rather
than biomedical-specific data, and predict that the
results would improve if we trained our features over
PubMed.
The results are uniformly better than the unsuper-
vised baselines for all four corpora, although there
is quite a bit of room for improvement relative to the
upper bound. To better gauge the quality of these
results, we carry out a direct comparison of our pro-
posed method with the best-performing method of
MSZ in Section 5.3.
1542
Looking to the top-1 average score results over the
different candidate sets, we observe first that the up-
per bound for the combined candidate set (?All?) is
higher than the scores for the candidate subsets in all
cases, underlining the complementarity of the differ-
ent candidate sets. We also observe that the top-5
topic term candidate set is the lowest performer out
of the three subsets across all four corpora, in terms
of both upper bound and the results for the super-
vised method. This reinforces our comments about
the inferiority of the topic word selection method of
Lau et al (2010) for topic labelling purposes. For
NEWS and PUBMED, there is a noticeable differ-
ence between the results of the supervised method
over the full candidate set and each of the candidate
subsets. In contrast, for BOOKS and BLOGS, the re-
sults for the primary candidate subset are at times
actually higher than those over the full candidate set
in most cases (but not for the upper bound). This is
due to the larger search space in the full candidate
set, and the higher median quality of candidates in
the primary candidate set.
5.3 Comparison with MSZ
The best performing method out of the suite of
approaches proposed by MSZ method exclusively
uses bigrams extracted from the document collec-
tion, ranked based on Student?s t-test. The potential
drawbacks to this approach are: all labels must be
bigrams, there must be explicit token instances of
a given bigram in the document collection for it to
be considered as a label candidate, and furthermore,
there must be enough token instances in the docu-
ment collection for it to have a high t score.
To better understand the performance difference
of our approach to that of MSZ, we perform direct
comparison of our proposed method with the bench-
mark method of MSZ.
5.3.1 Candidate Ranking
First, we compare the candidate ranking method-
ology of our method with that of MSZ, using the
label candidates extracted by the MSZ method.
We first extracted the top-2000 bigrams using the
N -gram Statistics Package (Banerjee and Pedersen,
2003). We then ranked the bigrams for each topic
using the Student?s t-test. We included the top-5 la-
bels generated for each topic by the MSZ method
in our Mechanical Turk annotation task, and use the
annotations to directly compare the two methods.
To measure the performance of candidate rank-
ing between our supervised method and MSZ?s, we
re-rank the top-5 labels extracted by MSZ using
our SVR methodology (in-domain) and compare the
top-1 average rating and nDCG scores. Results are
shown in Table 3. We do not include results for the
BOOKS domain because the text collection is much
larger than the other domains, and the computation
for the MSZ relevance score ranking is intractable
due to the number of n-grams (a significant short-
coming of the method).
Looking at the results for the other domains, it is
clear that our ranking system has the upper hand:
it consistently outperforms MSZ over every evalu-
ation metric.8 Comparing the top-1 average rating
results back to those in Table 2, we observe that
for all three domains, the results for MSZ are be-
low those of the unsupervised baseline, and well be-
low those of our supervised method. The nDCG re-
sults are more competitive, and the nDCG-3 results
are actually higher than our original results in Ta-
ble 2. It is important to bear in mind, however, that
the numbers are in each case relative to a different la-
bel candidate set. Additionally, the results in Table 3
are based on only 5 candidates, with a relatively flat
gold-standard rating distribution, making it easier to
achieve higher nDCG-5 scores.
5.3.2 Candidate Generation
The method of MSZ makes the implicit assump-
tion that good bigram labels are discoverable within
the document collection. In our method, on the other
hand, we (efficiently) access the much larger and
variable n-gram length set of English Wikipedia ar-
ticle titles, in addition to the top-5 topic terms. To
better understand the differences in label candidate
sets, and the relative coverage of the full label can-
didate set in each case, we conducted another survey
where human users were asked to suggest one topic
label for each topic presented.
The survey consisted, once again, of presenting
annotators with a topic, but in this case, we gave
them the open task of proposing the ideal label for
8Based on a single ANOVA, the difference in results is sta-
tistically significant at the 5% level for BLOGS, and 1% for
NEWS and PUBMED.
1543
Test Domain Candidate Ranking Top-1 nDCG-1 nDCG-3 nDCG-5System Avg. Rating
BLOGS
MSZ 1.26 0.65 0.76 0.87
SVR 1.41 0.75 0.85 0.92
Upper bound 1.87 1.00 1.00 1.00
NEWS
MSZ 1.37 0.73 0.81 0.90
SVR 1.66 0.88 0.90 0.95
Upper bound 1.86 1.00 1.00 1.00
PUBMED
MSZ 1.53 0.77 0.85 0.93
SVR 1.73 0.87 0.91 0.96
Upper bound 1.98 1.00 1.00 1.00
Table 3: Comparison of results for our proposed supervised ranking method (SVR) and that of MSZ
the topic. In this, we did not enforce any restrictions
on the type or size of label (e.g. the number of terms
in the label).
Of the manually-generated gold-standard labels,
approximately 36% were contained in the original
document collection, but 60% were Wikipedia arti-
cle titles. This indicates that our method has greater
potential to generate a label of the quality of the ideal
proposed by a human in a completely open-ended
task.
6 Discussion
On the subject of suitability of using Amazon Me-
chanical Turk for natural language tasks, Snow et al
(2008) demonstrated that the quality of annotation
is comparable to that of expert annotators. With that
said, the PUBMED topics are still a subject of inter-
est, as these topics often contain biomedical terms
which could be difficult for the general populace to
annotate.
As the number of annotators per topic and the
number of annotations per annotator vary, there is
no immediate way to calculate the inter-annotator
agreement. Instead, we calculated the MAE score
for each candidate, which is an average of the ab-
solute difference between an annotator?s rating and
the average rating of a candidate, summed across all
candidates to get the MAE score for a given corpus.
The MAE scores for each corpus are shown in Ta-
ble 4, noting that a smaller value indicates higher
agreement.
As the table shows, the agreement for the
PUBMED domain is comparable with the other
datasets. BLOGS and NEWS have marginally better
Corpus MAE
BLOGS 0.50
BOOKS 0.56
NEWS 0.52
PUBMED 0.56
Table 4: Average MAE score for label candidate rating
over each corpus
agreement, almost certainly because of the greater
immediacy of the topics, covering everyday areas
such as lifestyle and politics. BOOKS topics are oc-
casionally difficult to label due to the breadth of the
domain; e.g. consider a topic containing terms ex-
tracted from Shakespeare sonnets.
7 Conclusion
This paper has presented the task of topic labelling,
that is the generation and scoring of labels for a
given topic. We generate a set of label candidates
from the top-ranking topic terms, titles of Wikipedia
articles containing the top-ranking topic terms, and
also a filtered set of sub-phrases extracted from the
Wikipedia article titles. We rank the label candidates
using a combination of association measures, lexical
features and an Information Retrieval feature. Our
method is shown to perform strongly over four inde-
pendent sets of topics, and also significantly better
than a competitor system.
Acknowledgements
NICTA is funded by the Australian government as rep-
resented by Department of Broadband, Communication
and Digital Economy, and the Australian Research Coun-
cil through the ICT centre of Excellence programme. DN
has also been supported by a grant from the Institute of
Museum and Library Services, and a Google Research
Award.
1544
References
S. Banerjee and T. Pedersen. 2003. The design, im-
plementation, and use of the Ngram Statistic Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
D.M. Blei and J.D. Lafferty. 2006. Dynamic topic mod-
els. In ICML 2006.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet alocation. JMLR, 3:993?1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. In EACL 2009, pages 103?111.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and
D. Blei. 2009. Reading tea leaves: How humans in-
terpret topic models. In NIPS, pages 288?296.
B. Croft, D. Metzler, and T. Strohman. 2009. Search
Engines: Information Retrieval in Practice. Addison
Wesley.
Y. Feng and M. Lapata. 2010. Topic models for im-
age annotation and text illustration. In Proceedings
of Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL HLT
2010), pages 831?839, Los Angeles, USA, June.
K. Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.
2011. Using ontological and document similarity to
estimate museum exhibit relatedness. ACM Journal
on Computing and Cultural Heritage, 3(3):1?20.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topics. In PNAS, volume 101, pages 5228?5235.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
HLT: NAACL 2009, pages 362?370.
K. Jarvelin and J. Kekalainen. 2002. Cumulated gain-
based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4).
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining (KDD), pages 217?226,
New York, NY, USA. ACM.
J.H. Lau, D. Newman, S. Karimi, and T. Baldwin. 2010.
Best topic word selection for topic labelling. In Coling
2010: Posters, pages 605?613, Beijing, China.
D. Magatti, S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In ISDA 2009, pages
1227?1232, Pisa, Italy.
Q. Mei, C. Liu, H. Su, and C. Zhai. 2006. A probabilistic
approach to spatiotemporal theme pattern mining on
weblogs. In WWW 2006, pages 533?542.
Q. Mei, X. Shen, and C. Zhai. 2007. Automatic labeling
of multinomial topic models. In SIGKDD, pages 490?
499.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Journal of Nat-
ural Language Processing, 7(3):207?223.
D. Newman, T. Baldwin, L. Cavedon, S. Karimi, D. Mar-
tinez, and J. Zobel. 2010a. Visualizing document col-
lections and search results using topic mapping. Jour-
nal of Web Semantics, 8(2-3):169?175.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin.
2010b. Automatic evaluation of topic coherence. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010), pages 100?108, Los Angeles,
USA, June. Association for Computational Linguis-
tics.
D. O` Se?aghdha. 2010. Latent variable models of selec-
tional preference. In ACL 2010.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In HLT/NAACL-04, pages
321?328.
P. Pecina. 2009. Lexical Association Measures: Collo-
cation Extraction. Ph.D. thesis, Charles University.
A. Ritter, Mausam, and O. Etzioni. 2010. A la-
tent Dirichlet alocation method for selectional pref-
erences. In ACL 2010.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In EMNLP
?08: Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 254?
263, Morristown, NJ, USA.
I. Titov and R. McDonald. 2008. Modeling online re-
views with multi-grain topic models. In WWW ?08,
pages 111?120.
X. Wang and A. McCallum. 2006. Topics over time: A
non-Markov continuous-time model of topical trends.
In KDD, pages 424?433.
S. Wei and W.B. Croft. 2006. LDA-based document
models for ad-hoc retrieval. In SIGIR ?06, pages 178?
185.
1545
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259?270,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Word Sense Distributions, Detecting Unattested Senses and
Identifying Novel Senses Using Topic Models
Jey Han Lau,
?
Paul Cook,
?
Diana McCarthy,
?
Spandana Gella,
?
and Timothy Baldwin
?
? Dept of Philosophy, King?s College London
? Dept of Computing and Information Systems, The University of Melbourne
? University of Cambridge
jeyhan.lau@gmail.com, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net
Abstract
Unsupervised word sense disambiguation
(WSD) methods are an attractive approach
to all-words WSD due to their non-reliance
on expensive annotated data. Unsuper-
vised estimates of sense frequency have
been shown to be very useful for WSD due
to the skewed nature of word sense distri-
butions. This paper presents a fully unsu-
pervised topic modelling-based approach
to sense frequency estimation, which is
highly portable to different corpora and
sense inventories, in being applicable to
any part of speech, and not requiring a hi-
erarchical sense inventory, parsing or par-
allel text. We demonstrate the effective-
ness of the method over the tasks of pre-
dominant sense learning and sense distri-
bution acquisition, and also the novel tasks
of detecting senses which aren?t attested
in the corpus, and identifying novel senses
in the corpus which aren?t captured in the
sense inventory.
1 Introduction
The automatic determination of word sense infor-
mation has been a long-term pursuit of the NLP
community (Agirre and Edmonds, 2006; Navigli,
2009). Word sense distributions tend to be Zip-
fian, and as such, a simple but surprisingly high-
accuracy back-off heuristic for word sense dis-
ambiguation (WSD) is to tag each instance of a
given word with its predominant sense (McCarthy
et al, 2007). Such an approach requires knowl-
edge of predominant senses; however, word sense
distributions ? and predominant senses too ?
vary from corpus to corpus. Therefore, meth-
ods for automatically learning predominant senses
and sense distributions for specific corpora are re-
quired (Koeling et al, 2005; Lapata and Brew,
2004).
In this paper, we propose a method which uses
topic models to estimate word sense distributions.
This method is in principle applicable to all parts
of speech, and moreover does not require a parser,
a hierarchical sense representation or parallel text.
Topic models have been used for WSD in a num-
ber of studies (Boyd-Graber et al, 2007; Li et
al., 2010; Lau et al, 2012; Preiss and Stevenson,
2013; Cai et al, 2007; Knopp et al, 2013), but
our work extends significantly on this earlier work
in focusing on the acquisition of prior word sense
distributions (and predominant senses).
Because of domain differences and the skewed
nature of word sense distributions, it is often the
case that some senses in a sense inventory will
not be attested in a given corpus. A system ca-
pable of automatically finding such senses could
reduce ambiguity, particularly in domain adapta-
tion settings, while retaining rare but nevertheless
viable senses. We further propose a method for ap-
plying our sense distribution acquisition system to
the task of finding unattested senses ? i.e., senses
that are in the sense inventory but not attested in
a given corpus. In contrast to the previous work
of McCarthy et al (2004a) on this topic which
uses the sense ranking score from McCarthy et
al. (2004b) to remove low-frequency senses from
WordNet, we focus on finding senses that are unat-
tested in the corpus on the premise that, given ac-
curate disambiguation, rare senses in a corpus con-
tribute to correct interpretation.
Corpus instances of a word can also correspond
to senses that are not present in a given sense in-
ventory. This can be due to, for example, words
taking on new meanings over time (e.g. the rela-
259
tively recent senses of tablet and swipe related to
touchscreen computers) or domain-specific terms
not being included in a more general-purpose
sense inventory. A system for automatically iden-
tifying such novel senses ? i.e. senses that are
attested in the corpus but not in the sense inven-
tory ? would be a very valuable lexicographi-
cal tool for keeping sense inventories up-to-date
(Cook et al, 2013). We further propose an appli-
cation of our proposed method to the identification
of such novel senses. In contrast to McCarthy et al
(2004b), the use of topic models makes this possi-
ble, using topics as a proxy for sense (Brody and
Lapata, 2009; Yao and Durme, 2011; Lau et al,
2012). Earlier work on identifying novel senses
focused on individual tokens (Erk, 2006), whereas
our approach goes further in identifying groups of
tokens exhibiting the same novel sense.
2 Background and Related Work
There has been a considerable amount of research
on representing word senses and disambiguating
usages of words in context (WSD) as, in order
to produce computational systems that understand
and produce natural language, it is essential to
have a means of representing and disambiguat-
ing word sense. WSD algorithms require word
sense information to disambiguate token instances
of a given ambiguous word, e.g. in the form of
sense definitions (Lesk, 1986), semantic relation-
ships (Navigli and Velardi, 2005) or annotated
data (Zhong and Ng, 2010). One extremely use-
ful piece of information is the word sense prior
or expected word sense frequency distribution.
This is important because word sense distributions
are typically skewed (Kilgarriff, 2004), and sys-
tems do far better when they take bias into ac-
count (Agirre and Martinez, 2004).
Typically, word frequency distributions are esti-
mated with respect to a sense-tagged corpus such
as SemCor (Miller et al, 1993), a 220,000 word
corpus tagged with WordNet (Fellbaum, 1998)
senses. Due to the expense of hand tagging, and
sense distributions being sensitive to domain and
genre, there has been some work on trying to
estimate sense frequency information automati-
cally (McCarthy et al, 2004b; Chan and Ng, 2005;
Mohammad and Hirst, 2006; Chan and Ng, 2006).
Much of this work has been focused on ranking
word senses to find the predominant sense in a
given corpus (McCarthy et al, 2004b; Mohammad
and Hirst, 2006), which is a very powerful heuris-
tic approach to WSD. Most WSD systems rely upon
this heuristic for back-off in the absence of strong
contextual evidence (McCarthy et al, 2007). Mc-
Carthy et al (2004b) proposed a method which
relies on distributionally similar words (nearest
neighbours) associated with the target word in
an automatically acquired thesaurus (Lin, 1998).
The distributional similarity scores of the nearest
neighbours are associated with the respective tar-
get word senses using a WordNet similarity mea-
sure, such as those proposed by Jiang and Conrath
(1997) and Banerjee and Pedersen (2002). The
word senses are ranked based on these similar-
ity scores, and the most frequent sense is selected
for the corpus that the distributional similarity the-
saurus was trained over.
As well as sense ranking for predominant sense
acquisition, automatic estimates of sense fre-
quency distribution can be very useful for WSD
for training data sampling purposes (Agirre and
Martinez, 2004), entropy estimation (Jin et al,
2009), and prior probability estimates, all of which
can be integrated within a WSD system (Chan and
Ng, 2005; Chan and Ng, 2006; Lapata and Brew,
2004). Various approaches have been adopted,
such as normalizing sense ranking scores to ob-
tain a probability distribution (Jin et al, 2009), us-
ing subcategorisation information as an indication
of verb sense (Lapata and Brew, 2004) or alter-
natively using parallel text (Chan and Ng, 2005;
Chan and Ng, 2006; Agirre and Martinez, 2004).
The work of Boyd-Graber and Blei (2007) is
highly related in that it extends the method of Mc-
Carthy et al (2004b) to provide a generative model
which assumes the words in a given document are
generated according to the topic distribution ap-
propriate for that document. They then predict the
most likely sense for each word in the document
based on the topic distribution and the words in
context (?corroborators?), each of which, in turn,
depends on the document?s topic distribution. Us-
ing this approach, they get comparable results to
McCarthy et al when context is ignored (i.e. us-
ing a model with one topic), and at most a 1% im-
provement on SemCor when they use more topics
in order to take context into account. Since the
results do not improve on McCarthy et al as re-
gards sense distribution acquisition irrespective of
context, we will compare our model with that pro-
posed by McCarthy et al
260
Recent work on finding novel senses has tended
to focus on comparing diachronic corpora (Sagi
et al, 2009; Cook and Stevenson, 2010; Gulor-
dava and Baroni, 2011) and has also considered
topic models (Lau et al, 2012). In a similar vein,
Peirsman et al (2010) considered the identifica-
tion of words having a sense particular to one
language variety with respect to another (specif-
ically Belgian and Netherlandic Dutch). In con-
trast to these studies, we propose a model for com-
paring a corpus with a sense inventory. Carpuat
et al (2013) exploit parallel corpora to identify
words in domain-specific monolingual corpora
with previously-unseen translations; the method
we propose does not require parallel data.
3 Methodology
Our methodology is based on the WSI system
described in Lau et al (2012),
1
which has been
shown (Lau et al, 2012; Lau et al, 2013a; Lau et
al., 2013b) to achieve state-of-the-art results over
the WSI tasks from SemEval-2007 (Agirre and
Soroa, 2007), SemEval-2010 (Manandhar et al,
2010) and SemEval-2013 (Navigli and Vannella,
2013; Jurgens and Klapaftis, 2013). The system
is built around a Hierarchical Dirichlet Process
(HDP: Teh et al (2006)), a non-parametric variant
of a Latent Dirichlet Allocation topic model (Blei
et al, 2003) where the model automatically opti-
mises the number of topics in a fully-unsupervised
fashion over the training data.
To learn the senses of a target lemma, we train
a single topic model per target lemma. The sys-
tem reads in a collection of usages of that lemma,
and automatically induces topics (= senses) in the
form of a multinomial distribution over words, and
per-usage topic assignments (= probabilistic sense
assignments) in the form of a multinomial distri-
bution over topics. Following Lau et al (2012),
we assign one topic to each usage by selecting the
topic that has the highest cumulative probability
density, based on the topic allocations of all words
in the context window for that usage.
2
Note that in
their original work, Lau et al (2012) experimented
with the use of features extracted from a depen-
dency parser. Due to the computational overhead
associated with these features, and the fact that the
empirical impact of the features was found to be
1
Based on the implementation available at: https://
github.com/jhlau/hdp-wsi
2
This includes all words in the usage sentence except
stopwords, which were filtered in the preprocessing step.
marginal, we make no use of parser-based features
in this paper.
3
The induced topics take the form of word multi-
nomials, and are often represented by the top-N
words in descending order of conditional probabil-
ity. We interpret each topic as a sense of the target
lemma.
4
To illustrate this, we give the example of
topics induced by the HDP model for network in
Table 1.
We refer to this method as HDP-WSI hence-
forth.
5
In predominant sense acquisition, the task is to
learn, for each target lemma, the most frequently
occurring word sense in a particular domain or
corpus, relative to a predefined sense inventory.
The WSI system provides us with a topic alloca-
tion per usage of a given word, from which we can
derive a distribution of topics over usages and a
predominant topic. In order to map this onto the
predominant sense, we need to have some way of
aligning a topic with a sense. We design our topic?
sense alignment methodology with portability in
mind ? it should be applicable to any sense in-
ventory. As such, our alignment methodology as-
sumes only that we have access to a conventional
sense gloss or definition for each sense, and does
not rely on ontological/structural knowledge (e.g.
the WordNet hierarchy).
To compute the similarity between a sense
and a topic, we first convert the words in the
gloss/definition into a multinomial distribution
over words, based on simple maximum likeli-
hood estimation.
6
We then calculate the Jensen?
Shannon divergence between the multinomial dis-
tribution (over words) of the gloss and that of the
topic, and convert the divergence value into a sim-
ilarity score by subtracting it from 1. Formally, the
similarity sense s
i
and topic t
j
is:
sim(s
i
, t
j
) = 1? JS(S?T ) (1)
where S and T are the multinomial distributions
3
For hyper-parameters ? and ?, we used 0.1 for both. We
did not tune the parameters, and opted to use the default pa-
rameters introduced in Teh et al (2006).
4
To avoid confusion, we will refer to the HDP-induced
topics as topics, and reserve the term sense to denote senses
in a sense inventory.
5
The code used to learn predominant sense and run all
experiments described in this paper is available at: https:
//github.com/jhlau/predom_sense.
6
Words are tokenised using OpenNLP and lemmatised
with Morpha (Minnen et al, 2001). We additionally remove
the target lemma, stopwords and words that are less than 3
characters in length.
261
Topic Num Top-10 Terms
1 network support @card@ information research service group development community member
2 service @card@ road company transport rail area government network public
3 network social model system family structure analysis form relationship neural
4 network @card@ computer system service user access internet datum server
5 system network management software support corp company service application product
6 @card@ radio news television show bbc programme call think film
7 police drug criminal terrorist intelligence network vodafone iraq attack cell
8 network atm manager performance craigavon group conference working modelling assistant
9 root panos comenius etd unipalm lse brazil telephone xxx discuss
Table 1: An example to illustrate the topics induced for network by the HDP model. The top-10 highest
probability terms are displayed to represent each topic (@card@ denotes a tokenised cardinal number).
over words for sense s
i
and topic t
j
, respectively,
and JS(X?Y ) is the Jensen?Shannon divergence
for distribution X and Y .
To learn the predominant sense, we compute the
prevalence score of each sense and take the sense
with the highest prevalence score as the predom-
inant sense. The prevalence score for a sense is
computed by summing the product of its similar-
ity scores with each topic (i.e. sim(s
i
, t
j
)) and the
prior probability of the topic in question (based
on maximum likelihood estimation). Formally, the
prevalence score of sense s
i
is given as follows:
prevalence(s
i
) =
T
?
j
(sim(s
i
, t
j
)? P (t
j
)) (2)
=
T
?
j
(
sim(s
i
, t
j
)?
f(t
j
)
?
T
k
f(t
k
)
)
where f(t
j
) is the frequency of topic t
j
(i.e. the
number of usages assigned to topic t
j
), and T is
the number of topics.
The intuition behind the approach is that the
predominant sense should be the sense that has rel-
atively high similarity (in terms of lexical overlap)
with high-probability topic(s).
4 WordNet Experiments
We first test the proposed method over the tasks
of predominant sense learning and sense distribu-
tion induction, using the WordNet-tagged dataset
of Koeling et al (2005), which is made up of
3 collections of documents: a domain-neutral
corpus (BNC), and two domain-specific corpora
(SPORTS and FINANCE). For each domain,
annotators were asked to sense-annotate a ran-
dom selection of sentences for each of 40 target
nouns, based on WordNet v1.7. The predominant
sense and distribution across senses for each target
lemma was obtained by aggregating over the sense
annotations. The authors evaluated their method in
terms of WSD accuracy over a given corpus, based
on assigning all instances of a target word with the
predominant sense learned from that corpus. For
the remainder of the paper, we denote their system
as MKWC.
To compare our system (HDP-WSI) with
MKWC, we apply it to the three datasets of Koel-
ing et al (2005). For each dataset, we use HDP
to induce topics for each target lemma, compute
the similarity between the topics and the WordNet
senses (Equation (1)), and rank the senses based
on the prevalence scores (Equation (2)). In addi-
tion to the WSD accuracy based on the predomi-
nant sense inferred from a particular corpus, we
additionally compute: (1) Acc
UB
, the upper bound
for the first sense-based WSD accuracy (using the
gold standard predominant sense for disambigua-
tion);
7
and (2) ERR, the error rate reduction be-
tween the accuracy for a given system (Acc) and
the upper bound (Acc
UB
), calculated as follows:
ERR = 1?
Acc
UB
? Acc
Acc
UB
Looking at the results in Table 2, we see lit-
tle difference in the results for the two methods,
with MKWC performing better over two of the
datasets (BNC and SPORTS) and HDP-WSI per-
forming better over the third (FINANCE), but all
differences are small. Based on the McNemar?s
Test with Yates correction for continuity, MKWC
is significantly better over BNC and HDP-WSI is
significantly better over FINANCE (p < 0.0001
in both cases), but the difference over SPORTS
is not statistically significance (p > 0.1). Note
that there is still much room for improvement with
7
The upper bound for a WSD approach which tags all to-
ken occurrences of a given word with the same sense, as a
first step towards context-sensitive unsupervised WSD.
262
Dataset
FS
CORPUS
MKWC HDP-WSI
Acc
UB
Acc ERR Acc ERR
BNC 0.524 0.407 (0.777) 0.376 (0.718)
FINANCE 0.801 0.499 (0.623) 0.555 (0.693)
SPORTS 0.774 0.437 (0.565) 0.422 (0.545)
Table 2: WSD accuracy for MKWC and HDP-WSI
on the WordNet-annotated datasets, as compared
to the upper-bound based on actual first sense in
the corpus (higher values indicate better perfor-
mance; the best system in each row [other than the
FS
CORPUS
upper bound] is indicated in boldface).
Dataset MKWC HDP-WSI
BNC 0.226 0.214
FINANCE 0.426 0.375
SPORTS 0.420 0.363
Table 3: Sense distribution evaluation of MKWC
and HDP-WSI on the WordNet-annotated datasets,
evaluated using JS divergence (lower values indi-
cate better performance; the best system in each
row is indicated in boldface).
both systems, as we see in the gap between the up-
per bound (based on perfect determination of the
first sense) and the respective system accuracies.
Given that both systems compute a continuous-
valued prevalence score for each sense of a tar-
get lemma, a distribution of senses can be ob-
tained by normalising the prevalence scores across
all senses. The predominant sense learning task
of McCarthy et al (2007) evaluates the ability of
a method to identify only the head of this dis-
tribution, but it is also important to evaluate the
full sense distribution (Jin et al, 2009). To this
end, we introduce a second evaluation metric:
the Jensen?Shannon (JS) divergence between the
inferred sense distribution and the gold-standard
sense distribution, noting that smaller values are
better in this case, and that it is now theoretically
possible to obtain a JS divergence of 0 in the case
of a perfect estimate of the sense distribution. Re-
sults are presented in Table 3.
HDP-WSI consistently achieves lower JS diver-
gence, indicating that the distribution of senses
that it finds is closer to the gold standard distri-
bution. Testing for statistical significance over the
paired JS divergence values for each lemma using
the Wilcoxon signed-rank test, the result for FI-
NANCE is significant (p < 0.05) but the results
for the other two datasets are not (p > 0.1 in each
case).
Dataset
FS
CORPUS
FS
DICT
HDP-WSI
Acc
UB
Acc ERR Acc ERR
UKWAC 0.574 0.387 (0.674) 0.514 (0.895)
TWITTER 0.468 0.297 (0.635) 0.335 (0.716)
Table 4: WSD accuracy for HDP-WSI on the
Macmillan-annotated datasets, as compared to the
upper-bound based on actual first sense in the cor-
pus (higher values indicate better performance; the
best system in each row [other than the FS
CORPUS
upper bound] is indicated in boldface).
Dataset FS
CORPUS
FS
DICT
HDP-WSI
UKWAC 0.210 0.393 0.156
TWITTER 0.259 0.472 0.171
Table 5: Sense distribution evaluation of HDP-
WSI on the Macmillan-annotated datasets as com-
pared to corpus- and dictionary-based first sense
methods, evaluated using JS divergence (lower
values indicate better performance; the best sys-
tem in each row is indicated in boldface).
To summarise, the results for MKWC and HDP-
WSI are fairly even for predominant sense learn-
ing (each outperforms the other at a level of statis-
tical significance over one dataset), but HDP-WSI
is better at inducing the overall sense distribution.
It is important to bear in mind that MKWC in
these experiments makes use of full-text parsing in
calculating the distributional similarity thesaurus,
and the WordNet graph structure in calculating the
similarity between associated words and different
senses. Our method, on the other hand, uses no
parsing, and only the synset definitions (and not
the graph structure) of WordNet.
8
The non-reliance
on parsing is significant in terms of portability to
text sources which are less amenable to parsing
(such as Twitter: (Baldwin et al, 2013)), and the
non-reliance on the graph structure of WordNet is
significant in terms of portability to conventional
?flat? sense inventories. While comparable results
on a different dataset have been achieved with a
proximity thesaurus (McCarthy et al, 2007) com-
pared to a dependency one,
9
it is not stated how
8
McCarthy et al (2004b) obtained good results with def-
inition overlap, but their implementation uses the relation
structure alongside the definitions (Banerjee and Pedersen,
2002). Iida et al (2008) demonstrate that further exten-
sions using distributional data are required when applying the
method to resources without hierarchical relations.
9
The thesauri used in the reimplementation of MKWC
in this paper were obtained from http://webdocs.cs.
ualberta.ca/
?
lindek/downloads.htm.
263
wide a window is needed for the proximity the-
saurus. This could be a significant issue with Twit-
ter data, where context tends to be limited. In the
next section, we demonstrate the robustness of the
method in experimenting with two new datasets,
based on Twitter and a web corpus, and the Macmil-
lan English Dictionary.
5 Macmillan Experiments
In our second set of experiments, we move to a
new dataset (Gella et al, to appear) based on text
from ukWaC (Ferraresi et al, 2008) and Twit-
ter, and annotated using the Macmillan English Dic-
tionary
10
(henceforth ?Macmillan?). For the pur-
poses of this research, the choice of Macmillan is
significant in that it is a conventional dictionary
with sense definitions and examples, but no link-
ing between senses.
11
In terms of the original re-
search which gave rise to the sense-tagged dataset,
Macmillan was chosen over WordNet for reasons in-
cluding: (1) the well-documented difficulties of
sense tagging with fine-grained WordNet senses
(Palmer et al, 2004; Navigli et al, 2007); (2) the
regular update cycle of Macmillan (meaning it con-
tains many recently-emerged senses); and (3) the
finding in a preliminary sense-tagging task that it
better captured Twitter usages than WordNet (and
also OntoNotes: Hovy et al (2006)).
The dataset is made up of 20 target nouns which
were selected to span the high- to mid-frequency
range in both Twitter and the ukWaC corpus, and
have at least 3 Macmillan senses. The average sense
ambiguity of the 20 target nouns in Macmillan is 5.6
(but 12.3 in WordNet). 100 usages of each target
noun were sampled from each of Twitter (from a
crawl over the time period Jan 3?Feb 28, 2013 us-
ing the Twitter Streaming API) and ukWaC, after
language identification using langid.py (Lui
and Baldwin, 2012) and POS tagging (based on
the CMU ARK Twitter POS tagger v2.0 (Owoputi
et al, 2012) for Twitter, and the POS tags provided
with the corpus for ukWaC). Amazon Mechani-
cal Turk (AMT) was then used to 5-way sense-tag
each usage relative to Macmillan, including allow-
ing the annotators the option to label a usage as
?Other? in instances where the usage was not cap-
tured by any of the Macmillan senses. After qual-
ity control over the annotators/annotations (see
10
http://www.macmillandictionary.com/
11
Strictly speaking, there is limited linking in the form of
sets of synonyms in Macmillan, but we choose to not use this
information in our research.
Gella et al (to appear) for details), and aggregation
of the annotations into a single sense per usage
(possibly ?Other?), there were 2000 sense-tagged
ukWaC sentences and Twitter messages over the
20 target nouns. We refer to these two datasets as
UKWAC and TWITTER henceforth.
To apply our method to the two datasets, we use
HDP-WSI to train a model for each target noun,
based on the combined set of usages of that lemma
in each of the two background corpora, namely the
original Twitter crawl that gave rise to the TWIT-
TER dataset, and all of ukWaC.
5.1 Learning Sense Distributions
As in Section 4, we evaluate in terms of WSD
accuracy (Table 4) and JS divergence over the
gold-standard sense distribution (Table 5). We
also present the results for: (a) a supervised base-
line (?FS
CORPUS
?), based on the most frequent
sense in the corpus; and (b) an unsupervised base-
line (?FS
DICT
?), based on the first-listed sense in
Macmillan. In each case, the sense distribution is
based on allocating all probability mass for a given
word to the single sense identified by the respec-
tive method.
We first notice that, despite the coarser-grained
senses of Macmillan as compared to WordNet, the
upper bound WSD accuracy using Macmillan is
comparable to that of the WordNet-based datasets
over the balanced BNC, and quite a bit lower than
that of the two domain corpora of Koeling et al
(2005). This suggests that both datasets are di-
verse in domain and content.
In terms of WSD accuracy, the results over
UKWAC (ERR = 0.895) are substantially higher
than those for BNC, while those over TWITTER
(ERR = 0.716) are comparable. The accuracy is
significantly higher than the dictionary-based first
sense baseline (FS
DICT
) over both datasets (McNe-
mar?s test; p < 0.0001), and the ERR is also con-
siderably higher than for the two domain datasets
in Section 4 (FINANCE and SPORTS). One
cause of difficulty in sense-modelling TWITTER
is large numbers of missing senses, with 12.3%
of usages in TWITTER and 6.6% in UKWAC hav-
ing no corresponding Macmillan sense.
12
This chal-
lenges the assumption built into the sense preva-
lence calculation that all topics will align to a pre-
existing sense, a point we return to in Section 5.2.
12
The relative occurrence of unlisted/unclear senses in the
datasets of Koeling et al (2005) is comparable to UKWAC.
264
Dataset P R F
UKWAC 0.73 0.85 0.74
TWITTER 0.56 0.88 0.65
Table 6: Evaluation of our method for identify-
ing unattested senses, averaged over 10 runs of 10-
fold cross validation
The JS divergence results for both datasets are
well below (= better than) the results for all three
WordNet-based datasets, and also superior to both
the supervised and unsupervised first-sense base-
lines. Part of the reason for this improvement is
simply that the average polysemy in Macmillan (5.6
senses per target lemma) is slightly less than in
WordNet (6.7 senses per target lemma),
13
making
the task slightly easier in the Macmillan case.
5.2 Identification of Unattested Senses
We observed in Section 5.1 that there are rela-
tively frequent occurrences of usages (e.g. 12.3%
for TWITTER) which aren?t captured by Macmil-
lan. Conversely, there are also senses in Macmillan
which aren?t attested in the annotated sample of
usages. Specifically, of the 112 senses defined for
the 20 target lemmas, 25 (= 22.3%) of the senses
are not attested in the 2000 usages in either cor-
pora. Given that our methodology computes a
prevalence score for each sense, it can equally be
applied to the detection of these unattested senses,
and it is this task that we address in this section:
the identification of senses that are defined in the
sense inventory but not attested in a given corpus.
Intuitively, an unused sense should have low
similarity with the HDP induced topics. As such,
we introduce sense-to-topic affinity, a measure
that estimates how likely a sense is not attested in
the corpus:
st-affinity(s
i
) =
?
T
j
sim(s
i
, t
j
)
?
S
k
?
T
l
sim(s
k
, t
l
)
(3)
where sim(s
i
, t
j
) is carried over from Equa-
tion (1), and T and S represent the number of top-
ics and senses, respectively.
We treat the task of identification of unused
senses as a binary classification problem, where
the goal is to find a sense-to-topic affinity thresh-
old below which a sense will be considered to
13
Note that the set of lemmas differs between the respec-
tive datasets, so this isn?t an accurate reflection of the relative
granularity of the two dictionaries.
be unused. We pool together all the senses and
run 10-fold cross validation to learn the threshold
for identifying unused senses,
14
evaluated using
sense-level precision (P ), recall (R) and F-score
(F ) at detecting unattested senses. We repeat the
experiment 10 times (partitioning the items ran-
domly into folds) and collect the mean precision,
recall and F-scores across the 10 runs. We found
encouraging results for the task, as detailed in Ta-
ble 6. For the threshold, the average value with
standard deviation is 0.092? 0.044 over UKWAC
and 0.125?0.052 over TWITTER, indicating rela-
tive stability in the value of the threshold both in-
ternally within a dataset, and also across datasets.
5.3 Identification of Novel Senses
In both TWITTER and UKWAC, we observed fre-
quent occurrences of usages of our target nouns
which didn?t map onto a pre-existing Macmillan
sense. A natural question to ask is whether our
method can be used to predict word senses that are
missing from our sense inventory, and identify us-
ages associated with each such missing sense. We
will term these ?novel senses?, and define ?novel
sense identification? to be the task of identifying
new senses that are not recorded in the inventory
but are seen in the corpus.
An immediate complication in evaluating novel
sense identification is that we are attempting to
identify senses which explicitly aren?t in our sense
inventory. This contrasts with the identification of
unattested senses, e.g., where we were attempting
to identify which of the known senses wasn?t ob-
served in the corpus. Also, while we have annota-
tions of ?Other? usages in TWITTER and UKWAC,
there is no real expectation that all such usages
will correspond to the same sense: in practice,
they are attributable to a myriad of effects such as
incorporation in a non-compositional multiword
expression, and errors in POS tagging (i.e. the us-
age not being nominal). As such, we can?t use the
?Other? annotations to evaluate novel sense iden-
tification. The evaluation of systems for this task
is a known challenge, which we address similarly
to Erk (2006) by artificially synthesising novel
senses through removal of senses from the sense
inventory. In this way, even if we remove multi-
ple senses for a given word, we still have access
to information about which usages correspond to
14
We used a fixed step and increment at steps of 0.001, up
to the max value of st-affinity when optimising the threshold.
265
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
20 0.0?0.2 0.052?0.009 0.35 0.42 0.36
9 0.2?0.4 0.089?0.024 0.24 0.59 0.29
6 0.4?0.6 0.061?0.004 0.63 0.64 0.63
Table 7: Classification of usages with novel sense for all target lemmas.
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
9 0.2?0.4 0.093?0.023 0.50 0.66 0.52
6 0.4?0.6 0.099?0.018 0.73 0.90 0.80
Table 8: Classification of usages with novel sense for target lemmas with a removed sense.
which novel sense. An additional advantage of
this procedure is that it allows us to control an im-
portant property of novel senses: their frequency
of occurrence.
In the experiments that follow, we randomly
select senses for removal from three frequency
bands: low, medium and high frequency senses.
Frequency is defined by relative occurrence in the
annotated usages: low = 0.0?0.2; medium = 0.2?
0.4; and high = 0.4?0.6. Note that we do not con-
sider high-frequency senses with frequency higher
than 0.6, as it is rare for a medium- to high-
frequency word to take on a novel sense which
is then the predominant sense in a given corpus.
Note also that not all target lemmas will have a
novel sense through synthesis, as they may have
no senses that fall within the indicated bounds of
relative occurrence (e.g. if > 60% of usages are a
single sense). For example, only 6 of our 20 target
nouns have senses which are candidates for high-
frequency novel senses.
As before, we treat the novel sense identifica-
tion task as a classification problem, although with
a significantly different formulation: we are no
longer attempting to identify pre-existing senses,
as novel senses are by definition not included in
the sense inventory. Instead, we are seeking to
identify clusters of usages which are instances of
a novel sense, e.g. for presentation to a lexicogra-
pher as part of a dictionary update process (Run-
dell and Kilgarriff, 2011; Cook et al, 2013). That
is, for each usage, we want to classify whether it
is an instance of a given novel sense.
A usage that corresponds to a novel sense
should have a topic that does not align well with
any of the pre-existing senses in the sense inven-
tory. Based on this intuition, we introduce topic-
to-sense affinity to estimate the similarity of a
topic to the set of senses, as follows:
ts-affinity(t
j
) =
?
S
i
sim(s
i
, t
j
)
?
T
l
?
S
k
sim(s
k
, t
l
)
(4)
where, once again, sim(s
i
, t
j
) is defined as in
Equation (1), and T and S represent the number
of topics and senses, respectively.
Using topic-to-sense affinity as the sole fea-
ture, we pool together all instances and optimise
the affinity feature to classify instances that have
novel senses. Evaluation is done by computing the
mean precision, recall and F-score across 10 sepa-
rate runs; results are summarised in Table 7. Note
that we evaluate only over UKWAC in this section,
for ease of presentation.
The results show that instances with high-
frequency novel senses are more easily identifi-
able than instances with medium/low-frequency
novel senses. This is unsurprising given that high-
frequency senses have a higher probability of gen-
erating related topics (sense-related words are ob-
served more frequently in the corpus), and as such
are more easily identifiable.
We are interested in understanding whether
pooling all instances ? instances from target lem-
mas that have a sense artificially removed and
those that do not ? impacted the results (re-
call that not all target lemmas have a removed
sense). To that end, we chose to include only
instances from lemmas with a removed sense,
and repeated the experiment for the medium- and
high-frequency novel sense condition (for the low-
frequency condition, all target lemmas have a
novel sense). In other words, we are assuming
knowledge of which words have novel sense, and
the task is to identify specifically what the novel
sense is, as represented by novel usages. Results
are presented in Table 8.
266
No. of Lemmas with No. of Lemmas without Relative Freq Wilcoxon Rank Sum
a Removed Sense a Removed Sense of Removed Sense p-value
10 0 0.0?0.2 0.4543
9 11 0.2?0.4 0.0391
6 14 0.4?0.6 0.0247
Table 9: Wilcoxon Rank Sum p-value results for testing target lemmas with removed sense vs. target
lemmas without removed sense using novelty.
From the results, we see that the F-scores im-
proved notably. This reveals that an additional step
is necessary to determine whether a target lemma
has a potential novel sense before feeding its in-
stances to learn which of them contains the usage
of the novel sense.
In the last experiment, we propose a new mea-
sure to tackle this: the identification of target lem-
mas that have a novel sense. We introduce novelty,
a measure of the likelihood of a target lemma w
having a novel sense:
novelty(w) = min
t
j
(
max
s
i
sim(s
i
, t
j
)
f(t
j
)
)
(5)
where f(t
j
) is the frequency of topic t
j
in the
corpus. The intuition behind novelty is that a
target lemma with a novel sense should have a
(somewhat-)frequent topic that has low associa-
tion with any sense. That we use the frequency
rather than the probability of the topic here is de-
liberate, as topics with a higher raw number of oc-
currences (whether as a low-probability topic for
a high-frequency word, or a high-probability topic
for a low-frequency word) are indicative of a novel
word sense.
For each of our three datasets (with low-,
medium- and high-frequency novel senses, respec-
tively), we compute the novelty of the target lem-
mas and the p-value of a one-tailed Wilcoxon rank
sum test to test if the two groups of lemmas (i.e.
lemmas with a novel sense vs. lemmas without a
novel sense) are statistically different.
15
Results
are presented in Table 9. We see that the nov-
elty measure can readily identify target lemmas
with high- and medium-frequency novel senses
(p < 0.05), but the results are less promising for
the low-frequency novel senses.
6 Discussion
Our methodologies for the two proposed tasks of
identifying unused and novel senses are simple
15
Note that the number of words with low-frequency novel
senses here is restricted to 10 (cf. 20 in Table 7) to ensure we
have both positive and negative lemmas in the dataset.
extensions to demonstrate the flexibility and ro-
bustness of our methodology. Future work could
pursue a more sophisticated methodology, using
non-linear combinations of sim(s
i
, t
j
) for com-
puting the affinity measures or multiple features
in a supervised context. We contend, however,
that these extensions are ultimately a preliminary
demonstration to the flexibility and robustness of
our methodology.
A natural next step for this research would be to
couple sense distribution estimation and the detec-
tion of unattested senses with evidence from the
context, using topics or other information about
the local context (e.g. Agirre and Soroa (2009))
to carry out unsupervised WSD of individual token
occurrences of a given word.
In summary, we have proposed a topic
modelling-based method for estimating word
sense distributions, based on Hierarchical Dirich-
let Processes and the earlier work of Lau et al
(2012) on word sense induction, in probabilisti-
cally mapping the automatically-learned topics to
senses in a sense inventory. We evaluated the abil-
ity of the method to learn predominant senses and
induce word sense distributions, based on a broad
range of datasets and two separate sense invento-
ries. In doing so, we established that our method
is comparable to the approach of McCarthy et al
(2007) at predominant sense learning, and supe-
rior at inducing word sense distributions. We fur-
ther demonstrated the applicability of the method
to the novel tasks of detecting word senses which
are unattested in a corpus, and identifying novel
senses which are found in a corpus but not cap-
tured in a word sense inventory.
Acknowledgements
We wish to thank the anonymous reviewers for
their valuable comments. This research was sup-
ported in part by funding from the Australian Re-
search Council.
267
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer, Dordrecht, Netherlands.
Eneko Agirre and David Martinez. 2004. Unsuper-
vised WSD based on automatically retrieved exam-
ples: The importance of bias. In Proceedings of
EMNLP 2004, pages 25?32, Barcelona, Spain.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 7?12, Prague, Czech Republic.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the EACL (EACL
2009), pages 33?41, Athens, Greece.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356?364, Nagoya, Japan.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using WordNet. In Proceedings of the 3rd In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2002),
pages 136?145, Mexico City, Mexico.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David Blei. 2007. Putop:
Turning predominant senses into a topic model for
word sense disambiguation. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 277?281, Prague, Czech Re-
public.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proc. of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024?1033, Prague, Czech
Republic.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103?
111, Athens, Greece.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.
2007. NUS-ML: Improving word sense disam-
biguation using topic features. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 249?252, Prague, Czech Re-
public.
Marine Carpuat, Hal Daum?e III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your par-
allel data tie you to an old domain. In Proc. of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013), pages 1435?1445,
Sofia, Bulgaria.
Yee Seng Chan and Hwee Tou Ng. 2005. Word
sense disambiguation with distribution estimation.
In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI 2005), pages 1010?
1015, Edinburgh, UK.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proc. of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 89?96, Sydney, Australia.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally identifying changes in the semantic orientation
of words. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC 2010), pages 28?34, Valletta, Malta.
Paul Cook, Jey Han Lau, Michael Rundell, Diana Mc-
Carthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for de-
tecting new word senses. In Proceedings of eLex
2013, pages 49?65, Tallinn, Estonia.
Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proc. of the Main Conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 128?135, New York
City, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proc. of the 4th Web as Corpus Workshop: Can
we beat Google, pages 47?54, Marrakech, Morocco.
Spandana Gella, Paul Cook, and Timothy Baldwin. to
appear. One sense per tweeter ... and other lexical
semantic tales of Twitter. In Proceedings of the 14th
Conference of the EACL (EACL 2014), Gothenburg,
Sweden.
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the Google Books Ngram corpus.
In Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67?71, Edinburgh, UK.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
268
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57?60, New York City, USA.
Ryu Iida, Diana McCarthy, and Rob Koeling. 2008.
Gloss-based semantic similarity metrics for predom-
inant sense acquisition. In Proc. of the Third In-
ternational Joint Conference on Natural Language
Processing, pages 561?568.
Jay Jiang and David Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings on International Conference on Re-
search in Computational Linguistics, pages 19?33,
Taipei, Taiwan.
Peng Jin, Diana McCarthy, Rob Koeling, and John Car-
roll. 2009. Estimating and exploiting the entropy
of sense distributions. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics ? Human Language Technologies
2009 (NAACL HLT 2009): Short Papers, pages 233?
236, Boulder, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290?299, Atlanta, USA.
Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? Technical Report ITRI-04-10,
Information Technology Research Institute, Univer-
sity of Brighton.
Johannes Knopp, Johanna V?olker, and Simone Paolo
Ponzetto. 2013. Topic modeling for word sense in-
duction. In Proc. of the International Conference of
the German Society for Computational Linguistics
and Language Technology, pages 97?103, Darm-
stadt, Germany.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419?
426, Vancouver, Canada.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?75.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings
of the 13th Conference of the EACL (EACL 2012),
pages 591?601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a.
unimelb: Topic modelling-based word sense induc-
tion. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013), pages
307?311, Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b.
unimelb: Topic modelling-based word sense induc-
tion for web snippet clustering. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 217?221, Atlanta, USA.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 1986 SIGDOC Conference, pages 24?26, On-
tario, Canada.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proc. of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1138?1147, Uppsala,
Sweden.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the ACL and 17th International Confer-
ence on Computational Linguistics (COLING/ACL-
98), pages 768?774, Montreal, Canada.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25?30, Jeju, Republic of Ko-
rea.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task
14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 63?68, Uppsala, Swe-
den.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004a. Automatic identification of infre-
quent word senses. In Proc. of the 20th International
Conference of Computational Linguistics, COLING-
2004, pages 1220?1226, Geneva, Switzerland.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004b. Finding predominant senses in
untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2004), pages 280?287, Barcelona,
Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553?590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of the ARPA Workshop on Human Language
Technology, pages 303?308.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
269
Saif Mohammad and Graeme Hirst. 2006. Determin-
ing word sense dominance using a thesaurus. In
Proc. of EACL-2006, pages 121?128, Trento, Italy.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?
201, Atlanta, USA.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 27(7):1075?1088.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30?35, Prague, Czech Republic.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49?56, Boston,
USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Judita Preiss and Mark Stevenson. 2013. Unsuper-
vised domain tuning to improve word sense dis-
ambiguation. In Proc. of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 680?684, Atlanta, USA.
Michael Rundell and Adam Kilgarriff. 2011. Au-
tomating the creation of dictionaries: where will
it all end? In Fanny Meunier, Sylvie De
Cock, Ga?etanelle Gilquin, and Magali Paquot, ed-
itors, A Taste for Corpora. In honour of Sylviane
Granger, pages 257?282. John Benjamins, Amster-
dam, Netherlands.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101:1566?1581.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, USA.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proc. of the ACL 2010 System
Demonstrations, pages 78?83, Uppsala, Sweden.
270
Automatic Detection and Language Identification of Multilingual Documents
Marco Lui??, Jey Han Lau? and Timothy Baldwin??
? Department of Computing and Information Systems
The University of Melbourne
? NICTA Victoria Research Laboratory
? Department of Philosophy
King?s College London
mhlui@unimelb.edu.au, jeyhan.lau@gmail.com, tb@ldwin.net
Abstract
Language identification is the task of automat-
ically detecting the language(s) present in a
document based on the content of the docu-
ment. In this work, we address the problem
of detecting documents that contain text from
more than one language (multilingual docu-
ments). We introduce a method that is able to
detect that a document is multilingual, iden-
tify the languages present, and estimate their
relative proportions. We demonstrate the ef-
fectiveness of our method over synthetic data,
as well as real-world multilingual documents
collected from the web.
1 Introduction
Language identification is the task of automatically
detecting the language(s) present in a document
based on the content of the document. Language
identification techniques commonly assume that ev-
ery document is written in one of a closed set of
known languages for which there is training data,
and is thus formulated as the task of selecting the
most likely language from the set of training lan-
guages. In this work, we remove this monolingual
assumption, and address the problem of language
identification in documents that may contain text
from more than one language from the candidate set.
We propose a method that concurrently detects that a
document is multilingual, and estimates the propor-
tion of the document that is written in each language.
Detecting multilingual documents has a variety
of applications. Most natural language processing
techniques presuppose monolingual input data, so
inclusion of data in foreign languages introduces
noise, and can degrade the performance of NLP sys-
tems (Alex et al., 2007; Cook and Lui, 2012). Au-
tomatic detection of multilingual documents can be
used as a pre-filtering step to improve the quality of
input data. Detecting multilingual documents is also
important for acquiring linguistic data from the web
(Scannell, 2007; Abney and Bird, 2010), and has
applications in mining bilingual texts for statistical
machine translation from online resources (Resnik,
1999; Nie et al., 1999; Ling et al., 2013). There has
been particular interest in extracting text resources
for low-density languages from multilingual web
pages containing both the low-density language and
another language such as English (Yamaguchi and
Tanaka-Ishii, 2012; King and Abney, 2013). King
and Abney (2013, p1118) specifically mention the
need for an automatic method ?to examine a mul-
tilingual document, and with high accuracy, list the
languages that are present in the document?.
We introduce a method that is able to detect multi-
lingual documents, and simultaneously identify each
language present as well as estimate the propor-
tion of the document written in that language. We
achieve this with a probabilistic mixture model, us-
ing a document representation developed for mono-
lingual language identification (Lui and Baldwin,
2011). The model posits that each document is gen-
erated as samples from an unknown mixture of lan-
guages from the training set. We introduce a Gibbs
sampler to map samples to languages for any given
set of languages, and use this to select the set of lan-
guages that maximizes the posterior probability of
the document.
27
Transactions of the Association for Computational Linguistics, 2 (2014) 27?40. Action Editor: Kristina Toutanova.
Submitted 1/2013; Revised 7/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
Our method is able to learn a language identi-
fier for multilingual documents from monolingual
training data. This is an important property as there
are no standard corpora of multilingual documents
available, whereas corpora of monolingual docu-
ments are readily available for a reasonably large
number of languages (Lui and Baldwin, 2011). We
demonstrate the effectiveness of our method empir-
ically, firstly by evaluating it on synthetic datasets
drawn from Wikipedia data, and then by applying it
to real-world data, showing that we are able to iden-
tify multilingual documents in targeted web crawls
of minority languages (King and Abney, 2013).
Our main contributions are: (1) we present a
method for identifying multilingual documents, the
languages contained therein and the relative propor-
tion of the document in each language; (2) we show
that our method outperforms state-of-the-art meth-
ods for language identification in multilingual doc-
uments; (3) we show that our method is able to es-
timate the proportion of the document in each lan-
guage to a high degree of accuracy; and (4) we show
that our method is able to identify multilingual doc-
uments in real-world data.
2 Background
Most language identification research focuses on
language identification for monolingual documents
(Hughes et al., 2006). In monolingual LangID, the
task is to assign each documentD a unique language
Li ? L. Some work has reported near-perfect accu-
racy for language identification of large documents
in a small number of languages (Cavnar and Tren-
kle, 1994; McNamee, 2005). However, in order to
attain such accuracy, a large number of simplifying
assumptions have to be made (Hughes et al., 2006;
Baldwin and Lui, 2010a). In this work, we tackle
the assumption that each document is monolingual,
i.e. it contains text from a single language.
In language identification, documents are mod-
eled as a stream of characters (Cavnar and Trenkle,
1994; Kikui, 1996), often approximated by the cor-
responding stream of bytes (Kruengkrai et al., 2005;
Baldwin and Lui, 2010a) for robustness over vari-
able character encodings. In this work, we follow
Baldwin and Lui (2010a) in training a single model
for languages that naturally use multiple encodings
(e.g. UTF8, Big5 and GB encodings for Chinese), as
issues of encoding are not the focus of this research.
The document representation used for language
identification generally involves estimating the rel-
ative distributions of particular byte sequences, se-
lected such that their distributions differ between
languages. In some cases the relevant sequences
may be externally specified, such as function words
and common suffixes (Giguet, 1995) or grammati-
cal word classes (Dueire Lins and Gonc?alves, 2004),
though they are more frequently learned from la-
beled data (Cavnar and Trenkle, 1994; Grefenstette,
1995; Prager, 1999a; Lui and Baldwin, 2011).
Learning algorithms applied to language identi-
fication fall into two general categories: Bayesian
classifiers and nearest-prototype (Rocchio-style)
classifiers. Bayesian approaches include Markov
processes (Dunning, 1994), naive Bayes methods
(Grefenstette, 1995; Lui and Baldwin, 2011; Tiede-
mann and Ljubes?ic?, 2012), and compressive mod-
els (Teahan, 2000). The nearest-prototype methods
vary primarily in the distance measure used, includ-
ing measures based on rank order statistics (Cav-
nar and Trenkle, 1994), information theory (Bald-
win and Lui, 2010a), string kernels (Kruengkrai et
al., 2005) and vector space models (Prager, 1999a;
McNamee, 2005).
Language identification has been applied in do-
mains such as USENET messages (Cavnar and
Trenkle, 1994), web pages (Kikui, 1996; Mar-
tins and Silva, 2005; Liu and Liang, 2008), web
search queries (Ceylan and Kim, 2009; Bosca and
Dini, 2010), mining the web for bilingual text
(Resnik, 1999; Nie et al., 1999), building minor-
ity language corpora (Ghani et al., 2004; Scannell,
2007; Bergsma et al., 2012) as well as a large-
scale database of Interlinear Glossed Text (Xia et al.,
2010), and the construction of a large-scale multilin-
gual web crawl (Callan and Hoy, 2009).
2.1 Multilingual Documents
Language identification over documents that contain
text from more than one language has been identified
as an open research question (Hughes et al., 2006).
Common examples of multilingual documents are
web pages that contain excerpts from another lan-
guage, and documents from multilingual organiza-
tions such as the European Union.
28
English French Italian German Dutch Japanese
character the pour di auf voo ?
byte 74 68 65 20 70 6F 75 7 20 64 69 20 20 61 75 66 76 6F 6 E3 81 AF
Table 1: Examples of per-language byte sequences selected by information gain.
The Australiasian Language Technology Work-
shop 2010 hosted a shared task where participants
were required to predict the language(s) present in a
held-out test set containing monolingual and bilin-
gual documents (Baldwin and Lui, 2010b). The
dataset was prepared using data from Wikipedia, and
bilingual documents were produced using a segment
from a page in one language, and a segment from the
same page in another language. We use the dataset
from this shared task for our initial experiments.
To the authors? knowledge, the only other work to
directly tackle identification of multiple languages
and their relative proportions in a single document is
the LINGUINI system (Prager, 1999a). The system
is based on a vector space model, and cosine simi-
larity between a feature vector for the test document
and a feature vector for each language Li, computed
as the sum of feature vectors for all the documents
for language Li in the training data. The elements
in the feature vectors are frequency counts over
byte n-grams (2?n?5) and words. Language iden-
tification for multilingual documents is performed
through the use of virtual mixed languages. Prager
(1999a) shows how to construct vectors representa-
tive of particular combinations of languages inde-
pendent of the relative proportions, and proposes a
method for choosing combinations of languages to
consider for any given document.
Language identification in multilingual docu-
ments could also be performed by application of su-
pervised language segmentation algorithms. Given
a system that can segment a document into la-
beled monolingual segments, we can then extract
the languages present as well as the relative propor-
tion of text in each language. Several methods for
supervised language segmentation have been pro-
posed. Teahan (2000) proposed a system based on
text compression that identifies multilingual docu-
ments by first segmenting the text into monolingual
blocks. Rehurek and Kolkus (2009) perform lan-
guage segmentation by computing a relevance score
between terms and languages, smoothing across ad-
joining terms and finally identifying points of transi-
tion between high and low relevance, which are in-
terpreted as boundaries between languages. Yam-
aguchi and Tanaka-Ishii (2012) use a minimum de-
scription length approach, embedding a compressive
model to compute the description length of text seg-
ments in each language. They present a linear-time
dynamic programming solution to optimize the lo-
cation of segment boundaries and language labels.
3 Methodology
Language identification for multilingual documents
is a multi-label classification task, in which a doc-
ument can be mapped onto any number of labels
from a closed set. In the remainder of this paper,
we denote the set of all languages by L. We de-
note a document D which contains languages Lx
and Ly as D ? {Lx, Ly}, where Lx, Ly ? L.
We denote a document that does not contain a lan-
guage Lx by D ? {Lx}, though we generally omit
all the languages not contained in the document for
brevity. We denote classifier output using .; e.g.
D . {La, Lb} indicates that document D has been
predicted to contain text in languages La and Lb.
3.1 Document Representation and Feature
Selection
We represent each document D as a frequency dis-
tribution over byte n-gram sequences such as those
in Table 1. Each document is converted into a vector
where each entry counts the number of times a par-
ticular byte n-gram is present in the document. This
is analogous to a bag-of-words model, where the vo-
cabulary of ?words? is a set of byte sequences that
has been selected to distinguish between languages.
The exact set of features is selected from the
training data using Information Gain (IG), an
information-theoretic metric developed as a split-
ting criterion for decision trees (Quinlan, 1993). IG-
based feature selection combined with a naive Bayes
classifier has been shown to be particularly effective
for language identification (Lui and Baldwin, 2011).
29
3.2 Generative Mixture Models
Generative mixture models are popular for text mod-
eling tasks where a mixture of influences governs the
content of a document, such as in multi-label doc-
ument classification (McCallum, 1999; Ramage et
al., 2009), and topic modeling (Blei et al., 2003).
Such models normally assume full exchangeability
between tokens (i.e. the bag-of-words assumption),
and label each token with a single discrete label.
Multi-label text classification, topic modeling and
our model for language identification in multilingual
documents share the same fundamental representa-
tion of the latent structure of a document. Each la-
bel is modeled with a probability distribution over
tokens, and each document is modeled as a proba-
bilistic mixture of labels. As presented in Griffiths
and Steyvers (2004), the probability of the ith token
(wi) given a set of T labels z1? ? ?zT is modeled as:
P (wi) =
T?
j=1
P (wi|zi = j)P (zi = j) (1)
The set of tokens w is the document itself, which
in all cases is observed. In the case of topic model-
ing, the tokens are words and the labels are topics,
and z is latent. Whereas topic modeling is gener-
ally unsupervised, multi-label text classification is
a supervised text modeling task, where the labels
are a set of pre-defined categories (such as RUBBER,
IRON-STEEL, TRADE, etc. in the popular Reuters-
21578 data set (Lewis, 1997)), and the tokens are
individual words in documents. z is still latent, but
constrained in the training data (i.e. documents are
labeled but the individual words are not). Some ap-
proaches to labeling unseen documents require that
z for the training data be inferred, and methods for
doing this include an application of the Expectation-
Maximization (EM) algorithm (McCallum, 1999)
and Labeled LDA (Ramage et al., 2009).
The model that we propose for language identifi-
cation in multilingual documents is similar to multi-
label text classification. In the framework of Equa-
tion 1, each per-token label zi is a language and the
vocabulary of tokens is not given by words but rather
by specific byte sequences (Section 3.1). The key
difference with multi-label text classification is that
we use monolingual (i.e. mono-label) training data.
Hence, z is effectively observed for the training data
(since all tokens must share the same label). To infer
z for unlabeled documents, we utilize a Gibbs sam-
pler, closely related to that proposed by Griffiths and
Steyvers (2004) for LDA. The sampling probability
for a label zi for token w in a document d is:
P (zi = j|z?i, w) ? ?(w)j ? ?
(d)
j (2)
?(w)j = P (wi|zi = j, z?i, w?i)
?(d)j = P (zi = j|z?i)
In the LDA model, ?(d)j is assumed to have a Dirich-
let distribution with hyperparameter ?, and the word
distribution for each topic ?(w)j is also assumed to
have a Dirichlet distribution with hyperparameter
?. Griffiths (2002) describes a generative model for
LDA where both ?(w)j and ?(d)j are inferred from
the output of a Gibbs sampler. In our method, we
estimate ?(w)j using maximum likelihood estima-
tion (MLE) from the training data. Estimating ?(w)j
through MLE is equivalent to a multinomial Naive
Bayes model (McCallum and Nigam, 1998):
??(w)j =
n(w)j + ?
n(.)j +W?
(3)
where n(w)j is the number of times word w occurs
with label j, and n(.)j is the total number of words
that occur with label j. By setting ? to 1, we obtain
standard Laplacian smoothing. Hence, only ??(d)j is
updated at each step in the Gibbs sampler:
??(d)j =
n(d)?i,j + ?
n(d)?i + T?
(4)
where n(d)?i,j is the number of tokens in document d
that are currently mapped to language j, and n(d)?i is
the total number of tokens in document d. In both
cases, the current assignment of zi is excluded from
the count. T is the number of languages (i.e. the size
of the label set). For simplicity, we set ? to 0. We
note that in the LDA model, ? and ? influence the
sparsity of the solution, and so it may be possible
to tune these parameters for our model as well. We
leave this as an avenue for further research.
30
3.3 Language Identification in Multilingual
Documents
The model described in Section 3.2 can be used to
compute the most likely distribution to have gen-
erated an unlabeled document over a given set of
languages for which we have monolingual training
data, by letting the set of terms w be the byte n-gram
sequences we selected using per-language informa-
tion gain (Section 3.1), and allowing the labels z to
range over the set of all languages L. Using train-
ing data, we compute ??(w)j (Equation 3), and then
we infer P (Lj |D) for each Lj ? L for the unla-
beled document, by running the Gibbs sampler until
the samples for zi converge and then tabulating zi
over the whole d and normalizing by |d|. Naively,
we could identify the languages present in the doc-
ument by D . {Lx if ?(zi = Lx|D)}, but closely-
related languages tend to have similar frequency dis-
tributions over byte n-gram features, and hence it is
likely that some tokens will be incorrectly mapped to
a language that is similar to the ?correct? language.
We address this issue by finding the subset of lan-
guages ? from the training set L that maximizes
P (?|D) (a similar approach is taken in McCallum
(1999)). Through an application of Bayes? theorem,
P (?|D) ? P (D|?)?P (?), noting that P (D) is a
normalizing constant and can be dropped. We as-
sume that P (?) is constant (i.e. any subset of lan-
guages is equally likely, a reasonable assumption in
the absence of other evidence), and hence maximize
P (D|?). For any given D = w1? ? ?wn and ?, we
infer P (D|?) from the output of the Gibbs sampler:
P (D|?) =
N?
i=1
P (wi|?) (5)
=
N?
i=1
?
j??
P (wi|zi = j)P (zi = j) (6)
where both P (wi|zi = j) and P (zi = j) are esti-
mated by their maximum likelihood estimates.
In practice, exhaustive evaluation of the powerset
of L is prohibitively expensive, and so we greed-
ily approximate the optimal ? using Algorithm 1. In
essence, we initially rank all the candidate languages
by computing the most likely distribution over the
full set of candidate languages. Then, for each of
the top-N languages in turn, we consider whether
Algorithm 1 DetectLang(L,D)
LN ? top-N z ? L by P (z|D)
?? {Lu}
for each Lt ? LN do
?? ? ? ? Lt
if P (D|?) + t < P (D|??) then
?? ??
end if
end for
?? ? \ {Lu}
return D . ?
to add it to ?. ? is initialized with Lu, a dummy
language with a uniform distribution over terms (i.e.
P (w|Lu) = 1|w| ). A language is added if it improves
P (D|?) by at least t. The threshold t is required
to suppress the addition of spurious classes. Adding
languages gives the model additional freedom to fit
parameters, and so will generally increase P (D|?).
In the limit case, adding a completely irrelevant lan-
guage will result in no tokens being mapped to the
a language, and so the model will be no worse than
without the language. The threshold t is thus used to
control ?how much? improvement is required before
including the new language in ?.
3.4 Benchmark Approaches
We compare our approach to two methods for
language identification in multilingual documents:
(1) the virtual mixed languages approach (Prager,
1999a); and (2) the text segmentation approach (Ya-
maguchi and Tanaka-Ishii, 2012).
Prager (1999a) describes LINGUINI, a language
identifier based on the vector-space model com-
monly used in text classification and information re-
trieval. The document representation used by Prager
(1999a) is a vector of counts across a set of charac-
ter sequences. Prager (1999a) selects the feature set
based on a TFIDF-like approach. Terms with occur-
rence count m < n? k are rejected, where m is the
number of times the term occurs in the training data
(the TF component), n is the number of languages in
which the term occurred (the IDF component, where
?document? is replaced with ?language?), and k is a
parameter to control the overall number of terms se-
lected. In Prager (1999a), the value of k is reported
to be optimal in the region 0.3 to 0.5. In practice,
31
the value of k indirectly controls the number of fea-
tures selected. Values of k are not comparable across
datasets as m is not normalized for the size of the
training data, so in this work we do not report the
values of k and instead directly select the top-N fea-
tures, weighted by mn . In LINGUINI, each languageis modeled as a single pseudo-document, obtained
by concatenating all the training data for the given
language. A document is then classified according
to the vector with which it has the smallest angle;
this is implemented by finding the language vector
with the highest cosine with the document vector.
Prager (1999a) also proposes an extension to the
approach to allow identification of bilingual docu-
ments, and suggests how this may be generalized to
any number of languages in a document. The gist
of the method is simple: for any given pair of lan-
guages, the projection of a document vector onto
the hyperplane containing the language vectors of
the two languages gives the mixture proportions of
the two languages that minimizes the angle with the
document vector. Prager (1999a) terms this projec-
tion a virtual mixed language (VML), and shows
how to find the angle between the document vec-
tor and the VML. If this angle is less than that be-
tween the document vector and any individual lan-
guage vector, the document is labeled as bilingual in
the two languages from which the mixed vector was
derived. The practical difficulty presented by this
approach is that exhaustively evaluating all possible
combinations of languages is prohibitively expen-
sive. Prager (1999a) addresses this by arguing that in
multilingual documents, ?the individual component
languages will be close to d (the document vector)
? probably closer than most or all other languages?.
Hence, language mixtures are only considered for
combinations of the top m languages.
Prager (1999a) shows how to obtain the mixture
coefficients for bilingual VMLs, arguing that the
process generalizes. Prager (1999b) includes the
coefficients for 3-language VMLs, which are much
more complex than the 2-language variants. Us-
ing a computer algebra system, we verified the an-
alytic forms of the coefficients in the 3-language
VML. We also attempted to obtain an analytic form
for the coefficients in a 4-language VML, but these
were too complex for the computer algebra system
to compute. Thus, our evaluation of the VML ap-
proach proposed by Prager (1999a) is limited to 3-
language VMLs. Neither Prager (1999a) nor Prager
(1999b) include an empirical evaluation over mul-
tilingual documents, so to the best of our knowl-
edge this paper is the first empirical evaluation of
the method on multilingual documents. As no refer-
ence implementation of this method is available, we
have produced our own implementation, which we
have made freely available.1
The other benchmark we consider in this paper is
the method for text segmentation by language pro-
posed by Yamaguchi and Tanaka-Ishii (2012) (here-
after referred to as SEGLANG). The actual task ad-
dressed by Yamaguchi and Tanaka-Ishii (2012) is to
divide a document into monolingual segments. This
is formulated as the task of segmenting a document
D = x1, ? ? ? , x|D| (where xi denotes the ith char-
acter of D and |D| is the length of the document)
by finding a list of boundaries B = [B1, ? ? ? , B|B|]
where each Bi indicates the location of a language
boundary as an offset from the start of the document,
resulting in a list of segments X = [X0, ? ? ? , X|B|].
For each segment Xi, the system predicts Li, the
language associated with the segment, producing a
list of labellings L = [L0, ? ? ? , L|B|], with the con-
straint that adjacent elements in L must differ. Ya-
maguchi and Tanaka-Ishii (2012) solve the problem
of determining X and L for an unlabeled text us-
ing a method based on minimum description length.
They present a dynamic programming solution to
this problem, and analyze a number of parameters
that affect the overall accuracy of the system. Given
this method to determine X and L, it is then triv-
ial to label an unlabeled document according to
D . {Lx if ?Lx ? L}, and the length of each seg-
ment in X can then be used to determine the pro-
portions of the document that are in each language.
In this work, we use a reference implementation of
SEGLANG kindly provided to us by the authors.
Using the text segmentation approach of
SEGLANG to detect multilingual documents differs
from LINGUINI and our method primarily in that
LINGUINI and our method fragment the document
into small sequences of bytes, and discard informa-
tion about the relative order of the fragments. This
is in contrast to SEGLANG, where this information
1https://github.com/saffsd/linguini.py
32
System PM RM FM P? R? F?
Benchmark .497 .467 .464 .833 .826 .829
Winner .718 .703 .699 .932 .931 .932
SEGLANG .801 .810 .784 .866 .946 .905
LINGUINI .616 .535 .513 .713 .688 .700
Our method .753 .771 .748 .945 .922 .933
Table 2: Results on the ALTW2010 dataset.
?Benchmark? is the benchmark system proposed by
the shared task organizers. ?Winner? is the highest-
F? system submitted to the shared task.
is utilized in the sequential prediction of labels for
consecutive segments of text, and is thus able to
make better use of the locality of text (since there are
likely to be monolingual blocks of text in any given
multilingual document). The disadvantage of this is
that the underlying model becomes more complex
and hence more computationally expensive, as we
observe in Section 5.
3.5 Evaluation
We seek to evaluate the ability of each method:
(1) to correctly identify the language(s) present in
each test document; and (2) for multilingual doc-
uments, to estimate the relative proportion of the
document written in each language. In the first in-
stance, this is a classification problem, and the stan-
dard notions of precision (P), recall (R) and F-score
(F) apply. Consistent with previous work in lan-
guage identification, we report both the document-
level micro-average, as well as the language-level
macro-average. For consistency with Baldwin and
Lui (2010a), the macro-averaged F-score we report
is the average of the per-class F-scores, rather than
the harmonic mean of the macro-averaged precision
and recall; as such, it is possible for the F-score
to not fall between the precision and recall values.
As is common practice, we compute the F-score for
? = 1, giving equal importance to precision and
recall.2 We tested the difference in performance
for statistical significance using an approximate ran-
domization procedure (Yeh, 2000) with 10000 iter-
ations. Within each table of results (Tables 2, 3 and
2Intuitively, it may seem that the maximal precision and re-
call should be achieved when precision and recall are balanced.
However, because of the multi-label nature of the task and vari-
able number of labels assigned to a given document by our mod-
els, it is theoretically possible and indeed common in our results
for the maximal macro-averaged F-score to be achieved when
macro-averaged precision and recall are not balanced.
4), all differences between systems are statistically
significant at a p < 0.05 level.
To evaluate the predictions of the relative propor-
tions of a document D written in each detected lan-
guageLi, we compare the topic proportion predicted
by our model to the gold-standard proportion, mea-
sured as a byte ratio as follows:
gs(Li|D) =
length of Li part of D in bytes
length of D in bytes (7)
We report the correlation between predicted and ac-
tual proportions in terms of Pearson?s r coefficient.
We also report the mean absolute error (MAE) over
all document?language pairs.
4 Experiments on ALTW2010
Our first experiment utilizes the ALTW2010 shared
task dataset (Baldwin and Lui, 2010b), a synthetic
dataset of 10000 bilingual documents3 generated
from Wikipedia data, introduced in the ALTW2010
shared task,4 The dataset is organized into training,
development and test partitions. Following standard
machine learning practice, we train each system us-
ing the training partition, and tune parameters using
the development partition. We then report macro and
micro-averaged precision, recall and F-score on the
test partition, using the tuned parameters.
The results on the ALTW2010 shared task dataset
are summarized in Table 2. Each of the three sys-
tems we compare was re-trained using the training
data provided for the shared task, with a slight dif-
ference: in the shared task, participants were pro-
vided with multilingual training documents, but the
systems targeted in this research require monolin-
gual training data. We thus split the training doc-
uments into monolingual segments using the meta-
data provided with the dataset. The metadata was
only published after completion of the task and was
not available to task participants. For comparison,
we have included the benchmark results published
by the shared task organizers, as well as the score
attained by the winning entry (Tran et al., 2010).
3With a small number of monolingual documents, formed
by randomly selecting the two languages for a given docu-
ment independently, leaving the possibility of the same two lan-
guages being selected.
4http://comp.mq.edu.au/programming/task_
description/
33
We tune the parameters for each system using the
development partition of the dataset, and report re-
sults on the test partition. For LINGUINI, there is a
single parameter k to be tuned: the number of fea-
tures per language. We tested values between 10000
and 50000, and selected 46000 features as the opti-
mal value. For our method, there are two parameters
to be tuned: (1) the number of features selected for
each language, and (2) the threshold t for including
a language. We tested features-per-language counts
between 30 and 150, and found that adding features
beyond 70 per language had minimal effect. We
tested values of the threshold t from 0.01 to 0.15,
and found the best value was 0.14. For SEGLANG,
we introduce a threshold t on the minimum propor-
tion of a document (measured in bytes) that must
be labeled by a language before that language is in-
cluded in the output set. This was done because our
initial experiments indicate that SEGLANG tends to
over-produce labels. Using the development data,
we found the best value of t was 0.10.
We find that of the three systems tested, two out-
perform the winning entry to the shared task. This
is more evident in the macro-averaged results than
in the micro-averaged results. In micro-averaged
terms, our method is the best performer, whereas
on the macro-average, SEGLANG has the high-
est F-score. This suggests that our method does
well on higher-density languages (relative to the
ALTW2010 dataset), and poorly on lower-density
languages. This also accounts for the higher micro-
averaged precision but lower micro-averaged recall
for our method as compared to SEGLANG. The im-
proved macro-average F-score of SEGLANG comes
at a much higher computational cost, which in-
creases dramatically as the number of languages is
increased. In our testing on a 16-core worksta-
tion, SEGLANG took almost 24 hours to process the
ALTW2010 shared task test data, compared to 2
minutes for our method and 40 seconds for LIN-
GUINI. As such, SEGLANG is poorly suited to de-
tecting multilingual documents where a large num-
ber of candidate languages is considered.
The ALTW2010 dataset is an excellent starting
point for this research, but it predominantly contains
bilingual documents, making it difficult to assess the
ability of systems to distinguish multilingual docu-
ments from monolingual ones. Furthermore, we are
unable to use it to assess the ability of systems to
detect more than 2 languages in a document. To ad-
dress these shortcomings, we construct a new dataset
in a similar vein. The dataset and experiments per-
formed on it are described in the next section.
5 Experiments on WIKIPEDIAMULTI
To fully test the capabilities of our model, we gen-
erated WIKIPEDIAMULTI, a dataset that contains
a mixture of monolingual and multilingual docu-
ments. To allow for replicability of our results and
to facilitate research in language identification, we
have made the dataset publicly available.5 WIKI-
PEDIAMULTI is generated using excerpts from the
mediawiki sources of Wikipedia pages downloaded
from the Wikimedia foundation.6 The dumps we
used are from July?August 2010.
To generate WIKIPEDIAMULTI, we first normal-
ized the raw mediawiki documents. Mediawiki doc-
uments typically contain one paragraph per line, in-
terspersed with structural elements. We filtered each
document to remove all structural elements, and
only kept documents that exceeded 2500 bytes after
normalization. This yielded a collection of around
500,000 documents in 156 languages. From this
initial document set (hereafter referred to as WI-
KICONTENT), we only retained languages that had
more than 1000 documents (44 languages), and gen-
erated documents for WIKIPEDIAMULTI as follows:
1. randomly select the number of languages K
(1?K?5)
2. randomly select a set of K languages S =
{Li?L for i = 1? ? ?K} without replacement
3. randomly select a document for each Li?S
from WIKICONTENT without replacement
4. take the top 1K lines of the document5. join the K sections into a single document.
As a result of the procedure, the relative propor-
tion of each language in a multilingual document
tends not to be uniform, as it is conditioned on the
length of the original document from which it was
sourced, independent of the otherK?1 for the other
languages that it was combined with. Overall, the
average document length is 5500 bytes (standard de-
viation = 3800 bytes). Due to rounding up in taking
5http://www.csse.unimelb.edu.au/?tim/
6http://dumps.wikimedia.org
34
System PM RM FM P? R? F?
SEGLANG .809 .975 .875 .771 .975 .861
LINGUINI .853 .772 .802 .838 .774 .805
Our method .962 .954 .957 .963 .955 .959
Table 3: Results on the WIKIPEDIAMULTI dataset.
the top 1k lines (step 4), documents with higher Ktend to be longer (6200 bytes for K = 5 vs 5100
bytes for K = 1).
The WIKIPEDIAMULTI dataset contains training,
development and test partitions. The training parti-
tion consists of 5000 monolingual (i.e. K = 1) doc-
uments. The development partition consists of 5000
documents, 1000 documents for each value of K
where 1?K?5. The test partition contains 200 doc-
uments for each K, for a total of 1000 documents.
There is no overlap between any of the partitions.
5.1 Results over WIKIPEDIAMULTI
We trained each system using the monolingual train-
ing partition, and tuned parameters using the devel-
opment partition. For LINGUINI, we tested feature
counts between 10000 and 50000, and found that
the effect was relatively small. We thus use 10000
features as the optimum value. For SEGLANG, we
tested values for threshold t between 0.01 and 0.20,
and found that the maximal macro-averaged F-score
is attained when t = 0.06. Finally, for our method
we tested features-per-language counts between 30
and 130 and found the best performance with 120
features per language, although the actual effect of
varying this value is rather small. We tested values
of the threshold t for adding an extra language to
? from 0.01 to 0.15, and found that the best results
were attained when t = 0.02.
The results of evaluating each system on the
test partition are summarized in Table 3. In this
evaluation, our method clearly outperforms both
SEGLANG and LINGUINI. The results on WIKI-
PEDIAMULTI and ALTW2010 are difficult to com-
pare directly due to the different compositions of the
two datasets. ALTW2010 is predominantly bilin-
gual, whereas WIKIPEDIAMULTI contains docu-
ments with text in 1?5 languages. Furthermore, the
average document in ALTW2010 is half the length
of that in WIKIPEDIAMULTI. Overall, we observe
that SEGLANG has a tendency to over-label (despite
the introduction of the t parameter to reduce this ef-
fect), evidenced by high recall but lower precision.
LINGUINI is inherently limited in that it is only able
to detect up to 3 languages per document, causing
recall to suffer on WIKIPEDIAMULTI. However, it
also tends to always output 3 languages, regardless
of the actual number of languages in the document,
hurting precision. Furthermore, even on ALTW2010
it has lower recall than the other two systems.
6 Estimating Language Proportions
In addition to detecting multiple languages within
a document, our method also estimates the relative
proportions of the document that are written in each
language. This information may be useful for detect-
ing documents that are candidate bitexts for training
machine translation systems, since we may expect
languages in the document to be present in equal
proportions. It also allows us to identify the pre-
dominant language of a document.
A core element of our model of a document is
a distribution over a set of labels. Since each la-
bel corresponds to a language, as a first approxima-
tion, we take the probability mass associated with
each label as a direct estimate of the proportion of
the document written in that language. We examine
the results for predicting the language proportions
in the test partition of WIKIPEDIAMULTI. Mapping
label distributions directly to language proportions
produces excellent results, with a Pearson?s r value
of 0.863 and an MAE of 0.108.
Although labels have a one-to-one correspon-
dence with languages, the label distribution does
not actually correspond directly to the language pro-
portion, because the distribution estimates the pro-
portion of byte n-gram sequences associated with
a label and not the proportion of bytes directly.
The same number of bytes in different languages
can produce different numbers of n-gram sequences,
because after feature selection not all n-gram se-
quences are retained in the feature set. Hereafter,
we refer to each n-gram sequence as a token, and the
average number of tokens produced per byte of text
as the token emission rate.
We estimate the per-language token emission rate
(Figure 1) using the training partition of WIKIPE-
DIAMULTI. To improve our estimate of the lan-
guage proportions, we correct our label distribution
35
Original text the cat in the hat
n-gram features
?
???
???
he : 2 the : 2
hat : 1 in : 1
th : 1 the : 1
hat : 1 he c : 1
in t : 1 n th : 1
?
???
???
Emission rate #bytes#tokens = 1812 = 1.5 bytes/token
Figure 1: Example of calculating n-gram emission
rate for a text string.
using estimates of the per-language token emission
rate RLi in bytes per token for Li?L. Assume that
a document D of length |D| is estimated to contain
K languages in proportions Pi for i = 1? ? ?K. The
corrected estimate for the proportion of Li is:
Prop(Li) =
Pi ?RLi?K
j=1 (Pj ?RLj )
(8)
Note that the |D| term is common to the numerator
and denominator and has thus been eliminated.
This correction improves our estimates of lan-
guage proportions. After correction, the Pearson?s
r rises to 0.981, and the MAE is reduced to 0.024.
The improvement is most noticeable for language?
document pairs where the proportion of the docu-
ment in the given language is about 0.5 (Figure 2).
7 Real-world Multilingual Documents
So far, we have demonstrated the effectiveness of
our proposed approach using synthetic data. The
results have been excellent, and in this section we
validate the approach by applying it to a real-world
task that has recently been discussed in the lit-
erature. Yamaguchi and Tanaka-Ishii (2012) and
King and Abney (2013) both observe that in trying
to gather linguistic data for ?non-major? languages
from the web, one challenge faced is that documents
retrieved often contain sections in another language.
SEGLANG (the solution of Yamaguchi and Tanaka-
Ishii (2012)) concurrently detects multilingual doc-
uments and segments them by language, but the ap-
proach is computationally expensive and has a ten-
dency to over-label (Section 5). On the other hand,
the solution of King and Abney (2013) is incom-
plete, and they specifically mention the need for an
automatic method ?to examine a multilingual docu-
ment, and with high accuracy, list the languages that
are present in the document?. In this section, we
show that our method is able to fill this need. We
System P R F
Baseline 0.719 1.00 0.837
SEGLANG 0.779 0.991 0.872
LINGUINI 0.729 0.981 0.837
Our method 0.907 0.916 0.912
Table 4: Detection accuracy for English-language
inclusion in web documents from targeted web
crawls for low-density languages.
make use of manually-annotated data kindly pro-
vided to us by Ben King, which consists of 149 doc-
uments containing 42 languages retrieved from the
web using a set of targeted queries for low-density
languages. Note that the dataset described in King
and Abney (2013) was based on manual confirma-
tion of the presence of English in addition to the low-
density language of primary interest; our dataset
contains these bilingual documents as well as mono-
lingual documents in the low-density language of in-
terest. Our purpose in this section is to investigate
the ability of automatic systems to select this subset
of bilingual documents. Specifically, given a col-
lection of documents retrieved for a target language,
the task is to identify the documents that contain text
in English in addition to the target language. Thus,
we re-train each system for each target language, us-
ing only training data for English and the target lan-
guage. We reserve the data provided by Ben King
for evaluation, and train our methods using data sep-
arately obtained from the Universal Declaration of
Human Rights (UDHR). Where UDHR translations
for a particular language were not available, we used
data from Wikipedia or from a bible translation. Ap-
proximately 20?80 kB of data were used for each
language. As we do not have suitable development
data, we made use of the best parameters for each
system from the experiments on WIKIPEDIAMULTI.
We find that all 3 systems are able to detect that
each document contains the target language with
100% accuracy. However, systems vary in their abil-
ity to detect if a document also contains English in
addition to the target language. The detection accu-
racy for English-language inclusion is summarized
in Table 4.7 For comparison, we include a heuristic
baseline based on labeling all documents as contain-
7Note that Table 2 and Table 3 both report macro and micro-
averaged results across a number of languages. In contrast Ta-
ble 4 only reports results for English, and the values are not
directly comparable to our earlier evaluation.
36
0.2 0.4 0.6 0.8 1.0Actual Proportion
0.2
0.4
0.6
0.8
1.0
Pred
icted
 Prop
ortio
n
Pearson's r: 0.863MAE: 0.108
(a) without emission rate correction
0.2 0.4 0.6 0.8 1.0Actual Proportion
0.2
0.4
0.6
0.8
1.0
Pred
icted
 Prop
ortio
n
Pearson's r: 0.981MAE: 0.0241
(b) with emission rate correction
Figure 2: Scatterplot of the predicted vs. actual language proportions in a document for the test partition of
WIKIPEDIAMULTI (predictions are from our method; each point corresponds to a document-language pair).
ing English. We find that, like the heuristic base-
line, SEGLANG and LINGUINI both tend to over-
label documents, producing false positive labels of
English, resulting in increased recall at the expense
of precision. Our method produces less false pos-
itives (but slightly more false negatives). Overall,
our method attains the best F for detecting En-
glish inclusions. Manual error analysis suggests that
the false negatives for our method generally occur
where a relatively small proportion of the document
is written in English.
8 Future Work
Document segmentation by language could be ac-
complished by a combination of our method and the
method of King and Abney (2013), which could be
compared to the method of Yamaguchi and Tanaka-
Ishii (2012) in the context of constructing corpora
for low-density languages using the web. Another
area we have identified in this paper is the tuning
of the parameters ? and ? in our model (currently
? = 0 and ? = 1), which may have some effect on
the sparsity of the model.
Further work is required in dealing with cross-
domain effects, to allow for ?off-the-shelf? language
identification in multilingual documents. Previous
work has shown that it is possible to generate a docu-
ment representation that is robust to variation across
domains (Lui and Baldwin, 2011), and we intend to
investigate if these results are also applicable to lan-
guage identification in multilingual documents. An-
other open question is the extension of the genera-
tive mixture models to ?unknown? language identi-
fication (i.e. eliminating the closed-world assump-
tion (Hughes et al., 2006)), which may be possible
through the use of non-parametric mixture models
such as Hierarchical Dirichlet Processes (Teh et al.,
2006).
9 Conclusion
We have presented a system for language identifi-
cation in multilingual documents using a generative
mixture model inspired by supervised topic model-
ing algorithms, combined with a document represen-
tation based on previous research in language iden-
tification for monolingual documents. We showed
that the system outperforms alternative approaches
from the literature on synthetic data, as well as on
real-world data from related research on linguistic
corpus creation for low-density languages using the
web as a resource. We also showed that our system
is able to accurately estimate the proportion of the
document written in each of the languages identi-
fied. We have made a full reference implementation
of our system freely available,8 as well as the syn-
thetic dataset prepared for this paper (Section 5), in
order to facilitate the adoption of this technology and
further research in this area.
8https://github.com/saffsd/polyglot
37
Acknowledgments
We thank Hiroshi Yamaguchi for making a reference
implementation of SEGLANG available to us, and
Ben King for providing us with a collection of real-
world multilingual web documents. This work was
substantially improved as a result of the insightful
feedback received from the reviewers.
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and the
Australian Research Council through the ICT Cen-
tre of Excellence program.
References
Steven Abney and Steven Bird. 2010. The human
language project: building a universal corpus of the
world?s languages. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 88?97. Association for Computational Lin-
guistics.
Beatrice Alex, Amit Dubey, and Frank Keller. 2007.
Using foreign inclusion detection to improve parsing
performance. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
2007 (EMNLP-CoNLL 2007), pages 151?160, Prague,
Czech Republic.
Timothy Baldwin and Marco Lui. 2010a. Language
identification: The long and the short of the matter. In
Proceedings of Human Language Technologies: The
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010), pages 229?237, Los Angeles,
USA.
Timothy Baldwin and Marco Lui. 2010b. Multilin-
gual language identification: ALTW 2010 shared task
dataset. In Proceedings of the Australasian Language
Technology Workshop 2010 (ALTW 2010), pages 5?7,
Melbourne, Australia.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings the Second Workshop on
Language in Social Media (LSM2012), pages 65?74,
Montre?al, Canada.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Alessio Bosca and Luca Dini. 2010. Language identi-
fication strategies for cross language information re-
trieval. In Working Notes of the Cross Language Eval-
uation Forum (CLEF).
Jamie Callan and Mark Hoy, 2009. ClueWeb09
Dataset. Available at http://boston.lti.cs.
cmu.edu/Data/clueweb09/.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on Document Analysis and Informa-
tion Retrieval, pages 161?175, Las Vegas, USA.
Hakan Ceylan and Yookyung Kim. 2009. Language
identification of search engine queries. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1066?1074, Singapore.
Paul Cook and Marco Lui. 2012. langid.py for bet-
ter language modelling. In Proceedings of the Aus-
tralasian Language Technology Association Workshop
2012, pages 107?112, Dunedin, New Zealand.
Rafael Dueire Lins and Paulo Gonc?alves. 2004. Au-
tomatic language identification of written texts. In
Proceedings of the 2004 ACM Symposium on Applied
Computing (SAC 2004), pages 1128?1133, Nicosia,
Cyprus.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS 940-273, Computing
Research Laboratory, New Mexico State University.
Rayid Ghani, Rosie Jones, and Dunja Mladenic. 2004.
Building minority language corpora by learning to
generate web search queries. Knowledge and Infor-
mation Systems, 7(1):56?83.
Emmanuel Giguet. 1995. Categorisation according to
language: A step toward combining linguistic knowl-
edge and statistical learning. In Proceedings of the
4th International Workshop on Parsing Technologies
(IWPT-1995), Prague, Czech Republic.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of Analisi
Statistica dei Dati Testuali (JADT), pages 263?268,
Rome, Italy.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Thomas Griffiths. 2002. Gibbs sampling in the gener-
ative model of latent Dirichlet allocation. Technical
Report, Stanford University.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006), pages 485?488, Genoa, Italy.
38
Genitiro Kikui. 1996. Identifying the coding system
and language of on-line documents on the internet. In
Proceedings of the 16th International Conference on
Computational Linguistics (COLING ?96), pages 652?
657, Kyoto, Japan.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents using
weakly supervised methods. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119, At-
lanta, Georgia.
Canasai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In Pro-
ceedings of the 5th International Symposium on Com-
munications and Information Technologies (ISCIT-
2005), pages 896?899, Beijing, China.
David D. Lewis. 1997. The Reuters-21578 data set.
available at http://www.daviddlewis.
com/resources/testcollections/
reuters21578/.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 176?186, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Jicheng Liu and Chunyan Liang. 2008. Text Categoriza-
tion of Multilingual Web Pages in Specific Domain.
In Proceedings of the 12th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
PAKDD?08, pages 938?944, Osaka, Japan.
Marco Lui and Timothy Baldwin. 2011. Cross-domain
feature selection for language identification. In Pro-
ceedings of the 5th International Joint Conference on
Natural Language Processing (IJCNLP 2011), pages
553?561, Chiang Mai, Thailand.
Bruno Martins and Ma?rio J. Silva. 2005. Language iden-
tification in web pages. In Proceedings of the 2005
ACM symposium on Applied computing, pages 764?
768, Santa Fe, USA.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text classifi-
cation. In Proceedings of the AAAI-98 Workshop on
Learning for Text Categorization, pages Available as
Technical Report WS?98?05, AAAI Press., Madison,
USA.
Andrew Kachites McCallum. 1999. Multi-label text
classification with a mixture model trained by EM. In
Proceedings of AAAI 99 Workshop on Text Learning.
Paul McNamee. 2005. Language identification: a solved
problem suitable for undergraduate instruction. Jour-
nal of Computing Sciences in Colleges, 20(3):94?101.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the web. In Proceedings
of 22nd International ACM-SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR?99), pages 74?81, Berkeley, USA.
John M. Prager. 1999a. Linguini: language identification
for multilingual documents. In Proceedings the 32nd
Annual Hawaii International Conference on Systems
Sciences (HICSS-32), Maui, Hawaii.
John M. Prager. 1999b. Linguini: Language identifica-
tion for multilingual documents. Journal of Manage-
ment Information Systems, 16(3):71?101.
John Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, USA.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009), pages 248?256, Singapore.
Radim Rehurek and Milan Kolkus. 2009. Language
Identification on the Web: Extending the Dictionary
Method. In Proceedings of Computational Linguis-
tics and Intelligent Text Processing, 10th International
Conference (CICLing 2009), pages 357?368, Mexico
City, Mexico.
Philip Resnik. 1999. Mining the Web for bilingual text.
In Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 527?534,
College Park, USA.
Kevin P Scannell. 2007. The Cru?bada?n Project: Cor-
pus building for under-resourced languages. In Build-
ing and Exploring Web Corpora: Proceedings of the
3rd Web as Corpus Workshop, pages 5?15, Louvain-
la-Neuve, Belgium.
W. J. Teahan. 2000. Text Classification and Seg-
mentation Using Minimum Cross-Entropy. In Pro-
ceedings the 6th International Conference ?Recherche
d?Information Assistee par Ordinateur? (RIAO?00),
pages 943?961, Paris, France.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
Jo?rg Tiedemann and Nikola Ljubes?ic?. 2012. Efficient
discrimination between closely related languages. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
2619?2634, Mumbai, India.
Giang Binh Tran, Dat Ba Nguyen, and Bin Thanh
Kieu. 2010. N-gram based approach for mul-
tilingual language identification. poster. available
39
at http://comp.mq.edu.au/programming/
task_description/VILangTek.pdf.
Fei Xia, Carrie Lewis, and William D. Lewis. 2010. Lan-
guage ID for a thousand languages. In LSA Annual
Meeting Extended Abstracts, Baltimore,USA.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 969?978, Jeju Is-
land, Korea.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947?953,
Saarbru?cken, Germany.
40
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 217?221, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
unimelb: Topic Modelling-based Word Sense Induction for Web Snippet
Clustering
Jey Han Lau, Paul Cook and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
This paper describes our system for Task 11
of SemEval-2013. In the task, participants
are provided with a set of ambiguous search
queries and the snippets returned by a search
engine, and are asked to associate senses with
the snippets. The snippets are then clus-
tered using the sense assignments and sys-
tems are evaluated based on the quality of the
snippet clusters. Our system adopts a pre-
existing Word Sense Induction (WSI) method-
ology based on Hierarchical Dirichlet Process
(HDP), a non-parametric topic model. Our
system is trained over extracts from the full
text of English Wikipedia, and is shown to per-
form well in the shared task.
1 Introduction
The basic premise behind research on word sense
disambiguation (WSD) is that there exists a static,
discrete set of word senses that can be used to la-
bel distinct usages of a given word (Agirre and Ed-
monds, 2006; Navigli, 2009). There are various pit-
falls underlying this premise, including: (1) what
sense inventory is appropriate for a particular task
(given that sense inventories can vary considerably
in their granularity and partitioning of word usages)?
(2) given that word senses tend to take the form of
prototypes, is discrete labelling a felicitous represen-
tation of word usages, especially for non-standard
word usages? (3) how should novel word usages be
captured under this model? and (4) given the rapid
pace of language evolution on real-time social me-
dia such as Twitter and Facebook, is it reasonable
to assume a static sense inventory? Given this back-
drop, there has been a recent growth of interest in the
task of word sense induction (WSI), where the word
sense representation for a given word is automati-
cally inferred from a given data source, and word
usages are labelled (often probabilistically) accord-
ing to that data source. While WSI has considerable
appeal as a task, intrinsic cross-comparison of WSI
systems is fraught with many of the same issues as
WSD (Agirre and Soroa, 2007; Manandhar et al,
2010), leading to a move towards task-based WSI
evaluation, such as in Task 11 of SemEval-2013, ti-
tled ?Evaluating Word Sense Induction & Disam-
biguation within an End-User Application?.
This paper presents the UNIMELB system entry to
SemEval-2013 Task 11. Our method is based heav-
ily on the WSI methodology proposed by Lau et
al. (2012) for novel word sense detection. Largely
the same methodology was also applied to SemEval-
2013 Task 13 on WSI (Lau et al, to appear).
2 System Description
Our system is based on the WSI methodology pro-
posed by Lau et al (2012) for the task of novel word
sense detection. The core machinery of our sys-
tem is driven by a Latent Dirichlet Allocation (LDA)
topic model (Blei et al, 2003). In LDA, the model
learns latent topics for a collection of documents,
and associates these latent topics with every docu-
ment in the collection. A topic is represented by
a multinomial distribution of words, and the asso-
ciation of topics with documents is represented by a
multinomial distribution of topics, with one distribu-
tion per document. The generative process of LDA
217
for drawing word w in document d is as follows:
1. draw latent topic z from document d;
2. draw word w from the chosen latent topic z.
The probability of selecting word w given a doc-
ument d is thus given by:
P (w|d) =
T?
z=1
P (w|t = z)P (t = z|d).
where t is the topic variable, and T is the number of
topics.
The number of topics, T , is a parameter in LDA,
and the model tends to be highly sensitive to this set-
ting. To remove the need for parameter tuning over
development data, we make use of a non-parametric
variant of LDA, in the form of a Hierarchical Dirich-
let Process (HDP: Teh et al (2006)). HDP learns the
number of topics based on data, and the concentra-
tion parameters ? and ?0 control the variability of
topics in the documents (for details of HDP please
refer to the original paper, Teh et al (2006)).
To apply HDP in the context of WSI, the latent
topics are interpreted as the word senses, and the
documents are usages that contain the target word of
interest (or search query in the case of Task 11). That
is, given a search query (e.g. Prince of Persia), a
?document? in our application is a sentence/snippet
containing the target word. In addition to the bag of
words surrounding the target word, we also include
positional context word information, as used in the
original methodology of Lau et al (2012). That is,
we introduce an additional word feature for each of
the three words to the left and right of the target
word. An example of the topic model features for
a context sentence is given in Table 1.
2.1 Background Corpus and Preprocessing
As part of the task setup, we were provided with
snippets for each search query, constituting the doc-
uments for the topic model for that query (each
search query is topic-modelled separately). Our sys-
tem uses only the text of the snippets as features, and
ignores the URL information. The text of the snip-
pets is tokenised and lemmatised using OpenNLP
and Morpha (Minnen et al, 2001).
As there are only 64 snippets for each query in
the test dataset, which is very small by topic mod-
elling standards, we turn to English Wikipedia to
expand the data, by extracting all context sentences
that contain the search query in the full collection
of Wikipedia articles.1 Each extracted usage is a
three-sentence context containing the search query:
the original sentence that contains the actual usage
and its preceding and succeeding sentences. The
extraction of usages from Wikipedia significantly
increases the amount of information for the topic
model to learn the senses for the search queries. To
give an estimate: for very ambiguous queries such
as queen we extracted almost 150,000 usages from
Wikipedia; for most queries, however, this number
tends to be a few thousand usages.
To summarise, for each search query we apply the
HDP model to the combined collection of the 64
snippets and the extracted usages from Wikipedia.
The topic model learns the senses/topics for all
documents in the collection, but we only use the
sense/topic distribution for the 64 snippets as they
are the documents that are evaluated in the shared
task.
Our English Wikipedia collection is tokenised and
lemmatised using OpenNLP and Morpha (Minnen et
al., 2001). The search queries provided in the task,
however, are not lemmatised. Two approaches are
used to extract the usages of search queries from
Wikipedia:
HDP-CLUSTERS-LEMMA Search queries are lem-
matised using Morpha (Minnen et al, 2001),
and both the original and lemmatised forms are
used for extraction;2
HDP-CLUSTERS-NOLEMMA Search queries are
not lemmatised and only their original forms
are used for extraction.
1The Wikipedia dump was retrieved on November 28th
2009.
2Morpha requires the part-of-speech (POS) of a given word,
which is determined by the majority POS aggregated over all of
that word?s occurrences in Wikipedia.
218
Search query dogs
Context sentence Most breeds of dogs are at most a few hundred years old
Bag-of-word features most, breeds, of, are, at, most, a, few, hundred, years, old
Positional word features most #-3, breeds #-2, of #-1, are #1, at #2, most #3
Table 1: An example of topic model features.
System F1 ARI RI JI
Avg. No. of Avg. Cluster
Clusters Size
HDP-CLUSTERS-LEMMA 0.6830 0.2131 0.6522 0.3302 6.6300 11.0756
HDP-CLUSTERS-NOLEMMA 0.6803 0.2149 0.6486 0.3375 6.5400 11.6803
TASK11.DULUTH.SYS1.PK2 0.5683 0.0574 0.5218 0.3179 2.5300 26.4533
TASK11.DULUTH.SYS7.PK2 0.5878 0.0678 0.5204 0.3103 3.0100 25.1596
TASK11.DULUTH.SYS9.PK2 0.5702 0.0259 0.5463 0.2224 3.3200 19.8400
TASK11-SATTY-APPROACH1 0.6709 0.0719 0.5955 0.1505 9.9000 6.4631
TASK11-UKP-WSI-WACKY-LLR 0.5826 0.0253 0.5002 0.3394 3.6400 32.3434
TASK11-UKP-WSI-WP-LLR2 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702
TASK11-UKP-WSI-WP-PMI 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098
RAKESH 0.3949 0.0811 0.5876 0.3052 9.0700 2.9441
SINGLETON 1.0000 0.0000 0.6009 0.0000 64.0000 1.0000
ALLINONE 0.5442 0.0000 0.3990 0.3990 1.0000 64.0000
GOLD 1.0000 0.9900 1.0000 1.0000 7.6900 11.5630
Table 2: Cluster quality results for all systems. The best result for each column is presented in boldface. SINGLETON
and ALLINONE are baseline systems and GOLD is the theoretical upper-bound for the task.
3 Experiments and Results
Following Lau et al (2012), we use the default pa-
rameters (? = 0.1 and ?0 = 1.0) for HDP.3 For each
search query, we apply HDP to induce the senses,
and a distribution of senses is produced for each
?document? in the model. As the snippets in the test
dataset correspond to the documents in the model
and evaluation is based on ?hard? clusters of snip-
pets, we assign a sense to each snippet based on the
sense (= topic) which has the highest probability for
that snippet.
The task requires participants to produce a ranked
list of snippets for each induced sense, based on the
relative fit between the snippet and the sense. We in-
duce the ranking based on the sense probabilities as-
signed to the senses, such that snippets that have the
highest probability of the induced sense are ranked
highest, and snippets with lower sense probabilities
3Our implementation can be accessed via https://
github.com/jhlau/hdp-wsi.
are ranked lower.
Two classes of evaluation are used in the shared
task:
1. cluster quality measures: Jaccard Index (JI),
RandIndex (RI), Adjusted RandIndex (ARI)
and F1;
2. diversification of search results: Subtopic Re-
call@K and Subtopic Precision@r.
Details of the evaluation measures are described in
Navigli and Vannella (2013).
The idea behind the second form of evaluation
(i.e. diversification of search results) is that search
engine results should cluster the results based on
senses (of the query term in the documents) given an
ambiguous query. For example, if a user searches for
apple, the search engine may return results related to
both the computer brand sense and the fruit sense of
apple. Given this assumption, the best WSI/WSD
system is the one that can correctly identify the di-
versity of senses in the snippets.
219
Figure 1: Subtopic Recall@K for all participating systems.
Cluster quality, subtopic recall@K and subtopic
precision@r results for all systems entered in the
task are presented in Table 2, Figure 1 and Figure 2,
respectively.
In terms of cluster quality, our systems
(HDP-CLUSTERS-LEMMA and HDP-CLUSTERS-
NOLEMMA) consistently outperform the other teams
for all measures except for the Jaccard Index (where
we rank second and third, by a narrow margin). The
average number of induced clusters and the average
cluster size of our systems are similar to those
of the gold standard system (GOLD), indicating
that our systems are learning an appropriate sense
granularity.
In terms of diversification of search results, our
systems perform markedly better than most teams,
other than RAKESH which trails closely behind our
systems (despite a relatively low ranking in terms of
the cluster quality evaluation). Overall, the results
are encouraging and our system performs very well
over the task.
4 Discussion and Conclusion
Our system adopts the WSI system proposed in Lau
et al (2012) with no parameters tuned for this task,
and performs very well over it. Parameter tuning and
exploiting URL information in the snippets could
potentially boost the system performance further.
Other background corpora (such as news articles)
could also be used to increase the size of the training
data. We leave these ideas for future work.
Inspecting the difference between the HDP-
CLUSTERS-LEMMA and HDP-CLUSTERS-
NOLEMMA approaches, only 6 out of the 100
lemmas have a lemmatised form which differs from
the original query composition: pods (pod), ten
commandments (ten commandment), guild wars
(guild war), stand by me (stand by i), sisters of
mercy (sister of mercy) and lord of the flies (lord of
the fly). In most cases, including the lemmatised
query results in the extraction of additional useful
usages, e.g. using only the original form lord of
the flies would extract no usages from Wikipedia
(because this corpus has itself been lemmatised).
In other cases, however, including the lemmatised
forms results in many common noun usages, e.g.
the number of usages of the lemmatised pod is
significantly greater than that of the original form
pods (which corresponds to proper noun usages in
the lemmatised corpus), resulting in senses being
induced only for common noun usages of pods. The
220
Figure 2: Subtopic Precision@r for all participating systems.
advantages and disadvantages of both approaches
are reflected in the results: performance is mixed
and no one method clearly outperforms the other.
To conclude, we apply a topic model-based WSI
methodology to the task of web result clustering, us-
ing English Wikipedia as an external resource for ex-
tracting additional usages. Our system is completely
unsupervised and requires no annotated resources,
and appears to perform very well on the task.
References
Eneko Agirre and Philip Edmonds. 2006. Word
Sense Disambiguation: Algorithms and Applications.
Springer, Dordrecht, Netherlands.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proc. of the 4th International Work-
shop on Semantic Evaluations, pages 7?12, Prague,
Czech Republic.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proc. of the 13th
Conference of the EACL (EACL 2012), pages 591?
601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. to ap-
pear. unimelb: Topic modelling-based word sense in-
duction. In Proc. of the 7th International Workshop on
Semantic Evaluation (SemEval 2013).
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task 14:
Word sense induction & disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 63?68, Uppsala, Sweden.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Roberto Navigli and Daniele Vannella. 2013. SemEval-
2013 task 11: Evaluating word sense induction & dis-
ambiguation within an end-user application. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), in conjunction with
the Second Joint Conference on Lexical and Compu-
tational Semantcis (*SEM 2013), Atlanta, USA.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
221
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 307?311, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
unimelb: Topic Modelling-based Word Sense Induction
Jey Han Lau, Paul Cook and Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
tb@ldwin.net
Abstract
This paper describes our system for shared
task 13 ?Word Sense Induction for Graded and
Non-Graded Senses? of SemEval-2013. The
task is on word sense induction (WSI), and
builds on earlier SemEval WSI tasks in ex-
ploring the possibility of multiple senses be-
ing compatible to varying degrees with a sin-
gle contextual instance: participants are asked
to grade senses rather than selecting a sin-
gle sense like most word sense disambigua-
tion (WSD) settings. The evaluation measures
are designed to assess how well a system per-
ceives the different senses in a contextual in-
stance. We adopt a previously-proposed WSI
methodology for the task, which is based on a
Hierarchical Dirichlet Process (HDP), a non-
parametric topic model. Our system requires
no parameter tuning, uses the English ukWaC
as an external resource, and achieves encour-
aging results over the shared task.
1 Introduction
In our previous work (Lau et al, 2012) we devel-
oped a word-sense induction (WSI) system based on
topic modelling, specifically a Hierarchical Dirich-
let Process (Teh et al, 2006). In evaluations over
the SemEval-2007 and SemEval-2010 WSI tasks we
achieved performance on par with the current state-
of-the art. The SemEval-2007 and SemEval-2010
WSI tasks assumed that each usage of a word has
a single gold-standard sense. In this paper we apply
this WSI method ?off-the-shelf?, with no adaptation,
to the novel SemEval-2013 task of ?Word Sense In-
duction for Graded and Non-Graded Senses?. Given
that the topic model allocates a multinomial distri-
bution over topics to each word usage (?document?,
in topic modelling terms), the SemEval-2013 WSI
task is an ideal means for evaluating this aspect of
the topic model.
2 System Description
Our system is based on the WSI methodology pro-
posed by Lau et al (2012), and also applied to
SemEval-2013 Task 11 on WSI for web snippet
clustering (Lau et al, to appear). The core machin-
ery of our system is driven by a Latent Dirichlet Al-
location (LDA) topic model (Blei et al, 2003). In
LDA, the model learns latent topics for a collection
of documents, and associates these latent topics with
every document in the collection. A topic is repre-
sented by a multinomial distribution of words, and
the association of topics with documents is repre-
sented by a multinomial distribution of topics, a dis-
tribution for each document. The generative process
of LDA for drawing word w in document d is as fol-
lows:
1. draw latent topic z from document d;
2. draw word w from the chosen latent topic z.
The probability of selecting word w given a doc-
ument d is thus given by:
P (w|d) =
T?
z=1
P (w|t = z)P (t = z|d).
where t is the topic variable, and T is the number of
topics.
307
The number of topics, T , is a parameter in LDA.
We relax this assumption by extending the model
to be non-parametric, using a Hierarchical Dirichlet
Process (HDP: (Teh et al, 2006)). HDP learns the
number of topics based on data, with the concentra-
tion parameters ? and ?0 controlling the variability
of topics in the documents (for details of HDP please
refer to the original paper).
To apply HDP to WSI, the latent topics are in-
terpreted as the word senses, and the documents are
usages that contain the target word of interest. That
is, given a target word (e.g. paper), a ?document?
in our application is a sentence context surround-
ing the target word. In addition to the bag of words
surrounding the target word, we also include posi-
tional context word information, which was used in
our earlier work (Lau et al, 2012). That is, we in-
troduce an additional word feature for each of the
three words to the left and right of the target word.
An example of the topic model features is given in
Table 1.
2.1 Background Corpus and Preprocessing
The test dataset provides us with contextual in-
stances for each target word, and these instances
constitute the documents for the topic model. The
text of the test data is tokenised and lemmatised us-
ing OpenNLP and Morpha (Minnen et al, 2001).
Note, however, that there are only 100 instances
for most target words in the test dataset, and as such
the dataset may be too small for the topic model
to induce meaningful senses. To this end, we turn
to the English ukWaC ? a web corpus of approxi-
mately 1.9 billion tokens ? to expand the data, by
extracting context sentences that contain the target
word. Each extracted usage is a three-sentence con-
text containing the target word: the original sentence
that contains the actual usage and its preceding and
succeeding sentences. The extraction of usages from
the ukWaC significantly increases the amount of in-
formation for the topic model to learn the senses for
the target words from. However, HDP is compu-
tationally intensive, so we limit the number of ex-
tracted usages from the ukWaC using two sampling
approaches:
UNIMELB (5P) Take a 5% random sample of us-
ages;
UNIMELB (50K) Limit the maximum number of
randomly-sampled usages to 50,000 instances.
The usages from the ukWaC are tokenised and
lemmatised using TreeTagger (Schmid, 1994), as
provided by the corpus.
To summarise, for each target word we apply
the HDP model to the combined collection of the
test instances (provided by the shared task) and
the extracted usages from the English ukWaC (not-
ing that each instance/usage corresponds to a topic
model ?document?). The topic model learns the
senses/topics for all documents in the collection, but
we only use the sense/topic distribution for the test
instances as they are the ones evaluated in the shared
task.
3 Experiments and Results
Following Lau et al (2012), we use the default pa-
rameters (? = 0.1 and ?0 = 1.0) for HDP.1 For each
target word, we apply HDP to induce the senses, and
a distribution of senses is produced for each ?docu-
ment? in the model. To grade the senses for the in-
stances in the test dataset, we apply the sense proba-
bilities learnt by the topic model as the sense weights
without any modification.
To illustrate the senses induced by our model, we
present the top-10 words of the induced senses for
the verb strike in Table 2. Although 13 senses in
total are induced and some of them do not seem very
coherent, only the first 8 senses ? the more coherent
ones ? are observed (i.e., have non-zero probability
for any usage) in the test dataset.
Two forms of evaluation are used in the task:
WSD evaluation and clustering comparison. For
WSD evaluation, three measures are used: (1)
Jaccard Index (JI), which measures the degree of
overlap between the induced senses and the gold
senses; (2) positionally-weighted Kendall?s tau (KT:
(Kumar and Vassilvitskii, 2010)), which measures
the correlation between the ranking of the induced
senses and that of the gold senses; and (3) nor-
malised discounted cumulative gain (NDCG), which
1These settings were considered ?vague? priors in Teh et
al. (2006). They were tested in Lau et al (2012) and the
model was shown to be robust under different parameter set-
tings. As such we decided to keep the settings. The imple-
mentation of our WSI system can be accessed via GitHub:
https://github.com/jhlau/hdp-wsi.
308
Target word dogs
Context sentence Most breeds of dogs are at most a few hundred years old
Bag-of-word features most, breeds, of, are, at, most, a, few, hundred, years, old
Positional word features most #-3, breeds #-2, of #-1, are #1, at #2, most #3
Table 1: An example of the topic model features.
Sense Num Top-10 Terms
1 strike @card@ worker union war iraq week pay government action
2 strike hand god head n?t look face fall leave blow
3 strike @card@ balance court company case need balance #1 order claim
4 strike ball @card@ minute game goal play player shot half
5 strike @card@ people fire disaster area road car ship lightning
6 @card@ strike new news post deal april home business week
7 strike n?t people thing think way life book find new
8 @card@ strike coin die john church police age house william
9 div ukl syn color hunter text-decoration australian verb condom font-size
10 invent rocamadour cost mp3 terminal total wav honor omen node
11 training run rush kata performance marathon exercise technique workout interval
12 wrong qha september/2000 sayd ? hawksmoor thyna pan salt common
13 zidane offering stone blow zidane #-1 type type #2 zidane #1 blow #3 materials
Table 2: The top-10 terms for each of the senses induced for the verb strike by the HDP model.
measures the correlation between the weights of
the induced senses and that of the gold senses.
For clustering comparison, fuzzy normalised mu-
tual information (FNMI) and fuzzy b-cubed (FBC)
are used. Note that the WSD systems participat-
ing in this shared task are not evaluated with clus-
tering comparison metrics, as they do not induce
senses/clusters in the same manner as WSI systems.
WSI systems produce senses that are different to
the gold standard sense inventory (WordNet 3.1),
and the induced senses are mapped to the gold stan-
dard senses using the 80/20 validation setting. De-
tails of this mapping procedure are described in Jur-
gens (2012).
Results for all test instances are presented in Ta-
ble 3. Note that many baselines are used, only some
of which we present in this paper, namely: (1) RAN-
DOM ? label instances with one of three random in-
duced senses; (2) SEMCOR MFS ? label instances
with the most frequently occurring sense in Semcor;
(3) TEST MFS ? label instances with the most fre-
quently occurring sense in the test dataset. To bench-
mark our method, we present one or two of the best
systems from each team.
Looking at Table 3, our system performs encour-
agingly well. Although not the best system, we
achieve results close to the best system for each eval-
uation measure.
Most of the instances in the data were annotated
with only one sense; only 11% were annotated with
two senses, and 0.5% with three. As a result, the
task organisers categorised the instances into single-
sense instances and multi-sense instances to bet-
ter analyse the performance of participating sys-
tems. Results for single-sense and multi-sense in-
stances are presented in Table 4 and Table 5, re-
spectively. Note that for single-sense instances, only
precision is used for WSD evaluation as the Jaccard
Index, positionally-weighted Kendall?s tau and nor-
malised discounted cumulative gain are not applica-
ble. Our system performs relatively well, and trails
marginally behind the best system in most cases.
4 Conclusion
We adopt a WSI methodology from Lau et al (2012)
for the task of grading senses in a WSD setting.
309
System JI KT NDCG FNMI FBC
RANDOM 0.244 0.633 0.287 0.018 0.382
SEMCOR MFS 0.455 0.465 0.339 ? ?
TEST MFS 0.552 0.560 0.412 ? ?
AI-KU 0.197 0.620 0.387 0.065 0.390
AI-KU (REMOVE5-AD1000) 0.244 0.642 0.332 0.039 0.451
LA SAPIENZA (2) 0.149 0.510 0.383 ? ?
UOS (TOP-3) 0.232 0.625 0.374 0.045 0.448
UNIMELB (5P) 0.218 0.614 0.365 0.056 0.459
UNIMELB (50K) 0.213 0.620 0.371 0.060 0.483
Table 3: Results for all instances. The best-performing system is indicated in boldface.
System Precision FNMI FBC
RANDOM 0.555 0.010 0.359
SEMCOR MFS 0.477 ? ?
TEST MFS 0.578 ? ?
AI-KU 0.641 0.045 0.351
AI-KU (REMOVE5-AD1000) 0.628 0.026 0.421
UOS (TOP-3) 0.600 0.028 0.414
UNIMELB (5P) 0.596 0.035 0.421
UNIMELB (50K) 0.605 0.039 0.441
Table 4: Results for single-sense instances. The best-performing system is indicated in boldface.
System JI KT NDCG FNMI FBC
RANDOM 0.429 0.548 0.236 0.006 0.113
SEMCOR MFS 0.283 0.373 0.197 ? ?
TEST MFS 0.354 0.426 0.248 ? ?
AI-KU 0.394 0.617 0.317 0.029 0.078
AI-KU (REMOVE5-AD1000) 0.434 0.586 0.291 0.004 0.116
LA SAPIENZA (2) 0.263 0.531 0.365 ? ?
UOS (#WN SENSES) 0.387 0.628 0.314 0.036 0.037
UNIMELB (5P) 0.426 0.586 0.287 0.019 0.130
UNIMELB (50K) 0.414 0.602 0.299 0.021 0.134
Table 5: Results for multi-sense instances. The best-performing system is indicated in boldface.
310
With no parameter tuning and using only the English
ukWaC as an external resource, our system performs
relatively well at the task.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
David Jurgens. 2012. An evaluation of graded sense
disambiguation using word sense induction. In Proc.
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM 2012), pages 189?198,
Montre?al, Canada.
Ravi Kumar and Sergei Vassilvitskii. 2010. Generalized
distances between rankings. In Proc. of the 19th Inter-
national Conference on the World Wide Web (WWW
2010), pages 571?580, Raleigh, USA.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proc. of the 13th
Conference of the EACL (EACL 2012), pages 591?
601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. to ap-
pear. unimelb: Topic modelling-based word sense in-
duction for web snippet clustering. In Proc. of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013).
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of the Confer-
ence on New Methods in Natural Language Process-
ing, Manchester, 1994.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566?1581.
311
