Proceedings of the 12th European Workshop on Natural Language Generation, pages 82?89,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
An Alignment-capable Microplanner for Natural Language Generation
Hendrik Buschmeier, Kirsten Bergmann and Stefan Kopp
Sociable Agents Group, CITEC, Bielefeld University
PO-Box 10 01 31, 33501 Bielefeld, Germany
{hbuschme, kbergman, skopp}@TechFak.Uni-Bielefeld.DE
Abstract
Alignment of interlocutors is a well known
psycholinguistic phenomenon of great rel-
evance for dialogue systems in general and
natural language generation in particular.
In this paper, we present the alignment-
capable microplanner SPUD prime. Us-
ing a priming-based model of interactive
alignment, it is flexible enough to model
the alignment behaviour of human speak-
ers to a high degree. This will allow for
further investigation of which parameters
are important to model alignment and how
the human?computer interaction changes
when the computer aligns to its users.
1 Introduction
A well known phenomenon in dialogue situations
is alignment of the interlocutors. An illustrative
example is given by Levelt and Kelter (1982), who
telephoned shops and either asked the question
?What time does your shop close?? or the ques-
tion ?At what time does your shop close??. The
answers were likely to mirror the form of the ques-
tion. When asked ?At what . . . ??, answers tended
to begin with the preposition ?at? (e.g., ?At five
o?clock.?). Conversely, when asked ?What . . . ??,
answers tended to begin without the preposition
(e.g., ?Five o?clock.?). Similar alignment phenom-
ena can be observed in many aspects of speech pro-
duction inter alia in syntactic and lexical choice.
Pickering and Garrod (2004) present the inter-
active alignment model bringing together all align-
ment phenomena of speech processing in dialogue.
According to this model, human language com-
prehension and production are greatly facilitated
by alignment of the interlocutors during conversa-
tion. The process of alignment is explained through
mutual priming of the interlocutors? linguistic rep-
resentations. Thus, it is automatic, efficient, and
non-conscious. A stronger claim of the authors is
that alignment ? in combination with routines and
a dialogue lexicon ? is a prerequisite for fluent
speech production in humans.
Alignment effects also occur in human?com-
puter interaction. Brennan (1991) and Branigan
et al (in press) present evidence that syntactic
structures and lexical items used by a computer
are subsequently adopted by users. For this reason,
alignment is an important concept for natural lan-
guage human?computer interaction in general, and
for dialogue systems with natural language gener-
ation in particular. Integrating ideas from the in-
teractive alignment model into the microplanning
component of natural language generation systems
should be beneficial for several reasons. First, mi-
croplanning may become more efficient since the
subsets of rules or lexical items in the dialogue
lexicon that have been used before can be prefer-
entially searched. Second, due to self-alignment,
the output of the system can become more con-
sistent and therefore easier to understand for the
user. Finally, mutual alignment of user and dia-
logue system might make the conversation itself
more natural and, presumably, cognitively more
lightweight for the user.
In this paper we present a computational model
for parts of the interactive alignment model that
are particularly important in the context of natural
language generation. We describe how this model
has been incorporated into the existing SPUD lite
system (Stone et al, 2003; Stone, 2002) to yield
the alignment-capable microplanner SPUD prime.
In Section 2 we describe previous approaches to
integrate alignment into natural language genera-
tion. In Sections 3 and 4, we present our priming-
based model of alignment and its implementation
in SPUD prime. In Section 5, we describe results
of an evaluation on a corpus of task-oriented dia-
logue, and in Section 6 we conclude our work and
describe possible future directions.
82
2 Related Work
Computational modelling is an important method-
ology for evaluating and testing psycholinguistic
theories. Thus, it is certainly not a new idea to
implement the interactive alignment model compu-
tationally. Indeed, a call for ?explicit computational
models? is made as early as in the open peer com-
mentary on Pickering and Garrod?s (2004) paper.
Brockmann et al (2005) and Isard et al (2006)
present a ?massive over-generation? approach to
modelling alignment and individuality in natural
language generation. Their system generates a
huge number of alternative sentences ? up to
3000 ? and evaluates each of these sentences with
a trigram model consisting of two parts: a default
language model computed from a large corpus and
a cache model which is calculated from the user?s
last utterance. The default language model is lin-
early interpolated with the cache model, whose in-
fluence on the resulting combined language model
is determined by a weighting factor ? ? [0,1] that
controls the amount of alignment the system exhib-
its.
Purver et al (2006) take a more formal approach.
They use an implementation of the Dynamic Syn-
tax formalism, which uses the same representations
and mechanisms for parsing as well as for genera-
tion of natural language, and extend it with a model
of context. In their model, context consists of two
distinct representations: a record of the semantic
trees generated and parsed so far and a record of
the transformation actions used for the construction
of these semantic trees. Re-use of semantic trees
and actions is used to model many dialogue phe-
nomena in Dynamic Syntax and can also explain
alignment. Thus, the authors declare alignment to
be a corollary of context re-use. In particular, re-use
of actions is assumed to have a considerable influ-
ence on alignment in natural language generation.
Instead of looking through the complete lexicon
each time a lexical item is chosen, this kind of lex-
ical search is only necessary if no action ? which
constructed the same meaning in the given con-
text before ? exists in the record. If such an action
exists, it can simply be re-used, which obviously
leads to alignment.
A completely different approach to alignment
in natural language generation is presented by de
Jong et al (2008), whose goal is to make a vir-
tual museum guide more believable by aligning
to the user?s level of politeness and formality. In
order to achieve this, the virtual guide analyses sev-
eral features of the user?s utterance and generates a
reply with the same level of politeness and formal-
ity. According to the authors, lexical and syntactic
alignment occur automatically because the lexical
items and syntactic constructions to choose from
are constrained by the linguistic style adopted.
Finally, Bateman (2006) advocates another pro-
posal according to which alignment in dialogue is
predictable for it is an inherently social activity.
Following the social-semiotic view of language,
Bateman suggests to model alignment as arising
from register and microregister. More specifically,
in his opinion priming of a linguistic representation
is comparable with pre-selecting a microregister
that must be considered when generating an utter-
ance in a particular social context.
The approaches presented above primarily focus
on the linguistic aspects of alignment in natural
language generation. The work of Brockmann et
al. (2005) and Isard et al (2006) concentrates on
the surface form of language, Bateman (2006) sees
alignment arising from social-semiotic aspects, and
Purver et al (2006) are primarily interested in fit-
ting alignment into a formal linguistic framework.
In this paper we adopt a more psycholinguistic and
cognitive stance on alignment. Pickering and Gar-
rod (2004) propose that low-level priming is the
basic mechanism underlying interactive alignment.
Here, we propose that computational modelling of
these priming mechanisms also opens up an inter-
esting and new perspective for alignment in natural
language generation.
3 A Priming-based Model of Alignment
We are interested here in those parts of the inter-
active alignment model that are most relevant for
microplanning in natural language generation and
it is out of our scope to model all the facets and
details of direct/repetition priming in the alignment
of linguistic representations. For instance, exact
timing effects are likely to be not even relevant as,
in an actual system, it does not matter how many
milliseconds faster the retrieval of a primed lexical
item is in contrast to the retrieval of an item that
is not primed. For this reason we adopt an ideal-
ised view, in which priming of linguistic structures
results from two basic activation mechanisms:
Temporary activation This kind of activation
should increase abruptly and then decrease
slowly over time until it reaches zero again.
83
Permanent activation This kind of activation
should increase by a certain quantity and then
maintain the new level.
These two mechanisms of priming are in ac-
cordance with empirical findings. Branigan et al
(1999) present evidence for rapid decay of activa-
tion of primed syntactic structures, whereas Bock
and Griffin (2000) report evidence for their long(er)
term activation. In any case, Reitter (2008) found
both types of priming in his analysis of several
corpora, with temporary activation being the more
important one. The assumption that both mechan-
isms play a role in dialogue is also supported by
Brennan and Clark (1996) whose terminology will
be followed in this paper: temporary priming will
be called ?recency of use effects? and permanent
priming will be called ?frequency of use effects?.
Reitter (2008) assumes the repetition probability
of primed syntactic structures to depend logarith-
mically on the distance between priming and usage.
Here, recency of use effects are modelled by a
more general exponential decay function, modified
to meet the needs for modelling activation decay of
primed structures:
ta(?r) = exp
(
?
?r?1
?
)
, (1)
?r ? N+; ? > 0; ta ? [0,1]
ta(?r) is the temporary activation value of a lin-
guistic structure depending on the distance ?r
between the current time T and the time r at which
the structure was primed. The slope of the function
is determined by the parameter ? . Additionally, the
function is shifted right in order to yield an activa-
tion value of 1 for ?r = 1. This shift is due to the
assumption of discrete time steps with a minimal
distance of 1. A plot of ta(?r) with different values
for ? is given in Figure 1a.
Using exponential decay to model temporary ac-
tivation appears to be a sensible choice that is often
used to model natural processes. The advantage of
this model of temporary activation lies in its flexib-
ility. By changing the slope parameter ? , different
empirical findings as well as variation among hu-
mans can be modelled easily.
Next, a mathematical model for frequency of use
effects is needed. To prevent that frequency effects
lead to an ever increasing activation value, a max-
imum activation level exists. This is also found in
Reitter?s (2008) corpus studies, which indicate that
 0
 0.2
 0.4
 0.6
 0.8
 1
 1  3  5  7  9  11  13  15
T
em
po
ra
ry
 A
ct
iv
at
io
n 
 ta
(?
r)
Recency Distance  ?r
(a)
? = 1
2
4
8
16
 0
 0.2
 0.4
 0.6
 0.8
 1
 1  3  5  7  9  11  13  15
Pe
rm
an
en
t A
ct
iv
at
io
n 
 p
a(
 f 
)
Frequency Counter f
(b)
? = 1
2
4
8
16
Figure 1: Plots of the mathematical functions that
model recency and frequency effects. Plot (a) dis-
plays temporary activation depending on the re-
cency of priming. Plot (b) shows permanent activ-
ation depending on the frequency count. Both are
shown for different values of the slope parameter
? respectively ? .
the frequency effect is inversely connected to the
recency effect. Here, we model recency effects with
a general exponential saturation function, modified
to meet the requirements for modelling permanent
activation of linguistic structures:
pa( f ) = 1? exp
(
?
f ?1
?
)
, (2)
f ? N+; ? > 0; pa ? [0,1]
The most important point to note here is that the
permanent activation value pa( f ) is not a function
of time but a function of the frequency-counter f
attached to each linguistic structure. Whenever a
structure is primed, its counter is increased by the
value of 1. Again, the slope of the function is de-
termined by the parameter ? and the function is
84
shifted right in order to get an activation value of
0 for f = 1. A plot of equation (2) with different
slope parameters is given in Figure 1b. Similar to
the advantages of the model of temporary activa-
tion, this model for frequency effects is very flex-
ible so that different empirical findings and human
individuality can be expressed easily.
Now, both priming models need to be combined
for a model of alignment. We opted for a weighted
linear combination of temporary and permanent
activation:
ca(?r, f ) = ? ? ta(?r)+(1??) ? pa( f ), (3)
0? ? ? 1; ca ? [0,1]
Different values of ? allow different forms of align-
ment. With a value of ? = 0.5 recency and fre-
quency effects are equally important, with a value
of ? = 1 alignment depends on recency only, and
with a value of ? = 0 alignment is governed solely
by frequency. Being able to adjust the influence
of the different sorts of priming on alignment is
crucial as it has not yet been empirically determ-
ined to what extent recency and frequency of use
affect alignment (in Section 5.2 we will exploit this
flexibility for matching empircial data).
In contrast to the models of alignment presented
in Section 2, the computational alignment model
presented here will not only consider alignment
between the interlocutors (interpersonal- or other-
alignment), but also alignment to oneself (intra-
personal- or self-alignment). Pickering et al (2003)
present results from three experiments which sug-
gest self-alignment to be even more important than
other-alignment. In our model, self-alignment is
accounted for with the same priming-based mech-
anisms. To this end, four counters are attached to
each linguistic structure:
? ?rs: recency of use by the system itself
? ?ro: recency of use by the interlocutor
? fs: frequency of use by the system itself
? fo: frequency of use by the interlocutor
The overall activation value of the structure is
a linear combination of the combined activation
value ca(?rs, fs) and the combined activation value
ca(?ro, fo) from equation (3):
act(?rs, fs,?ro, fo) =
? ? (? ? ca(?rs, fs)+(1??) ? ca(?ro, fo)),
(4)
0? ? ,? ? 1; act ? [0,1]
Again, by changing the factor ? , smooth interpola-
tion between pure self-alignment (? = 1) and pure
other-alignment (? = 0) is possible, which can ac-
count for different empirical findings or human
individual differences. Furthermore, the strength
of alignment is modelled with a scaling factor ? ,
which determines whether alignment is considered
during generation (? > 0) or not (? = 0).
4 The Alignment-capable Microplanner
SPUD prime
The previously described priming-based model of
alignment has been implemented by extending
the integrated microplanning system SPUD lite
(Stone, 2002). SPUD lite is a lightweight Prolog
re-implementation of the SPUD microplanning sys-
tem (Stone et al, 2003) based on the context-free
tree rewriting grammar formalism TAGLET. Not
only the microplanner itself, but also the linguistic
structures (the initial TAGLET trees) are represen-
ted as Prolog clauses.
SPUD lite carries out the different microplan-
ning tasks (lexical choice, syntactic choice, refer-
ring expression generation and aggregation) at once
by treating microplanning as a search problem. Dur-
ing generation it tries to find an utterance which is
in accordance with the constraints set by its input
(a grammar, a knowledge base and a query). This is
done by searching the search space spanned by the
linguistic grammar rules and the knowledge base
until a goal state is found. Non-goal search states
are preliminary utterances that are extended by one
linguistic structure in each step until a syntactically
complete utterance is found which conveys all the
specified communicative goals. Since this search
space is large even for relatively small grammars,
a heuristic greedy search strategy is utilised.
Our alignment-capable microplanner SPUD
prime extends SPUD lite in several ways. First, we
altered the predicate for the initial TAGLET trees
by adding a unique identifier ID as well as counters
for self/other-recency/frequency values (rs, fs, ro
and fo; see Section 3). The activation value of an
initial tree is then calculated with equation (4).
Furthermore, we have created a mechanism that
enables SPUD lite to change the recency and fre-
quency information attached to the initial trees on-
line during generation. This is done in three steps
with the help of Prolog?s meta-programming cap-
abilities: Firstly, the clause of a tree is retrieved
85
from the knowledge base. Secondly, it is retrac-
ted from the knowledge base. Finally, the clause
is (re-)asserted in the knowledge base with up-
dated recency and frequency information. As a
welcome side effect of this procedure, primed ini-
tial trees are moved to the top of the knowledge
base and ? since Prolog evaluates clauses and facts
in the order of their appearance in the knowledge
base ? they can be accessed earlier than unprimed
initial trees or initial trees that were primed longer
ago. Thus, in SPUD prime recency of priming dir-
ectly influences the access of linguistic structures.
Most importantly, the activation values of the ini-
tial trees are considered during generation. Thus, in
addition to the evaluation measures used by SPUD
lite?s heuristic state evaluation function, the mean
activation value
act(S) =
?Ni=1 actti(?rsti , fsti ,?roti , foti )
N
of the N initial trees {t1, . . . , tN} of a given search
state S is taken into account as a further evaluation
measure. Hence, when SPUD prime evaluates (oth-
erwise equal) successor search states, the one with
the highest mean activation value is chosen as the
next current state.
5 Evaluation
In order to find out whether our priming-based
alignment model and its implementation work as
intended, we evaluated SPUD prime on a corpus
that was collected in an experiment designed to
investigate the alignment behaviour of humans in
a controlled fashion (Wei? et al, 2008). The part
of the corpus that we used consists of eight recor-
ded and transcribed dialogues between two inter-
locutors that play the ?Jigsaw Map Game?, a task
in which different objects have to be placed cor-
rectly on a table. Speakers take turns in explaining
each other where to place the next object in re-
lation to the objects that are already on the table.
Each speaker has to learn a set of name?object rela-
tions before the game, such that both use the same
names for all but three objects. Due to this precon-
dition, both speakers use the same lexical referring
expressions for most objects and the speaker?s lex-
ical alignment behaviour for the differently named
objects can be observed easily.
In our evaluation, we concentrate on the gener-
ation of nouns by simulating the uses of the three
differently learned nouns in the eight dialogues
from the perspective of all sixteen interlocutors.
In each test, SPUD prime plays the role of one
of the speakers talking to a simulated interlocutor
who behaves exactly as in the real experiment.
With this test setup we examined, first, how well
SPUD prime can model the alignment behaviour
of a real speaker in a real dialogue context and,
second, whether our model is flexible enough to
consistently emulate different speakers with differ-
ent alignment behaviour.
In order to find the best model (i.e., the best
parameter set {?,? ,?,?}) for each speaker, we
simulated all tests with all parameter combinations
and counted the number of mismatches between
our model?s choice and the real speaker?s choice.
To make this exhaustive search possible, we limit
the set of values for the parameters ? and ? to
{1,2,4,6,8,10,14,18,24,30} and the set of values
for the parameters ? and ? to {0,0.1,0.2, ...,1},
resulting in a total of 112?102 = 12100 different
parameter sets. Since we want to investigate align-
ment, ? is constantly set to 1.
5.1 An Illustrative Example
To illustrate our evaluation method, we first present
and discuss the simulation of one particular dia-
logue (from the Jigsaw Map Game corpus) from
the perspective of participant (A). Before the exper-
iment started, both interlocutors learned the name?
object relations ?Raute? (rhombus), ?Ring? (ring),
?Schraube? (bolt) and ?Wu?rfel? (cube), additionally
participant (A) learned ?Spielfigur? (token), ?Ball?
(sphere) and ?Block? (cuboid) and participant (B)
learned ?Ma?nnchen? (token), ?Kugel? (sphere) and
?Klotz? (cuboid). In our simulation, we focus on the
use of the differently learned names (the targets)
and not on the other names (non-targets). Table 1
shows the sequence of target nouns as they oc-
curred in the real dialogue (non-targets omitted).
For each parameter set {?,? ,?,?} the dialogue
is simulated in the following way:
? When participant (A) used a referring non-
target noun in the dialogue, self-priming of
the corresponding rule(s) in SPUD prime?s
knowledge base is simulated (i.e., the recency
and frequency counters are increased).
? When participant (A) used a referring target
noun in the dialogue, SPUD prime is queried
to generate a noun for the target object. Then
it is noted whether the noun actually generated
86
B: der Klotz 14 A: der Klotz
1 A: die Spielfigur 15 A: die Kugel
2 A: der Klotz 16 A: der Klotz
B: das Ma?nnchen B: der Klotz
B: der Klotz B: die Kugel
3 A: die Spielfigur B: der Klotz
B: das Ma?nnchen 17 A: der Klotz
4 A: das Ma?nnchen B: das Ma?nnchen
5 A: das Ma?nnchen B: der Klotz
6 A: das Ma?nnchen 18 A: das Ma?nnchen
7 A: das Ma?nnchen 19 A: der Klotz
8 A: das Ma?nnchen B: das Ma?nnchen
B: das Ma?nnchen 20 A: der Ball
9 A: das Ma?nnchen 21 A: das Ma?nnchen
10 A: der Ball B: der Ball
B: der Ball B: das Ma?nnchen
11 A: der Ball 22 A: die Kugel
12 A: der Ball 23 A: der Ball
B: die Kugel B: der Klotz
B: das Ma?nnchen 24 A: der Ball
13 A: der Ball B: der Klotz
B: die Kugel 25 A: der Klotz
Table 1: Sequence of referring target nouns used by
participants (A) and (B) in our example dialogue.
is the noun used in the actual dialogue (match)
or not (mismatch).
? When participant (B) used a referring noun
(target or non-target), priming of the corres-
ponding rule(s) in SPUD prime?s knowledge
base is simulated.
The evaluation measure for a specific parameter
set is the number of mismatches it produces when
simulating a dialogue. Thus the parameter set (or
rather sets) which produce the least number of mis-
matches are the ones that best model the particular
speaker under consideration. For participant (A)
of our example dialogue the distribution of para-
meter sets p producing m mismatches is shown in
Table 2. Four parameter sets produce only two mis-
matches (in phrase 15 and 22; cf. Table 1) and thus
our priming-based alignment model can account
for 92% of the target nouns produced by speaker
(A). However, it must be noted that these two mis-
matches occur at points in the dialogue where the
alignment behaviour of (A) is not straightforward.
At target noun 15, both interlocutors have already
used the name ?Ball? and then both switch to ?Ku-
gel?. The mismatch at target 22 is a special case: (A)
used ?Kugel? and immediately corrected himself to
?Ball?, the name he learned prior to the experiment.
It seems as if the task instruction, to use the learned
nouns, suddenly became prevalent.
m 0 1 2 3 4 5
# p 0 0 4 833 3777 2248
m 6 7 8 9 10 . . .
# p 3204 1105 478 148 294 0
Table 2: Number of parameter sets p leading to m
mismatches for participant (A) in dialogue 7.
5.2 Simulation Results
To evaluate our alignment-capable microplanner,
we simulated the noun production for each of the
interlocutors from the experiment. One dialogue
has been excluded from the data analysis as the
dialogue partners used nouns that none of them had
learned in the priming phase. For each of the re-
maining 14 interlocutors we varied the parameters
? , ? , ? and ? as described above to identify those
parameter set(s) which result in the least number
of mismatches.
Each interlocutor produced between 18 and 32
target nouns (N=14, M=23.071, SD=3.936). Our
simulation runs contain between 0 and 19 mis-
matches overall (N=169400, M=6.35, SD=3.398).
The minimal number of mismatches for each
speaker simulation ranges between 0 and 6 (N=14,
M=2.286, SD=1.684). That is, our model can sim-
ulate a mean of 89.9% of all target nouns (N=14,
M=.899, Min=.667, Max=1.000, SD=.082), which
is an improvement of 24.6% on the baseline con-
dition (alignment switched off), where 65.3% of
the target nouns are generated correctly (N=14,
M=.653, Min=.360, Max=1.000, SD=.071). As
already illustrated in Section 5.1, mismatches typic-
ally occur at points in the dialogue where the align-
ment behaviour of the interlocutor is not straight-
forward.
As displayed in Table 3 the parameter assign-
ments resulting in least mismatches differ consid-
erably from speaker to speaker. However, there are
some remarkable trends to be observed in the data.
As concerns the parameter ? , which determines
the combination of self- and other-alignment, the
majority of values are in the upper range of the
interval [0,1]. For 8 of 14 speakers the mean is
above 0.7 with relatively low standard deviations.
Only for one speaker (P13) the mean ? is below
0.3. Thus, the parameter values indicate a consider-
able tendency toward self-alignment in contrast to
other-alignment.
For the parameter ? that interpolates between
recency and frequency effects of priming, the res-
87
? ? ? ?
m # p M SD M SD M SD M SD
P13 2 4 3.0 1.155 19.5 9.14 .1 .0 .3 .0
P14 1 72 5.53 1.52 14.32 9.61 .819 .040 .901 .108
P17 1 200 1.66 .823 12.94 9.529 .353 .169 .955 .069
P18 3 2445 15.37 8.758 10.98 9.76 .597 .211 .706 .236
P19 0 4321 11.81 9.492 11.01 8.929 .824 .148 .387 .291
P20 2 8 1.0 .0 15.75 9.285 .737 .052 .388 .146
P23 6 987 6.85 6.681 12.08 9.354 .331 .374 .4 .33
P24 3 256 12.95 9.703 13.63 8.937 .537 .201 .468 .298
P39 5 1 1.0 .0 2.0 .0 .9 .0 .8 .0
P40 0 3504 12.08 9.33 10.30 8.753 .843 .147 .343 .282
P41 2 609 11.37 8.475 15.34 8.921 .770 .106 .655 .213
P42 3 30 6.0 1.486 17.53 9.016 .783 .059 .760 .122
P47 2 326 13.75 7.794 13.53 9.508 .772 .095 .816 .166
P48 2 2478 12.87 9.545 10.74 8.538 .764 .175 .166 .148
Table 3: Mean parameter values for those simulation runs which result in a minimal number of mismatches
for each speaker.
ults are less revealing. For two speaker simulations
(P13 and P48) the mean ? is 0.3 or lower, for an-
other four speaker simulations the mean ? is above
0.7. That is, our model produces good matching be-
haviour in adopting different alignment strategies,
depending either primarily on frequency or recency,
respectively. All other simulations, however, are
characterised by a mean ? in the medium range
along with a relatively high standard deviation.
6 Conclusion
In this paper, we introduced a priming-based model
of alignment which focusses more on the psycho-
linguistic aspects of interactive alignment and mod-
els recency and frequency of use effects ? as pro-
posed by Reitter (2008) and Brennan and Clark
(1996) ? as well as the difference between intraper-
sonal and interpersonal alignment (Pickering et al,
2003; Pickering and Garrod, 2004). The presented
model is fully parameterisable and can account for
different empirical findings and ?personalities?. It
has been implemented in the SPUD prime micro-
planner which activates linguistic rules by changing
its knowledge base on-line and considers the ac-
tivation values of those rules used in constructing
the current utterance by using their mean activation
value as an additional feature in its state evaluation
function.
We evaluated our alignment model and its im-
plementation in SPUD prime on a corpus of task-
oriented dialogue collected in an experimental
setup especially designed for alignment research.
The results of this evaluation show that our priming-
based model of alignment is flexible enough to sim-
ulate the alignment behaviour of different human
speakers (generating target nouns) in the experi-
mental setting. It should be noted, however, that
our model tries to give a purely mechanistic ex-
planation of lexical and syntactic choice and that
it, therefore, cannot explain alignment phenomena
that are due to social factors (e.g., politeness, rela-
tionship, etc.), audience design or cases, in which a
speaker consciously decides whether to align or not
(e.g., whether to use a word or its synonym). While
the evaluation has shown that our model can repro-
duce human alignment behaviour to a high degree,
it remains to be investigated which influence each
parameter exerts and how exactly the parameters
vary across individual speakers.
Nevertheless, the development of the alignment-
capable microplanner is only one step in the dir-
ection of an intuitive natural language human?
computer interaction system. In order to reach this
goal, the next step is to combine SPUD prime with
a natural language understanding system, which
should ideally work with the same linguistic rep-
resentations so that the linguistic structures used
by the interlocutor could be primed automatically.
This work is underway.
Furthermore, user studies should be carried
out in order to evaluate SPUD prime in a more
sophisticated way. Branigan et al (in press) found
that human?computer alignment was even stronger
than human?human alignment. But how would
the alignment behaviour of human interlocutors
change if the computer they are speaking to also
aligns to them? Further, would integration of an
alignment-capable dialogue system into a computer
interface make the interaction more natural? And
would an embodied conversational agent appear
88
more resonant and more sociable (Kopp, 2008) if
it aligned to users during conversation? The work
presented here provides a starting point for the
investigation of these questions.
Acknowledgements ? This research is supported
by the Deutsche Forschungsgemeinschaft (DFG) in
the Center of Excellence in ?Cognitive Interaction
Technology? (CITEC) as well as in the Collabor-
ative Research Center 673 ?Alignment in Commu-
nication?. We also thank Petra Wei? for making
the ?Jigsaw Map Game? corpus available and three
anonymous reviewers for their helpful comments.
References
John A. Bateman. 2006. A social-semiotic view of
interactive alignment and its computational instanti-
ation: A brief position statement and proposal. In
Kerstin Fischer, editor, How People Talk to Com-
puters, Robots and Other Artificial Communication
Partners, SFB/TR 8 Report No. 010-09/2006, pages
157?170, Bremen, Germany.
J. Kathryn Bock and Zenzi M. Griffin. 2000. The per-
sistence of structural priming: Transient activation
or implicit learning? Journal of Experimental Psy-
chology: General, 129(2):177?192.
Holly P. Branigan, Martin J. Pickering, and Alexan-
dra A. Cleland. 1999. Syntactic priming in written
production: Evidence for rapid decay. Psychonomic
Bulletin & Review, 6(4):635?640.
Holly P. Branigan, Martin J. Pickering, Jamie Pearson,
and Janet F. McLean. in press. Linguistic alignment
between people and computers. Journal of Pragmat-
ics.
Susan E. Brennan and Herbert H. Clark. 1996.
Conceptual pacts and lexical choice in conversa-
tion. Journal of Experimental Psychology: Learn-
ing, Memory, and Cognition, 22(6):1482?1493.
Susan E. Brennan. 1991. Conversation with and
through computers. User Modeling and User-Adapt-
ed Interaction, 1(1):67?86.
Carsten Brockmann, Amy Isard, Jon Oberlander, and
Michael White. 2005. Modelling alignment for af-
fective dialogue. In Proc. of the Workshop on Adapt-
ing the Interaction Style to Affective Factors at the
10th Int. Conf. on User Modeling.
Markus A. de Jong, Marie?t Theune, and Dennis Hofs.
2008. Politeness and alignment in dialogues with
a virtual guide. In Proc. of the 7th Int. Conf. on
Autonomous Agents and Multiagent Systems, pages
207?214.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and alignment in generated dia-
logues. In Proc. of the 4th Int. Natural Language
Generation Conf., pages 25?32.
Stefan Kopp. 2008. From communicators to reson-
ators ? Making embodied conversational agents so-
ciable. In Proc. of the Speech and Face to Face
Communication Workshop in Memory of Christian
Beno??t, pages 34?36.
Willem J. M. Levelt and Stephanie Kelter. 1982. Sur-
face form and memory in question answering. Cog-
nitive Psychology, 14(1):78?106.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences, 27(2):169?226.
Martin J. Pickering, Holly P. Branigan, and Janet F.
McLean. 2003. Dialogue structure and the activ-
ation of syntactic information. In Proc. of the 9th
Annual Conf. on Architectures and Mechanisms for
Language Processing, page 126.
Matthew Purver, Ronnie Cann, and Ruth Kempson.
2006. Grammars as parsers: Meeting the dialogue
challenge. Research on Language and Computation,
4(2?3):289?326.
David Reitter. 2008. Context Effects in Language Pro-
duction: Models of Syntactic Priming in Dialogue
Corpora. Ph.D. thesis, University of Edinburgh.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with communicative intentions: The SPUD sys-
tem. Computational Intelligence, 19(4):311?381.
Matthew Stone. 2002. Lexicalized grammar 101. In
Proc. of the ACL-02 Workshop on Effective Tools
and Methodologies for Teaching Natural Language
Processing and Computational Linguistics, pages
77?84.
Petra Wei?, Thies Pfeiffer, Gesche Schaffranietz, and
Gert Rickheit. 2008. Coordination in dialog: Align-
ment of object naming in the Jigsaw Map Game. In
Proc. of the 8th Annual Conf. of the Cognitive Sci-
ence Society of Germany, pages 4?20.
89
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 51?54,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Middleware for Incremental Processing in Conversational Agents
David Schlangen?, Timo Baumann?, Hendrik Buschmeier?, Okko Bu??
Stefan Kopp?, Gabriel Skantze?, Ramin Yaghoubzadeh?
?University of Potsdam ?Bielefeld University ?KTH, Stockholm
Germany Germany Sweden
david.schlangen@uni-potsdam.de
Abstract
We describe work done at three sites on
designing conversational agents capable of
incremental processing. We focus on the
?middleware? layer in these systems, which
takes care of passing around and maintain-
ing incremental information between the
modules of such agents. All implementa-
tions are based on the abstract model of
incremental dialogue processing proposed
by Schlangen and Skantze (2009), and the
paper shows what different instantiations
of the model can look like given specific
requirements and application areas.
1 Introduction
Schlangen and Skantze (2009) recently proposed
an abstract model of incremental dialogue process-
ing. While this model introduces useful concepts
(briefly reviewed in the next section), it does not
talk about how to actually implement such sys-
tems. We report here work done at three different
sites on setting up conversational agents capable
of incremental processing, inspired by the abstract
model. More specifically, we discuss what may
be called the ?middleware? layer in such systems,
which takes care of passing around and maintaining
incremental information between the modules of
such agents. The three approaches illustrate a range
of choices available in the implementation of such
a middle layer. We will make our software avail-
able as development kits in the hope of fostering
further research on incremental systems.1
In the next section, we briefly review the abstract
model. We then describe the implementations cre-
ated at Uni Bielefeld (BF), KTH Stockholm (KTH)
and Uni Potsdam (UP). We close with a brief dis-
cussion of similarities and differences, and an out-
look on further work.
1Links to the three packages described here can be found
at http://purl.org/net/Middlewares-SIGdial2010.
2 The IU-Model of Incremental Processing
Schlangen and Skantze (2009) model incremental
systems as consisting of a network of processing
modules. Each module has a left buffer, a proces-
sor, and a right buffer, where the normal mode of
processing is to take input from the left buffer, pro-
cess it, and provide output in the right buffer, from
where it goes to the next module?s left buffer. (Top-
down, expectation-based processing would work
in the opposite direction.) Modules exchange incre-
mental units (IUs), which are the smallest ?chunks?
of information that can trigger connected modules
into action. IUs typically are part of larger units;
e.g., individual words as parts of an utterance, or
frame elements as part of the representation of an
utterance meaning. This relation of being part of
the same larger unit is recorded through same level
links; the information that was used in creating a
given IU is linked to it via grounded in links. Mod-
ules have to be able to react to three basic situa-
tions: that IUs are added to a buffer, which triggers
processing; that IUs that were erroneously hypothe-
sised by an earlier module are revoked, which may
trigger a revision of a module?s own output; and
that modules signal that they commit to an IU, that
is, won?t revoke it anymore (or, respectively, expect
it to not be revoked anymore).
Implementations of this model then have to re-
alise the actual details of this information flow, and
must make available the basic module operations.
3 Sociable Agents Architecture
BF?s implementation is based on the ?D-Bus? mes-
sage bus system (Pennington et al, 2007), which
is used for remote procedure calls and the bi-
directional synchronisation of IUs, either locally
between processes or over the network. The bus sys-
tem provides proxies, which make the interface of
a local object accessible remotely without copying
data, thus ensuring that any access is guaranteed to
yield up-to-date information. D-Bus bindings exist
for most major programming languages, allowing
51
for interoperability across various systems.
IUs exist as objects implementing a D-Bus in-
terface, and are made available to other modules
by publishing them on the bus. Modules are ob-
jects comprising a main thread and right and left
buffers for holding own IUs and foreign IU proxies,
respectively. Modules can co-exist in one process
as threads or occupy one process each?even dis-
tributed across a network.
A dedicated Relay D-Bus object on the network
is responsible for module administration and up-
date notifications. At connection time, modules
register with the relay, providing a list of IU cat-
egories and/or module names they are interested
in. Category interests create loose functional links
while module interests produce more static ones.
Whenever a module chooses to publish informa-
tion, it places a new IU in its right buffer, while
removal of an IU from the right buffer corresponds
to retraction. The relay is notified of such changes
and in turn invokes a notification callback in all
interested modules synchronising their left buffers
by immediately and transparently creating or re-
moving proxies of those IUs.
IUs consist of the fields described in the abstract
model, and an additional category field which the
relay can use to identify the set of interested mod-
ules to notify. They furthermore feature an optional
custom lifetime, on the expiration of which they
are automatically retracted.
Incremental changes to IUs are simply realised
by changing their attributes: regardless of their lo-
cation in either a right or left buffer, the same setter
functions apply (e.g., set payload). These generate
relay-transported update messages which commu-
nicate the ID of the changed IU. Received update
messages concerning self-owned and remotely-
owned objects are discerned automatically to allow
for special treatment of own IUs. The complete
process is illustrated in Figure 1.
Current state and discussion. Our support for
bi-directional IU editing is an extension to the con-
cepts of the general model. It allows higher-level
modules with a better knowledge of context to re-
vise uncertain information offered by lower levels.
Information can flow both ways, bottom-up and
top-down, thus allowing for diagnostic and causal
networks linked through category interests.
Coming from the field of embodied conversa-
tional agents, and being especially interested in
modelling human-like communication, for exam-
A B
C
IU
IU proxy
Write access
Relay
Data access
Update notification
RBuf LBuf
Interest sets
Figure 1: Data access on the IU proxies is transparently dele-
gated over the D-Bus; module A has published an IU. B and C
are registered in the corresponding interest set, thus receiving
a proxy of this IU in their left buffer. When B changes the IU,
A and C receive update notifications.
ple for on-line production of listener backchannel
feedback, we constantly have to take incremen-
tally changing uncertain input into account. Using
the presented framework consistently as a network
communication layer, we are currently modelling
an entire cognitive architecture for virtual agents,
based on the principle of incremental processing.
The decision for D-Bus as the transportation
layer has enabled us to quickly develop ver-
sions for Python, C++ and Java, and produced
straightforward-to-use libraries for the creation of
IU-exchanging modules: the simplest fully-fledged
module might only consist of a periodically in-
voked main loop callback function and any subset
of the four handlers for IU events (added, removed,
updated, committed).
4 Inpro Toolkit
The InproTK developed at UP offers flexibility on
how tightly or loosely modules are coupled in a
system. It provides mechanisms for sending IU up-
dates between processes via a messaging protocol
(we have used OAA [Cheyer and Martin, 2001], but
other communication layers could also be used) as
well as for using shared memory within one (Java)
process. InproTK follows an event-based model,
where modules create events, for which other mod-
ules can register as Listeners. Module networks are
configured via a system configuration file which
specifies which modules listen to which.
Modules push information to their right, hence
the interface for inter-module communication is
called PushBuffer. (At the moment, InproTK only
implements left-to-right IU flow.) The PushBuffer
interface defines a hypothesis-change method
which a module will call for all its listening mod-
ules. A hypothesis change is (redundantly) charac-
terised by passing both the complete current buffer
state (a list of IUs) as well as the delta between
52
the previous and the current state, leaving listen-
ing modules a choice of how to implement their
internal update.
Modules can be fully event-driven, only trig-
gered into action by being notified of a hypothesis
change, or they can run persistently, in order to cre-
ate endogenous events like time-outs. Event-driven
modules can run concurrently in separate threads or
can be called sequentially by a push buffer (which
may seem to run counter the spirit of incremental
processing, but can be advantageous for very quick
computations for which the overhead of creating
threads should be avoided).
IUs are typed objects, where the base class IU
specifies the links (same-level, grounded-in) that
allow to create the IU network and handles the
assignment of unique IDs. The payload and addi-
tional properties of an IU are specified for the IU?s
type. A design principle here is to make all relevant
information available, while avoiding replication.
For instance, an IU holding a bit of semantic rep-
resentation can query which interval of input data
it is based on, where this information is retrieved
from the appropriate IUs by automatically follow-
ing the grounded-in links. IU networks ground out
in BaseData, which contains user-side input such
as speech from the microphone, derived ASR fea-
ture vectors, camera feeds from a webcam, derived
gaze information, etc., in several streams that can
be accessed based on their timing information.
Besides IU communication as described in the
abstract model, the toolkit also provides a separate
communication track along which signals, which
are any kind of information that is not seen as incre-
mental hypotheses about a larger whole but as infor-
mation about a single current event, can be passed
between modules. This communication track also
follows the observer/listener model, where proces-
sors define interfaces that listeners can implement.
Finally, InproTK also comes with an extensive
set of monitoring and profiling modules which can
be linked into the module network at any point and
allow to stream data to disk or to visualise it online
through a viewing tool (ANON 2009), as well as
different ways to simulate input (e.g., typed or read
from a file) for bulk testing.
Current state and discussion. InproTK is cur-
rently used in our development of an incremental
multimodal conversational system. It is usable in its
current state, but still evolves. We have built and in-
tegrated modules for various tasks (post-processing
of ASR output, symbolic and statistical natural lan-
guage understanding [ANON 2009a,b,c]). The con-
figuration system and the availability of monitoring
and visualisation tools enables us to quickly test
different setups and compare different implementa-
tions of the same tasks.
5 Jindigo
Jindigo is a Java-based framework for implement-
ing and experimenting with incremental dialogue
systems currently being developed at KTH. In
Jindigo, all modules run as separate threads within
a single Java process (although the modules them-
selves may of course communicate with external
processes). Similarly to InproTK, IUs are mod-
elled as typed objects. The modules in the system
are also typed objects, but buffers are not. Instead,
a buffer can be regarded as a set of IUs that are
connected by (typed) same-level links. Since all
modules have access to the same memory space,
they can follow the same-level links to examine
(and possibly alter) the buffer. Update messages
between modules are relayed based on a system
specification that defines which types of update
messages from a specific module go where. Since
the modules run asynchronously, update messages
do not directly invoke methods in other modules,
but are put on the input queues of the receiving
modules. The update messages are then processed
by each module in their own thread.
Jindigo implements a model for updating buffers
that is slightly different than the two previous ap-
proaches. In this approach, IUs are connected by
predecessor links, which gives each IU (words,
widest spanning phrases from the parser, commu-
nicative acts, etc), a position in a (chronologically)
ordered stream. Positional information is reified by
super-imposing a network of position nodes over
the IU network, with the IUs being associated with
edges in that network. These positional nodes then
give us names for certain update stages, and so
revisions can be efficiently encoded by reference
to these nodes. An example can make this clearer.
Figure 2 shows five update steps in the right buffer
of an incremental ASR module. By reference to po-
sitional nodes, we can communicate easily (a) what
the newest committed IU is (indicated in the figure
as a shaded node) and (b) what the newest non-
revoked or active IU is (i.e., the ?right edge? (RE);
indicated in the figure as a node with a dashed line).
So, the change between the state at time t1 and t2
is signalled by RE taking on a different value. This
53
Figure 2: The right buffer of an ASR module, and update
messages at different time-steps.
value (w3) has not been seen before, and so the
consuming module can infer that the network has
been extended; it can find out which IUs have been
added by going back from the new RE to the last
previously seen position (in this case, w2). At t3, a
retraction of a hypothesis is signalled by a return to
a previous state, w2. All consuming modules have
to do now is to return to an internal state linked
to this previous input state. Commitment is repre-
sented similarly through a pointer to the rightmost
committed node; in the figure, that is for example
w5 at t5.
Since information about whether an IU has been
revoked or committed is not stored in the IU it-
self, all IUs can (if desirable) be defined as im-
mutable objects. This way, the pitfalls of having
asynchronous processes altering and accessing the
state of the IUs may be avoided (while, however,
more new IUs have to be created, as compared to
altering old ones). Note also that this model sup-
ports parallel hypotheses as well, in which case the
positional network would turn into a lattice.
The framework supports different types of up-
date messages and buffers. For example, a parser
may incrementally send NPs to a reference reso-
lution (RR) module that has access to a domain
model, in order to prune the chart. Thus, informa-
tion may go both left-to-right and right-to-left. In
the buffer between these modules, the order be-
tween the NPs that are to be annotated is not im-
portant and there is no point in revoking such IUs
(since they do not affect the RR module?s state).
Current state and discussion. Jindigo uses con-
cepts from (Skantze, 2007), but has been rebuilt
from ground up to support incrementality. A range
of modules for ASR, semantic interpretation, TTS,
monitoring, etc., have been implemented within
the framework, allowing us to do experiments
with complete systems interacting with users. We
are currently using the framework to implement a
model of incremental speech production.
6 Discussion
The three implementations of the abstract IU model
presented above show that concrete requirements
and application areas result in different design de-
cisions and focal points.
While BF?s approach is loosely coupled and han-
dles exchange of IUs via shared objects and a me-
diating module, KTH?s implementation is rather
closely coupled and publishes IUs through a single
buffer that lies in shared memory. UP?s approach
is somewhat in between: it abstracts away from the
transportation layer and enables message passing-
based communication as well as shared memory
transparently through one interface.
The differences in the underlying module com-
munication infrastructure affect the way incremen-
tal IU updates are handled in the systems. In BF?s
framework modules holding an IU in one of their
buffers just get notified when one of the IU?s fields
changed. Conversely, KTH?s IUs are immutable
and new information always results in new IUs
being published and a change to the graph repre-
sentation of the buffer?but this allows an efficient
coupling of module states and cheap revoke op-
erations. Again, UP?s implementation lies in the
middle. Here both the whole new state and the delta
between the old and new buffer is communicated,
which leads to flexibility in how consumers can be
implemented, but also potentially to some commu-
nication overhead.
In future work, we will explore if further gener-
alisations can be extracted from the different im-
plementations presented here. For now, we hope
that the reference architectures presented here can
already be an inspiration for further work on incre-
mental conversational systems.
References
Adam Cheyer and David Martin. 2001. The open
agent architecture. Journal of Autonomous Agents
and Multi-Agent Systems, 4(1):143?148, March.
H. Pennington, A. Carlsson, and A. Larsson. 2007.
D-Bus Specification Version 0.12. http://dbus.free-
desktop.org/doc/dbus-specification.html.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of EACL 2009, Athens,
Greece.
Gabriel Skantze. 2007. Error Handling in Spoken Dia-
logue Systems. Ph.D. thesis, KTH, Stockholm, Swe-
den, November.
54
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 12?16,
Utica, May 2012. c?2012 Association for Computational Linguistics
Referring in Installments: A Corpus Study of Spoken Object References
in an Interactive Virtual Environment
Kristina Striegnitz?, Hendrik Buschmeier?and Stefan Kopp?
?Computer Science Department, Union College, Schenectady, NY
striegnk@union.edu
?Sociable Agents Group ? CITEC, Bielefeld University, Germany
{hbuschme,skopp}@techfak.uni-bielefeld.de
Abstract
Commonly, the result of referring expression
generation algorithms is a single noun phrase.
In interactive settings with a shared workspace,
however, human dialog partners often split re-
ferring expressions into installments that adapt
to changes in the context and to actions of their
partners. We present a corpus of human?human
interactions in the GIVE-2 setting in which in-
structions are spoken. A first study of object
descriptions in this corpus shows that refer-
ences in installments are quite common in this
scenario and suggests that contextual factors
partly determine their use. We discuss what
new challenges this creates for NLG systems.
1 Introduction
Referring expression generation is classically consid-
ered to be the problem of producing a single noun
phrase that uniquely identifies a referent (Krahmer
and van Deemter, 2012). This approach is well suited
for non-interactive, static contexts, but recently, there
has been increased interest in generation for situated
dialog (Stoia, 2007; Striegnitz et al, 2011).
Most human language use takes place in dynamic
situations, and psycholinguistic research on human?
human dialog has proposed that the production of
referring expressions should rather be seen as a pro-
cess that not only depends on the context and the
choices of the speaker, but also on the reactions of
the addressee. Thus the result is often not a single
noun phrase but a sequence of installments (Clark
and Wilkes-Gibbs, 1986), consisting of multiple utter-
ances which may be interleaved with feedback from
the addressee. In a setting where the dialog partners
have access to a common workspace, they, further-
more, carefully monitor each other?s non-linugistic
actions, which often replace verbal feedback (Clark
and Krych, 2004; Gergle et al, 2004). The following
example from our data illustrates this. A is instructing
B to press a particular button.
(1) A: the blue button
B: [moves and then hesitates]
A: the one you see on your right
B: [starts moving again]
A: press that one
While computational models of this behavior are still
scarce, some first steps have been taken. Stoia (2007)
studies instruction giving in a virtual environment
and finds that references to target objects are often
not made when they first become visible. Instead in-
teraction partners are navigated to a spot from where
an easier description is possible. Garoufi and Koller
(2010) develop a planning-based approach of this be-
havior. But once their system decides to generate a
referring expression, it is delivered in one unit.
Thompson (2009), on the other hand, proposes a
game-theoretic model to predict how noun phrases
are split up into installments. While Thompson did
not specify how the necessary parameters to calculate
the utility of an utterance are derived from the context
and did not implement the model, it provides a good
theoretical basis for an implementation.
The GIVE Challenge is a recent shared task on sit-
uated generation (Striegnitz et al, 2011). In the GIVE
scenario a human user goes on a treasure hunt in a
virtual environment. He or she has to press a series of
buttons that unlock doors and open a safe. The chal-
lenge for the NLG systems is to generate instructions
in real-time to guide the user to the goal. The instruc-
tions are presented to the user as written text, which
12
means that there is less opportunity for interleaving
language and actions than with spoken instructions.
While some systems generate sentence fragments in
certain situations (e.g., not this one when the user
is moving towards the wrong button), instructions
are generally produced as complete sentences and
replaced with a new full sentence when the context
changes (a strategy which would not work for spoken
instructions). Nevertheless, timing issues are a cause
for errors that is cited by several teams who devel-
oped systems for the GIVE challenge, and generating
appropriate feedback has been an important concern
for almost all teams (see the system descriptions in
(Belz et al, 2011)). Unfortunately, no systematic er-
ror analysis has been done for the interactions from
the GIVE challenges. Anecdotally, however, not re-
acting to signs of confusion in the user?s actions at
all or reacting too late seem to be common causes for
problems. Furthermore, we have found that the strat-
egy of replacing instructions with complete sentences
to account for a change in context can lead to con-
fusion because it seems unclear to the user whether
this new instruction is a correction or an elaboration.
In this paper we report on a study of the com-
municative behavior of human dyads in the GIVE
environment where instead of written text instruction
givers use unrestricted spoken language to direct in-
struction followers through the world. We find that
often multiple installments are used to identify a ref-
erent and that the instruction givers are highly respon-
sive to context changes and the instruction followers?
actions. Our goal is to inform the development of a
generation system that generates object descriptions
in installments while taking into account the actions
of its interaction partner.
2 A corpus of spoken instructions in a
virtual environment
Data collection method The setup of this study
was similar to the one used to collect the GIVE-2
corpus of typed instructions (Gargett et al, 2010).
Instruction followers (IFs) used the standard GIVE-2
client to interact with the virtual environment. In-
struction givers (IGs) could observe the followers?
position and actions in the world using an interactive
map, and they were also provided with the same 3D
view into the scene that the IFs saw on their screen.
Differently from the normal GIVE-2 scenario, the
IGs did not type their instructions but gave spoken
instructions, which were audio recorded as well as
streamed to the IFs over the network. A log of the IFs?
position, orientation and actions that was updated ev-
ery 200ms was recorded in a database.
Participants were recruited in pairs on Bielefeld
University?s campus and received a compensation
of six euros each. They were randomly assigned
to the roles of IG and IF and were seated and in-
structed separately. To become familiar with the task,
they switched roles in a first, shorter training world.
These interactions were later used to devise and test
the annotation schemes. They then played two dif-
ferent worlds in their assigned roles. After the first
round, they received a questionnaire assessing the
quality of the interaction; after the second round, they
completed the Santa Barbara sense of direction test
(Hegarty et al, 2006) and answered some questions
about themselves.
Annotations The recorded instructions of the IGs
were transcribed and segmented into utterances (by
identifying speech pauses longer than 300ms) using
Praat (Boersma and Weenink, 2011). We then created
videos showing the IGs? map view as well as the IFs?
scene view and aligned the audio and transcriptions
with them. The data was further annotated by the first
two authors using ELAN (Wittenburg et al, 2006).
Most importantly for this paper, we classified ut-
terances into the following types:
(i) move (MV) ? instruction to turn or to move
(ii) manipulate (MNP) ? instruction to manipulate an object
(e.g., press a button)
(iii) reference (REF) ? utterance referring to an object
(iv) stop ? instruction to stop moving
(v) warning ? telling the user to not do something
(vi) acknowledgment (ACK) ? affirmative feedback
(vii) communication management (CM) ? indicating that the
IG is planning (e.g., uhmm, just a moment, sooo etc.)
(viii) negative acknowledgment ? indicating a mistake on the
player?s part
(ix) other ? anything else
A few utterances which contained both move and
press instructions were further split, but in general
we picked the label that fit best (using the above list
as a precedence order to make a decision if two labels
fit equally well). The inter-annotator agreement for
utterance types was ? = 0.89 (Cohen?s kappa), which
13
is considered to be very good. Since the categories
were of quite different sizes (cf. Table 1), which may
skew the ? statistic, we also calculated the kappa per
category. It was satisfactory for all ?interesting? cate-
gories. The agreement for category REF was ? = 0.77
and the agreement for other was ? = 0.58. The kappa
values for all other categories were 0.84 or greater.
We reviewed all cases with differing annotations and
reached a consensus, which is the basis for all results
presented in this paper. Furthermore, we collapsed
the labels warning, negative acknowledgment and
other which only occurred rarely.
To support a later more in depth analysis, we also
annotated what types of properties are used in object
descriptions, the givenness status of information in
instructions, and whether an utterance is giving pos-
itive or negative feedback on a user action (even if
not explicitly labeled as (negative) acknowledgment).
Finally, information about the IF?s movements and
actions in the world as well as the visible context was
automatically calculated from the GIVE log files and
integrated into the annotation.
Collected data We collected interactions between
eight pairs. Due to failures of the network connection
and some initial problems with the GIVE software,
only four pairs were recorded completely, so that
we currently have data from eight interactions with
four different IGs. We are in the process of collect-
ing additional data in order to achieve a corpus size
that will allow for a more detailed statistical analy-
sis. Furthermore, we are collecting data in English
to be able to make comparisons with the existing
corpus of written instructions in the GIVE world and
to make the data more easily accessible to a wider
audience. The corpus will be made freely available
at http://purl.org/net/sgive-corpus.
Participants were between 20 and 30 years old and
all of them are native German speakers. Two of the
IGs are male and two female; three of the IFs are
female. The mean length of the interactions is 5.24
minutes (SD= 1.86), and the IGs on average use 325
words (SD = 91).
Table 1 gives an overview of the kinds of ut-
terances used by the IGs. While the general pic-
ture is similar for all speakers, there are statisti-
cally significant differences between the frequen-
cies with which different IGs use the utterance types
Table 1: Overall frequency of utterance types.
utterance type count %
MV 334 46.58
MNP 66 9.21
REF 65 9.07
stop 38 5.30
ACK 92 12.83
CM 97 13.53
other 25 3.49
Table 2: Transitional probabilities for utterance types.
M
V
M
N
P
R
E
F
st
op
A
C
K
C
M
ot
he
r
IF pr
es
s
MV .53 .08 .06 .06 .15 .08 .03 .00
MNP .02 .03 .09 .02 .02 .02 .02 .80
REF .00 .33 .19 .02 .14 .00 .02 .30
stop .47 .03 .18 .03 .03 .16 .11 .00
ACK .64 .08 .09 .03 .01 .10 .00 .05
CM .53 .05 .10 .08 .01 .18 .05 .00
other .44 .04 .12 .12 .08 .16 .00 .04
IF press .21 .01 .00 .01 .36 .36 .04 .00
(?2 = 78.82, p ? 0.001). We did not find a signifi-
cant differences (in terms of the utterance types used)
between the two worlds that we used or between the
two rounds that each pair played.
3 How instruction givers describe objects
We now examine how interaction partners establish
what the next target button is. Overall, there are 76
utterance sequences in the data that identify a target
button and lead to the IF pressing that button. We
discuss a selection of seven representative examples.
(2) IG: und dann dr?ckst du den ganz rechten Knopf den
blauen (and then you press the rightmost button the
blue one; MNP)
IF: [goes across the room and does it]
In (2) the IG generates a referring expression iden-
tifying the target and integrates it into an object ma-
nipulation instruction. In our data, 55% of the tar-
get buttons (42 out of 76) get identified in this way
(which fits into the traditional view of referring ex-
pression generation). In all other cases a sequence of
at least two, and in 14% of the cases more than two,
utterances is used.
The transitional probabilities between utterance
types shown in Table 2 suggest what some common
patterns may be. For example, even though move
instructions are so prevalent in our data, they are
uncommon after reference or manipulate utterances.
14
Instead, two thirds of the reference utterances are
followed by object manipulation instruction, another
reference or an acknowledgement. In the remaining
cases, IFs press a button in response to the reference.
(3) IG: vor dir der blaue Knopf (in front of you the blue button;
REF)
IF: [moves across the room toward the button]
IG: drauf dr?cken (press it; MNP)
(4) IG: und auf der rechten Seite sind zwei rote Kn?pfe (and
on the right are two red buttons; REF)
IF: [turns and starts moving towards the buttons]
IG: und den linken davon dr?ckst du (and you press the left
one; MNP)
In (3) and (4) a first reference utterance is followed
by a separate object manipulation utterance. While
in (3) the first reference uniquely identifies the target,
in (4) the first utterance simply directs the player?s
attention to a group of buttons. The second utterance
then picks out the target.
(5) IG: dreh dich nach links etwas (turn left a little; MV)
IF: [turns left] there are two red buttons in front of him
(and some other red buttons to his right)
IG: so, da siehst du zwei rote Schalter (so now you see two
red buttons; REF)
IF: [moves towards buttons]
IG: und den rechten davon dr?ckst du (and you press the
right one; MNP)
IF: [moves closer, but more towards the left one]
IG: rechts (right; REF)
Stoia (2007) observed that IGs use move instruc-
tions to focus the IF?s attention on a particular area.
This is also common in our data. For instance in (5),
the IF is asked to turn to directly face the group of
buttons containing the target. (5) also shows how IGs
monitor their partners? actions and respond to them.
The IF is moving towards the wrong button causing
the IG to repeat part of the previous description.
(6) IG: den blauen Schalter (the blue button; REF)
IF: [moves and then stops]
IG: den du rechts siehst (the one you see on your right;
REF)
IF: [starts moving again]
IG: den dr?cken (press that one; MNP)
Similarly, in (6) the IG produces an elaboration
when the IF stops moving towards the target, indicat-
ing her confusion.
(7) IG: und jetzt rechts an der (and now to the right on the;
REF)
IF: [turns right, is facing the wall with the target button]
IG: ja . . . genau . . . an der Wand den blauen Knopf (yes
. . . right . . . on the wall the blue button; ACK, REF)
IF: [moves towards button]
IG: einmal dr?cken (press once; MNP)
In (7) the IG inserts affirmative feedback when
the IF reacts correctly to a portion of his utterance.
As can be seen in Table 2, reference utterances are
relatively often followed by affirmative feedback.
(8) IF: [enters room, stops, looks around, ends up looking at
the target]
IG: ja genau den gr?nen Knopf neben der Lampe dr?cken
(yes right, press the green button next to the lamp;
MNP)
IGs can also take advantage of IF actions that are
not in direct response to an utterance. This happens
in (8). The IF enters a new room and looks around.
When she looks towards the target, the IG seizes the
opportunity and produces affirmative feedback.
4 Conclusions and future work
We have described a corpus of spoken instructions in
the GIVE scenario which we are currently building
and which we will make available once it is com-
pleted. This corpus differs from other corpora of task-
oriented dialog (specifically, the MapTask corpus
(Anderson et al, 1991), the TRAINS corpus (Hee-
man and Allen, 1995), the Monroe corpus (Stent,
2000)) in that the IG could observe the IF?s actions
in real-time. This led to interactions in which in-
structions are given in installments and linguistic and
non-linguistic actions are interleaved.
This poses interesting new questions for NLG sys-
tems, which we have illustrated by discussing the
patterns of utterance sequences that IGs and IFs use
in our corpus to agree on the objects that need to
be manipulated. In line with results from psycholin-
guistics, we found that the information necessary to
establish a reference is often expressed in multiple
installments and that IGs carefully monitor how their
partners react to their instructions and quickly re-
spond by giving feedback, repeating information or
elaborating on previous utterance when necessary.
The NLG system thus needs to be able to de-
cide when a complete identifying description can
be given in one utterance and when a description in
installments is more effective. Stoia (2007) as well
as Garoufi and Koller (2010) have addressed this
question, but their approaches only make a choice be-
tween generating an instruction to move or a uniquely
identifying referring expression. They do not con-
sider cases in which another type of utterance, for
instance, one that refers to a group of objects or gives
15
an initial ambiguous description, is used to draw the
attention of the IF to a particular area and they do not
generate referring expressions in installments.
The system, furthermore, needs to be able to in-
terpret the IF?s actions and decide when to insert an
acknowledgment, elaboration or correction. It then
has to decide how to formulate this feedback. The
addressee, e.g., needs to be able to distinguish elabo-
rations from corrections. If the feedback was inserted
in the middle of a sentence, if finally has to decide
whether this sentence should be completed and how
the remainder may have to be adapted.
Once we have finished the corpus collection, we
plan to use it to study and address the questions dis-
cussed above. We are planning on building on the
work by Stoia (2007) on using machine learning tech-
niques to develop a model that takes into account var-
ious contextual factors and on the work by Thompson
(2009) on generating references in installments. The
set-up under which the corpus was collected, further-
more, lends itself well to Wizard-of-Oz studies to test
the effectiveness of different interactive strategies for
describing objects.
Acknowledgments This research was supported
by the Deutsche Forschungsgemeinschaft (DFG) in
the Center of Excellence in ?Cognitive Interaction
Technology? (CITEC) and by the Skidmore Union
Network which was funded through an ADVANCE
grant from the National Science Foundation.
References
Anne H. Anderson, Miles Bader, Ellen Gurman Bard, Eliz-
abeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen
Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller,
Catherine Sotillo, Henry S. Thompson, and Regina
Weinert. 1991. The HCRC map task corpus. Lan-
guage and Speech, 34:351?366.
Anja Belz, Albert Gatt, Alexander Koller, and Kristina
Striegnitz, editors. 2011. Proceedings of the Genera-
tion Challenges Session at the 13th European Workshop
on Natural Language Generation, Nancy, France.
Paul Boersma and David Weenink. 2011. Praat: doing
phonetics by computer. Computer program. Retrieved
May 2011, from http://www.praat.org/.
Herbert H. Clark and Meredyth A. Krych. 2004. Speaking
while monitoring addressees for understanding. Jour-
nal of Memory and Language, 50:62?81.
Herbert H Clark and Deanna Wilkes-Gibbs. 1986. Refer-
ring as a collaborative process. Cognition, 22:1?39.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus
of giving instructions in virtual environments. In Pro-
ceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC?10), pages
2401?2406, Valletta, Malta.
Konstantina Garoufi and Alexander Koller. 2010. Au-
tomated planning for situated natural language gener-
ation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1573?1582, Uppsala, Sweden.
Darren Gergle, Robert E. Kraut, and Susan R. Fussell.
2004. Action as language in a shared visual space. In
Proceedings of the 2004 ACM Conference on Computer
Supported Cooperative Work, pages 487?496, Chicago,
IL.
Peter A. Heeman and James Allen. 1995. The Trains 93
dialogues. Technical Report Trains 94-2, Computer Sci-
ence Department, University of Rochester, Rochester,
NY.
Mary Hegarty, Daniel R. Montello, Anthony E. Richard-
son, Toru Ishikawa, and Kristin Lovelace. 2006. Spa-
tial abilities at different scales: Individual differences in
aptitude-test performance and spatial-layout learning.
Intelligence, 34:151?176.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38:173?218.
Amanda Stent. 2000. The Monroe corpus. Technical
Report 728/TN 99-2, Computer Science Department,
University of Rochester, Rochester, NY.
Laura Stoia. 2007. Noun Phrase Generation for Situated
Dialogs. Ph.D. thesis, Graduate School of The Ohio
State University, Columbus, OH.
Kristina Striegnitz, Alexandre Denis, Andrew Gargett,
Konstantina Garoufi, Alexander Koller, and Mari?t The-
une. 2011. Report on the second second challenge on
generating instructions in virtual environments (GIVE-
2.5). In Proceedings of the Generation Challenges
Session at the 13th European Workshop on Natural
Language Generation, pages 270?279, Nancy, France.
Will Thompson. 2009. A Game-Theoretic Model of
Grounding for Referential Communication Tasks. Ph.D.
thesis, Northwestern University, Evanston, IL.
Peter Wittenburg, Hennie Brugman, Albert Russel, Alex
Klassmann, and Han Sloetjes. 2006. ELAN: A pro-
fessional framework for multimodality research. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), pages
1556?1559, Genoa, Italy.
16
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 295?303,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Combining Incremental Language Generation and
Incremental Speech Synthesis for Adaptive Information Presentation
Hendrik Buschmeier1, Timo Baumann3, Benjamin Dosch, Stefan Kopp1, David Schlangen2
1Sociable Agents Group, CITEC and Faculty of Technology, Bielefeld University
2Dialogue Systems Group, Faculty of Linguistics and Literary Studies, Bielefeld University
{hbuschme,bdosch,skopp,david.schlangen}@uni-bielefeld.de
3Natural Language Systems Division, Department of Informatics, University of Hamburg
baumann@informatik.uni-hamburg.de
Abstract
Participants in a conversation are normally re-
ceptive to their surroundings and their inter-
locutors, even while they are speaking and can,
if necessary, adapt their ongoing utterance. Typ-
ical dialogue systems are not receptive and can-
not adapt while uttering. We present combin-
able components for incremental natural lan-
guage generation and incremental speech syn-
thesis and demonstrate the flexibility they can
achieve with an example system that adapts to
a listener?s acoustic understanding problems
by pausing, repeating and possibly rephrasing
problematic parts of an utterance. In an eval-
uation, this system was rated as significantly
more natural than two systems representing the
current state of the art that either ignore the
interrupting event or just pause; it also has a
lower response time.
1 Introduction
Current spoken dialogue systems often produce pre-
scripted system utterances or use templates with vari-
able substitution during language generation. If a
dialogue system uses grammar-based generation at
all, it produces complete utterances that are then syn-
thesised and realised in one big chunk. As systems
become increasingly more conversational, however,
the need arises to make output generation1 more flex-
ible. In particular, capabilities for incrementally gen-
erating output become desirable, for two kinds of
reasons.
(a) In situations where fast system responses are
important, production of output can begin before the
1We will use the term ?output generation? here to cover both
natural language generation and speech synthesis.
content that is to be presented is fully specified ? even
if what is being produced is just a turn-taking signal
(Skantze and Hjalmarsson, 2010).
(b) A system that produces its output incrementally
can react to events happening while it is realising an
utterance. This can be beneficial in domains where
the state of the world that the system relays informa-
tion about can change mid-utterance, so that a need
may arise to adapt while speaking. It should also
improve naturalness by allowing the system to react
to dialogue phenomena such as concurrent feedback
signals from the user (Buschmeier and Kopp, 2011).
We present work towards enabling such capabil-
ities. We have implemented and connected a com-
ponent for incremental natural language genera-
tion (iNLG) that works with specifications of sub-
utterance-sized communicative intentions and a com-
ponent for incremental speech synthesis (iSS) that can
handle sub-utterance-sized input and modifications
to not-yet-spoken parts of the utterance with very low
latencies. To explore whether such an output genera-
tion capability can indeed be advantageous, we have
created a test system that can react to random noise
events that occur during a system utterance by repeat-
ing and modifying the last sub-utterance chunk. In
an evaluation, we found that this system is in general
more reactive than a non-incremental variant and that
humans rate its behaviour to be more natural than
two non-incremental and non-responsive systems.
2 Related Work
Psycholinguistic research has identified incremen-
tality as an important property of human language
production early on and it has been incorporated into
several models (e. g., Kempen and Hoenkamp, 1987;
295
Levelt, 1989). Guhe (2007) presents a computational
model of incremental conceptualisation. However,
work on iNLG itself is rare, partly because NLG re-
search focusses on text (instead of spoken language).
Notable exceptions are the in-depth analysis of
requirements for and properties of incremental gen-
eration by Kilger and Finkler (1995), who describe
the LTAG-based incremental syntactic generator VM-
GEN. It takes incremental input, processes it and pro-
duces output as soon as at least a prefix of the final
sentence is syntactically complete. If VM-GEN no-
tices that it committed itself to a prefix too early, it
can initiate an overt repair. More recently, Skantze
and Hjalmarsson (2010) presented a system that per-
forms incremental generation in the context of a spo-
ken dialogue system. It can already start to produce
output when the user has not yet finished speaking
and only a preliminary interpretation exists. By flexi-
bly changing what to say and by being able to make
self-repairs the system can recover from situations
where it selected and committed on an inadequate
speech plan. Both systems, however, are not able
to flexibly adapt the language that they generate to
changing requirements due to changes in the situation
or changing needs on the side of the user.
Real-time on-the-fly control of speech synthesis
is rare, especially the full integration into a dialogue
system. Matsuyama et al (2010) describe a system
that feeds back to the dialogue system the word at
which it has been interrupted by a barge-in. Edlund
(2008) additionally enables a system to continue at
the point where it was interrupted. He also outlines
some requirements for incremental speech synthe-
sis: to give constant feedback about what has been
delivered, to be interruptible (and possibly continue
from that position), and to run in real time. Edlund?s
system, which uses diphone synthesis, performed
non-incrementally before delivery starts. We go be-
yond this in also enabling changes during delivery
and conducting synthesis steps just-in-time.
Dutoit et al (2011) present an incremental HMM
optimiser which allows to change pitch and tempo
of upcoming phonemes. However, as that system is
fed from a (non-incrementally produced) label file, it
cannot easily be used in an incremental system.
A predecessor of our iSS component (which was
not yet fully incremental on the HMM level) is de-
scribed in detail in (Baumann and Schlangen, 2012a).
3 Incremental and Adaptive NLG
3.1 The SPUD microplanning framework
The NLG component presented here is based on
the SPUD microplanning framework (Stone et al,
2003) and realised in DeVault?s (2008) implemen-
tation ?Java SPUD?. SPUD frames microplannig as
a constraint satisfaction problem, solving the tasks
that are involved in generating a sentence (lexical
and syntactic choice, referring expression generation
and aggregation) in an integrated manner. Genera-
tion starts from a communicative goal that specifies
constraints for the final utterance. The generation pro-
cess is further shaped by (a) general constraints that
model pragmatic properties of language use such as
the Gricean maxims (a principle called ?textual econ-
omy?); (b) specific constraints imposed through the
communicative status of the propositions to be com-
municated (i. e., what knowledge can be presupposed
and what needs to be communicated explicitly); and
(c) linguistic resources (a context-free tree rewriting
formalism based on LTAG; Stone, 2002).
To deal efficiently with the infinite search space
spanned by the linguistic resources, SPUD uses a
heuristic search algorithm to find an utterance that
satisfies the imposed constraints (Stone et al, [2003]
describe the heuristic function). In each search step,
the algorithm expands the ?provisional? utterance by
adding the linguistic resource that maximally reduces
the estimated distance to the final utterance.
If the generation process runs into a dead-end state,
it could in principle deal with the situation by track-
ing back and expanding a different branch. This,
however, is impractical, as it becomes impossible
to project when ? if at all ? generation will finish.
Hence, in that case, SPUD stops without providing a
result, delegating the problem back to the preceding
component in the generation pipeline.
3.2 Partially incremental generation
SPUD generates utterances incrementally in the sense
that the completeness of the provisional utterance
increases monotonically with every step. This, how-
ever, does not mean that the surface structure of pro-
visional utterances is constructed incrementally (i. e.,
from left to right) as well, which would only be pos-
sible, if (a) the search strategy would always expand
the leftmost non-lexicalised node in the provisional
296
Utterance IC
1
IC
2
IC
n
 ?
Utterance
outline
IMPT
1
IMPT
2
IMPT
n
 ?
  MCP
? {U
1
, ?}
? KB
1
? {U
i
, ?}
? KB
2
? {U
k
, ?}
? KB
n
  MPP
 ?state
t
Figure 1: Incremental microplanning consists of two pro-
cesses, micro content planning (MCP) and microplanning-
proper (MPP). The former provides incremental microplan-
ning tasks from an utterance outline to the latter, which
incrementally transforms them into communicative intent
and intonation unit-sized chunks of natural language.
utterance first and if (b) the linguistic resources are
specified (and ordered) in a way that allows left-to-
right expansion of the trees in all possible situations.
In practice, both requirements are difficult to meet
and full word-by-word incrementality in natural lan-
guage microplanning is not within reach in the SPUD
framework. Because of this, we take a slightly more
coarse grained approach to incremental microplan-
ning and choose chunks of the size of intonation
phrases instead of words as our incremental units.
We say that our microplanner does ?partially incre-
mental generation?.
Our incremental microplanner comprises two inter-
acting processes, micro content planning and micro-
planning-proper (MCP and MPP; schematised in Fig-
ure 1), each of which fulfils a distinct task and oper-
ates on different structures.
MCP takes as input utterance outlines that describe
the communicative goal (a set of desired updates Ux)
intended to be communicated in an utterance and the
presuppositions and private knowledge needed to do
so. Importantly, utterance outlines specify how the
communicative goal can be decomposed into an or-
dered list of incremental microplanning-tasks IMPTx.
Each such task comprises (a) a subset of the commu-
nicative goal?s desired updates that belong together
and fit into one intonation unit sized chunk of speech
and (b) knowledge KBx used in generation.
MPP takes one incremental microplanning-task at
a time and uses SPUD to generate the IMPT?s commu-
nicative intent as well as its linguistic surface form
ICx. The communiciative intent is added to a repre-
sentation (?state? in Figure 1) that is shared between
the two processes. While processing the IMPTs of
an utterance outline, MCP can access this representa-
tion, which holds information about all the desired
updates that were achieved before, and thus knows
that a desired update that is shared between subse-
quent IMPTs has already been communicated. MPP
can also take this information into account during
generation. This makes it possible that an utterance
is coherent and adheres to pragmatic principles even
though generation can only take local decisions.
3.3 Adaptive generation
Being able to generate language in sub-utterance
chunks makes it possible to dynamically adapt later
increments of an utterance to changes in the situa-
tion that occur while the utterance is being realised.
Decisions about these adaptations need not be taken
almost until the preceding increment finishes, mak-
ing the generation process very responsive. This is
important to be able to deal with interactive dialogue
phenomena, such as communicative feedback of the
interlocutor (Allwood et al, 1992) or compound con-
tributions (Howes et al, 2011), in a timely manner.
Adaptation may happen in both parts of incremen-
tal microplanning. In MCP, adaptation takes the form
of dynamically changing the choice of which IMPT to
generate next or changing the internal structure of an
IMPT; adaptation in MPP changes the choices the gen-
eration process makes while transforming IMPTs into
communicative intent and surface form. Adaptation
in MCP is triggered top-down, by higher-level pro-
cesses such as dialogue management. Adaptation in
MPP on the other hand depends on the task given and
on the status of the knowledge used during generation.
The details are then governed by global parameter
settings MPP uses during generation.
If there is, for example, reason for the system to
believe that the current increment was not commu-
nicated clearly because of noise in the transmission
channel, the MCP process might delay future IMPTs
and initiate a repair of the current one by re-inserting
it at the beginning of the list of upcoming IMPTs of
this utterance outline. The MPP process? next task
is then to re-generate the same IMPT again. Due to
297
Table 1: Surface forms generated from the same IMPT (de-
sired updates = {hasSubject(event6, ?Vorlesung
Linguistik?)}; KB = {event6}) but with different
levels of verbosity.
Verbosity Generated sub-utterance chunk
0 ?Vorlesung Linguistik?
(lecture Linguistcs)
1 ?Betreff: Vorlesung Linguistik?
(subject: lecture Linguistics)
2 ?mit dem Betreff Vorlesung Linguistik?
(with the subject: lecture Linguistics)
changes in the state information and situation that
influence microplanning, the resulting communica-
tive intent and surface form might then differ from
its previous result.
3.4 Adaptation mechanisms
As a proof of concept, we integrated several adapta-
tion mechanism into our NLG-microplanning system.
The goal of these mechanisms is to respond to a dia-
logue partner?s changing abilities to perceive and/or
understand the information the system wants to con-
vey. Some of the mechanisms operate on the level of
MCP, others on the level of MPP. The mechanisms are
implemented either with the knowledge and its con-
versational status used in generation or by altering
the decision structure of SPUD?s search algorithm?s
heuristic function. Similar to the approach of flexi-
ble NLG described by Walker et al (2007), most of
the mechanism are conditioned upon individual flags,
that in our case depend on a numeric value that repre-
sents the level of understanding the system attributes
to the user. Here we describe the two most relevant
mechanisms used to adapt verbosity and redundancy.
Verbosity The first mechanism aims at influenc-
ing the length of a sub-utterance chunk by making
it either more or less verbose. The idea is that actual
language use of human speakers seldom adheres to
the idealised principle of textual economy. This is
not only the case for reasons of cognitive constraints
during speech production, but also because words
and phrases that do not contribute much to an utter-
ance?s semantics can serve a function, for example by
drawing attention to specific aspects of an utterance
or by giving the listener time to process.
To be able to vary utterance verbosity, we anno-
tated the linguistic resources in our system with val-
ues of their verbosity (these are hand-crafted similar
to the rule?s annotation with production costs). Dur-
ing generation in MPP the values of all linguistic re-
sources used in a (provisional) utterance are added up
and used as one factor in SPUD?s heuristic function.
When comparing two provisional utterances that only
deviate in their verbosity value, the one that is nearer
to a requested verbosity level is chosen. Depend-
ing on this level, more or less verbose constructions
are chosen and it is decided whether sub-utterance
chunks are enriched with additional words. Table 1
shows the sub-utterance chunk ?Betreff: Vorlesung
Linguistik? (subject: lecture Linguistics) generated
with different levels of verbosity.
Redundancy The second adaptation mechanism is
redundancy. Again, redundancy is something that an
ideal utterance does not contain and by design SPUD
penalises the use of redundancy in its heuristic func-
tion. Two provisional utterances being equal, the one
exhibiting less redundancy is normally preferred. But
similar to verbosity, redundancy serves communica-
tive functions in actual language use. It can highlight
important information, it can increase the probability
of the message being understood (Reiter and Sripada,
2002) and it is often used to repair misunderstanding
(Baker et al, 2008).
In incremental microplanning, redundant informa-
tion can be present both within one sub-utterance
chunk (e. g., ?tomorrow, March 26, . . . ? vs. ?tomorrow
. . . ?) or across IMPTs. For the former case, we modi-
fied SPUD?s search heuristic in order to conditionally
either prefer an utterance that contains redundant in-
formation or an utterance that only contains what is
absolutely necessary. In the latter case, redundancy
only becomes an option when later IMPTs enable the
choice of repeating information previously conveyed
and therefore already established as shared knowl-
edge. This is controlled via the internal structure of
an IMPT and thus decided on the level of MCP.
4 Incremental Speech Synthesis
In this section we describe our component for incre-
mental speech synthesis. We extend Edlund?s (2008)
requirements specification cited in Section 2, requir-
ing additionally that an iSS supports changes to as-yet
298
unspoken parts of an ongoing utterance.
We believe that the iSS?s requirements of inter-
ruptability, changeability, responsiveness, and feed-
back are best resolved by a processing paradigm in
which processing takes place just-in-time, i. e., tak-
ing processing steps as late as possible such as to
avoid re-processing if assumptions change. Before
we describe these ideas in detail, we give a short
background on speech synthesis in general.
4.1 Background on speech synthesis
Text-to-speech (TTS) synthesis functions in a top-
down processing approach, starting on the utterance
level and descending onto words and phonemes, in
order to make good decisions (Taylor, 2009). For
example, top-down modelling is necessary to assign
stress patterns and sentence-level intonation which
ultimately lead to pitch and duration contours, and to
model co-articulation effects.
TTS systems start out assigning intonation patterns
to the utterance?s words and then generate a target
phoneme sequence which is annotated with the tar-
gets? durations and pitch contour; all of this is called
the linguistic pre-processing step. The synthesis step
proper can be executed in one of several ways with
HMM-based and unit-selection synthesis currently
producing the perceptually best results.
In HMM-based synthesis, the target sequence is
first turned into a sequence of HMM states. A global
optimisation then determines a stream of vocoding
features that optimise both HMM emission probabili-
ties and continuity constraints (Tokuda et al, 2000).
The stream may also be enhanced to consider global
variance of features (Toda and Tokuda, 2007). The
parameter frames are then fed to a vocoder which
generates the final speech audio signal.
Unit-selection, in contrast, searches for the best
sequence of (variably sized) units of speech in a
large, annotated corpus, aiming to find a sequence
that closely matches the target sequence while having
few and if possible smooth joints between units.
We follow the HMM-based approach for our com-
ponent for the following reasons: (a) even though
only global optimisation is optimal for both tech-
niques, the influence of look-ahead on the continuity
constraints of HMM-based synthesis is linear leading
to a linear loss in optimality with smaller look-aheads
(whereas unit-selection with limited look-ahead may
Figure 2: Hierarchical structure of incremental units de-
scribing an example utterance as it is being produced
during delivery.
jump erratically between completely different unit se-
quences). (b) HMM-based synthesis nicely separates
the production of vocoding parameter frames from
the production of the speech audio signal which al-
lows for fine-grained concurrent processing (see next
subsection). (c) Parameters in the vocoding frames
are partially independent. This allows us to indepen-
dently manipulate, e. g., pitch without altering other
parameters or deteriorating speech quality (in unit-
selection, a completely different unit sequence might
become optimal even for slight changes of pitch).
4.2 Incrementalising speech synthesis
As explained in the previous subsection, speech syn-
thesis is performed top-down, starting at the utterance
and progressing down to the word, target and finally,
in the HMM approach, vocoding parameter and signal
processing levels. It is, however, not necessary that
all details at one level of processing are worked out
before starting to process at the next lower level. To
be precise, some syntactic structure is sufficient to
produce sentence-level intonation, but all words need
not be known. Likewise, post-lexical phonological
processes can be computed as long as a local context
of one word is available and vocoding parameter com-
putation (which must model co-articulation effects)
should in turn be satisfied with about one phoneme of
context. Vocoding itself does not need any lookahead
at all (aside from audio buffering considerations).
Thus, we generate our data structures incremen-
tally in a top-down and left-to-right fashion with dif-
ferent amounts of pre-planning and we do this using
several processing modules that work concurrently.
This results in a ?triangular? structure as shown in
299
Figure 2. At the top stands a pragmatic plan for the
full utterance from which a syntactic plan can be de-
vised. This plan is filled with words, as they become
available. On the vocoding parameter level, only a
few frames into the future have been computed so
far ? even though much more context is already avail-
able. That is, we generate structure just-in-time, only
shortly before it is needed by the next processor. This
holds very similarly for the vocoding step that pro-
duces the speech signal just-in-time.
The just-in-time processing approach, combined
with the increasing temporal granularity of units to-
wards the lower levels has several advantages: (a) lit-
tle utterance-initial processing (only what is neces-
sary to produce the beginning of the signal) allows for
very responsive systems; and (b) changes to the ini-
tial plan result only in a modest processing overhead
because little structure has to be re-computed.
4.3 Technical overview
As a basis, we use MaryTTS (Schr?der and Trouvain,
2003), but replace Mary?s internal data structures
and processing strategies with structures from our
incremental SDS architecture, the INPROTK toolkit
(Schlangen et al, 2010; Baumann and Schlangen,
2012b), which implements the IU model for incre-
mental dialogue processing (Schlangen and Skantze,
2009). The model conceptualises ? and the toolkit
implements ? incremental processing as the process-
ing of incremental units (IUs), which are the smallest
?chunks? of information at a specific level (the boxes
in Figure 2). IUs are interconnected to form a network
(e. g., words keep links to their associated phonemes
and neighbouring words and vice-versa) which repre-
sents the system?s information state.
The component is fed with chunk IUs which con-
tain some words to be synthesised (on their own or
appended to an ongoing utterance). For simplicity,
all units below the chunk level are currently gener-
ated using Mary?s (non-incremental) linguistic pre-
processing capabilities to obtain the target phoneme
sequence. For continuations, the preceding parts of
the utterance are taken into account when generating
prosodic characteristics for the new chunk. Also, our
component is able to revoke and exchange chunks
(or unspoken parts thereof) to change what is to be
spoken; this capability however is not used in the
example system presented in Section 5.
The lowest level module of our component is what
may be called a crawling vocoder, which actively
moves along the phoneme IU layer and executes two
processing steps: (a) for each phoneme it generates
the sequence of HMM parameter frames using a local
optimisation technique (using up to four neighbour-
ing phonemes as context) similar to the one described
by Dutoit et al (2011); and (b) vocoding the HMM
parameters into an audio stream which contains the
actual speech signal.
IUs have a ?progress? field which is set by the
crawling vocoder to one of ?upcoming?, ?ongoing?,
or ?completed?, as applicable. IUs provide a generic
update mechanism to support notification about
progress changes in delivery. The next section de-
scribes how this is used to drive the system.
5 Integrating iNLG and iSS for Adaptive
Information Presentation
Integrating incremental microplanning with incre-
mental speech synthesis in one incremental output
generation architecture allows us to test and explore
how their capabilities act in a coordinated way. As a
first example, we implemented a system that presents
information about events in an appointment database
(e. g., new, conflicting or rescheduled appointments)
and is able to cope with external noise burst events,
as they might for example occur on a bad telephone
line or when using a dialogue system next to a busy
street. The focus is on the incremental capabilities of
the system which enable its adaptive behaviour.
5.1 Component interplay
iNLG and iSS are implemented as IU modules in the
INPROTK architecture. The control flow of the sys-
tem (Figure 3) is managed without special coupling
between the modules, relying only on the left-to-right
processing capabilities of INPROTK combined with
its generic IU update mechanism for transporting
feedback from iSS to iNLG. Both modules can be
(and have been) combined with other IU modules.
To communicate an appointment event, the iNLG
module starts by generating two initial chunk IUs,
the first to be expressed immediately, the second as
additional prosodic context (chunk lengths differ with
an average of about 4 words). The iNLG registers as a
?progress listener? on each chunkIU, which registers
300
Figure 3: Information flow (dashed lines) between iNLG
and iSS components (rounded boxes) and incremental
units (rectangular boxes). The vocoder crawls along with
time and triggers the updates.
as a progress listener on a phonemeIUnear its end.
Shortly before iSS finishes speaking the chunk, iNLG
is thus informed and can generate and send the next
chunk to iSS just-in-time.
If adaptation to noise is needed, iNLG re-generates
and re-sends the previous chunk, taking altered pa-
rameters into account. Again, a subsequent chunk
is immediately pre-generated for additional prosodic
context. This way of generating sub-utterance chunks
ensures that there is always one chunk lookahead to
allow the iSS module to compute an adequate in-
tonation for the current chunk, while maintaining
the single chunk as increment size for the system
and minimising redundant work on the side of iNLG
(this lookahead is not required for iSS; but if it is un-
available, sub-utterance chunks may be inadequately
connected prosodically).
5.2 Responding to a noise event
A third module, the noise detector connects to both
iSS and iNLG. On noise onset, it informs iSS to inter-
rupt the ongoing utterance after the current word (this
works by breaking the links between words so that
the crawling vocoder finishes after the currently ongo-
ing word). Once a noise burst ends, iNLG is informed,
re-generates the interrupted sub-utterance chunk with
the verbosity level decreased by one and the assumed
understanding value increased by one (this degree
of adaptation results in a noticeable difference, it is,
however, not based on empirical study). The values
are then reset, the following chunk is generated and
both chunks are sent to iSS.
It should be noted, that we have not implemented
a real noise source and noise detector. Instead, our
random noise simulator generates bursts of noise of
1000 ms after a random time interval (between 2 and
Table 2: Processing time per processing step before deliv-
ery can begin (in ms; averaged over nine stimuli taking the
median of three runs for each stimulus; calculated from
log messages; code paths preheated for optimisation).
non-incr. incr.
NLG-microplanning 361 52
Synthesis (ling. pre-processing) 217 4472
Synthesis (HMM and vocoding) 1004 21
total response time 1582 519
5 seconds) and directly informs the system 300 ms
after noise starts and ends. We think it is reasonable
to assume that a real noise detector should be able to
give accurate information with a similar delay.
6 Evaluation
6.1 Quantitative evaluation
One important argument in favour of incremental
processing is the possibility of speeding up system
response time, which for non-incremental systems
is the sum of the times taken by all processors to
do their work. An incremental system, in contrast,
can fold large amounts of its processing time into the
ongoing speech output; what matters is the sum of
the onset times of each processor, i. e., the time until
a first output becomes available to the next processor.
Table 2 summarises the runtime for the three major
steps in output production of our system using nine
utterances from our domain. Both NLG and speech
synthesis? onset times are greatly reduced in the in-
cremental system.2 Combined, they reduce system
response time by more than a second. This is mostly
due to the almost complete folding of HMM opti-
misation and vocoding times into the spoken utter-
ance. NLG profits from the fact that at the beginning
of an utterance only two chunks have to be gener-
ated (instead of an average of 6.5 chunks in the non-
incremental system) and that the first chunk is often
very simple.
6.2 Subjective evaluation
To further test whether the system?s behaviour in
noisy situations resembles that of a human speaker
2The iSS component by mistake takes the symbolic pre-
processing step twice. Unfortunately, we found this bug only
after creating the stimuli used in the subjective evaluation.
301
in a similar situation, we let humans rate utterances
produced by the fully incremental, adaptive system
and utterances produced by two non-incremental
and less responsive variants (we have not used non-
incremental TTS in combination with iNLG as another
possible base-line as pretests showed this to sound
very unnatural due to the missing prosodic linkage be-
tween phrases). The participants were to rate whether
they agree to the statement ?I found the behaviour of
the system in this situation as I would expect it from
a human speaker? on a 7-point Likert-scale.
In condition A, full utterances were generated non-
incrementally, synthesised non-incrementally and
played without responding to noise-interruptions in
the channel (as if the system did not notice them).
Utterances in condition B were generated and synthe-
sised as in condition A, but playback responded to the
noisy channel, stopping when the noise was noticed
and continuing when noise ended. For condition C,
utterances were generated with the fully incremental
and adaptive system described in Section 5. Upon
noise detection, speech synthesis is interrupted and,
when the noise ends, iNLG will re-generate the in-
terrupted sub-utterance chunk ? using the adaptation
strategy outlined in Section 5.2. This then triggers
iSS into action and shortly after, the system contin-
ues speaking. Nine system runs, each producing a
different utterance from the calendar domain, were
recorded in each of the three conditions, resulting in
a total of 27 stimuli.
Before the actual stimuli were presented, partici-
pants listened to two example stimuli without noise
interruptions to get an impression of how an aver-
age utterance produced by the system sounds. After
the presentation of these two examples, the 27 stim-
uli were presented in the same random order. Par-
ticipants listened once to each stimulus and rated it
immediately after every presentation.
Twelve PhD-students (3 female, 9 male; mean age
30.5 years; 11 with German as one of their first lan-
guages; none with uncorrected hearing impairment)
from Bielefeld University participated in our study
and listened to and rated the 27 stimuli.
A Friedman rank sum test revealed a highly sig-
nificant difference between the perceived human-
likeness of the three systems (?2 = 151, p < .0001).
Median values of stimulus ratings in the conditions
A, B and C were 2, 2 and 6 respectively, indicat-
ing that the fully incremental system was rated con-
siderably more human-like. This was also shown
through a post-hoc analysis with Wilcoxon signed
rank tests which found no significant difference be-
tween condition A and B (V = 1191.5, p = .91)3.
Conditions A and C, however, differed highly signifi-
cantly (V = 82, p < .0001), as did conditions B and
C (V = 22.5, p < .0001) ? even after applying a Bon-
ferroni correction to correct for a possible cumulation
of ?-errors.
7 Conclusion
We have presented what is ? to the best of our knowl-
edge ? the first integrated component for incremental
NLG and speech synthesis and demonstrated the flex-
ibility that an incremental approach to output gener-
ation for speech systems offers by implementing a
system that can repair understanding problems.
From the evaluation we can conclude that incre-
mental output generation (both iNLG and iSS in iso-
lation or combined) is able to greatly speed up sys-
tem response time and can be used as a means to
speed up system response even in an otherwise non-
incremental system. Furthermore, we showed that the
behaviour of our fully incremental and adaptive sys-
tem was perceived as significantly more human-like
than the non-incremental and the non-incremental
but responsive baseline systems.
The understanding problem that our demonstra-
tor system tackled was of the simplest kind, namely
acoustic non-understanding, objectively detectable
as the presence of noise. In principle, however, the
same mechanisms of stopping and rephrasing can be
used to tackle more subjective understanding prob-
lems as can be signalled by linguistic feedback. Our
incremental output generation component gives us an
ideal basis to explore such problems in future work.
Acknowledgements This research is partially sup-
ported by the Deutsche Forschungsgemeinschaft
(DFG) in the Center of Excellence in ?Cognitive Inter-
action Technology? (CITEC) and through an Emmy
Noether Fellowship to the last author.
3This suggests that it does not matter whether a system re-
sponds to problems in the communication channel by waiting or
totally ignores these problems. Notice, however, that we did not
test recall of the calendar events. In that case, condition B should
outperform A, as some information was clearly inaudible in A.
302
References
Jens Allwood, Joakim Nivre, and Elisabeth Ahls?n. 1992.
On the semantics and pragmatics of linguistic feedback.
Journal of Semantics, 9:1?26.
Rachel Baker, Alastair Gill, and Justine Cassell. 2008.
Reactive redundancy and listener comprehension in
direction-giving. In Proceedings of the 9th SIGdial
Workshop on Discourse and Dialogue, pages 37?45,
Columbus, OH.
Timo Baumann and David Schlangen. 2012a. INPRO_iSS:
A component for just-in-time incremental speech syn-
thesis. In Proceedings of ACL System Demonstrations,
Jeju, South Korea.
Timo Baumann and David Schlangen. 2012b. The
INPROTK 2012 release. In Proceedings of the NAACL-
HLT Workshop on Future directions and needs in the
Spoken Dialog Community: Tools and Data, pages 29?
32, Montr?al, Canada.
Hendrik Buschmeier and Stefan Kopp. 2011. Towards
conversational agents that attend to and adapt to com-
municative user feedback. In Proceedings of the 11th
International Conference on Intelligent Virtual Agents,
pages 169?182, Reykjavik, Iceland.
David DeVault. 2008. Contribution Tracking: Partici-
pating in Task-oriented Dialogue Under Uncertainty.
Ph.D. thesis, Rutgers, The State University of New Jer-
sey, New Brunswick, NJ.
Thierry Dutoit, Maria Astrinaki, Onur Babacan, Nicolas
d?Alessandro, and Benjamin Picart. 2011. pHTS for
Max/MSP: A streaming architecture for statistical para-
metric speech synthesis. Technical Report 1, numediart
Research Program on Digital Art Technologies, Mons,
Belgium.
Jens Edlund. 2008. Incremental speech synthesis. In
Second Swedish Language Technology Conference,
pages 53?54, Stockholm, Sweden, November. System
Demonstration.
Markus Guhe. 2007. Incremental Conceptualization for
Language Production. Lawrence Erlbaum, Mahwah,
NJ.
Christine Howes, Matthew Purver, Patrick G. T. Healey,
Gregory Mills, and Eleni Gregoromichelaki. 2011. On
incrementality in dialogue: Evidence from compound
contributions. Discourse & Dialogue, 2:279?311.
Gerard Kempen and Edward Hoenkamp. 1987. An incre-
mental procedural grammar for sentence formulation.
Cognitive Science, 11:201?258.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical
Report RR-95-11, Deutsches Forschungszentrum f?r
K?nstliche Intelligenz, Saarbr?cken, Germany.
Willem J. M. Levelt. 1989. Speaking: From Intention to
Articulation. The MIT Press, Cambridge, UK.
Kyoko Matsuyama, Kazunori Komatani, Ryu Takeda,
Toru Takahashi, Tetsuya Ogata, and Hiroshi G. Okuno.
2010. Analyzing user utterances in barge-in-able spo-
ken dialogue system for improving identification accu-
racy. In Proceedings of INTERSPEECH 2010, pages
3050?3053, Makuhari, Japan.
Ehud Reiter and Somayajulu Sripada. 2002. Human vari-
ation and lexical choice. Computational Linguistics,
28:545?553.
David Schlangen and Gabriel Skantze. 2009. A general,
abstract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 710?718, Athens, Greece.
David Schlangen, Timo Baumann, Hendrik Buschmeier,
Okko Bu?, Stefan Kopp, Gabriel Skantze, and Ramin
Yaghoubzadeh. 2010. Middleware for incremental
processing in conversational agents. In Proceedings of
SIGdial 2010: the 11th Annual Meeting of the Special
Interest Group in Discourse and Dialogue, pages 51?
54, Tokyo, Japan.
Marc Schr?der and J?rgen Trouvain. 2003. The Ger-
man text-to-speech synthesis system MARY: A tool
for research, development and teaching. International
Journal of Speech Technology, 6:365?377.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
incremental speech generation in dialogue systems. In
Proceedings of SIGDIAL 2010: the 11th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 1?8, Tokyo, Japan.
Matthew Stone, Christine Doran, Bonnie Webber, Tonia
Bleam, and Martha Palmer. 2003. Microplanning with
communicative intentions: The SPUD system. Compu-
tational Intelligence, 19:311?381.
Matthew Stone. 2002. Lexicalized grammar 101. In
Proceedings of the ACL-02 Workshop on Effective Tools
and Methodologies for Teaching Natural Language
Processing and Computational Linguistics, pages 77?
84, Philadelphia, PA.
Paul Taylor. 2009. Text-to-Speech Synthesis. Cambridge
Univ Press, Cambridge, UK.
Tomoki Toda and Keiichi Tokuda. 2007. A speech param-
eter generation algorithm considering global variance
for HMM-based speech synthesis. IEICE TRANSAC-
TIONS on Information and Systems, 90:816?824.
Keiichi Tokuda, Takayoshi Yoshimura, Takashi Masuko,
Takao Kobayashi, and Tadashi Kitamura. 2000.
Speech parameter generation algorithms for HMM-
based speech synthesis. In Proceedings of ICASSP
2000, pages 1315?1318, Istanbul, Turkey.
Marylin Walker, Amanda Stent, Fran?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research, 30:413?456.
303
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 68?72,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Situationally Aware In-Car Information Presentation
Using Incremental Speech Generation: Safer, and More Effective
Spyros Kousidis
1
, Casey Kennington
1,2
, Timo Baumann
4
, Hendrik Buschmeier
2,3
,
Stefan Kopp
2,3
, and David Schlangen
1
1
Dialogue Systems Group,
2
CITEC,
3
Sociable Agents Group ? Bielefeld University
4
Department of Informatics, Natural Language Systems Division ? University of Hamburg
spyros.kousidis@uni-bielefeld.de
Abstract
Holding non-co-located conversations
while driving is dangerous (Horrey and
Wickens, 2006; Strayer et al., 2006),
much more so than conversations with
physically present, ?situated? interlocutors
(Drews et al., 2004). In-car dialogue
systems typically resemble non-co-located
conversations more, and share their
negative impact (Strayer et al., 2013). We
implemented and tested a simple strategy
for making in-car dialogue systems aware
of the driving situation, by giving them
the capability to interrupt themselves
when a dangerous situation is detected,
and resume when over. We show that this
improves both driving performance and
recall of system-presented information,
compared to a non-adaptive strategy.
1 Introduction
Imagine you are driving on a relatively free high-
way at a constant speed and you are talking with the
person next to you. Suddenly, you need to overtake
another car. This requires more attention from you;
you check the mirrors before you change lanes, and
again before you change back. Plausibly, an attent-
ive passenger would have noticed your attention
being focused more on the driving, and reacted to
this by interrupting their conversational contribu-
tion, resuming when back on the original lane.
Using a driving simulation setup, we implemen-
ted a dialogue system that realises this strategy. By
employing incremental output generation, the sys-
tem can interrupt and flexibly resume its output.
We tested the system using a variation of a stand-
ard driving task, and found that it improved both
driving performance and recall, as compared to a
non-adaptive baseline system.
Figure 1: Overview of our system setup: human
controls actions of a virtual car; events are sent to
DM, which controls the speech output.
2 The Setup
2.1 The Situated In-Car System
Figure 1 shows an overview of our system setup,
with its main components: a) the driving simulator
that presents via computer graphics the driving task
to the user; b) the dialogue system, that presents,
via voice output, information to the user (here, cal-
endar entries).
Driving Simulation For the driving simulator,
we used the OpenDS Toolkit,
1
connected to a steer-
ing wheel and a board with an acceleration and
brake pedal, using standard video game hardware.
We developed our own simple driving scenarios
(derived from the ?ReactionTest? task, which is dis-
tributed together with OpenDS) that specified the
driving task and timing of the concurrent speech,
as described below. We modified OpenDS to pass
real-time data (e.g. car position/velocity/events in
the simulation, such as a gate becoming visible
or a lane change) using the mint.tools architec-
ture (Kousidis et al., 2013). In addition, we have
bridged INPROTK (Baumann and Schlangen, 2012)
with mint.tools via the Robotics Service Bus (RSB,
Wienke and Wrede (2011)) framework.
1http://www.opends.eu/
68
Figure 2: Driver?s view during experiment. The
green signal on the signal-bridge indicates the tar-
get lane.
Dialogue System Using INPROTK, we imple-
mented a simple dialogue system. The notion of
?dialogue? is used with some liberty here: the user
did not interact directly with the system but rather
indirectly (and non-intentionally) via driving ac-
tions. Nevertheless, we used the same modularisa-
tion as in more typical dialogue systems by using a
dialoge management (DM) component that controls
the system actions based on the user actions. We
integrated OpenDial (Lison, 2012) as the DM into
INPROTK,
2
though we only used it to make simple,
deterministic decisions (there was no learned dia-
logue policy) based on the state of the simulator
(see below). We used the incremental output gen-
eration capabilities of INPROTK, as described in
(Buschmeier et al., 2012).
3 Experiment
We evaluated the adaptation strategy in a driving
simulation setup, where subjects performed a 30
minute, simulated drive along a straight, five-lane
road, during which they were occasionally faced
with two types of additional tasks: a lane-change
task and a memory task, which aim to measure the
driving performance and the driver?s ability to pay
attention to speech while driving, respectively. The
two tasks occured in isolation or simultaneoulsy.
The Lane-Change Task The driving task we
used is a variant of the well-known lane-change
task (LCT), which is standardised in (ISO, 2010):
It requires the driver to react to a green light posi-
tioned on a signal gate above the road (see Figure 2).
The driver (otherwise instructed to remain in the
middle lane) must move to the lane indicated by
2
OpenDial can be found at http://opendial.
googlecode.com/.
Table 1: Experiment conditions.
Lane Change Presentation mode Abbreviation
Yes CONTROL CONTROL_LANE
Yes ADAPTIVE ADAPTIVE_LANE
Yes NO_TALK NO_TALK_LANE
No CONTROL CONTROL_EMPTY
the green light, remain there until a tone is sounded,
and then return again to the middle lane. OpenDS
gives a success or fail result to this task depending
on whether the target lane was reached within 10
seconds (if at all) and the car was in the middle lane
when the signal became visible. We also added a
speed constraint: the car maintained 40 km/h when
the pedal was not pressed, with a top speed of 70
km/h when fully pressed. During a Lane-change,
the driver was to maintain a speed of 60 km/h, thus
adding to the cognitive load.
The Memory Task We tested the attention of
the drivers to the generated speech using a simple
true-false memory task. The DM generated utter-
ances such as ?am Samstag den siebzehnten Mai
12 Uhr 15 bis 14 Uhr 15 hast du ?gemeinsam Essen
im Westend mit Martin? ? (on Saturday the 17th
of May from 12:15?14:15 you are meeting Mar-
tin for Lunch). Each utterance had 5 information
tokens: day, time, activity, location and partner,
spoken by a female voice. After utterance comple-
tion, and while no driving distraction occurred, a
confirmation question was asked by a male voice,
e.g. ?Richtig oder Falsch? ? Freitag? (Right or
wrong? ? Friday). The subject was then required
to answer true or false by pressing one of two re-
spective buttons on the steering wheel. The token
of the confirmation question was chosen randomly,
although tokens near the beginning of the utterance
(day and time) were given a higher probability of
occurrence. The starting time of the utterance re-
lative to the gate was varied randomly between 3
and 6 seconds before visibility. Figure 3 gives a
schematic overview of the task and describes the
strategy we implemented for interrupting and re-
suming speech, triggered by the driving situation.
3.1 Conditions
Table 1 shows the 4 experiment conditions, de-
noting if a lane change was signalled, and what
presentation strategy was used. Each condition ap-
peared exactly 11 times in the scenario, for a total
of 44 episodes. The order of episodes was randomly
69
t1
t
2
suc
gate
lane t
3
0
1
2
3
4
am Samstag den siebzehn- den siebzehnten Mai ?
am Samstag den siebzehnten Mai um 12 Uhr hast du ?Besprechung mit Peter?
ADAPTIVE
CONTROL
Figure 3: Top view of driving task: as the car moves to the right over time, speech begins at t
1
, the gate with
the lane-change indicator becomes visible at t
2
, where in the adaptive version speech pauses. Successful
lane change is detected at suc; successful change back to the middle lane is detected at lane, and resumes.
(If no change back is detected, the interruption times out at t
3
). All red-dotted lines denote events sent
from OpenDS to the Dialogue Manager.
generated for each subject. With this design, sub-
jects perceive conditions to be entirely random.
3.2 Dependent Variables
The dependent variables for the Memory task
are (a) whether the subject?s answer was correct
(true/false), and (b) the response delay, which is
the time from the end of the clarification ques-
tion to the time the true or false button was
pressed. For the driving task, the dependent vari-
ables are the OpenDS performance measurements
success/failure (as defined above) and reaction time
(time to reach the target lane).
3.3 Procedure
After signing a consent form, subjects were led into
the experiment room, where seat position and audio
level were adjusted, and were given written instruc-
tions. Next, the OpenDS scenario was initiated. The
scenario started with 10 successive lane-change sig-
nal gates without speech, for driving training. An
experimenter provided feedback during training
while the subjects familiarized themselves with the
driving task. Following the training gates came a
clearly-marked ?START? gate, signifying the be-
ginning of the experiment to the subjects (at this
point, the experimenter left). There was a ?FINISH?
gate at the end of the scenario. The whole stretch of
road was 23 km and took approximately 30 minutes
to complete. After the driving task, the subjects
were given a questionnaire, which asked them to
identify the information presentation strategies and
assign a preference.
Table 2: Subjects?
judgement of task
difficulty.
Diff. Freq.
4 (easy) 8
3 7
2 1
1 (hard) 1
Table 3: Subjects? system
preference.
Preference Freq.
ADAPTIVE 3
CONTROL 9
Neither 5
4 Results
In total, 17 subjects (8 male, 9 female, aged 19-
36) participated in the study. All of the subjects
were native German speakers affiliated with AN-
ONYMIZED University. As reported in the post-test
questionnaire, all held a driving license, two had
previous experience with driving simulators and
only one had previous experience with spoken dia-
logue systems. Table 2 shows the subjects? assess-
ment of difficulty, while Table 3 shows their prefer-
ence between the different strategies. Most subjects
found the task relatively easy and either prefer the
speech not to adapt or have no preference.
Memory task The overall percentages of correct
answers to the system?s recall questions (across all
subjects) are shown in Table 4. We see that the sub-
jects? performance in this task is considerably bet-
ter when the system adapts to the driving situation
(ADAPTIVE_LANE condition) rather than speaking
through the lane change (CONTROL_LANE con-
dition). In fact, the performance in the ADAPT-
IVE_LANE condition is closer to the control upper
70
Table 4: Performance in memory task per condi-
tion.
Condition Percentage
CONTROL_EMPTY 169/180 (93.9%)
ADAPTIVE_LANE 156/172 (90.7%)
CONTROL_LANE 150/178 (84.3%)
Table 5: Success in driving task per condition (as
reported by OpenDS).
Condition Success
NOTALK_LANE 175/185 (94.6%)
ADAPTIVE_LANE 165/174 (94.8%)
CONTROL_LANE 165/180 (91.7%)
bound (CONTROL_EMPTY condition). We tested
significance of the results using a generalized lin-
ear mixed model with CONDITION and SUBJECT
as factors, which yields a p-value of 0.027 when
compared against a null model in which only SUB-
JECT is a factor. No significant effects of between-
subjects factors gender, difficulty or preference
were found. In addition, the within-subject variable
time did not have any significant effect (subjects do
not improve in the memory task with time).
The average response delay (from the end of
the recall question to the button press) per condi-
tion across all subjects is shown in Figure 4. Sub-
jects reply slower to the recall questions in the
CONTROL_LANE condition, while their perform-
ance in the ADAPTIVE_LANE condition is indis-
tinguishable from the CONTROL_EMPTY condi-
tion (in which there is no distraction). Addition-
ally, there is a general decreasing trend of response
delay with time, which means that users get ac-
quainted with the task (type of information, format
of question) over time. Both factors (condition
and time) are significant (repeated measures AN-
OVA, 2x2 factorial design, F
condition
= 3.858, p =
0.0359,F
time
= 4.672, p= 0.00662). No significant
effects were found for any of the between-subject
factors (gender, difficulty, preference).
Driving task The success rate in the lane-change
task per condition is shown in Table 5. Here too
we find that the performance is lower in the CON-
TROL_LANE condition, while ADAPTIVE_LANE
does not seem to affect driving performance, when
compared to the NOTALK_LANE condition. The
effect is significant (p = 0.01231) using the same
GLMM approach and factors as above.
ADAPTIVE_LANE CONTROL_EMPTY CONTROL_LANECondition0
500
1000
1500
2000
2500
3000
3500
4000
User
 Res
pons
e De
lay (
ms)
Figure 4: User answer response delay under three
conditions.
5 Discussion, Conclusions, Future Work
We have developed and tested a driving simula-
tion scenario where information is presented by a
spoken dialogue system. Our system has the unique
ability (compared to today?s commercial systems)
to adapt its speech to the driving situation: it in-
terrupts itself when a dangerous situation occurs
and later resumes with an appropriate continuation.
Using this strategy, information presentation had
no impact on driving, and dangerous situations no
impact on information recall. In contrast, a system
that blindly spoke while the driver was distracted
by the lane-change task resulted in worse perform-
ance in both tasks: subjects made more errors in
the memory task and also failed more of the lane-
change tasks, which could prove dangerous in a
real situation.
Interestingly, very few of the subjects preferred
the adaptive version of the system in the post-task
questionnaire. Among the reasons that they gave
for this was their inability to control the interrup-
tions/resumptions of the system. We plan to ad-
dress the issue of control by allowing future ver-
sions of our system to accept user signals, such as
speech or head gestures; it will be interesting to see
whether this will impact driving performance or not.
Further, more sophisticated presentation strategies
(e.g., controlling the complexity of the generated
language in accordance to the driving situation) can
be tested in this framework.
Acknowledgments This research was partly sup-
ported by the Deutsche Forschungsgemeinschaft
(DFG) in the CRC 673 ?Alignment in Communic-
71
ation? and the Center of Excellence in ?Cognit-
ive Interaction Technology? (CITEC). The authors
would like to thank Oliver Eckmeier and Michael
Bartholdt for helping implement the system setup,
as well as Gerdis Anderson and Fabian Wohlge-
muth for assisting as experimenters.
References
Timo Baumann and David Schlangen. 2012. The In-
proTK 2012 release. In NAACL-HLT Workshop on
Future directions and needs in the Spoken Dialog
Community: Tools and Data (SDCTD 2012), pages
29?32, Montr?al, Canada.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dosch, Stefan Kopp, and David Schlangen. 2012.
Combining incremental language generation and in-
cremental speech synthesis for adaptive information
presentation. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 295?303, Seoul, South Korea.
Frank A. Drews, Monisha Pasupathi, and David L.
Strayer. 2004. Passenger and cell-phone conver-
sations in simulated driving. In Proceedings of the
48th Annual Meeting of the Human Factors and Er-
gonomics Society, pages 2210?2212, New Orleans,
USA.
William J. Horrey and Christopher D. Wickens. 2006.
Examining the impact of cell phone conversations
on driving using meta-analytic techniques. Human
Factors, 48:196?205.
ISO. 2010. Road vehicles ? Ergonomic aspects of
transport information and control systems ? Simu-
lated lane change test to assess in-vehicle second-
ary task demand. ISO 26022:2010, Geneva, Switzer-
land.
Spyros Kousidis, Thies Pfeiffer, and David Schlangen.
2013. MINT.tools: Tools and adaptors supporting
acquisition, annotation and analysis of multimodal
corpora. In Interspeech 2013, Lyon, France. ISCA.
Pierre Lison. 2012. Probabilistic dialogue models with
prior domain knowledge. In Proceedings of the 13th
Annual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 179?188, Seoul, South
Korea.
David L Strayer, Frank A Drews, and Dennis J Crouch.
2006. A comparison of the cell phone driver and the
drunk driver. Human Factors, 48:381?91.
David L Strayer, Joel M Cooper, Jonna Turrill, James
Coleman, and Nate Medeiros. 2013. Measuring
cognitive distraction in the automobile. Technical
report, AAA Foundation for Traffice Safety.
J Wienke and S Wrede. 2011. A middleware for col-
laborative research in experimental robotics. In Sys-
tem Integration (SII), 2011 IEEE/SICE International
Symposium on, pages 1183?1190.
72
