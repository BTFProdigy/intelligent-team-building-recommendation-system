Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 457?465,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic induction of FrameNet lexical units
Marco Pennacchiotti(?), Diego De Cao(?), Roberto Basili(?), Danilo Croce(?), Michael Roth(?)
(?) Computational Linguistics
Saarland University
Saarbru?cken, Germany
{pennacchiotti,mroth}@coli.uni-sb.de
(?) DISP
University of Roma Tor Vergata
Roma, Italy
{decao,basili,croce}@info.uniroma2.it
Abstract
Most attempts to integrate FrameNet in NLP
systems have so far failed because of its lim-
ited coverage. In this paper, we investigate the
applicability of distributional and WordNet-
based models on the task of lexical unit induc-
tion, i.e. the expansion of FrameNet with new
lexical units. Experimental results show that
our distributional and WordNet-based models
achieve good level of accuracy and coverage,
especially when combined.
1 Introduction
Most inference-based NLP tasks require a large
amount of semantic knowledge at the predicate-
argument level. This type of knowledge allows to
identify meaning-preserving transformations, such
as active/passive, verb alternations and nominal-
izations, which are crucial in several linguistic in-
ferences. Recently, the integration of NLP sys-
tems with manually-built resources at the predi-
cate argument-level, such as FrameNet (Baker et
al., 1998) and PropBank (Palmer et al, 2005) has
received growing interest. For example, Shen and
Lapata (2007) show the potential improvement that
FrameNet can bring on the performance of a Ques-
tion Answering (QA) system. Similarly, several
other studies (e.g. (Bar-Haim et al, 2005; Garoufi,
2007)) indicate that frame semantics plays a central
role in Recognizing Textual Entailment (RTE). Un-
fortunately, most attempts to integrate FrameNet or
similar resources in QA and RTE systems have so
far failed, as reviewed respectively in (Shen and La-
pata, 2007) and (Burchardt and Frank, 2006). These
studies indicate limited coverage as the main reason
of insuccess. Indeed, the FrameNet database only
contains 10,000 lexical units (LUs), far less than
the 210,000 entries in WordNet 3.0. Also, frames
are based on more complex information than word
senses, so that their manual development is much
more demanding (Burchardt et al, 2006; Subirats
and Petruck, 2003).
Therefore, there is nowadays a pressing need to
adopt learning approaches to extend the coverage
of the FrameNet lexicon by automatically acquiring
new LUs, a task we call LU induction, as recently
proposed at SemEval-2007 (Baker et al, 2007). Un-
fortunately, research in this area is still somehow
limited and fragmentary. The aim of our study is
to pioneer in this field by proposing two unsuper-
vised models for LU induction, one based on dis-
tributional techniques and one using WordNet as a
support; and a combined model which mixes the
two. The goal is to investigate to what extent distri-
butional and WordNet-based models can be used to
induce frame semantic knowledge in order to safely
extend FrameNet, thus limiting the high costs of
manual annotation.
In Section 2 we introduce the LU induction task
and present related work. In Sections 3, 4 and 5 we
present our distributional, WordNet-based and com-
bined models. Then, in Section 6 we report experi-
mental results and comparative evaluations. Finally,
in Section 7 we draw final conclusions and outline
future work.
2 Task Definition and Related Work
As defined in (Fillmore, 1985), a frame is a con-
ceptual structure modeling a prototypical situation,
evoked in texts through the occurrence of its lex-
ical units. A lexical unit (LU) is a predicate that
linguistically expresses the situation of the frame.
Lexical units of the same frame share semantic ar-
guments. For example the frame KILLING has lex-
ical units such as assassin, assassinate, blood-bath,
fatal, murderer, kill, suicide that share semantic ar-
guments such as KILLER, INSTRUMENT, CAUSE,
VICTIM. Building on this frame-semantic model,
the Berkeley FrameNet project (Baker et al, 1998)
has been developing a frame-semantic lexicon for
457
the core vocabulary of English since 1997. The
current FrameNet release contains 795 frames and
about 10,000 LUs. Part of FrameNet is also a cor-
pus of 135,000 annotated example sentences from
the British National Corpus (BNC).
LU induction is a fairly new task. Formally,
it can be defined as the task of assigning a
generic lexical unit not yet present in the FrameNet
database (hereafter called unknown LU) to the cor-
rect frame(s). As the number of frames is very
large (about 800) the task is intuitively hard to solve.
A further complexity regards multiple assignments.
Lexical units are sometimes ambiguous and can then
be mapped to more than one frame (for example
the word tea could map both to FOOD and SO-
CIAL EVENT). Also, even unambiguous words can
be assigned to more than one frame ? e.g. child maps
to both KINSHIP and PEOPLE BY AGE.
LU induction is relevant to many NLP tasks, such
as the semi-automatic creation of new FrameNets,
and semantic role labelling. LU induction has been
integrated at SemEval-2007 as part of the Frame Se-
mantic Structure Extraction shared task (Baker et
al., 2007), where systems are requested to assign
the correct frame to a given LU, even when the
LU is not yet present in FrameNet. Johansson and
Nugues (2007) approach the task as a machine learn-
ing problem: a Support Vector Machine trained on
existing LUs is applied to assign unknown LUs to
the correct frame, using features derived from the
WordNet hierarchy. Tested on the FrameNet gold
standard, the method achieves an accuracy of 0.78,
at the cost of a low coverage of 31% (i.e. many LUs
are not assigned). Johansson and Nugues (2007)
also experiment with a simple model based on stan-
dard WordNet similarity measures (Pedersen et al,
2004), achieving lower performance. Burchardt and
colleagues (2005) present Detour, a rule-based sys-
tem using words in a WordNet relation with the un-
known LU to find the correct frame. The system
achieves an accuracy of 0.39 and a coverage of 87%.
Unfortunately this algorithm requires the LU to be
previously disambiguated, either by hand or using
contextual information.
In a departure from previous work, our first model
leverages distributional properties to induce LUs, in-
stead of relying on pre-existing lexical resources as
WordNet. This guarantees two main advantages.
First, it can predict a frame for any unknown LU,
while WordNet based approaches can be applied
only to words having a WordNet entry. Second, it
allows to induce LUs in languages for which Word-
Net is not available or has limited coverage. Our
second WordNet-based model uses sense informa-
tion to characterize the frame membership for un-
known LU, by adopting a semantic similarity mea-
sure which is sensitive to all the known LUs of a
frame.
3 Distributional model
The basic idea behind the distributional approach is
to induce new LUs by modelling existing frames and
unknown LUs in a semantic space, where they are
represented as distributional co-occurrence vectors
computed over a corpus.
Semantic spaces are widely used in NLP for rep-
resenting the meaning of words or other lexical en-
tities. They have been successfully applied in sev-
eral tasks, such as information retrieval (Salton et al,
1975) and harvesting thesauri (Lin, 1998). The intu-
ition is that the meaning of a word can be described
by the set of textual contexts in which it appears
(Distributional Hypothesis (Harris, 1964)), and that
words with similar vectors are semantically related.
In our setting, the goal is to find a semantic space
model able to capture the notion of frame ? i.e. the
property of ?being characteristic of a frame?. In
such a model, an unknown LU is induced by first
computing the similarity between its vector and the
vectors of the existing frames, and then assigning the
LU to the frame with the highest similarity.
3.1 Assigning unknown LUs to frames
In our model, a LU l is represented by a vector ~l
whose dimensions represent the set of contexts C
of the semantic space. The value of each dimen-
sion is given by the co-occurrence value of the LU
with a contextual feature c ? C, computed over a
large corpus using an association measure. We ex-
periment with two different association measures:
normalized frequency and pointwise mutual infor-
mation. We approximate these measures by using
Maximum Likelihood Estimation, as follows:
458
F (l, c) =MLE |l, c||?, ?|
MI(l, c) =MLE |l, c||?, ?||?, c||l, ?|
(1)
where |l, c| denotes the co-occurrence counts
of the pair (l, c) in the corpus, |?, c| =?
l?L |l, c|, |l, ?| =
?
c?C |l, c| and finally |?, ?| =?
l?L,c?C |l, c|.
A frame f is modeled by a vector ~f , representing
the distributional profile of the frame in the seman-
tic space. We here assume that a frame can be fully
described by the set of its lexical units F . We imple-
ment this intuition by computing ~f as the weighted
centroid of the set F , as follows:
~f =
?
l?F
wlf ?~l (2)
where wlf is a weighting factor, accounting for
the relevance of a given lexical unit with respect to
the frame, estimated as:
wlf = |l|?
l?F
|l|
(3)
where |l| denotes the counts of l in the corpus.
From a more cognitive perspective, the vector ~f rep-
resents the prototypical lexical unit of the frame.
Given the set of all framesN and an unknown lex-
ical unit ul, we assign ul to the frame fmaxul which
is distributionally most similar ? i.e. we intuitively
map an unknown lexical unit to the frame whose
prototypical lexical unit ~f has the highest similarity
with ~ul:
fmaxul = argmaxf?N simD(~ul, ~f) (4)
In our model, we used the traditional cosine simi-
larity:
simcos(ul, f) =
~ul ? ~f
|~ul| ? |~f |
(5)
3.2 Choosing the space
Different types of contexts C define spaces with dif-
ferent semantic properties. We are here looking for
a space able to capture the properties which charac-
terise a frame. The most relevant of these properties
is that LUs in the same frame tend to be either co-
occurring or substitutional words (e.g. assassin/kill
or assassinate/kill) ? i.e. they are either in paradig-
matic and syntagmatic relation. In an ideal space,
a high similarity value simD would be then given
both to assassinate/kill and to assassin/kill. We ex-
plore three spaces which seem to capture the above
property well:
Word-based space: Contexts are words appear-
ing in a n-window of the lexical unit. Such spaces
model a generic notion of semantic relatedness.
Two LUs close in the space are likely to be re-
lated by some type of generic semantic relation,
either paradigmatic (e.g. synonymy, hyperonymy,
antonymy) or syntagmatic (e.g. meronymy, concep-
tual and phrasal association).1
Syntax-based space: Contexts are syntactic re-
lations (e.g. X-VSubj-man where X is the LU), as
described in (Pado?, 2007). These spaces are good
at modeling semantic similarity. Two LUs close in
the space are likely to be in a paradigmatic relation,
i.e. to be close in a is-a hierarchy (Budanitsky and
Hirst, 2006; Lin, 1998; Pado?, 2007). Indeed, as con-
texts are syntactic relations, targets with the same
part of speech are much closer than targets of differ-
ent types.
Mixed space: In a combination of the two above
spaces, contexts are words connected to the LU by a
dependency path of at most length n. Unlike word-
based spaces, contexts are selected in a more princi-
pled way: only syntactically related words are con-
texts, while other (possibly noisy) material is filtered
out. Unlike syntax-based spaces, the context c does
not explicitly state the type of syntactic relation with
the LU: this usually allows to capture both paradig-
matic and syntagmatic relations.
4 WordNet-based model
In a departure from previous work, our WordNet-
based model does not rely on standard WordNet sim-
ilarity measures (Pedersen et al, 2004), as these
measures can only be applied to pairs of words,
while we here need to capture the meaning of whole
frames, which typically consist of larger sets of LUs.
Our intuition is that senses able to evoke a frame can
be detected via WordNet, by jointly considering the
WordNet synsets activated by all LUs of the frame.
We implement this intuition in a weakly-
supervised model, where each frame f is repre-
sented as a set of specific sub-graphs of the WordNet
1See (Pado?, 2007; Sahlgren, 2006) for an in depth analysis.
459
hyponymy hierarchy. As different parts of speech
have different WordNet hierarchies, we build a sub-
graph for each of them: Snf for nouns, Svf for verbs
and Saf for adjectives.2 These sub-graphs repre-
sent the lexical semantic properties characterizing
the frame. An unknown LU ul of a given part of
speech is assigned to the frame whose correspond-
ing sub-graph is semantically most similar to one of
the senses of ul:
fmaxul = argmaxf?N simWN (ul, f) (6)
where simWN is a WordNet-based similarity
measure. In the following subsections we will de-
scribe how we build sub-graphs and model the sim-
ilarity measure for the different part of speech.
Figure 1 reports an excerpt of the noun sub-
graph for the frame PEOPLE BY AGE, cover-
ing the suitable senses of its nominal LUs
{adult, baby, boy, kid, youngster, youth}. The
relevant senses (e.g. sense 1 of youth out of the 6
potential ones) are generally selected, as they share
the most specific generalizations in WordNet with
the other words.
Nouns. To compute similarity for nouns we adopt
conceptual density (cd) (Agirre and Rigau, 1996),
a semantic similarity model previously applied to
word sense disambiguation tasks.
Given a frame f and its set of nominal lexical
units Fn, the nominal subgraph Snf is built as fol-
lows. All senses of all words in Fn are activated
in WordNet. All hypernyms Hnf of these senses are
then retrieved. Every synset ? ? Hnf is given a cd
score, representing the density of the WordNet sub-
hierarchy rooted at ? in representing the set of nouns
Fn. The intuition behind this model is that the larger
the number of LUs in Fn that are generalized by ? is,
the better it captures the lexical semantics intended
by the frame f . Broader generalizations are penal-
ized as they give rise to bigger hierarchies, not well
correlated with the full set of targets Fn.
To build the final sub-graph Snf , we apply the
greedy algorithm proposed by Basili and colleagues
(2004). It first computes the set of WordNet synsets
that generalize at least two LUs in Fn, and then se-
lects the subset of most dense ones Snf ? Hnf that
2Our WordNet model does not cover the limited number of
LUs which are not nouns, verbs or adjectives.
cover Fn. If a LU has no common hypernym with
other members of Fn, it is not represented in Snf , and
its similarity is set to 0 . Snf disambiguates words in
Fn as only the lexical senses with at least one hyper-
nym in Snf are considered.
Figure 1 shows the nominal sub-graph automati-
cally derived using conceptual density for the frame
PEOPLE BY AGE. The word boy is successfully dis-
ambiguated, as its only hypernym in the sub-graph
refers to its third sense (a male human offspring)
which correctly maps to the given frame. Notice
that this model departs from the first sense heuris-
tics largely successful in word sense disambigua-
tion: most frames in fact are characterized by non
predominant senses. The only questionable disam-
biguation is for the word adult: the wrong sense
(adult mammal) is selected. However, even in these
cases, the cd values are very low (about 10?4), so
that they do not impact much on the quality of the
resulting inference.
Figure 1: The noun sub-graph for the frame PEO-
PLE BY AGE as evoked by a subset of the words. Sense
numbers #n refers to WordNet 2.0.
Using this model, LU induction is performed as
follows. Given an unknown lexical unit ul, for each
frame f ? N we first build the sub-graph Snf from
the set Fn ? {ul}. We then compute simWN (f, ul)
as the maximal cd of any synset ? ? Snf that gener-
alizes one of the lexical senses of ul. In the example
baby would receive a score of 0.117 according to its
first sense in WordNet 2.0 (?baby,babe,infant?). In
a final step, we assign the LU to the most similar
frame, according to Eq. 6
Verbs and Adjectives. As the conceptual density
algorithm can be used only for nouns, we apply dif-
ferent similarity measures for verbs and adjectives.
460
For verbs we exploit the co-hyponymy relation:
the sub-graph Svf is given by all hyponyms of all
verbs Fv in the frame f . Similarity simWN (f, ul)
is computed as follows:
simWN (ul, f) =
?
???
???
1 iff ?K ? F such that
|K| > ? AND
?l ? K, l is a co-hyponym of ul
? otherwise
(7)
As for adjectives, WordNet does not provide a hy-
ponymy hierarchy. We then compute similarity sim-
ply on the basis of the synonymy relation, as fol-
lows:
simWN (ul, f) =
?
?
?
1 iff ?l ? F such that
l is a synonym of ul
? otherwise
(8)
5 Combined model
The methods presented so far use two independent
information sources to induce LUs: distributional
similarity simD and WordNet similarity simWN .
We also build a joint model, leveraging both ap-
proaches: we expect the combination of different
information to raise the overall performance. We
here choose to combine the two approaches using a
simple back-off model, that uses the WordNet-based
model as a default and backs-off to the distributional
one when no frame is proposed by the former. The
intuition is that WordNet should guarantee the high-
est precision in the assignment, while distributional
similarity should recover cases of low coverage.
6 Experiments
In this section we present a comparative evaluation
of our models on the task of inducing LUs, in a
leave-one-out setting over a reference gold standard.
6.1 Experimental Setup
Our gold standard is the FrameNet 1.3 database,
containing 795 frames and a set L of 7,522 unique
LUs (in all there are 10,196 LUs possibly assigned
to more than one frame). Given a lexical unit l ? L,
we simulate the induction task by executing a leave-
one-out procedure, similarly to Burchardt and col-
leagues (2005). First, we remove l from all its origi-
nal frames. Then, we ask our models to reassign it to
the most similar frame(s) f , according to the simi-
larity measure3. We repeat this procedure for all lex-
ical units. Though our experiment is not completely
realistic (we test over LUs already in FrameNet), it
has the advantage of a reliable gold standard pro-
duced by expert annotators. A second, more re-
alistic, small-scale experiment is described in Sec-
tion 6.2.
We compute accuracy as the fraction of LUs in L
that are correctly re-assigned to the original frame.
Accuracy is computed at different levels k: a LU l is
correctly assigned if its gold standard frame appears
among the best-k frames f ranked by the model us-
ing the sim(l, f) measure. As LUs can have more
than one correct frame, we deem as correct an as-
signment for which at least one of the correct frames
is among the best-k.
We also measure coverage, intended as the per-
centage of LUs that have been assigned to at least
one frame by the model. Notice that when no
sense preference can be found above the threshold ?,
the WordNet-based model cannot predict any frame,
thus decreasing coverage.
We present results for the following models and
parametrizations (further parametrizations have re-
vealed comparable performance).
Dist-word : the word-based space described in
Section 3. Contextual features correspond to the
set of the 4,000 most frequent words in the BNC.4
The association measure between LUs and contexts
is the pointwise mutual information. Valid contexts
for LUs are fixed to a 20-window.
Dist-syntax : the syntax-based space described
in Section 3. Context features are the 10,000 most
frequent syntactic relations in the BNC5. As associ-
ation measure we apply log-likelihood ratio (Dun-
ning, 1993) to normalized frequency. Syntactic rela-
tions are extracted using the Minipar parser.
Dist-mixed : the mixed space described in Sec-
3In the distributional model, we recompute the centroids for
each frame f in which the LU appeared, applying Eq. 2 to the
set F ? {l}.
4We didn?t use the FrameNet corpus directly, as it is too
small to obtain reliable statistics.
5Specifically, we use the minimum context selection func-
tion and the plain path value function described in Pado (2007).
461
tion 3. As for the Dist-word model, contextual fea-
tures are 4,000 and pointwise mutual information is
the association measure. The maximal dependency
path length for selecting each context word is 3.
Syntactic relations are extracted using Minipar.
WNet-full : the WordNet based model described
in Section 4.
WNet-bsense : this model is computed as WNet-
full but using only the most frequent sense for each
LU as defined in WordNet.
Combined : the combined method presented in
Section 5. Specifically, it uses WNet-full as a default
and Dist-word as back-off.
Baseline-rnd : a baseline model, randomly as-
signing LUs to frames.
Baseline-mostfreq : a model predicting as best-k
frames the most likely ones in FrameNet ? i.e. those
containing the highest number of LUs.
6.2 Experimental Results
Table 1 reports accuracy and coverage results for the
different models, considering only 6792 LUs with
frequency higher than 5 in the BNC, and frames
with more than 2 lexical units (to allow better gen-
eralizations in all models). Results show that all our
models largely outperform both baselines, achieving
a good level of accuracy and high coverage. In
particular, accuracy for the best-10 frames is high
enoungh to support tasks such as the semi-automatic
creation of new FrameNets. This claim is supported
by a further task-driven experiment, in which we
asked 3 annotators to assign 60 unknown LUs (from
the Detour system log) to frames, with and without
the support of the Dist-word model?s predictions as
suggestions6. We verified that our model guarantee
an annotation speed-up of 25% ? i.e. in average an
annotator saves 25% of annotation time by using
the system?s suggestions.
Distributional vs. WordNet-based models.
WordNet-based models are significantly better than
distributional ones, for several reasons. First, distri-
butional models acquire information only from the
contexts in the corpus. As we do not use a FrameNet
annotated corpus, there is no guarantee that the us-
age of a LU in the texts reflects exactly the semantic
6For this purpose, the dataset is evenly split in two parts.
properties of the LU in FrameNet. In the extreme
cases of polysemous LUs, it may happen that the
textual contexts refer to senses which are not ac-
counted for in FrameNet. In our study, we explicitly
ignore the issue of polisemy, which is a notoriously
hard task to solve in semantics spaces (see (Schu?tze,
1998)), as the occurrences of different word senses
need to be clustered separately. We will approach
the problem in future work. The WordNet-based
model suffers from the problem of polisemy to a
much lesser extent, as all senses are explicitly rep-
resented and separated in WordNet, including those
related to the FrameNet gold standard.
A second issue regards data sparseness. The vec-
torial representation of LUs with few occurrences in
the corpus is likely to be semantically incomplete,
as not enough statistical evidence is available. Par-
ticularly skewed distributions can be found when
some frames are very rarely represented in the cor-
pus. A more in-depth descussion on these two issues
is given later in this section.
Regarding the WordNet-based models, WNet-full
in most cases outperforms WNet-bsense. The first
sense heuristic does not seem to be as effective as
in other tasks, such as Word Sense Disambigua-
tion. Although sense preferences (or predominance)
across two general purpose resources, such as Word-
Net and FrameNet, should be a useful hint, the con-
ceptual density algorithm seems to produce better
distributions (i.e. higher accuracy), especially when
several solutions are considered. Indeed, for many
LUs the first WordNet sense is not the one repre-
sented in the FrameNet database.
As for distributional models, results show that the
Dist-word model performs best. In general, syntac-
tic relations (Dist-syntax model) do not help to cap-
ture frame semantic properties better than a simple
window-based approach. This seems to indicate that
LUs in a same frame are related both by paradig-
matic and syntagmatic relations, in accordance to
the definition given in Section 3.2 ? i.e. they are
mostly semantically related, but not similar.
Coverage. Distributional models show a coverage
15% higher than WordNet-based ones. Indeed, as far
as corpus evidence is available (i.e. the unknown LU
appears in the corpus), distributional methods are al-
ways able to predict a frame. WordNet-based mod-
462
MODEL B-1 B-2 B-3 B-4 B-5 B-6 B-7 B-8 B-9 B-10 COVERAGE
Dist-word 0.27 0.36 0.42 0.46 0.49 0.51 0.53 0.55 0.56 0.57 95%
Dist-syntax 0.22 0.29 0.34 0.38 0.41 0.44 0.46 0.48 0.50 0.51 95%
Dist-mixed 0.25 0.35 0.40 0.44 0.47 0.49 0.51 0.53 0.54 0.56 95%
WNet-full 0.47 0.59 0.65 0.69 0.72 0.73 0.75 0.76 0.77 0.78 80%
WNet-bsense 0.52 0.61 0.64 0.66 0.67 0.68 0.69 0.69 0.70 0.70 72%
Combined 0.43 0.54 0.60 0.64 0.66 0.68 0.70 0.71 0.72 0.73 95%
Baseline-rnd 0.02 0.03 0.05 0.06 0.08 0.10 0.11 0.12 0.14 0.15
Baseline-mostfreq 0.02 0.05 0.07 0.08 0.10 0.11 0.13 0.14 0.15 0.17
Table 1: Accuracy and coverage of different models on best-k ranking with frequency threshold 5 and frame threshold
2
els cannot make predictions in two specific cases.
First, when the LU is not present in WordNet. Sec-
ond, when the function simWN does not has suffi-
cient relational information to find a similar frame.
This second factor is particularly evident for adjec-
tives, as Eq. 8 assigns a frame only when a synonym
of the unknown LU is found. It is then not surpris-
ing that 68% of the missed assignment are indeed
adjectives.
Results for the Combined model suggest that
the integration of distributional and WordNet-based
methods can offer a viable solution to the cover-
age problem, as it achieves an accuracy comparable
to the pure WordNet approaches, while keeping the
coverage high.
Figure 2: Dist-word model accuracy at different LU fre-
quency cuts.
Data Sparseness. A major issue when using dis-
tributional approaches is that words with low fre-
quency tend to have a very sparse non-meaningful
representation in the vector space. This highly im-
pacts on the accuracy of the models. To measure
the impact of data sparseness, we computed the ac-
curacy at different frequency cuts ? i.e. we exclude
LUs below a given frequency threshold from cen-
troid computation and evaluation. Figure 2 reports
the results for best-10 assignment at different cuts,
for the Dist-word model. As expected, accuracy im-
proves by excluding infrequent LUs. Only at a fre-
quency cut of 200 performance becomes stable, as
statistical evidence is enough for a reliable predic-
tion. Yet, in a real setting the improvement in accu-
racy implies a lower coverage, as the system would
not classify LUs below the threshold. For example,
by discarding LUs occurring less than 200 times in
the corpus, we obtain a +0.12 improvement in accu-
racy, but the coverage decreases to 57%. However,
uncovered LUs are also the most rare ones and their
relevance in an application may be negligible.
Lexical Semantics, Ambiguity and Plausible As-
signments. The overall accuracies achieved by
our methods are ?pessimistic?, in the sense that they
should be intended as lower-bounds. Indeed, a qual-
itative analysis of erroneous predictions reveals that
in many cases the frame assignments produced by
the models are semantically plausible, even if they
are considered incorrect in the leave-one-out test.
Consider for example the LU guerrilla, assigned in
FrameNet to the frame PEOPLE BY VOCATION. Our
mixed model proposes as two most similar frames
MILITARY and TERRORISM, which could still be
considered plausible assignment. The same holds
for the LU caravan, for which the most similar
frame is VEHICLE, while in FrameNet the LU is as-
signed only to the frame BUILDINGS. These cases
are due to the low FrameNet coverage, i.e LUs are
not fully annotated and they appear only in a subset
of their potential frames. The real accuracy of our
463
models is therefore expected to be higher.
To explore the issue, we carried out a qualita-
tive analysis of 5 words (i.e. abandon.v, accuse.v,
body.n, charge.v and partner.n). For each of them,
we randomly picked 60 sentences from the BNC
corpus, and asked two human annotators to assign
to the correct frame the occurrence of the word in
the given sentence. For 2 out of 5 words, no frame
could be found for most of the sentences, suggesting
that the most frequent frames for these words were
missing from FrameNet7. We can then conclude that
100% accuracy cannot be considered as the upper-
bound of our experiment, as word usage in texts is
not well reflected in the FrameNet modelling.
Further experiments. We also tested our models
on a realistic gold-standard set of 24 unknown LUs
extracted from the SemEval-2007 corpus (Baker et
al., 2007). These are words not present in FrameNet
1.3 which have been assigned by human annotators
to an existing frame8. WNet-full achieves an accu-
racy of 0.25 for best-1 and 0.69 for best-10, with a
coverage of 67%. A qualitative analysis showed that
the lower performance wrt to our main experiment is
due to higher ambiguity of the LUs (e.g. we assign
tea to SOCIAL EVENT instead of FOOD).
Comparison to other approaches. We compare
our models to the system presented by Johans-
son and Nugues (2007) and Burchardt and col-
leagues (2005). Johansson and Nugues (2007) eval-
uate their machine learning system using 7,000
unique LUs to train the Support Vector Machine, and
the remaining LUs as test. They measure accuracy at
different coverage levels. At 80% coverage accuracy
is about 0.42, 10 points below our best WordNet-
based system. At 90% coverage, the system shows
an accuracy below 0.10 and is significantly out-
performed by both our distributional and combined
methods. These results confirm that WordNet-based
approaches, while being highly accurate wrt dis-
tributional ones, present strong weaknesses as far
as coverage is concerned. Furthermore, Johansson
and Nugues (2007) show that their machine learn-
7Note that the need of new frames to account for seman-
tic phenomena in free texts has been also demonstrated by the
SemEval-2007 competition.
8The set does not contain 4 LUs which have no frame in
FrameNet.
ing approach outperforms a simple approach based
on WordNet similarity: thus, our results indirectly
prove that our WordNet-based method is more ef-
fective than the application of the similarity measure
presented in (Pedersen et al, 2004).
We also compare our results to those reported
by Burchardt and colleagues (2005) for Detour.
Though the experimental setting is slightly different
(LU assignment is done at the text-level), they use
the same gold standard and leave-one-out technique,
reporting a best-1 accuracy of 0.38 and a coverage
of 87%. Our WordNet-based models significantly
outperform Detour on best-1 accuracy, at the cost of
lower coverage. Yet,our combined model is signifi-
cantly better both on accuracy (+5%) and coverage
(+8%). Also, in most cases Detour cannot predict
more than one frame (best-1), while our accuracies
can be improved by relaxing to any best-k level.
7 Conclusions
In this paper we presented an original approach for
FrameNet LU induction. Results show that mod-
els combining distributional and WordNet informa-
tion offer the most viable solution to model the no-
tion of frame, as they allow to achieve a reasonable
trade-off between accuracy and coverage. We also
showed that in contrast to previous work, simple se-
mantic spaces are more helpful than complex syn-
tactic ones. Results are accurate enough to support
the creation and the development of new FrameNets.
As future work, we will evaluate new types of
spaces (e.g. dimensionality reduction methods) to
improve the generalization capabilities of the space
models. We will also address the data sparseness is-
sue, by testing smoothing techniques to better model
low frequency LUs. Finally, we will implement
the presented models in a complex architecture for
semi-supervised FrameNets development, both for
specializing the existing English FrameNet in spe-
cific domains, and for creating new FrameNets in
other languages.
Acknowledgements
This work has partly been funded by the German Re-
search Foundation DFG (grant PI 154/9-3). Thanks
to Richard Johansson and Aljoscha Burchardt for
providing the data of their systems.
464
References
E. Agirre and G. Rigau. 1996. Word Sense Disam-
biguation using Conceptual Density. In Proceedings
of COLING-96, Copenhagen, Denmark.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL, Montreal, Canada.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
SemEval-2007 Task 19: Frame Semantic Structure
Extraction. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 99?104, Prague, Czech Republic, June.
Roy Bar-Haim, Idan Szpektor, and Oren Glickman.
2005. Definition and Analysis of Intermediate Entail-
ment Levels. In ACL-05 Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, Ann
Arbor, Michigan.
R. Basili, M. Cammisa, and F.M. Zanzotto. 2004. A
semantic similarity measure for unsupervised semantic
disambiguation. In Proceedings of LREC-04, Lisbon,
Portugal.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Aljoscha Burchardt and Anette Frank. 2006. Approx-
imating Textual Entailment with LFG and FrameNet
Frames. In Proceedings of PASCAL RTE2 Workshop.
Aljoscha Burchardt, Katrin Erk, and Anette Frank. 2005.
A WordNet Detour to FrameNet. In Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8 of Computer Studies in Language
and Speech. Peter Lang, Frankfurt/Main.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC, Genova,
Italy.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 18(1):61?74.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222?254.
K. Garoufi. 2007. Towards a better understanding of
applied textual entailment: Annotation and evaluation
of the rte-2 dataset. M.Sc. thesis, saarland university.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Phi-
losophy of Linguistics, New York. Oxford University
Press.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic Re-
sources for Scandinavian and Baltic Languages, at
NODALIDA, Tartu, Estonia, May 24.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar word. In Proceedings of COLING-ACL, Mon-
treal, Canada.
Sebastian Pado?. 2007. Cross-Lingual Annotation Projec-
tion Models for Role-Semantic Information. Saarland
University.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1).
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concept. In Proc. of 5th NAACL, Boston,
MA.
Magnus Sahlgren. 2006. The Word-Space Model. De-
partment of Linguistics, Stockholm University.
G. Salton, A. Wong, and C. Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18:613620.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1):97?124.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceedings
of EMNLP-CoNLL, pages 12?21, Prague.
C. Subirats and M. Petruck. 2003. Surprise! Spanish
FrameNet! In Proceedings of the Workshop on Frame
Semantics at the XVII. International Congress of Lin-
guists, Prague.
465
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2345?2354, Dublin, Ireland, August 23-29 2014.
A context-based model for Sentiment Analysis in Twitter
Andrea Vanzo and Danilo Croce and Roberto Basili
Department of Enterprise Engineering
University of Roma Tor Vergata
Via del Politecnico 1, 00133 Roma Italy
{vanzo,croce,basili}@info.uniroma2.it
Abstract
Most of the recent literature on Sentiment Analysis over Twitter is tied to the idea that the senti-
ment is a function of an incoming tweet. However, tweets are filtered through streams of posts,
so that a wider context, e.g. a topic, is always available. In this work, the contribution of this
contextual information is investigated. We modeled the polarity detection problem as a sequen-
tial classification task over streams of tweets. A Markovian formulation of the Support Vector
Machine discriminative model as embodied by the SVM
hmm
algorithm has been here employed
to assign the sentiment polarity to entire sequences. The experimental evaluation proves that se-
quential tagging effectively embodies evidence about the contexts and is able to reach a relative
increment in detection accuracy of around 20% in F1 measure. These results are particularly
interesting as the approach is flexible and does not require manually coded resources.
1 Introduction
Since in the Web 2.0 users can write about their life, personal experiences, share contents about facts and
ideas, Social Networks became valuable sources of opinions and sentiments. This huge amount of data
is crucial in the study of the interactions and dynamics of subjectivity on the Web, largely relevant for
marketing tasks. Twitter is one among these microblogging services that counts about a billion of active
users and 500 million of daily messages
1
. However, the analysis of this huge amount of information is
still challenging, as language is very informal, affected by misspelling and characterized by slang and
#hashtags, i.e. special user-generated tags used to contextualize different tweets around a specific topic.
Researches focused on the computational study and automatic recognition of opinions and sentiments
as they are expressed in free texts. It gave rise to what is currently known as Sentiment Analysis, a set
of tasks aiming to detect the subjective attitude of a writer with respect to some topic. Many Sentiment
Analysis studies map sentiment detection in a Machine Learning (ML) setting (Pang and Lee, 2008),
where labeled data, i.e. known examples, allow to induce the detection function from real world exam-
ples. In general, sentiment detection in tweets has been generally treated as any other text classification
task, as proved by most papers participating to the Sentiment Analysis in Twitter task in SemEval-2013
challenge (Nakov et al., 2013): a computational representation for an incoming instance is generated by
just considering one tweet at a time. The short length of the message and the resulting semantic ambi-
guity are critical limitations and make the task very complex. Let us consider the following example, in
which a tweet from ColMustard cites SergGray:
ColMustard : @SergGray Yes, I totally agree with you about the substitutions! #Bayern #Freiburg
The tweet sounds like to be a reply to the previous one. Notice how no lexical nor syntactic property
allows to determine the sentiment polarity. However, if we look at the entire conversation that follows:
ColMustard : Amazing match yesterday!!#Bayern vs. #Freiburg 4-0 #easyvictory
SergGray : @ColMustard Surely, but #Freiburg wasted lot of chances to score.. wrong substitutions by
#Guardiola during the 2nd half!!
ColMustard : @SergGray Yes, I totally agree with you about the substitutions! #Bayern #Freiburg
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://expandedramblings.com/
2345
it is easy to establish that a first positive tweet has been produced, followed by a second negative one so
that the third tweet is negative as well. It is the conversation that allows us here to disambiguate even a
very short message and properly characterize it according to its author and posting time.
We want here to capitalize such a richer set of observations (i.e. entire conversations) and to define
a context-sensitive SA model along two lines: first, by enriching a tweet representation to include the
conversation information, and then introducing a more complex classification model that works over an
entire tweet sequence and not on one tweet (i.e. the target) at a time. Accordingly, in the paper we will
first focus on different representations of tweets that can be made available to the sentiment detection
process. They will also account for contexts, that are conversations, as chains of tweets that are reply
to the previous ones, and topics, built around hashtags. These are in fact topics made explicit by users,
such as events (#easyvictory) or people (#Guardiola). It represents a wider notion of conversation that
enforces the sense of belonging to a community. From a computational perspective, the polarity detection
of a tweet in a context is here modeled as a sequential classification task. In fact, both conversation and
topic-based context are arbitrarily long sequences of messages, ordered according to time with the target
tweet being the last. The SVM
hmm
learning algorithm (Altun et al., 2003) has been employed, as it
allows to classify an instance (here, a tweet) within an entire sequence. While SVM based classifiers
allow to recognize the sentiments from one specific tweet at a time, the SVM
hmm
learning algorithm
collectively labels all tweets in a sequence. It is thus expected to capture patterns within a conversation
and apply them in novel sequences, through a standard decoding task.
While all the above contexts extend a tweet representation, they are still local to a specific notion
of conversation. In this work, we also explore the somehow more abstract notion of contexts given
by the emotional attitude shown by each user in his overall usage of Twitter. In the above example,
ColMustard shows a specific attitude while discussing about the Bayern Munchen. We can imagine
that this feature characterizes most of its future messages at least about football. We suggest to enrich
the tweet representation with features that synthesize a user?s profile, in order to catch possible biases
towards a particular sentiment polarity. This is quite interesting as it has been shown that communities
behave in a coherent way and users tend to take stable standing points. Experimental evaluation (Chapter
4) proves the effectiveness of this proposed sequential tagging approach combined with the adopted
contextual information, improving the percentage of correctly recognized tweets up to 12%.
A survey of the existing approaches is presented into Section 2. Then, Section 3 provides an account
of the context-based models: conversation, topic-based and user sentiment profiling. The experimental
evaluation into Section 4 prove the positive impact of social dynamics on the SA task.
2 Related Work
Sentiment Analysis has been described as a Natural Language Processing task at many levels of gran-
ularity. Starting from being mapped into a document level classification task (Turney, 2002; Pang and
Lee, 2004), it has been also applied at sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more
recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009).
The spreading of microblog services where users post real-time opinions about ?everything?, poses
newer and different challenges. Indeed, classical approaches to Sentiment Analysis (Pang et al., 2002;
Pang and Lee, 2008) are not directly applicable to tweets: while most of them focus on relatively large
texts, e.g. movie or product reviews, tweets are very short and fine-grained lexical analysis is required.
Nevertheless, the great prominence of Social Media during the last few years encouraged a focus on
the sentiment detection over a microblogging domain. Recent works tried to model the sentiment in
tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet
and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Zanzotto et al., 2011; Si et al.,
2013; Agarwal et al., 2011). Specific approaches and feature modeling are used to improve accuracy
levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicons and
tweet specific features (e.g. hashtags, re-tweets) are some of the component exploited by these works, in
combination with different machine learning algorithms: among these latter, probabilistic paradigms, e.g.
Naive Bayes (Pak and Paroubek, 2010), or Kernel-based machines, as discussed in (Barbosa and Feng,
2346
2010; Agarwal et al., 2011; Castellucci et al., 2013), are mostly employed. An interesting perspective,
where a kind of contextual information is studied, is presented in (Mukherjee and Bhattacharyya, 2012):
the sentiment detection of tweets is here modeled according to lexical features as well as discourse
relations like the presence of connectives, conditionals and semantic operators like modals and negations.
Nevertheless, in all the above approaches, features are derived only from lexical resources or from the
tweet itself and no contextual information is exploited. However, given one tweet targeted for sentiment
detection, more awareness about its content is available to writers and readers by the entire stream of
related posts immediately preceding it. In order to exploit this wider information, a Markovian extension
of a Kernel-based categorization approach is proposed in the next section.
3 A context based model for Sentiment Analysis in Twitter
As discussed in the introduction, contextual information about one tweet stems from various aspects: an
explicit conversation, the user attitude or the overall set of recent tweets about a topic (for example an
hastag like #Bayern). As individual perspectives on the context are independent (a conversation may
or may not depend on user preference or cheer) and they also obey to different notion of analogies or
similarity, we should avoid a unified feature vector, but employ independent representations. A structured
view on a tweet can thus be provided by considering it as multifaceted entity where a set of vectors, each
one contributing to one aspect of the overall representation, exhibits a specific similarity metrics. Notice
how this is exactly what Kernel-based learning supports, whereas the combination of the different Kernel
functions can be easily made a Kernel function itself (Shawe-Taylor and Cristianini, 2004). Kernel
functions are used to capture specific aspects of the semantic relatedness between two tweets and are
easily integrated in various Machine Learning algorithms, such as SVM.
3.1 Representing tweets through different Kernel functions
Many Machine Learning approaches for Sentiment Analysis in Twitter benefited by complex ways of
modeling of individual tweets, as discussed in many works (Nakov et al., 2013). The representation we
propose makes use of individual Kernels as models of different aspects usable within a SVM paradigm.
Bag of Word Kernel (BoWK). The simplest Kernel function describes the lexical overlap between
tweets, thus represented as vectors, whose dimensions correspond to the different words. Components
denote the presence or not of the corresponding word in the text and Kernel function corresponds to
the cosine similarity between vector pairs. Even if very simple, the BoW model is one of the most
informative representation in Sentiment Analysis, as emphasized since (Pang et al., 2002).
Lexical Semantic Kernel (LSK). Lexical information in tweets can be very sparse, as we will also
show in the next Section 4. In order to extend the BoW model, we provide a further vector representation
aiming to generalize the lexical information. It can be obtained for every term of a dictionary by a
co-occurrence Word Space built according to the Distributional Analysis described in (Sahlgren, 2006).
A word-by-context matrix, M , is built through large scale corpus analysis and then processed through
Latent Semantic Analysis (Landauer and Dumais, 1997). The dimensionality of the space represented by
M can be reduced through Singular Value Decomposition (SVD) (Golub and Kahan, 1965). The original
statistical information about M is captured by the new k-dimensional space, which preserves the global
structure while removing low-variance dimensions, i.e. distribution noise. The result is that every word
is projected in the reduced Word Space and a vector for each tweet is represented through the linear
combination of the co-occurring word vectors (also called additive linear combination in (Mitchell and
Lapata, 2010)). The resulting Kernel function is the cosine similarity between tweet vector pairs, in line
with (Cristianini et al., 2002). Notice that the adoption of a distributional approach does not limit the
overall application, as it can be automatically applied without relying on any manually coded resource.
User Sentiment Profile Context (USPK). A source of evidence about a tweet is its author, with his
attitude towards some polarities. Specific features based on the users? previous tweets can be derived as
follows. Let t
i
? T be a tweet and i ? N
+
its identifier. The User Profile Context (U
i
) can be defined as
the set of the lastH tweets posted by the author of t
i
, hereafter denoted by u
i
. This information is a body
of evidence about the opinion holder?s profile on which a further tweet representation can be defined. A
tweet t
i
is here mapped into a three dimensional vector ~?
i
=
(
?
1
i
, ?
2
i
, ?
3
i
)
, where each component ?
j
i
is
2347
the indicator of polarity inclination, i.e. positive, negative and neutral, expressed through the conditional
probability P (j | u
i
) for the polarity labels j ? Y given the user u
i
. We can suppose that, for each
t
k
? U
i
, its corresponding label y
k
is available either as a gold standard annotation or predicted in a semi-
supervised fashion by trained classifiers. The estimation of ?
j
i
? P (j | u
i
), is a ?-parameterized Laplace
smoothed version of the observations in U
i
: ?
j
i
=
?
|U
i
|
k=1
(1
{y
k
=j}
(t
k
) + ?)/(|U
i
|+ ?|Y|) where ? ? R
is the smoothing parameter, j ? Y , i.e. the set of polarity labels. The Kernel function, called User
Sentiment Profile Kernel (USPK), is the cosine similarity between two vectors (~?
i
, ~?
m
).
The multiple Kernel approach. Whenever the different Kernels are available, we can apply a lin-
ear combination ?BoWK+?LSK or ?BoWK+?LSK+?USPK in order to exploit lexical and semantic
properties captured by BoWK and LSK, or user properties as captured by USPK.
3.2 Modeling tweet conversation as a sequential tagging problem
The User Sentiment Profile Kernel (UPSK) can be seen as an implicit representation of a context de-
scribing the writer. However, contextual information is usually embodied by the stream of tweets in
which the target one t
i
is immersed. Usually, the stream is something available to a reader and includes
an entire conversation (where links to the previous tweets are made explicit and are supposed to be all
available) or a topic, i.e. a hashtag, the reader has searched for. In all cases, the stream give rise to an
entire sequence on which sequence labeling can be applied: the target tweet is here always labeled within
the entire sequence, where contextual constraints are provided by the preceding tweets. More formally,
two types of context are defined:
Conversational context. For every tweet t
i
? T , let r(t
i
) : T ? T be a function that returns either
the tweet to which t
i
is a reply to, or null if t
i
is not a reply. Then, the conversation-based context ?
C,l
i
of tweet t
i
(i.e., the target tweet) is the sequence of tweet iteratively built by applying r(?), until l tweets
have been selected or r(?) = null. In other words, l allows to limit the size of the input context. An
example of conversation-based context is given in Section 1.
Topical context. Hashtags allow to aggregate different tweets around a specific topic. An entire tweet
sequence can be derived including the n tweets preceding the target t
i
that contain the same hashtag set.
This is usually the output of a search in Twitter and it is likely the source information that influenced
the writer?s opinion. Let t
i
? T be a tweet and h(i) : T ? P(H) be a function that returns the entire
hashtag set H
i
? H observed into t
i
. Then, the hashtag-based context ?
H,l
i
for a tweet t
i
(i.e., target
tweet) is a sequence of the most recent l tweets t
j
such that H
j
?H
i
6= ?, i.e. t
j
and t
i
share at least one
hashtag, and t
j
has been posted before t
i
. As an example, the following hashtag-based context of size 4
has been obtained about #Bayern:
MrGreen : Fun fact: #Freiburg is the only #Bundesliga team #Pep has never beaten in his coaching career. #Bayern
MrsPeacock : Young starlet Xherdan #Shaqiri fires #Bayern into a 2-0 lead. Is there any hope for #Freiburg?
pic.twitter.com/krzbFJFJyN
ProfPlum : It is clear that #Bayern is on a rampage leading by 4-0, the latest by Mandzukic... hoping for
another 2 goals from #bayernmunich
MissScarlet : Noooo! I cant believe what #Bayern did!
It is clear that MissScarlet expressed an opinion, but the corresponding polarity is easily evident
when the entire stream is available about the #Bayern hashtag. As well as in a conversational context,
a specific context size n can be imposed by focusing only on the last n tweets of the sequence. Once
different representations and contexts are available a structured learning-based approach can be applied
to Sentiment Analysis. Firstly, we will discuss a discriminative learning approach that follows the multi-
classification schema proposed in (Joachims et al., 2009), namely SVM
multiclass
. Then a sequence
labeling approach, based on the SVM
hmm
learning algorithm (Altun et al., 2003), will be introduced, as
an explicit account of both conversational and topical contexts.
The multi-class approach. The SVM
multiclass
schema described in (Joachims et al., 2009) is applied
2
to implicitly compare all polarity labels and select the most likely one, using the multi-class formulation
described in (Crammer and Singer, 2001). The algorithm thus acquires a specific function f
y
(x) for
2
http://svmlight.joachims.org/svm multiclass.html
2348
each sentiment polarity label y ? Y , where Y = {positive, negative, neutral}. Given a feature vector
x ? X representing a tweet t
i
, SVM
multiclass
allows to predict a specific polarity y
?
? Y by applying the
discriminant function y
?
= arg max
y?Y
f
y
(x
i
), where f
y
(x) = w
y
? x is a linear classifier associated to
each label y. Given a training set (x
1
, y
1
) . . . (x
n
, y
n
), the learning algorithm determines each classifier
parameters w
y
by solving the following optimization problem:
min
1
2
?
i=1...k
?w
i
?
2
+
C
n
?
i=1...n
?
i
s.t. ?i,?y ? Y : x
i
? w
y
i
? x
i
? w
y
+ 100?(y
i
, y)? ?
i
where C is a regularization parameter that trades off margin size and training error, while ?(y
i
, y) is the
loss function that returns 0 if y
i
equals y, and 1 otherwise.
The markovian approach. The sentiment prediction of a target tweet can be seen as a sequential
classification task over a context, and the SVM
hmm
algorithm can be thus applied. Given an input
sequence x = (x
1
. . . x
l
) ? X , where x is a tweet context, e.g. the conversational and the hashtag-based
one (i.e. ?
C,l
i
and ?
H,l
i
, respectively) and x
i
is a feature vector representing a tweet, the model predicts
a tag sequence y = (y
1
. . . y
l
) ? Y
+
after learning a linear discriminant function F : P(X )? Y
+
? R
over input/output pairs. The labeling f(x) is thus defined as: f(x) = arg max
y?Y
+
F (x,y;w). It is
obtained by maximizing F over the response variable, y, for a specific given input, x. In these models,
F is linear in some combined feature representation of inputs and outputs ?(x,y), i.e. F (x,y;w) =
?w,?(x,y)?. As ? extracts meaningful properties from an observation/label sequence pair (x,y), in
SVM
hmm
it is modeled through two types of features: interactions between attributes of the observation
vectors x
i
and a specific label y
i
(i.e. emissions of x
i
by y
i
) as well as interactions between neighboring
labels y
i
along the chain (transitions). In other words, ? is defined so that the complete labeling y =
f(x) can be computed efficiently from F , using a Viterbi-like decoding algorithm, according to the linear
discriminant function
y
?
= arg max
y?Y
+
{
?
i=1...l
[
?
j=1...k
(x
i
? w
y
i?j
... y
i
) + ?
tr
(y
i?j
, . . . , y
i
) ? w
tr
]}
In the training phase, SVM
hmm
solves the following optimization problem given training examples
(x
1
,y
1
) . . . (x
n
,y
n
) of sequences of feature vectors x
j
with their correct tag sequences y
j
min
1
2
?w?
2
+
C
n
?
i=1...n
?
i
s.t. ?y, n : {
?
i=1...l
(x
n
i
? w
y
n
i
) + ?
tr
(y
n
i?1
, y
n
i
) ? w
tr
} ? {
?
i=1...l
(x
n
i
? w
y
i
) + ?
tr
(y
i?1
, y
i
) ? w
tr
}+ ?(y
n
, y)
where ?(y
n
, y) is the loss function, computed as the number of misclassified tags in the sequence,
(x
i
? w
y
i
) represents the emissions and ?
tr
(y
i?1
, y
i
) the transitions. Indeed, through SVM
hmm
learning
the label for the target tweet is made dependent on its context history. The markovian setting thus
acquires patterns across tweet sequences to recognize sentiment even for truly ambiguous tweets.
4 Experimental Evaluation
The aim of the experiments is to estimate the contribution of the proposed contextual models to the
accuracy reachable in different scenarios, whereas rich contexts (e.g. popular hashtags) are possibly
made available or just singleton tweets, with no context, are targeted.
We adopted the ?Sentiment Analysis in Twitter? dataset
3
as it has been made available in the ACL
SemEval-2013 (Nakov et al., 2013). However, in order to rely on tweet identifiers (needed to retrieve
contexts from Twitter servers), only the Training and Development portions of the data (11,338 exam-
ples), for which id?s were made available, have been employed. As about 10,045 tweets were available
from the servers,
4
a static split 80/10/10 in Training/Held-out/Test respectively, has been carried out as
reported in Table 1. As the performance evaluation is always carried out against one target tweet (in
analogy with the benchmark of SemEval-2013), the multi-classification may happen when no context is
available (i.e. there is no conversation nor hashtag to built the context from) or when a rich conversa-
tional or topical context is available. In Table 1 different datasets are shown in columns 2-4, 5-7 and 8-10
3
http://www.cs.york.ac.uk/semeval-2013/task2/index.php?id=data
4
Several original messages were no longer reachable during the experiment time of March-June 2013
2349
respectively: the entire corpus of 10,045 is represented in columns 2-4, while 5-7 and 8-10 represents
the subsets of target tweets for which a conversational or topical context, respectively, was available.
Conversational contexts are available only for 1,391 tweets (columns 5-7), while hashtag-based contexts
include 1,912 instances (columns 8-10).
whole dataset conversation-filtered hashtag-filtered
train dev test train dev test train dev test
Positive 2984 359 387 454 51 56 621 83 66
Negative 1271 147 142 197 31 24 245 28 22
Neutral 3790 495 470 455 68 55 688 79 80
8045 1001 999 1106 150 135 1554 190 168
Table 1: Whole dataset composition
As tweets are noisy texts, a pre-processing phase has been applied to improve the quality of linguistic
features observable and reduce data sparseness. In particular, a normalization step is applied to each
post: fully capitalized words are converted in lowercase; reply marks are replaced with the pseudo-token
USER, hyperlinks by LINK, hashtags by HASHTAG and emoticons by special tokens
5
. Afterwards, an
almost standard NLP chain is applied through the Chaos parser (Basili et al., 1998; Basili and Zanzotto,
2002). In particular, each tweet, with its pseudo-tokens produced by the normalization step, is mapped
into a sequence of POS tagged lemmas. Emoticons are treated as nouns. In order to feed the LSK, lexical
vectors correspond to a Word Space derived from a corpus of about 1.5 million tweets, downloaded
during the experimental period and using the topic names from the trial material as query terms. Every
word w in such corpus is represented as one co-occurrence vector as in (Sahlgren, 2006) with the setting
discussed in (Croce and Previtali, 2010): left and right co-occurrence scores are obtained in a window of
size n = ?5 around each w. Vector components w
f
correspond to Pointwise Mutual Information values
pmi(w, f) between the word w (the row) and the feature f . Dimensionality reduction is applied to the
co-occurrence matrix, through SVD, with a dimensionality cut of k = 250.
Existing state-of-the-art approaches neglect the tweet context, so that datasets with labeled contexts
are not available: USPK or the markovian approach would not be applicable. The solution consisted
in creating a semi-supervised Gold-Standard by training the multi-class classifier (not employing any
context) fed through a combination of BoWK and LSK Kernel functions and get the classification of all
tweets within the context of at least one target tweet. Unfortunately, this can introduce noise, but it is a
realistic solution to a cold-start approach, easily portable to other datasets.
Performance scores report the classification accuracy in terms of Precision, Recall and standard F-
measure. However, in line with SemEval-2013, we also report the F
pnn
1
score as the arithmetic mean
between the F
1
s of positive, negative and neutral classes.
4.1 Experiment 1: Using contexts in a general tweet classification setting
A first experiment has been run to validate the impact of contextual information over generic tweets,
independently from the availability of the context. In this case, the entire data set is used. The different
settings adopted are reported in independent rows, corresponding to different classification approaches:
? multi-class refers to the application of the multi-classification of SVM
multiclass
, that does not require
any context and can be considered as a baseline for the employed Kernel combinations;
? conversation refers to the SVM
hmm
classifier observing the conversation-based contexts. The train-
ing and testing of the classifier is here run with different context sizes, by parameterizing l in ?
C,l
i
;
? likewise, hashtag refers to the SVM
hmm
classifier observing the topic-based contexts, when hash-
tags are considered. Different context sizes have been considered, by parameterizing l in ?
H,l
i
.
In both conversation and hashtag models, when no context is available, the SVM
hmm
classifier acts on
a sequence of length one, and no transition is used. Table 2 shows the empirical results over the whole
test dataset. The first general outcome is that algorithmic baselines, i.e. context-free models that use
no contextual information, in the multi-class rows are better performing whenever richer representations
are provided. The LSA information (+8.29%) as well as the user profiling (+10.73%) seem beneficial in
5
We normalized 113 well-known emoticons in 13 classes.
2350
Context size Precision Recall F
1
F
pnn
1
l pos neg neu pos neg neu pos neg neu
BoWK
multi-class - .713 .496 .680 .649 .401 .770 .679 .444 .723 .615 ( - )
conversation
3 .761 .493 .695 .651 .465 .789 .702 .478 .739 .640 (+4.07%)
6 .728 .500 .718 .677 .479 .768 .701 .489 .742 .644 (+4.72%)
? .723 .511 .722 .695 .472 .762 .709 .491 .741 .647 (+5.20%)
hashtag
3 .766 .533 .675 .633 .401 .821 .693 .458 .741 .631 (+2.60%)
6 .727 .575 .711 .682 .514 .770 .704 .543 .740 .662 (+7.64%)
16 .717 .561 .730 .693 .549 .755 .704 .555 .743 .667 (+8.46%)
31 .717 .533 .738 .705 .570 .732 .711 .551 .735 .666 (+8.29%)
BoWK+LSK
multi-class - .754 .595 .704 .674 .486 .804 .712 .535 .751 .666 ( - )
conversation
3 .759 .595 .712 .682 .486 .811 .718 .535 .758 .670 (+0.60%)
6 .760 .536 .737 .713 .521 .781 .736 .529 .758 .674 (+1.20%)
? .774 .554 .717 .682 .542 .791 .725 .548 .752 .675 (+1.35%)
hashtag
3 .731 .541 .737 .729 .556 .732 .730 .549 .734 .671 (+0.75%)
6 .770 .580 .736 .700 .585 .789 .733 .582 .762 .693 (+4.05%)
16 .742 .519 .732 .693 .570 .751 .717 .544 .742 .667 (+0.15%)
31 .751 .537 .729 .685 .556 .774 .716 .547 .751 .671 (+0.75%)
BoWK+LSK+USPK
multi-class - .778 .612 .716 .680 .500 .830 .726 .550 .768 .681 ( - )
conversation
3 .771 .563 .689 .625 .507 .817 .690 .533 .748 .657 (-3.67%)
6 .753 .654 .707 .693 .493 .806 .721 .562 .753 .679 (-0.29%)
? .767 .566 .713 .690 .514 .791 .727 .539 .750 .672 (-1.32%)
hashtag
3 .753 .556 .735 .693 .599 .766 .721 .576 .750 .683 (+0.29%)
6 .747 .594 .735 .711 .556 .779 .728 .575 .756 .686 (+0.73%)
16 .742 .519 .742 .700 .592 .745 .721 .553 .743 .672 (-1.32%)
31 .738 .530 .739 .693 .556 .766 .715 .543 .752 .670 (-1.62%)
Table 2: Evaluation results on whole dataset.
their relative improvements with respect to the simple BoW Kernel accuracy. Second, almost all context-
driven models (i.e. SVM
hmm
operating on different context sizes) improve wrt their SVM
multiclass
coun-
terpart. Every polarity category benefits from the introduction of contexts, although this is particularly
true for the negative (neg) case, where a 15.5% of the entire dataset examples are available: it seems
clear that contexts allow to compensate against poor training conditions.
4.2 Experiment 2: Measuring the full impact of context-based models over rich contexts
Given the above outcomes, a second set of experiments has been run against the subset of the test data
restricted to tweets for which rich contexts are available, as introduced in Table 1. In Figure 1, the per-
formances of different learning paradigms and Kernels trained and tested over these corpora are shown.
On the Left of the figure, the performance over the conversation-filtered corpus (Table 1) are reported:
these tweets are characterized by rich conversational contexts of different increasing sizes on the X-axis.
On the Right of Figure 1, the corresponding performances obtained over the hashtag-filtered corpus
are reported. As the number of available examples in both test corpora is much smaller, the baselines
corresponding to the SVM
multiclass
approach are lower.
On the contrary, such poorer training evidence does not seem to afflict the contextual models in both
corpora, as the markovian modeling seems to bring a straight benefit. In particular, increasing amount of
contextual information is usually beneficial to accuracy scores. In general, the SVM
hmm
accuracy plots
seem to increase up to a given context size, that is around 6 for conversational contexts vs. 16 previous
tweets for topical contexts. It seems that a wider context (i.e. a window of 8 or 10 tweets) is not so
beneficial, as the generalization emphasized by LSK and USPK tends to diverge. Different genres of
discussions seem to provide different useful contexts for sentiment detection. The overall benefit reach-
able by SVM
hmm
relatively to the SVM
multiclass
baseline is striking as only rich contexts are used for
training and testing. The BoW Kernel over the conversation corpus has an overall relative improvement
of 18.26% in F
pnn
1
, where the richer BoWK+LSK Kernel improves of about 5.94%. Boosts in F
pnn
1
over topical contexts are more significant: 23.73% for the BoW Kernel vs. 17.93% for BoWK+LSK.
This latter Kernel is optimal, suggesting that user profiling requires possibly a richer description that is
not entirely captured by the vectors of the user sentiment profile. In fact USPK, when combined with
2351
0,50 
0,55 
0,60 
0,65 
0,70 
0 2 4 6 8 10 SVMmulti (BOWK) SVM-HMM (BOWK) SVMmulti (BOWK+LSK) SVM-HMM (BOWK+LSK) SVMmulti (BOWK+LSK+USPK) SVM-HMM (BOWK+LSK+USPK) 
0,50 
0,55 
0,60 
0,65 
0,70 
0 5 10 15 20 25 30 SVMmulti (BOWK) SVM-HMM (BOWK) SVMmulti (BOWK+LSK) SVM-HMM (BOWK+LSK) SVMmulti (BOWK+LSK+USPK) SVM-HMM (BOWK+LSK+USPK) 
Figure 1: The F
pnn
1
measure of the different classifiers vs. different context sizes. On the Left: perfor-
mances when conversational contexts are employed. On the Right: topical contexts are adopted.
BOWK+LSK into the markovian approach, seems to not provide any useful contribution. A clash be-
tween the global information (as modeled by the USPK) and the local information (embedded in the
recent tweets about a topic) is here observed: when these enter in an opposition, the contrast penalizes
the accuracy of the linear combination of Kernels. In general, the improvements implied by contextual
information are related to the treatment of particularly ambiguous tweets. In a conversation, such as
MrGreen : Cannot wait to meet @therealjuicyj and @RealWizKhalifa with @Hill Gonzz
November 29th #trippyniqqas (positive)
ColMustard : @MrGreen where they gone be?? (neutral)
MrGreen : @ColMustard New Orleans!!! (positive)
ColMustard : @MrGreen house of blues? (neutral)
MrGreen : @ColMustard no it?s at the UNO lakefront arena (neutral)
ColMustard : @MrGreen I?m going Tuesday to the house of blues to see ASAP Rocky (neutral)
the switch to a neutral mode characterizing the target tweet is a consequence of the entire sequence and
captured as a pattern. The contribution of the topical contexts is finally evident in the following example:
... ... ... ... ... ...
ProfPlum : Can?t wait to get out there with my boys Go Team! #goeagles (positive)
MrsPeacock : GO my awesome team @WestCoastEagles!!!!! #goeagles #weflyhigh :D (positive)
MissScarlet : Let?s go eagles :) #goeagles (positive)
SergGray : keen for the eagles game today. #goeagles (positive)
5 Conclusions
In this work the role of contextual information in supervised Sentiment Analysis over Twitter is investi-
gated. While the task is eminently linguistic, as resources and phenomena lie in the textual domain, other
semantic dimensions are worth to be explored. In this work, three types of context for a target tweet have
been studied. Structured Learning through a markovian approach has been adopted to inject contextual
evidence (e.g. the history of preceding posts) in the classification of the most recent, i.e. a target, tweet.
The improvement of accuracy in the investigated task are striking as for the large applicability of the
approach that does not require additional manually coded resources. The different employed contexts
show specific but systematic benefits. On the one side, this proofs the correctness of the initial intuitions.
Moreover, the observed relative improvements around 20% over tweets characterized by rich topical or
conversational contexts (see Fig. 1) suggest that larger training datasets can even provide better results.
In these first experiments, user modeling has only been partially explored, whereas the USPK model
does not seem very effective. In fact, USPK seems to express a more static notion of context (i.e. the
attitude of the user as observed across a longer period than individual conversations) and two different
notions (i.e. information embedded into recent tweets) risk to be incompatible. However, the learning of
the optimal Kernel combination as well as a proper history size for the USPK are still worth of deeper
investigation. Finally, user interaction dynamics are particularly complex in social networks and deserve
better representations about reputation, authority and influence in future explorations.
2352
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mckeown. 2009. Contextual phrase-level polarity analysis using
lexical affect scoring and syntactic n-grams. In Proceedings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics, EACL ?09, pages 24?32, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of
twitter data. In Proceedings of the Workshop on Languages in Social Media, LSM ?11, pages 30?38, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hidden Markov support vector machines. In Proceedings of
the International Conference on Machine Learning.
Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In
Chu-Ren Huang and Dan Jurafsky, editors, COLING (Posters), pages 36?44. Chinese Information Processing
Society of China.
Roberto Basili and Fabio Massimo Zanzotto. 2002. Parsing engineering and empirical robustness. Nat. Lang.
Eng., 8(3):97?120, June.
Roberto Basili, Maria Teresa Pazienza, and Fabio Massimo Zanzotto. 1998. Efficient parsing for information
extraction. In Proc. of the European Conference on Artificial Intelligence, pages 135?139.
Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in twitter streaming data. In Proceedings
of the 13th International Conference on Discovery Science, DS?10, pages 1?15, Berlin, Heidelberg. Springer-
Verlag.
Giuseppe Castellucci, Simone Filice, Danilo Croce, and Roberto Basili. 2013. Unitor: Combining syntactic and
semantic kernels for twitter sentiment analysis. In Second Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation
(SemEval 2013), pages 369?374, Atlanta, Georgia, USA, June. Association for Computational Linguistics.
K. Crammer and Y. Singer. 2001. On the algorithmic implementation of multi-class svms. Journal of Machine
Learning Research, 2:265?292.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi. 2002. Latent semantic kernels. J. Intell. Inf. Syst.,
18(2-3):127?152, March.
Danilo Croce and Roberto Basili. 2012. Grammatical feature engineering for fine-grained ir tasks. In Giambattista
Amati, Claudio Carpineto, and Giovanni Semeraro, editors, IIR, volume 835 of CEUR Workshop Proceedings,
pages 133?143. CEUR-WS.org.
Danilo Croce and Daniele Previtali. 2010. Manifold learning for the semi-supervised induction of framenet pred-
icates: An empirical investigation. In Proceedings of the 2010 Workshop on GEometrical Models of Natural
Language Semantics, GEMS ?10, pages 7?16, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and
smileys. In Chu-Ren Huang and Dan Jurafsky, editors, COLING (Posters), pages 241?249. Chinese Information
Processing Society of China.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Pro-
cessing, pages 1?6.
G. Golub and W. Kahan. 1965. Calculating the singular values and pseudo-inverse of a matrix. Journal of the
Society for Industrial and Applied Mathematics, 2(2).
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ?04, pages 168?177, New
York, NY, USA. ACM.
Thorsten Joachims, Thomas Finley, and Chun-Nam Yu. 2009. Cutting-plane training of structural SVMs. Machine
Learning, 77(1):27?59.
Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th
International Conference on Computational Linguistics, COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
2353
Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the
bad and the omg! In Lada A. Adamic, Ricardo A. Baeza-Yates, and Scott Counts, editors, ICWSM. The AAAI
Press.
T. Landauer and S. Dumais. 1997. A solution to plato?s problem: The latent semantic analysis theory of acquisi-
tion, induction and representation of knowledge. Psychological Review, 104(2):211?240.
Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Subhabrata Mukherjee and Pushpak Bhattacharyya. 2012. Sentiment analysis in twitter with lightweight discourse
analysis. In Proceedings of COLING, pages 1847?1864.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter. In Second Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation
(SemEval 2013), pages 312?320, Atlanta, Georgia, USA, June. Association for Computational Linguistics.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining.
In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC?10),
Valletta, Malta, May. European Language Resources Association (ELRA).
Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42Nd Annual Meeting on Association for Computational Lin-
guistics, ACL ?04, Stroudsburg, PA, USA. Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine
learning techniques. In Proceedings of EMNLP, pages 79?86.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D. thesis, Stockholm University.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li, Huayi Li, and Xiaotie Deng. 2013. Exploiting topic based
twitter sentiment for stock prediction. In ACL (2), pages 24?29.
Peter D. Turney. 2002. Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ?02,
pages 417?424, Stroudsburg, PA, USA. Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in
Natural Language Processing, HLT ?05, pages 347?354, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Fabio M. Zanzotto, Marco Pennaccchiotti, and Kostas Tsioutsiouliklis. 2011. Linguistic Redundancy in Twitter.
In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 659?669,
Edinburgh, Scotland, UK., July. Association for Computational Linguistics.
2354
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1034?1046,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Lexical Similarity via Convolution Kernels on Dependency Trees
Danilo Croce
DII
University of Tor Vergata
00133 Roma, Italy
croce@info.uniroma2.it
Alessandro Moschitti
DISI
University of Trento
38123 Povo (TN), Italy
moschitti@disi.unitn.it
Roberto Basili
DII
University of Tor Vergata
00133 Roma, Italy
basili@info.uniroma2.it
Abstract
A central topic in natural language process-
ing is the design of lexical and syntactic fea-
tures suitable for the target application. In this
paper, we study convolution dependency tree
kernels for automatic engineering of syntactic
and semantic patterns exploiting lexical simi-
larities. We define efficient and powerful ker-
nels for measuring the similarity between de-
pendency structures, whose surface forms of
the lexical nodes are in part or completely dif-
ferent. The experiments with such kernels for
question classification show an unprecedented
results, e.g. 41% of error reduction of the for-
mer state-of-the-art. Additionally, semantic
role classification confirms the benefit of se-
mantic smoothing for dependency kernels.
1 Introduction
A central topic in Natural Language Processing is
the design of lexical and syntactic features suitable
for the target application. The selection of effective
patterns composed of syntactic dependencies and
lexical constraints is typically a complex task.
Additionally, the availability of training data is
usually scarce. This requires the development of
generalized features or the definition of seman-
tic similarities between them, e.g. as proposed in
(Resnik, 1995; Jiang and Conrath, 1997; Schtze,
1998; Pedersen et al, 2004a; Bloehdorn and Mos-
chitti, 2007b; Davis et al, 2007) or in semi-
supervised settings, e.g. (Chapelle et al, 2006).
A semantic similarity can be defined at structural
level over a graph, e.g. (Freeman, 1977; Bunke and
Shearer, 1998; Brandes, 2001; Zhao et al, 2009), as
well as combining structural and lexical similarity
over semantic networks, e.g. (Cowie et al, 1992; Wu
and Palmer, 1994; Resnik, 1995; Jiang and Conrath,
1997; Schtze, 1998; Leacock and Chodorow, 1998;
Pedersen et al, 2004a; Budanitsky and Hirst, 2006).
More recent research also focuses on mechanisms
to define if two structures, e.g. graphs, are enough
similar, as explored in (Mihalcea, 2005; Zhao et al,
2009; Fu?rstenau and Lapata, 2009; Navigli and La-
pata, 2010).
On one hand, previous work shows that there is
a substantial lack of automatic methods for engi-
neering lexical/syntactic features (or more in gen-
eral syntactic/semantic similarity). On the other
hand, automatic feature engineering of syntactic or
shallow semantic structures has been carried out
by means of structural kernels, e.g. (Collins and
Duffy, 2002; Kudo and Matsumoto, 2003; Cumby
and Roth, 2003; Cancedda et al, 2003; Daume? III
and Marcu, 2004; Toutanova et al, 2004; Shen et al,
2003; Gliozzo et al, 2005; Kudo et al, 2005; Titov
and Henderson, 2006; Zelenko et al, 2002; Bunescu
and Mooney, 2005; Zhang et al, 2006). The main
idea of structural kernels is to generate structures
that in turn represent syntactic or shallow semantic
features. Most notably, the work in (Bloehdorn and
Moschitti, 2007b) encodes lexical similarity in such
kernels. This is essentially the syntactic tree ker-
nel (STK) proposed in (Collins and Duffy, 2002) in
which syntactic fragments from constituency trees
can be matched even if they only differ in the leaf
nodes (i.e. they have different surface forms). This
implies matching scores lower than 1, depending on
the semantic similarity of the corresponding leaves
in the syntactic fragments.
Although this kernel achieves state-of-the-art per-
formance in NLP tasks, such as Question Classifica-
1034
tion (Bloehdorn and Moschitti, 2007b) and Textual
Entailment (Mehdad et al, 2010), it offers clearly
possibility of improvement: (i) better possibility to
exploit semantic smoothing since, e.g., trivially STK
only matches the syntactic structure apple/orange
when comparing the big beautiful apple to a nice
large orange; and (ii) STK cannot be effectively ap-
plied to dependency structures, e.g. see experiments
and motivation in (Moschitti, 2006a). Additionally,
to our knowledge, there is no previous study that
clearly describes how dependency structures should
be converted in trees to be fully and effectively ex-
ploitable by convolution kernels. Indeed, although
the work in (Culotta and Sorensen, 2004) defines a
dependency tree also using node similarity, it is not
a convolution kernel: this results in a much poorer
feature space.
In this paper, we propose a study of convolution
kernels for dependency structures aiming at jointly
modeling syntactic and lexical semantic similarity.
More precisely, we define several dependency trees
exploitable by the Partial Tree Kernel (PTK) (Mos-
chitti, 2006a) and compared them with STK over
constituency trees. Most importantly, we define
an innovative and efficient class of kernels, i.e. the
Smoothed Partial Tree Kernels (SPTKs), which can
measure the similarity of structural similar trees
whose nodes are associated with different but re-
lated lexicals. Given the convolution nature of such
kernels any possible node path of lexicals provide
a contribution smoothed by the similarity accounted
by its nodes.
The extensive experimentation on two datasets of
question classification (QC) and semantic role label-
ing (SRL), shows that: (i) PTK applied to our depen-
dency trees outperforms STK, demonstrating that
dependency parsers are fully exploitable for feature
engineering based on structural kernels; (ii) SPTK
outperforms any previous kernels achieving an un-
precedented result of 41% of error reduction with re-
spect to the former state-of-the-art on QC; and (iii)
the experiments on SRL confirm that the approach
can be applied to different tasks without any tuning
and again achieving state-of-the-art accuracy.
In the reminder of this paper, Section 2 provides
the background for structural and lexical similar-
ity kernels. Section 3 introduces SPTK. Section 4
provides our representation models for dependency
trees. Section 5 presents the experimental evaluation
for QC and SRL. Section 6 derives the conclusions.
2 Kernel Background
In kernel-based machines, both learning and classi-
fication algorithms only depend on the inner prod-
uct between instances. This in several cases can be
efficiently and implicitly computed by kernel func-
tions by exploiting the following dual formulation:?
i=1..l yi?i?(oi)?(o) + b = 0, where oi and o are
two objects, ? is a mapping from the objects to fea-
ture vectors ~xi and ?(oi)?(o) = K(oi, o) is a ker-
nel function implicitly defining such mapping. In
case of structural kernels,K determines the shape of
the substructures describing the objects above. The
most general kind of kernels used in NLP are string
kernels, e.g. (Shawe-Taylor and Cristianini, 2004),
the Syntactic Tree Kernels (Collins and Duffy, 2002)
and the Partial Tree Kernels (Moschitti, 2006a).
2.1 String Kernels
The String Kernels (SK) that we consider count
the number of subsequences shared by two strings
of symbols, s1 and s2. Some symbols during the
matching process can be skipped. This modifies
the weight associated with the target substrings as
shown by the following SK equation:
SK(s1, s2) =
?
u???
?u(s1) ? ?u(s2) =
?
u???
?
~I1:u=s1[~I1]
?
~I2:u=s2[~I2]
?d(~I1)+d(~I2)
where, ?? = ??n=0 ?n is the set of all strings, ~I1 and
~I2 are two sequences of indexes ~I = (i1, ..., i|u|),
with 1 ? i1 < ... < i|u| ? |s|, such that u = si1 ..si|u| ,
d(~I) = i|u| ? i1 + 1 (distance between the first and
last character) and ? ? [0, 1] is a decay factor.
It is worth noting that: (a) longer subsequences
receive lower weights; (b) some characters can be
omitted, i.e. gaps; (c) gaps determine a weight since
the exponent of ? is the number of characters and
gaps between the first and last character; and (c)
the complexity of the SK computation is O(mnp)
(Shawe-Taylor and Cristianini, 2004), where m and
n are the lengths of the two strings, respectively and
p is the length of the largest subsequence we want to
consider.
1035
2.2 Tree Kernels
Convolution Tree Kernels compute the number
of common substructures between two trees T1
and T2 without explicitly considering the whole
fragment space. For this purpose, let the set
F = {f1, f2, . . . , f|F|} be a tree fragment space and
?i(n) be an indicator function, equal to 1 if the
target fi is rooted at node n and equal to 0 oth-
erwise. A tree-kernel function over T1 and T2 is
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), NT1
and NT2 are the sets of the T1?s and T2?s nodes,
respectively and ?(n1, n2) = ?|F|i=1 ?i(n1)?i(n2).
The latter is equal to the number of common frag-
ments rooted in the n1 and n2 nodes. The ? func-
tion determines the richness of the kernel space and
thus different tree kernels. Hereafter, we consider
the equation to evaluate STK and PTK 1.
2.2.1 Syntactic Tree Kernels (STK)
To compute STK is enough to compute
?STK(n1, n2) as follows (recalling that since
it is a syntactic tree kernels, each node can be
associated with a production rule): (i) if the
productions at n1 and n2 are different then
?STK(n1, n2) = 0; (ii) if the productions at
n1 and n2 are the same, and n1 and n2 have
only leaf children then ?STK(n1, n2) = ?; and
(iii) if the productions at n1 and n2 are the
same, and n1 and n2 are not pre-terminals then
?STK(n1, n2) = ?
?l(n1)
j=1 (1 + ?STK(c
j
n1 , cjn2)),
where l(n1) is the number of children of n1 and cjn
is the j-th child of the node n. Note that, since the
productions are the same, l(n1) = l(n2) and the
computational complexity of STK is O(|NT1 ||NT2 |)
but the average running time tends to be linear,
i.e.O(|NT1 |+ |NT2 |), for natural language syntactic
trees (Moschitti, 2006a).
2.2.2 The Partial Tree Kernel (PTK)
The computation of PTK is carried out by the
following ?PTK function: if the labels of n1
and n2 are different then ?PTK(n1, n2) = 0; else
?PTK(n1, n2) =
?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(~I1)+d(~I2)
l(~I1)?
j=1
?PTK(cn1(~I1j), cn2(~I2j))
)
1To have a similarity score between 0 and 1, a normalization
in the kernel space, i.e. TK(T1,T2)?
TK(T1,T1)?TK(T2,T2)
is applied.
where d(~I1) = ~I1l(~I1)?~I11+1 and d(~I2) = ~I2l(~I2)?
~I21 + 1. This way, we penalize both larger trees and
child subsequences with gaps. PTK is more general
than the STK as if we only consider the contribu-
tion of shared subsequences containing all children
of nodes, we implement the STK kernel. The com-
putational complexity of PTK is O(p?2|NT1 ||NT2 |)
(Moschitti, 2006a), where p is the largest subse-
quence of children that we want consider and ? is the
maximal outdegree observed in the two trees. How-
ever the average running time again tends to be lin-
ear for natural language syntactic trees (Moschitti,
2006a).
2.3 Lexical Semantic Kernel
Given two text fragments d1 and d2 ? D (the text
fragment set), a general lexical kernel (Basili et al,
2005) defines their similarity as:
K(d1, d2) =
?
w1?d1,w2?d2
(?1?2)? ?(w1, w2) (1)
where ?1 and ?2 are the weights of the words (fea-
tures) w1 and w2 in the documents d1 and d2, re-
spectively, and ? is a term similarity function, e.g.
(Pedersen et al, 2004b; Sahlgren, 2006; Corley and
Mihalcea, 2005; Mihalcea et al, 2005). Technically,
any ? can be used, provided that the resulting Gram
matrix, G = K(d1, d2) ?d1, d2 ? D is positive
semi-definite (Shawe-Taylor and Cristianini, 2004)
(D is typically the training text set).
We determine the term similarity function through
distributional analysis (Pado and Lapata, 2007), ac-
cording to the idea that the meaning of a word can
be described by the set of textual contexts in which it
appears (Distributional Hypothesis, (Harris, 1964)).
The contexts are words appearing in a n-window
with target words: such a space models a generic
notion of semantic relatedness, i.e. two words
close in the space are likely to be either in paradig-
matic or syntagmatic relation as in (Sahlgren, 2006).
The original word-by-word context matrix M is de-
composed through Singular Value Decomposition
(SVD) (Golub and Kahan, 1965) into the product
of three new matrices: U , S, and V so that S is di-
agonal and M = USV T . M is approximated by
Ml = UlSlV Tl in which only the first l columns of
U and V are used, and only the first l greatest singu-
lar values are considered. This approximation sup-
plies a way to project a generic term wi into the l-
1036
dimensional space using W = UlS1/2l , where eachrow corresponds to the representation vectors ~wi.
Therefore, given two words w1 and w2, the term
similarity function ? is estimated as the cosine simi-
larity between the corresponding projections ~w1, ~w2,
i.e ?(w1, w2) = ~w1? ~w2? ~w1?? ~w2? . The latent semantic ker-nels (Siolas and d?Alch Buc, 2000; Cristianini et al,
2001) derive G by applying LSA, resulting in a valid
kernel.
Another methods to design a valid kernel is to rep-
resent words as word vectors and compute ? as their
scalar product between such vectors. For example,
in (Bloehdorn et al, 2006), bag of hyponyms and
hypernyms (up to a certain level of WordNet hierar-
chy) were used to build such vectors. We will refer
to such similarity as WL (word list).
3 Smoothing Partial Tree Kernel (SPTK)
Combining lexical and structural kernels provides
clear advantages on all-vs-all words similarity,
which tends to semantically diverge. Indeed syn-
tax provides the necessary restrictions to com-
pute an effective semantic similarity. Following
this idea, Bloedhorn & Moschitti (2007a) mod-
ified step (i) of ?STK computation as follows:
(i) if n1 and n2 are pre-terminal nodes with
the same number of children, ?STK(n1, n2) =
?
?nc(n1)
j=1 ?(lex(n1), lex(n2)), where lex returns
the node label. This allows to match fragments hav-
ing same structure but different leaves by assigning a
score proportional to the product of the lexical sim-
ilarities of each leaf pair. Although it is an inter-
esting kernel, the fact that lexicals must belong to
the leaf nodes of exactly the same structures limits
its applications. Trivially, it cannot work on depen-
dency trees. Hereafter, we define a much more gen-
eral smoothed tree kernel that can be applied to any
tree and exploit any combination of lexical similari-
ties, respecting the syntax enforced by the tree.
3.1 SPTK Definition
If n1 and n2 are leaves then ??(n1, n2) =
???(n1, n2); else
??(n1, n2) = ??(n1, n2)?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(~I1)+d(~I2)
l(~I1)?
j=1
??(cn1(~I1j), cn2(~I2j))
)
, (2)
where ? is any similarity between nodes, e.g. be-
tween their lexical labels, and the other variables are
the same of PTK.
3.2 Soundness
A completely formal proof of the validity of the
Eq. 2 is beyond the purpose of this paper (mainly
due to space reason). Here we give a first sketch:
let us consider ? as a string matching between
node labels and ? = ? = 1. Each recursive
step of Eq. 2 can be seen as a summation of (1 +
?l(~I1)
j=1 ?STK(cn1(~I1j), cn2(~I2j))), i.e. the ?STK
recursive equation (see Sec. 2.2.1), for all subse-
quences of children cn1(~I1j). In other words, PTK
is a summation of an exponential number of STKs,
which are valid kernels. It follows that PTK is a ker-
nel. Note that the multiplication by ? and ? elevated
to any power only depends on the target fragment.
Thus, it just gives an additional weight to the frag-
ment and does not violate the Mercer?s conditions.
In contrast, the multiplication by ?(n1, n2) does de-
pend on both comparing examples, i.e. on n1 and n2.
However, if the matrix [?(n1, n2)
]
?n1, n2 ? f ? F
is positive semi-definite, a decomposition exists
such that ?(n1, n2) = ?(n1)?(n2) ? ??(n1, n2)
can be written as ?|F|i=1 ?(n1)?i(n1)?(n2)?i(n2)
= ?|F|i=1 ??(n1)??(n2) (see Section 2.2), which
proves SPTK to be a valid kernel.
3.3 Efficient Evaluation
We followed the idea in (Moschitti, 2006a) for effi-
ciently computing SPTK. We consider Eq. 2 evalu-
ated with respect to sequences of different length p;
it follows that
?(n1, n2) = ??(n1, n2)
(
?2 +
m?
p=1
?p(cn1 , cn2)
)
,
where ?p evaluates the number of common sub-
trees rooted in subsequences of exactly p children
(of n1 and n2) and m = min{l(cn1), l(cn2)}.
Given the two child sequences s1a = cn1 and
s2b = cn2 (a and b are the last children)
?p(s1a, s2b) = ?(a, b)?
|s1|?
i=1
|s2|?
r=1
?|s1|?i+|s2|?r ?
??p?1(s1[1 : i], s2[1 : r])
where s1[1 : i] and s2[1 : r] are the child subse-
quences from 1 to i and from 1 to r of s1 and s2. If
we name the double summation term as Dp, we can
1037
S1
SBARQ
.
?::.
SQ
VP
NP
PP
NP
NN
field::n
NN
football::n
DT
a::d
IN
of::i
NP
NN
width::n
DT
the::d
AUX
be::v
WHNP
WP
what::w
Figure 1: Constituent Tree (CT)
rewrite the relation as:
?p(s1a, s2b) =
{
?(a, b)Dp(|s1|, |s2|) if ?(a, b) > 0;
0 otherwise.
Note that Dp satisfies the recursive relation:
Dp(k, l) = ?p?1(s1[1 : k], s2[1 : l]) + ?Dp(k, l ? 1)
+?Dp(k ? 1, l)? ?2Dp(k ? 1, l ? 1)
By means of the above relation, we can compute the
child subsequences of two sequences s1 and s2 in
O(p|s1||s2|). Thus the worst case complexity of the
SPTK is identical to PTK, i.e. O(p?2|NT1 ||NT2 |),
where ? is the maximum branching factor of the two
trees. The latter is very small in natural language
parse trees and we also avoid the computation of
node pairs with non similar labels.
We note that PTK generalizes both (i) SK, allow-
ing the similarity between sequences (node children)
structured in a tree and (ii) STK, allowing the com-
putation of STK over any possible pair of subtrees
extracted from the original tree. For this reason,
we do not dedicate additional space on the defini-
tion of the smoothed SK or smoothed STK, which
are in any case important corollary findings of our
research.
3.4 Innovative Features of SPTK
The most similar kernel to SPTK is the Syntactic
Semantic Tree Kernel (SSTK) proposed in (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b). However, the following aspects show
the remarkable innovativeness of SPTK:
? SSTK can only work on constituency trees
and not on dependency trees (see (Moschitti,
2006a)).
? The lexical similarity in SSTK is only applied
to leaf nodes in exactly the same syntactic
constituents. Only complete matching of the
structure of subtrees is allowed: there is abso-
lutely no flexibility, e.g. the NP structure ?ca-
ble television system? has no match with the
NP ?video streaming system?. SPTK provides
matches between all possible relevant subparts,
e.g. ?television system? and ?video system? (so
also exploiting the meaningful similarity be-
tween ?video? and ?television?).
? The similarity in the PTK equation is added
such that SPTK still corresponds to a scalar
product in the semantic/structure space2.
? We have provided a fast evaluation of SPTK
with dynamic programming (otherwise the
computation would have required exponential
time).
4 Dependency Tree Structures
The feature space generated by the structural ker-
nels, presented in the previous section, obviously de-
pends on the input structures. In case of PTK and
SPTK different tree representations may lead to en-
gineer more or less effective syntactic/semantic fea-
ture spaces. The next two sections provide our repre-
sentation models for dependency trees and their dis-
cussion.
4.1 Proposed Computational Structures
Given the following sentence:
(s1) What is the width of a football field?
The representation tree for a phrase structure
paradigm leaves little room for variations as shown
by the constituency tree (CT) in Figure 1. We ap-
ply lemmatization to the lexicals to improve gener-
alization and, at the same time, we add a generalized
PoS-tag, i.e. noun (n::), verb (v::), adjective (::a), de-
terminer (::d) and so on, to them. This is useful to
measure similarity between lexicals belonging to the
same grammatical category.
In contrast, the conversion of dependency struc-
tures in computationally effective trees (for the
above kernels) is not straightforward. We need to
decide the role of lexicals, their grammatical func-
tions (GR), PoS-tags and dependencies. It is natural
2This is not trivial: for example if sigma is added in Eq. 2 by
only multiplying the ?d1+d2 term, no valid space is generated.
1038
ROOT
VBZ
P
.
?::.
PRD
NN
NMOD
IN
PMOD
NN
field::nNMOD
NN
football::n
NMOD
DT
a::d
of::i
width::nNMOD
DT
the::d
be::vSBJ
WP
what::w
Figure 2: PoS-Tag Centered Tree (PCT)
ROOT
P
.
?::.
PRD
NMOD
PMOD
NN
field::n
NMOD
NN
football::n
NMOD
DT
a::d
IN
of::i
NN
width::n
NMOD
DT
the::d
VBZ
be::v
SBJ
WP
what::w
Figure 3: Grammatical Relation Centered Tree (GRCT)
be::v
VBZROOT?::.
.P
width::n
NNPRDof::i
INNMODfield::n
the::d
DTNMOD
what::w
WPSBJ
NNPMODfootball::n
NNNMOD
a::d
DTNMOD
Figure 4: Lexical Centered Tree (LCT)
to associate edges with dependencies but, since our
kernels cannot process labels on the arcs, they must
be associated with tree nodes. The basic idea of our
structures is to use (i) one of the three kinds of infor-
mation above as central node, from which depen-
be::v
?::.width::n
of::i
field::n
football::na::d
the::d
what::w
Figure 5: Lexical Only Centered Tree (LOCT)
TOP
.
?::.
NN
field::n
NN
football::n
DT
a::d
IN
of::i
NN
width::n
DT
the::d
VBZ
be::v
WP
what::w
Figure 6: Lexical and PoS-Tag Sequences Tree (LPST)
TOP
?::.field::nfootball::na::dof::iwidth::nthe::dbe::vwhat::w
Figure 7: Lexical Sequences Tree (LST)
dencies are drawn and (ii) all the other information
as features (in terms of additional nodes) attached to
the central nodes.
We define three main trees: the PoS-Tag Centered
Tree (PCT), e.g. see Figure 2, where the GR is added
as father and the lexical as a child; the GR Centered
Tree (GRCT), e.g. see Figure 3, where the PoS-Tags
are children of GR nodes and fathers of their associ-
ated lexicals; and the Lexical Centered Tree (LCT),
e.g. see Figure 4, in which both GR and PoS-Tag are
added as the rightmost children.
TOP
ROOT
P
.
?::.
PRD
NMOD
PMOD
NN
goal::n
NMOD
NN
hockey::n
NMOD
NN
ice::n
NMOD
DT
an::d
IN
of::i
NN
dimension::n
NMOD
DT
the::d
VBP
be::v
SBJ
WP
what::w
Figure 8: Grammatical Relation Centered Tree of (s2)
4.2 Comparative Structures
To better study the role of the above dependency
structures, especially from a performance perspec-
tive, we define additional structures: the Lexical
Only Centered Tree (LOCT), e.g. see Figure 5,
which is an LCT only containing lexical nodes; the
Lexical and PoS-Tag Sequences Tree (LPST), e.g.
see Figure 6, which ignores the syntactic structure
of the sentence being a simple sequence of PoS-Tag
nodes, where lexicals are simply added as children;
and the Lexical Sequence Tree (LST), where only
lexical items are leaves of a single root node. PTK
1039
and PSTK applied to it simulates a standard SK and
an SK with smoothing, respectively.
4.3 Structural Features
Section 2 has already described the kind of features
generated by SK, STK and PTK. However, it is
interesting to analyze what happens when SPTK is
applied. For example, given the following sentence
syntactically and semantically similar to s1:
(s2) What is the dimension of an ice hockey goal?
Figure 8 shows the corresponding GRCT, whose
largest PTK fragment shared with the GRTC of s1
(Fig. 3) is: (ROOT (SBJ (WP (what::w))) (PRD (NMOD
(DT (the::d))) (NN) (NMOD (IN (of::i)) (PMOD (NMOD (DT))
(NMOD (NN)) (NN)))) (P (. (?::.)))). If smoothing is ap-
plied the matching is almost total, i.e. also the chil-
dren: width::n/dimension::n, football::n/hockey::n
and field::n/goal::n will be matched (with a smooth-
ing equal to the product of their similarities).
The matching using LCT is very interesting:
without smoothing, the largest subtree is: (be::v
(what::w (SBJ) (WP)) (ROOT)); when smoothing is used
only the fragment (NMOD (NN (ice::n)) will not be part
of the match. This suggests that LCT will probably
receive the major benefit from smoothing. Addition-
ally, with respect to all the above structures, LCT is
the only one that can produce only lexical fragments,
i.e. paths only composed by similar lexical nodes
constrained by syntactic dependencies. All the other
trees produce fragments in which lexicals play the
role of features of GR or PoS-Tag nodes.
5 Experiments
The aim of the experiments is to analyze different
levels of representation, i.e. structure, for syntactic
dependency parses. At the same time, we compare
with the constituency trees and different kernels to
derive the best syntactic paradigm for convolution
kernels. Most importantly, the role of lexical simi-
larity embedded in syntactic structures will be inves-
tigated. For this purpose, we first carry out extensive
experiments on coarse and fine grained QC and then
we verify our findings on a completely different task,
i.e. Argument Classification in SRL.
5.1 General experimental setup
Tools: for SVM learning, we extended the SVM-
LightTK software3 (Moschitti, 2006a) (which in-
3http://disi.unitn.it/moschitti/Tree-Kernel.htm
cludes structural kernels in SVMLight (Joachims,
2000)) with the smooth match between tree nodes.
For generating constituency trees, we used the Char-
niak parser (Charniak, 2000) whereas we applied
LTH syntactic parser (described in (Johansson and
Nugues, 2008a)) to generate dependency trees.
Lexical Similarity: we used the Eq. 1 with ?1 =
?2 = 1 and ? is derived with both approaches de-
scribed in Sec. 2.3. The first approach is LSA-based:
LSA was applied to ukWak (Baroni et al, 2009),
which is a large scale document collection made by
2 billion tokens. More specifically, to build the ma-
trix M, POS tagging is first applied to build rows
with pairs ?lemma, ::POS?, or lemma::POS in brief.
The contexts of such items are the columns of M
and are short windows of size [?3,+3], centered on
the items. This allows for better capturing syntactic
properties of words. The most frequent 20,000 items
are selected along with their 20k contexts. The en-
tries of M are the point-wise mutual information be-
tween them. The SVD reduction is then applied to
M, with a dimensionality cut of l = 250. The sec-
ond approach uses the similarity based on word list
(WL) as provided in (Li and Roth, 2002).
Models: SVM-LightTK is applied to the different
tree representations discussed in Section 4. Since
PTK and SPTK are typically used in our experi-
ments, to have a more compact acronym for each
model, we associate the latter with the name of the
structure, i.e. this indicates that PTK is applied to
it. Then the presence of the subscript WL and LSA
indicates that SPTK is applied along with the corre-
sponding similarity, e.g. LCTWL is the SPTK ker-
nel applied to LCT structure, using WL similarity.
We experiment with multi-classification, which we
model through one-vs-all scheme by selecting the
category associated with the maximum SVM mar-
gin. The quality of such classification is measured
with accuracy. We determine the statistical signi-
cance by using the model described in (Yeh, 2000)
and implemented in (Pado?, 2006).
The parameterization of each classifier is carried on
a held-out set (30% of the training) and concerns
with the setting of the trade-off parameter (option -
c) and the Leaf Weight (LeW ) (see Sec. 5.2), which
is used to linearly scale the contribution of the leaf
nodes. In contrast, the cost-factor parameter of the
SVM-LightTK is set as the ratio between the num-
1040
80% 
82% 
84% 
86% 
88% 
90% 
92% 
0 1000 2000 3000 4000 5000 
Accu
racy
 
Number of Examples 
PCT LPST CT LOCT GRCT LCT BOW 
Figure 9: Learning curves: comparison with no similarity
80% 
82% 
84% 
86% 
88% 
90% 
92% 
94% 
0 1000 2000 3000 4000 5000 
Accu
racy
 
Number of Examples 
PCT-WL LPST-WL CT-WL LOCT-WL GRCT-WL LCT-WL PCT 
Figure 10: Learning curves: comparison with similarity
ber of negative and positive examples for attempting
to have a balanced Precision/Recall.
5.2 QC experiments
For these experiments, we used the UIUC dataset
(Li and Roth, 2002). It is composed by a training
set of 5,452 questions and a test set of 500 ques-
tions4. Question classes are organized in two levels:
6 coarse-grained classes (like ENTITY or HUMAN)
and 50 fine-grained sub-classes (e.g. Plant, Food
as subclasses of ENTITY).
The outcome of the several kernels applied to sev-
eral structures for the coarse and fine grained QC
is reported in Table 1. The first column shows
the experimented models, obtained by applying
PTK/SPTK to the structures described in Sec. 4. The
last two rows are: CT-STK, i.e. STK applied to a
constituency tree and BOW, which is a linear ker-
4http://cogcomp.cs.illinois.edu/Data/QA/QC/
nel applied to lexical vectors. Column 2, 3 and 4
report the accuracy using no, LSA and WL similar-
ity, where LeW is the amplifying parameter, i.e. a
weight associated with the leaves in the tree. The
last three columns refer to the fine grained task.
It is worth nothing that when no similarity is ap-
plied: (i) BOW produces high accuracy, i.e. 88.8%
but it is improved by STK (the current state-of-the-
art5 in QC (Zhang and Lee, 2003; Moschitti et al,
2007)); (ii) PTK applied to the same tree of STK
produces a slightly lower value (non-statistically
significant difference); (iii) interestingly, when PTK
is instead applied to dependency structures, it im-
proves STK, i.e. 91.60% vs 91.40% (although not
significantly); and (iv) LCT, strongly based on lexi-
cal nodes, is the least accurate, i.e 90.80% since it is
obviously subject to data sparseness (fragments only
composed by lexicals are very sparse).
The very important results can be noted when lex-
ical similarity is used, i.e. SPTK is applied: (a) all
the syntactic-base structures using both LSA or WL
improve the classification accuracy. (b) CT gets the
lowest improvement whereas LCT achieves an im-
pressive result of 94.80%, i.e more than 41% of rel-
ative error reduction. It seems that the lexical similar
paths when driven by syntax produces accurate fea-
tures. Indeed, when syntax is missing such as for the
unstructured lexical path of LSTLSA, the accuracy
does not highly improve or may also decrease. Ad-
ditionally, the result of our best model is so high that
its errors only refer to questions like What did Jesse
Jackson organize ?, where the classifier selected En-
tity instead ofHuman category. These refer to clear
cases where a huge amount of background knowl-
edge is needed for deriving the exact solution.
Finally, on the fine grained experiments LCT
still produces the most accurate outcome again ex-
ceeding the state-of-the-art (Zhang and Lee, 2003),
where WL significantly improves on all models (CT
included).
5.3 Learning curves
It is interesting to study the impact of syntac-
tic/semantic kernels on the learning generalization.
For this purpose, Fig. 9 reports the learning curve
5Note that in (Bloehdorn and Moschitti, 2007b), higher ac-
curacy values for smoothed STK are shown for different param-
eters but the best according to a validation set is not highlighted.
1041
COARSE FINE
NO LSA WL NO LSA WL
LeW Acc. LeW Acc. LeW Acc. LeW Acc. LeW Acc. LeW Acc.
CT 4 90.80% 2 91.00% 5 92.20% 4 84.00% 5 83.00% 7 86.60%
GRCT 3 91.60% 4 92.60% 2 94.20% 3 83.80% 4 83.20% 2 85.00%
LCT 1 90.80% 1 94.80% 1 94.20% 0.33 85.40% 1 86.20% 0.33 87.40%
LOCT 1 89.20% 1 93.20% 1 91.80% 1 85.40% 1 86.80% 1 87.00%
LST 1 88.20% 1 85.80% 1 89.60% 1 84.00% 1 80.00% 1 85.00%
LPST 3 89.40% 1 89.60% 1 92.40% 3 84.20% 4 82.20% 1 84.60%
PCT 4 91.20% 4 92.20% 5 93.40% 4 84.80% 5 84.00% 5 85.20%
CT-STK - 91.20% - - - - - 82.20% - - - -
BOW - 88.80% - - - - - 83.20% - - - -
Table 1: Accuracy of structural several kernels on different structures for coarse and fine grained QC
y = 0.051x2.005 
y = 0.030x1.609 
y = 0.068x1.213 
y = 0.081x1.705 
0 
20 
40 
60 
80 
100 
120 
0 10 20 30 40 50 60 
micr
osec
onds
 
Number of Nodes 
LPST-WL GRCT-WL GRCT LCT-WL LCT LPST 
Figure 11: Micro-seconds for each kernel computation
of the previous models without lexical similarity
whereas Fig. 10 shows the complete SPTK behavior
through the different structures. We note that when
no similarity is used the dependency trees better
generalize than constituency trees or non-syntactic
structures like LPST or BOW. When WL is acti-
vated, all models outperform the best kernel of the
previous pool, i.e. PCT (see dashed line of Fig. 10
or the top curve in Fig. 9).
5.4 Kernel Efficiency
We plotted the average running time of each compu-
tation of PTK/SPTK applied to the different struc-
tures. We divided the examples from QC based
on the number of nodes in each example. Fig-
ure 11 shows the elapsed time in function of the
number of nodes for different tree representations.
We note that: (i) when the WL is not active, LCT
and GRCT are very fast as they impose hierarchical
matching of subtrees; (ii) when the similarity is ac-
tivated, LCTWL and GRCTWL tend to match many
more tree fragments thus their complexity increases.
However, the equations of the curve fit, shown in the
figure, suggests that the trend is sub-quadratic (x1.7).
Only LPSTWL, which has no structure, matches a
very large number of sequences of nodes, when the
similarity is active. This increases the complexity,
which results in an order higher than 2.
5.5 FrameNet Role Classification Experiments
To verify that our findings are general and that our
syntactic/semantic dependency kernels can be effec-
tively exploited for diverse NLP tasks, we experi-
mented with a completely different application, i.e.
FrameNet SRL classification (gold standard bound-
aries). We used the FrameNet version 1.3 with
the 90/10% split between training and test set (i.e
271,560 and 30,173 examples respectively), as de-
fined in (Johansson and Nugues, 2008b), one of the
best system for FrameNet parsing. We used the LTH
dependency parser. LSA was applied to the BNC
corpus, the source of the FrameNet annotations.
For each of 648 frames, we applied SVM along
with the best models for QC, i.e. GRCT and LCT, to
learn its associated binary role classifiers (RC) for
a total of 4,254 classifiers. For example, Figure 12
shows the LCT representation of the first two roles
of the following sentence:
[Bootleggers]CREATOR, then copy [the film]ORIGINAL
[onto hundreds of V HS tapes]GOAL
Table 2 shows the results of the different multi-
classifiers. GRCT and LCT show a large ac-
curacy, i.e. 87.60. This improves up to 88.74
by activating the LSA similarity. The combina-
tion GRCTLSA+LCTLSA significantly improves the
above model, achieving 88.91%. This is very close
to the state-of-the-art of SRL for classification (us-
ing a single classifier, i.e. no joint model), i.e.
89.6%, achieved in (Johansson and Nugues, 2008b).
1042
copy::v
VBPROOTbootlegger::n
NNSSBJ
copy::v
VBPROOTfilm::n
NNOBJthe::d
DTNMOD
Figure 12: LCT Examples for argument roles
Kernel Accuracy
GRCT 87.60%
GRCTLSA 88,61%
LCT 87.61%
LCTLSA 88.74%
GRCT + LCT 87.99%
GRCTLSA + LCTLSA 88.91%
Table 2: Argument Classification Accuracy
Finally, it should be noted that, to learn and test the
SELF MOTION multi-classifier, containing 14,584
examples, distributed on 22 roles, SVM-SPTK em-
ployed 1.5 h and 10 minutes, respectively6.
6 Final Remarks and Conclusion
In this paper, we have proposed a study on repre-
sentation of dependency structures for the design of
effective structural kernels. Most importantly, we
have defined a new class of kernel functions, i.e. SP-
TKs, that carry out syntactic and lexical similarities
on the above structures. SPTK exploits the latter
by providing generalization trough lexical similar-
ities constrained in them. This allows for automat-
ically generating feature spaces of generalized syn-
tactic/semantic dependency substructures.
To test our models, we carried out experiments
on QC and SRL. These show that by exploiting the
similarity between two sets of words carried out ac-
cording to their dependency structure leads to an un-
precedented result for QC, i.e. 94.8% of accuracy.
In contrast, when no structure is used the accuracy
does not significantly improves. We have also pro-
vided a fast algorithm for the computation of SPTK
and empirically shown that it can easily scale.
It should be noted that our models are not abso-
lutely restricted to QC and SRL. Indeed, since most
of the NLP applications are based on syntactic and
lexical representations, SPTK will have a major im-
pact in most of them, e.g.:
6Using one of the 8 processors of an Intel(R) Xeon(R) CPU
E5430 @ 2.66GHz machine, 32Gb Ram.
? Question Answering, the high results for QC
will positively impact on the overall task.
? SRL, SPTK alone reaches the state-of-the-art
(SOA) (only 0.7% less) in FrameNet role clas-
sification. This is very valuable as previous
work showed that tree kernels (TK) alone per-
form lower than models based on manually en-
gineered features for SRL tasks, e.g., (Mos-
chitti, 2004; Giuglea and Moschitti, 2004; Giu-
glea and Moschitti, 2006; Moschitti, 2006b;
Che et al, 2006; Moschitti et al, 2008). Thus
for the first time in an SRL task, a general
tree kernel reaches the same accuracy of heavy
manual feature design. This also suggests an
improvement when used in combinations with
manual feature vectors.
? Relation Extraction and Pronominal Corefer-
ence, whose state-of-the-art for some tasks is
achieved with the simple STK-CT (see (Zhang
et al, 2006) and (Yang et al, 2006; Versley et
al., 2008), respectively).
? In word sense disambiguation tasks, SPTK can
generalize context according to syntactic and
semantic constraints (selectional restrictions)
making very effective distributional semantic
approaches.
? In Opinion Mining SPTK will allow to match
sentiment words within their corresponding
syntactic counterparts and improve the state-
of-the-art (Johansson and Moschitti, 2010b; Jo-
hansson and Moschitti, 2010a).
? Experiments on Recognizing Textual Entail-
ment (RTE) tasks, the use of SSTK (in-
stead of STK-CT) improved the state-of-the-art
(Mehdad et al, 2010). SPTK may provide fur-
ther enhancement and innovative and effective
dependency models.
The above points also suggest many promising fu-
ture research directions, which we would like to ex-
plore.
Acknowledgements
This work has been partially supported by the EC
project FP247758: Trustworthy Eternal Systems via
Evolving Software, Data and Knowledge (EternalS).
1043
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of CoNLL-
2005, pages 1?8, Ann Arbor, Michigan. Association
for Computational Linguistics.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Proceedings of ECIR 2007, Rome, Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels. In
In Proceedings of CIKM ?07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and
Alessandro Moschitti. 2006. Semantic kernels for text
classification based on topological measures of feature
similarity. In Proceedings of ICDM 06, Hong Kong,
2006.
Ulrik Brandes. 2001. A Faster Algorithm for Between-
ness Centrality. Journal of Mathematical Sociology,
25:163?177.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Horst Bunke and Kim Shearer. 1998. A graph distance
metric based on the maximal common subgraph. Pat-
tern Recogn. Lett., 19(3-4):255?259, March.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
O. Chapelle, B. Schlkopf, and A. Zien. 2006. Semi-
Supervised Learning. Adaptive computation and ma-
chine learning. MIT Press, Cambridge, MA, USA, 09.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00.
Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.
2006. A hybrid convolution tree kernel for semantic
role labeling. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ?06,
pages 73?80, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings of
the ACL Workshop on Empirical Modeling of Semantic
Equivalence and Entailment, pages 13?18, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Jim Cowie, Joe Guthrie, and Louise Guthrie. 1992. Lex-
ical disambiguation using simulated annealing. In in
COLING, pages 359?365.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2001. Latent semantic kernels. In Carla Brodley and
Andrea Danyluk, editors, Proceedings of ICML-01,
18th International Conference on Machine Learning,
pages 66?73, Williams College, US. Morgan Kauf-
mann Publishers, San Francisco, US.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423?429, Barcelona, Spain, July.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and
Inderjit S. Dhillon. 2007. Information-theoretic met-
ric learning. In Proceedings of the 24th international
conference on Machine learning, ICML ?07, pages
209?216, New York, NY, USA. ACM.
Linton C. Freeman. 1977. A Set of Measures of Central-
ity Based on Betweenness. Sociometry, 40(1):35?41.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
In Proceedings of EMNLP ?09, pages 11?20, Morris-
town, NJ, USA.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovering using FrameNet, VerbNet and
PropBank. In In Proceedings of the Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
A.-M. Giuglea and A. Moschitti. 2006. Semantic role
labeling via framenet, verbnet and propbank. In Pro-
ceedings of ACL, Sydney, Australia.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
G. Golub and W. Kahan. 1965. Calculating the singular
values and pseudo-inverse of a matrix. Journal of the
Society for Industrial and Applied Mathematics: Se-
ries B, Numerical Analysis, 2(2):pp. 205?224.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics. Oxford University Press.
1044
J. J. Jiang and D. W. Conrath. 1997. Semantic Similarity
Based on Corpus Statistics and Lexical Taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X).
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proceedings of
ICML?00.
Richard Johansson and Alessandro Moschitti. 2010a.
Reranking models in fine-grained opinion analysis. In
Proceedings of the 23rd International Conference of
Computational Linguistics (Coling 2010), pages 519?
527, Beijing, China.
Richard Johansson and Alessandro Moschitti. 2010b.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden.
Richard Johansson and Pierre Nugues. 2008a.
Dependency-based syntactic?semantic analysis with
PropBank and NomBank. In CoNLL 2008: Proceed-
ings of the Twelfth Conference on Natural Language
Learning, pages 183?187, Manchester, United King-
dom.
Richard Johansson and Pierre Nugues. 2008b. The effect
of syntactic representation on semantic role labeling.
In Proceedings of COLING, Manchester, UK, August
18-22.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Claudia Leacock and Martin Chodorow, 1998. Combin-
ing Local Context and WordNet Similarity for Word
Sense Identification, chapter 11, pages 265?283. The
MIT Press.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL?02.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In HLT-NAACL,
pages 1020?1028.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2005. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence (AAAI
2006), Boston, July.
Rada Mihalcea. 2005. unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In In HLT/EMNLP
2005, pages 411?418.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL?07.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
A. Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of ACL,
Barcelona, Spain.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Alessandro Moschitti. 2006b. Making tree kernels prac-
tical for natural language learning. In Proccedings of
EACL?06.
Roberto Navigli and Mirella Lapata. 2010. An Experi-
mental Study of Graph Connectivity for Unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 32(4):678?
692.
Sebastian Pado and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004a. WordNet::Similarity - Measuring the Re-
latedness of Concept. In Proc. of 5th NAACL, Boston,
MA.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004b. Wordnet::similarity - measuring the re-
latedness of concepts. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence, pages 448?453.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Hinrich Schtze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97?
123.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Empirical Methods for Natural Language Processing
(EMNLP), pages 89?96, Sapporo, Japan.
Georges Siolas and Florence d?Alch Buc. 2000. Sup-
port vector machines based on a semantic kernel for
1045
text categorization. In Proceedings of the IEEE-INNS-
ENNS International Joint Conference on Neural Net-
works (IJCNN?00)-Volume 5, page 5205. IEEE Com-
puter Society.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In The 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing?08), Manchester, England.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In 32nd. Annual Meeting of the
Association for Computational Linguistics, pages 133
?138, New Mexico State University, Las Cruces, New
Mexico.
Xiaofeng Yang, Jian Su, and Chewlim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proc. COLING-ACL 06.
Alexander S. Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In COLING,
pages 947?953.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings of
the 26th annual international ACM SIGIR conference
on Research and development in informaion retrieval,
pages 26?32. ACM Press.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
Peixiang Zhao, Jiawei Han, and Yizhou Sun. 2009. P-
Rank: a comprehensive structural similarity measure
over information networks. In CIKM ?09: Proceed-
ing of the 18th ACM conference on Information and
knowledge management, pages 553?562, New York,
NY, USA. ACM.
1046
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 237?246,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Towards Open-Domain Semantic Role Labeling
Danilo Croce, Cristina Giannone, Paolo Annesi, Roberto Basili
{croce,giannone,annesi,basili}@info.uniroma2.it
Department of Computer Science, Systems and Production
University of Roma, Tor Vergata
Abstract
Current Semantic Role Labeling technolo-
gies are based on inductive algorithms
trained over large scale repositories of
annotated examples. Frame-based sys-
tems currently make use of the FrameNet
database but fail to show suitable general-
ization capabilities in out-of-domain sce-
narios. In this paper, a state-of-art system
for frame-based SRL is extended through
the encapsulation of a distributional model
of semantic similarity. The resulting argu-
ment classification model promotes a sim-
pler feature space that limits the potential
overfitting effects. The large scale em-
pirical study here discussed confirms that
state-of-art accuracy can be obtained for
out-of-domain evaluations.
1 Introduction
The availability of large scale semantic lexicons,
such as FrameNet (Baker et al, 1998), allowed the
adoption of a wide family of learning paradigms
in the automation of semantic parsing. Building
upon the so called frame semantic model (Fill-
more, 1985), the Berkeley FrameNet project has
developed a semantic lexicon for the core vocab-
ulary of English, since 1997. A frame is evoked
in texts through the occurrence of its lexical units
(LU ), i.e. predicate words such verbs, nouns, or
adjectives, and specifies the participants and prop-
erties of the situation it describes, the so called
frame elements (FEs).
Semantic Role Labeling (SRL) is the task of
automatic recognition of individual predicates to-
gether with their major roles (e.g. frame ele-
ments) as they are grammatically realized in in-
put sentences. It has been a popular task since
the availability of the PropBank and FrameNet an-
notated corpora (Palmer et al, 2005), the seminal
work of (Gildea and Jurafsky, 2002) and the suc-
cessful CoNLL evaluation campaigns (Carreras
and Ma`rquez, 2005). Statistical machine learning
methods, ranging from joint probabilistic models
to support vector machines, have been success-
fully adopted to provide very accurate semantic
labeling, e.g. (Carreras and Ma`rquez, 2005).
SRL based on FrameNet is thus not a novel task,
although very few systems are known capable of
completing a general frame-based annotation pro-
cess over raw texts, noticeable exceptions being
discussed for example in (Erk and Pado, 2006),
(Johansson and Nugues, 2008b) and (Coppola et
al., 2009). Some critical limitations have been out-
lined in literature, some of them independent from
the underlying semantic paradigm.
Parsing Accuracy. Most of the employed
learning algorithms are based on complex sets of
syntagmatic features, as deeply investigated in (Jo-
hansson and Nugues, 2008b). The resulting recog-
nition is thus highly dependent on the accuracy of
the underlying parser, whereas wrong structures
returned by the parser usually imply large misclas-
sification errors.
Annotation costs. Statistical learning ap-
proaches applied to SRL are very demanding with
respect to the amount and quality of the train-
ing material. The complex SRL architectures
proposed (usually combining local and global,
i.e. joint, models of argument classification, e.g.
(Toutanova et al, 2008)) require a large number
of annotated examples. The amount and quality of
the training data required to reach a significant ac-
curacy is a serious limitation to the exploitation of
SRL in many NLP applications.
Limited Linguistic Generalization. Several
studies showed that even when large training
sets exist the corresponding learning exhibits
poor generalization power. Most of the CoNLL
2005 systems show a significant performance drop
when the tested corpus, i.e. Brown, differs from
237
the training one (i.e. Wall Street Journal), e.g.
(Toutanova et al, 2008). More recently, the state-
of-art frame-based semantic role labeling system
discussed in (Johansson and Nugues, 2008b) re-
ports a 19% drop in accuracy for the argument
classification task when a different test domain is
targeted (i.e. NTI corpus). Out-of-domain tests
seem to suggest the models trained on BNC do not
generalize well to novel grammatical and lexical
phenomena. As also suggested in (Pradhan et al,
2008), the major drawback is the poor generaliza-
tion power affecting lexical features. Notice how
this is also a general problem of statistical learning
processes, as large fine grain feature sets are more
exposed to the risks of overfitting.
The above problems are particularly critical
for frame-based shallow semantic parsing where,
as opposed to more syntactic-oriented semantic
labeling schemes (as Propbank (Palmer et al,
2005)), a significant mismatch exists between the
semantic descriptors and the underlying syntac-
tic annotation level. In (Johansson and Nugues,
2008b) an upper bound of about 83.9% for the ac-
curacy of the argument identification task is re-
ported, it is due to the complexity in projecting
frame element boundaries out from the depen-
dency graph: more than 16% of the roles in the
annotated material lack of a clear grammatical sta-
tus.
The limited level of linguistic generalization
outlined above is still an open research problem.
Existing solutions have been proposed in litera-
ture along different lines. Learning from richer
linguistic descriptions of more complex structures
is proposed in (Toutanova et al, 2008). Limit-
ing the cost required for developing large domain-
specific training data sets has been also studied,
e.g., (Fu?rstenau and Lapata, 2009). Finally, the ap-
plication of semi-supervised learning is attempted
to increase the lexical expressiveness of the model,
e.g. (Goldberg and Elhadad, 2009).
In this paper, this last direction is pursued. A
semi-supervised statistical model exploiting use-
ful lexical information from unlabeled corpora is
proposed. The model adopts a simple feature
space by relying on a limited set of grammati-
cal properties, thus reducing its learning capac-
ity. Moreover, it generalizes lexical information
about the annotated examples by applying a ge-
ometrical model, in a Latent Semantic Analysis
style, inspired by a distributional paradigm (Pado
and Lapata, 2007). As we will see, the accu-
racy reachable through a restricted feature space is
still quite close to the state-of-art, but interestingly
the performance drops in out-of-domain tests are
avoided.
In the following, after discussing existing ap-
proaches to SRL (Section 2), a distributional ap-
proach is defined in Section 3. Section 3.2 dis-
cusses the proposed HMM-based treatment of
joint inferences in argument classification. The
large scale experiments described in Section 4 will
allow to draw the conclusions of Section 5.
2 Related Work
State-of-art approaches to frame-based SRL are
based on Support Vector Machines, trained over
linear models of syntactic features, e.g. (Jo-
hansson and Nugues, 2008b), or tree-kernels, e.g.
(Coppola et al, 2009). SRL proceeds through two
main steps: the localization of arguments in a sen-
tence, called boundary detection (BD), and the as-
signment of the proper role to the detected con-
stituents, that is the argument classification, (AC)
step. In (Toutanova et al, 2008) a SRL model
over Propbank that effectively exploits the seman-
tic argument frame as a joint structure, is pre-
sented. It incorporates strong dependencies within
a comprehensive statistical joint model with a rich
set of features over multiple argument phrases.
This approach effectively introduces a new step
in SRL, also called Joint Re-ranking, (RR), e.g.
(Toutanova et al, 2008) or (Moschitti et al, 2008).
First local models are applied to produce role
labels over individual arguments, then the joint
model is used to decide the entire argument se-
quence among the set of the n-best competing
solutions. While these approaches increase the
expressive power of the models to capture more
general linguistic properties, they rely on com-
plex feature sets, are more demanding about the
amount of training information and increase the
overall exposure to overfitting effects.
In (Johansson and Nugues, 2008b) the impact of
different grammatical representations on the task
of frame-based shallow semantic parsing is stud-
ied and the poor lexical generalization problem
is outlined. An argument classification accuracy
of 89.9% over the FrameNet (i.e. BNC) dataset
is shown to decrease to 71.1% when a different
test domain is evaluated (i.e. the Nuclear Threat
Initiative corpus). The argument classification
238
component is thus shown to be heavily domain-
dependent whereas the inclusion of grammatical
function features is just able to mitigate this sen-
sitivity. In line with (Pradhan et al, 2008), it is
suggested that lexical features are domain specific
and their suitable generalization is not achieved.
The lack of suitable lexical information is also
discussed in (Fu?rstenau and Lapata, 2009) through
an approach aiming to support the creation of
novel annotated resources. Accordingly a semi-
supervised approach for reducing the costs of the
manual annotation effort is proposed. Through a
graph alignment algorithm triggered by annotated
resources, the method acquires training instances
from an unlabeled corpus also for verbs not listed
as existing FrameNet predicates.
2.1 The role of Lexical Semantic Information
It is widely accepted that lexical information (as
features directly derived from word forms) is cru-
cial for training accurate systems in a number of
NLP tasks. Indeed, all the best systems in the
CoNLL shared task competitions (e.g. Chunk-
ing (Tjong Kim Sang and Buchholz, 2000)) make
extensive use of lexical information. Also lexi-
cal features are beneficial in SRL usually either
for systems on Propbank as well as for FrameNet-
based annotation.
In (Goldberg and Elhadad, 2009), a different
strategy to incorporate lexical features into clas-
sification models is proposed. A more expres-
sive training algorithm (i.e. anchored SVM) cou-
pled with an aggressive feature pruning strategy
is shown to achieve high accuracy over a chunk-
ing and named entity recognition task. The sug-
gested perspective here is that effective semantic
knowledge can be collected from sources exter-
nal to the annotated corpora (very large unanno-
tated corpora or on manually constructed lexical
resources) rather than learned from the raw lexi-
cal counts of the annotated corpus. Notice how
this is also the strategy pursued in recent work on
deep learning approaches to NLP tasks. In (Col-
lobert and Weston, 2008) a unified architecture
for Natural Language Processing that learns fea-
tures relevant to the tasks at hand given very lim-
ited prior knowledge is presented. It embodies the
idea that a multitask learning architecture coupled
with semi-supervised learning can be effectively
applied even to complex linguistic tasks such as
SRL. In particular, (Collobert and Weston, 2008)
proposes an embedding of lexical information us-
ing Wikipedia as source, and exploiting the result-
ing language model within the multitask learning
process. The idea of (Collobert and Weston, 2008)
to obtain an embedding of lexical information by
acquiring a language model from unlabeled data is
an interesting approach to the problem of perfor-
mance degradation in out-of-domain tests, as al-
ready pursued by (Deschacht and Moens, 2009).
The extensive use of unlabeled texts allows to
achieve a significant level of lexical generalization
that seems better capitalize the smaller annotated
data sets.
3 A Distributional Model for Argument
Classification
High quality lexical information is crucial for ro-
bust open-domain SRL, as semantic generaliza-
tion highly depends on lexical information. For
example, the following two sentences evoke the
STATEMENT frame, through the LUs say and
state, where the FEs, SPEAKER and MEDIUM, are
shown.
[President Kennedy] SPEAKER said to an astronaut, ?Man
is still the most extraordinary computer of all.? (1)
[The report] MEDIUM stated, that some problems needed
to be solved. (2)
In sentence (1), for example, President Kennedy
is the grammatical subject of the verb say and
this justifies its role of SPEAKER. However, syn-
tax does not entirely characterize argument seman-
tics. In (1) and (2), the same syntactic relation is
observed. It is the semantics of the grammatical
heads, i.e. report and Kennedy, the main respon-
sible for the difference between the two resulting
proto-agentive roles, SPEAKER and MEDIUM.
In this work we explore two different aspects.
First, we propose a model that does not depend
on complex syntactic information in order to min-
imize the risk of overfitting. Second, we improve
the lexical semantic information available to the
learning algorithm. The proposed ?minimalistic?
approach will consider only two independent fea-
tures:
? the semantic head (h) of a role, as it can
be observed in the grammatical structure. In
sentence (2), for example, the MEDIUM FE is
realized as the logical subject, whose head is
report.
239
? the dependency relation (r) connecting the
semantic head to the predicate words. In (2),
the semantic head report is connected to the
LU stated through the subject (SBJ) relation.
In the rest of the section the distributional model
for the argument classification step is presented.
A lexicalized model for individual semantic roles
is first defined in order to achieve robust seman-
tic classification local to each argument. Then a
Hidden Markov Model is introduced in order to
exploit the local probability estimators, sensitive
to lexical similarity, as well as the global informa-
tion on the entire argument sequence.
3.1 Distributional Local Models
As the classification of semantic roles is strictly
related to the lexical meaning of argument heads,
we adopt a distributional perspective, where the
meaning is described by the set of textual con-
texts in which words appear. In distributional
models, words are thus represented through vec-
tors built over these observable contexts: similar
vectors suggest semantic relatedness as a func-
tion of the distance between two words, capturing
paradigmatic (e.g. synonymy) or syntagmatic re-
lations (Pado, 2007). Vectors
??
h are described by
an adjacency matrix M , whose rows describe tar-
get words (h) and whose columns describe their
corpus contexts. Latent Semantic Analysis (LSA)
(Landauer and Dumais, 1997), is then applied to
M to acquire meaningful representations
??
h . LSA
exploits the linear transformation called Singular
Value Decomposition (SVD) and produces an ap-
proximation of the original matrix M , capturing
(semantic) dependencies between context vectors.
M is replaced by a lower dimensional matrix Ml,
capturing the same statistical information in a new
l-dimensional space, where each dimension is a
linear combination of some of the original fea-
tures (i.e. contexts). These derived features may
be thought as artificial concepts, each one repre-
senting an emerging meaning component, as the
linear combination of many different words.
In the argument classification task, the similar-
ity between two argument heads h1 and h2 ob-
served in FrameNet can be computed over
??
h1 and??
h2. The model for a given frame element FEk
is built around the semantic heads h observed in
the role FEk: they form a set denoted by HFE
k
.
These LSA vectors
??
h express the individual an-
notated examples as they are immerse in the LSA
Role, FEk Clusters of semantic heads
MEDIUM
c1: {article, report, statement}
c2: {constitution, decree, rule}
SPEAKER
c3: {brother, father, mother, sister }
c4: {biographer, philosopher, ....}
c5: {he, she, we, you}
c6: {friend}
TOPIC
c7: {privilege, unresponsiveness}
c8: {pattern}
Table 1: Clusters of semantic heads in the Subj
position for the frame STATEMENT with ? = 0.5
space acquired from the unlabeled texts. More-
over, given FEk, a model for each individual syn-
tactic relation r (i.e. that links h labeled as FEk
to their corresponding predicates) is a partition of
the set HFE
k
called HFE
k
r , i.e. the subset of
HFE
k
produced by examples of the relation r (e.g.
Subj). Given the annotated sentence (2), we have
that report ? HMEDIUMSBJ .
As the LSA vectors
??
h are available for the se-
mantic heads h, a vector representation
???
FEk for
the role FEk can be obtained from the annotated
data. However, one single vector is a too simplis-
tic representation given the rich nature of seman-
tic roles FEk. In order to better represent FEk,
multiple regions in the semantic space are used.
They are obtained by a clustering process applied
to the set HFE
k
r according to the Quality Thresh-
old (QT) algorithm (Heyer et al, 1999). QT is a
generalization of k-mean where a variable number
of clusters can be obtained. This number depends
on the minimal value of intra-cluster similarity ac-
cepted by the algorithm and controlled by a pa-
rameter, ?: lower values of ? correspond to more
heterogeneous (i.e. larger grain) clusters, while
values close to 1 characterize stricter policies and
more fine-grained results. Given a syntactic rela-
tion r, CFE
k
r denotes the clusters derived by QT
clustering over HFE
k
r . Each cluster c ? C
FEk
r
is represented by a vector ??c , computed as the
geometric centroid of its semantic heads h ? c.
For a frame F , clusters define a geometric model
of every frame elements FEk: it consists of cen-
troids ??c with c ? HFE
k
r . Each c represents FE
k
through a set of similar heads, as role fillers ob-
served in FrameNet. Table 1 represents clusters
for the heads HFE
k
Subj of the STATEMENT frame.
In argument classification we assume that the
evoking predicate word for the frame F in an
input sentence s is known. A sentence s can
be seen as a sequence of role-relation pairs:
240
s = {(r1, h1), ..., (rn, hn)} where the heads hi
are in the syntactic relation ri with the underlying
lexical unit of F .
For every head h in s, the vector
??
h can be then
used to estimate its similarity with the different
candidate roles FEk. Given the syntactic relation
r, the clusters c ? CFE
k
r whose centroid vector ~c
is closer to ~h are selected. Dr,h is the set of the
representations semantically related to h:
Dr,h =
?
k
{ckj ? C
FEk
r |sim(h, ckj) ? ?} (3)
where the similarity between the j-th cluster for
the FEk, i.e. ckj ? CFE
k
r , and h is the usual
cosine similarity: simcos(h, ckj) =
??
h ???c kj
?
??
h ? ???c kj?
Then, through a k-nearest neighbours (k-NN)
strategy withinDr,h, them clusters ckj most simi-
lar to h are retained in the set D(m)r,h . A probabilis-
tic preference for the role FEk is estimated for h
through a cluster-based voting scheme,
prob(FEk|r, h) =
|CFE
k
r ?D
(m)
r,h |
|D(m)r,h |
(4)
or, alternatively, an instance-based one overD(m)r,h :
prob(FEk|r, h) =
?
c?CFEkr ?D
(m)
r,h
|c|
?
c?D(m)r,h
|c|
(5)
In Fig. 1 the preference estimation for the
incoming head h = professor connected to
a LU by the Subj relation is shown. Clus-
ters for the heads in Table 1 are also reported.
First, in the set of clusters whose similarity
with professor is higher than a threshold ? the
m = 5 most similar clusters are selected. Ac-
cordingly, the preferences given by Eq. 4 are
prob(SPEAKER|SBJ, h) = 3/5, prob(MEDIUM|SBJ, h) =
2/5 and prob(TOPIC|SBJ, h) = 0. The strategy mod-
eled by Eq. 5 amplifies the role of larger
clusters, e.g. prob(SPEAKER|SBJ, h) = 9/14 and
prob(MEDIUM|SBJ, h) = 5/14. We call Distribu-
tional, the model that applies Eq. 5 to the source
(r, h) arguments, by rejecting cases only when no
information about the head h is available from the
unlabeled corpus or no example of relation r for
the role FEk is available from the annotated cor-
pus. Eq. 4 and 5 in fact do not cover all possible
cases. Often the incoming head h or the relation r
may be unavailable:
1. If the head h has never been met in the un-
labeled corpus or the high grammatical am-
biguity of the sentence does not allow to
locate it reliably, Eq. 4 (or 5) should be
backed off to a purely syntactic model, that
is prob(FEk|r)
2. If the relation r can not be properly located
in s, h is also unknown: the prior probability
of individual arguments, i.e. prob(FEk), is
here employed.
Both prob(FEk|r) and prob(FEk) can be esti-
mated from the training set and smoothing can be
also applied1. A more robust argument preference
function for all arguments (ri, hi) ? s of the frame
F is thus given by:
prob(FEk|ri, hi) = ?1prob(FE
k|ri, hi) +
?2prob(FE
k|ri) + ?3prob(FE
k) (6)
where weights ?1, ?2, ?3 can be heuristically as-
signed or estimated from the training set2. The
resulting model is hereafter called Backoff model:
although simply based on a single feature (i.e. the
syntactic relation r), it accounts for information at
different reliability degrees.
3.2 A Joint Model for Argument
Classification
Eq. 6 defines roles preferences local to individual
arguments (ri, hi). However, an argument frame
is a joint structure, with strong dependencies be-
tween arguments. We thus propose to model the
reranking phase (RR) as a HMM sequence label-
ing task. It defines a stochastic inference over
multiple (locally justified) alternative sequences
through a Hidden Markov Model (HMM). It in-
fers the best sequence FE(k1,...,kn) over all the
possible hidden state sequences (i.e. made by the
target FEki) given the observable emissions, i.e.
the arguments (ri, hi). Viterbi inference is applied
to build the best (role) interpretation for the input
sentence.
Once Eq. 6 is available, the best frame element
sequence FE(?(1),...,?(n)) for the entire sentence s
can be selected by defining the function ?(?) that
maps arguments (ri, hi) ? s to frame elements
FEk:
?(i) = k s.t. FEk ? F (7)
1Lindstone smoothing was applied with ? = 1.
2In each test discussed hereafter, ?1, ?2, ?3 were assigned
to .9,.09 and .01, in order to impose a strict priority to the
model contributions.
241
report
statement
article
survey
review
constitution
decree
rule
translator
archaeologist
philosopher
biographer
friend
pattern
president
king
sister
mother
brother
father
we
she
he
you
MEDIUM
SPEAKER
TOPIC
target head
professor
manifesto
privilege
unresponsiveness
Figure 1: A k-NN approach to the role classification for hi = professor
Notice that different transfer functions ?(?)
are usually possible. By computing their prob-
ability we can solve the SRL task by select-
ing the most likely interpretation, ??(?), via
argmax? P
(
?(?) | s
)
, as follows:
??(?) = argmax
?
P
(
s|?(?)
)
P
(
?(?)
)
(8)
In Eq. 8, the emission probability P
(
s|?(?)
)
and
the transition probability P
(
?(?)
)
are explicit. No-
tice that the emission probability corresponds to
an argument interpretation (e.g. Eq. 5) and it is
assigned independently from the rest of the sen-
tence. On the other hand, transition probabilities
model role sequences and support the expectations
about argument frames of a sentence.
The emission probability is approximated as:
P
(
s | ?(1) . . . ?(n)
)
?
n?
i=1
P (ri, hi | FE
?(i))
(9)
as it is made independent from previous states in
a Viterbi path. Again the emission probability can
be rewritten as:
P (ri, hi|FE
?(i)) =
P (FE?(i)|ri, hi) P (ri, hi)
P (FE?(i))
(10)
Since P (ri, hi) does not depend on the role la-
beling, maximizing Eq. 10 corresponds to maxi-
mize:
P (FE?(i)|ri, hi)
P (FE?(i))
(11)
whereas P (FE?(i)|ri, hi) is thus estimated
through Eq. 6.
The transition probability, estimated through
P
(
?(1) . . . ?(n)
)
?
n?
i=1
P
(
FE?(i)|FE?(i?1), FE?(i?2)
)
(12)
accounts FEs sequence via a 3-gram model3 .
4 Empirical Analysis
The aim of the evaluation is to measure the reach-
able accuracy of the simple model proposed and
to compare its impact over in-domain and out-of-
domain semantic role labeling tasks. In particular,
we will evaluate the argument classification (AC)
task in Section 4.2.
Experimental Set-Up. The in-domain test has
been run over the FrameNet annotated corpus, de-
rived from the British National Corpus (BNC).
The splitting between train and test set is 90%-
10% according to the same data set of (Johans-
son and Nugues, 2008b). In all experiments,
the FrameNet 1.3 version and the dependency-
based system using the LTH parser (Johansson
and Nugues, 2008a) have been employed. Out-
of-domain tests are run over the two training cor-
pora as made available by the Semeval 2007 Task
194 (Baker et al, 2007): the Nuclear Threat Ini-
tiative (NTI) and the American National Corpus
3Two empty states are added at the beginning of any se-
quence. Moreover, Laplace smoothing was also applied to
each estimator.
4The NTI and ANC annotated collections are download-
able at:
nlp.cs.swarthmore.edu/semeval/tasks/task19/data/train.tar.gz
242
Corpus Predicates Arguments
training FN-BNC 134,697 271,560
test
in-domain FN-BNC 14,952 30,173
out-of-domain
NTI 8,208 14,422
ANC 760 1,389
Table 2: Training and Testing data sets
(ANC)5. Table 2 shows the predicates and argu-
ments in each data set. All null-instantiated ar-
guments were removed from the training and test
sets.
Vectors ~h representing semantic heads have
been computed according to the ?dependency-
based? vector space discussed in (Pado and La-
pata, 2007). The entire BNC corpus has been
parsed and the dependency graphs derived from
individual sentences provided the basic observ-
able contexts: every co-occurrence is thus syntac-
tically justified by a dependency arc. The most
frequent 30,000 basic features, i.e. (syntactic re-
lation,lemma) pairs, have been used to build the
matrix M , vector components corresponding to
point-wise mutual information scores. Finally, the
final space is obtained by applying the SVD reduc-
tion overM , with a dimensionality cut of l = 250.
In the evaluation of the AC task, accuracy is
computed over the nodes of the dependency graph,
in line with (Johansson and Nugues, 2008b) or
(Coppola et al, 2009). Accordingly, also recall,
precision and F-measure are reported on a per
node basis, against the binary BD task or for the
full BD +AC chain.
4.1 The Role of Lexical Clustering
The first study aims at detecting the impact of dif-
ferent clustering policies on the resulting AC ac-
curacy. Clustering, as discussed in Section 3.1,
allows to generalize lexical information: similar
heads within the latent semantic space are built
from the annotated examples and they allow to
predict the behavior of new unseen words as found
in the test sentences. The system performances
have been here measured under different cluster-
ing conditions, i.e. grains at which the clustering
of annotated examples is applied. This grain is de-
termined by the parameter ? of the applied Quality
Threshold algorithm (Heyer et al, 1999). Notice
that small values of ? imply large clusters, while if
5Sentences whose arguments were not represented in the
FrameNet training material were removed from all tests.
Frames with a number of annotated examples
Eq. - ? >0 >100 >500 >1K >3K >5K
(5) - .85 86.3 86.5 87.2 88.3 85.9 82.0
(4) - .5 85.1 85.5 85.8 87.2 83.5 79.4
(4) - .1 84.5 84.8 85.1 86.5 83.0 78.7
Table 3: Accuracy on Arg classification tasks wrt
different clustering policies
? ? 1 then many singleton clusters are promoted
(i.e. one cluster for each example). By varying the
threshold ? we thus account for prototype-based
as well exemplar-based strategies, as discussed in
(Erk, 2009).
We measured the performance on the argument
classification tasks of different models obtained by
combing different choices of ? with Eq. (4) or (5).
Results are reported in Table 3. The leftmost col-
umn reports the different clustering settings, while
in the remaining columns we see performances
over test sentences related to different frames: we
selected frames for which an increasing number of
annotated examples are available: from all frames
(for more than 0 examples) to the only frame (i.e.
SELF MOTION) that has more than 5,000 exam-
ples in our training data set.
The reported accuracies suggest that Eq. (5),
promoting an example driven strategy, better cap-
tures the role preference, as it always outperforms
alternative settings (i.e. more prototype oriented
methods). It limits overgeneralization and pro-
motes fine grained clusters. An interesting result is
that a per-node accuracy of 86.3 (i.e. only 3 points
under the state of-the art on the same data set,
(Johansson and Nugues, 2008b)) is achieved. All
the remaining tests have been run with the clus-
tering configuration characterized by Eq. (5) and
? = 0.85.
4.2 Argument Classification Accuracy
In these experiments we evaluate the quality of
the argument classification step against the lexi-
cal knowledge acquired from unlabeled texts and
the reranking step. The accuracy reachable on the
gold standard argument boundaries has been com-
pared across several experimental settings. Two
baseline systems have been obtained. The Local
Prior model outputs the sequence that maximizes
the prior probability locally to individual argu-
ments. The Global Prior model is obtained by ap-
plying re-ranking (Section 3.2) to the best n = 10
candidates provided by the Local Prior model. Fi-
243
Model FN-BNC NTI ANC
Local Prior 43.9 50.9 50.4
Global Prior 67.7 (+54.2%) 75.9 (+49.0%) 68.8 (+36.4%)
Distributional 81.1 (+19.8%) 82.3 (+8.4%) 69.7 (+1.3%)
Backoff 84.6 (+4.3%) 87.2 (+6.0%) 76.2 (+9.3%)
Backoff+HMMRR 86.3 (+2.0%) 90.5 (+3.8%) 79.9 (+5.0%)
(Johansson&Nugues, 2008) 89.9 71.1 -
Table 4: Accuracy of the Argument Classification task over the different corpora. In parenthesis the
relative increment with respect to the immediately simpler model, previous row
nally, the application of the backoff strategies (as
in Eq. 6) and the HMM-based reranking character-
ize the final two configurations. Table 4 reports the
accuracy results obtained over the three corpora
(defined as in Table 2): the accuracy scores are av-
eraged over different values ofm in Eq. 5, ranging
from 3 to 30. In the in-domain scenario, i.e. the
FN-BNC dataset reported in column 2, it is worth
noticing that the proposed model, with backoff and
global reranking, is quite effective with respect to
the state-of-the-art.
Although results on the FN-BNC do not outper-
form the state-of-the-art for the FrameNet corpus,
we still need to study the generalization capabil-
ity of our SRL model in out-of-domain conditions.
In a further experiment, we applied the same sys-
tem, as trained over the FN-BNC data, to the other
corpora, i.e. NTI and ANC, used entirely as test
sets. Results, reported in column 3 and 4 of Ta-
ble 4 and shown in Figure 2, confirm that no ma-
jor drop in performance is observed. Notice how
the positive impact of the backoff models and the
HMM reranking policy is similarly reflected by all
the collections. Moreover, the results on the NTI
corpus are even better than those obtained on the
BNC, with a resulting 90.5% accuracy on the AC
task.
86,3%90,5%79,9%
40,0%
50,0%
60,0%
70,0%
80,0%
90,0%
100,0%
Local        Prior Global     Prior Distributional Backoff Backoff    +HMMRR
FN-BNCNTIANC
Figure 2: Accuracy of the AC task over different
corpora
4.3 Discussion
The above empirical findings are relevant if com-
pared with the outcome of a similar test on the NTI
collection, discussed in (Johansson and Nugues,
2008b)6. There, under the same training condi-
tions, a performance drop of about -19% is re-
ported (from 89.9 to 71.1%) over gold standard
argument boundaries. The model proposed in this
paper exhibits no such drop in any collection (NTI
and ANC). This seems to confirm the hypothesis
that the model is able to properly generalize the
required lexical information across different do-
mains.
It is interesting to outline that the individual
stages of the proposed model play different roles
in the different domains, as Table 4 suggests. Al-
though the positive contributions of the individual
processing stages are uniformly confirmed, some
differences can be outlined:
? The beneficial impact of the lexical informa-
tion (i.e. the distributional model) applies dif-
ferently across the different domains. The
ANC domain seems not to significantly ben-
efit when the distributional model (Eq. 5) is
applied. Notice how Eq. 5 depends both from
the evidence gathered in the corpus about lex-
ical heads h as well as about the relation r. In
ANC the percentage of times that the Eq. 5 is
backed off against test instances (as h or r are
not available from the training data) is twice
as high as in the BNC-FN or in the NTI do-
main (i.e. 15.5 vs. 7.2 or 8.7, respectively).
The different syntactic style of ANC seems
thus the main responsible of the poor impact
of distributional information, as it is often un-
applicable to ANC test cases.
? The complexity of the three test sets is dif-
ferent, as the three plots show. The NTI col-
6Notice that in this paper only the training portion of the
NTI data set is employed as reported in Table 2 and results are
not directly comparable to (Johansson and Nugues, 2008b).
244
lections seems characterized by a lower level
of complexity (see for example the accuracy
of the Local prior model, that is about 51%
as for the ANC). It then gets benefits from
all the analysis stages, in particular the final
HMM reranking. The BNC-FN test collec-
tion seems the most complex one, and the im-
pact of the lexical information brought by the
distributional model is here maximal. This
is mainly due to the coherence between the
distributions of lexical and grammatical phe-
nomena in the test and training data.
? The role of HMM reranking is an effective
way to compensate errors in the local argu-
ment classifications for all the three domains.
However, it is particularly effective for the
outside domain cases, while, in the BNC cor-
pus, it produces just a small improvement in-
stead (i.e. +2%, as shown in Table 4 ). It is
worth noticing that the average length of the
sentences in the BNC test collection is about
23 words per sentence, while it is higher for
the NTI and ANC data sets (i.e. 34 and 31,
respectively). It seems that the HMM model
well captures some information on the global
semantic structure of a sentence: this is help-
ful in cases where errors in the grammati-
cal recognition (of individual arguments or
at sentence level) are more frequent and af-
flict the local distributional model. The more
complex is the syntax of a corpus (e.g. in the
NTI and ANC data sets), the higher seems the
impact of the reranking phase.
The significant performance of the AC model
here presented suggest to test it when integrated
within a full SRL architecture. Table 5 reports the
results of the processing cascade over three col-
lections. Results on the Boundary Detection BD
task are obtained by training an SVM model on
the same feature set presented in (Johansson and
Nugues, 2008b) and are slightly below the state-
of-the art BD accuracy reported in (Coppola et
al., 2009). However, the accuracy of the complete
BD + AC + RR chain (i.e. 68%) improves the
corresponding results of (Coppola et al, 2009).
Given the relatively simple feature set adopted
here, this result is very significant as for its result-
ing efficiency. The overall BD recognition pro-
cess is, on a standard architecture, performed at
about 6.74 sentences per second, that is basically
Corpus Eval. Setting Recall Precision F1
BNC
BD 72.6 85.1 78.4
BD+AC+RR 62.6 74.5 68.0
NTI
BD 63.9 80.0 71.0
BD+AC+RR 56.7 72.1 63.5
ANC
BD 64.0 81.5 71.7
BD+AC+RR 47.4 62.5 53.9
Table 5: Accuracy of the full cascade of the SRL
system over three domain
the same as the time needed for applying the en-
tire BD+AC +RR chain, i.e. 6.21 sentence per
second.
5 Conclusions
In this paper, a distributional approach for acquir-
ing a semi-supervised model of argument classi-
fication (AC) preferences has been proposed. It
aims at improving the generalization capability of
the inductive SRL approach by reducing the com-
plexity of the employed grammatical features and
through a distributional representation of lexical
features. The obtained results are close to the
state-of-art in FrameNet semantic parsing. State
of the art accuracy is obtained instead in out-of-
domain experiments. The model seems to cap-
italize from simple methods of lexical modeling
(i.e. the estimation of lexico-grammatical pref-
erences through distributional analysis over unla-
beled data), estimation (through syntactic or lexi-
cal back-off where necessary) and reranking. The
result is an accurate and highly portable SRL cas-
cade. Experiments on the integrated SRL archi-
tecture (i.e. BD + AC + RR chain) show that
state-of-art accuracy (i.e. 68%) can be obtained
on raw texts. This result is also very significant
as for the achieved efficiency. The system is able
to apply the entire BD + AC + RR chain at a
speed of 6.21 sentences per second. This signif-
icant efficiency confirms the applicability of the
SRL approach proposed here in large scale NLP
applications. Future work will study the appli-
cation of the flexible SRL method proposed to
other languages, for which less resources are avail-
able and worst training conditions are the norm.
Moreover, dimensionality reduction methods al-
ternative to LSA, as currently studied on semi-
supervised spectral learning (Johnson and Zhang,
2008), will be experimented.
245
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc. of
COLING-ACL, Montreal, Canada.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic struc-
ture extraction. In Proceedings of SemEval-2007,
pages 99?104, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Intro-
duction to the CoNLL-2005 Shared Task: Seman-
tic Role Labeling. In Proc. of CoNLL-2005, pages
152?164, Ann Arbor, Michigan, June.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In In Pro-
ceedings of ICML ?08, pages 160?167, New York,
NY, USA. ACM.
Bonaventura Coppola, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Shallow semantic parsing
for spoken language understanding. In Proceedings
of NAACL ?09, pages 85?88, Morristown, NJ, USA.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In EMNLP ?09: Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 21?29,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser -
a flexible toolbox for semantic role assignment. In
Proceedings of LREC 2006, Genoa, Italy.
Katrin Erk. 2009. Representing words as regions
in vector space. In In Proceedings of CoNLL ?09,
pages 57?65, Morristown, NJ, USA. Association for
Computational Linguistics.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222?
254.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph
alignment for semi-supervised semantic role label-
ing. In In Proceedings of EMNLP ?09, pages 11?20,
Morristown, NJ, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Yoav Goldberg and Michael Elhadad. 2009. On the
role of lexical features in sequence labeling. In In
Proceedings of EMNLP ?09, pages 1142?1151, Sin-
gapore, August. Association for Computational Lin-
guistics.
L.J. Heyer, S. Kruglyak, and S. Yooseph. 1999. Ex-
ploring expression data: Identification and analysis
of coexpressed genes. Genome Research, (9):1106?
1115.
Richard Johansson and Pierre Nugues. 2008a.
Dependency-based syntactic-semantic analysis with
propbank and nombank. In Proceedings of CoNLL-
2008, Manchester, UK, August 16-17.
Richard Johansson and Pierre Nugues. 2008b. The
effect of syntactic representation on semantic role
labeling. In Proceedings of COLING, Manchester,
UK, August 18-22.
Rie Johnson and Tong Zhang. 2008. Graph-based
semi-supervised learning and spectral kernel de-
sign. IEEE Transactions on Information Theory,
54(1):275?288.
Tom Landauer and Sue Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104.
A. Moschitti, D. Pighin, and R. Basili. 2008. Tree
kernels for semantic role labeling. Computational
Linguistics, 34.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics, 33(2).
Sebastian Pado. 2007. Cross-Lingual Annotation
Projection Models for Role-Semantic Information.
Ph.D. thesis, Saarland University.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with
semantic roles. Computational Linguistics, 31(1),
March.
Sameer S. Pradhan, Wayne Ward, and James H. Mar-
tin. 2008. Towards robust semantic role labeling.
Comput. Linguist., 34(2):289?310.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: chunk-
ing. In Proceedings of the 2nd workshop on Learn-
ing language in logic and the 4th conference on
Computational natural language learning, pages
127?132, Morristown, NJ, USA. Association for
Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Comput. Linguist., 34(2):161?191.
246
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 263?272,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Verb Classification using Distributional Similarity
in Syntactic and Semantic Structures
Danilo Croce
University of Tor Vergata
00133 Roma, Italy
croce@info.uniroma2.it
Alessandro Moschitti
University of Trento
38123 Povo (TN), Italy
moschitti@disi.unitn.it
Roberto Basili
University of Tor Vergata
00133 Roma, Italy
basili@info.uniroma2.it
Martha Palmer
University of Colorado at Boulder
Boulder, CO 80302, USA
mpalmer@colorado.edu
Abstract
In this paper, we propose innovative repre-
sentations for automatic classification of verbs
according to mainstream linguistic theories,
namely VerbNet and FrameNet. First, syntac-
tic and semantic structures capturing essential
lexical and syntactic properties of verbs are
defined. Then, we design advanced similarity
functions between such structures, i.e., seman-
tic tree kernel functions, for exploiting distri-
butional and grammatical information in Sup-
port Vector Machines. The extensive empir-
ical analysis on VerbNet class and frame de-
tection shows that our models capture mean-
ingful syntactic/semantic structures, which al-
lows for improving the state-of-the-art.
1 Introduction
Verb classification is a fundamental topic of com-
putational linguistics research given its importance
for understanding the role of verbs in conveying se-
mantics of natural language (NL). Additionally, gen-
eralization based on verb classification is central to
many NL applications, ranging from shallow seman-
tic parsing to semantic search or information extrac-
tion. Currently, a lot of interest has been paid to
two verb categorization schemes: VerbNet (Schuler,
2005) and FrameNet (Baker et al, 1998), which
has also fostered production of many automatic ap-
proaches to predicate argument extraction.
Such work has shown that syntax is necessary
for helping to predict the roles of verb arguments
and consequently their verb sense (Gildea and Juras-
fky, 2002; Pradhan et al, 2005; Gildea and Palmer,
2002). However, the definition of models for opti-
mally combining lexical and syntactic constraints is
still far for being accomplished. In particular, the ex-
haustive design and experimentation of lexical and
syntactic features for learning verb classification ap-
pears to be computationally problematic. For exam-
ple, the verb order can belongs to the two VerbNet
classes:
? The class 60.1, i.e., order someone to do some-
thing as shown in: The Illinois Supreme Court or-
dered the commission to audit Commonwealth Edi-
son ?s construction expenses and refund any unrea-
sonable expenses .
? The class 13.5.1: order or request something like
in: ... Michelle blabs about it to a sandwich man
while ordering lunch over the phone .
Clearly, the syntactic realization can be used to dis-
cern the cases above but it would not be enough to
correctly classify the following verb occurrence: ..
ordered the lunch to be delivered .. in Verb class
13.5.1. For such a case, selectional restrictions are
needed. These have also been shown to be use-
ful for semantic role classification (Zapirain et al,
2010). Note that their coding in learning algorithms
is rather complex: we need to take into account syn-
tactic structures, which may require an exponential
number of syntactic features (i.e., all their possible
substructures). Moreover, these have to be enriched
with lexical information to trig lexical preference.
In this paper, we tackle the problem above
by studying innovative representations for auto-
matic verb classification according to VerbNet and
FrameNet. We define syntactic and semantic struc-
tures capturing essential lexical and syntactic prop-
erties of verbs. Then, we apply similarity between
263
such structures, i.e., kernel functions, which can also
exploit distributional lexical semantics, to train au-
tomatic classifiers. The basic idea of such functions
is to compute the similarity between two verbs in
terms of all the possible substructures of their syn-
tactic frames. We define and automatically extract
a lexicalized approximation of the latter. Then, we
apply kernel functions that jointly model structural
and lexical similarity so that syntactic properties are
combined with generalized lexemes. The nice prop-
erty of kernel functions is that they can be used
in place of the scalar product of feature vectors to
train algorithms such as Support Vector Machines
(SVMs). This way SVMs can learn the association
between syntactic (sub-) structures whose lexical ar-
guments are generalized and target verb classes, i.e.,
they can also learn selectional restrictions.
We carried out extensive experiments on verb
class and frame detection which showed that our
models greatly improve on the state-of-the-art (up
to about 13% of relative error reduction). Such re-
sults are nicely assessed by manually inspecting the
most important substructures used by the classifiers
as they largely correlate with syntactic frames de-
fined in VerbNet.
In the rest of the paper, Sec. 2 reports on related
work, Sec. 3 and Sec. 4 describe previous and our
models for syntactic and semantic similarity, respec-
tively, Sec. 5 illustrates our experiments, Sec. 6 dis-
cusses the output of the models in terms of error
analysis and important structures and finally Sec. 7
derives the conclusions.
2 Related work
Our target task is verb classification but at the same
time our models exploit distributional models as
well as structural kernels. The next three subsec-
tions report related work in such areas.
Verb Classification. The introductory verb classi-
fication example has intuitively shown the complex-
ity of defining a comprehensive feature representa-
tion. Hereafter, we report on analysis carried out in
previous work.
It has been often observed that verb senses tend
to show different selectional constraints in a specific
argument position and the above verb order is a clear
example. In the direct object position of the example
sentence for the first sense 60.1 of order, we found
commission in the role PATIENT of the predicate. It
clearly satisfies the +ANIMATE/+ORGANIZATION
restriction on the PATIENT role. This is not true
for the direct object dependency of the alternative
sense 13.5.1, which usually expresses the THEME
role, with unrestricted type selection. When prop-
erly generalized, the direct object information has
thus been shown highly predictive about verb sense
distinctions.
In (Brown et al, 2011), the so called dynamic
dependency neighborhoods (DDN), i.e., the set of
verbs that are typically collocated with a direct ob-
ject, are shown to be more helpful than lexical in-
formation (e.g., WordNet). The set of typical verbs
taking a noun n as a direct object is in fact a strong
characterization for semantic similarity, as all the
nounsm similar to n tend to collocate with the same
verbs. This is true also for other syntactic depen-
dencies, among which the direct object dependency
is possibly the strongest cue (as shown for example
in (Dligach and Palmer, 2008)).
In order to generalize the above DDN feature, dis-
tributional models are ideal, as they are designed
to model all the collocations of a given noun, ac-
cording to large scale corpus analysis. Their abil-
ity to capture lexical similarity is well established in
WSD tasks (e.g. (Schutze, 1998)), thesauri harvest-
ing (Lin, 1998), semantic role labeling (Croce et al,
2010)) as well as information retrieval (e.g. (Furnas
et al, 1988)).
Distributional Models (DMs). These models fol-
low the distributional hypothesis (Firth, 1957) and
characterize lexical meanings in terms of context of
use, (Wittgenstein, 1953). By inducing geometrical
notions of vectors and norms through corpus analy-
sis, they provide a topological definition of seman-
tic similarity, i.e., distance in a space. DMs can
capture the similarity between words such as dele-
gation, deputation or company and commission. In
case of sense 60.1 of the verb order, DMs can be
used to suggest that the role PATIENT can be inher-
ited by all these words, as suitable Organisations.
In supervised language learning, when few exam-
ples are available, DMs support cost-effective lexi-
cal generalizations, often outperforming knowledge
based resources (such as WordNet, as in (Pantel et
al., 2007)). Obviously, the choice of the context
264
type determines the type of targeted semantic prop-
erties. Wider contexts (e.g., entire documents) are
shown to suggest topical relations. Smaller con-
texts tend to capture more specific semantic as-
pects, e.g. the syntactic behavior, and better capture
paradigmatic relations, such as synonymy. In partic-
ular, word space models, as described in (Sahlgren,
2006), define contexts as the words appearing in a
n-sized window, centered around a target word. Co-
occurrence counts are thus collected in a words-by-
words matrix, where each element records the num-
ber of times two words co-occur within a single win-
dow of word tokens. Moreover, robust weighting
schemas are used to smooth counts against too fre-
quent co-occurrence pairs: Pointwise Mutual Infor-
mation (PMI) scores (Turney and Pantel, 2010) are
commonly adopted.
Structural Kernels. Tree and sequence kernels
have been successfully used in many NLP applica-
tions, e.g., parse reranking and adaptation, (Collins
and Duffy, 2002; Shen et al, 2003; Toutanova et
al., 2004; Kudo et al, 2005; Titov and Hender-
son, 2006), chunking and dependency parsing, e.g.,
(Kudo and Matsumoto, 2003; Daume? III and Marcu,
2004), named entity recognition, (Cumby and Roth,
2003), text categorization, e.g., (Cancedda et al,
2003; Gliozzo et al, 2005), and relation extraction,
e.g., (Zelenko et al, 2002; Bunescu and Mooney,
2005; Zhang et al, 2006).
Recently, DMs have been also proposed in in-
tegrated syntactic-semantic structures that feed ad-
vanced learning functions, such as the semantic
tree kernels discussed in (Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b; Mehdad et
al., 2010; Croce et al, 2011).
3 Structural Similarity Functions
In this paper we model verb classifiers by exploiting
previous technology for kernel methods. In particu-
lar, we design new models for verb classification by
adopting algorithms for structural similarity, known
as Smoothed Partial Tree Kernels (SPTKs) (Croce et
al., 2011). We define new innovative structures and
similarity functions based on LSA.
The main idea of SPTK is rather simple: (i) mea-
suring the similarity between two trees in terms of
the number of shared subtrees; and (ii) such number
also includes similar fragments whose lexical nodes
are just related (so they can be different). The con-
tribution of (ii) is proportional to the lexical similar-
ity of the tree lexical nodes, where the latter can be
evaluated according to distributional models or also
lexical resources, e.g., WordNet.
In the following, we define our models based on
previous work on LSA and SPTKs.
3.1 LSA as lexical similarity model
Robust representations can be obtained through
intelligent dimensionality reduction methods. In
LSA the original word-by-context matrix M is de-
composed through Singular Value Decomposition
(SVD) (Landauer and Dumais, 1997; Golub and Ka-
han, 1965) into the product of three new matrices:
U , S, and V so that S is diagonal and M = USV T .
M is then approximated by Mk = UkSkV Tk , where
only the first k columns of U and V are used,
corresponding to the first k greatest singular val-
ues. This approximation supplies a way to project
a generic term wi into the k-dimensional space us-
ing W = UkS
1/2
k , where each row corresponds to
the representation vectors ~wi. The original statisti-
cal information about M is captured by the new k-
dimensional space, which preserves the global struc-
ture while removing low-variant dimensions, i.e.,
distribution noise. Given two words w1 and w2,
the term similarity function ? is estimated as the
cosine similarity between the corresponding projec-
tions ~w1, ~w2 in the LSA space, i.e ?(w1, w2) =
~w1? ~w2
? ~w1?? ~w2?
. This is known as Latent Semantic Ker-
nel (LSK), proposed in (Cristianini et al, 2001),
as it defines a positive semi-definite Gram matrix
G = ?(w1, w2) ?w1, w2 (Shawe-Taylor and Cris-
tianini, 2004). ? is thus a valid kernel and can be
combined with other kernels, as discussed in the
next session.
3.2 Tree Kernels driven by Semantic Similarity
To our knowledge, two main types of tree kernels
exploit lexical similarity: the syntactic semantic tree
kernel defined in (Bloehdorn and Moschitti, 2007a)
applied to constituency trees and the smoothed
partial tree kernels (SPTKs) defined in (Croce et
al., 2011), which generalizes the former. We report
the definition of the latter as we modified it for our
purposes. SPTK computes the number of common
substructures between two trees T1 and T2 without
explicitly considering the whole fragment space. Its
265
SVP
S
-
NP-1
NN
commission::n
DT
the::d
VBD
TARGET-order::v
NP-SBJ
NNP
court::n
NNP
supreme::n
NNP
illinois::n
DT
the::d
Figure 1: Constituency Tree (CT) representation of verbs.
ROOT
OPRD
IM
VB
audit::v
TO
to::t
OBJ
NN
commission::n
NMOD
DT
the::d
VBD
TARGET-order::v
SBJ
NNP
court::n
NMOD
NNP
supreme::n
NMOD
NNP
illinois::n
NMOD
DT
the::d
Figure 2: Representation of verbs according to the Grammatical Relation Centered Tree (GRCT)
general equations are reported hereafter:
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), (1)
where NT1 and NT2 are the sets of the T1?s and T2?s
nodes, respectively and ?(n1, n2) is equal to the
number of common fragments rooted in the n1 and
n2 nodes1. The ? function determines the richness
of the kernel space and thus induces different tree
kernels, for example, the syntactic tree kernel (STK)
(Collins and Duffy, 2002) or the partial tree kernel
(PTK) (Moschitti, 2006).
The algorithm for SPTK?s ? is the follow-
ing: if n1 and n2 are leaves then ??(n1, n2) =
???(n1, n2); else
??(n1, n2) = ??(n1, n2)?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(
~I1)+d(~I2)
l(~I1)?
j=1
??(cn1(~I1j), cn2(~I2j))
)
, (2)
where (1) ? is any similarity between nodes, e.g., be-
tween their lexical labels; (2) ?, ? ? [0, 1] are decay
factors; (3) cn1(h) is the h
th child of the node n1;
(4) ~I1 and ~I2 are two sequences of indexes, i.e., ~I =
(i1, i2, .., l(I)), with 1 ? i1 < i2 < .. < il(I); and (5)
d(~I1) = ~I1l(~I1)?
~I11+1 and d(~I2) = ~I2l(~I2)?
~I21+1.
Note that, as shown in (Croce et al, 2011), the av-
erage running time of SPTK is sub-quadratic in the
number of the tree nodes. In the next section we
show how we exploit the class of SPTKs, for verb
classification.
1To have a similarity score between 0 and 1, a normalization
in the kernel space, i.e. TK(T1,T2)?
TK(T1,T1)?TK(T2,T2)
is applied.
4 Verb Classification Models
The design of SPTK-based algorithms for our verb
classification requires the modeling of two differ-
ent aspects: (i) a tree representation for the verbs;
and (ii) the lexical similarity suitable for the task.
We also modified SPTK to apply different similarity
functions to different nodes to introduce flexibility.
4.1 Verb Structural Representation
The implicit feature space generated by structural
kernels and the corresponding notion of similarity
between verbs obviously depends on the input struc-
tures. In the cases of STK, PTK and SPTK different
tree representations lead to engineering more or less
expressive linguistic feature spaces.
With the aim of capturing syntactic features, we
started from two different parsing paradigms: phrase
and dependency structures. For example, for repre-
senting the first example of the introduction, we can
use the constituency tree (CT) in Figure 1, where the
target verb node is enriched with the TARGET label.
Here, we apply tree pruning to reduce the computa-
tional complexity of tree kernels as it is proportional
to the number of nodes in the input trees. Accord-
ingly, we only keep the subtree dominated by the
target VP by pruning from it all the S-nodes along
with their subtrees (i.e, all nested sentences are re-
moved). To further improve generalization, we lem-
matize lexical nodes and add generalized POS-Tags,
i.e., noun (n::), verb (v::), adjective (::a), determiner
(::d) and so on, to them. This is useful for constrain-
ing similarity to be only contributed by lexical pairs
of the same grammatical category.
266
TARGET-order::v
VBDROOTto::t
TOOPRDaudit::v
VBIM
commission::n
NNOBJthe::d
DTNMOD
court::n
NNPSBJsupreme::n
NNPNMOD
illinois::n
NNPNMOD
the::d
DTNMOD
Figure 3: Representation of verbs according to the Lexical Centered Tree (LCT)
To encode dependency structure information in a
tree (so that we can use it in tree kernels), we use
(i) lexemes as nodes of our tree, (ii) their dependen-
cies as edges between the nodes and (iii) the depen-
dency labels, e.g., grammatical functions (GR), and
POS-Tags, again as tree nodes. We designed two
different tree types: (i) in the first type, GR are cen-
tral nodes from which dependencies are drawn and
all the other features of the central node, i.e., lexi-
cal surface form and its POS-Tag, are added as ad-
ditional children. An example of the GR Centered
Tree (GRCT) is shown in Figure 2, where the POS-
Tags and lexemes are children of GR nodes. (ii) The
second type of tree uses lexicals as central nodes on
which both GR and POS-Tag are added as the right-
most children. For example, Figure 3 shows an ex-
ample of a Lexical Centered Tree (LCT). For both
trees, the pruning strategy only preserves the verb
node, its direct ancestors (father and siblings) and
its descendants up to two levels (i.e., direct children
and grandchildren of the verb node). Note that, our
dependency tree can capture the semantic head of
the verbal argument along with the main syntactic
construct, e.g., to audit.
4.2 Generalized node similarity for SPTK
We have defined the new similarity ?? to be used in
Eq. 2, which makes SPTK more effective as shown
by Alg. 1. ?? takes two nodes n1 and n2 and applies
a different similarity for each node type. The latter is
derived by ? and can be: GR (i.e., SYNT), POS-Tag
(i.e., POS) or a lexical (i.e., LEX) type. In our exper-
iment, we assign 0/1 similarity for SYNT and POS
nodes according to string matching. For LEX type,
we apply a lexical similarity learned with LSA to
only pairs of lexicals associated with the same POS-
Tag. It should be noted that the type-based similarity
allows for potentially applying a different similarity
for each node. Indeed, we also tested an amplifica-
tion factor, namely, leaf weight (lw), which ampli-
fies the matching values of the leaf nodes.
Algorithm 1 ?? (n1, n2, lw)
?? ? 0,
if ?(n1) = ?(n2) = SYNT ? label(n1) = label(n2) then
?? ? 1
end if
if ?(n1) = ?(n2) = POS ? label(n1) = label(n2) then
?? ? 1
end if
if ?(n1) = ?(n2) = LEX ? pos(n1) = pos(n2) then
?? ? ?LEX(n1, n2)
end if
if leaf(n1) ? leaf(n2) then
?? ? ?? ? lw
end if
return ??
5 Experiments
In these experiments, we tested the impact of our dif-
ferent verb representations using different kernels,
similarities and parameters. We also compared with
simple bag-of-words (BOW) models and the state-
of-the-art.
5.1 General experimental setup
We consider two different corpora: one for VerbNet
and the other for FrameNet. For the former, we used
the same verb classification setting of (Brown et al,
2011). Sentences are drawn from the Semlink cor-
pus (Loper et al, 2007), which consists of the Prop-
Banked Penn Treebank portions of the Wall Street
Journal. It contains 113K verb instances, 97K of
which are verbs represented in at least one VerbNet
class. Semlink includes 495 verbs, whose instances
are labeled with more than one class (including one
single VerbNet class or none). We used all instances
of the corpus for a total of 45,584 instances for 180
verb classes. When instances labeled with the none
class are not included, the number of examples be-
comes 23,719.
The second corpus refers to FrameNet frame clas-
sification. The training and test data are drawn from
the FrameNet 1.5 corpus2, which consists of 135K
sentences annotated according the frame semantics
2http://framenet.icsi.berkeley.edu
267
(Baker et al, 1998). We selected the subset of
frames containing more than 100 sentences anno-
tated with a verbal predicate for a total of 62,813
sentences in 187 frames (i.e., very close to the Verb-
Net datasets). For both the datasets, we used 70% of
instances for training and 30% for testing.
Our verb (multi) classifier is designed with
the one-vs-all (Rifkin and Klautau, 2004) multi-
classification schema. This uses a set of binary
SVM classifiers, one for each verb class (frame) i.
The sentences whose verb is labeled with the class
i are positive examples for the classifier i. The sen-
tences whose verbs are compatible with the class i
but evoking a different class or labeled with none
(no current verb class applies) are added as negative
examples. In the classification phase the binary clas-
sifiers are applied by (i) only considering classes that
are compatible with the target verbs; and (ii) select-
ing the class associated with the maximum positive
SVM margin. If all classifiers provide a negative
score the example is labeled with none.
To learn the binary classifiers of the schema
above, we coded our modified SPTK in SVM-Light-
TK3 (Moschitti, 2006). The parameterization of
each classifier is carried on a held-out set (30% of
the training) and is concerned with the setting of the
trade-off parameter (option -c) and the leaf weight
(lw) (see Alg. 1), which is used to linearly scale
the contribution of the leaf nodes. In contrast, the
cost-factor parameter of SVM-Light-TK is set as the
ratio between the number of negative and positive
examples for attempting to have a balanced Preci-
sion/Recall.
Regarding SPTK setting, we used the lexical simi-
larity ? defined in Sec. 3.1. In more detail, LSA was
applied to ukWak (Baroni et al, 2009), which is a
large scale document collection made up of 2 billion
tokens. M is constructed by applying POS tagging to
build rows with pairs ?lemma, ::POS? (lemma::POS
in brief). The contexts of such items are the columns
of M and are short windows of size [?3,+3], cen-
tered on the items. This allows for better captur-
ing syntactic properties of words. The most frequent
20,000 items are selected along with their 20k con-
texts. The entries of M are the point-wise mutual
3(Structural kernels in SVMLight (Joachims, 2000)) avail-
able at http://disi.unitn.it/moschitti/Tree-Kernel.htm
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 83.83% 8 84.57% 8 84.46%
GRCT - 84.83% 8 85.15% 8 85.28%
LCT - 77.73% 0.1 86.03% 0.2 86.72%
Br. et Al. 84.64%
BOW 79.08%
SK 82.08%
Table 1: VerbNet accuracy with the none class
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
GRCT - 92.67% 6 92.97% 0.4 93.54%
LCT - 90.28% 6 92.99% 0.3 93.78%
BOW 91.13%
SK 91.84%
Table 2: FrameNet accuracy without the none class
information between them. SVD reduction is then
applied to M, with a dimensionality cut of l = 250.
For generating the CT, GRCT and LCT struc-
tures, we used the constituency trees generated by
the Charniak parser (Charniak, 2000) and the de-
pendency structures generated by the LTH syntactic
parser (described in (Johansson and Nugues, 2008)).
The classification performance is measured with
accuracy (i.e., the percentage of correct classifica-
tion). We also derive statistical significance of the
results by using the model described in (Yeh, 2000)
and implemented in (Pado?, 2006).
5.2 VerbNet and FrameNet Classification
Results
To assess the performance of our settings, we also
derive a simple baseline based on the bag-of-words
(BOW) model. For it, we represent an instance of
a verb in a sentence using all words of the sentence
(by creating a special feature for the predicate word).
We also used sequence kernels (SK), i.e., PTK ap-
plied to a tree composed of a fake root and only one
level of sentence words. For efficiency reasons4, we
only consider the 10 words before and after the pred-
icate with subsequence features of length up to 5.
Table 1 reports the accuracy of different mod-
els for VerbNet classification. It should be noted
that: first, SK produces a much higher accuracy than
BOW, i.e., 82.08 vs. 79.08. On one hand, this is
4The average running time of the SK is much higher than the
one of PTK. When a tree is composed by only one level PTK
collapses to SK.
268
STK PTK SPTK
lw Acc. lw Acc. lw Acc.
CT - 91.14% 8 91.66% 6 91.66%
GRCT - 91.71% 8 92.38% 4 92.33%
LCT - 89.20% 0.2 92.54% 0.1 92.55%
BOW 88.16%
SK 89.86%
Table 3: VerbNet accuracy without the none class
generally in contrast with standard text categoriza-
tion tasks, for which n-gram models show accuracy
comparable to the simpler BOW. On the other hand,
it simply confirms that verb classification requires
the dependency information between words (i.e., at
least the sequential structure information provided
by SK).
Second, SK is 2.56 percent points below the state-
of-the-art achieved in (Brown et al, 2011) (BR), i.e,
82.08 vs. 84.64. In contrast, STK applied to our rep-
resentation (CT, GRCT and LCT) produces compa-
rable accuracy, e.g., 84.83, confirming that syntactic
representation is needed to reach the state-of-the-art.
Third, PTK, which produces more general struc-
tures, improves over BR by almost 1.5 (statistically
significant result) when using our dependency struc-
tures GRCT and LCT. CT does not produce the same
improvement since it does not allow PTK to directly
compare the lexical structure (lexemes are all leaf
nodes in CT and to connect some pairs of them very
large trees are needed).
Finally, the best model of SPTK (i.e, using LCT)
improves over the best PTK (i.e., using LCT) by al-
most 1 point (statistically significant result): this dif-
ference is only given by lexical similarity. SPTK im-
proves on the state-of-the-art by about 2.08 absolute
percent points, which, given the high accuracy of the
baseline, corresponds to 13.5% of relative error re-
duction.
We carried out similar experiments for frame clas-
sification. One interesting difference is that SK im-
proves BOW by only 0.70, i.e., 4 times less than in
the VerbNet setting. This suggests that word order
around the predicate is more important for deriving
the VerbNet class than the FrameNet frame. Ad-
ditionally, LCT or GRCT seems to be invariant for
both PTK and SPTK whereas the lexical similarity
still produces a relevant improvement on PTK, i.e.,
13% of relative error reduction, for an absolute accu-
racy of 93.78%. The latter improves over the state-
50% 
60% 
70% 
80% 
90% 
0% 20% 40% 60% 80% 100% 
Accu
racy 
Percentage of train examples 
SPTK 
BOW 
Brown et al 
Figure 4: Learning curves: VerbNet accuracy with the
none Class
of-the-art, i.e., 92.63% derived in (Giuglea and Mos-
chitti, 2006), by using STK on CT on 133 frames.
We also carried out experiments to understand
the role of the none class. Table 3 reports on the
VerbNet classification without its instances. This is
of course an unrealistic setting as it would assume
that the current VerbNet release already includes all
senses for English verbs. In the table, we note that
the overall accuracy highly increases and the differ-
ence between models reduces. The similarities play
no role anymore. This may suggest that SPTK can
help in complex settings, where verb class character-
ization is more difficult. Another important role of
SPTK models is their ability to generalize. To test
this aspect, Figure 4 illustrates the learning curves
of SPTK with respect to BOW and the accuracy
achieved by BR (with a constant line). It is impres-
sive to note that with only 40% of the data SPTK can
reach the state-of-the-art.
6 Model Analysis and Discussion
We carried out analysis of system errors and its in-
duced features. These can be examined by apply-
ing the reverse engineering tool5 proposed in (Pighin
and Moschitti, 2010; Pighin and Moschitti, 2009a;
Pighin and Moschitti, 2009b), which extracts the
most important features for the classification model.
Many mistakes are related to false positives and neg-
atives of the none class (about 72% of the errors).
This class also causes data imbalance. Most errors
are also due to lack of lexical information available
to the SPTK kernel: (i) in 30% of the errors, the
argument heads were proper nouns for which the
lexical generalization provided by the DMs was not
5http://danielepighin.net/cms/software/flink
269
VerbNet class 13.5.1
(IM(VB(target))(OBJ))
(VC(VB(target))(OBJ))
(VC(VBG(target))(OBJ))
(OPRD(TO)(IM(VB(target))(OBJ)))
(PMOD(VBG(target))(OBJ))
(VB(target))
(VC(VBN(target)))
(PRP(TO)(IM(VB(target))(OBJ)))
(IM(VB(target))(OBJ)(ADV(IN)(PMOD)))
(OPRD(TO)(IM(VB(target))(OBJ)(ADV(IN)(PMOD))))
VerbNet class 60
(VC(VB(target))(OBJ))
(NMOD(VBG(target))(OPRD))
(VC(VBN(target))(OPRD))
(NMOD(VBN(target))(OPRD))
(PMOD(VBG(target))(OBJ))
(ROOT(SBJ)(VBD(target))(OBJ)(P(,)))
(VC(VB(target))(OPRD))
(ROOT(SBJ)(VBZ(target))(OBJ)(P(,)))
(NMOD(SBJ(WDT))(VBZ(target))(OPRD))
(NMOD(SBJ)(VBZ(target))(OPRD(SBJ)(TO)(IM)))
Table 4: GRCT fragments
available; and (ii) in 76% of the errors only 2 or less
argument heads are included in the extracted tree,
therefore tree kernels cannot exploit enough lexical
information to disambiguate verb senses. Addition-
ally, ambiguity characterizes errors where the sys-
tem is linguistically consistent but the learned selec-
tional preferences are not sufficient to separate verb
senses. These errors are mainly due to the lack of
contextual information. While error analysis sug-
gests that further improvement is possible (e.g. by
exploiting proper nouns), the type of generalizations
currently achieved by SPTK are rather effective. Ta-
ble 4 and 5 report the tree structures characterizing
the most informative training examples of the two
senses of the verb order, i.e. the VerbNet classes
13.5.1 (make a request for something) and 60 (give
instructions to or direct somebody to do something
with authority).
In line with the method discussed in (Pighin and
Moschitti, 2009b), these fragments are extracted as
they appear in most of the support vectors selected
during SVM training. As easily seen, the two classes
are captured by rather different patterns. The typ-
ical accusative form with an explicit direct object
emerges as characterizing the sense 13.5.1, denot-
ing the THEME role. All fragments of the sense 60
emphasize instead the sentential complement of the
verb that in fact expresses the standard PROPOSI-
TION role in VerbNet. Notice that tree fragments
correspond to syntactic patterns. The a posteriori
VerbNet class 13.5.1
(VP(VB(target))(NP))
(VP(VBG(target))(NP))
(VP(VBD(target))(NP))
(VP(TO)(VP(VB(target))(NP)))
(S(NP-SBJ)(VP(VBP(target))(NP)))
VerbNet class 60
(VBN(target))
(VP(VBD(target))(S))
(VP(VBZ(target))(S))
(VBP(target))
(VP(VBD(target))(NP-1)(S(NP-SBJ)(VP)))
Table 5: CT fragments
analysis of the learned models (i.e. the underlying
support vectors) confirm very interesting grammati-
cal generalizations, i.e. the capability of tree kernels
to implicitly trigger useful linguistic inductions for
complex semantic tasks. When SPTK are adopted,
verb arguments can be lexically generalized into
word classes, i.e., clusters of argument heads (e.g.
commission vs. delegation, or gift vs. present). Au-
tomatic generation of such classes is an interesting
direction for future research.
7 Conclusion
We have proposed new approaches to characterize
verb classes in learning algorithms. The key idea is
the use of structural representation of verbs based on
syntactic dependencies and the use of structural ker-
nels to measure similarity between such representa-
tions. The advantage of kernel methods is that they
can be directly used in some learning algorithms,
e.g., SVMs, to train verb classifiers. Very interest-
ingly, we can encode distributional lexical similar-
ity in the similarity function acting over syntactic
structures and this allows for generalizing selection
restrictions through a sort of (supervised) syntactic
and semantic co-clustering.
The verb classification results show a large im-
provement over the state-of-the-art for both Verb-
Net and FrameNet, with a relative error reduction
of about 13.5% and 16.0%, respectively. In the fu-
ture, we plan to exploit the models learned from
FrameNet and VerbNet to carry out automatic map-
ping of verbs from one theory to the other.
Acknowledgements This research is partially sup-
ported by the European Community?s Seventh Frame-
work Programme (FP7/2007-2013) under grant numbers
247758 (ETERNALS), 288024 (LIMOSINE) and 231126
(LIVINGKNOWLEDGE). Many thanks to the reviewers
for their valuable suggestions.
270
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a collec-
tion of very large linguistically processed web-crawled
corpora. LRE, 43(3):209?226.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text clas-
sification. In Gianni Amati, Claudio Carpineto, and
Gianni Romano, editors, Proceedings of ECIR, vol-
ume 4425 of Lecture Notes in Computer Science,
pages 307?318. Springer, APR.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In CIKM?07: Proceedings of the sixteenth ACM con-
ference on Conference on information and knowledge
management, pages 861?864, New York, NY, USA.
ACM.
Susan Windisch Brown, Dmitriy Dligach, and Martha
Palmer. 2011. Verbnet class assignment as a wsd task.
In Proceedings of the Ninth International Conference
on Computational Semantics, IWCS ?11, pages 85?94,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724?731,
Vancouver, British Columbia, Canada, October.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059?1082.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL?00.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL?02.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2001. Latent semantic kernels. In Carla Brodley and
Andrea Danyluk, editors, Proceedings of ICML-01,
18th International Conference on Machine Learning,
pages 66?73, Williams College, US. Morgan Kauf-
mann Publishers, San Francisco, US.
Danilo Croce, Cristina Giannone, Paolo Annesi, and
Roberto Basili. 2010. Towards open-domain semantic
role labeling. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 237?246, Uppsala, Sweden, July. Association
for Computational Linguistics.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured Lexical Similarity via Convolution
Kernels on Dependency Trees. In Proceedings of
EMNLP 2011.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daume? III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP?04.
Dmitriy Dligach and Martha Palmer. 2008. Novel se-
mantic features for verb sense disambiguation. In
ACL (Short Papers), pages 29?32. The Association for
Computer Linguistics.
J. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis. Philological
Society, Oxford. reprinted in Palmer, F. (ed. 1968) Se-
lected Papers of J. R. Firth, Longman, Harlow.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proc. of SIGIR ?88, New York, USA.
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA.
Ana-Maria Giuglea and Alessandro Moschitti. 2006. Se-
mantic role labeling via framenet, verbnet and prop-
bank. In Proceedings of ACL, pages 929?936, Sydney,
Australia, July.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings of ACL?05, pages 403?410.
G. Golub and W. Kahan. 1965. Calculating the singular
values and pseudo-inverse of a matrix. Journal of the
Society for Industrial and Applied Mathematics: Se-
ries B, Numerical Analysis.
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proceedings of
ICML?00.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of
CoNLL 2008, pages 183?187.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL?03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Tom Landauer and Sue Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis theory
271
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar word. In Proceedings of COLING-ACL, Mon-
treal, Canada.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between prop-
bank and verbnet. In In Proceedings of the 7th Inter-
national Workshop on Computational Linguistics.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In HLT-NAACL,
pages 1020?1028.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. Isp:
Learning inferential selectional preferences. In Pro-
ceedings of HLT/NAACL 2007.
Daniele Pighin and Alessandro Moschitti. 2009a. Ef-
ficient linearization of tree kernel functions. In Pro-
ceedings of CoNLL?09.
Daniele Pighin and Alessandro Moschitti. 2009b. Re-
verse engineering of tree kernel feature spaces. In Pro-
ceedings of EMNLP, pages 111?120, Singapore, Au-
gust. Association for Computational Linguistics.
Daniele Pighin and Alessandro Moschitti. 2010. On
reverse feature engineering of syntactic tree kernels.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, CoNLL ?10,
pages 223?233, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. Journal of Machine Learning
Research, 5:101?141.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylyania.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97?
123.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Empirical Methods for Natural Language Processing
(EMNLP), pages 89?96, Sapporo, Japan.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37:141?
188.
Ludwig Wittgenstein. 1953. Philosophical Investiga-
tions. Blackwells, Oxford.
Alexander S. Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In COLING,
pages 947?953.
Ben?at Zapirain, Eneko Agirre, Llu??s Ma`rquez, and Mi-
hai Surdeanu. 2010. Improving semantic role classi-
fication with selectional preferences. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 373?376,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
272
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 597?602,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNITOR: Combining Semantic Text Similarity functions
through SV Regression
Danilo Croce, Paolo Annesi, Valerio Storch and Roberto Basili
Department of Enterprise Engineering
University of Roma, Tor Vergata
00133 Roma, Italy
{croce,annesi,storch,basili}@info.uniroma2.it
Abstract
This paper presents the UNITOR system that
participated to the SemEval 2012 Task 6: Se-
mantic Textual Similarity (STS). The task is
here modeled as a Support Vector (SV) regres-
sion problem, where a similarity scoring func-
tion between text pairs is acquired from exam-
ples. The semantic relatedness between sen-
tences is modeled in an unsupervised fashion
through different similarity functions, each
capturing a specific semantic aspect of the
STS, e.g. syntactic vs. lexical or topical vs.
paradigmatic similarity. The SV regressor ef-
fectively combines the different models, learn-
ing a scoring function that weights individual
scores in a unique resulting STS. It provides a
highly portable method as it does not depend
on any manually built resource (e.g. WordNet)
nor controlled, e.g. aligned, corpus.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic equivalence between two phrases
or texts. An effective method to compute similar-
ity between short texts or sentences has many appli-
cations in Natural Language Processing (Mihalcea
et al, 2006) and related areas such as Information
Retrieval, e.g. to improve the effectiveness of a se-
mantic search engine (Sahami and Heilman, 2006),
or databases, where text similarity can be used in
schema matching to solve semantic heterogeneity
(Islam and Inkpen, 2008).
STS is here modeled as a Support Vector (SV) re-
gression problem, where a SV regressor learns the
similarity function over text pairs. Regression learn-
ing has been already applied to different NLP tasks.
In (Pang and Lee, 2005) it is applied to Opinion
Mining, in particular to the rating-inference prob-
lem, wherein one must determine an author evalua-
tion with respect to a multi-point scale. In (Albrecht
and Hwa, 2007) a method is proposed for develop-
ing sentence-level MT evaluation metrics using re-
gression learning without directly relying on human
reference translations. In (Biadsy et al, 2008) it has
been used to rank candidate sentences for the task
of producing biographies from Wikipedia. Finally,
in (Becker et al, 2011) SV regressor has been used
to rank questions within their context in the multi-
modal tutorial dialogue problem.
In this paper, the semantic relatedness between
two sentences is modeled as a combination of dif-
ferent similarity functions, each describing the anal-
ogy between the two texts according to a specific
semantic perspective: in this way, we aim at captur-
ing syntactic and lexical equivalences between sen-
tences and exploiting either topical relatedness or
paradigmatic similarity between individual words.
The variety of semantic evidences that a system can
employ here grows quickly, according to the genre
and complexity of the targeted sentences. We thus
propose to combine such a body of evidence to learn
a comprehensive scoring function y = f(~x) over in-
dividual measures from labeled data through SV re-
gression: y is the gold similarity score (provided by
human annotators), while ~x is the vector of the dif-
ferent individual scores, provided by the chosen sim-
ilarity functions. The regressor objective is to learn
the proper combination of different functions redun-
dantly applied in an unsupervised fashion, without
involving any in-depth description of the target do-
main or prior knowledge. The resulting function se-
lects and filters the most useful information and it
597
is a highly portable method. In fact, it does not de-
pend on manually built resources (e.g. WordNet),
but mainly exploits distributional analysis of unla-
beled corpora.
In Section 2, the employed similarity functions
are described and the application of SV regression
is presented. Finally, Section 3 discusses results on
the SemEval 2012 - Task 6.
2 Combining different similarity function
through SV regression
This section describes the UNITOR systems partic-
ipating to the SemEval 2012 Task 6: in Section 2.1
the different similarity functions between sentence
pairs are discussed, while Section 2.2 describes how
the SV regression learning is applied.
2.1 STS functions
Each STS depends on a variety of linguistic aspects
in data, e.g. syntactic or lexical information. While
their supervised combination can be derived through
SV regression, different unsupervised estimators of
STS exist.
Lexical Overlap (LO). A basic similarity function
is first employed as the lexical overlap between sen-
tences, i.e. the cardinality of the set of words occur-
ring in both sentences.
Document-oriented similarity based on Latent
Semantic Analysis (LSA). This function captures
latent semantic topics through LSA. The adjacency
terms-by-documents matrix is first acquired through
the distributional analysis of a corpus and reduced
through the application of Singular Value Decom-
position (SVD), as described in (Landauer and Du-
mais, 1997). In this work, the individual sentences
are assumed as pseudo documents and represented
by vectors in the lower dimensional LSA space. The
cosine similarity between vectors of a sentence pair
is the metric hereafter referred to as topical similar-
ity.
Compositional Distributional Semantics (CDS).
Lexical similarity can also be extended to account
for syntactic compositions between words. This
makes sentence similarity to depend on the set of in-
dividual compounds, e.g. subject-verb relationship
instances. While basic lexical information can still
be obtained by distributional analysis, phrase level
Figure 1: Example of dependency graph
similarity can be here modeled as a specific func-
tion of the co-occurring words, i.e. a complex alge-
braic composition of their corresponding word vec-
tors. Differently from the document-oriented case
used in the LSA function, base lexical vectors are
here derived from co-occurrence counts in a word
space, built according to the method discussed in
(Sahlgren, 2006; Croce and Previtali, 2010). In or-
der to keep dimensionality as low as possible, SVD
is also applied here (Annesi et al, 2012). The result
is that every noun, verb, adjective and adverb is then
projected in the reduced word space and then dif-
ferent composition functions can be applied as dis-
cussed in (Mitchell and Lapata, 2010) or (Annesi et
al., 2012).
Convolution kernel-based similarity. The similar-
ity function is here the Smoothed Partial Tree Ker-
nel (SPTK) proposed in (Croce et al, 2011). This
convolution kernel estimates the similarity between
sentences, according to the syntactic and lexical in-
formation in both sentences. Syntactic representa-
tion of a sentence like ?A man is riding a bicycle? is
derived from the dependency parse tree, as shown
in Fig. 1. It allows to define different tree struc-
tures over which the SPTK operates. First, a tree
including only lexemes, where edges encode their
dependencies, is generated and called Lexical Only
Centered Tree (LOCT), see Fig. 2. Then, we add
to each lexical node two leftmost children, encod-
ing the grammatical function and the POS-Tag re-
spectively: it is the so-called Lexical Centered Tree
(LCT), see Fig. 3. Finally, we generate the Gram-
matical Relation Centered Tree (GRCT), see Fig.
4, by setting grammatical relation as non-terminal
nodes, while PoS-Tags are pre-terminals and fathers
of their associated lexemes. Each tree representation
provides a different kernel function so that three dif-
ferent SPTK similarity scores, i.e. LOCT, LCT and
GRCT, are here obtained.
598
be::v
ride::v
bicycle::n
a::d
man::n
a::d
Figure 2: Lexical Only Centered Tree (LOCT)
be::v
VBZROOTride::v
VBGVCbicycle::n
NNOBJa::d
DTNMOD
man::n
NNSBJa::d
DTNMOD
Figure 3: Lexical Centered Tree (LCT)
ROOT
VC
OBJ
NN
bicycle::n
NMOD
DT
a::d
VBG
ride::v
VBZ
be::v
SBJ
NN
man::n
NMOD
DT
a::d
Figure 4: Grammatical Relation Centered Tree (GRCT)
2.2 Combining STSs with SV Regression
The similarity functions described above provide
scores capturing different linguistic aspects and an
effective way to combine such information is made
available by Support Vector (SV) regression, de-
scribed in (Smola and Scho?lkopf, 2004). The idea
is to learn a higher level model by weighting scores
according to specific needs implicit in training data.
Given similarity scores ~xi for the i-th sentence pair,
the regressor learns a function yi = f(~xi), where yi
is the score provided by human annotators.
The ?-SV regression (Vapnik, 1995) algorithm al-
lows to define the best f approximating the train-
ing data, i.e. the function that has at most ? de-
viation from the actually obtained targets yi for
all the training data. Given a training dataset
{(~x1, y1), . . . , (~xl, yl)} ? X ? R, where X is the
space of the input patterns, i.e. the original similar-
ity scores, we can acquire a linear function
f(~x) = ?~w, ~x?+ b with ~w ? X, b ? R
by solving the following optimization problem:
minimize
1
2
||~w||2
subject to
{
yi ? ?~w, ~xi? ? b ? ?
?~w, ~xi?+ b? yi ? ?
Since the function f approximating all pairs
(~xi, yi) with ? precision, may not exist, i.e. the con-
vex optimization problem is infeasible, slack vari-
ables ?i, ??i are introduced:
minimize
1
2
||~w||2 + C
l?
i=1
(?i + ?
?
i )
subject to
?
??
??
yi ? ?~w, ~xi? ? b ? ?+ ?i
?~w, ~xi?+ b? yi ? ?+ ??i
?i, ??i ? 0
where ?i, ??i measure the error introduced by training
data with a deviation higher than ? and the constant
C > 0 determines the trade-off between the norm
?~w? and the amount up to which deviations larger
than ? are tolerated.
3 Experimental Evaluation
This section describes results obtained in the Se-
mEval 2012 Task 6: STS. First, the experimental
setup of different similarity functions is described.
Then, results obtained over training datasets are re-
ported. Finally, results achieved in the competition
are discussed.
3.1 Experimental setup
In order to estimate the Latent Semantic Analysis
(LSA) based similarity function, the distributional
analysis of the English version of the Europarl Cor-
pus (Koehn, 2002) has been carried out. It is the
same source corpus of the SMTeuroparl dataset and
it allows to acquire a semantic space capturing the
same topics characterizing this dataset. A word-by-
sentence matrix models the sentence representation
space. The entire corpus has been split so that each
vector represents a sentence: the number of different
sentences is about 1.8 million and the matrix cells
contain tf-idf scores between words and sentences.
The SVD is applied and the space dimensionality
599
is reduced to k = 250. Novel sentences are im-
mersed in the reduced space, as described in (Lan-
dauer and Dumais, 1997) and the LSA-based simi-
larity between two sentences is estimated according
the cosine similarity.
To estimate the Compositional Distributional Se-
mantics (CDS) based function, a co-occurrence
Word Space is first acquired through the distribu-
tional analysis of the UKWaC corpus (Baroni et al,
2009), i.e. a Web document collection made of
about 2 billion tokens. UKWaC is larger than the
Europarl corpus and we expect it makes available
a more general lexical representation suited for all
datasets. An approach similar to the one described in
(Croce and Previtali, 2010) has been adopted for the
acquisition of the word space. First, all words occur-
ring more than 200 times (i.e. the targets) are rep-
resented through vectors. The original space dimen-
sions are generated from the set of the 20,000 most
frequent words (i.e. features) in the UKWaC cor-
pus. One dimension describes the Pointwise Mutual
Information score between one feature as it occurs
on a left or right window of 3 tokens around a target.
Left contexts of targets are treated differently from
the right ones, in order to also capture asymmetric
syntactic behaviors (e.g., useful for verbs): 40,000
dimensional vectors are thus derived for each target.
The particularly small window size allows to better
capture paradigmatic relations between targets, e.g.
hyponymy or synonymy. Again, the SVD reduction
is applied to the original matrix with a k = 250.
Once lexical vectors are available, a compositional
similarity measure can be obtained by combining
the word vectors according to a CDS operator, e.g.
(Mitchell and Lapata, 2010) or (Annesi et al, 2012).
In this work, the adopted compositional representa-
tion is the additive operator between lexical vectors,
as described in (Mitchell and Lapata, 2010) and the
similarity function between two sentences is the co-
sine similarity between their corresponding compo-
sitional vectors. Moreover, two additive operators
that only sum over nouns and verbs are also adopted,
denoted by CDSV and CDSN , respectively.
The estimation of the semantically Smoothed Par-
tial Tree Kernel (SPTK) is made available by an ex-
tended version of SVM-LightTK software1 (Mos-
1http://disi.unitn.it/moschitti/Tree-Kernel.htm
chitti, 2006) implementing the smooth matching be-
tween tree nodes. The tree representation described
in Sec. 2.1 allows to define 3 different kernels, i.e.
SPTKLOCT , SPTKLCT and SPTKGRCT . Similarity
between lexical nodes is estimated as the cosine sim-
ilarity in the co-occurrence Word Space described
above, as in (Croce et al, 2011).
In all corpus analysis and experiments, sentences
are processed with the LTH dependency parser, de-
scribed in (Johansson and Nugues, 2007), for Part-
of-speech tagging and lemmatization. Dependency
parsing of datasets is required for the SPTK appli-
cation. Finally, SVM-LightTK is employed for the
SV regression learning to combine specific similar-
ity functions.
3.2 Evaluating the impact of unsupervised
models
Table 1 compares the Pearson Correlation of differ-
ent similarity functions described in Section 2.1, i.e.
mainly the results of the unsupervised approaches,
against the challenge training data. Regarding to
MSRvid dataset, the topical similarity (LSA func-
tion) achieves the best result, i.e. 0.748. Paradig-
matic lexical information as in CDS, CDSN and LO
provides also good results, confirming the impact of
lexical generalization. However, only nouns seem
to contribute significantly, as for the poor results of
CDSV suggest. As the dataset is characterized by
short sentences with negligible syntactic differences,
SPTK-based kernels are not discriminant. On the
contrary, the SPTKLCT achieves the best result in
the MSRpar dataset, where paraphrasing phenom-
ena are peculiar. Notice that the other SPTK kernels
are not equivalently performant, in line with previ-
ous results on question classification and semantic
role labeling (Croce et al, 2011). Lexical informa-
tion provides a crucial contribution also for LO, al-
though the contribution of topical or paradigmatic
generalization seems negligible over MSRpar. Fi-
nally, in the SMTeuroparl, longer sentences are the
norm and length seems to compromise the perfor-
mance of LO. The best results seem to require the
lexical and syntactic information provided by CDS
and SPTK.
600
Models
Dataset
MSRvid MSRpar SMTeuroparl
CDS .652 .393 .681
CDSN .630 .234 .485
CDSV .219 .317 .264
LSA .748 .344 .477
SPTKLOCT .300 .251 .611
SPTKLCT .297 .464 .622
SPTKGRCT .278 .255 .626
LO .560 .446 .248
Table 1: Unsupervised results over the training dataset
3.3 Evaluating the role of SV regression
The SV regressors have been trained over a feature
space that enumerates the different similarity func-
tions: one feature is provided by the LSA function,
three by the CDS, i.e. CDS, CDSN and CDSV ,
three by SPTK, i.e. SPTKLOCT , SPTKLCT and
SPTKGRCT and one by LO, i.e. the number of
words in common. Two more features are obtained
by the sentence lengths of a pair, i.e. the number
of words in the first and second sentence, respec-
tively. Table 2 shows Pearson Correlation results
when the regressor is trained according a 10-fold
cross validation schema. First, all possible feature
combinations are attempted for the SV regression,
so that every subset of the 10 features is evaluated.
Results of the best feature combination are shown in
column bestfeat: for MSRvid, the best performance
is achieved when all 10 features are considered; in
MSRpar, SPTK combined with LO is sufficient; fi-
nally, in the SMTeuroparl the combination is LO,
CDS and SPTK. In column allfeat results achieved
by considering all features are reported. Last col-
umn specifies the performance increase with respect
to the corresponding best results in the unsupervised
settings.
Results of the regressors are always higher with
respect to the unsupervised settings, with up to a
35% improvement for the MSRpar, i.e. the most
complex domain. Moreover, differences when best
and all features are employed are negligible. It
means that SV regressor allows to automatically
combine and select the most informative similarity
aspects, confirming the applicability of the proposed
redundant approach to STS.
Dataset
Experiments
Gain
bestfeat allfeat
MSRvid .789 .789 5,0%
MSRpar .615 .612 32,4%
SMTeuroparl .692 .691 1,6%
Table 2: SV regressor results over the training dataset
3.4 Results over the SemEval Task 6
According to the above evidence, we participated to
the SemEval challenge with three different systems.
Sys1 - Best Features. Scores between pairs from a
specific dataset are obtained by applying a regressor
trained over pairs from the same dataset. It means
that, for example, the test pairs from the MSRvid
dataset are processed with a regressor trained over
the MSRvid training data. Moreover, the most rep-
resentative similarity function estimated for the col-
lection is employed: the feature combination provid-
ing the best correlation results over training pairs is
adopted for the test. The same is applied to MSRpar
and SMTeuroparl. No selection is adopted for the
Surprise data and training data for all the domains
are used, as described in Sys3.
Sys2 - All Features. Relatedness scores between
pairs from a specific dataset are obtained using a re-
gressor trained using pairs from the same dataset.
Differently from the Sys1, the similarity function
here is employed within the SV regressors trained
over all 10 similarity functions (i.e. all features).
Sys3 - All features and All domains. The SV re-
gressor is trained using training pairs from all col-
lections and over all 10 features. It means that one
single model is trained and employed to score all
test data. This approach is also used for the Surprise
data, i.e. the OnWN and SMTnews datasets.
Table 3 reports the general outcome for the UN-
ITOR systems. Rank of the individual scores with
respect to the other systems participating to the chal-
lenge is reported in parenthesis. This allows to draw
some conclusions. First, the proposed system ranks
around the 12 and 13 system positions (out of 89
systems), and the 6th group. The adoption of all pro-
posed features suggests that more evidence is better,
as it can be properly modeled by regression. It seems
generally better suited for the variety of semantic
phenomena observed in the tests. Regressors seem
601
Dataset
Results
BL Sys1 Sys2 Sys3
MSRvid .299 .821 .821 .802
MSRpar .433 .569 .576 .468
SMTeuroparl .454 .516 .510 .457
surp.OnWN .586 .659
surp.SMTnews .390 .471
ALL .311 .747 (13) .747 (12) .628 (40)
ALLnrm .673 .829 (12) .830 (11) .815 (21)
Mean .436 .632 (10) .632 ( 9) .594 (28)
Table 3: Results over the challenge test dataset
to be robust enough to select the proper features and
make the feature selection step (through collection
specific cross-validation) useless. Collection spe-
cific training seems useful, as Sys3 achieves lower
results, basically due to the significant stylistic dif-
ferences across the collections. However, the good
level of accuracy achieved over the surprise data sets
(between 11% and 17% performance gain with re-
spect to the baselines) confirms the large applica-
bility of the overall technique: our system in fact
does not depend on any manually coded resource
(e.g. WordNet) nor on any controlled (e.g. parallel
or aligned) corpus. Future work includes the study
of the learning rate and its correlation with differ-
ent and richer similarity functions, e.g. CDS as in
(Annesi et al, 2012).
Acknowledgements This research is partially
supported by the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant numbers 262491 (INSEARCH). Many
thanks to the reviewers for their valuable sugges-
tions.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression for
sentence-level mt evaluation with pseudo references.
In Proceedings of ACL, pages 296?303, Prague, Czech
Republic, June.
Paolo Annesi, Valerio Storch, and Roberto Basili. 2012.
Space projections as distributional models for seman-
tic composition. In CICLing (1), Lecture Notes in
Computer Science, pages 323?335. Springer.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Lee Becker, Martha Palmer, Sarel van Vuuren, and
Wayne Ward. 2011. Evaluating questions in context.
Fadi Biadsy, Julia Hirschberg, and Elena Filatova. 2008.
An unsupervised approach to biography production
using wikipedia. In ACL, pages 807?815.
Danilo Croce and Daniele Previtali. 2010. Manifold
learning for the semi-supervised induction of framenet
predicates: An empirical investigation. In Proceed-
ings of the GEMS 2010 Workshop, pages 7?16, Upp-
sala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Trans. Knowl. Discov. Data,
2:10:1?10:25, July.
Richard Johansson and Pierre Nugues. 2007. Semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval-2007, Prague, Czech
Republic, June 23-24.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation. Draft.
Thomas K Landauer and Susan T. Dumais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, pages 211?240.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In In AAAI06.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, WWW ?06,
pages 377?386, New York, NY, USA. ACM.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199?222, August.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer?Verlag, New York.
602
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 59?65, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UNITOR-CORE TYPED: Combining Text Similarity
and Semantic Filters through SV Regression
Danilo Croce, Valerio Storch and Roberto Basili
Department of Enterprise Engineering
University of Roma, Tor Vergata
00133 Roma, Italy
{croce,storch,basili}@info.uniroma2.it
Abstract
This paper presents the UNITOR system that
participated in the *SEM 2013 shared task on
Semantic Textual Similarity (STS). The task is
modeled as a Support Vector (SV) regression
problem, where a similarity scoring function
between text pairs is acquired from examples.
The proposed approach has been implemented
in a system that aims at providing high ap-
plicability and robustness, in order to reduce
the risk of over-fitting over a specific datasets.
Moreover, the approach does not require any
manually coded resource (e.g. WordNet), but
mainly exploits distributional analysis of un-
labeled corpora. A good level of accuracy is
achieved over the shared task: in the Typed
STS task the proposed system ranks in 1st and
2nd position.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic equivalence between two phrases
or texts. An effective method to compute similarity
between sentences or semi-structured material has
many applications in Natural Language Processing
(Mihalcea et al, 2006) and related areas such as
Information Retrieval, improving the effectiveness
of semantic search engines (Sahami and Heilman,
2006), or databases, using text similarity in schema
matching to solve semantic heterogeneity (Islam and
Inkpen, 2008).
This paper describes the UNITOR system partic-
ipating in both tasks of the *SEM 2013 shared task
on Semantic Textual Similarity (STS), described in
(Agirre et al, 2013):
? the Core STS tasks: given two sentences, s1
and s2, participants are asked to provide a score
reflecting the corresponding text similarity. It is
the same task proposed in (Agirre et al, 2012).
? the Typed-similarity STS task: given two
semi-structured records t1 and t2, containing
several typed fields with textual values, partic-
ipants are asked to provide multiple similarity
scores: the types of similarity to be studied in-
clude location, author, people involved, time,
events or actions, subject and description.
In line with several participants of the STS 2012
challenge, such as (Banea et al, 2012; Croce et al,
2012a; S?aric? et al, 2012), STS is here modeled as
a Support Vector (SV) regression problem, where a
SV regressor learns the similarity function over text
pairs. The semantic relatedness between two sen-
tences is first modeled in an unsupervised fashion
by several similarity functions, each describing the
analogy between the two texts according to a spe-
cific semantic perspective. We aim at capturing sep-
arately syntactic and lexical equivalences between
sentences and exploiting either topical relatedness or
paradigmatic similarity between individual words.
Such information is then combined in a supervised
schema through a scoring function y = f(~x) over
individual measures from labeled data through SV
regression: y is the gold similarity score (provided
by human annotators), while ~x is the vector of the
different individual scores, provided by the chosen
similarity functions.
For the Typed STS task, given the specificity of
the involved information and the heterogeneity of
target scores, individual measures are not applied to
entire texts. Specific phrases are filtered according
to linguistic policies, e.g. words characterized by
specific Part-of-Speech (POS), such as nouns and
verbs, or Named Entity (NE) Category, i.e. men-
59
tions to specific name classes, such as of a PER-
SON, LOCATION or DATE. The former allows to
focus the similarity functions over entities (nouns)
or actions (verbs), while the latter allows to focus on
some aspects connected with the targeted similarity
functions, such as person involved, location or time.
The proposed approach has been implemented in
a system that aims at providing high applicability
and robustness. This objective is pursued by adopt-
ing four similarity measures designed to avoid the
risk of over-fitting over each specific dataset. More-
over, the approach does not require any manually
coded resource (e.g. WordNet), but mainly exploits
distributional analysis of unlabeled corpora. Despite
of its simplicity, a good level of accuracy is achieved
over the 2013 STS challenge: in the Typed STS task
the proposed system ranks 1st and 2nd position (out
of 18); in the Core STS task, it ranks around the 37th
position (out of 90) and a simple refinement to our
model makes it 19th.
In the rest of the paper, in Section 2, the employed
similarity functions are described and the applica-
tion of SV regression is presented. Finally, Section
3 discusses results on the *SEM 2013 shared task.
2 Similarity functions, regression and
linguistic filtering
This section describes the approach behind the UN-
ITOR system. The basic similarity functions and
their combination via SV regressor are discussed in
Section 2.1, while the linguistic filters are presented
in Section 2.2.
2.1 STS functions
Each STS function depends on a variety of linguistic
aspects in data, e.g. syntactic or lexical information.
While their supervised combination can be derived
through SV regression, different unsupervised esti-
mators of STS exist.
Lexical Overlap. A basic similarity function is
modeled as the Lexical Overlap (LO) between sen-
tences. Given the sets Wa and Wb of words oc-
curring in two generic texts ta and tb, LO is esti-
mated as the Jaccard Similarity between the sets, i.e.
LO= |Wa?Wb||Wa?Wb| . In order to reduce data sparseness,
lemmatization is applied and each word is enriched
with its POS to avoid the confusion between words
from different grammatical classes.
Compositional Distributional Semantics. Other
similarity functions are obtained by accounting for
the syntactic composition of the lexical information
involved in the sentences. Basic lexical information
is obtained by a co-occurrence Word Space that is
built according to (Sahlgren, 2006; Croce and Pre-
vitali, 2010). Every word appearing in a sentence is
then projected in such space. A sentence can be thus
represented neglecting its syntactic structure, by ap-
plying an additive linear combination, i.e. the so-
called SUM operator. The similarity function be-
tween two sentences is then the cosine similarity be-
tween their corresponding vectors.
A second function is obtained by applying a Dis-
tributional Compositional Semantics operator, in
line with the approaches introduced in (Mitchell and
Lapata, 2010), and it is adopted to account for se-
mantic composition. In particular, the approach de-
scribed in (Croce et al, 2012c) has been applied.
It is based on space projection operations over ba-
sic geometric lexical representations: syntactic bi-
grams are projected in the so called Support Sub-
space (Annesi et al, 2012), aimed at emphasiz-
ing the semantic features shared by the compound
words. The aim is to model semantics of syntac-
tic bi-grams as projections in lexically-driven sub-
spaces. In order to extend this approach to handle
entire sentences, we need to convert them in syn-
tactic representations compatible with the compo-
sitional operators proposed. A dependency gram-
mar based formalism captures binary syntactic re-
lations between the words, expressed as nodes in
a dependency graph. Given a sentence, the parse
structure is acquired and different triples (w1, w2, r)
are generated, where w1 is the relation governor, w2
is the dependent and r is the grammatical type. In
(Croce et al, 2012c) a simple approach is defined,
and it is inspired by the notion of Soft Cardinal-
ity, (Jimenez et al, 2012). Given a triple set T =
{t1, . . . , tn} extracted from a sentence S and a sim-
ilarity sim(ti, tj), the Soft Cardinality is estimated
as |S|?sim u
?|T |
ti (
?|T |
tj sim(ti, tj)
p)?1, where pa-
rameter p controls the ?softness? of the cardinality:
with p = 1 element similarities are unchanged while
higher value will tend to the Classical Cardinality
measure. Notice that differently from the previous
60
usage of the Soft Cardinality notion, we did not ap-
ply it to sets of individual words, but to the sets of
dependencies (i.e. triples) derived from the two sen-
tences. The sim function here can be thus replaced
by any compositional operator among the ones dis-
cussed in (Annesi et al, 2012). Given two sen-
tences, higher Soft Cardinality values mean that the
elements in both sentences (i.e. triples) are different,
while the lower values mean that common triples are
identical or very similar, suggesting that sentences
contain the same kind of information. Given the sets
of triples A and B extracted from the two candidate
sentences, our approach estimates a syntactically re-
stricted soft cardinality operator, the Syntactic Soft
Cardinality (SSC) as SSC(A,B) = 2|A?B|
?
|A|?+|B|? , as
a ?soft approximation? of Dice?s coefficient calcu-
lated on both sets1.
capture::v
VBNROOTmarine::n
NNSPREP-BYmexico::n
NNPPREP-IN
lord::n
NNNSUBJdrug::n
NNNN
Figure 1: Lexical Centered Tree (LCT)
Convolution kernel-based similarity. The similar-
ity function is here the Smoothed Partial Tree Ker-
nel (SPTK) proposed in (Croce et al, 2011). SPTK
is a generalized formulation of a Convolution Ker-
nel function (Haussler, 1999), i.e. the Tree Kernel
(TK), by extending the similarity between tree struc-
tures with a function of node similarity. The main
characteristic of SPTK is its ability to measure the
similarity between syntactic tree structures, which
are partially similar and whose nodes can differ but
are semantically related. One of the most important
outcomes is that SPTK allows ?embedding? exter-
nal lexical information in the kernel function only
through a similarity function among lexical nodes,
namely words. Moreover, SPTK only requires this
similarity to be a valid kernel itself. This means that
such lexical information can be derived from lexical
resources or it can be automatically acquired by a
Word Space. The SPTK is applied to a specific tree
representation that allowed to achieve state-of-the-
1Notice that, since the intersection |A ? B|? tends to be too
strict, we approximate it from the union cardinality estimation
|A|? + |B|? ? |A ?B|?.
art results on several complex semantic tasks, such
as Question Classification (Croce et al, 2011) or
Verb Classification (Croce et al, 2012b): each sen-
tence is represented through the Lexical Centered
Tree (LCT), as shown in Figure 1 for the sentence
?Drug lord captured by Marines in Mexico?. It is de-
rived from the dependency parse tree: nodes reflect
lexemes and edges encode their syntactic dependen-
cies; then, we add to each lexical node two leftmost
children, encoding the grammatical function and the
POS-Tag respectively.
Combining STSs with SV Regression The similar-
ity functions described above provide scores captur-
ing different linguistic aspects and an effective way
to combine such information is made available by
Support Vector (SV) regression, described in (Smola
and Scho?lkopf, 2004). The idea is to learn a higher
level model by weighting scores according to spe-
cific needs implicit in training data. Given similar-
ity scores ~xi for the i-th sentence pair, the regressor
learns a function yi = f(~xi), where yi is the score
provided by human annotators. Moreover, since the
combination of kernel is still a kernel, we can ap-
ply polynomial and RBF kernels (Shawe-Taylor and
Cristianini, 2004) to the regressor.
2.2 Semantic constraints for the Typed STS
Typed STS insists on records, i.e. sequence of typed
textual fields, rather than on individual sentences.
Our aim is to model the typed task with the same
spirit as the core one, through a combination of
different linguistic evidences, which are modeled
through independent kernels. The overall similarity
model described in 2.1 has been thus applied also to
the typed task according to two main model changes:
? Semantic Modeling. Although SV regression
is still applied to model one similarity type,
each type depends on a subset of the multiple
evidences originating from individual fields:
one similarity type acts as a filter on the set of
fields, on which kernels will be then applied.
? Learning Constraints. The selected fields pro-
vide different evidences to the regression steps.
Correspondingly, each similarity type corre-
sponds to specific kernels and features for its
fields. These constraints are applied by select-
ing features and kernels for each field.
61
dcTitle dcSubject dcDescription dcCreator dcDate dcSource
author - - PER ? - -
people inv. PER PER PER - - -
time DATE DATE DATE - ? -
location LOC LOC LOC - - -
event N , V , N ? V N , V , N ? V N , V , N ? V - - -
subject N , V , J , N ? J ? V N , V , J , N ? J ? V - - - -
description - - N , V , J , N ? J ? V - - -
general + + + ? ? ?
Table 1: Filtering Schema adopted for the Typed STS task.
Notice how some kernels loose significance in the
typed STS task. Syntactic information is no useful
so that no tree kernel and compositional kernel is
applied here. Most of the fields are non-sentential2.
Moreover, not all morpho-syntactic information are
extracted as feature from some fields. Filters usu-
ally specify some syntactic categories or Named En-
tities (NEs): they are textual mentions to specific
real-world categories, such as of PERSONS (PER),
LOCATIONS (LOC) or DATES. They are detected
in a field and made available as feature to the cor-
responding kernel: this introduces a bias on typed
measures and emphasizes specific semantic aspects
(e.g. places LOC or persons PER, in location or au-
thor measures, respectively). For example, in the
sentence ?The chemist R.S. Hudson began manufac-
turing soap in the back of his small shop in West
Bomich in 1837?, when POS tag filters are applied,
only verbs (V), nouns (N) or adjectives (J) can be
selected as features. This allows to focus on spe-
cific actions, e.g. the verb ?manufacture?, entities,
e.g. nouns ?soap? and ?shop?, or some properties,
e.g. the adjective ?small?. When Named Entity cat-
egories are used, a mention to a person like ?R.S.
Hudson? or to a location, e.g. ?West Bomich?, or
date, e.g. ?1837?, can be useful to model the the
person involved, the location or time similarity mea-
sures, respectively.
The Semantic Modeling and the Learning Con-
straints system adopted to model the Typed STS
task are defined in Table 1. There rows are the
different target similarities, while columns indicate
document fields, such as dcTitle, dcSubject,
dcDescription, dcCreator, dcDate and
2The dcDescription is also made of multiple sen-
tences and it reduces the applicability of SPTK and SSC: parse
trees have no clear alignment.
dcSource, as described in the *SEM 2013 shared
task description. Each entry in the Table represents
the feature set for that fields, i.e. POS tags (i.e. V ,
N , J) or Named Entity classes. The ??? symbol
corresponds to all features, i.e. no restriction is
applied to any POS tag or NE class. Finally, the
general similarity function makes use of every NE
class and POS tags adopted for that field in any
measure, as expressed by the special notation +, i.e.
?all of the above features?.
Every feature set denoted in the Table 1 sup-
ports the application of a lexical kernel, such as
the LO described in Section 2.1. When different
POS tags are requested (such as N and V ) mul-
tiple feature sets and kernels are made available.
The ?-? symbol means that the source field is fully
neglected from the SV regression. As an exam-
ple, the SV regressor for the location similarity
has been acquired considering the fields dcTitle,
dcSubject, dcDescription. Only features used
for the kernel correspond to LOCATIONs (LOC). For
each of the three feature, the LO and SUM simi-
larity function has been applied, giving rise to an
input 6-dimensional feature space for the regressor.
Differently, in the subject similarity, nouns, adjec-
tives and verbs are the only features adopted from
the fields dcSubject, dcTitle, so that 8 feature
sets are used to model these fields, giving rise to a
16-dimensional feature space.
3 Results and discussion
This section describes results obtained in the *SEM
2013 shared task. The experimental setup of differ-
ent similarity functions is described in Section 3.1.
Results obtained over the Core STS task and Typed
STS task are described in Section 3.2 and 3.3.
62
3.1 Experimental setup
In all experiments, sentences are processed with the
Stanford CoreNLP3 system, for Part-of-Speech tag-
ging, lemmatization, named entity recognition4 and
dependency parsing.
In order to estimate the basic lexical similarity
function employed in the SUM, SSC and SPTK
operators, a co-occurrence Word Space is acquired
through the distributional analysis of the UkWaC
corpus (Baroni et al, 2009), a Web document col-
lection made of about 2 billion tokens. The same
setting of (Croce et al, 2012a) has been adopted
for the space acquisition. The same setup described
in (Croce et al, 2012c) is applied to estimate the
SSC function. The similarity between pairs of syn-
tactically restricted word compound is evaluated
through a Symmetric model: it selects the best 200
dimensions of the space, selected by maximizing the
component-wise product of each compound as in
(Annesi et al, 2012), and combines the similarity
scores measured in each couple subspace with the
product function. The similarity score in each sub-
space is obtained by summing the cosine similarity
of the corresponding projected words. The ?soft car-
dinality? is estimated with the parameter p = 2.
The estimation of the semantically Smoothed Par-
tial Tree Kernel (SPTK) is made available by an ex-
tended version of SVM-LightTK software5 (Mos-
chitti, 2006) implementing the smooth matching
between tree nodes. Similarity between lexical
nodes is estimated as the cosine similarity in the
co-occurrence Word Space described above, as in
(Croce et al, 2011). Finally, SVM-LightTK is em-
ployed for the SV regression learning to combine
specific similarity functions.
3.2 Results over the Core STS
In the Core STS task, the resulting text similarity
score is measured by the regressor: each sentence
pair from all datasets is modeled according to a 13
dimensional feature space derived from the different
functions introduced in Section 2.1, as follows.
The first 5 dimensions are derived by applying
3
http://nlp.stanford.edu/software/corenlp.shtml
4The TIME and DURATION classes are collapsed with
DATE, while the PERSON and LOCATION classes are consid-
ered without any modification.
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
Run1 Run2 Run3 Run?1
headlines .635 (50) .651 (39) .603 (58) .671 (30)
OnWN .574 (33) .561 (36) .549 (40) .637 (25)
FNWN .352 (35) .358 (32) .327 (44) .459 (07)
SMT .328 (39) .310 (49) .319 (44) .348 (21)
Mean .494 (37) .490 (42) .472 (52) .537 (19)
Table 2: Results over the Core STS task
the LO operator over lemmatized words in the noun,
verb, adjective and adverb POS categories: 4 ker-
nels look at individual categories, while a fifth ker-
nel insists on the union of all POS. A second set of
5 dimensions is derived by the same application of
the SUM operator to the same syntactic selection of
features. The SPTK is then applied to estimate the
similarity between the LCT structures derived from
the dependency parse trees of sentences. Then, the
SPTK is applied to derive an additional score with-
out considering any specific similarity function be-
tween lexical nodes; in this setting, the SPTK can be
considered as a traditional Partial Tree Kernel (Mos-
chitti, 2006), in order to capture a more strict syn-
tactical similarity between texts. The last score is
generated by applying the SSC operator.
We participated in the *SEM challenge with three
different runs. The main difference between each
run is the dataset employed in the training phase
and the employed kernel within the regressor. With-
out any specific information about the test datasets,
a strategy to prevent the regressor to over-fit train-
ing material has been applied. We decided to use
a training dataset that achieved the best results over
datasets radically different from the training material
in the STS challenge of Semeval 2012. In particular,
for the FNWN and OnWN datasets, we arbitrarily
selected the training material achieving best results
over the 2012 surprise.OnWN; for the headlines and
SMT datasets we maximized performance training
over surprise.SMTnews. In Run1 the SVM regres-
sor is trained using dataset combinations providing
best results according to the above criteria: MSR-
par, MSRvid, SMTeuroparl and surprise.OnWN are
employed against FNWN and OnWN; MSRpar,
SMTeuroparl and surprise.SMTnews are employed
against headline and SMT. A linear kernel is ap-
plied when training the regressor. In Run2, differ-
ently from the previous one, the SVM regressor is
63
rank general author people inv. time location event subject description mean
Run1 1 .7981 .8158 .6922 .7471 .7723 .6835 .7875 .7996 .7620
Run2 2 .7564 .8076 .6758 .7090 .7351 .6623 .7520 .7745 .7341
Table 3: Results over the Typed STS task
trained using all examples from the training datasets.
A linear kernel is applied when training the regres-
sor. Finally, in Run3 the same training dataset selec-
tion schema of Run1 is applied and a gaussian kernel
is employed in the regressor.
Table 2 reports the general outcome for the UN-
ITOR systems in term of Pearson Correlation. The
best system, based on the linear kernel, ranks around
the 35th position (out of 90 systems), that reflects
the mean rank of all the systems in the ranking of
the different datasets. The gaussian kernel, em-
ployed for the Run3 does not provide any contri-
bution, as it ranks 50th. We think that the main
reason of these results is due to the intrinsic dif-
ferences between training and testing datasets that
have been heuristically coupled. This is first mo-
tivated by lower rank achieved by Run2. More-
over, it is in line with the experimental findings of
(Croce et al, 2012a), where a performance drop is
shown when the regressor is trained over data that
is not constrained over the corresponding source.
In Run?1 we thus optimized the system by manu-
ally selecting the training material that does provides
best performance on the test dataset: MSRvid, SM-
Teuroparl and surprise.OnWN are employed against
OnWN; surprise.OnWN against FNWN, SMTeu-
roparl against headlines; SMTeuroparl and sur-
prise.SMTnews against SMT. A linear kernel within
the regressor allow to reach the 19th position, even
reducing the complexity of the representation to a
five dimensional feature space: LO and SUM with-
out any specific filter, SPTK, PTK and SSC.
3.3 Results over the Typed STS
SV regression has been also applied to the Typed
STS task through seven type-specific regressors plus
a general one. Each SV regressor insists on the LO
and SUM kernel as applied to the features in Table
1. Notice that it was mainly due to the lack of rich
syntactic structures in almost all fields.
As described in Section 2.2, a specific modeling
strategy has been applied to derive the feature space
of each target similarity. For example, the regres-
sor associated with the event similarity score is fed
with 18 scores. Each of the 3 fields, , i.e. dcTitle,
dcSubject and dcDescription, provides the 2
kernels (LO and SUM) with 3 feature sets (i.e. N ,
V and N ? V ). In particular, the general simi-
larity function considers all extracted features for
each field, giving rise to a space of 51 dimensions.
We participated in the task with two different runs,
whose main difference is the adopted kernel within
the SV regressor. In Run1, a linear kernel is used,
while in Run2 a RBF kernel is applied.
Table 3 reports the general outcome for the UN-
ITOR system. The adopted semantic modeling, as
well as the selection of the proper information, e.g.
the proper named entity, allows the system to rank
in the 1st and 2nd positions (out of 18 systems). The
proposed selection schema in Table 1 is very effec-
tive, as confirmed by the results for almost all typed
similarity scores. Again, the RBF kernel does not
improve result over the linear kernel. The impact
of the proposed approach can be noticed for very
specific scores, such as time and location, especially
for text pairs where structured information is absent,
such as in the dcDate field. Moreover, the regres-
sor is not affected by the differences between train-
ing and test dataset as for the previous Core STS
task. A deep result analysis showed that some simi-
larity scores are not correctly estimated within pairs
showing partial similarities. For example, the events
or actions typed similarity is overestimated for the
texts pairs ?The Octagon and Pavilions, Pavilion
Garden, Buxton, c 1875? and ?The Beatles, The Oc-
tagon, Pavillion Gardens, St John?s Road, Buxton,
1963? because they mention the same location (i.e.
?Pavillion Gardens?).
Acknowledgements This work has been partially
supported by the Regione Lazio under the project
PROGRESS-IT (FILAS-CR-2011-1089) and the
Italian Ministry of Industry within the ?Industria
2015? Framework, project DIVINO (MI01 00234).
64
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In *SEM 2012, pages
385?393, Montre?al, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Paolo Annesi, Valerio Storch, and Roberto Basili. 2012.
Space projections as distributional models for seman-
tic composition. In CICLing (1), Lecture Notes in
Computer Science, pages 323?335. Springer.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised synergistic
approach to semantic text similarity. In *SEM 2012,
pages 635?642, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Danilo Croce and Daniele Previtali. 2010. Manifold
learning for the semi-supervised induction of framenet
predicates: An empirical investigation. In Proceed-
ings of the GEMS 2010 Workshop, pages 7?16, Upp-
sala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. 2012a. Unitor: Combining semantic text simi-
larity functions through sv regression. In *SEM 2012,
pages 597?602, Montre?al, Canada, 7-8 June.
Danilo Croce, Alessandro Moschitti, Roberto Basili, and
Martha Palmer. 2012b. Verb classification using dis-
tributional similarity in syntactic and semantic struc-
tures. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 263?272, Jeju Island, Ko-
rea, July.
Danilo Croce, Valerio Storch, Paolo Annesi, and Roberto
Basili. 2012c. Distributional compositional seman-
tics and text similarity. 2012 IEEE Sixth International
Conference on Semantic Computing, 0:242?249.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Santa Cruz.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Trans. Knowl. Discov. Data,
2:10:1?10:25, July.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality: A parameterized sim-
ilarity function for text comparison. In *SEM 2012,
pages 449?453, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In In AAAI06.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
ECML, pages 318?329, Berlin, Germany, September.
Machine Learning: ECML 2006, 17th European Con-
ference on Machine Learning, Proceedings.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, WWW ?06,
pages 377?386, New York, NY, USA. ACM.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199?222, August.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems
for measuring semantic text similarity. In *SEM 2012,
pages 441?448, Montre?al, Canada, 7-8 June.
65
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 369?374, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UNITOR: Combining Syntactic and Semantic Kernels for
Twitter Sentiment Analysis
Giuseppe Castellucci(?), Simone Filice(?), Danilo Croce(?), Roberto Basili(?)
(?) Dept. of Electronic Engineering
(?) Dept. of Civil Engineering and Computer Science Engineering
(?) Dept. of Enterprise Engineering
University of Rome, Tor Vergata
Rome, Italy
{castellucci,filice,croce,basili}@info.uniroma2.it
Abstract
In this paper, the UNITOR system participat-
ing in the SemEval-2013 Sentiment Analysis
in Twitter task is presented. The polarity de-
tection of a tweet is modeled as a classifica-
tion task, tackled through a Multiple Kernel
approach. It allows to combine the contribu-
tion of complex kernel functions, such as the
Latent Semantic Kernel and Smoothed Par-
tial Tree Kernel, to implicitly integrate syn-
tactic and lexical information of annotated ex-
amples. In the challenge, UNITOR system
achieves good results, even considering that
no manual feature engineering is performed
and no manually coded resources are em-
ployed. These kernels in-fact embed distri-
butional models of lexical semantics to deter-
mine expressive generalization of tweets.
1 Introduction
Web 2.0 and Social Networks technologies allow
users to generate contents on blogs, forums and new
forms of communication (such as micro-blogging)
writing their opinion about facts, things, events. The
analysis of this information is crucial for companies,
politicians or other users in order to learn what peo-
ple think, and consequently to adjust their strategies.
In such a scenario, the interest in the analysis of the
sentiment expressed by people is rapidly growing.
Twitter1 represents an intriguing source of informa-
tion as it is used to share opinions and sentiments
about brands, products, or situations (Jansen et al,
2009).
1http://www.twitter.com
On the other hand, tweet analysis represents a
challenging task for natural language processing
systems. Let us consider the following tweets, evok-
ing a positive (1), and negative (2) polarity, respec-
tively.
Porto amazing as the sun sets... http://bit.ly/c28w (1)
@knickfan82 Nooooo ;( they delayed the knicks game
until Monday! (2)
Tweets are short, informal and characterized by
their own particular language with ?Twitter syntax?,
e.g. retweets (?RT?), user references (?@?), hash-
tags (?#?) or other typical web abbreviations, such
as emoticons or acronyms.
Classical approaches to sentiment analysis (Pang
et al, 2002; Pang and Lee, 2008) are not directly ap-
plicable to tweets: most of them focus on relatively
large texts, e.g. movie or product reviews, and per-
formance drops are experimented in tweets scenario.
Some recent works tried to model the sentiment in
tweets (Go et al, 2009; Pak and Paroubek, 2010;
Kouloumpis et al, 2011; Davidov et al, 2010; Bifet
and Frank, 2010; Croce and Basili, 2012; Barbosa
and Feng, 2010; Agarwal et al, 2011). Specific ap-
proaches and feature modeling are used to achieve
good accuracy levels in tweet polarity recognition.
For example, the use of n-grams, POS tags, polar-
ity lexicon and tweet specific features (e.g. hash-
tag, retweet) are some of the component exploited
by these works in combination with different ma-
chine learning algorithms (e.g. Naive Bayes (Pak
and Paroubek, 2010), k-NN strategies (Davidov et
al., 2010), SVM and Tree Kernels (Agarwal et al,
2011)).
In this paper, the UNITOR system participating
369
in the SemEval-2013 Sentiment Analysis in Twit-
ter task (Wilson et al, 2013) models the senti-
ment analysis stage as a classification task. A Sup-
port Vector Machine (SVM) classifier learns the as-
sociation between short texts and polarity classes
(i.e. positive, negative, neutral). Different kernel
functions (Shawe-Taylor and Cristianini, 2004) have
been used: each kernel aims at capturing specific as-
pects of the semantic similarity between two tweets,
according to syntactic and lexical information. In
particular, in line with the idea of using convolu-
tion tree kernels to model complex semantic tasks,
e.g. (Collins and Duffy, 2001; Moschitti et al, 2008;
Croce et al, 2011), we adopted the Smoothed Par-
tial Tree Kernel (Croce et al, 2011) (SPTK). It is
a state-of-the-art convolution kernel that allows to
measure the similarity between syntactic structures,
which are partially similar and whose nodes can dif-
fer but are nevertheless semantically related. More-
over, a Bag-of-Word and a Latent Semantic Kernel
(Cristianini et al, 2002) are also combined with the
SPTK in a multi-kernel approach.
Our aim is to design a system that exhibits wide
applicability and robustness. This objective is pur-
sued by adopting an approach that avoids the use
of any manually coded resource (e.g. a polarity
lexicon), but mainly exploits distributional analysis
of unlabeled corpora: the generalization of words
meaning is achieved through the construction of a
Word Space (Sahlgren, 2006), which provides an ef-
fective distributional model of lexical semantics.
In the rest of the paper, in Section 2 we will
deeply explain our approach. In Section 3 the re-
sults achieved by our system in the SemEval-2013
challenge are described and discussed.
2 System Description
This section describes the approach behind the
UNITOR system. Tweets pre-processing and lin-
guistic analysis is described in Section 2.1, while the
core modeling is described in 2.2.
2.1 Tweet Preprocessing
Tweets are noisy texts and a pre-processing phase is
required to reduce data sparseness and improve the
generalization capability of the learning algorithms.
The following set of actions is performed before ap-
plying the natural language processing chain:
? fully capitalized words are converted in their
lowercase counterparts;
? reply marks are replaced with the pseudo-token
USER, and POS tag is set to $USR;
? hyperlinks are replaced by the token LINK,
whose POS is $URL;
? hashtags are replaced by the pseudo-token
HASHTAG, whose POS is imposed to $HTG;
? characters consecutively repeated more than
three times are cleaned as they cause high lev-
els of lexical data sparseness (e.g. ?nooo!!!!!?
and ?nooooo!!!? are both converted into
?noo!!?);
? all emoticons are replaced by SML CLS, where
CLS is an element of a list of classified emoti-
cons (113 emoticons in 13 classes).
For example, the tweet in the example 2 is nor-
malized in ?user noo sml cry they delayed the knicks
game until monday?. Then, we apply an almost stan-
dard NLP chain with Chaos (Basili and Zanzotto,
2002). In particular, we process each tweet to pro-
duce chunks. We adapt the POS Tagging and Chunk-
ing phases in order to correctly manage the pseudo-
tokens introduced in the normalization step. This is
necessary because tokens like SML SAD are tagged
as nouns, and they influence the chunking quality.
2.2 Modeling Kernel Functions
Following a summary of the employed kernel func-
tions is provided.
Bag of Word Kernel (BOWK) A basic kernel func-
tion that reflects the lexical overlap between tweets.
Each text is represented as a vector whose dimen-
sions correspond to different words. Each dimen-
sion represents a boolean indicator of the presence
or not of a word in the text. The kernel function is
the cosine similarity between vector pairs.
Lexical Semantic Kernel (LSK) A kernel function
is obtained to generalize the lexical information of
tweets, without exploiting any manually coded re-
source. Basic lexical information is obtained by
a co-occurrence Word Space built accordingly to
the methodology described in (Sahlgren, 2006) and
(Croce and Previtali, 2010). A word-by-context ma-
trixM is obtained through a large scale corpus anal-
ysis. Then the Latent Semantic Analysis (Lan-
370
dauer and Dumais, 1997) technique is applied as fol-
lows. The matrix M is decomposed through Singu-
lar Value Decomposition (SVD) (Golub and Kahan,
1965) into the product of three new matrices: U , S,
and V so that S is diagonal and M = USV T . M is
then approximated by Mk = UkSkV Tk , where only
the first k columns of U and V are used, correspond-
ing to the first k greatest singular values. The orig-
inal statistical information about M is captured by
the new k-dimensional space, which preserves the
global structure while removing low-variant dimen-
sions, i.e. distribution noise. The result is that every
word is projected in the reduced Word Space and
an entire tweet is represented by applying an addi-
tive linear combination. Finally, the resulting ker-
nel function is the cosine similarity between vector
pairs, in line with (Cristianini et al, 2002).
Smoothed Partial Tree Kernel (SPTK) In order
to exploit the syntactic information of tweets, the
Smoothed Partial Tree Kernel proposed in (Croce et
al., 2011) is adopted. Tree kernels exploit syntactic
similarity through the idea of convolutions among
substructures. Any tree kernel evaluates the number
of common substructures between two trees T1 and
T2 without explicitly considering the whole frag-
ment space. Its general equation is reported here-
after:
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), (3)
where NT1 and NT2 are the sets of the T1?s and
T2?s nodes, respectively and ?(n1, n2) is equal to
the number of common fragments rooted in the n1
and n2 nodes. The function ? determines the na-
ture of the kernel space. In the SPTK formulation
(Croce et al, 2011) this function emphasizes lexical
nodes. It computes the similarity between lexical
nodes as the similarity between words in the Word
Space. So, this kernel allows a generalization both
from the syntactic and the lexical point of view.
However, tree kernel methods are biased by pars-
ing accuracy and standard NLP parsers suffer accu-
racy loss in this scenario (Foster et al, 2011). It
is mainly due to the complexities of the language
adopted in tweets. In this work, we do not use a
representation that depends on full parse trees. A
syntactic representation derived from tweets chunk-
ing (Tjong Kim Sang and Buchholz, 2000) is here
adopted, as shown in Figure 1.
Notice that no explicit manual feature engineering
is applied. On the contrary we expect that discrim-
inative lexical and syntactic information (e.g. nega-
tion) is captured by the kernel in the implicit feature
space, as discussed in (Collins and Duffy, 2001).
A multiple kernel approach Kernel methods are
appealing as they can be integrated in various ma-
chine learning algorithms, such as SVM. Moreover
a combination of kernels is still a kernel function
(Shawe-Taylor and Cristianini, 2004). We employed
a linear combination ?BOWK + ?LSK + ?SPTK
in order to exploit the lexical properties captured by
BOWK (and generalized by LSK) and the syntac-
tic information of the SPTK. In our experiments, the
kernel weights ?, ? and ? are set to 1.
3 Results and Discussion
In this section experimental results of the UNITOR
system are reported.
3.1 Experimental setup
In the Sentiment Analysis in Twitter task, two
subtasks are defined: Contextual Polarity
Disambiguation (Task A), and Message
Polarity Classification (Task B). The for-
mer deals with the polarity classification (positive,
negative or neutral) of a marked occurrence of a
word or phrase in a tweet context. For example
the adjective ?amazing? in example 1 expresses a
positive marked word. The latter deals with the
classification of an entire tweet with respect to
the three classes positive, negative and neutral. In
both subtasks, we computed a fixed (80%-20%)
split of the training data for classifiers parameter
tuning. Tuned parameters are the regularization
parameter and the cost factor (Morik et al, 1999)
of the SVM formulation. The former represents the
trade off between a training error and the margin.
The latter controls the trade off between positive
and negative examples. The learning phase is made
available by an extended version of SVM-LightTK2,
implementing the smooth matching between tree
nodes.
We built a Word Space based on about 1.5 mil-
lion of tweets downloaded during the challenge pe-
riod using the topic name from the trial material as
2
http://disi.unitn.it/moschitti/Tree-Kernel.htm
371
TW
LINK
$URL
link::$
punt
.
.::.
VerFin
VBZ
set::v
Prep
NN
sun::n
DT
the::d
IN
as::i
Agg
JJ
amazing::j
Nom
NNP
porto::n
(a)
TW
punt
.
!::.
Prep
NNP
monday::n
IN
until::i
Nom
NN
game::n
NNS
knicks::n
VerFin
VBD
delay::v
Nom
PRP
they::p
SMILE
$SML
sml cry::
UH
UH
noo::u
USER
$USR
user::$
(b)
Figure 1: Chunk-based tree derived from examples (1) and (2)
query terms. We normalized and analyzed tweets as
described in section 2.1. Words occurring more than
100 times in the source corpus are represented as
vectors. The 10, 000 most frequent words in the cor-
pus are considered as contexts and the co-occurrence
scores are measured in a window of size n = ?5.
Vector components are weighted through the Point-
wise Mutual Information (PMI), and dimensional-
ity reduction is applied through SVD with a cut of
k = 250.
The task requires to classify two different texts:
tweets and sms. Sms classification is intended to
verify how well a system can scale on a different
domain. In the testing phase two types of submis-
sions are allowed. Constrained results refer to the
case where systems are trained only with the re-
leased data. Unconstrained results refer to the case
where additional training material is allowed. Eval-
uation metrics adopted to compare systems are Pre-
cision, Recall and F1-Measure. Average F1 of the
positive and negative classes is then used to generate
ranks. Further information about the task is avail-
able in (Wilson et al, 2013).
3.2 Results over Contextual Polarity
Disambiguation
We tackled Task A with a multi-kernel approach
combining the kernel functions described in Section
2.2. The final kernel is computed as the linear com-
bination of the kernels, as shown in Equation 4.
k(t1, t2) = SPTK(?A(t1), ?A(t2))
+BOWK(?A(t1), ?A(t2))
+ LSK(?A(t1), ?A(t2)) (4)
where t1, t2 are two tweet examples. The ?A(x)
function extracts the 4-level chunk tree from the
tweet x; nodes (except leaves) covering the marked
instance in x are highlighted in the tree with -POL.
The ?A(x) function extracts the vector representing
the Bag-of-Word of the words inside the marked in-
stance of x, while ?A builds the LSA vectors of the
words occurring within the marked span of x. Re-
ferring to example 1, both ?A(x) and ?A point to
the ?amazing? adjective. Finally, k(t1, t2) returns
the similarity between t1 and t2 accordingly to our
modeling. As three polarity classes are considered,
we adopt a multi-classification schema accordingly
to a One-Vs-All strategy (Rifkin and Klautau, 2004):
the final decision function consists in the selection
of the category associated with the maximum SVM
margin.
Rank 4/19
class precision recall f1
positive .8375 .7750 .8050
Avg-F1 .8249
negative .8103 .8822 .8448
neutral .3475 .3082 .3267
Table 1: Task A results for the sms dataset
Rank 7/21
class precision recall f1
positive .8739 .8844 .8791
Avg-F1 .8460
negative .8273 .7988 .8128
neutral .2778 .3125 .2941
Table 2: Task A results for the twitter dataset
Tables 1 and 2 report the results of the UNITOR
system in the Task A. Only the constrained set-
ting has been submitted. The performance of the
proposed approach is among the best ones and we
ranked 4th and 7th among about 20 systems.
The system seems to be able to generalize well
from the provided training data, and results are re-
markable, especially considering that no manual an-
notated lexical resources were adopted and no man-
ual feature engineering is exploited. It demonstrates
that a multi-kernel approach, with the proposed shal-
low syntactic representation, is able to correctly
classify the sentiment in out-of-domain contexts too.
Syntax is well captured by the SPTK and the lexical
generalization provided by the Word Space allows
to generalize in the sms scenario.
372
3.3 Results over Message Polarity
Classification
A multi-kernel approach is adopted for this task too,
as described in the following Equation 5:
k(t1, t2) = SPTK(?B(t1), ?B(t2))
+BOWK(?B(t1), ?B(t2))
+ LSK(?B(t1), ?B(t2)) (5)
The ?B(x) function extracts a tree representation of
x. In this case no nodes in the trees are marked.
The ?B(x) function extracts Bag-of-Word vectors
for all the words in the tweet x, while ?B(x) extracts
the linear combination of vectors in the Word Space
for adjectives, nouns, verbs and special tokens (e.g.
hashtag, smiles) of the words in x. Again, a One-Vs-
All strategy (Rifkin and Klautau, 2004) is applied.
Constrained run. Tables 3 and 4 report the result
in the constrained case. In the sms dataset our sys-
tem suffers more with respect to the tweet one. In
both cases, the system shows a performance drop
on the negative class. It seems that the multi-kernel
approach needs more examples to correctly disam-
biguate elements within this class. Indeed, nega-
tive class cardinality was about 15% of the training
data, while the positive and neutral classes approxi-
mately equally divided the remaining 85%. More-
over, it seems that our system confuses polarized
classes with the neutral one. For example, the tweet
?going Hilton hotel on Thursday for #cantwait? is
classified as neutral (the gold label is positive). In
this case, the hashtag is the sentiment bearer, and
our model is not able to capture this information.
Rank 13/29
class precision recall f1
positive .5224 .7358 .6110
Avg-F1 .5122
negative .6019 .3147 .4133
neutral .7883 .7798 .7840
Table 3: Task B results for the sms dataset in the
constrained case
Rank 13/36
class precision recall f1
positive .7394 .6514 .6926
Avg-F1 .5827
negative .6366 .3760 .4728
neutral .6397 .8085 .7142
Table 4: Task B results for the twitter dataset in the
constrained case
Unconstrained run. In the unconstrained case we
trained our system adding 2000 positive examples
and 2000 negative examples to the provided training
set. These additional tweets were downloaded from
Twitter during the challenge period using positive
and negative emoticons as query terms. The under-
lying hypothesis is that the polarity of the emoticons
can be extended to the tweet (Pak and Paroubek,
2010; Croce and Basili, 2012). In tables 5 and 6
performance measures in this setting are reported.
Rank 10/15
class precision recall f1
positive .4337 .7317 .5446
Avg-F1 .4888
negative .3294 .6320 .4330
neutral .8524 .3584 .5047
Table 5: Task B results for the sms dataset in the
unconstrained case
Rank 5/15
class precision recall f1
positive .7375 .6399 .6853
Avg-F1 .5950
negative .5729 .4509 .5047
neutral .6478 .7805 .7080
Table 6: Task B results for the twitter dataset in the
unconstrained case
In this scenario, sms performances are again
lower than the twitter case. This is probably due to
the fact that the sms context is quite different from
the twitter one. This is not true for Task A: polar ex-
pressions are more similar in sms and tweets. Again,
we report a performance drop on the negative class.
However, using more negative tweets seems to be
beneficial. The F1 for this class increased of about
3 points for both datasets. Our approach thus needs
more examples to better generalize from data.
In the future, we should check the redundancy and
novelty of the downloaded material, as early dis-
cussed in (Zanzotto et al, 2011). Moreover, we will
explore the possibility to automatically learn the ker-
nel linear combination coefficients in order to op-
timize the balancing between kernel contributions
(Go?nen and Alpaydin, 2011).
Acknowledgements
This work has been partially funded by the Ital-
ian Ministry of Industry within the ?Industria
2015? Framework, under the project DIVINO
(MI01 00234).
373
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis of
twitter data. In Proceedings of the Workshop on Lan-
guages in Social Media, pages 30?38, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In COLING, pages 36?44, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Roberto Basili and Fabio Massimo Zanzotto. 2002. Pars-
ing engineering and empirical robustness. Nat. Lang.
Eng., 8(3):97?120, June.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Proceed-
ings of the 13th international conference on Discov-
ery science, pages 1?15, Berlin, Heidelberg. Springer-
Verlag.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems (NIPS?2001), pages
625?632.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2002. Latent semantic kernels. J. Intell. Inf. Syst.,
18(2-3):127?152.
Danilo Croce and Roberto Basili. 2012. Grammatical
feature engineering for fine-grained ir tasks. In IIR,
pages 133?143.
Danilo Croce and Daniele Previtali. 2010. Manifold
learning for the semi-supervised induction of framenet
predicates: an empirical investigation. In GEMS 2010,
pages 7?16, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In COLING, pages 241?249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jennifer Foster, O?zlem C?etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: Pos tagging and parsing the twitterverse. In
Analyzing Microtext.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
sentiment classification using distant supervision.
G. Golub and W. Kahan. 1965. Calculating the singular
values and pseudo-inverse of a matrix. Journal of the
Society for Industrial and Applied Mathematics: Se-
ries B, Numerical Analysis, 2(2):pp. 205?224.
Mehmet Go?nen and Ethem Alpaydin. 2011. Multi-
ple kernel learning algorithms. Journal of Machine
Learning Research, 12:2211?2268.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188, November.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In ICWSM.
Tom Landauer and Sue Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis theory
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning with a
knowledge-based approach - a case study in intensive
care monitoring. In ICML, pages 268?277, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Alessandro Moschitti, Daniele Pighin, and Robert Basili.
2008. Tree kernels for semantic role labeling. Com-
putational Linguistics, 34.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In EMNLP, volume 10,
pages 79?86, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101?
141, December.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the conll-2000 shared task: chunking. In
ConLL ?00, pages 127?132, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyonov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Kostas Tsioutsiouliklis. 2011. Linguistic redundancy
in twitter. In EMNLP, pages 659?669.
374
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 573?579, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UNITOR-HMM-TK: Structured Kernel-based Learning
for Spatial Role Labeling
Emanuele Bastianelli(?)(?), Danilo Croce(?), Daniele Nardi(?), Roberto Basili(?)
(?) DICII
University of Roma Tor Vergata
Rome, Italy
{bastianelli}@ing.uniroma2.it
(?) DII
University of Roma Tor Vergata
Rome, Italy
{croce,basili}@info.uniroma2.it
(?) DIAG
University of Roma La Sapienza
Rome, Italy
{nardi}@dis.uniroma1.it
Abstract
In this paper the UNITOR-HMM-TK system
participating in the Spatial Role Labeling
task at SemEval 2013 is presented. The
spatial roles classification is addressed as a
sequence-based word classification problem:
the SVMhmm learning algorithm is applied,
based on a simple feature modeling and a ro-
bust lexical generalization achieved through a
Distributional Model of Lexical Semantics. In
the identification of spatial relations, roles are
combined to generate candidate relations, later
verified by a SVM classifier. The Smoothed
Partial Tree Kernel is applied, i.e. a con-
volution kernel that enhances both syntactic
and lexical properties of the examples, avoid-
ing the need of a manual feature engineering
phase. Finally, results on three of the five tasks
of the challenge are reported.
1 Introduction
Referring to objects or entities in the space, as well
as to relations holding among them, is one of the
most important functionalities in natural language
understanding. The detection of spatial utterances
thus finds many applications, such as in GPS navi-
gation systems, or Human-Robot Interaction (HRI).
In Computational Linguistics, the task of recog-
nizing spatial information is known as Spatial Role
Labeling (SpRL), as discussed in (KordJamshidi et
al., 2010). Let us consider the sentence:
[A man]TRAJECTOR is sitting [on]SPATIAL INDICATOR
[a chair]LANDMARK and talking on the phone. (1)
where three roles are labeled: the phrase ?A man?
refers to a TRAJECTOR, ?a chair? to a LAND-
MARK and they are related by the spatial expres-
sion ?on? denoted as SPATIAL INDICATOR. The
last role establishes the type of the spatial relation,
e.g. Regional. The ambiguity of natural language
makes this task very challenging. For example, in
the same Example 1, another preposition ?on? can
be considered, but the phrase ?the phone? is not a
spatial role, as it refers to a communication mean.
This mainly depends on the semantics of the gram-
matical head words, i.e. chair and phone. Such phe-
nomena are crucial in many learning frameworks,
as in kernel-based learning (Shawe-Taylor and Cris-
tianini, 2004), where the decision is based on the
similarity between training and testing data.
This paper describes the UNITOR-HMM-TK sys-
tem participating in the Semeval 2013 Spatial Role
Labeling Task (Kolomiyets et al, 2013), addressing
three of the five defined sub-tasks:
? Task A: Spatial Role Classification. It con-
sists in labeling short sentences with spatial
roles among SPATIAL INDICATOR, TRAJEC-
TOR and LANDMARK.
? Task B: Relation Identification. It consists in
the identification of relations among roles iden-
tified in Task A. This task does not involve the
semantic relation classification.
? Task C: Spatial Role Classification. It con-
sists in labeling short documents with spa-
tial roles among the extended role set: TRA-
JECTOR, LANDMARK, SPATIAL INDICATOR,
MOTION INDICATOR, PATH, DIRECTION and
DISTANCE.
The UNITOR-HMM-TK system addresses both the
problems of identifying spatial roles and relations as
a sequence of two main classification steps.
573
In the first step, each word in the sentence is
classified by a sequence-based classifier with re-
spect to the possible spatial roles. It is in line
with other methods based on sequence-based clas-
sifier for SpRL (Kordjamshidi et al, 2011; Kord-
jamshidi et al, 2012b). Our labeling has been in-
spired by the work in (Croce et al, 2012), where
the SVMhmm learning algorithm, formulated in (Al-
tun et al, 2003), has been applied to the classi-
cal FrameNet-based Semantic Role Labeling. The
main contribution in (Croce et al, 2012) is the adop-
tion of shallow grammatical features (e.g. POS-
tag sequences) instead of the full syntax of the sen-
tence, in order to avoid over-fitting over training
data. Moreover, lexical information has been gen-
eralized through the use of a Word Space, in line
with (Schutze, 1998; Sahlgren, 2006): it consists
in a Distributional Model of Lexical Semantics de-
rived from the unsupervised analysis of an unla-
beled large-scale corpus. The result is a geometri-
cal space where words with similar meaning, e.g.
involved in a paradigmatic or almost-synonymic re-
lations, will be projected in similar vectors. As an
example, we expect that a word like ?table?, maybe
a LANDMARK in a training example, is more similar
to ?chair? as compared with ?phone?.
In the second step, all roles found in a sentence are
combined to generate candidate relations, which are
then verified by a Support Vector Machine (SVM)
classifier. As the entire sentence is informative to de-
termine the proper conjunction of all roles, we apply
a kernel function within the classifier, that enhances
both syntactic and lexical information of the exam-
ples. We adopted the Smoothed Partial Tree Kernel
(SPTK), defined in (Croce et al, 2011): it is con-
volution kernel that allows to measure the similar-
ity between syntactic structures, which are partially
similar and whose nodes can differ, but are semanti-
cally related. Each example is represented as a tree
structure directly derived from the sentence depen-
dency parse, thus avoiding the manual definition of
features. Similarity between lexical nodes is mea-
sured in the same Word Space mentioned above.
In the rest of the paper, Section 2 discusses the
SVMhmm based approach. The SPTK-based learn-
ing algorithm will be presented in Section 3. Finally,
results obtained in the competition are discussed in
Section 4.
2 Sequential Tagging for Spatial Role
Classification
The system proposed for the Spatial Role Classi-
fication task is based on the SVMhmm formula-
tion discussed in (Altun et al, 2003). It extends
classical SVMs by learning a discriminative model
isomorphic to a k-order Hidden Markov Model
through the Structural SVM formulation (Tsochan-
taridis et al, 2005). In the discriminative view
of SVMhmm, given an observed input word se-
quence x = (x1 . . . xl) ? X of feature vectors
x1 . . . xl, the model predicts a sequence of labels
y = (y1 . . . yl) ? Y after learning a linear discrim-
inant function F : X ? Y ? R over input/output
pairs. Each word is then modeled as a set of linear
features that express lexical information as well as
syntactic information surrogated by POS n-grams.
With respect to other works using SVMhmm for
SpRL, such as (Kordjamshidi et al, 2012b), we in-
vestigate another set of possible features, as the ones
proposed in (Croce et al, 2012): the aim is to pro-
vide an agile system that takes advantages in adopt-
ing only shallow grammatical features, thus ignoring
the full syntactic information of a sentence. The syn-
tactic features derived from a dependency parse pro-
cess are surrogated by POS n-grams. According to
this, our feature modeling adopts the IOB notation
discussed in (Croce et al, 2012). It provides a class
label for each token, mapping them into artificial
classes representing the beginning (B), the inside (I)
or ending (O) of a spatial role, plus the label of the
classified role (i.e. BSPIND for the starting token of a
SPATIAL INDICATOR); words external to every role
are labeled with the special class ( ). According to
this notation, the labeling of Example 1 can be ex-
pressed as follows: ?A/BTRAJ man/OTRAJ is/ sitting/
on/BSPIND a/BLAND chair/OLAND and/ . . . ?.
In order to reduce the complexity of the entire
classification task, two phases are applied. In Task
A, as in (Kordjamshidi et al, 2011), the first phase
aims at labeling only SPATIAL INDICATOR, as they
should relate remaining spatial expressions. For the
same reason, in Task C we first label only SPA-
TIAL INDICATOR and MOTION INDICATOR. Roles
classified in this step are considered pivot and they
can be used as features for the classification of the
other roles: TRAJECTORS and LANDMARKS for
574
Task A while TRAJECTORS, LANDMARKS, PATHS,
DISTANCES and DIRECTIONS for Task C.
For the classification of SPATIAL and MOTION
INDICATOR, each word, such as the first ?on? occur-
rence in the Example 1, is modeled through the fol-
lowing features: its lemma (on) and POS tag (IN);
the left and right lexical contexts, represented by the
n words before (man::NN is::VBZ sitting::VBG) and
after (a::DT chair::NN and::CC); the left and right
syntactic contexts as the POS n-grams occurring be-
fore (i.e. NN VBZ VBZ VBG NN VBZ VBG) and
after (i.e. DT NN NN CC DT NN CC) the word.
For the TRAJECTOR and LANDMARK classifica-
tion in Task A, each word is represented by the same
features described above, plus the following ones
(with respect to Example 1, the token relative to
the word man): lemma of the SPATIAL INDICATOR
(on); Positional Feature: distance from the SPATIAL
INDICATOR in terms of number of tokens (-3); rel-
ative position with respect to the SPATIAL INDICA-
TOR, that is before or after (before); a boolean fea-
ture that indicates whether or not the current token is
a SPATIAL INDICATOR; the number of words com-
posing the SPATIAL INDICATOR (here 1).
In Task C, for the classification with respect to
the complete set of roles, each word is modeled by
the previous features together with the following:
distance from the MOTION INDICATOR in terms
of number of tokens; relative position with respect
to the MOTION INDICATOR (before and after); a
boolean feature that indicates whether or not the cur-
rent token is a MOTION INDICATOR; the number of
words that composes the MOTION INDICATOR. In
both Tasks A and C the symbols SI and MI to rep-
resent a SPATIAL INDICATOR or a MOTION INDI-
CATOR are used respectively to represent the target
pivot role within any n-gram.
In order to increase the robustness of our model-
ing, we extended the lexical information with fea-
tures derived from a distributional analysis over
large texts. In essence, we represent the lexical se-
mantic similarity between different words with sim-
ilar meaning. We extend a supervised approach
through the adoption of vector based models of lex-
ical meaning: a large-scale corpus is statistically an-
alyzed and a Word Space, (Sahlgren, 2006), is ac-
quired as follows. A word-by-context matrix M
is obtained through a large scale corpus analysis.
Then the Latent Semantic Analysis (Landauer and
Dumais, 1997) technique is applied to reduce the
space dimensionality. Moreover it provides a way
to project a generic word wi into a k-dimensional
space where each row corresponds to the representa-
tion vector ~wi. In such a space, the distance between
vectors reflects the similarity between correspond-
ing words. The resulting feature vector representing
wi is then augmented with ~wi, as in (Croce et al,
2010), where the benefits of such information have
been reported in the FrameNet-based Semantic Role
Labeling task.
3 Relation identification
The UNITOR-HMM-TK system tackles Relation Iden-
tification task by determining which spatial roles,
discovered in the previous classification phase, can
be combined to determine valid spatial relations.
Our method is inspired by the work of (Roberts and
Harabagiu, 2012), where all possible spatial roles
are first generated through heuristics and then com-
binatorially combined to acquire candidate relations;
valid spatial relations are finally determined using a
SVM classifier. We aim at reducing the potentially
huge search space, by considering only spatial roles
proposed by our sequential tagging approach, de-
scribed in Section 2. Most importantly, we avoid the
manual feature engineering phase of (Roberts and
Harabagiu, 2012). Candidate relations are not rep-
resented as vectors, whose dimensions are manually
defined features useful for the target classification.
We directly apply the Smoothed Partial Tree-Kernel
(SPTK), proposed in (Croce et al, 2011), to estimate
the similarity among a specific tree representation.
Tree kernels exploit syntactic similarity through
the idea of convolutions among substructures.
Any tree kernel computes the number of common
substructures between two trees T1 and T2 without
explicitly considering the whole fragment space. Its
general equation is reported hereafter:
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2)
where NT1 and NT2 are the sets of the T1?s and
T2?s nodes respectively, and ?(n1, n2) is equal to
the number of common fragments rooted in the n1
and n2 nodes1. The SVM classifier is thus trained in
1To have a similarity score between 0 and 1, a normalization
575
a implicit very high-dimensional space, where each
dimension reflects a possible tree sub-structure, thus
avoiding the need of an explicit feature definition.
The function ? determines the nature of such space.
For example, Syntactic Tree Kernel (STK) are used
to model complete context free rules as in (Collins
and Duffy, 2001).
The algorithm for SPTK (Croce et al, 2011)
pushes for more emphasis on lexical nodes. The
? function allows to recursively matches tree struc-
tures and lexical nodes: this allows to match frag-
ments having same structure but different lexical
nodes, by assigning a score proportional to the
product of the lexical similarities, thus generalizing
grammatical and lexical information in training data.
While similarity can be modeled directly over lexi-
cal resources, e.g. WordNet as discussed in (Peder-
sen et al, 2004), their development can be very ex-
pensive, thus limiting the coverage of the resulting
convolution kernel, especially in specific application
domains. Again, a Word Space model is adopted:
given two words, the term similarity function ? is
estimated as the cosine similarity between the corre-
sponding projections.
As proposed in (Croce et al, 2011), the SPTK is
applied to examples modeled according the Gram-
matical Relation Centered Tree (GRCT) representa-
tion, which is derived from the original dependency
parse structure. Figure 1 shows the GRCT for Exam-
ple 1: non-terminal nodes reflect syntactic relations,
such as subject (NSUBJ); pre-terminals are the POS,
such as nouns (NN), and leaves are lexemes, such as
man::n2. Non-terminal nodes associated with a role
are enriched with the role name, e.g. NSUBJTRAJ.
All nodes not covering any role are pruned out, so
that all information not concerning spatial aspects
that would introduce noise is ignored.
In this setting, positive examples are provided by
considering sentences labeled by roles involved in
a valid relation. The definition of negative exam-
ples is more difficult. We considered all roles la-
belled by the SVMhmm based system, discussed in
Section 2. For each incorrect labeling over the an-
in the kernel space, i.e. TK(T1,T2)?
TK(T1,T1)?TK(T2,T2)
is applied.
2Each word is lemmatized to reduce data sparseness, but
they are enriched with POS tags to avoid confusing words from
different grammatical categories.
ROOT
PREPSPIND
POBJLAND
NN
chair::n
DETLAND
DT
a::d
IN
on::i
VBG
sit::v
NSUBJTRAJ
NN
man::n
DETTRAJ
DT
a::d
Figure 1: GRCT representation of a positive example de-
rived from a correct labeling from Example 1
ROOT
CONJ
PREPSPIND
POBJLAND
NN
phone::n
DETLAND
DT
the::d
IN
on::i
VBG
talk::v
VBG
sit::v
NSUBJTRAJ
NN
man::n
DETTRAJ
DT
a::d
Figure 2: GRCT representation of a negative example de-
rived from a wrong labeling from Example 1
notated material, a set of negative examples is ac-
quired by combining all proposed roles. In order
to avoid over-fitting, a n-fold schema has been ap-
plied: it is needed to avoid the SVMhmm label-
ing the same sentences used for training. More-
over, constraints over the relation are imposed to
avoid violations of the Spatial Role theory: in
Task B each relation must be composed at least by
a SPATIAL INDICATOR, LANDMARK and a TRA-
JECTOR or by a SPATIAL INDICATOR, implicit
LANDMARK and a TRAJECTOR. Let us consider
a possible labeling of Example 1: ?[A man]TRAJ
is sitting [on]SPIND [a chair]LAND and talking
[on]SPIND[the phone]LAND?; here, the second SPA-
TIAL INDICATOR ?on? and the LANDMARK ?the
phone? are incorrectly labeled. A negative example
is thus obtained by considering these roles togheter
with the TRAJECTOR ?the phone?, as shown in Fig-
ure 2. Other two negative examples can be generated
by combining the remaining two roles.
4 Results
In this section experimental results of the
UNITOR-HMM-TK system in the Spatial Role
Labeling task at SemEval 2013 are reported. In
Tasks A and B, the dataset is a corrected version
576
of the same training dataset employed in (Kord-
jamshidi et al, 2012a)3. The dataset for Task C was
part of the Confluence corpus4. More details about
the dataset are provided in (Kolomiyets et al, 2013).
In all experiments, sentences are processed with the
Stanford CoreNLP5, for Part-of-Speech tagging,
lemmatization (Task A and C) and dependency
parsing (Task B).
The sequential labeling system described in Sec-
tion 2 has been made available by the SVMhmm
software6. The estimation of the semantically
Smoothed Partial Tree Kernel (SPTK), described in
Section 3 is made available by an extended ver-
sion of SVM-LightTK software7 (Moschitti, 2006),
implementing the smooth matching between tree
nodes. Similarity between lexical nodes is estimated
as the cosine similarity in the co-occurrence Word
Space described above, as in (Croce et al, 2011).
The co-occurrence Word Space is acquired
through the distributional analysis of the UkWaC
corpus (Baroni et al, 2009). First, all words oc-
curring more than 100 times (i.e. the targets) are
represented through vectors. The original space di-
mensions are generated from the set of the 20,000
most frequent words (i.e. features) in the UkWaC
corpus. One dimension describes the Pointwise Mu-
tual Information score between one feature, as it oc-
curs on a left or right window of 3 tokens around a
target. Left contexts of targets are treated differently
from the right ones, in order to capture asymmetric
syntactic behaviors (e.g., useful for verbs): 40,000
dimensional vectors are thus derived for each tar-
get. The Singular Value Decomposition is applied
and the space dimensionality is reduced to k = 100.
4.1 Results in Task A
Two different runs were submitted for Task A. The
first takes into account all roles labeled accordingly
to the approach described in Section 2. Results, in
term of precision, recall and F-measure for each spa-
tial role are shown in Table 1. The second run con-
siders only those roles composing the relations that
3The initial number of sentences was of 600, but it decreased
after the elimination of 21 duplicated sentences.
4Three of the original 95 files were ignored because of some
issues with their format. See http://confluence.org
5
http://nlp.stanford.edu/software/corenlp.shtml
6
http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html
7
http://disi.unitn.it/moschitti/Tree-Kernel.htm
are positively classified in Task B and it will be dis-
cussed in Section 4.2.
A tuning phase has been carried out through a 10-
fold cross validation: it allowed to find the best clas-
sifier parameters. The evaluation of the system per-
formances is measured using a character based mea-
sure, i.e. considering the number of characters in the
span that overlap a role in the gold-standard test.
Spatial Role Precision Recall F-Measure
SPATIAL INDICATOR 0.967 0.889 0.926
TRAJECTOR 0.684 0.681 0.682
LANDMARK 0.741 0.835 0.785
Table 1: Task A results (first run)
The overall performances of the first run are
very promising in terms of both precision and re-
call. In particular, the SPATIAL INDICATOR label-
ing achieves a significant F-Measure of 0.926 with a
precision of 0.967. The sequence labeling approach
provides good results for the LANDMARK and the
TRAJECTOR roles too. Unfortunately, these results
are not comparable with the performances obtained
the last year edition of the SpRL task, where a gram-
matical head word-based measure has been applied.
The main difficulty in the SPATIAL INDICATOR
classification concerns the tagging of a larger or
smaller span for the roles, as for ?at the back? that is
tagged as ?at the back of?. On the contrary, for roles
like ?to the left and the right? the system produces a
tag covering just the first three words, ?to the left?,
because this shortest sequence was far more repre-
sented within the training set. Some roles corre-
sponding to unknown word sequences, such as ?on
the very right?, were not labeled, leading to the little
drop in terms of recall for the SPATIAL INDICATOR.
Another issue in the TRAJECTOR and LAND-
MARK labeling is due to the absence of specific role
sequences in the training set, such as LANDMARK-
TRAJECTOR-SPATIAL INDICATOR labeled in the
test sentence ?there is a [coffee table]LANDMARK with
a [sofa]TRAJECTOR [around]SP.IND?: the SVMhmm
classifier in fact tends to discard any sequence un-
seen during training. Another issue concerns the
difficulty in assigning the TRAJECTOR role to the
proper SPATIAL INDICATOR: in the sentence ?a
bench with a person lying on it? where both ?a
bench? and ?a person? are tagged as TRAJECTOR.
577
4.2 Results in Task B
Task B has been tackled using the SPTK-based Re-
lation Identification approach, described in Section
3. In particular, the SVM classifier is fed with 741
positive examples, corresponding to the number of
gold relations, while the negative examples genera-
tion process, described in Section 3, yielded 2,256
examples. The same Word Space described in the
previous section has been used to compute the se-
mantic similarity within the SPTK. For the tuning
phase, a 80-20 fixed split has been applied.
For this task, two different measures are pre-
sented. The Relaxed measure considers a relation
correct if each role composing it has at least one
character overlapping the corresponding gold role.
The Strict measure considers a relation correct only
if each role in it has all the characters overlapping
with the gold role. The first measure is more com-
parable with the one used in (Kordjamshidi et al,
2012a), where a relation is considered correct only
if each grammatical head word of the involved roles
were correctly labeled. The results achieved in this
task by our system are reported in Table 2.
Spatial Role Precision Recall F-Measure
RELAXED 0.551 0.391 0.458
STRICT 0.431 0.306 0.358
Table 2: Task B results
The problem for this task is more challenging. In
fact, the overall task is strictly biased by the quality
of the SVMhmm based classifier and inherits all the
limitations underlined in Section 4.2. This mostly
affects the recall, because every error generated dur-
ing the role classification is cumulative and losing
only one role in Task A implies a misclassification
of the whole relation. However, it is important to
notice that these results have been achieved without
any manual feature engineering nor any heuristics or
hand coded lexical resource.
Spatial Role Precision Recall F-Measure
SPATIAL INDICATOR 0.968 0.585 0.729
TRAJECTOR 0.682 0.493 0.572
LANDMARK 0.801 0.560 0.659
Table 3: Task A results (second run)
In the second run of Task A, we evaluate the con-
tribution of this syntactic information to filter out
roles. In Table 3 results of the second run for Task
A are reported (see previous Section). As expected,
the recall measure shows a performance drop with
respect to results shown in Table 1: the results pro-
posed in the first run represents an upperbound to the
recall as any novel role is added here. However, the
precision measure for the LANDMARK role classifi-
cation is improved of about 10%.
Spatial Role Precision Recall F-Measure
SPATIAL INDICATOR 0.609 0.479 0.536
MOTION INDICATOR 0.892 0.294 0.443
TRAJECTOR 0.565 0.317 0.406
LANDMARK 0.662 0.476 0.554
PATH 0.775 0.295 0.427
DIRECTION 0.312 0.229 0.264
DISTANCE 0.946 0.331 0.490
Table 4: Task C results
4.3 Results in Task C
In Task C the extended set of roles is considered.
According to this, the number of possible labels to
be learnt by the system increases, thus making the
problem more challenging. As for Task A, here the
SVMhmm has been trained over the whole training
set, using a 10-fold cross validation in the tuning
phase. Moreover, the sentences of the Confluence
corpus are far more complex than the ones from the
CLEF corpus. Confluence sentences have a more
narrative nature with respect to the CLEF sentences,
that are simple description of images. The combina-
tion of these two factors resulted in a large drop in
the performance, especially for the recall.
As shown by the results in Table 4, DIRECTION
is the most difficult role to be classified, probably
because it is represented by many different word se-
quences. Other roles are found in few instances, but
almost all correct, as for DISTANCE and MOTION
INDICATOR. The high value of Precision for the
DISTANCE role is justified by the fact that when this
role is composed by a number, (i.e. ?530 meters?),
the system identified and classified it well, while for
a representation with only words (i.e. ?very close?)
the system did not retrieved it at all.
Acknowledgements This work has been partially
funded by European Union VII Framework Pro-
gramme under the project Speaky for Robot within
the framework of the ECHORD Project.
578
References
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hid-
den Markov support vector machines. In Proceedings
of the International Conference on Machine Learning.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems (NIPS?2001), pages
625?632.
Danilo Croce, Cristina Giannone, Paolo Annesi, and
Roberto Basili. 2010. Towards open-domain semantic
role labeling. In ACL, pages 237?246.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Danilo Croce, Giuseppe Castellucci, and Emanuele Bas-
tianelli. 2012. Structured learning for semantic role
labeling. In Intelligenza Artificiale, 6(2):163?176,
January.
Oleksandr Kolomiyets, Parisa Kordjamshidi, Steven
Bethard, and Marie-Francine Moens. 2013. Semeval-
2013 task 2: Spatial role labeling. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion. Association for Computational Linguistics.
Parisa KordJamshidi, Martijn van Otterlo, and Marie-
Francine Moens. 2010. Spatial role labeling: Task
definition and annotation scheme. In LREC.
Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-
Francine Moens. 2011. Spatial role labeling: To-
wards extraction of spatial relations from natural lan-
guage. ACM Trans. Speech Lang. Process., 8(3):4:1?
4:36, December.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012a. Semeval-2012 task 3: Spa-
tial role labeling. In SemEval 2012, pages 365?373,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Parisa Kordjamshidi, Paolo Frasconi, Martijn Van Ot-
terlo, Marie-Francine Moens, and Luc De Raedt.
2012b. Relational learning for spatial relation extrac-
tion from natural language. In Proceedings of the
21st international conference on Inductive Logic Pro-
gramming, ILP?11, pages 204?220, Berlin, Heidel-
berg. Springer-Verlag.
T. Landauer and S. Dumais. 1997. A solution to plato?s
problem: The latent semantic analysis theory of ac-
quisition, induction and representation of knowledge.
Psychological Review, 104(2):211?240.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
ECML, pages 318?329, Berlin, Germany, September.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concept. In Proc. of 5th NAACL, Boston,
MA.
Kirk Roberts and Sanda Harabagiu. 2012. Utd-sprl: A
joint approach to spatial role labeling. In SemEval
2012), pages 419?424, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Hinrich Schutze. 1998. Automatic word sense discrimi-
nation. Journal of Computational Linguistics, 24:97?
123.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin meth-
ods for structured and interdependent output variables.
J. Machine Learning Reserach., 6, December.
579
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 761?767,
Dublin, Ireland, August 23-24, 2014.
UNITOR: Aspect Based Sentiment Analysis with Structured Learning
Giuseppe Castellucci
(?)
, Simone Filice
(?)
, Danilo Croce
(?)
, Roberto Basili
(?)
(?) Dept. of Electronic Engineering
(?) Dept. of Civil Engineering and Computer Science Engineering
(?) Dept. of Enterprise Engineering
University of Roma, Tor Vergata, Italy
{castellucci,filice}@ing.uniroma2.it; {croce,basili}@info.uniroma2.it
Abstract
In this paper, the UNITOR system partici-
pating in the SemEval-2014 Aspect Based
Sentiment Analysis competition is pre-
sented. The task is tackled exploiting Ker-
nel Methods within the Support Vector
Machine framework. The Aspect Term
Extraction is modeled as a sequential tag-
ging task, tackled through SVM
hmm
. The
Aspect Term Polarity, Aspect Category
and Aspect Category Polarity detection are
tackled as a classification problem where
multiple kernels are linearly combined to
generalize several linguistic information.
In the challenge, UNITOR system achieves
good results, scoring in almost all rank-
ings between the 2
nd
and the 8
th
position
within about 30 competitors.
1 Introduction
In recent years, many websites started offering a
high level interaction with users, who are no more
a passive audience, but can actively produce new
contents. For instance, platforms like Amazon
1
or
TripAdvisor
2
allow people to express their opin-
ions on products, such as food, electronic items
or clothes. Obviously, companies are interested
in understanding what customers think about their
brands and products, in order to implement correc-
tive strategies on products themselves or on mar-
keting solutions. Performing an automatic analy-
sis of user opinions is then a very hot topic. The
automatic extraction of subjective information in
text materials is generally referred as Sentiment
Analysis or Opinion Mining and it is performed
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
http://www.amazon.com
2
http://www.tripadvisor.com
via natural language processing, text analysis and
computational linguistics techniques. Task 4 in
SemEval 2014 edition
3
(Pontiki et al., 2014) aims
at promoting research on Aspect Based Opinion
Mining (Liu, 2007), which is approached as a cas-
cade of 4 subtasks. For example, let us consider
the sentence:
The fried rice is amazing here. (1)
The Aspect Term Extraction (ATE) subtask aims
at finding words suggesting the presence of as-
pects on which an opinion is expressed, e.g.
fried rice in sentence 1. In the Aspect Term
Polarity (ATP) task the polarity evoked for each
aspect is recognized, i.e. a positive polarity is
expressed with respect to fried rice. In the
Aspect Category Detection (ACD) task the cate-
gory evoked in a sentence is identified, e.g. the
food category in sentence 1). In the Aspect Cat-
egory Polarity (ACP) task the polarity of each ex-
pressed category is recognized, e.g. a positive
category polarity is expressed in sentence 1.
Different strategies have been experimented in
recent years. Classical approaches are based on
machine learning techniques and rely on sim-
ple representation features, such as unigrams, bi-
grams, Part-Of-Speech (POS) tags (Pang et al.,
2002; Pang and Lee, 2008; Wiebe et al., 1999).
Other approaches adopt sentiment lexicons in or-
der to exploit some sort of prior knowledge about
the polar orientation of words. These resources are
usually semi-automatically compiled and provide
scores associating individual words to sentiments
or polarity orientation.
In this paper, the UNITOR system participat-
ing to the SemEval-2014 Aspect Based Sentiment
Analysis task (Pontiki et al., 2014) is presented.
The ATE task is modeled as a sequential labeling
problem. A sentence is considered as a sequence
of tokens: a Markovian algorithm is adopted in
3
http://alt.qcri.org/semeval2014/task4/
761
order to decide what is an aspect term . All the
remaining tasks are modeled as multi-kernel clas-
sification problems based on Support Vector Ma-
chines (SVMs). Various representation have been
exploited using proper kernel functions (Shawe-
Taylor and Cristianini, 2004a). Tree Kernels
(Collins and Duffy, 2001; Moschitti et al., 2008;
Croce et al., 2011) are adopted in order to capture
structural sentence information derived from the
parse tree. Moreover, corpus-driven methods are
used in order to acquire meaning generalizations
in an unsupervised fashion (e.g. see (Pado and La-
pata, 2007)) through the analysis of distributions
of word occurrences in texts. It is obtained by the
construction of a Word Space (Sahlgren, 2006),
which provides a distributional model of lexical
semantics. Latent Semantic Kernel (Cristianini et
al., 2002) is thus applied within such space.
In the remaining, in Section 2 and 3 we will ex-
plain our approach in more depth. Section 4 dis-
cusses the results in the SemEval-2014 challenge.
2 Sequence Labeling for ATE
The Aspect Term Extraction (ATE) has been mod-
eled as a sequential tagging process. We con-
sider each token representing the beginning (B),
the inside (I) or the outside (O) of an argu-
ment. Following this IOB notation, the resulting
ATE representation of a sentence like ?The [fried
rice]
ASPECTTERM
is amazing here? can be expressed
by labeling each word according to its relative po-
sition, i.e.: [The]
O
[fried]
B
[rice]
I
[is]
O
[amaz-
ing]
O
[here]
O
.
The ATE task is thus a labeling process that
determines the individual (correct IOB) class for
each token. The labeling algorithm used is
SVM
hmm
(Altun et al., 2003)
4
: it combines
both a discriminative approach to estimate the
probabilities in the model and a generative ap-
proach to retrieve the most likely sequence of
tags that explains a sequence. Given an input
sequence x = (x
1
. . . x
l
) ? X of feature vec-
tors x
1
. . . x
l
, the model predicts a tag sequence
y = (y
1
. . . y
l
) ? Y after learning a linear dis-
criminant function F : X ? Y ? R over input-
output pairs. The labeling f(x) is thus defined as:
f(x) = arg max
y?Y
F (x,y;w) and it is obtained
by maximizing F over the response variable, y,
for a specific given input x. F is linear in some
4
www.cs.cornell.edu/People/tj/svm light/svm hmm.html
combined feature representation of inputs and out-
puts ?(x,y), i.e. F (x,y;w) = ?w,?(x,y)?.
In SVM
hmm
the observations x
1
. . . x
l
can be
naturally expressed in terms of feature vectors. In
particular, we modeled each word through a set of
lexical and syntactic features, as described in the
following section.
2.1 Modeling Features for ATE
In the discriminative view of SVM
hmm
, each
word is represented by a feature vector, describ-
ing its different observable properties. For in-
stance, the word rice in the example 1 is modeled
through the following features: Lexical features:
its lemma (rice) and POS tag (NN); Prefixes and
Suffixes: the first n and the last m characters of
the word (n = m = 3) (e.g. ric and ice); Con-
textual features: the left and right lexical contexts
represented by the 3 words before (BEGIN::BB
the::DT fried::JJ) and after (is::VBZ amazing::JJ
here::RB); the left and right syntactic contexts as
the POS bi-grams and tri-grams occurring before
(i.e. BB DT DT JJ BB DT JJ) and after (i.e.
VBZ JJ JJ RB VBZ JJ RB) the word; Gram-
matical features: features derived from the de-
pendency graph associated to the sentence, i.e.
boolean indicators that capture if the token is in-
volved in a Subj, Obj or Amod relation in the cor-
responding graph.
3 Multiple Kernel Approach for Polarity
and Category Detection
We approached the remaining three subtasks of the
pipeline as classification problems with multiple
kernels, in line with (Castellucci et al., 2013). We
used Support Vector Machines (SVMs) (Joachims,
1999), a maximum-margin classifier that realizes
a linear discriminative model. The kernelized ver-
sion of SVM learns from instances x
i
exploiting
rich similarity measures (i.e.the kernel functions)
K(x
i
, x
j
) = ??(x
i
) ? ?(x
j
)?. In this way projec-
tion functions ?(?) can be implicitly used in order
to transform the initial feature space into a more
expressive one, where a hyperplane that separates
the data with the widest margin can be found.
Kernels can directly operate on variegate forms
of representation, such as feature vectors, trees,
sequences or graphs. Then, modeling instances
in different representations, specific kernels can
be defined in order to explore different linguis-
tic information. These variety of kernel functions
762
K1
. . .K
n
can be independently defined and the
combinations K
1
+ K
2
+ . . . of multiple func-
tions can be integrated into SVM as they are still
kernels. The next section describes the represen-
tations as well as the kernel functions.
3.1 Representing Lexical Information
The Bag of Word (BoW) is a simple repre-
sentation reflecting the lexical information of the
sentence. Each text is represented as a vector
whose dimensions correspond to different words,
i.e. they represent a boolean indicator of the pres-
ence or not of a word in the text. The resulting
kernel function is the cosine similarity (or linear
kernel) between vector pairs, i.e. lin
BoW
. In line
with (Shawe-Taylor and Cristianini, 2004b) we in-
vestigated the contribution of the Polynomial Ker-
nel of degree 2, poly
2
BoW
as it defines an implicit
space where also feature pairs, i.e. words pairs,
are considered.
In the polarity detection tasks, several polarity
lexicons have been exploited in order to have use-
ful hints of the intrinsic polarity of words. We
adopted MPQA Subjectivity Lexicon
5
(Wilson et
al., 2005) and NRC Emotion Lexicon (Moham-
mad and Turney, 2013): they are large collection
of words provided with the underlying emotion
they generally evoke. While the former consid-
ers only positive and negative sentiments, the lat-
ter considers also eight primary emotions, orga-
nized in four opposing pairs, joy-sadness, anger-
fear, trust-disgust, and anticipation-surprise. We
define the Lexicon Based (LB) vectors as follows.
For each lexicon, let E = {e
1
, ..., e
|E|
} be the
emotion vocabulary defined in it. Let w ? s be
a word occurring in sentence s, with I(w, i) be-
ing the indicator function whose output is 1 if w
is associated to the emotion label e
i
, or 0 other-
wise. Then, given a sentence s, each e
i
, i.e. a di-
mension of the emotional vocabularyE, receives a
score s
i
=
?
w?s
I(w, i). Each sentence produces
a vector ~s ? R
|E|
, for each lexicon, on which a lin-
ear kernel lin
LB
is applied.
3.2 Generalizing Lexical Information
Another representation is used to generalize the
lexical information of each text, without exploit-
ing any manually coded resource. Basic lexical
information is obtained by a co-occurrence Word
Space (WS) built accordingly to the methodology
5
http://mpqa.cs.pitt.edu/lexicons/subj lexicon
described in (Sahlgren, 2006) and (Croce and Pre-
vitali, 2010). A word-by-context matrix M is ob-
tained through a large scale corpus analysis. Then
the Latent Semantic Analysis (Landauer and Du-
mais, 1997) technique is applied as follows. The
matrix M is decomposed through Singular Value
Decomposition (SVD) (Golub and Kahan, 1965)
into the product of three new matrices: U , S, and
V so that S is diagonal and M = USV
T
. M
is then approximated by M
k
= U
k
S
k
V
T
k
, where
only the first k columns of U and V are used,
corresponding to the first k greatest singular val-
ues. This approximation supplies a way to project
a generic wordw
i
into the k-dimensional space us-
ing W = U
k
S
1/2
k
, where each row corresponds to
the representation vector ~w
i
. The result is that ev-
ery word is projected in the reduced Word Space
and a sentence is represented by applying an addi-
tive model as an unbiased linear combination. We
adopted these vector representations using a linear
kernel, as in (Cristianini et al., 2002), i.e. lin
WS
and a Radial Basis Function Kernel rbf
WS
.
In Aspect Category Detection, and more gen-
erally in topic classification tasks, some specific
words can be an effective indicator of the under-
lying topic. For instance, in the restaurant do-
main, the word tasty may refer to the quality of
food. These kind of word-topic relationships can
be automatically captured by a Bag-of-Word ap-
proach, but with some limitations. As an exam-
ple, a BoW representation may not capture syn-
onyms or semantically related terms. This lack
of word generalization is partially compensated
by the already discussed Word Space. However,
this last representation aims at capturing the sense
of an overall sentence, and no particular rele-
vance is given to individual words, even if they
can be strong topic indicators. To apply a model-
ing more focused on topics, we manually selected
m seed words {?
1
, . . . , ?
m
} that we consider re-
liable topic-indicators, for example spaghetti for
food. Notice that for every seed ?
i
, as well as for
every word w the similarity function sim(?
i
, w)
can be derived from the Word Space represen-
tations ~?
i
and ~w, respectively. What we need
is a specific seed-based representation reflecting
the similarity between topic indicators and sen-
tences s. Given the words w occurring in s, the
Seed-Oriented (SO) representation of s is an m-
dimensional vector ~so(s) whose components are:
so
i
(s) = max
w?s
sim(?
i
, w). Alternatively, as
763
seeds ? refer to a set of evoked topics (i.e. as-
pect categories such as food) ?
1
, ...,?
t
, we can
define a t-dimensional vector
~
to(s) called Topic-
Oriented (TO) representation for s, whose fea-
tures are: to
i
(s) = max
w?s,?
k
??
i
sim(?
k
, w).
The adopted word similarity function sim(?, ?)
over ~so(s) and
~
to(s) depends on the experiments.
In the unconstrained setting, i.e. the Word Space
Topic Oriented WSTO system, sim(?, ?) consists
in the dot product over the Word Space represen-
tations ~?
i
and ~w. In the constrained case sim(?, ?)
corresponds to the Wu & Palmer similarity based
on WordNet (Wu and Palmer, 1994), in the so
called WordNet Seed Oriented WNSO system.
The Radial Basis Function (RBF) kernel is then
applied onto the resulting feature vectors
~
to(s) and
~so(s) in the rbf
WSTO
and rbf
WNSO
, respectively.
3.3 Generalizing Syntactic Information
In order to exploit the syntactic information, Tree
Kernel functions proposed in (Collins and Duffy,
2001) are adopted. Tree kernels exploit syntactic
similarity through the idea of convolutions among
syntactic tree substructures. Any tree kernel evalu-
ates the number of common substructures between
two trees T
1
and T
2
without explicitly considering
the whole fragment space. Many tree represen-
tations can be derived to represent the syntactic
information, according to different syntactic theo-
ries. For this experiment, dependency formalism
of parse trees is employed to capture sentences
syntactic information. As proposed in (Croce et
al., 2011), the kernel function is applied to ex-
amples modeled according the Grammatical Rela-
tion Centered Tree representation from the orig-
inal dependency parse structures, shown in Fig.
1: non-terminal nodes reflect syntactic relations,
such as NSUBJ, pre-terminals are the Part-Of-
Speech tags, such as nouns, and leaves are lex-
emes, such as rice::n and amazing::j
6
. In each ex-
ample, the aspect terms and the covering nodes are
enriched with a a suffix and all lexical nodes are
duplicated by the node asp in order to reduce data
sparseness. Moreover, prior information derived
by the lexicon can be injected in the tree, by du-
plicating all lexical nodes annotated in the MPQA
Subjectivity Lexicon, e.g. the adjective amazing,
with a node expressing the polarity (pos).
Given two tree structures T
1
and T
2
, the
6
Each word is lemmatized to reduce data sparseness, but
they are enriched with POS tags.
ROOT
ADVM
RB
here::r
JJ
posamazing::j
COP
VBZ
be::v
NSUBJ
a
NN
a
asprice::n
AMOD
a
VBN
a
aspfry::v
DET
DT
the::d
Figure 1: Tree representation of the sentence 1.
Tree Kernel formulation is reported hereafter:
TK(T
1
, T
2
) =
?
n
1
?N
T
1
?
n
2
?N
T
2
?(n
1
, n
2
)
where N
T
1
and N
T
2
are the sets of the T
1
?s and
T
2
?s nodes, respectively and ?(n
1
, n
2
) is equal to
the number of common fragments rooted in the n
1
and n
2
nodes. The function ? determines the na-
ture of the kernel space. In the constrained case the
Partial Tree Kernel formulation (Moschitti, 2006)
is used, i.e. ptk
GRCT
. In the unconstrained set-
ting the Smoothed Partial Tree Kernel formulation
(Croce et al., 2011) is adopted to emphasizes the
lexicon in the Word Space, i.e. the sptk
GRCT
. It
computes the similarity between lexical nodes as
the similarity between words in the Word Space.
So, this kernel allows a generalization both from a
syntactic and lexical point of view.
4 Results
In this Section the experimental results of the
UNITOR system in the four different subtasks of
Semeval 2014 competition are discussed. Teams
were allowed to submit two different outcomes for
each task: constrained submissions (expressed by
the suffix C in all the tables) are intended to mea-
sure systems ability to learn sentiment analysis
models only over the provided data; unconstrained
(expressed by the suffix U in all the tables) sub-
missions allows teams to exploit additional train-
ing data. The first two tasks, i.e. ATE and ATP,
are defined on the laptop and restaurant domains,
while the last two tasks, i.e. ACD and ACP, are
defined for the restaurant dataset only.
The unconstrained versions are derived by ex-
ploiting word vectors derived in an unsupervised
fashion through the analysis of large scale cor-
pora. All words in a corpus occurring more than
100 times (i.e. the targets) are represented through
vectors. The original space dimensions are gen-
erated from the set of the 20,000 most frequent
words (i.e. features) in the corpus. One dimension
describes the Point-wise Mutual Information score
between one feature, as it occurs on a left or right
window of 3 tokens around a target. Left contexts
of targets are distinguished from the right ones, in
order to capture asymmetric syntactic behaviors
764
(e.g., useful for verbs): 40,000 dimensional vec-
tors are thus derived for each target. The Singular
Value Decomposition is applied and the space di-
mensionality is reduced to k = 250. Two corpora
are used for generating two different Word Spaces,
one for the laptop and one for the restaurant do-
main. The Opinosis dataset (Ganesan et al., 2010)
is used to build the electronic domain Word Space,
while the restaurant domain corpus adopted is the
TripAdvisor dataset
7
. Both provided data and in-
domain data are first pre-processed through the
Stanford Parser (Klein and Manning, 2003) in or-
der to obtain POS tags or Dependency Trees.
A modified version of LibSVM has been
adopted to implement Tree Kernel. Parameters
such as the SVM regularization coefficient C, the
kernel parameters (for instance the degree of the
polynomial kernel) have been selected after a tun-
ing stage based on a 5-fold cross validation.
4.1 Aspect Term Extraction
The Aspect Term Extraction task is modeled as a
sequential labeling problem. The feature represen-
tation described in Section 2.1, where each token
is associated to a specific target class according to
the IOB notation, is used in the SVM
hmm
learn-
ing algorithm. In the constrained version of the
UNITOR system only the training data are used
to derive features. In the unconstrained case the
UNITOR system exploits lexical vectors derived
from a Word Space. Each token feature repre-
sentation is, in this sense, augmented through dis-
tributional vectors derived from the Word Spaces
described above. Obviously, the Opinosis Word
Space is used in the laptop subtask, while the Tri-
pAdvisor Word Space is used in the restaurant sub-
task. These allow the system to generalize the lex-
ical information, enabling a smoother match be-
tween words during training and test phases, hope-
fully capturing similarity phenomena such as the
relation between screen and monitor.
In Table 1 results in the laptop case are reported.
Our system performed quite well, and ranked in
6
th
and 10
th
position over 28 submitted systems.
In this case, the use of the Word Space is effec-
tive, as noticed by the 4 position gain in the final
ranking (almost 2 points in F1-measure). In Table
2 results in the restaurant case are reported. Here,
the use of Word Space does not give an improve-
ment in the final performance.
7
http://sifaka.cs.uiuc.edu/?wang296/Data/index.html
Table 1: Aspect Term Extraction Results - Laptop.
System (Rank) P R F1
UNITOR-C (10/28) .7741 .5764 .6608
UNITOR-U (6/28) .7575 .6162 .6795
Best-System-C (1/28) .8479 .6651 .7455
Best-System-U (2/28) .8251 .6712 .7403
Table 2: Aspect Term Extraction - Restaurants.
System (Rank) P R F1
UNITOR-C (5/29) .8244 .7786 .8009
UNITOR-U (6/29) .8131 .7865 .7996
Best-System-C (2/29) .8624 .8183 .8398
Best-System-U (1/29) .8535 .8271 .8401
In both cases, we observed that most of the
errors were associated to aspect terms composed
by multiple words. For example, in the sen-
tence The portions of the food that came out were
mediocre the gold aspect term is portions of
the food while our system was able only to re-
trieve food as aspect term. The system is mainly
able to recognize single word aspect terms and, in
most of the cases, double words aspect terms.
4.2 Aspect Term Polarity
The Aspect Term Polarity subtask has been mod-
eled as a multi-class classification problem: for
a given set of aspect terms within a sentence, it
aims at determining whether the polarity of each
aspect term is positive, negative, neutral or con-
flict. It has been tackled using multi-kernel SVMs
in a One-vs-All Schema. In the constrained set-
ting, the linear combination of the following ker-
nel functions have been used: ptk
GRCT
, poly
2
BoW
that consider all the lemmatized terms in the sen-
tence, a poly
2
BoW
that considers only the aspect
terms, poly
2
BoW
of the terms around the aspect
terms in a window of size 5, lin
LB
derived from
the Emolex lexicon. In the unconstrained setting
the sptk
GRCT
replaces the ptk counterpart and
the rbf
WS
is added by linearly combining Word
Space vectors for verbs, nouns adjective and ad-
verbs. Results in Table 3 show that the proposed
kernel combination allows to achieve the 8
th
posi-
tion with the unconstrained system in the restau-
rant domain. The differences with the constrained
setting demonstrate the contribution of the Word
Space acquired from the TripAdvisor corpus. Un-
fortunately, it is not true in the laptop domain, as
shown in Table 4. The use of the Opinosis corpus
lets to a performance drop of the unconstrained
setting. An error analysis shows that the main lim-
765
itation of the proposed model is the inability to
capture deep semantic phenomena such as irony,
as in the negative sentence ?the two waitress?s
looked like they had been sucking on lemons?.
Table 3: Aspect Term Polarity Results - Restau-
rant.
System (Rank) Accuracy
UNITOR-C (12/36) .7248
UNITOR-U (8/36) .7495
Best-System-C (1/36) .8095
Best-System-U (5/36) .7768
Table 4: Aspect Term Polarity Results - Laptop.
System (Rank) Accuracy
UNITOR-C (10/32) .6299
UNITOR-U (17/32) .5856
Best-System-C (1/32) .7048
Best-System-U (5/32) .6666
4.3 Aspect Category Detection
The Aspect Category Detection has been mod-
eled as a multi-label classification task where 5
categories (ambience, service, food, price, anec-
dotes/miscellaneous) must be recognized. In the
constrained version, each class has been tack-
led using a binary multi-kernel SVM equipped
with a linear combination of poly
2
BoW
and
rbf
WNSO
. A category is assigned if the SVM
classifiers provides a positive prediction. The
anecdotes/miscellaneous acceptance threshold has
been set to 0.3 (it has been estimated over a de-
velopment set) due to its poor precision observed
during the tuning phase. Moreover, considering
each sentence is always associated to at least one
category, if no label has been assigned, then the
sentence is labelled with the category associated
to the highest prediction.
In the unconstrained case, each class has been
tackled using an ensemble of a two binary SVM-
based classifiers. The first classifier is a multi-
kernel SVM operating on a linear combination of
rbf
WS
and poly
2
BoW
. The second classifier is a
SVM equipped with a rbf
WSTO
. A sentence is la-
belled with a category if at least one of the two cor-
responding classifiers predicts that label. The first
classifier assigns a label if the corresponding pre-
diction is positive. A more conservative strategy
is applied to the second classifier, and a category
is selected if its corresponding prediction is higher
than 0.3; again this threshold has been estimated
over a development set. As in the constrained ver-
sion, we observed a poor precision in the anec-
dotes/miscellaneous category, so we increased the
first classifier acceptance threshold to 0.3, while
the second classifier output is completely ignored.
Finally, if no label has been assigned, the sentence
is labelled with the category associated to the high-
est prediction of the first classifier.
Table 5: Aspect Category Detection Results.
System (Rank) P R F1
UNITOR-C (6/21) .8368 .7804 .8076
UNITOR-U (2/21) .8498 .8556 .8526
Best-System-C (1/21) .9104 .8624 .8857
Best-System-U (4/21) .8435 .7892 .8155
Table 5 reports the achieved results. Consider-
ing the simplicity of the proposed approach, the
results are impressive. The ensemble schema,
adopted in the unconstrained version, is very use-
ful in improving the recall and allows the system
to achieve the second position in the competition.
4.4 Aspect Category Polarity
The Aspect Category Polarity subtask has been
modeled as a multi-class classification problem:
given a set of pre-identified aspect categories for a
sentence, it aims at determining the polarity (pos-
itive, negative, neutral or conflict) of each cate-
gory. It has been tackled using multi-kernel SVMs
in a One-vs-All Schema. In the constrained set-
ting, the linear combination of the following ker-
nel functions has been used: ptk
GRCT
, poly
2
BoW
that consider all the lemmatized terms in the sen-
tence, a poly
2
BoW
that considers only verbs, nouns
adjective and adverbs in the sentence, lin
LB
de-
rived from the MPQA sentiment lexicon. In the
unconstrained case the sptk
GRCT
replaces the ptk
counterpart and the rbf
WS
is added by linearly
combining Word Space vectors for verbs, nouns
adjective and adverbs.
Again, results shown in Table 6 suggest the pos-
itive contribution of the lexical generalization pro-
vided by the Word Space (in the sptk
GRCT
and
rbf
WS
) allows to achieve a good rank, i.e. the
4
th
position with the unconstrained system in the
restaurant domain. The error analysis underlines
that the proposed features do not capture irony.
Table 6: Aspect Category Polarity Results.
System (Rank) Accuracy
UNITOR-C (7/25) .7307
UNITOR-U (4/25) .7629
Best-System-C (1/25) .8292
Best-System-U (9/25) .7278
766
References
Yasemin Altun, I. Tsochantaridis, and T. Hofmann.
2003. Hidden Markov support vector machines. In
Proceedings of the International Conference on Ma-
chine Learning.
Giuseppe Castellucci, Simone Filice, Danilo Croce,
and Roberto Basili. 2013. Unitor: Combining
syntactic and semantic kernels for twitter sentiment
analysis. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 369?
374, Atlanta, Georgia, USA, June. ACL.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neu-
ral Information Processing Systems (NIPS?2001),
pages 625?632.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent semantic kernels. J. Intell.
Inf. Syst., 18(2-3):127?152.
Danilo Croce and Daniele Previtali. 2010. Mani-
fold learning for the semi-supervised induction of
framenet predicates: an empirical investigation. In
GEMS 2010, pages 7?16, Stroudsburg, PA, USA.
ACL.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of EMNLP, Edinburgh, Scotland, UK.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 340?348. ACL.
Gene Golub and W. Kahan. 1965. Calculating the sin-
gular values and pseudo-inverse of a matrix. Journal
of the Society for Industrial and Applied Mathemat-
ics: Series B, Numerical Analysis, 2(2):pp. 205?224.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. MIT Press, Cambridge, MA.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
ACL?03, pages 423?430.
Tom Landauer and Sue Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104.
Bing Liu. 2007. Web data mining. Springer.
Saif Mohammad and Peter D. Turney. 2013. Crowd-
sourcing a word-emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Alessandro Moschitti, Daniele Pighin, and Robert
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML, Berlin, Germany, September.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics, 33(2).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In EMNLP, vol-
ume 10, pages 79?86, Stroudsburg, PA, USA. ACL.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D. thesis, Stockholm University.
John Shawe-Taylor and Nello Cristianini. 2004a. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, New York, NY, USA.
John Shawe-Taylor and Nello Cristianini. 2004b. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P.
O?Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proceedings of the 37th annual meeting of the
ACL on Computational Linguistics, pages 246?253,
Stroudsburg, PA, USA. ACL.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technologies Conference/Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005), Vancouver, CA.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the
32Nd Annual Meeting of ACL, ACL ?94, pages 133?
138, Stroudsburg, PA, USA. ACL.
767
Combining Word Sense and
Usage for Modeling Frame
Semantics
Diego De Cao1
Danilo Croce1
Marco Pennacchiotti2
Roberto Basili1
1University of Rome Tor Vergata (Italy)
2University of the Saarland (Germany)
email: decao@info.uniroma2.it
Abstract
Models of lexical semantics are core paradigms in most NLP applica-
tions, such as dialogue, information extraction and document understand-
ing. Unfortunately, the coverage of currently available resources (e.g.
FrameNet) is still unsatisfactory. This paper presents a largely applicable
approach for extending frame semantic resources, combining word sense
information derived fromWordNet and corpus-based distributional infor-
mation. We report a large scale evaluation over the English FrameNet,
and results on extending FrameNet to the Italian language, as the basis of
the development of a full FrameNet for Italian.
85
86 De Cao, Croce, Pennacchiotti, and Basili
1 Introduction and Related Work
Models of lexical meaning are explicit or implicit basic components of any text pro-
cessing system devoted to information extraction, question answering or dialogue.
Several paradigms proposed for a variety of notions, such as word sense (Miller et al,
1990) or frame semantics (Baker et al, 1998), have given rise to large scale resources,
respectively WordNet and FrameNet. Recent studies (e.g. Shen and Lapata (2007))
show that the use of FrameNet can potentially improve the performance of Question
Answering systems. Yet, Shen and Lapata (2007) also point out that the low cov-
erage of the current version of FrameNet significantly limits the expected boost in
performance. Other studies have shown similar evidences for Recognizing Textual
Entailment (RTE) (Clark et al, 2007; Burchardt et al, 2008): most examples of the
RTE challenges corpora can be solved at the predicate-argument structure level, but
FrameNet coverage is still a major problem.
Approaches to (semi-)automatically acquire frame information are then today a
priority to solve these problems. Despite this, not many efforts have been paid so far
in this direction. Burchardt et al (2005) presented Detour, a system for predicting
frame assignment of potential lexical units not covered by FrameNet, by using the
paradigmatic information enclosed in WordNet. Although the authors do not fully
solve the problem related to the fuzzy relationships between senses and frames, they
propose an empirical association measure for ranking frame candidates according to
sense information as stored in WordNet. To our knowledge, this is the only work
trying to bridge frame membership to referential properties of lexical senses. Pitel
(2006) presents a preliminary study on the applicability of semantic spaces and space
geometrical transformations (namely, Latent Semantic Analysis) to expand FrameNet,
but the investigation is too limited in scope to draw relevant conclusions. Finally, Pad?
et al (2008) propose a method to automatically label unknown semantic roles of event
nominalizations in FrameNet, but their method needs a large amount of annotated
verbal data.
Another important limitation of FrameNet is the limited support to multilingual-
ity, which is becoming a critical issue in real NLP applications. In recent years,
some efforts have focused on the manual adaptation of the English FrameNet to other
languages (e.g., German (Burchardt et al, 2006) and Spanish (Subirats and Petruck,
2003)). Unlike PropBank, FrameNet is in fact suitable to cross-lingual induction, as
frames are mostly defined at the conceptual level, thus allowing cross-lingual interpre-
tation. Yet, all these efforts consist in manually defining frame linguistic knowledge
(e.g. lexical units) in the specific language, and in annotating a large corpus, thus
requiring a large human effort. While attempts to automate the annotation process
are quite promising (Pado and Lapata, 2007), they require the availability of a par-
allel corpus, and leave open the issue of inducing the resource as a whole in a new
language.
In this work, we investigate novel methods for automatically expanding the English
FrameNet, and supporting the creation of new ones in other languages (namely Ital-
ian), thus tackling the abovementioned problems of coverage and multilinguality. The
proposed methods are inspired by the basic hypothesis that FrameNet can be automat-
ically modeled by a fruitful interaction between advanced distributional techniques,
and paradigmatic properties derived from WordNet.
Combining Word Sense and Usage for Modeling Frame Semantics 87
In particular, in this paper we focus on the application of such methods to study
the semantics of the core elements of FrameNet, i.e. the lexical units (hereafter LUs).
Lexical units are predicates (nouns, verbs, adjectives, etc.) that linguistically express
the situation described by a frame. Lexical units of the same frame share semantic
arguments. For example the frame KILLING has lexical units such as: assassin, blood-
bath, fatal, massacre, kill, suicide. These predicates share semantic arguments such
as KILLER, INSTRUMENT and VICTIM. Our goal is to combine corpus distributional
evidence with WordNet information to supply three tasks: the induction of new LUs
not already in FrameNet (unknown LUs); the reduction of LUs polysemy by mapping
them to WordNet synsets; the translation of English LUs into Italian.
The paper is organized as follows. In Section 2 we describe our FrameNet paradig-
matic and distributional model, and we discuss how these two models can be com-
bined in a single framework. In Section 3 we analyze the applicability of these models
to the three proposed experimental tasks, and discuss the results. Finally, in Section 4
we draw final conclusions and outline future work.
2 Paradigmatic and distributional models of frame semantics
In this section we describe our paradigmatic, distributional and combined models for
representing FrameNet. The general goal of each of these three methods is to offer
a computational model of FrameNet. In such a model, frames and LUs should have
a specific computational representation (e.g. vectors), and allow the computation of
similarity either among different LUs or between a frame and a LU. Such model thus
offers explicit means to use FrameNet in a NLP task or to expand FrameNet, e.g. by
assigning unknown LUs to its most similar frame, or by mapping a LU to its proper
WordNet synset(s). A key notion for these tasks is the definition of a principled and
reliable semantic similarity measure sim to be applied to frames and LUs.
2.1 Paradigmatic model
The basic intuition behind our paradigmatic model is that knowledge about predicates
of a frame, through a (possibly limited) set of LUs, allows to detect the set of the
suitable WordNet senses able to evoke the same frame. These senses are topologically
related to (one or more) sub-hierarchies capturing the lexical semantics implicit in the
frame. We propose a weakly-supervised approach to discover these structures. The
main idea is that frames correspond to specific sub-graphs of the WordNet hyponymy
hierarchy, so that these latter can be used to predict frames valid for other LUs, not
yet coded in FrameNet. Figure 1 reports the WordNet sub-hierarchy covering the
frame PEOPLE_BY_AGE: here, the frame?s nominal LUs {adult, adolescent, baby,
boy, infant, kid, geezer, teenager, youngster, youth} are all represented with the senses
correctly referring to the frame. The correct senses (e.g. sense 1 of youth out of its
6 potential senses) are selected as they share most specific generalizations with the
other LUs. This graph can be intended as an ?explanation? of the lexical semantic
properties characterizing the frame: future predictions about new LUs can be done on
the basis of the graph as a paradigmatic model for PEOPLE_BY_AGE. We call such a
graph the WordNet model of the frame. As WordNet organizes nouns, verbs and other
parts-of-speech in different hierarchies, three independent WordNet models (one for
each part-of-speech) are created for each frame.
88 De Cao, Croce, Pennacchiotti, and Basili
Formally, given the set F of the LUs of a frame, a WordNet model is built around
the subset SF of WordNet synsets able to generalize the largest number of words in
F1.
Figure 1: The WordNet model for the frame People_by_Age as evoked by the set of
its nouns. Sense numbers #n refer to WordNet 2.0
The WordNet modelWNF(?,W ) of a frame F , is a graph
(3) WNF(?,W ) =<W,SF ,LF ,h,simWN ,m >
where: W ? F are the subset of all LUs in F having the same part-of-speech ? ?
{verb,noun,ad jective}, SF is the subset of synsets in WN needed to generalize words
w ?W ; LF ? SF are the lexical senses of w ?W subsumed by SF ; h ? SF ?SF is the
projection of the hyponymy relation of WN in SF ; m?W ?2LF is the lexical relation
between words w ?W and synsets in LF ; simWN : SF ? ? is a weighting function that
expresses the relevance of each sense ? ? SF for the frame, as it is represented by its
words in F .
The model exemplified in Figure 1 is
WNPeople_by_Age(noun, {adult, ..., youth}), where LF = {adult#1, adolescent#1, baby#1,
boy#1, boy#2, boy#3, ..., youth#1} and the set SF corresponds to the sub-hierarchies dom-
inated by the synsets #6026, #9622621, #9285271 and #9015843.
The overall goal of computing the WordNet model is to determine the similarity
function simWN : SF ? ?, expressing the relevance of a synset ? ? SF as a good
representative of a frame F . This is what is hereafter referred to as the paradigmatic
similarity model between words senses and frames.
Paradigmatic Similarity measures
Given the WordNet hierarchy separation on part-of-speaches, the similarity function
simWN is independently defined for verbs, nouns and adjectives.
1In the following, we will use the same notation for a frame F and for the set of its known lexical units,
as in our approach we use LU membership as a basic definition of a frame.
Combining Word Sense and Usage for Modeling Frame Semantics 89
For nouns we adopt conceptual density (cd) (Agirre and Rigau, 1996; Basili et al,
2004), a semantic similarity measure defined for word sense disambiguation tasks.
The cd score for a sense ? ? SF is the density of the WordNet sub-hierarchy rooted
at ? in representing the set of nouns in F . The intuition behind this model is that the
larger is the number of all and only LUs in F that are generalized by ?, the better
it captures the lexical semantics intended by the frame. Coarse generalizations (i.e.
synsets higher in the hierarchy) are penalized, as they give rise to bushy hierarchies,
covering too many words not in the target F . The greedy algorithm proposed in Basili
et al (2004) selects the subset of synsets able to ?cover? (i.e. generalize) all the input
words and characterized by the highest cd values. The set of such synsets and their
corresponding sub-hierarchies forms a graph derived from a set of LUs F . The result
is the WordNet model WNF for F , i.e. the minimal subset of WordNet that explains
all (the possible) LUs in F with the maximally similar senses.
Figure 1 shows that correct senses (e.g. the sense 1 of youth out of the 6 potential
senses) are generally detected and preserved in the model. Irrelevant senses that do
not share any common hypernym with other words in F are neglected. Conceptual
density scores can be used to rank individual senses as in the case of boy.
Given a frame F , the above model can be naturally used to compute the similarity
between a noun n /? F and F . This is particularly useful in LU induction task, as de-
scribed in Section 3.1. To do so, the similarity simWN(F,n) between n and F is derived
by computing the cd scores over the set F ?{n}. The simWN(F,n) is the maximal cd
of any synset ?n ? SF that is also hypernym of a lexical sense of n. In the exam-
ple, the noun boy would receive a score of 0.117 through the hypernym {child,kid},
according to its third sense in WordNet 2.0 (?{son,boy}?).
As conceptual density can be only applied to nouns, when verbs v are consid-
ered, we exploit the synonymy and co-hyponymy relations. The following similarity
simWN(F,v) is computed:
(4) simWN(F,v) =
?
?
?
1 iff ?K ? F such that |K| > ? AND
?w ? K w is a co-hyponym of v
? otherwise
For adjectives, the similarity simWN(F,a), is computed on the basis of the syn-
onymy relation, as follows:
(5) simWN(F,a) =
?
?
?
1 iff ?w ? F such that
w is a synonym of tw
? otherwise
The overall model WNF is used to predict if a frame F is a correct situation for a
given unknownLU ul /?F (a noun, a verb or an adjective), whenever simWN(F,ul) > ?.
This can be used as a frame predictor for a ul currently not foreseen in the Berkley
database but possibly very frequent in a specific corpus, as described in Section 3.1.
2.2 Distributional model
The distributional model is based on the intuition that FrameNet frames and LUs can
be modelled in a semantic space, where they are represented as distributional co-
occurrence vectors computed over a corpus. Such framework, it is possible to compute
90 De Cao, Croce, Pennacchiotti, and Basili
the similarity between a LU and a frame, by evaluating the distance of their vectors in
the space.
Semantic spaces have been widely applied in several NLP tasks, ranging from in-
formation retrieval to paraphrase rules extraction (Lin and Pantel, 2001). The intuition
is that the meaning of a word can be described by the set of textual contexts in which
it appears (Distributional Hypothesis (Harris, 1964)), and that words with similar vec-
tors are semantically related. This distributional approach has been often claimed to
support the language in use view on meaning. Word space models (Sch?tze, 1993)
have been shown to emphasize different aspects of lexical semantics: associative (i.e.
topical) information between words, as well as paradigmatic information (i.e. in ab-
sentia) or syntagmatic information (i.e. in presentia).
In our setting, the goal is to leverage semantic spaces to capture the notion of frame
? i.e. the property of ?being characteristic of a frame?. To do so, we model a lexical
unit l as a vector ~l, whose dimensions represent the set of contexts of the semantic
space. In our space, contexts are words appearing in a n-window of the lexical unit:
such a space models a generic notion of semantic relatedness ? i.e. two LUs close in
the space are likely to be either in paradigmatic or syntagmatic relation (Pado, 2007;
Sahlgren, 2006). The overall semantic space is then represented by a matrix M, whose
rows describe LUs and whose columns describe contexts.
We reduce in dimensionality the matrix M by applying Singular Value Decom-
position (SVD) (Landauer and Dumais, 1997), a decomposition process that creates
an approximation of the original matrix, aiming to capture semantic dependencies
between source vectors, i.e. contexts. The original space is replaced by a lower di-
mensional space Mk, called k-space in which each dimension is a derived concept.
The matrix M is transformed in the product of three new matrices: U , S, and V such
that M = USVT . Truncating M to its first k dimensions means neglecting the least
meaningful dimensions according to the original distribution. Mk captures the same
statistical information in a new k-dimensional space, where each dimension is a linear
combination of some original features. These newly derived features may be thought
of as artificial concepts, each one representing an emerging meaning component as a
linear combination of many different words (or contexts).
The SVD reduction has two main advantages. First, the overall computational cost
of the model is reduced, as similarities are computed on a space with much fewer
dimensions. Secondly, it allows to capture second-order relations among LUs, thus
improving the quality of the similarity measure.
Once the vectors~l for all FrameNet LUs are available in the reduced space, it is
also possible to derive a vectorial representation ~F of a whole frame F . Intuitively,
~F should be computed as the geometric centroid of the vectors of its lexical units.
Unfortunately, such a simple approach is prone to errors due to the semantic nature
of frames. Indeed, even if the LUs of a given frame describe the same particular
situation, they can typically do that in different type of contexts. For example, the
LUs assassinate and holocaust evoke the KILLING frame, but are likely to appear in
very different linguistic contexts. Then, the vectors of the two words are likely to be
distant in the space. Consequently, different regions of the semantic space may act
as good representations for the same frame: these regions corresponds to clusters of
LUs which appear in similar contexts (e.g. {holocaust,extermination,genocide} and
Combining Word Sense and Usage for Modeling Frame Semantics 91
{suicide,euthanasia}).
We then adopt a clustering approach to model frames: each frame is represented
by the set of clustersCF of its lexical units. ClustersCF are composed by lexical units
close in the space and can have different size. They are computed by using an adaptive
(unsupervised) algorithm, based on k-means (Heyer et al, 1999; Basili et al, 2007),
applied to all known LUs of F . Each cluster c ? CF is represented in the space by a
vector~c, computed as the geometric centroid of all its lexical units.
In this framework, it is then possible to compute the similarity between an unknown
LU ul and a frame F , as the the cosine distance between the vector ~ul and the closest
centroid ~c ?CF :
(6) simLSF (F,ul) = argmaxc?CF simcos(~ul, ~CF )
Given this measure, it is finally possible to assign an unknown ul to one or more of
the most similar frames.
2.3 Combining paradigmatic and distributional models
In order to be effective in a NLP task, a model of lexical meaning should typically ac-
count for both the paradigmatic and distributional similarity. The following definition
thus hold:
(7) ?(F,w) = ?(D(w,F),P(w,F))
where ?(F,w) is the association between a word w and a frame F , ? is a com-
position operator applied to the corpus-driven distributional measure D(F,w) and
to the paradigmatic similarity P(F,w). Notice that in this work simLSF (F,w) and
simWN(F,w) are used as models of D(F,w) and P(F,W ), respectively. Different com-
binations ? are here possible, from simple algebraic operations (e.g. linear combina-
tions) to more complex algorithmics. We will explore this latter issue in section 3.1
where the evaluation of a combined model for LU induction is reported.
3 Experiments
In this section we experiment our proposed models on three different tasks: induc-
tion of new LUs (Section 3.1), mapping LUs to WordNet synsets (Section 3.2), and
automatic acquisition of LUs in Italian (Section 3.3). In all the experiments we use
FrameNet 1.3, consisting of 795 frames and about 10,196 LUs (7,522 unique LUs), as
source information and as a gold standard. As regardsWordNet, we adopt version 2.0,
with all mappings from 1.6 applied through the Italian component of MultiWordNet
(Pianta et al, 2008)2
For computing vectors in the distributional model, we use the TREC-2002 Vol.2
corpus, consisting of about 110 million words for English. The contexts for the de-
scription of LUs are obtained as ?5 windows around each individual LU occurrence:
each word occurring in this windows is retained as a potential context 3. A resulting
set of about 30,000 contexts (i.e. individual words) has been obtained. The vector~l
2http://www.lsi.upc.es/~nlp/tools/download-map.php
3For all occurrences of feature words, the POS tag has been neglected in order to verify the applicability
of the model even with a shallow preprocessing. Words occurring less than 10 times in all windows are
neglected in our experiments.
92 De Cao, Croce, Pennacchiotti, and Basili
representing an individual LU is derived by computing pointwise mutual information
between the LU and each context. The SVD reduction has been run over the result-
ing 7.522? 30,000 matrix, with a dimension cut of k = 50, other values resulting in
non-statistically different outcomes.
Experiments for Italian are run against the italian component of the Europarliament
corpus (Koehn, 2002), made of about 1 million sentences for about 36 millions tokens,
for which about 87,000 contexts are used for the targeted LUs. Also for Italian a
dimension cut of k = 50 has been applied.
3.1 Lexical Unit induction
The goal of this experiment is to tackle the FrameNet low coverage problem, by check-
ing if our models are good in expanding FrameNet with new LUs. Formally, we de-
fine LU induction as the task of assigning a generic unknown lexical unit ul not yet
present in the FrameNet database to the correct frame(s). As the number of frames
is very large, the task is intuitively hard to solve. A further complexity regards mul-
tiple assignments. Lexical units are sometimes ambiguous and can then be mapped
to more than one frame (for example the word tea could map both to FOOD and SO-
CIAL_EVENT). Also, even unambiguous words can be assigned to more than one
frame ? e.g. child maps to both KINSHIP and PEOPLE_BY_AGE.
In the experiment, we simulate the induction task by executing a leave-one-out
procedure over the set of existing FrameNet LUs, as follows. First, we remove a LU
from all its original frames. Then, we ask our model to reassign it to the most similar
frame(s), according to its similarity measure. We repeat this procedure for all lexical
units and compute the accuracy in the assignment.
We experiment all three models: distributional, paradigmatic and the combined
one. In particular, the combined model is applied as follows. First, for each frame
F we create its cluster set CF in the LSA space. Then, at each iteration of the leave-
one-out a different LU ul is removed from FrameNet, and the following steps are
performed:
? We recompute the clusters for all frames Ful which contain ul, by neglecting
ul.4
? We compute the similarity simLFS(F,ul) between ul and all frames. During
the computation we empirically impose a threshold: if a cluster c ? C has a
similarity cos(c,ul) < 0.1 (i.e. poorly similar to ul), it is neglected. Finally, all
suggested frames are ranked according to simLFS(F,ul).
? For each frame F we also compute the similarity simWN(F,ul) according to the
paradigmatic model, by neglecting ul in the computation of the WordNet model
of each frame.
? We combine the distributional and paradigmatic similarities following the gen-
eral Equation 7, by applying the following specific equation:
(8) sim(F,ul) = simLFS(F,ul) ? simWN(F,ul)
4Note that all appearances of ul in the database are neglected (irrespectively from its POS tag, e.g.
march as a verb vs. march as a noun)
Combining Word Sense and Usage for Modeling Frame Semantics 93
Table 1: The Gold Standard for the test over English and Italian
English Number of frames: 220
Number of LUs: 5042
Most likely frames: Self_Motion (p=0.015), Clothing (p=0.014)
Italian Number of frames: 10
Number of LUs: 112
Frames: Buildings, Clothing, Killing, Kinship, Make_noise
Medical_conditions, Natural_Features, Possession, Self_Motion, Text
Note, that in practice sim(F,ul) acts as a re-ranking function of the previously
obtained clusters CF
? We execute LU induction, by mapping ul to the most similar k frames according
to sim(F,ul).
Evaluation
We evaluate the model by computing the accuracy over the FrameNet gold standard.
Accuracy is defined as the fraction of LUs that are correctly re-assigned to the original
frame during the leave-one-out. Accuracy is computed at different levels k: a LU is
correctly assigned if its gold standard frame appears among the best-k frames ranked
by the model. We experimented both on English (using FrameNet version 1.3), and on
Italian. Since an Italian FrameNet is not available, we manually created a gold stan-
dard of 11 frames. Overall statistics on the data are reported in Table 1: the number of
frames and LUs analysed is slightly reduced wrt FrameNet as we ignored the predicate
words absent from the targeted corpus (e.g. moo in MAKE_NOISE) and multiwords
expressions as it was not possible to locate them unambiguously in the corpus (e.g.
shell out in COMMERCE_PAY). Also, in order to get reliable distributional statistics,
we filter out LUs occurring less than 50 times in the corpus, and frames with less than
10 LUs.
For all the experiments, the parameter ? in the Equation 4 of the paradigmaticmodel
has been set to 2. As a baseline, we adopt a model predicting as best-k frames the most
likely ones in FrameNet ? i.e. those containing the highest number of LUs.
Results for English are reported in Figure 2.
As shown, all methods improve significantly the baseline whereas accuracy values
naturally improve along increasing values for k. The performance of the paradigmatic
model are significantly high even for very small k. The best model is given by the
combination of distributional and paradigmatic similarity, producing significant im-
provements wrt the paradigmatic model alone.
Results for Italian are reported in Figure 3. The leave-one-out test has been applied
as for English, but over a manually compiled set of 527 LUs for the 11 frames used
as gold standard. These LUs have been obtained via direct translation of the English
Framenet LUs. In order to evaluate LUs for which a consistent distributional model
was available, only those occurring at least 50 times in the Europarliament corpus have
been selected: this amounts to a total number of 112 Italian LUs. The paradigmatic
94 De Cao, Croce, Pennacchiotti, and Basili
Figure 2: Accuracy of the leave-one-out over the English FrameNet 1.3
model for the test has been obtained using as source the LUs in the English FrameNet.
As the computation of the simWN(F,w) depends only on the hyponymy hierarchy, for
each Italian noun n the conceptual density computation over the set {n}?F is applied,
where F is given by the LUs in English. The interlingual index is here used to map
every n to its lexical senses (i.e. synsets) in the English WN. Then, the computation
of the greedy algorithm is applied exactly as in the monolingual process. The same
approach has been used for verbs (Equation 4) and adjectives (Equation 5).
Although the limited scale of the experiment (only 11 frames are targeted), the ev-
idence are similar as for the test over English: the combined model is always superior
to the individual ones. High levels of accuracy are achieved, although the ?most likely
frame? baseline is much higher than for the English test. Similar trends are also ob-
served for the paradigmatic model, reaching a plateau for smaller values of k. Overall
results indicate that reliable predictions can be obtained for unknown LUs also when
a whole Italian FrameNet is not yet available. Our method can then be used to support
lexicographers in the task of building a new FrameNet, in the specific stage of adding
LUs to frames.
Results suggest that the WordNet models derived from the English LUs are valid
predictors also for Italian words, as confirmed by the experiments in the next sections.
3.2 Assessing WordNet models of Frames
The goal of the experiment is to validate the notion of WordNet model of a frame as
derived through the method discussed in Section 2.1. Formally, given the set of all
possible WordNet senses Sl of a given LU l, we aim at mapping each sense s ? Sl to
the correct frame f ? Fl , where Fl is the set of frames in which l appears. If a frame
cannot be found for a given sense, the sense is simply neglected.
For example, the LU burn has 15 senses in WordNet and it belongs to 3 frames:
EMOTION_HEAT, EXPERIENCE_BODILY_HARM and PERCEPTION_BODY. Figure 2
Combining Word Sense and Usage for Modeling Frame Semantics 95
Figure 3: Accuracy of the leave-one-out tests over 11 frames in Italian
reports some of the possible correct mapping between senses and frames. Other
senses, such as ?destroy by fire? cannot be mapped to any existing frame.
By creating such an automatic mapping we achieve three goals. First, we disam-
biguate FrameNet lexical units. Second, we enrich WordNet synsets with new infor-
mation ? i.e. a computational description of the situations they refer to, as repre-
sented in FrameNet. Third, we derive a language independent model of frames based
on WordNet synsets.
The mapping targeted by the experiment is carried out according to the discussion
in Section 2.1. The WordNet model of a frame F for nouns is the outcome of the
greedy cd computation over the set F of all frame?s LUs: given a LU, a sense is
accepted if it is a member of the set LF in the model. For verbs and adjectives all co-
hyponyms and synonyms used in Equations 4 and 5 are included in LF . The procedure
for developing a WordNet model is completely automatic, this avoiding the costs of
manual annotation.5
Note that our approach is easily portable to languages different from English. In-
deed, the WordNet hierarchy is the backbone of sense repositories in other languages
(as for example in MultiWordNet (Pianta et al, 2008)). The English modelsWNF can
be then interpreted in a different language, by applying the interlingual indexes to all
synsets LF in WNF . The corresponding sets of synonyms are natural candidates as
LUs in the target language.
Evaluation
In this experiment, in order to account for data sparseness we reduce the dataset in
two ways. First, we neglect low frequency lexical units: LUs occurring less than 50
times in the corpus are not considered. Second, we exclude frames that have less than
5In this experiment we focus on verbs and nouns, since they are core predicates expressing the targeted
situation in sentences.
96 De Cao, Croce, Pennacchiotti, and Basili
Table 2: Mapping between WordNet senses and frames for verb burn, as induced by
the paradigmatic method
Synset Evoked FRAME Co-Hyponyms WordNet Definition
1775952 EMOTION_HEAT chafe, fume, smolder Feel strong emotion, especially anger or
passion; ?She was burning with anger?;
?He was burning to try out his new
skies?
189569 EXPERIENCE_BODILY_HARM break, bruise, hurt,
injure
Burn with heat, fire, or radiation; ?The
iron burnt a hole in my dress?
2059143 PERCEPTION_BODY itch, sting Cause a sharp or stinging pain or dis-
comfort; ?The sun burned his face?
10 LUs. This leaves us with 220 frames, involving 2,200 nominal LUs and 2,180
verbal LUs. Table 3 reports overall statistics. Over the 2,200 nouns and 2,180 verbs
examined, the vast majority is covered by WordNet (fourth row). For these words, a
large set of lexical senses exist in WordNet giving an average polysemy between 3 and
6 senses per word (sixth row). Our paradigmatic method is able to significantly reduce
the average polysemy: only 1.79 senses per verb survive among the initial 5.29, while
only 1.29 among the 3.62 are retained for nouns. Moreover, the number of senses
used to entirely represent a frame in a paradigmatic model (i.e. SF ) is about 3,512 and
2,718 respectively for nouns and verbs, as averaged across all frames. An example of
the mapping produced by our method is reported in Table 2.
The above statistics suggest that a consistent reduction in average polysemy can be
obtained when the context of a frame is used to model semantic similarity among LUs
in WordNet.
Table 3: Statistics on nominal and verbal senses in the paradigmatic model of the
English FrameNet
Nouns Verbs
Targeted Frames 220 220
Involved LUs 2,200 2,180
Average LUs per frame 10.0 9.91
LUs covered by WordNet 2,187 2,169
Number of Evoked Senses 7,443 11,489
Average Polysemy 3.62 5.97
Represented words (i.e. ?FWF ) 2,145 1,270
Average represented LUs 9.94 9.85
Active Lexical Senses (LF ) 3,095 2,282
Average Active Lexical Senses (|LF |/|WF |) per word over frames 1.27 1.79
Active synsets (SF ) 3,512 2,718
Average Active synsets (|SF |/|WF |) per word over frames 1.51 2.19
We evaluated the quality of the above process through manual validation. Given a
frame, for each LU we provided two annotators with the list of all its WordNet senses,
and asked to select those that correctly map to FrameNet. Then, we evaluated our
automatic mapping method by computing standard Precision and Recall. In all, we
Combining Word Sense and Usage for Modeling Frame Semantics 97
analysed all 786 senses of 306 LUs in 4 frames (i.e. KILLING, PEOPLE_BY_AGE,
STATEMENT and CLOTHING). The Cohen?s kappa, computed over two frames (i.e.
KILLING and PEOPLE_BY_AGE for 192 senses of 77 words) results in a 0.90 inter-
annotator agreement: this indicates that senses and frames are highly correlated and
their mapping is consistent and motivated, as Table 2 suggests.
The system is considered to accept a sense ? for a given frame F iff the conceptual
density score characterizing such a sense is positive, i.e. the ? ? LF . Our method
obtained a Precision of 0.803 and a Recall of 0.79 (F-measure=0.796). Among the 786
senses tested, 85 false positives and 92 false negatives have been found: 346 senses
have been correctly accepted and 263 true negatives have been rejected by the sytem.
It must be also noticed that the conceptual density scores obtained are well correlated
with correct senses. If senses of a word with significantly lower cd scores than others
are removed from the set LF of a frame, a significant improvement in precision can
be obtained. For example, tie in CLOTHING has 9 senses, of which 3 are proposed by
the system, corresponding to 1 true positives, 2 false positives and 6 true negatives.
It is interesting to note that the true positive sense (i.e. {necktie, tie} as ?a neckwear
consisting of a long narrow piece of material worn ...?) has a cd score of 0.492, while
0.018 is the score of all the three false positives (i.e. sense #5 {link,linkup tie, tie-in}
as ?a fastener that serves to join or link?; sense #8 {tie, railroad tie, crosstie, sleeper}
as ?one of the cross braces that support the rails on a railway track?; sense #9 {tie} as
?a cord with which something is tied?). A careful selection policy can be thus easily
devised to deal with such skewed preference distributions and achieve higher values
of precision by neglecting lower preferences.
These results show that the proposed frame WordNet model is not only effective
in reducing the average lexical polysemy (as shown in Table 3), but it is also a rather
accurate method to capture the lexical semantics implied by frames. The achieved
level of accuracy justifies the adoption of the model defined in (3) for the development
of FrameNets in languages other than English.
3.3 Development of an Italian FrameNet
In this section we explore the use of our English paradigmatic model to automatically
support the building process of a FrameNet in a different language, namely Italian. In
particular, we leverage the model WNF for the English language to induce new LUs
for F in the new language. To do so, we proceed as follows.
For each frame F in FrameNet we first generate the WordNet model for English
WNF using all the LUs available in the database, as discussed in Section 2.1. Then,
we use an interlingual index (e.g. MultiWordNet) to obtain words in the new language
corresponding to lexical senses LF in the model WNF . Each of these translated LU
l is a cross-lingual synonym of at least a sense in SF and is a candidate LUs for the
frame in the new language, since it satisfies simWN(F, l) > ?.
Evaluation
In the experiment we focus on Italian, for which a full FrameNet is not yet available,
though a manual building process is currently underway (Tonelli and Pianta, 2008).
As interlingual index we adopt the Italian component of MultiWordNet (Pianta et al,
2008). As shown in Table 4, the WordNet model allows to generate approximately
15,000 Italian LUs, partitioned in 6,600 nouns, 8,300 verbs and 130 adjectives.
98 De Cao, Croce, Pennacchiotti, and Basili
Table 4: Number of generated Lexical Units
Number of LUs
Nouns 6611
Verbs 8332
Adjectives 129
Total 15072
To evaluate the quality of the translated LUs we performed two different tests. In
the first test, we collected the 776 most frequent words in the Europarliament corpus,
including many generic nouns and verbs, such as produrre (to_produce/make), fare
(to_make/fabricate), avere (to_have). Then we manually validated all the 1,500 sys-
tem decisions regarding these words. A decision is accepted if the frame suggested
for the word is correct for at least one of its senses. Accuracy is computed as the per-
centage of the correct system decisions over the number of validated cases. For some
words no frame was predicted, as they were not in Wordnet or as no Wordnet model
was able to correctly generalize them. The percentage of words receiving at least one
correct prediction, i.e. assigned to at least one frame accepted by the annotators, is
here called Coverage, and reported with the accuracy scores in the second line of Ta-
ble 5. The above test was repeated also for more specific words, with a number of
occurrences in the corpus ranging from 100 to 200. Results are reported in the third
line of Table 5. These outcomes are surprisingly good especially considering that the
computation of the individual simWN(F, l) scores is fully automatic.
Table 5: Manual validation of the italian LUs generated through WordNet
Frequency Numb. of Numb. of
Range Test pairs Words Acc. Cov.
[722;55,000] 1,500 776 0.79 93.0%
[100;200] 558 357 0.87 94.3%
In a second test, the results generated by our method were compared against the
words of the oracle manually developed for the experiment in Section 3.1. In this
case, the predictions of the method about frames and words are compared with the
oracle. As no filter has been here applied with respect to the corpus, all the 527
< LU,Frame > pairs in the manually created oracle have been used6, although only
437 pairs were represented through the MultiWordNet resources. The system was
not able to decide for 71 words, and produced wrong guesses for 49 words: 317
correct guesses are thus produced. The results is a Precision of 0.87 and a Recall of
0.72. The recall value, lower than the coverage observed in the previous test, is also
by a significant generative effect: the method discovers a number of new entries not
accounted for in the oracle.
6Notice that no LU was multiply assigned to different frames in the oracle, so that the number of predi-
cate words here is exactly the number of different pairs.
Combining Word Sense and Usage for Modeling Frame Semantics 99
Table 6: Excerpt of the Italian LUs in four different frames
FRAME FRAME definition Italian LUs
BUILDINGS Words which name permanent
fixed structures forming an
enclosure and providing pro-
tection from the elements
autorimessa, cuccia, casolare,
casotto, dependance, masseria,
palazzina, ...
CLOTHING Anything that people conven-
tionally wear
cappotto, calzetta, cami-
cia_da_notte, duepezzi, ...
SELF_MOTION The Self mover, a living being,
moves under its own power in a
directed fashion
annaspare, arrancare, buttarsi,
claudicare, giro, ...
TEXT An entity that contains linguis-
tic, symbolic information on a
Topic, created by an Author at
the Time of creation
arringa, articolo_di_fondo,
canzonetta, conto, polizza,
vademecum, ...
Typical new LUs introduced in the oracle are words not accounted for in the En-
glish Framenet, as reported in Table 6. The table shows that most of the new guesses
of the system are indeed highly plausible. They represent widely used dialectal forms
(e.g. masseria in the frame BUILDING), jargon (e.g. duepezzi in CLOTHING), techni-
cal terms (e.g. polizza in TEXT) and specific nouns (e.g. autorimessa in BUILDINGS).
Although it is certainly questionable if words like articolo_di_fondo (i.e. main article
in a newspaper) are worth to be considering as LUs for the frame TEXT, it is clear that
if the application domain requires frame-like information, the presented model (even
only the paradigmatic association here discussed) provides an effective tool for fast
and robust prototyping. Notice that the low Recall (only 0.60 of the oracle words are
correctly addressed), can be compensated by combining paradigmatic and distribu-
tional similarity (Equation 8), as experiments reported in Figure 3 suggest. We leave
this last point as a future work.
4 Conclusions
We presented a combined model for representing frame semantics through paradig-
matic and distributional evidence. We reported three experiments, which indicate pos-
sible application scenarios of these models. First, the combination of the presented
models has been applied to extend FrameNet in a LU induction task, for English and
Italian. In both cases the evaluation has shown that the combination of the two models
achieves better performance against their independent uses, and that the level of accu-
racy is high enough to support lexicographers in the task of building FrameNets. In a
second experiment, we showed that a strong association exists between lexical senses,
as defined by WordNet, and the frame?s lexical units in FrameNet. Its automatic de-
tection, as proposed in this paper, results in a significant reduction of the polysemy of
LUs and in a highly accurate selection of those lexical senses semantically related to
the situations represented by a frame. Finally, we demonstrated that this paradigmatic
information can be used to develop a FrameNet resource in another language. For
100 De Cao, Croce, Pennacchiotti, and Basili
Italian, we automatically generated a very large and accurate set of 15,000 LUs.
The overall framework has encouraged us to develop a robust toolbox for the large
scale acquisition of FrameNet-like lexicons in different domains and languages. The
tool will be made publicly available for research studies in this area. Future work is on
going on the adoption of richer models for Framenet, able to take into account more
evidence than LUs, such as frame elements and syntagmatic information. Moreover,
the use of the derived space as a model for the recognition of frames in free-texts is
expected to speed-up the development of a large collection of annotated sentences for
the Italian language.
References
Agirre, E. and G. Rigau (1996). Word sense disambiguation using conceptual density.
In Proceedings of COLING-96, Copenhagen, Denmark.
Baker, C. F., C. J. Fillmore, and J. B. Lowe (1998). The Berkeley FrameNet project.
In Proceedings of COLING-ACL, Montreal, Canada.
Basili, R., M. Cammisa, and F. Zanzotto (2004). A semantic similarity measure for
unsupervised semantic disambiguation. In Proceedings of LREC-04, Lisbon, Por-
tugal.
Basili, R., D. D. Cao, P. Marocco, and M. Pennacchiotti (2007). Learning selectional
preferences for entailment or paraphrasing rules. In Proceedings of RANLP 07.
Burchardt, A., K. Erk, and A. Frank (2005). A wordnet detour to framenet. In Pro-
ceedings of the GLDV 2005 GermaNet II Workshop, Bonn, Germany.
Burchardt, A., K. Erk, A. Frank, A. Kowalski, S. Pado, and M. Pinkal (2006). The
salsa corpus: a german corpus resource for lexical semantics. In Proceedings of
the 5th International Conference on Language Resources and Evaluation, Genova,
Italy.
Burchardt, A., M. Pennacchiotti, S. Thater, and M. Pinkal (2008). Assessing the
impact of frame semantics on textual entailment. Journal of Natural Language
Engineering (to appear).
Clark, P., P. Harrison, J. Thompson, W. Murray, J. Hobbs, and C. Fellbaum (2007,
June). On the Role of Lexical and World Knowledge in RTE3. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, Prague, pp.
54?59. Association for Computational Linguistics.
Harris, Z. (1964). Distributional structure. In J. J. Katz and J. A. Fodor (Eds.), The
Philosophy of Linguistics, New York. Oxford University Press.
Heyer, L., S. Kruglyak, and S. Yooseph (1999). Exploring expression data: Identifi-
cation and analysis of coexpressed genes. Genome Research 9, 1106?1115.
Koehn, P. (2002). Europarl: A multilingual corpus for evaluation of machine transla-
tion. Draft.
Combining Word Sense and Usage for Modeling Frame Semantics 101
Landauer, T. and S. Dumais (1997). A solution to plato?s problem: The latent se-
mantic analysis theory of acquisition, induction and representation of knowledge.
Psychological Review 104, 211?240.
Lin, D. and P. Pantel (2001). DIRT-discovery of inference rules from text. In Proceed-
ings of the ACM Conference on Knowledge Discovery and Data Mining (KDD-01),
San Francisco, CA.
Miller, G., R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. (1990). An on-line
lexical database. International Journal of Lexicography 13(4), 235?312.
Pado, S. (2007). Cross-Lingual Annotation Projection Models for Role-Semantic In-
formation, Volume 21 of Saarbr?cken Dissertations in Computational Linguistics
and Language Technology. Saarland University.
Pado, S. and M. Lapata (2007). Dependency-based construction of semantic space
models. Computational Linguistics 33(2), 161?199.
Pad?, S., M. Pennacchiotti, and C. Sporleder (2008). Semantic role assignment for
event nominalisations by leveraging verbal data. In Proceedings of COLING 2008,
Manchester, UK.
Pianta, E., L. Bentivogli, and C. Girardi (2008). MultiWordNet: Developing an
aligned multilingual database. In Proceedings of the 1st International Global
WordNet Conference, Marrakech, Morocco, pp. 293?302.
Pitel, G. (2006). Using bilingual lsa for framenet annotation of french text from
generic resources. In Workshop on Multilingual Semantic Annotation: Theory and
Applications, Saarbr?ijcken, Germany.
Sahlgren, M. (2006). The Word-Space Model. Ph. D. thesis, Department of Linguis-
tics, Stockholm University.
Sch?tze, H. (1993). Word space. In S. Hanson, J. Cowan, and C. Giles (Eds.), Ad-
vances in Neural Information Processing Systems 5. MorganKaufmann Publishers.
Shen, D. and M. Lapata (2007). Using semantic roles to improve question answer-
ing. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing and on Computational Natural Language Learning, Prague, pp. 12?21.
Subirats, C. and M. Petruck (2003). Surprise! Spanish FrameNet! In Proceedings of
the Workshop on Frame Semantics at the XVII. International Congress of Linguists,
Prague.
Tonelli, S. and E. Pianta (2008). Frame Information Transfer from English to Italian.
In Proceedings of LREC 2008, Marrakech, Morocco.
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 7?16,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Manifold Learning for the Semi-Supervised Induction
of FrameNet Predicates: An Empirical Investigation
Danilo Croce and Daniele Previtali
{croce,previtali}@info.uniroma2.it
Department of Computer Science, Systems and Production
University of Roma, Tor Vergata
Abstract
This work focuses on the empirical inves-
tigation of distributional models for the
automatic acquisition of frame inspired
predicate words. While several seman-
tic spaces, both word-based and syntax-
based, are employed, the impact of ge-
ometric representation based on dimen-
sionality reduction techniques is inves-
tigated. Data statistics are accordingly
studied along two orthogonal perspectives:
Latent Semantic Analysis exploits global
properties while Locality Preserving Pro-
jection emphasizes the role of local reg-
ularities. This latter is employed by em-
bedding prior FrameNet-derived knowl-
edge in the corresponding non-euclidean
transformation. The empirical investiga-
tion here reported sheds some light on the
role played by these spaces as complex
kernels for supervised (i.e. Support Vector
Machine) algorithms: their use configures,
as a novel way to semi-supervised lexical
learning, a highly appealing research di-
rection for knowledge rich scenarios like
FrameNet-based semantic parsing.
1 Introduction
Automatic Semantic Role Labeling (SRL) is a
natural language processing (NLP) technique that
maps sentences to semantic representations and
identifies the semantic roles conveyed by senten-
tial constituents (Gildea and Jurafsky, 2002). Sev-
eral NLP applications have exploited this kind of
semantic representation ranging from Information
Extraction (Surdeanu et al, 2003; Moschitti et al,
2003)) to Question Answering (Shen and Lapata,
2007), Paraphrase Identification (Pado and Erk,
2005), and the modeling of Textual Entailment re-
lations (Tatu and Moldovan, 2005). Large scale
annotated resources have been used by Seman-
tic Role Labeling methods: they are commonly
developed using a supervised learning paradigm
where a classifier learns to predict role labels
based on features extracted from annotated train-
ing data. One prominent resource has been de-
veloped under the Berkeley FrameNet project as
a semantic lexicon for the core vocabulary of En-
glish, according to the so-called frame seman-
tic model (Fillmore, 1985). Here, a frame is a
conceptual structure modeling a prototypical sit-
uation, evoked in texts through the occurrence of
its lexical units (LU) that linguistically expresses
the situation of the frame. Lexical units of the
same frame share semantic arguments. For ex-
ample, the frame KILLING has lexical units such
as assassin, assassinate, blood-bath, fatal, mur-
derer, kill or suicide that share semantic arguments
such as KILLER, INSTRUMENT, CAUSE, VICTIM.
The current FrameNet release contains about 700
frames and 10,000 LUs. A corpus of 150,000 an-
notated examples sentences, from the British Na-
tional Corpus (BNC), is also part of FrameNet.
Despite the size of this resource, it is un-
der development and hence incomplete: several
frames are not represented by evoking words and
the number of annotated sentences is unbalanced
across frames. It is one of the main reason for the
performance drop of supervised SRL systems in
out-of-domain scenarios (Baker et al, 2007) (Jo-
hansson and Nugues, 2008). The limited cover-
age of FrameNet corpus is even more noticeable
for the LUs dictionary: it only contains 10,000
lexical units, far less than the 210,000 entries in
WordNet 3.0. For example, the lexical unit crown,
according to the annotations, evokes the ACCOU-
TREMENT frame. It refers to a particular sense:
according to WordNet, it is ?an ornamental jew-
eled headdress signifying sovereignty?. Accord-
ing to the same lexical resource, this LU has 12
lexical senses and the first one (i.e. ?The Crown
7
(or the reigning monarch) as the symbol of the
power and authority of a monarchy?) could evoke
other frames, like LEADERSHIP. In (Pennacchiotti
et al, 2008) and (De Cao et al, 2008), the prob-
lem of LU automatic induction has been treated
in a semi-supervised fashion. First, LUs are mod-
eled by exploiting the distributional analysis of an
unannotated corpus and the lexical information of
WordNet. These representations were used in or-
der to find out frames potentially evoked by novel
words in order to extend the FrameNet dictionary
limiting the effort of manual annotations.
In this work the distributional model of LUs
is further developed. As in (Pennacchiotti et al,
2008), several word spaces (Pado and Lapata,
2007) are investigated in order to find the most
suitable representation of the properties which
characterize a frame. Two dimensionality reduc-
tion techniques are applied here in this context.
Latent Semantic Analysis (Landauer and Dumais,
1997) uses the Singular Value Decomposition to
find the best subspace approximation of the orig-
inal word space, in the sense of minimizing the
global reconstruction error projecting data along
the directions of maximal variance. Locality Pre-
serving Projection (He and Niyogi, 2003) is a
linear approximation of the nonlinear Laplacian
Eigenmap algorithm: its locality preserving prop-
erties allows to add a set of constraints forcing
LUs that belong to the same frame to be near in
the resulting space after the transformation. LSA
performs a global analysis of a corpus capturing
relations between LUs and removing the noise in-
troduced by spurious directions. However it risks
to ignore lexical senses poorly represented into the
corpus. In (De Cao et al, 2008) external knowl-
edge about LUs is provided by their lexical senses
from a lexical resource (e.g WordNet). In this
work, prior knowledge about the target problem is
directly embedded into the space through the LPP
transformation, by exploiting locality constraints.
Then a Support Vector Machine is employed to
provide a robust acquisition of lexical units com-
bining global information provided by LSA and
the local information provided by LPP into a com-
plex kernel function.
In Section 2 related work is presented. In Sec-
tions 3 the investigated distributional model of
LUs is presented as well as the dimensionality re-
duction techniques. Then, in Section 4 the exper-
imental investigation and comparative evaluations
are reported. Finally, in Section 5 we draw final
conclusions and outline future work.
2 Related Work
As defined in (Pennacchiotti et al, 2008), LU in-
duction is the task of assigning a generic lexical
unit not yet present in the FrameNet database (the
so-called unknown LU) to the correct frame(s).
The number of possible classes (i.e. frames) and
the multiple assignment problem make it a chal-
lenging task. LU induction has been integrated
at SemEval-2007 as part of the Frame Seman-
tic Structure Extraction shared task (Baker et al,
2007), where systems are requested to assign the
correct frame to a given LU, even when the LU is
not yet present in FrameNet. Several approaches
show low coverage (Johansson and Nugues, 2007)
or low accuracy, like (Burchardt et al, 2005). This
task is presented in (Pennacchiotti et al, 2008) and
(De Cao et al, 2008), where two different mod-
els which combine distributional and paradigmatic
(i.e. lexical) information have been discussed. The
distributional model is used to select a list of frame
suggested by the corpus? evidences and then the
plausible lexical senses of the unknown LU are
used to re-rank proposed frames.
In order to exploit prior information provided
by the frame theory, the idea underlying is that se-
mantic knowledge can be embedded from exter-
nal sources (i.e the FrameNet database) into the
distributional model of unannotated corpora. In
(Basu et al, 2006) a limited prior knowledge is ex-
ploited in several clustering tasks, in term of pair-
wise constraints (i.e., pairs of instances labeled
as belonging to same or different clusters). Sev-
eral existing algorithms enhance clustering qual-
ity by applying supervision in the form of con-
straints. These algorithms typically utilize the
pairwise constraints to either modify the clustering
objective function or to learn the clustering distor-
tion measure. The approach discussed in (Basu et
al., 2006) employs Hidden Markov Random Fields
(HMRFs) as a probabilistic generative model for
semi-supervised clustering, providing a principled
framework for incorporating constraint-based su-
pervision into prototype-based clustering.
Another possible approach is to directly embed
the prior-knowledge into data representations. The
main idea is to employ effective and efficient algo-
rithms for constructing nonlinear low-dimensional
manifolds from sample data points embedded
8
in high-dimensional spaces. Several algorithms
are defined, including Isometric feature mapping
(ISOMAP) (Tenenbaum et al, 2000), Locally Lin-
ear Embedding (LLE) (Roweis and Saul, 2000),
Local Tangent Space alignment (LTSA) (Zhang
and Zha, 2004) and Locality Preserving Projec-
tion (LPP) (He and Niyogi, 2003) and they have
been successfully applied in several computer vi-
sion and pattern recognition problems. In (Yang
et al, 2006) it is demonstrated that basic nonlinear
dimensionality reduction algorithms, such as LLE,
ISOMAP, and LTSA, can be modified by taking
into account prior information on exact mapping
of certain data points. The sensitivity analysis
of these algorithms shows that prior information
improves stability of the solution. In (Goldberg
and Elhadad, 2009), a strategy to incorporate lexi-
cal features into classification models is proposed.
Another possible approach is the strategy pursued
in recent works on deep learning techniques to
NLP tasks. In (Collobert and Weston, 2008) a
unified architecture for NLP that learns features
relevant to the tasks at hand given very limited
prior knowledge is presented. It embodies the
idea that a multitask learning architecture coupled
with semi-supervised learning can be effectively
applied even to complex linguistic tasks such as
Semantic Role Labeling. In particular, (Collobert
and Weston, 2008) proposes an embedding of lex-
ical information using Wikipedia as source, and
exploits the resulting language model for the mul-
titask learning process. The extensive use of unla-
beled texts allows to achieve a significant level of
lexical generalization in order to better capitalize
on the smaller annotated data sets.
3 Geometrical Embeddings as models of
Frame Semantics
The aim of this distributional approach is to model
frames in semantic spaces where words are repre-
sented from the distributional analysis of their co-
occurrences over a corpus. Semantic spaces are
widely used in NLP for representing the meaning
of words or other lexical entities. They have been
successfully applied in several tasks, such as in-
formation retrieval (Salton et al, 1975) and har-
vesting thesauri (Lin, 1998). The fundamental in-
tuition is that the meaning of a word can be de-
scribed by the set of textual contexts in which it
appears (Distributional Hypothesis as described in
(Harris, 1964)), and that words with similar vec-
tors are semantically related. Contexts are words
appearing together with a LU: such a space mod-
els a generic notion of semantic relatedness, i.e.
two LUs spatially close in the space are likely to
be either in paradigmatic or syntagmatic relation
as in (Sahlgren, 2006). Here, LUs delimit sub-
spaces modeling the prototypical semantic of the
corresponding evoked frames and novel LUs can
be induced by exploiting their projections.
Since a semantic space supports the language
in use from the corpus statistics in an unsuper-
vised fashion, vectors representing LUs can be
characterized by different distributions. For exam-
ple, LUs of the frame KILLING, such as blood-
bath, crucify or fratricide, are statistically infe-
rior in a corpus if compared to a wide-spanning
term as kill. Moreover other ambiguous LUs, as
liquidate or terminate, could appear in sentences
evoking different frames. These problems of data-
sparseness and distribution noise can be over-
come by applying space transformation techniques
augmenting the space expressiveness in model-
ing frame semantics. Semantic space models very
elegantly map words in vector spaces (there are
as many dimensions as words in the dictionary)
and LUs collections into distributions of data-
points. Every distribution implicitly expresses two
orthogonal facets: global properties, as the occur-
rence scores computed for terms across the entire
collection (irrespectively from their word senses
or evoking situation) and local regularities, for ex-
ample the existence of subsets of terms that tend to
be used every time a frame manifests. These also
tend to be closer in the space and should be closer
in the transformed space too. Another important
aspect that a transformation could account is exter-
nal semantic information. In the new space, prior
knowledge can be exploited to gather a more regu-
lar LUs representation and a clearer separation be-
tween subspaces representing different frame se-
mantics.
In the following sections the investigated dis-
tributional model of LUs will be discussed. As
many criteria can be adopted to define a LU con-
text, one of the goals of this investigation is to find
a co-occurrence model that better captures the no-
tion of frames, as described in Section 3.1. Then,
two dimensionality reduction techniques, exploit-
ing semantic space distributions to improve frames
representation, are discussed. In Section 3.2 the
role of global properties of data statistics will be
9
investigated through the Latent Semantic Analy-
sis while in Section 3.3 the Locality Preserving
Projection algorithm will be discussed in order to
combine prior knowledge about frames with local
regularities of LUs obtained from text.
3.1 Choosing the space
Different types of context define spaces with dif-
ferent semantic properties. Such spaces model a
generic notion of semantic relatedness. Two LUs
close in the space are likely to be related by some
type of generic semantic relation, either paradig-
matic (e.g. synonymy, hyperonymy, antonymy)
or syntagmatic (e.g. meronymy, conceptual and
phrasal association), as observed in (Sahlgren,
2006). The target of this work is the construc-
tion of a space able to capture the properties which
characterize a frame, assuming those LUs in the
same frame tend to be either co-occurring or sub-
stitutional words (e.g. murder/kill). Two tradi-
tional word-based co-occurrence models capture
the above property:
Word-based space: Contexts are words, as
lemmas, appearing in a n-window of the LU.
The window width n is a parameter that allows
the space to capture different aspects of a frame:
higher values risk to introduce noise, since a frame
could not cover an entire sentence, while lower
values lead to sparse representations.
Syntax-based space: Contexts words are en-
riched through information about syntactic rela-
tions (e.g. X-VSubj-killer where X is the LU), as
described in (Pado and Lapata, 2007). Two LUs
close in the space are likely to be in a paradig-
matic relation, i.e. to be close in an IS-A hierarchy
(Budanitsky and Hirst, 2006; Lin, 1998). Indeed,
as contexts are syntactic relations, targets with the
same part of speech are much closer than targets
of different types.
3.2 Latent Semantic Analysis
Latent Semantic Analysis (LSA) is an algorithm
presented in (Furnas et al, 1988) afterwards dif-
fused by Landauer (Landauer and Dumais, 1997):
it can be seen as a variant of the Principal Compo-
nent Analysis idea. LSA aims to find the best sub-
space approximation to the original word space,
in the sense of minimizing the global reconstruc-
tion error projecting data along the directions of
maximal variance. It captures term (semantic)
dependencies by applying a matrix decomposi-
tion process called Singular Value Decomposition
(SVD). The original term-by-term matrix M is
transformed into the product of three new matri-
ces: U , S, and V so that M = USV T . Matrix
M is approximated by Ml = UlSlV Tl in which
only the first l columns of U and V are used, and
only the first l greatest singular values are consid-
ered. This approximation supplies a way to project
term vectors into the l-dimensional space using
Yterms = UlS
1/2
l . Notice that the SVD process
accounts for the eigenvectors of the entire original
distribution (matrix M ). LSA is thus an example
of a decomposition process strongly dependent on
a global property. The original statistical informa-
tion aboutM is captured by the new l-dimensional
space which preserves the global structure while
removing low-variant dimensions, i.e. distribu-
tion noise. These newly derived features may be
thought of as artificial concepts, each one repre-
senting an emerging meaning component as a lin-
ear combination of many different words (i.e. con-
texts). Such contextual usages can be used instead
of the words to represent texts. This technique has
two main advantages. First, the overall computa-
tional cost of the model is reduced, as similarities
are computed on a space with much fewer dimen-
sions. Secondly, it allows to capture second-order
relations among LUs, thus improving the quality
of the similarity measure.
3.3 The Locality Preserving Projection
Method
An alternative to LSA, much tighter to local prop-
erties of data, is the Locality Preserving Projection
(LPP ), a linear approximation of the non-linear
Laplacian Eigenmap algorithm introduced in (He
and Niyogi, 2003). LPP is a linear dimensional-
ity reduction method whose goal is, given a set of
LUs x1, x2, .., xm in Rn, to find a transformation
matrix A that maps these m points into a set of
points y1, y2, .., ym in Rk (k  n). LPP achieves
this result through a cascade of processing steps
described hereafter.
Construction of an Adjacency graph. Let G
denote a graph with m nodes. Nodes i and j have
got a weighted connection if vectors xi and xj are
close, according to an arbitrary measure of simi-
larity. There are many ways to build an adjacency
graph. The cosine graph with cosine weighting
scheme is explored: given two vectors xi and xj ,
the weight wij between them is set by
wij = max{0,
cos(xi, xj)? ?
|cos(xi, xj)? ? |
? cos(xi, xj)} (1)
10
where a cosine threshold ? is necessary. The ad-
jacency graph can be represented by using a sym-
metricm?m adjacency matrix, namedW , whose
element Wij contains the weight between nodes i
and j. The method of constructing an adjacency
graph outlined above is correct if the data actually
lie on a low dimensional manifold. Once such an
adjacency graph is obtained, LPP will try to opti-
mally preserve it in choosing projections.
Solve an Eigenmap problem. Compute the
eigenvectors and eigenvalues for the generalized
eigenvector problem:
XLXT a = ?XDXT a
where X is a n?m matrix whose columns are the
original m vectors in Rn, D is a diagonal m ?m
matrix whose entries are column (or row) sums of
W , Dii =
?
jWij and L = D ?W is the Lapla-
cian matrix. The solution of this problem is the
set of eigenvectors a0, a1, .., an?1, ordered accord-
ing to their eigenvalues ?0 < ?1 < .. < ?n?1.
LPP projection matrix A is obtained by selecting
the k eigenvectors corresponding to the k smallest
eigenvalues: therefore it is a n ? k matrix whose
columns are the selected n-dimensional k eigen-
vectors. Final projection of original vectors into
Rk can be linearly performed by Y = ATX . This
transformation provides a valid kernel that can be
efficiently embedded into a classifier.
Embedding predicate knowledge through
LPPs. While LSA finds a projection, according to
the global properties of the space, LPP tries to pre-
serve the local structures of the data. LPP exploits
the adjacency graph in order to represent neigh-
borhood information. It computes a transforma-
tion matrix which maps data points into a lower di-
mensional subspace. As the construction of an ad-
jacency graph G can be based on any principle, its
definition could account on some external infor-
mation reflecting prior knowledge available about
the task.
In this work, prior knowledge about LUs is em-
bedded by exploiting their membership to frame
dictionaries, thus removing from the graph all con-
nections between LUs xi and xj that do not evoke
the same prototypical situation. More formally
Equation 1 can be rewritten more formally as:
wij = max{0,
cos(xi, xj)? ?
|cos(xi, xj)? ? |
? cos(xi, xj) ? ?(i, j)}
where
?(i, j) =
{
1 iff ?F s.t. LUi ? F ? LUj ? F
0 otherwise
so the resulting manifold keeps close all LUs
evoking the same frame. Since the number of con-
nections could introduce too many constraints to
the Eigenmap problem, a threshold is introduced
to avoid the space collapse: for each LU, only
the most-similar c connections are selected. The
adoption of the proper a priori knowledge about
the target task can be thus seen as a promising re-
search direction.
4 Empirical Analysis
In this section the empirical evaluation of distribu-
tional models applied to the task of inducing LUs
is presented. Different spaces obtained through
the dimensionality reduction techniques imply dif-
ferent kernel functions used to independently train
different SVMs. Our aim is to investigate the im-
pact of these kernels in capturing both the frames
and LUs? properties, as well as the effectiveness
of their possible combination.
The problem of LUs? induction is here treated
as a multi-classification problem, where each LU
is considered as a positive or negative instance of a
frame. We use Support Vector Machines (SVMs),
(Joachims, 1999) a maximum-margin classifier
that realizes a linear discriminative model. In case
of not linearly separable examples, convolution
functions ?(?) can be used in order to transform
the initial feature space into another one, where a
hyperplane that separates the data with the widest
margin can be found. Here new similarity mea-
sures, the kernel functions, can be defined through
the dot-product K(oi, oj) = ??(oi) ? ?(oj)? over
the new representation. In this way, kernel func-
tions KLSA and KLPP can be induced through
the dimensionality reduction techniques ?LSA and
?LPP respectively, as described in sections 3.2
and 3.3. Kernel methods are advantageous be-
cause the combination of of kernel functions can
be integrated into the SVM as they are still kernels.
Consequently, the kernel combination ?KLSA +
?KLPP linearly combines the global properties
captured by LSA and the locality constraints im-
posed by the LPP transformation. Here, parame-
ters ? and ? weight the combination of the two
kernels. The evoking frame for a novel LU is
the one whose corresponding SVM has the high-
est (possibly negative) margin, according to a one-
11
train tune test overall
max 107 35 34 176
avg 28 8 8 44
total 2466 722 723 3911
Table 1: Number of LU examples for each data set
from the 100 frames
vs-all scheme. In order to evaluate the quality of
the presented models, accuracy is measured as the
percentage of LUs that are correctly re-assigned to
their original (gold-standard) frame. As the sys-
tem can suggest more than one frame, different
accuracy levels can be obtained. A LU is cor-
rectly assigned if its correct frame (according to
FrameNet) belongs to the set of the best b pro-
posals by the system (i.e. the first b scores from
the underlying SVMs). Assigning different val-
ues to b, we obtained different levels of accuracy
as the percentage of LUs that is correctly assigned
among the first b proposals, as shown in Table 3.
4.1 Experimental Setup
The adopted gold standard is a subset of the
FrameNet database and it consists of the most 100
represented frames in term of annotated examples
and LUs. As the number of example is extremely
unbalanced across frames1, the LUs dictionary of
each selected frame contains at least 10 LUs. It is
a reasonable amount of information for the SVMs
training and it is still a representative data set, be-
ing composed of 3,911 LUs, i.e. the 55% of the
entire dictionary2 of 7,230 evoking words. All
word spaces are derived from the British National
Corpus (BNC), which is underlying FrameNet and
consisting of about 100 million words for English.
Each selected frame is represented into the BNC
by at least 362 annotated sentences, as the lack
of a reasonable number of examples hardly pro-
duces a good distributional model of LUs. Each
frame?s list of LUs is split into train (60%), tuning
(20%) and test set (20%) and LUs having Part-of-
speech different from verb, noun or adjective are
removed. In Table 1 the number of LUs for each
set, as well as the maximum and the average num-
ber per frame, are summarized.
Four different approaches for the Word Space
1For example the SELF MOTION frame counts 6,248 ex-
amples while 119 frames are represented by less than 10 ex-
amples
2The entire database contains 10,228 LUs and the number
of evoking word is 7,230, without taking in account multiple
frame assignments.
construction are used. The first two correspond to
a Word-Based space, the last to a Syntax-Based,
as described in section 3.1:
Window-n (Wn): contextual features correspond
to the set of the 20,000 most frequent lemmatized
words in the BNC. The association measure be-
tween LUs and contexts is the Point-wise Mu-
tual Information (PMI). Valid contexts for LUs are
fixed to a n-window. Hereafter two window width
values will be investigated: Window5 (W5) and
Window10 (W10).
Sentence (Sent): contextual features are the same
above, but the valid contexts are extended to the
entire sentence length.
SyntaxBased (SyntB): contextual features have
been computed according to the ?dependency-
based? vector space discussed3 in (Pado and La-
pata, 2007). Observable contexts here are made of
syntactically-typed co-occurrences within depen-
dency graphs built from the entire set of BNC sen-
tences. The most frequent 20,000 basic features,
i.e. (syntactic relation,lemma) pairs, have been
employed as contextual features corresponding to
PMI scores. Syntactic relations are extracted using
the Minipar parser.
Word space models thus focus on the LUs of the
selected 100 frames and the dimensionality have
been reduced by applying LSA and LPP at a new
size of l = 100. Any prior knowledge informa-
tion is provided to the tuning and test sets during
the LPP transformation: the construction of the
reduced feature space takes in account only LUs
from the train set while remaining predicates are
represented through the LPP linear projection. In
these experiments the cosine threshold ? and the
maximum number of constraints c are estimated
over the tuning set and the best parametrizations
are shown in Table 2. The adopted implementa-
tion of SVM is SVM-Light-TK 4.
4.2 Results
In these experiments the impact of the lexical
knowledge gathered by different word-spaces is
evaluated over the LU induction task. Moreover,
the improvements achieved through LSA and LPP
is measured. SVM classifiers are trained over the
semantic spaces produced through the dimension-
3The Minimal context provided by the De-
pendency Vectors tool is used. It is available at
http://www.nlpado.de/?sebastian/dv.html
4SVM-Light-TK is available at the url
http://disi.unitn.it/?moschitt/Tree-Kernel.htm
12
?/? ? c
1.0/0.0 .9/.1 .8/.2 .7/.3 .6/.4 .5/.5 .4/.6 .3/.7 .2/.8 .1/.9 0.0/1.0
W5 0.668 0.669 0.672 0.673 0.669 0.662 0.649 0.632 0.612 0.570 0.033 0.55 5
W10 0.615 0.619 0.618 0.612 0.604 0.597 0.580 0.575 0.565 0.528 0.048 0.65 3
Sent 0.557 0.567 0.580 0.584 0.574 0.564 0.561 0.545 0.523 0.496 0.048 0.80 5
SyntB 0.654 0.664 0.662 0.652 0.651 0.647 0.649 0.634 0.627 0.592 0.056 0.40 3
Table 2: Accuracy at different combination weights of kernel ?KLSA + ?KLPP (specific baseline is
0.043)
b-1 b-2 b-3 b-4 b-5 b-6 b-7 b-8 b-9 b-10 ?/?
W5orig 0,563 0,685 0,733 0,770 0,801 0,835 0,841 0,854 0,868 0,879 -
W10orig 0,510 0,634 0,707 0,776 0,810 0,830 0,841 0,857 0,865 0,875 -
Sentorig 0,479 0,618 0,680 0,734 0,764 0,793 0,813 0,837 0,845 0,852 -
SyntBorig 0,585 0,741 0,803 0,840 0,866 0,874 0,886 0,903 0,907 0,913 -
W5LSA+LPP 0.673 0.781 0.831 0.865 0.881 0.891 0.906 0.912 0.926 0.938 0.7/0.3
W10LSA+LPP 0.619 0.739 0.786 0.818 0.849 0.865 0.878 0.888 0.901 0.909 0.9/0.1
SentLSA+LPP 0.584 0.705 0.766 0.798 0.825 0.835 0.848 0.864 0.876 0.889 0.7/0.3
SyntBLSA+LPP 0.664 0.791 0.840 0.864 0.878 0.893 0.901 0.903 0.907 0.911 0.9/0.1
Table 3: Accuracy of original word-space models (orig) and semantic space models (LSA+LPP) on
best-k proposed frames
ality reduction transformations. Representations
of both semantic spaces are linearly combined as
?KLSA + ?KLPP , where kernel weights ? and
? are estimated over the tuning set. Both ker-
nels are used even without a combination: a ra-
tio ? = 1.0/? = 0.0 denotes the LSA kernel
alone, while ? = 0.0/? = 1.0 the LPP kernel. Ta-
ble 2 shows best results, obtained through a RBF
kernel. The Window5 model achieves the high-
est accuracy, i.e. 67% of correct classification,
where a baseline of 4.3% is estimated assigning
LUs to the most likely frame in the training set (i.e.
the one containing the highest number of LUs).
Wider windows achieve lower classification accu-
racy confirming that most of lexical information
tied to a frame is near the LU. The Syntactic-based
word space does not outperform the accuracy of a
word-based space. The combination of both ker-
nels has always provided the best outcome and the
LSA space seems to be more accurate and expres-
sive respect to the LPP one, as shown in Figure
1. In particular LPP alone is extremely unstable,
suggesting that constraints imposed by the prior
knowledge are orthogonal with respect to the cor-
pus statistics.
Further experiments are carried out using the
original co-occurrence space models, to assess im-
provements due to LSA and LPP kernel. In the
latter investigation linear kernel achieved best re-
sults as confirmed in (Bengio et al, 2005), where
the sensitivity to the curse of dimensionality of
a large class of modern learning algorithms (e.g.
0,40 
0,45 
0,50 
0,55 
0,60 
0,65 
0,70 
1.0/0
.0 
0.9/0
.1 
0.8/0
.2 
0.7/0
.3 
0.6/0
.4 
0.5/0
.5 
0.4/0
.6 
0.3/0
.7 
0.2/0
.8 
0.1/0
.9 
?LSA / ?LPP weights 
Window5 
Window10 
Sentence 
SyntaxBased 
Figure 1: Accuracy at different combination
weights of kernel ?KLSA + ?KLPP
SVM) based on local kernels (e.g. RBF) is ar-
gued. As shown in Table 3, the performance drop
of original (orig) models against the best kernel
combination of LSA and LPP are significant,
i.e. ? 10%, showing how the latent semantic
spaces better capture properties of frames, avoid-
ing data-sparseness, dimensionality problem and
low-regularities of data-distribution.
Moreover, Table 3 shows how the accuracy level
largely increases when more than one frame is
considered: at a level b = 3, i.e. the novel
LU is correctly classified if one of the original
frames is comprised in the list (of three frames)
proposed by the system, accuracy is 0.84 (i.e the
SyntaxBased model), while at b = 10 accuracy is
13
LU (# WNsyns) frame 1 frame 2 frame 3 Correct frames
boil.v (5) FOOD FLUIDIC MOTION CONTAINERS CAUSE HARM
clap.v (7) SOUNDS MAKE NOISE COMMUNICATION NOISE BODY MOVEMENT
crown.n (12) LEADERSHIP ACCOUTREMENTS PLACING ACCOUTREMENTS
OBSERVABLE BODYPARTS
school.n (7) EDUCATION TEACHING BUILDINGS LOCALE BY USE
EDUCATION TEACHING
LOCALE BY USE
AGGREGATE
threat.n (4) HOSTILE ENCOUNTER IMPACT COMMITMENT COMMITMENT
tragedy.n (2) TEXT KILLING EMOTION DIRECTED TEXT
Table 4: Proposed 3 frames for each LU (ordered by SVM scores) and correct frames provided by the
FrameNet dictionary. In parenthesis the number of different WordNet lexical senses for each LU.
nearly 0.94 (i.e Window5). It is high enough to
support tasks such as the semi-automatic creation
of new FrameNets. An error analysis indicates that
many misclassifications are induced by a lack in
the frame annotations, especially those concern-
ing polysemic LUs5. Table 4 reports the analysis
of a LU subset where the first 3 frames proposed
for each evoking word are shown, ranked by the
margin of the SMVs. The last column contains the
frames evoked by LUs, according to the FrameNet
dictionary, and the frame names in bold suggest
their correct classification. Some LUs, like threat
(characterized by 4 lexical senses) seem to be mis-
classified: in this case the FrameNet annotation
regards a specific sense that evokes the COMMIT-
MENT frame (e.g. ?There was a real threat that
she might have to resign?) without taking in ac-
count other senses like WordNet?s ?menace, threat
(something that is a source of danger)? that could
evoke the HOSTILE ENCOUNTER frame. In other
cases proposed frames seem to enrich the LUs dic-
tionary, like BUILDINGS, here evoked by school.
5 Conclusions
The core purpose of this was to present an em-
pirical investigation of the impact of different dis-
tributional models on the lexical unit induction
task. The employed word-spaces, based on dif-
ferent co-occurrence models (either context and
syntax-driven), are used as vector models of the
LU semantics. On these spaces, two dimensional-
ity reduction techniques have been applied. Latent
Semantic Analysis (LSA) exploits global proper-
ties of data distributions and results in a global
model for lexical semantics. On the other hand,
the Locality Preserving Projection (LPP) method,
that exploits regularities in the neighborhood of
5According to WordNet, in our dataset an average of 3.6
lexical senses for each LU is estimated.
each lexical predicate, is also employed in a semi-
supervised manner: local constraints expressing
prior knowledge on frames are defined in the ad-
jacency graph. The resulting embedding is there-
fore expected to determine a new space where re-
gions for LU of a given frame can be more eas-
ily discovered. Experiments have been run using
the resulting spaces for task dependent kernels in
a SVM learning setting. The application of the
FrameNet KB on the 100 best represented frames
showed that a combined use of the global and lo-
cal models made available by LSA and LPP, re-
spectively, achieves the best results, as the 67.3%
of LUs recovers the same frames of the annotated
dictionary. This is a significant improvement with
respect to previous results achieved by the pure
distributional model reported in (Pennacchiotti et
al., 2008).
Future work is required to increase the level
of constraints made available from the semi-
supervised setting of LPP: syntactic informa-
tion, as well as role-related evidence, can be
both accommodated by the adjacency constraints
imposed for LPP. This constitutes a significant
area of research towards a comprehensive semi-
supervised model of frame semantics, entirely
based on manifold learning methods, of which this
study on LSA and LPP is just a starting point.
Acknowledgement We want to acknowledge
Prof. Roberto Basili because this work would not
exist without his ideas, inspiration and invaluable
support.
References
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic struc-
ture extraction. In Proceedings of SemEval-2007,
14
pages 99?104, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Sugato Basu, Mikhail Bilenko, Arindam Banerjee,
and Raymond Mooney. 2006. Probabilistic semi-
supervised clustering with constraints. In Semi-
Supervised Learning, pages 73?102. MIT Press.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le
Roux. 2005. The curse of dimensionality for lo-
cal kernel machines. Technical report, Departement
d?Informatique et Recherche Operationnelle.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of semantic dis-
tance. Computational Linguistics, 32(1):13?47.
Aljoscha Burchardt, Katrin Erk, and Anette Frank.
2005. A WordNet Detour to FrameNet. In
Sprachtechnologie, mobile Kommunikation und lin-
guistische Resourcen, volume 8 of Computer Stud-
ies in Language and Speech. Peter Lang, Frank-
furt/Main.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In In Pro-
ceedings of ICML ?08, pages 160?167, New York,
NY, USA. ACM.
Diego De Cao, Danilo Croce, Marco Pennacchiotti,
and Roberto Basili. 2008. Combining word sense
and usage for modeling frame semantics. In In Pro-
ceedings of STEP 2008, Venice, Italy.
Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 4(2):222?
254.
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-
dauer, R. A. Harshman, L. A. Streeter, and K. E.
Lochbaum. 1988. Information retrieval using a sin-
gular value decomposition model of latent semantic
structure. In Proc. of SIGIR ?88, New York, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Yoav Goldberg and Michael Elhadad. 2009. On the
role of lexical features in sequence labeling. In In
Proceedings of EMNLP ?09, pages 1142?1151, Sin-
gapore.
Zellig Harris. 1964. Distributional structure. In Jer-
rold J. Katz and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Xiaofei He and Partha Niyogi. 2003. Locality preserv-
ing projections. In Proceedings of NIPS03, Vancou-
ver, Canada.
T. Joachims. 1999. Making large-Scale SVM Learning
Practical. MIT Press, Cambridge, MA.
Richard Johansson and Pierre Nugues. 2007. Using
WordNet to extend FrameNet coverage. In Proceed-
ings of the Workshop on Building Frame-semantic
Resources for Scandinavian and Baltic Languages,
at NODALIDA, Tartu, Estonia, May 24.
Richard Johansson and Pierre Nugues. 2008. The
effect of syntactic representation on semantic role
labeling. In Proceedings of COLING, Manchester,
UK, August 18-22.
Tom Landauer and Sue Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar word. In Proceedings of COLING-ACL,
Montreal, Canada.
Alessandro Moschitti, Paul Morarescu, and Sanda M.
Harabagiu. 2003. Open domain information ex-
traction via automatic semantic labeling. In FLAIRS
Conference, pages 397?401.
Sebastian Pado and Katrin Erk. 2005. To cause or
not to cause: Cross-lingual semantic matching for
paraphrase modelling. In Proceedings of the Cross-
Language Knowledge Induction Workshop, Cluj-
Napoca, Romania.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of framenet lexical units. In Proceedings
of The Empirical Methods in Natural Language Pro-
cessing (EMNLP 2008) Waikiki, Honolulu, Hawaii.
S.T. Roweis and L.K. Saul. 2000. Nonlinear dimen-
sionality reduction by locally linear embedding. Sci-
ence, 290(5500):2323?2326.
Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D. thesis, Stockholm University.
G. Salton, A. Wong, and C. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, 18:613A??`620.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceed-
ings of EMNLP-CoNLL, pages 12?21, Prague.
Mihai Surdeanu, , Mihai Surdeanu, A Harabagiu, John
Williams, and Paul Aarseth. 2003. Using predicate-
argument structures for information extraction. In
In Proceedings of ACL 2003.
Marta Tatu and Dan I. Moldovan. 2005. A seman-
tic approach to recognizing textual entailment. In
HLT/EMNLP.
15
J. B. Tenenbaum, V. Silva, and J. C. Langford. 2000.
A Global Geometric Framework for Nonlinear Di-
mensionality Reduction. Science, 290(5500):2319?
2323.
Xin Yang, Haoying Fu, Hongyuan Zha, and Jesse Bar-
low. 2006. Semi-supervised nonlinear dimension-
ality reduction. In 23rd International Conference
on Machine learning, pages 1065?1072, New York,
NY, USA. ACM Press.
Zhenyue Zhang and Hongyuan Zha. 2004. Princi-
pal manifolds and nonlinear dimensionality reduc-
tion via tangent space alignment. SIAM J. Scientific
Computing, 26(1):313?338.
16
