Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of NAACL HLT 2007, pages 81?88,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
First-Order Probabilistic Models for Coreference Resolution
Aron Culotta and Michael Wick and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
{culotta,mwick,mccallum}@cs.umass.edu
Abstract
Traditional noun phrase coreference res-
olution systems represent features only
of pairs of noun phrases. In this paper,
we propose a machine learning method
that enables features over sets of noun
phrases, resulting in a first-order proba-
bilistic model for coreference. We out-
line a set of approximations that make this
approach practical, and apply our method
to the ACE coreference dataset, achiev-
ing a 45% error reduction over a com-
parable method that only considers fea-
tures of pairs of noun phrases. This result
demonstrates an example of how a first-
order logic representation can be incorpo-
rated into a probabilistic model and scaled
efficiently.
1 Introduction
Noun phrase coreference resolution is the problem
of clustering noun phrases into anaphoric sets. A
standard machine learning approach is to perform a
set of independent binary classifications of the form
?Is mention a coreferent with mention b??
This approach of decomposing the problem into
pairwise decisions presents at least two related diffi-
culties. First, it is not clear how best to convert the
set of pairwise classifications into a disjoint cluster-
ing of noun phrases. The problem stems from the
transitivity constraints of coreference: If a and b are
coreferent, and b and c are coreferent, then a and c
must be coreferent.
This problem has recently been addressed by a
number of researchers. A simple approach is to per-
form the transitive closure of the pairwise decisions.
However, as shown in recent work (McCallum and
Wellner, 2003; Singla and Domingos, 2005), bet-
ter performance can be obtained by performing rela-
tional inference to directly consider the dependence
among a set of predictions. For example, McCal-
lum and Wellner (2005) apply a graph partitioning
algorithm on a weighted, undirected graph in which
vertices are noun phrases and edges are weighted by
the pairwise score between noun phrases.
A second and less studied difficulty is that the
pairwise decomposition restricts the feature set to
evidence about pairs of noun phrases only. This re-
striction can be detrimental if there exist features of
sets of noun phrases that cannot be captured by a
combination of pairwise features. As a simple exam-
ple, consider prohibiting coreferent sets that consist
only of pronouns. That is, we would like to require
that there be at least one antecedent for a set of pro-
nouns. The pairwise decomposition does not make
it possible to capture this constraint.
In general, we would like to construct arbitrary
features over a cluster of noun phrases using the
full expressivity of first-order logic. Enabling this
sort of flexible representation within a statistical
model has been the subject of a long line of research
on first-order probabilistic models (Gaifman, 1964;
Halpern, 1990; Paskin, 2002; Poole, 2003; Richard-
son and Domingos, 2006).
Conceptually, a first-order probabilistic model
can be described quite compactly. A configura-
tion of the world is represented by a set of predi-
81
He
President Bush 
Laura Bush
She
0.2
0.9
0.7
0.4
0.001
0.6
Figure 1: An example noun coreference graph in
which vertices are noun phrases and edge weights
are proportional to the probability that the two nouns
are coreferent. Partitioning such a graph into disjoint
clusters corresponds to performing coreference res-
olution on the noun phrases.
cates, each of which has an associated real-valued
parameter. The likelihood of each configuration of
the world is proportional to a combination of these
weighted predicates. In practice, however, enu-
merating all possible configurations, or even all the
predicates of one configuration, can result in in-
tractable combinatorial growth (de Salvo Braz et al,
2005; Culotta and McCallum, 2006).
In this paper, we present a practical method to per-
form training and inference in first-order models of
coreference. We empirically validate our approach
on the ACE coreference dataset, showing that the
first-order features can lead to an 45% error reduc-
tion.
2 Pairwise Model
In this section we briefly review the standard pair-
wise coreference model. Given a pair of noun
phrases xij = {xi, xj}, let the binary random vari-
able yij be 1 if xi and xj are coreferent. Let F =
{fk(xij , y)} be a set of features over xij . For exam-
ple, fk(xij , y) may indicate whether xi and xj have
the same gender or number. Each feature fk has an
associated real-valued parameter ?k. The pairwise
model is
p(yij |xij) =
1
Zxij
exp
?
k
?kfk(xij , yij)
where Zxij is a normalizer that sums over the two
settings of yij .
This is a maximum-entropy classifier (i.e. logis-
tic regression) in which p(yij |xij) is the probability
that xi and xj are coreferent. To estimate ? = {?k}
from labeled training data, we perform gradient as-
cent to maximize the log-likelihood of the labeled
data.
Two critical decisions for this method are (1) how
to sample the training data, and (2) how to combine
the pairwise predictions at test time. Systems of-
ten perform better when these decisions complement
each other.
Given a data set in which noun phrases have been
manually clustered, the training data can be cre-
ated by simply enumerating over each pair of noun
phrases xij , where yij is true if xi and xj are in
the same cluster. However, this approach generates
a highly unbalanced training set, with negative ex-
amples outnumbering positive examples. Instead,
Soon et al (2001) propose the following sampling
method: Scan the document from left to right. Com-
pare each noun phrase xi to each preceding noun
phrase xj , scanning from right to left. For each pair
xi, xj , create a training instance ?xij , yij?, where yij
is 1 if xi and xj are coreferent. The scan for xj ter-
minates when a positive example is constructed, or
the beginning of the document is reached. This re-
sults in a training set that has been pruned of distant
noun phrase pairs.
At testing time, we can construct an undirected,
weighted graph in which vertices correspond to
noun phrases and edge weights are proportional to
p(yij |xij). The problem is then to partition the graph
into clusters with high intra-cluster edge weights and
low inter-cluster edge weights. An example of such
a graph is shown in Figure 1.
Any partitioning method is applicable here; how-
ever, perhaps most common for coreference is to
perform greedy clustering guided by the word or-
der of the document to complement the sampling
method described above (Soon et al, 2001). More
precisely, scan the document from left-to-right, as-
signing each noun phrase xi to the same cluster
as the closest preceding noun phrase xj for which
p(yij |xij) > ?, where ? is some classification
threshold (typically 0.5). Note that this method con-
trasts with standard greedy agglomerative cluster-
ing, in which each noun phrase would be assigned
to the most probable cluster according to p(yij |xij).
82
Choosing the closest preceding phrase is common
because nearby phrases are a priori more likely to
be coreferent.
We refer to the training and inference methods de-
scribed in this section as the Pairwise Model.
3 First-Order Logic Model
We propose augmenting the Pairwise Model to
enable classification decisions over sets of noun
phrases.
Given a set of noun phrases xj = {xi}, let the bi-
nary random variable yj be 1 if all the noun phrases
xi ? xj are coreferent. The features fk and weights
?k are defined as before, but now the features can
represent arbitrary attributes over the entire set xj .
This allows us to use the full flexibility of first-order
logic to construct features about sets of nouns. The
First-Order Logic Model is
p(yj |xj) =
1
Zxj
exp
?
k
?kfk(x
j , yj)
where Zxj is a normalizer that sums over the two
settings of yj .
Note that this model gives us the representational
power of recently proposed Markov logic networks
(Richardson and Domingos, 2006); that is, we can
construct arbitrary formulae in first-order logic to
characterize the noun coreference task, and can learn
weights for instantiations of these formulae. How-
ever, naively grounding the corresponding Markov
logic network results in a combinatorial explosion of
variables. Below we outline methods to scale train-
ing and prediction with this representation.
As in the Pairwise Model, we must decide how to
sample training examples and how to combine inde-
pendent classifications at testing time. It is impor-
tant to note that by moving to the First-Order Logic
Model, the number of possible predictions has in-
creased exponentially. In the Pairwise Model, the
number of possible y variables is O(|x|2), where
x is the set of noun phrases. In the First-Order
Logic Model, the number of possible y variables is
O(2|x|): There is a y variable for each possible el-
ement of the powerset of x. Of course, we do not
enumerate this set; rather, we incrementally instan-
tiate y variables as needed during prediction.
A simple method to generate training examples
is to sample positive and negative cluster examples
uniformly at random from the training data. Positive
examples are generated by first sampling a true clus-
ter, then sampling a subset of that cluster. Negative
examples are generated by sampling two positive ex-
amples and merging them into the same cluster.
At testing time, we perform standard greedy ag-
glomerative clustering, where the score for each
merger is proportional to the probability of the
newly formed clustering according to the model.
Clustering terminates when there exists no addi-
tional merge that improves the probability of the
clustering.
We refer to the system described in this section as
First-Order Uniform.
4 Error-driven and Rank-based training
of the First-Order Model
In this section we propose two enhancements to
the training procedure for the First-Order Uniform
model.
First, because each training example consists of
a subset of noun phrases, the number of possible
training examples we can generate is exponential in
the number of noun phrases. We propose an error-
driven sampling method that generates training ex-
amples from errors the model makes on the training
data. The algorithm is as follows: Given initial pa-
rameters ?, perform greedy agglomerative cluster-
ing on training document i until an incorrect cluster
is formed. Update the parameter vector according to
this mistake, then repeat for the next training docu-
ment. This process is repeated for a fixed number of
iterations.
Exactly how to update the parameter vector is ad-
dressed by the second enhancement. We propose
modifying the optimization criterion of training to
perform ranking rather than classification of clus-
ters. Consider a training example cluster with a neg-
ative label, indicating that not all of the noun phrases
it contains are coreferent. A classification training
algorithm will ?penalize? all the features associated
with this cluster, since they correspond to a negative
example. However, because there may exists subsets
of the cluster that are coreferent, features represent-
ing these positive subsets may be unjustly penalized.
To address this problem, we propose constructing
training examples consisting of one negative exam-
83
fc
y
12
x
2
x
1
y
23
x
3
y
13
f
c
f
c
f
t
Figure 2: An example noun coreference factor graph
for the Pairwise Model in which factors fc model the
coreference between two nouns, and ft enforce the
transitivity among related decisions. The number of
y variables increases quadratically in the number of
x variables.
ple and one ?nearby? positive example. In particular,
when agglomerative clustering incorrectly merges
two clusters, we select the resulting cluster as the
negative example, and select as the positive example
a cluster that can be created by merging other exist-
ing clusters.1 We then update the weight vector so
that the positive example is assigned a higher score
than the negative example. This approach allows
the update to only penalize the difference between
the two features of examples, thereby not penaliz-
ing features representing any overlapping coreferent
clusters.
To implement this update, we use MIRA (Mar-
gin Infused Relaxed Algorithm), a relaxed, online
maximum margin training algorithm (Crammer and
Singer, 2003). It updates the parameter vector with
two constraints: (1) the positive example must have
a higher score by a given margin, and (2) the change
to ? should be minimal. This second constraint is
to reduce fluctuations in ?. Let s+(?,xj) be the
unnormalized score for the positive example and
s?(?,xk) be the unnormalized score of the neg-
ative example. Each update solves the following
1Of the possible positive examples, we choose the one with
the highest probability under the current model to guard against
large fluctuations in parameter updates
f
c
y
12
x
2
x
1
y
23
x
3
y
13
f
c
f
c
f
t
y
123
f
c
Figure 3: An example noun coreference factor graph
for the First-Order Model in which factors fc model
the coreference between sets of nouns, and ft en-
force the transitivity among related decisions. Here,
the additional node y123 indicates whether nouns
{x1, x2, x3} are all coreferent. The number of y
variables increases exponentially in the number of
x variables.
quadratic program:
?t+1 = argmin
?
||?t ? ?||2
s.t.
s+(?,xj) ? s?(?,xk) ? 1
In this case, MIRA with a single constraint can be
efficiently solved in one iteration of the Hildreth and
D?Esopo method (Censor and Zenios, 1997). Ad-
ditionally, we average the parameters calculated at
each iteration to improve convergence.
We refer to the system described in this section as
First-Order MIRA.
5 Probabilistic Interpretation
In this section, we describe the Pairwise and First-
Order models in terms of the factor graphs they ap-
proximate.
For the Pairwise Model, a corresponding undi-
rected graphical model can be defined as
P (y|x) =
1
Zx
?
yij?y
fc(yij , xij)
?
yij ,yjk?y
ft(yij , yj,k, yik, xij , xjk, xik)
84
where Zx is the input-dependent normalizer and fac-
tor fc parameterizes the pairwise noun phrase com-
patibility as fc(yij , xij) = exp(
?
k ?kfk(yij , xij)).
Factor ft enforces the transitivity constraints by
ft(?) = ?? if transitivity is not satisfied, 1 oth-
erwise. This is similar to the model presented in
McCallum and Wellner (2005). A factor graph for
the Pairwise Model is presented in Figure 2 for three
noun phrases.
For the First-Order model, an undirected graphi-
cal model can be defined as
P (y|x) =
1
Zx
?
yj?y
fc(yj ,xj)
?
yj?y
ft(yj ,xj)
where Zx is the input-dependent nor-
malizer and factor fc parameterizes the
cluster-wise noun phrase compatibility as
fc(yj ,xj) = exp(
?
k ?kfk(yj , x
j)). Again,
factor ft enforces the transitivity constraints by
ft(?) = ?? if transitivity is not satisfied, 1 other-
wise. Here, transitivity is a bit more complicated,
since it also requires that if yj = 1, then for any
subset xk ? xj , yk = 1. A factor graph for the
First-Order Model is presented in Figure 3 for three
noun phrases.
The methods described in Sections 2, 3 and 4 can
be viewed as estimating the parameters of each fac-
tor fc independently. This approach can therefore
be viewed as a type of piecewise approximation of
exact parameter estimation in these models (Sutton
and McCallum, 2005). Here, each fc is a ?piece?
of the model trained independently. These pieces
are combined at prediction time using clustering al-
gorithms to enforce transitivity. Sutton and McCal-
lum (2005) show that such a piecewise approxima-
tion can be theoretically justified as minimizing an
upper bound of the exact loss function.
6 Experiments
6.1 Data
We apply our approach to the noun coreference ACE
2004 data, containing 443 news documents with
28,135 noun phrases to be coreferenced. 336 doc-
uments are used for training, and the remainder for
testing. All entity types are candidates for corefer-
ence (pronouns, named entities, and nominal enti-
ties). We use the true entity segmentation, and parse
each sentence in the corpus using a phrase-structure
grammar, as is common for this task.
6.2 Features
We follow Soon et al (2001) and Ng and Cardie
(2002) to generate most of our features for the Pair-
wise Model. These include:
? Match features - Check whether gender, num-
ber, head text, or entire phrase matches
? Mention type (pronoun, name, nominal)
? Aliases - Heuristically decide if one noun is the
acronym of the other
? Apposition - Heuristically decide if one noun is
in apposition to the other
? Relative Pronoun - Heuristically decide if one
noun is a relative pronoun referring to the other.
? Wordnet features - Use Wordnet to decide if
one noun is a hypernym, synonym, or antonym
of another, or if they share a hypernym.
? Both speak - True if both contain an adjacent
context word that is a synonym of ?said.? This
is a domain-specific feature that helps for many
newswire articles.
? Modifiers Match - for example, in the phrase
?President Clinton?, ?President? is a modifier
of ?Clinton?. This feature indicates if one noun
is a modifier of the other, or they share a modi-
fier.
? Substring - True if one noun is a substring of
the other (e.g. ?Egypt? and ?Egyptian?).
The First-OrderModel includes the following fea-
tures:
? Enumerate each pair of noun phrases and com-
pute the features listed above. All-X is true if
all pairs share a featureX ,Most-True-X is true
if the majority of pairs share a feature X , and
Most-False-X is true if most of the pairs do not
share feature X .
85
? Use the output of the Pairwise Model for each
pair of nouns. All-True is true if all pairs are
predicted to be coreferent, Most-True is true if
most pairs are predicted to be coreferent, and
Most-False is true if most pairs are predicted
to not be coreferent. Additionally, Max-True
is true if the maximum pairwise score is above
threshold, and Min-True if the minimum pair-
wise score is above threshold.
? Cluster Size indicates the size of the cluster.
? Count how many phrases in the cluster are
of each mention type (name, pronoun, nom-
inal), number (singular/plural) and gender
(male/female). The features All-X and Most-
True-X indicate how frequent each feature is
in the cluster. This feature can capture the soft
constraint such that no cluster consists only of
pronouns.
In addition to the listed features, we also include
conjunctions of size 2, for example ?Genders match
AND numbers match?.
6.3 Evaluation
We use the B3 algorithm to evaluate the predicted
coreferent clusters (Amit and Baldwin, 1998). B3
is common in coreference evaluation and is similar
to the precision and recall of coreferent links, ex-
cept that systems are rewarded for singleton clus-
ters. For each noun phrase xi, let ci be the number
of mentions in xi?s predicted cluster that are in fact
coreferent with xi (including xi itself). Precision for
xi is defined as ci divided by the number of noun
phrases in xi?s cluster. Recall for xi is defined as
the ci divided by the number of mentions in the gold
standard cluster for xi. F1 is the harmonic mean of
recall and precision.
6.4 Results
In addition to Pairwise, First-Order Uniform, and
First-Order MIRA, we also compare against Pair-
wise MIRA, which differs from First-Order MIRA
only by the fact that it is restricted to pairwise fea-
tures.
Table 1 suggests both that first-order features and
error-driven training can greatly improve perfor-
mance. The First-OrderModel outperforms the Pair-
F1 Prec Rec
First-Order MIRA 79.3 86.7 73.2
Pairwise MIRA 72.5 92.0 59.8
First-Order Uniform 69.2 79.0 61.5
Pairwise 62.4 62.5 62.3
Table 1: B3 results for ACE noun phrase corefer-
ence. FIRST-ORDER MIRA is our proposed model
that takes advantage of first-order features of the
data and is trained with error-driven and rank-based
methods. We see that both the first-order features
and the training enhancements improve performance
consistently.
wise Model in F1 measure for both standard train-
ing and error-driven training. We attribute some of
this improvement to the capability of the First-Order
model to capture features of entire clusters that may
indicate some phrases are not coreferent. Also, we
attribute the gains from error-driven training to the
fact that training examples are generated based on
errors made on the training data. (However, we
should note that there are also small differences in
the feature sets used for error-driven and standard
training results.)
Error analysis indicates that often noun xi is cor-
rectly not merged with a cluster xj when xj has a
strong internal coherence. For example, if all 5 men-
tions of France in a document are string identical,
then the system will be extremely cautious of merg-
ing a noun that is not equivalent to France into xj ,
since this will turn off the ?All-String-Match? fea-
ture for cluster xj .
To our knowledge, the best results on this dataset
were obtained by the meta-classification scheme of
Ng (2005). Although our train-test splits may differ
slightly, the best B-Cubed F1 score reported in Ng
(2005) is 69.3%, which is considerably lower than
the 79.3% obtained with our method. Also note that
the Pairwise baseline obtains results similar to those
in Ng and Cardie (2002).
7 Related Work
There has been a recent interest in training methods
that enable the use of first-order features (Paskin,
2002; Daume? III and Marcu, 2005b; Richardson
and Domingos, 2006). Perhaps the most related is
86
?learning as search optimization? (LASO) (Daume?
III and Marcu, 2005b; Daume? III and Marcu,
2005a). Like the current paper, LASO is also an
error-driven training method that integrates predic-
tion and training. However, whereas we explic-
itly use a ranking-based loss function, LASO uses
a binary classification loss function that labels each
candidate structure as correct or incorrect. Thus,
each LASO training example contains all candidate
predictions, whereas our training examples contain
only the highest scoring incorrect prediction and the
highest scoring correct prediction. Our experiments
show the advantages of this ranking-based loss func-
tion. Additionally, we provide an empirical study to
quantify the effects of different example generation
and loss function decisions.
Collins and Roark (2004) present an incremental
perceptron algorithm for parsing that uses ?early up-
date? to update the parameters when an error is en-
countered. Our method uses a similar ?early update?
in that training examples are only generated for the
first mistake made during prediction. However, they
do not investigate rank-based loss functions.
Others have attempted to train global scoring
functions using Gibbs sampling (Finkel et al, 2005),
message propagation, (Bunescu and Mooney, 2004;
Sutton and McCallum, 2004), and integer linear pro-
gramming (Roth and Yih, 2004). The main distinc-
tions of our approach are that it is simple to imple-
ment, not computationally intensive, and adaptable
to arbitrary loss functions.
There have been a number of machine learning
approaches to coreference resolution, traditionally
factored into classification decisions over pairs of
nouns (Soon et al, 2001; Ng and Cardie, 2002).
Nicolae and Nicolae (2006) combine pairwise clas-
sification with graph-cut algorithms. Luo et al
(2004) do enable features between mention-cluster
pairs, but do not perform the error-driven and rank-
ing enhancements proposed in our work. Denis and
Baldridge (2007) use a ranking loss function for pro-
noun coreference; however the examples are still
pairs of pronouns, and the example generation is not
error driven. Ng (2005) learns a meta-classifier to
choose the best prediction from the output of sev-
eral coreference systems. While in theory a meta-
classifier can flexibly represent features, they do not
explore features using the full flexibility of first-
order logic. Also, their method is neither error-
driven nor rank-based.
McCallum and Wellner (2003) use a conditional
random field that factors into a product of pairwise
decisions about pairs of nouns. These pairwise de-
cisions are made collectively using relational infer-
ence; however, as pointed out in Milch et al (2004),
this model has limited representational power since
it does not capture features of entities, only of pairs
of mention. Milch et al (2005) address these issues
by constructing a generative probabilistic model,
where noun clusters are sampled from a generative
process. Our current work has similar representa-
tional flexibility as Milch et al (2005) but is discrim-
inatively trained.
8 Conclusions and Future Work
We have presented learning and inference proce-
dures for coreference models using first-order fea-
tures. By relying on sampling methods at training
time and approximate inference methods at testing
time, this approach can be made scalable. This re-
sults in a coreference model that can capture features
over sets of noun phrases, rather than simply pairs of
noun phrases.
This is an example of a model with extremely
flexible representational power, but for which exact
inference is intractable. The simple approximations
we have described here have enabled this more flex-
ible model to outperform a model that is simplified
for tractability.
A short-term extension would be to consider fea-
tures over entire clusterings, such as the number of
clusters. This could be incorporated in a ranking
scheme, as in Ng (2005).
Future work will extend our approach to a wider
variety of tasks. The model we have described here
is specific to clustering tasks; however a similar for-
mulation could be used to approach a number of lan-
guage processing tasks, such as parsing and relation
extraction. These tasks could benefit from first-order
features, and the present work can guide the approx-
imations required in those domains.
Additionally, we are investigating more sophis-
ticated inference algorithms that will reduce the
greediness of the search procedures described here.
87
Acknowledgments
We thank Robert Hall for helpful contributions. This work
was supported in part by the Defense Advanced Research
Projects Agency (DARPA), through the Department of the
Interior, NBC, Acquisition Services Division, under con-
tract #NBCHD030010, in part by U.S. Government contract
#NBCH040171 through a subcontract with BBNT Solutions
LLC, in part by The Central Intelligence Agency, the National
Security Agency and National Science Foundation under NSF
grant #IIS-0326249, in part by Microsoft Live Labs, and in part
by the Defense Advanced Research Projects Agency (DARPA)
under contract #HR0011-06-C-0023. Any opinions, findings
and conclusions or recommendations expressed in this mate-
rial are the author(s)? and do not necessarily reflect those of the
sponsor.
References
B. Amit and B. Baldwin. 1998. Algorithms for scoring coref-
erence chains. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC7).
Razvan Bunescu and Raymond J. Mooney. 2004. Collective
information extraction with relational markov networks. In
ACL.
Y. Censor and S.A. Zenios. 1997. Parallel optimization : the-
ory, algorithms, and applications. Oxford University Press.
Michael Collins and Brian Roark. 2004. Incremental parsing
with the perceptron algorithm. In ACL.
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. JMLR, 3:951?
991.
Aron Culotta and Andrew McCallum. 2006. Tractable learn-
ing and inference with high-order representations. In ICML
Workshop on Open Problems in Statistical Relational Learn-
ing, Pittsburgh, PA.
Hal Daume? III and Daniel Marcu. 2005a. A large-scale explo-
ration of effective global features for a joint entity detection
and tracking model. In HLT/EMNLP, Vancouver, Canada.
Hal Daume? III and Daniel Marcu. 2005b. Learning as search
optimization: Approximate large margin methods for struc-
tured prediction. In ICML, Bonn, Germany.
Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth. 2005. Lifted
first-order probabilistic inference. In IJCAI, pages 1319?
1325.
Pascal Denis and Jason Baldridge. 2007. A ranking approach
to pronoun resolution. In IJCAI.
Jenny Rose Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating non-local information into information
extraction systems by gibbs sampling. In ACL, pages 363?
370.
H. Gaifman. 1964. Concerning measures in first order calculi.
Israel J. Math, 2:1?18.
J. Y. Halpern. 1990. An analysis of first-order logics of proba-
bility. Artificial Intelligence, 46:311?350.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kamb-
hatla, and Salim Roukos. 2004. A mention-synchronous
coreference resolution algorithm based on the Bell tree. In
ACL, page 135.
A. McCallum and B. Wellner. 2003. Toward conditional mod-
els of identity uncertainty with application to proper noun
coreference. In IJCAI Workshop on Information Integration
on the Web.
Andrew McCallum and Ben Wellner. 2005. Conditional mod-
els of identity uncertainty with application to noun corefer-
ence. In Lawrence K. Saul, Yair Weiss, and Le?on Bottou,
editors, NIPS17. MIT Press, Cambridge, MA.
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2004.
BLOG: Relational modeling with unknown objects. In
ICML 2004 Workshop on Statistical Relational Learning and
Its Connections to Other Fields.
Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag,
Daniel L. Ong, and Andrey Kolobov. 2005. BLOG: Proba-
bilistic models with unknown objects. In IJCAI.
Vincent Ng and Claire Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In ACL.
Vincent Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In ACL.
Cristina Nicolae and Gabriel Nicolae. 2006. Bestcut: A graph
algorithm for coreference resolution. In EMNLP, pages
275?283, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Mark A. Paskin. 2002. Maximum entropy probabilistic logic.
Technical Report UCB/CSD-01-1161, University of Califor-
nia, Berkeley.
D. Poole. 2003. First-order probabilistic inference. In IJCAI,
pages 985?991, Acapulco, Mexico. Morgan Kaufman.
Matthew Richardson and Pedro Domingos. 2006. Markov
logic networks. Machine Learning, 62:107?136.
D. Roth and W. Yih. 2004. A linear programming formulation
for global inference in natural language tasks. In The 8th
Conference on Compuational Natural Language Learning,
May.
Parag Singla and Pedro Domingos. 2005. Discriminative train-
ing of markov logic networks. In AAAI, Pittsburgh, PA.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim.
2001. A machine learning approach to coreference resolu-
tion of noun phrases. Comput. Linguist., 27(4):521?544.
Charles Sutton and Andrew McCallum. 2004. Collective seg-
mentation and labeling of distant entities in information ex-
traction. Technical Report TR # 04-49, University of Mas-
sachusetts, July.
Charles Sutton and Andrew McCallum. 2005. Piecewise train-
ing of undirected models. In 21st Conference on Uncertainty
in Artificial Intelligence.
88
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 603?611,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Field Compatibilities
to Extract Database Records from Unstructured Text
Michael Wick, Aron Culotta and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
{mwick, culotta, mccallum}@cs.umass.edu
Abstract
Named-entity recognition systems extract
entities such as people, organizations, and
locations from unstructured text. Rather
than extract these mentions in isolation,
this paper presents a record extraction sys-
tem that assembles mentions into records
(i.e. database tuples). We construct a
probabilistic model of the compatibility
between field values, then employ graph
partitioning algorithms to cluster fields
into cohesive records. We also investigate
compatibility functions over sets of fields,
rather than simply pairs of fields, to ex-
amine how higher representational power
can impact performance. We apply our
techniques to the task of extracting contact
records from faculty and student home-
pages, demonstrating a 53% error reduc-
tion over baseline approaches.
1 Introduction
Information extraction (IE) algorithms populate a
database with facts discovered from unstructured
text. This database is often used by higher-level
tasks such as question answering or knowledge
discovery. The richer the structure of the database,
the more useful it is to higher-level tasks.
A common IE task is named-entity recognition
(NER), the problem of locating mentions of en-
tities in text, such as people, places, and organi-
zations. NER techniques range from regular ex-
pressions to finite-state sequence models (Bikel et
al., 1999; Grishman, 1997; Sutton and McCallum,
2006). NER can be viewed as method of populat-
ing a database with single-tuple records, e.g. PER-
SON=Cecil Conner or ORGANIZATION= IBM.
We can add richer structure to these single-tuple
records by extracting the associations among en-
tities. For example, we can populate multi-field
records such as a contact record [PERSON=Steve
Jobs, JOBTITLE = CEO, COMPANY = Apple,
CITY = Cupertino, STATE = CA]. The relational
information in these types of records presents a
greater opportunity for text analysis.
The task of associating together entities is of-
ten framed as a binary relation extraction task:
Given a pair of entities, label the relation be-
tween them (e.g. Steve Jobs LOCATED-IN Cuper-
tino). Common approaches to relation extraction
include pattern matching (Brin, 1998; Agichtein
and Gravano, 2000) and classification (Zelenko et
al., 2003; Kambhatla, 2004).
However, binary relation extraction alone is not
well-suited for the contact record example above,
which requires associating together many fields
into one record. We refer to this task of piecing
together many fields into a single record as record
extraction.
Consider the task of extracting contact records
from personal homepages. An NER system may
label all mentions of cities, people, organizations,
phone numbers, job titles, etc. on a page, from
both semi-structured an unstructured text. Even
with a highly accurate NER system, it is not obvi-
ous which fields belong to the same record. For
example, a single document could contain five
names, three phone numbers and only one email.
Additionally, the layout of certain fields may be
convoluted or vary across documents.
Intuitively, we would like to learn the compat-
ibility among fields, for example the likelihood
that the organization University of North Dakota
is located in the state North Dakota, or that phone
numbers with area code 212 co-occur with the
603
city New York. Additionally, the system should
take into account page layout information, so that
nearby fields are more likely to be grouped into the
same record.
In this paper, we describe a method to induce a
probabilistic compatibility function between sets
of fields. Embedding this compatibility func-
tion within a graph partitioning method, we de-
scribe how to cluster highly compatible fields into
records.
We evaluate our approach on personal home-
pages that have been manually annotated with
contact record information, and demonstrate a
53% error reduction over baseline methods.
2 Related Work
McDonald et al (2005) present clustering tech-
niques to extract complex relations, i.e. relations
with more than two arguments. Record extraction
can be viewed as an instance of complex relation
extraction. We build upon this work in three ways:
(1) Our system learns the compatibility between
sets of fields, rather than just pairs of field; (2) our
system is not restricted to relations between en-
tities in the same sentence; and (3) our problem
domain has a varying number of fields per record,
as opposed to the fixed schema in McDonald et al
(2005).
Bansal et al (2004) present algorithms for the
related task of correlational clustering: finding an
optimal clustering from a matrix of pairwise com-
patibility scores. The correlational clustering ap-
proach does not handle compatibility scores calcu-
lated over sets of nodes, which we address in this
paper.
McCallum and Wellner (2005) discriminatively
train a model to learn binary coreference deci-
sions, then perform joint inference using graph
partitioning. This is analogous to our work, with
two distinctions. First, instead of binary coref-
erence decisions, our model makes binary com-
patibility decisions, reflecting whether a set of
fields belong together in the same record. Second,
whereas McCallum and Wellner (2005) factor the
coreference decisions into pairs of vertices, our
compatibility decisions are made between sets of
vertices. As we show in our experiments, factoring
decisions into sets of vertices enables more power-
ful features that can improve performance. These
higher-order features have also recently been in-
vestigated in other models of coreference, both
discriminative (Culotta and McCallum, 2006) and
generative (Milch et al, 2005).
Viola and Narasimhan (2005) present a prob-
abilistic grammar to parse contact information
blocks. While this model is capable of learn-
ing long-distance compatibilities (such as City and
State relations), features to enable this are not ex-
plored. Additionally, their work focuses on la-
beling fields in documents that have been pre-
segmented into records. This record segmentation
is precisely what we address in this paper.
Borkar et al (2001) and Kristjannson et al
(2004) also label contact address blocks, but ig-
nore the problem of clustering fields into records.
Also, Culotta et al (2004) automatically extract
contact records from web pages, but use heuristics
to cluster fields into records.
Embley et al (1999) provide heuristics to de-
tect record boundaries in highly structured web
documents, such as classified ads, and Embley
and Xu (2000) improve upon these heuristics for
slightly more ambiguous domains using a vector
space model. Both of these techniques apply to
data for which the records are highly contiguous
and have a distinctive separator between records.
These heuristic approaches are unlikely to be suc-
cessful in the unstructured text domain we address
in this paper.
Most other work on relation extraction focuses
only on binary relations (Zelenko et al, 2003;
Miller et al, 2000; Agichtein and Gravano, 2000;
Culotta and Sorensen, 2004). A serious difficulty
in applying binary relation extractors to the record
extraction task is that rather than enumerating over
all pairs of entities, the system must enumerate
over all subsets of entities, up to subsets of size
k, the maximum number of fields per record. We
address this difficulty by employing two sampling
methods: one that samples uniformly, and another
that samples on a focused subset of the combina-
torial space.
3 From Fields to Records
3.1 Problem Definition
Let a fieldF be a pair ?a, v?, where a is an attribute
(column label) and v is a value, e.g. Fi = ?CITY,
San Francisco?. Let record R be a set of fields,
R = {F1 . . . Fn}. Note that R may contain mul-
tiple fields with the same attribute but different
values (e.g. a person may have multiple job ti-
tles). Assume we are given the output of a named-
604
entity recognizer, which labels tokens in a doc-
ument with their attribute type (e.g. NAME or
CITY). Thus, a document initially contains a set
of fields, {F1 . . . Fm}.
The task is to partition the fields in each anno-
tated document into a set of records {R1 . . . Rk}
such that each record Ri contains exactly the set
of fields pertinent to that record. In this paper, we
assume each field belongs to exactly one record.
3.2 Solution Overview
For each document, we construct a fully-
connected weighted graph G = (V,E), with ver-
tices V and weighted edges E. Each field in the
document is represented by a vertex in V , and the
edges are weighted by the compatibility of adja-
cent fields, i.e. a measure of how likely it is that
Fi and Fj belong to the same record.
Partitioning V into k disjoint clusters uniquely
maps the set of fields to a set of k records. Be-
low, we provide more detail on the two principal
steps in our solution: (1) estimating the compati-
bility function and (2) partitioning V into disjoint
clusters.
3.3 Learning field compatibility
Let F be a candidate cluster of fields forming a
partial record. We construct a compatibility func-
tion C that maps two sets of fields to a real value,
i.e. C : Fi ? Fj ? R. We abbreviate the value
C(Fi,Fj) as Cij . The higher the value of Cij the
more likely it is that Fi and Fj belong to the same
record.
For example, in the contact record domain, Cij
can reflect whether a city and state should co-
occur, or how likely a company is to have a certain
job title.
We represent Cij by a maximum-entropy clas-
sifier over the binary variable Sij , which is true if
and only if field set Fi belongs to the same record
as field set Fj . Thus, we model the conditional
distribution
P?(Sij |Fi,Fj) ? exp
(
?
k
?kfk(Sij ,Fi,Fj)
)
where fk is a binary feature function that com-
putes attributes over the field sets, and ? = {?k}
is the set of real-valued weights that are the pa-
rameters of the maximum-entropy model. We set
Cij = P?(Sij =true|Fi,Fj). This approach can
be viewed as a logistic regression model for field
compatibility.
Examples of feature functions include format-
ting evidence (Fi appears at the top of the docu-
ment, Fj at the bottom), conflicting value infor-
mation (Fi and Fj contain conflicting values for
the state field), or other measures of compatibility
(a city value in Fi is known to exist in a state in
Fj). A feature may involve more than one field,
for example, if a name, title and university occurs
consecutively in some order. We give a more de-
tailed description of the feature functions in Sec-
tion 4.3.
We propose learning the ? weights for each of
these features using supervised machine learning.
Given a set of documents D for which the true
mapping from fields to set of records is known,
we wish to estimate P (Sij |Fi,Fj) for all pairs of
field sets Fi,Fj .
Enumerating all positive and negative pairs of
field sets is computationally infeasible for large
datasets, so we instead propose two sampling
methods to generate training examples. The first
simply samples pairs of field sets uniformly from
the training data. For example, given a document
D containing true records {R1 . . . Rk}, we sam-
ple positive and negative examples of field sets of
varying sizes from {Ri . . . Rj}. The second sam-
pling method first trains the model using the exam-
ples generated by uniform sampling. This model
is then used to cluster the training data. Additional
training examples are created during the clustering
process and are used to retrain the model parame-
ters. This second sampling method is an attempt to
more closely align the characteristics of the train-
ing and testing examples.
Given a sample of labeled training data, we set
the parameters of the maximum-entropy classi-
fier in standard maximum-likelihood fashion, per-
forming gradient ascent on the log-likelihood of
the training data. The resulting weights indi-
cate how important each feature is in determin-
ing whether two sets of fields belong to the same
record.
3.4 Partitioning Fields into Records
One could employ the estimated classifier to con-
vert fields into records as follows: Classify each
pair of fields as positive or negative, and perform
transitive closure to enforce transitivity of deci-
sions. That is, if the classifier determines that A
and B belong to the same record and that B and
C belong to the same record, then by transitivity
605
A and C must belong to the same record. The
drawback of this approach is that the compatibility
between A and C is ignored. In cases where the
classifier determines that A and C are highly in-
compatible, transitive closure can lead to poor pre-
cision. McCallum and Wellner (2005) explore this
issue in depth for the related task of noun corefer-
ence resolution.
With this in mind, we choose to avoid transitive
closure, and instead employ a graph partitioning
method to make record merging decisions jointly.
Given a document D with fields {F1 . . . Fn},
we construct a fully connected graph G = (V,E),
with edge weights determined by the learned com-
patibility functionC. We wish to partition vertices
V into clusters with high intra-cluster compatibil-
ity.
One approach is to simply use greedy agglom-
erative clustering: initialize each vertex to its own
cluster, then iteratively merge clusters with the
highest inter-cluster edge weights. The compati-
bility between two clusters can be measured using
single-link or average-link clustering. The clus-
tering algorithm converges when the inter-cluster
edge weight between any pair of clusters is below
a specified threshold.
We propose a modification to this approach.
Since the compatibility function we have de-
scribed maps two sets of vertices to a real value,
we can use this directly to calculate the compati-
bility between two clusters, rather than performing
average or single link clustering.
We now describe the algorithmmore concretely.
? Input: (1) Graph G = (V,E), where each
vertex vi represents a field Fi. (2) A threshold
value ? .
? Initialization: Place each vertex vi in its own
cluster R?i. (The hat notation indicates that
this cluster represents a possible record.)
? Iterate: Re-calculate the compatibility func-
tion Cij between each pair of clusters. Merge
the two most compatible clusters, R??i , R?
?
j .
? Termination: If there does not exist a pair of
clusters R?i, R?j such that Cij > ? , the algo-
rithm terminates and returns the current set of
clusters.
A natural threshold value is ? = 0.5, since this
is the point at which the binary compatibility clas-
sifier predicts that the fields belong to different
records. In Section 4.4, we examine how perfor-
mance varies with ? .
3.5 Representational power of cluster
compatibility functions
Most previous work on inducing compatibility
functions learns the compatibility between pairs of
vertices, not clusters of vertices. In this section,
we provide intuition to explain why directly mod-
eling the compatibility of clusters of vertices may
be advantageous. We refer to the cluster compat-
ibility function as Cij , and the pairwise (binary)
compatibility function as Bij .
First, we note that Cij is a generalization of
single-link and average-link clustering methods
that use Bij , since the output of these methods
can simply be included as features in Cij . For ex-
ample, given two clusters R?i = {v1, v2, v3} and
R?j = {v4, v5, v6}, average-link clustering calcu-
lates the inter-cluster score between R?i and R?j as
SAL(R?i, R?j) =
1
|R?i||R?j |
?
a?R?i,b?R?j
Bab
SAL(R?i, R?j) can be included as a feature for
the compatibility function Cij , with an associated
weight estimated from training data.
Second, there may exist phenomena of the data
that can only be captured by a classifier that con-
siders ?higher-order? features. Below we describe
two such cases.
In the first example, consider three vertices of
mild compatibility, as in Figure 1(a). (For these
examples, let Bij , Cij ? [0, 1].) Suppose that
these three phone numbers occur nearby in a doc-
ument. Since it is not uncommon for a person to
have two phone numbers with different area codes,
the pairwise compatibility function may score any
pair of nearby phone numbers as relatively com-
patible. However, since it is fairly uncommon for
a person to have three phone numbers with three
different area codes, we would not like all three
numbers to be merged into the same record.
Assume an average-link clustering algorithm.
After merging together the 333 and 444 numbers,
Bij will recompute the new inter-cluster compat-
ibility as 0.51, the average of the inter-cluster
edges. In contrast, the cluster compatibility func-
tion Cij can represent the fact that three numbers
with different area codes are to be merged, and can
penalize their compatibility accordingly. Thus, in
606
333-555-5555
666-555-5555444-555-5555
.6
.49
.53
333-555-5555
666-555-5555
444-555-5555
.6
C = 0.1
  B = 0.51
(a)
Univ of North 
Dakota, 
Pleasantville
Pleasantville
North 
Dakota
.48
.49
.9
Univ of North 
Dakota, 
Pleasantville
Pleasantville
North 
Dakota
C = 0.8
    B = 0.485
.9
(b)
Figure 1: Two motivating examples illustrating why the cluster compatibility measure (C) may have
higher representational power than the pairwise compatibility measure (B). In (a), the pairwise measure
over-estimates the inter-cluster compatibility when there exist higher-order features such as A person
is unlikely to have phone numbers with three different area codes. In (b), the pairwise measure under-
estimates inter-cluster compatibility when weak features like string comparisons can be combined into a
more powerful feature by examining multiple field values.
this example, the pairwise compatibility function
over-estimates the true compatibility.
In the second example (Figure 1(b)), we con-
sider the opposite case. Consider three edges,
two of which have weak compatibility, and one of
which has high compatibility. For example, per-
haps the system has access to a list of city-state
pairs, and can reliably conclude that Pleasantville
is a city in the state North Dakota.
Deciding that Univ of North Dakota, Pleas-
antville belongs in the same record as North
Dakota and Pleasantville is a bit more difficult.
Suppose a feature function measures the string
similarity between the city field Pleasantville and
the company field Univ of North Dakota, Pleas-
antville. Alone, this string similarity might not
be very strong, and so the pairwise compatibil-
ity is low. However, after Pleasantville and North
Dakota are merged together, the cluster compat-
ibility function can compute the string similarity
of the concatenation of the city and state fields,
resulting in a higher compatibility. In this ex-
ample, the pairwise compatibility function under-
estimates the true compatibility.
These two examples show that the cluster com-
patibility score can have more representational
power than the average of pairwise compatibility
scores.
FirstName MiddleName
LastName NickName
Suffix Title
JobTitle CompanyName
Department AddressLine
City1 City2
State Country
PostalCode HomePhone
Fax CompanyPhone
DirectCompanyPhone Mobile
Pager VoiceMail
URL Email
InstantMessage
Table 1: The 25 fields annotated in the contact
record dataset.
4 Experiments
4.1 Data
We hand-labeled a subset of faculty and student
homepages from the WebKB dataset1. Each page
was labeled with the 25 fields listed in Table 1.
In addition, we labeled the records to which each
field belonged. For example, in Figure 2, we la-
beled the contact information for Professor Smith
into a separate record from that of her administra-
tive assistant. There are 252 labeled pages in total,
containing 8996 fields and 16679 word tokens. We
perform ten random samples of 70-30 splits of the
data for all experiments.
4.2 Systems
We evaluate five different record extraction sys-
tems. With the exception of Transitive Closure,
all methods employ the agglomerative clustering
1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-
20/www/data/
607
Professor Jane Smith
Somesuch University
555-555-5555
Professor Smith is the Director of the Knowledge Lab ...
Mr. John Doe
Administrative Assistant
555-367-7777
Record 1
Record 2
Figure 2: A synthetic example representative of the labeled data. Note that Record 1 contains information
both from an address block and from free text, and that Record 2 must be separated from Record 1 even
though fields from each may be nearby in the text.
algorithm described previously. The difference is
in how the inter-cluster compatibility is calculated.
? Transitive Closure: The method described
in the beginning of Section 3.4, where hard
classification decisions are made, and transi-
tivity is enforced.
? Pairwise Compatibility: In this approach,
the compatibility function only estimates the
compatibility between pairs of fields, not sets
of fields. To compute inter-cluster compat-
ibility, the mean of the edges between the
clusters is calculated.
? McDonald: This method uses the pairwise
compatibility function, but instead of calcu-
lating the mean of inter-cluster edges, it cal-
culates the geometric mean of all pairs of
edges in the potential new cluster. That is,
to calculate the compatibility of records Ri
and Rj , we construct a new record Rij that
contains all fields of Ri and Rj , then calcu-
late the geometric mean of all pairs of fields
in Rij . This is analogous to the method used
in McDonald et al (2005) for relation extrac-
tion.
? Cluster Compatibility (uniform): Inter-
cluster compatibility is calculated directly by
the cluster compatibility function. This is the
method we advocate in Section 3. Training
examples are sampled uniformly as described
in Section 3.3.
? Cluster Compatibility (iterative): Same as
above, but training examples are sampled us-
ing the iterative method described in Section
3.3.
4.3 Features
For the pairwise compatibility classifier, we ex-
ploit various formatting as well as knowledge-
based features. Formatting features include the
number of hard returns between fields, whether
the fields occur on the same line, and whether the
fields occur consecutively. Knowledge-based fea-
tures include a mapping we compiled of cities and
states in the United States and Canada. Addition-
ally, we used compatibility features, such as which
fields are of the same type but have different val-
ues.
In building the cluster compatibility classifier,
we use many of the same features as in the bi-
nary classifier, but cast them as first-order existen-
tial features that are generated if the feature exists
between any pair of fields in the two clusters. Ad-
ditionally, we are able to exploit more powerful
compatibility and knowledge-base features. For
example, we examine if a title, a first name and a
last name occur consecutively (i.e., no other fields
occur in-between them). Also, we examine multi-
ple telephone numbers to ensure that they have the
same area codes. Additionally, we employ count
features that indicate if a certain field occurs more
than a given threshold.
4.4 Results
For these experiments, we compare performance
on the true record for each page. That is, we cal-
culate how often each system returns a complete
and accurate extraction of the contact record per-
taining to the owner of the webpage. We refer to
608
this record as the canonical record and measure
performance in terms of precision, recall and F1
for each field in the canonical record.
Table 2 compares precision, recall and F1 across
the various systems. The cluster compatibility
method with iterative sampling has the highest F1,
demonstrating a 14% error reduction over the next
best method and a 53% error reduction over the
transitive closure baseline.
Transitive closure has the highest recall, but it
comes at the expense of precision, and hence ob-
tains lower F1 scores than more conservative com-
patibility methods. The McDonald method also
has high recall, but drastically improves precision
over the transitivity method by taking into consid-
eration all edge weights.
The pairwise measure yields a slightly higher
F1 score than McDonald mostly due to precision
improvements. Because the McDonald method
calculates the mean of all edge weights rather
than just the inter-cluster edge weights, inter-
cluster weights are often outweighed by intra-
cluster weights. This can cause two densely-
connected clusters to be merged despite low inter-
cluster edge weights.
To further investigate performance differences,
we perform three additional experiments. The first
measures how sensitive the algorithms are to the
threshold value ? . Figure 3 plots the precision-
recall curve obtained by varying ? from 1.0 to 0.1.
As expected, high values of ? result in low recall
but high precision, since the algorithms halt with
a large number of small clusters. The highlighted
points correspond to ? = 0.5. These results indi-
cate that setting ? to 0.5 is near optimal, and that
the cluster compatibility method outperforms the
pairwise across a wide range of values for ? .
In the second experiment, we plot F1 versus
the size of the canonical record. Figure 4 indi-
cates that most of the performance gain occurs
in smaller canonical records (containing between
6 and 12 fields). Small canonical records are
most susceptible to precision errors simply be-
cause there are more extraneous fields that may
be incorrectly assigned to them. These precision
errors are often addressed by the cluster compati-
bility method, as shown in Table 2.
In the final experiment, we plot F1 versus the
total number of fields on the page. Figure 5 indi-
cates that the cluster compatibility method is best
at handling documents with large number of fields.
F1 Precision Recall
Cluster (I) 91.81 (.013) 92.87 (.005) 90.78 (.007)
Cluster (U) 90.02 (.012) 93.56 (.007) 86.74 (.011)
Pairwise 90.51 (.013) 91.07 (.004) 89.95 (.006)
McDonald 88.36 (.012) 83.55 (.004) 93.75 (.005)
Trans Clos 82.37 (.002) 70.75 (.009) 98.56 (.020)
Table 2: Precision, recall, and F1 performance for
the record extraction task. The standard error is
calculated over 10 cross-validation trials.
 
0.55 0.6
 
0.65 0.7
 
0.75 0.8
 
0.85 0.9
 
0.95 1  0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9
 
1
precision
recall
cluste
r
pairwis
e
Figure 3: Precision-recall curve for cluster, pair-
wise, and mcdonald. The graph is obtained by
varying the stopping threshold ? from 1.0 to 0.1.
The highlighted points correspond to ? = 0.5.
When there are over 80 fields in the document, the
performance of the pairwise method drops dramat-
ically, while cluster compatibility only declines
slightly. We believe the improved precision of the
cluster compatibility method explains this trend as
well.
We also examine documents where cluster com-
patibility outperforms the pairwise methods. Typ-
ically, these documents contain interleaving con-
tact records. Often, it is the case that a single pair
of fields is sufficient to determine whether a clus-
ter should not be merged. For example, the cluster
classifier can directly model the fact that a con-
tact record should not have multiple first or last
names. It can also associate a weight with the fact
that several fields overlap (e.g., the chances that
a cluster has two first names, two last names and
two cities). In contrast, the binary classifier only
examines pairs of fields in isolation and averages
these probabilities with other edges. This averag-
ing can dilute the evidence from a single pair of
fields. Embarrassing errors may result, such as
a contact record with two first names or two last
609
0.740.760.78
0.80.820.84
0.860.880.9
0.920.940.96
6-9 9-12 12+number fields per record
F1
pairwisemcdonaldcluster
Figure 4: Field F1 as the size of the canonical
record increases. This figure suggests that clus-
ter compatibility is most helpful for small records.
0.80.82
0.840.86
0.880.9
0.920.94
0.96
0-20 20-40 40-60 60-80 80+number fields per document
F1
pairwisemcdonaldcluster
Figure 5: Field F1 as the number of fields in
the document increases. This figure suggests that
cluster compatibility is most helpful when the doc-
ument has more than 80 fields.
names. These errors are particularly prevalent in
interleaving contact records since adjacent fields
often belong to the same record.
5 Conclusions and Future Work
We have investigated graph partitioning methods
for discovering database records from fields anno-
tated in text. We have proposed a cluster compat-
ibility function that measures how likely it is that
two sets of fields belong to the same cluster. We
argue that this enhancement to existing techniques
provides more representational power.
We have evaluated these methods on a set of
hand-annotated data and concluded that (1) graph
partitioning techniques are more accurate than per-
forming transitive closure, and (2) cluster compat-
ibility methods can avoid common mistakes made
by pairwise compatibility methods.
As information extraction systems become
more reliable, it will become increasingly impor-
tant to develop accurate ways of associating dis-
parate fields into cohesive records. This will en-
able more complex reasoning over text.
One shortcoming of this approach is that fields
are not allowed to belong to multiple records,
because the partitioning algorithm returns non-
overlapping clusters. Exploring overlapping clus-
tering techniques is an area of future work.
Another avenue of future research is to consider
syntactic information in the compatibility func-
tion. While performance on contact record extrac-
tion is highly influenced by formatting features,
many fields occur within sentences, and syntactic
information (such as dependency trees or phrase-
structure trees) may improve performance.
Overall performance can also be improved by
increasing the sophistication of the partitioning
method. For example, we can examine ?block
moves? to swap multiple fields between clusters
in unison, possibly avoiding local minima of the
greedy method (Kanani et al, 2006). This can be
especially helpful because many mistakes may be
made at the start of clustering, before clusters are
large enough to reflect true records.
Additionally, many personal web pages con-
tain a time-line of information that describe a per-
son?s educational and professional history. Learn-
ing to associate time information with each con-
tact record enables career path modeling, which
presents interesting opportunities for knowledge
discovery techniques, a subject of ongoing work.
Acknowledgments
We thank the anonymous reviewers for helpful
suggestions. This work was supported in part by
the Center for Intelligent Information Retrieval, in
part by U.S. Government contract #NBCH040171
through a subcontract with BBNT Solutions LLC,
in part by The Central Intelligence Agency, the
National Security Agency and National Science
Foundation under NSF grant #IIS-0326249, and in
part by the Defense Advanced Research Projects
Agency (DARPA), through the Department of the
Interior, NBC, Acquisition Services Division, un-
der contract number NBCHD030010. Any opin-
610
ions, findings and conclusions or recommenda-
tions expressed in this material are the author(s)
and do not necessarily reflect those of the sponsor.
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the Fifth ACM Interna-
tional Conference on Digital Libraries.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learining, 56:89?
113.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34:211?231.
Vinayak R. Borkar, Kaustubh Deshmukh, and Sunita
Sarawagi. 2001. Automatic segmentation of text
into structured records. In SIGMOD Conference.
Sergey Brin. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology.
Aron Culotta and Andrew McCallum. 2006. Practical
Markov logic containing first-order quantifiers with
application to identity uncertainty. In HLT Work-
shop on Computationally Hard Problems and Joint
Inference in Speech and Language Processing, June.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and contact
information from email and the web. In First Con-
ference on Email and Anti-Spam (CEAS), Mountain
View, CA.
David W. Embley and Lin Xu. 2000. Record location
and reconfiguration in unstructured multiple-record
web documents. In WebDB, pages 123?128.
David W. Embley, Xiaoyi Jiang, and Yiu-Kai Ng.
1999. Record-boundary discovery in web docu-
ments. In SIGMOD Conference, pages 467?478.
Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In SCIE, pages 10?27.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In ACL.
Pallika Kanani, Andrew McCallum, and Chris Pal.
2006. Improving author coreference by resource-
bounded information gathering from the web. Tech-
nical note.
Trausti Kristjannson, Aron Culotta, Paul Viola, and
Andrew McCallum. 2004. Interactive information
extraction with conditional random fields. Nine-
teenth National Conference on Artificial Intelligence
(AAAI 2004).
Andrew McCallum and Ben Wellner. 2005. Condi-
tional models of identity uncertainty with applica-
tion to noun coreference. In Lawrence K. Saul, Yair
Weiss, and Le?on Bottou, editors, Advances in Neu-
ral Information Processing Systems 17. MIT Press,
Cambridge, MA.
Ryan McDonald, Fernando Pereira, Seth Kulick, Scott
Winters, Yang Jin, and Pete White. 2005. Simple
algorithms for complex relation extraction with ap-
plications to biomedical ie. In 43rd Annual Meeting
of the Association for Computational Linguistics.
Brian Milch, Bhaskara Marthi, and Stuart Russell.
2005. BLOG: Probabilistic models with unknown
objects. In IJCAI.
Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In ANLP.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press. To appear.
Paul Viola and Mukund Narasimhan. 2005. Learning
to extract information from semi-structured text us-
ing a discriminative context free grammar. In SIGIR
?05: Proceedings of the 28th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 330?337, New
York, NY, USA. ACM Press.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
611
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1104?1113, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Monte Carlo MCMC: Efficient Inference by Approximate Sampling
Sameer Singh
University of Massachusetts
140 Governor?s Drive
Amherst MA
sameer@cs.umass.edu
Michael Wick
University of Massachsetts
140 Governor?s Drive
Amherst, MA
mwick@cs.umass.edu
Andrew McCallum
University of Massachusetts
140 Governor?s Drive
Amherst MA
mccallum@cs.umass.edu
Abstract
Conditional random fields and other graphi-
cal models have achieved state of the art re-
sults in a variety of tasks such as coreference,
relation extraction, data integration, and pars-
ing. Increasingly, practitioners are using mod-
els with more complex structure?higher tree-
width, larger fan-out, more features, and more
data?rendering even approximate inference
methods such as MCMC inefficient. In this
paper we propose an alternative MCMC sam-
pling scheme in which transition probabilities
are approximated by sampling from the set
of relevant factors. We demonstrate that our
method converges more quickly than a tradi-
tional MCMC sampler for both marginal and
MAP inference. In an author coreference task
with over 5 million mentions, we achieve a 13
times speedup over regular MCMC inference.
1 Introduction
Conditional random fields and other graphical mod-
els are at the forefront of many natural language
processing (NLP) and information extraction (IE)
tasks because they provide a framework for discrim-
inative modeling while succinctly representing de-
pendencies among many related output variables.
Previously, most applications of graphical models
were limited to structures where exact inference is
possible, for example linear-chain CRFs (Lafferty
et al 2001). More recently, there has been a de-
sire to include more factors, longer range depen-
dencies, and more sophisticated features; these in-
clude skip-chain CRFs for named entity recogni-
tion (Sutton and McCallum, 2004), probabilistic
DBs (Wick et al 2010), higher-order models for
dependency parsing (Carreras, 2007), entity-wise
models for coreference (Culotta et al 2007; Wick
et al 2009), and global models of relations (Hoff-
mann et al 2011). The increasing sophistication of
these individual NLP components compounded with
the community?s desire to model these tasks jointly
across cross-document considerations has resulted
in graphical models for which inference is compu-
tationally intractable. Even popular approximate in-
ference techniques such as loopy belief propagation
and Markov chain Monte Carlo (MCMC) may be
prohibitively slow.
MCMC algorithms such as Metropolis-Hastings
are usually efficient for graphical models because
the only factors needed to score a proposal are those
touching the changed variables. However, MCMC
is slowed in situations where a) the model exhibits
variables that have a high-degree (neighbor many
factors), b) proposals modify a substantial subset of
the variables to satisfy domain constraints (such as
transitivity in coreference), or c) evaluating a single
factor is expensive, for example when features are
based on string-similarity. For example, the seem-
ingly innocuous proposal changing the entity type of
a single entity requires examining all its mentions,
i.e. scoring a linear number of factors (in the num-
ber of mentions of that entity). Similarly, evaluating
coreference of a mention to an entity also requires
scoring factors to all the mentions of the entity. Of-
ten, however, the factors are somewhat redundant,
for example, not all mentions of the ?USA? entity
need to be examined to confidently conclude that it
is a COUNTRY, or that it is coreferent with ?United
1104
States of America?.
In this paper we propose an approximate MCMC
framework that facilitates efficient inference in high-
degree graphical models. In particular, we approx-
imate the acceptance ratio in the Metropolis Hast-
ings algorithm by replacing the exact model score
with a stochastic approximation that samples from
the set of relevant factors. We explore two sampling
strategies, a fixed proportion approach that samples
the factors uniformly, and a dynamic alternative that
samples factors until the method is confident about
its estimate of the model score.
We evaluate our method empirically on both syn-
thetic and real-world data. On synthetic classi-
fication data, our approximate MCMC procedure
obtains the true marginals faster than a traditional
MCMC sampler. On real-world tasks, our method
achieves 7 times speedup on citation matching, and
13 times speedup on large-scale author disambigua-
tion.
2 Background
2.1 Graphical Models
Factor graphs (Kschischang et al 2001) succinctly
represent the joint distribution over random vari-
ables by a product of factors that make the depen-
dencies between the random variables explicit. A
factor graph is a bipartite graph between the vari-
ables and factors, where each (log) factor f ? F is
a function that maps an assignment of its neighbor-
ing variables to a real number. For example, in a
linear-chain model of part-of-speech tagging, transi-
tion factors score compatibilities between consecu-
tive labels, while emission factors score compatibil-
ities between a label and its observed token.
The probability distribution expressed by the fac-
tor graph is given as a normalized product of the fac-
tors, which we rewrite as an exponentiated sum:
p(y) =
exp?(y)
Z
(1)
?(y) =
?
f?F
f(yf ) (2)
Z =
?
y?Y
exp?(y) (3)
Intuitively, the model favors assignments to the ran-
dom variables that yield higher factor scores and will
assign higher probabilities to such configurations.
The two common inference problems for graphi-
cal models in NLP are maximum a posterior (MAP)
and marginal inference. For models without latent
variables, the MAP estimate is the setting to the
variables that has the highest probability under the
model:
yMAP = argmax
y
p(y) (4)
Marginal inference is the problem of finding
marginal distributions over subsets of the variables,
used primarily in maximum likelihood gradients and
for max marginal inference.
2.2 Markov chain Monte Carlo (MCMC)
Often, computing marginal estimates of a model is
computationally intractable due to the normalization
constant Z, while maximum a posteriori (MAP) is
prohibitive due to the search space of possible con-
figurations. Markov chain Monte Carlo (MCMC) is
important tool for performing sample- and search-
based inference in these models. A particularly suc-
cessful MCMC method for graphical model infer-
ence is Metropolis-Hastings (MH). Since sampling
from the true model p(y) is intractable, MH instead
uses a simpler distribution q(y?|y) that conditions
on a current state y and proposes a new state y? by
modifying a few variables. This new assignment is
then accepted with probability ?:
? = min
(
1,
p(y?)
p(y)
q(y|y?)
q(y?|y)
)
(5)
Computing this acceptance probability is often
highly efficient because the partition function can-
cels, as do all the factors in the model that do not
neighbor the modified variables. MH can be used
for both MAP and marginal inference.
2.2.1 Marginal Inference
To compute marginals with MH, the variables are
initialized to an arbitrary assignment (i.e., randomly
or with some heuristic), and sampling is run until the
samples {yi|i = 0, ? ? ? , n} become independent of
the initial assignment. The ergodic theorem provides
the MCMC analog to the law-of-large-numbers, jus-
tifying the use of the generated samples to compute
the desired statistics (such as feature expectations or
variable marginals).
1105
2.2.2 MAP Inference
Since MCMC can efficiently explore the high
density regions for a given distribution, the distri-
bution p can be modified such that the high-density
region of the new distribution represents the MAP
configuration of p. This is achieved by adding a tem-
perature term ? to the distribution p, resulting in the
following MH acceptance probability:
? = min
(
1,
(
p(y?)
p(y)
) 1
?
)
(6)
Note that as ? ? 0, MH will sample closer to the
MAP configuration. If a cooling schedule is imple-
mented for ? then the MH sampler for MAP infer-
ence can be seen as an instance of simulated anneal-
ing (Bertsimas and Tsitsiklis, 1993).
3 Monte Carlo MCMC
In this section we introduce our approach for ap-
proximating the acceptance ratio of Metropolis-
Hastings that samples the factors, and describe two
sampling strategies.
3.1 Stochastic Proposal Evaluation
Although one of the benefits of MCMC lies in its
ability to leverage the locality of the proposal, for
some information extraction tasks this can become a
crucial bottleneck. In particular, evaluation of each
sample requires computing the score of all the fac-
tors that are involved in the change, i.e. all fac-
tors that neighbor any variable in the set that has
changed. This evaluation becomes a bottleneck for
tasks in which a large number of variables is in-
volved in each proposal, or in which the model con-
tains a number of high-degree variables, resulting in
a large number of factors, or in which computing
the factor score involves an expensive computation,
such as string similarity between mention text.
Instead of evaluating the log-score ? of the model
exactly, this paper proposes a Monte-Carlo estima-
tion of the log-score. In particular, if the set of fac-
tors for a given proposal y? y? is F(y,y?), we use
a sampled subset of the factors S ? F(y,y?) as an
approximation of the model score. In the following
we use F as an abbreviation for F(y,y?). Formally,
?(y) =
?
f?F
f(yf ) = |F| ? EF [f(yf )]
?S(y) = |F| ? ES [f(yf )] (7)
We use the sample log-score (?S) in the acceptance
probability ? to evaluate the samples. Since we are
using a stochastic approximation to the model score,
in general we need to take more MCMC samples
before we converge, however, since evaluating each
sample will be much faster (O(|S|) as opposed to
O(|F|)), we expect overall sampling to be faster.
In the next sections we describe several alternative
strategies for sampling the set of factors S. The pri-
mary restriction on the set of samples S is that their
mean should be an unbiased estimator ofEF[f ]. Fur-
ther, time taken to obtain the set of samples should
be negligible when compared to scoring all the fac-
tors in F. Note that there is an implicit minimum of
1 to the number of the sampled factors.
3.2 Uniform Sampling
The most direct approach for subsampling the set
of F is to perform uniform sampling. In particular,
given a proportion parameter 0 < p ? 1, we select a
random subset Sp ? F such that |Sp| = p ? |F|. Since
this approach is agnostic as to the actual factors
scores, ES[f ] ? EF[f ]. A low p leads to fast evalua-
tion, however it may require a large number of sam-
ples due to the substantial approximation. On the
other hand, although a higher p will converge with
fewer samples, evaluating each sample is slower.
3.3 Confidence-Based Sampling
Selecting the best value for p is difficult, requiring
analysis of the graph structure, and statistics on the
distribution of the factors scores; often a difficult
task in real-world applications. Further, the same
value for p can result in different levels of approxi-
mation for different proposals, either unnecessarily
accurate or problematically noisy. We would prefer
a strategy that adapts to the distribution of the scores
in F.
Instead of sampling a fixed proportion of factors,
we can sample until we are confident that the cur-
rent set of samples Sc is an accurate estimate of the
true mean of F. In particular, we maintain a run-
ning count of the sample mean ESc [f ] and variance
1106
?Sc , using them to compute a confidence interval IS
around our estimate of the mean. Since the num-
ber of sampled factors S could be a substantial frac-
tion of the set of factors F,1 we also incorporate fi-
nite population control (fpc) in our sample variance
computation. We compute the confidence interval as
follows:
?2S =
1
|S| ? 1
?
f?S
(f ? ES [f ])
2 (8)
IS = 2z
?S
?
|S|
?
|F| ? |S|
|F| ? 1
(9)
where we set the z to 1.96, i.e. the 95% confidence
interval. This approach starts with an empty set of
samples, S = {}, and iteratively samples factors
without replacement to add to S, until the confidence
interval around the estimated mean falls below a user
specified maximum interval width threshold i. As a
result, for proposals that contain high-variance fac-
tors, this strategy examines a large number of fac-
tors, while proposals that involve similar factors will
result in fewer samples. Note that this user-specified
threshold is agnostic to the graph structure and the
number of factors, and instead directly reflects the
score distribution of the relevant factors.
4 Experiments
In this section we evaluate our approach for both
marginal and MAP inference.
4.1 Marginal Inference on Synthetic Data
Consider the task of classifying entities into a set of
types, for example, POLITICIAN, VEHICLE, CITY,
GOVERMENT-ORG, etc. For knowledge base con-
struction, this prediction often takes place on the
entity-level, as opposed to the mention-level com-
mon in traditional NLP. To evaluate the type at the
entity-level, the scored factors examine features of
all the entity mentions of the entity, along with the
labels of all relation mentions for which it is an ar-
gument. See Yao et al(2010) and Hoffmann et al
(2011) for examples of such models. Since a sub-
set of the mentions can be sufficiently informative
for the model, we expect our stochastic MCMC ap-
proach to work well.
1Specifically, the fraction may be higher than > 5%
Label
(a) Binary Classification
Model (n = 100)
-4.8 -4 -3.2 -2.4 -1.6 -0.8 0 0.8 1.6 2.4 3.2 4 4.8 5.6 6.4 7.2
-0.4
-0.3
-0.2
-0.1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Label 1Label 0
(b) Distribution of Factor scores
Figure 1: Synthetic Model for Classification
1 0 2 0 3 0 100 200 1000 10000 100000 1000000Number of Factors Examined
0.0000.025
0.0500.075
0.1000.125
0.1500.175
0.2000.225
0.2500.275
0.3000.325
0.3500.375
0.4000.425
0.450
Erro
r in M
argin
al
p:1. p:0.75 p:0.5 p:0.2p:0.1 i:0.1 i:0.05 i:0.01i:0.005 i:0.001
Figure 2: Marginal Inference Error for Classification
on Synthetic Data
We use synthetic data for such a model to evaluate
the quality of marginals returned by the Gibbs sam-
pling form of MCMC. Since the Gibbs algorithm
samples each variable using a fixed assignment of
its neighborhood, we represent generating a single
sample as classification. We create star-shaped mod-
els with a single unobserved variable (entity type)
that neighbors many unary factors, each represent-
ing a single entity- or a relation-mention factor (See
Figure 1a for an example). We generate a synthetic
dataset for this model, creating 100 variables con-
sisting of 100 factors each. The scores of the fac-
tors are generated from gaussians, N(0.5, 1) for the
positive label, and N(?0.5, 1) for the negative label
(note the overlap between the weights in Figure 1b).
Although each structure contains only a single vari-
able, and no cycles, it is a valid benchmark to test
our sampling approach since the effects of the set-
ting of burn-in period and the thinning samples are
not a concern.
We perform standard Gibbs sampling, and com-
1107
pare the marginals obtained during sampling with
the true marginals, computed exactly. We evalu-
ate the previously described uniform sampling and
confidence-based sampling, with several parameter
values, and plot the L1 error to the true marginals
as more factors are examined. Note that here, and
in the rest of the evaluation, we shall use the num-
ber of factors scored as a proxy for running time,
since the effects of the rest of the steps of sam-
pling are relatively negligible. The error in compar-
ison to regular MCMC (p = 1) is shown in Fig-
ure 2, with standard error bars averaging over 100
models. Initially, as the sampling approach is made
more stochastic (lowering p or increasing i), we see
a steady improvement in the running time needed
to obtain the same error tolerance. However, the
amount of relative improvements slows as stochas-
ticity is increased further; in fact for extreme values
(i = 0.05, p = 0.1) the chains perform worse than
regular MCMC.
4.2 Entity Resolution in Citation Data
To evaluate our approach on a real world dataset,
we apply stochastic MCMC for MAP inference on
the task of citation matching. Given a large number
of citations (that appear at the end of research pa-
pers, for example), the task is to group together the
citations that refer to the same paper. The citation
matching problem is an instance of entity resolution,
in which observed mentions need to be partitioned
such that mentions in a set refer to the same under-
lying entity. Note that neither the identities, or the
number of underlying entities is known.
In this paper, the graphical model of entity reso-
lution consists of observed mentions (mi), and pair-
wise binary variables between all pairs of mentions
(yij) which represent whether the corresponding ob-
served mentions are coreferent. There is a local
factor for each coreference variable yij that has a
high score if the underlying mentions mi and mj
are similar. For the sake of efficiency, we only in-
stantiate and incorporate the variables and factors
when the variable is true, i.e. if yij = 1. Thus,
?(y) =
?
e
?
mi,mj?e
f(yij). The set of possible
worlds consists of all settings of the y variables that
are consistent with transitivity, i.e. the binary vari-
ables directly represent a valid clustering over the
mentions. An example of the model defined over 5
m2
m1
m3
m5
m4
1
1
1
1
y
12
y
23
y
13
y
45
Figure 3: Graphical Model for Entity Resolution:
defined over 5 mentions, with the setting of the vari-
ables resulting in 2 entities. For the sake of brevity,
we?ve only included variables set to 1; binary vari-
ables between mentions that are not coreferent have
been omitted.
mentions is given in Figure 3. This representation
is equivalent to Model 2 as introduced in McCal-
lum and Wellner (2004). As opposed to belief prop-
agation and other approximate inference techniques,
MCMC is especially appropriate for the task as it
can directly enforce transitivity.
When performing MCMC, each sample is a set-
ting to all the y variables that is consistent with tran-
sitivity. To maintain transitivity during sampling,
Metropolis Hastings is used to change the binary
variables in a way that is consistent with moving in-
dividual mentions. Our proposal function selects a
random mention, and moves it to a random entity,
changing all the pairwise variables with mentions in
its old entity, and the pairwise variables with men-
tions in its new entity. Thus, evaluation of such a
proposal function requires scoring a number of fac-
tors linear in the size of the entities, which, for large
datasets, can be a significant bottleneck. In prac-
tice, however, these set of factors are often highly
redundant, as many of the mentions that refer to the
same entity contain redundant information and fea-
tures, and entity membership may be efficiently de-
termined by observing a subset of its mentions.
We evaluate on the Cora dataset (McCallum et
al., 1999), used previously to evaluate a number
of information extraction approaches (Pasula et al
2003), including MCMC based inference (Poon and
Domingos, 2007; Singh et al 2009). The dataset
1108
10000 100000 1000000 10000000 100000000Number of Factors Examined
0.000.05
0.100.15
0.200.25
0.300.35
0.400.45
0.500.55
0.600.65
0.700.75
0.800.85
0.900.95
1.001.05
BCu
bed 
F1
p:1. p:0.5 p:0.2 p:0.1i:20. i:2. i:1. i:0.5 i:0.1
Figure 4: Citation Resolution Accuracy Plot for uni-
form and variance-based sampling compared to reg-
ular MCMC (p = 1)
consists of 1295 mentions, that refer to 134 true un-
derlying entities. We use the same features for our
model as (Poon and Domingos, 2007), using true
author, title, and venue segmentation for features.
Since our focus is on evaluating scalability of in-
ference, we combine all the three folds of the data,
and train the model using Samplerank (Wick et al
2011).
We run MCMC on the entity resolution model us-
ing the proposal function described above, running
our approach with different parameter values. Since
we are interested in the MAP configuration, we use
a temperature term for annealing. As inference pro-
gresses, we compute BCubed2 F1 of the current
sample, and plot it against the number of scored fac-
tors in Figure 4. We observe consistent speed im-
provements as stochasticity is improved, with uni-
form sampling and confidence-based sampling per-
forming competitively. To compute the speedup, we
measure the number of factors scored to obtain a de-
sired level of accuracy (90% F1), shown for a di-
verse set of parameters in Table 1. With a very
large confidence interval threshold (i = 20) and
small proportion (p = 0.1), we obtain up to 7 times
speedup over regular MCMC. Since the average en-
tity size in this data set is < 10, using a small pro-
portion (and a wide interval) is equivalent to picking
a single mention to compare against.
2B3 is a coreference evaluation metric, introduced by Bagga
and Baldwin (1998)
Method Factors Examined Speedup
Baseline 57,292,700 1x
Uniform Sampling
p = 0.75 34,803,972 1.64x
p = 0.5 28,143,323 2.04x
p = 0.3 17,778,891 3.22x
p = 0.2 12,892,079 4.44x
p = 0.1 7,855,686 7.29x
Variance-Based Sampling
i = 0.001 52,522,728 1.09x
i = 0.01 51,547,000 1.11x
i = 0.1 47,165,038 1.21x
i = 0.5 32,828,823 1.74x
i = 1 18,938,791 3.02x
i = 2 11,134,267 5.14x
i = 5 9,827,498 5.83x
i = 10 8,675,833 6.60x
i = 20 8,295,587 6.90x
Table 1: Speedups on Cora to obtain 90% B3 F1
4.3 Large-Scale Author Coreference
As the body of published scientific work continues
to grow, author coreference, the problem of clus-
tering mentions of research paper authors into the
real-world authors to which they refer, is becoming
an increasingly important step for performing mean-
ingful bibliometric analysis. However, scaling typi-
cal pairwise models of coreference (e.g., McCallum
and Wellner (2004)) is difficult because the number
of factors in the model grows quadratically with the
number of mentions (research papers) and the num-
ber of factors evaluated for every MCMC proposal
scales linearly in the size of the clusters. For author
coreference, the number of author mentions and the
number of references to an author entity can often be
in the millions, making the evaluation of the MCMC
proposals computationally expensive.
We use the publicly available DBLP dataset3 of
BibTex entries as our unlabeled set of mentions,
which contains nearly 5 million authors. For eval-
uation of accuracy, we also include author mentions
from the Rexa corpus4 that contains 2, 833 mentions
3http://www.informatik.uni-trier.de/
?ley/db/
4http://www2.selu.edu/Academics/Faculty/
aculotta/data/rexa.html
1109
10000000 100000000 1000000000 10000000000Number of Factors Examined
0.000.05
0.100.15
0.200.25
0.300.35
0.400.45
0.500.55
0.600.65
0.700.75
0.800.85
0.900.95
1.001.05
BCub
ed F1
p:1. p:0.5 p:0.2 p:0.1p:0.01 i:10. i:1. i:0.1
(a) Accuracy versus Number of Factors scored
10000000 100000000Number of Samples
0.000.05
0.100.15
0.200.25
0.300.35
0.400.45
0.500.55
0.600.65
0.700.75
0.800.85
0.900.95
1.001.05
BCub
ed F1
p:1. p:0.5 p:0.2 p:0.1p:0.01 i:10. i:1. i:0.1
(b) Accuracy versus Number of Samples
Figure 5: Performance of Different Sampling Strategies and Parameters for coreference over 5 million
mentions. Plot with p refer to uniform sampling with proportion p of factors picked, while plots with i
sample till confidence intervals are narrower than i.
labeled for coreference.
We use the same Metropolis-Hastings scheme that
we employ in the problem of citation matching. As
before, we initialize to the singleton configuration
and run the experiments for a fixed number of sam-
ples, plotting accuracy versus the number of factors
evaluated (Figure 5a) as well as accuracy versus the
number of samples generated (Figure 5b). We also
tabulate the relative speedups to obtain the desired
accuracy level in Table 2. Our proposed method
achieves substantial savings on this task: speedups
of 13.16 using the variance sampler and speedups
of 9.78 using the uniform sampler. As expected,
when we compare the performance using the num-
ber of generated samples, the approximate MCMC
chains appear to converge more slowly; however, the
overall convergence for our approach is substantially
faster because evaluation of each sample is signif-
icantly cheaper. We also present results on using
extreme approximations (for example, p = 0.01),
resulting in convergence to a low accuracy.
5 Discussion and Related Work
MCMC is a popular method for inference amongst
researchers that work with large and dense graphi-
cal models (Richardson and Domingos, 2006; Poon
and Domingos, 2006; Poon et al 2008; Singh et al
2009; Wick et al 2009). Some of the probabilistic
Method Factors Examined Speedup
Baseline 1,395,330,603 1x
Uniform
p = 0.5 689,254,134 2.02x
p = 0.2 327,616,794 4.26x
p = 0.1 206,157,705 6.77x
p = 0.05 152,069,987 9.17x
p = 0.02 142,689,770 9.78x
Variance
i = 0.00001 1,442,091,344 0.96x
i = 0.0001 1,419,110,724 0.98x
i = 0.001 1,374,667,077 1.01x
i = 0.1 1,012,321,830 1.38x
i = 1 265,327,983 5.26x
i = 10 179,701,896 7.76x
i = 100 106,850,725 13.16x
Table 2: Speedups on DBLP to reach 80% B3 F1
programming packages popular amongst NLP prac-
titioners also rely on MCMC for inference and learn-
ing (Richardson and Domingos, 2006; McCallum et
al., 2009). Although most of these methods apply
MCMC directly, the rate of convergence of MCMC
has become a concern as larger and more densely-
factored models are being considered, motivating
the need for more efficient sampling that uses par-
allelism (Singh et al 2011; Gonzalez et al 2011)
1110
and domain knowledge for blocking (Singh et al
2010). Thus we feel providing a method to speed up
MCMC inference can have a significant impact.
There has also been recent work in designing
scalable approximate inference techniques. Belief
propagation has, in particular, has gained some re-
cent interest. Similar to our approach, a number
of researchers propose modifications to BP that per-
form inference without visiting all the factors. Re-
cent work introduces dynamic schedules to priori-
tize amongst the factors (Coughlan and Shen, 2007;
Sutton and McCallum, 2007) that has been used to
only visit a small fraction of the factors (Riedel and
Smith, 2010). Gonzalez et al(2009) utilize these
schedules to facilitate parallelization.
A number of existing approaches in statistics
are also related to our contribution. Leskovec and
Faloutsos (2006) propose techniques to sample a
graph to compute certain graph statistics with asso-
ciated confidence. Christen and Fox (2005) also pro-
pose an approach to efficiently evaluate a proposal,
however, once accepted, they score all the factors.
Murray and Ghahramani (2004) propose an approx-
imate MCMC technique for Bayesian models that
estimates the partition function instead of comput-
ing it exactly.
Related work has also applied such ideas for
robust learning, for example Kok and Domingos
(2005), based on earlier work by Hulten and Domin-
gos (2002), uniformly sample the groundings of an
MLN to estimate the likelihood.
6 Conclusions and Future Work
Motivated by the need for an efficient inference tech-
nique that can scale to large, densely-factored mod-
els, this paper considers a simple extension to the
Markov chain Monto Carlo algorithm. By observ-
ing that many graphical models contain substantial
redundancy among the factors, we propose stochas-
tic evaluation of proposals that subsamples the fac-
tors to be scored. Using two proposed sampling
strategies, we demonstrate improved convergence
for marginal inference on synthetic data. Further,
we evaluate our approach on two real-world entity
resolution datasets, obtaining a 13 times speedup on
a dataset containing 5 million mentions.
Based on the ideas presented in the paper, we will
consider additional sampling strategies. In partic-
ular, we will explore dynamic sampling, in which
we sample fewer factors during the initial, burn-
in phase, but sample more factors as we get close
to convergence. Motivated by our positive results,
we will also study the application of this approach
to other approximate inference techniques, such as
belief propagation and variational inference. Since
training is often a huge bottleneck for information
extraction, we will also explore its applications to
parameter estimation.
Acknowledgements
This work was supported in part by the Center for
Intelligent Information Retrieval, in part by ARFL
under prime contract number is FA8650-10-C-7059,
and the University of Massachusetts gratefully ac-
knowledges the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
The U.S. Government is authorized to reproduce
and distribute reprint for Governmental purposes
notwithstanding any copyright annotation thereon.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect those of
the sponsor.
References
[Bagga and Baldwin1998] Amit Bagga and Breck Bald-
win. 1998. Algorithms for scoring coreference
chains. In International Conference on Language Re-
sources and Evaluation (LREC) Workshop on Linguis-
tics Coreference, pages 563?566.
[Bertsimas and Tsitsiklis1993] D. Bertsimas and J. Tsit-
siklis. 1993. Simulated annealing. Statistical Science,
pages 10?15.
[Carreras2007] Xavier Carreras. 2007. Experiments
with a higher-order projective dependency parser. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 957?961.
[Christen and Fox2005] J. Andre?s Christen and Colin
Fox. 2005. Markov chain monte carlo using an ap-
proximation. Journal of Computational and Graphi-
cal Statistics, 14(4):pp. 795?810.
[Coughlan and Shen2007] James Coughlan and Huiying
Shen. 2007. Dynamic quantization for belief propa-
1111
gation in sparse spaces. Computer Vision and Image
Understanding, 106:47?58, April.
[Culotta et al007] Aron Culotta, Michael Wick, and An-
drew McCallum. 2007. First-order probabilistic mod-
els for coreference resolution. In North American
Chapter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT).
[Gonzalez et al009] Joseph Gonzalez, Yucheng Low,
and Carlos Guestrin. 2009. Residual splash for op-
timally parallelizing belief propagation. In Artificial
Intelligence and Statistics (AISTATS).
[Gonzalez et al011] Joseph Gonzalez, Yucheng Low,
Arthur Gretton, and Carlos Guestrin. 2011. Paral-
lel gibbs sampling: From colored fields to thin junc-
tion trees. In Artificial Intelligence and Statistics (AIS-
TATS), Ft. Lauderdale, FL, May.
[Hoffmann et al011] Raphael Hoffmann, Congle
Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S.
Weld. 2011. Knowledge-based weak supervision
for information extraction of overlapping relations.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 541?550, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
[Hulten and Domingos2002] Geoff Hulten and Pedro
Domingos. 2002. Mining complex models from ar-
bitrarily large databases in constant time. In Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD), pages 525?531, New York, NY, USA.
ACM.
[Kok and Domingos2005] Stanley Kok and Pedro
Domingos. 2005. Learning the structure of markov
logic networks. In International Conference on
Machine Learning (ICML), pages 441?448, New
York, NY, USA. ACM.
[Kschischang et al001] Frank R. Kschischang, Bren-
dan J. Frey, and Hans Andrea Loeliger. 2001. Factor
graphs and the sum-product algorithm. IEEE Transac-
tions of Information Theory, 47(2):498?519, Feb.
[Lafferty et al001] John D. Lafferty, Andrew McCal-
lum, and Fernando Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting and
labeling sequence data. In International Conference
on Machine Learning (ICML).
[Leskovec and Faloutsos2006] Jure Leskovec and Chris-
tos Faloutsos. 2006. Sampling from large graphs.
In International Conference on Knowledge Discovery
and Data Mining (KDD), pages 631?636, New York,
NY, USA. ACM.
[McCallum and Wellner2004] Andrew McCallum and
Ben Wellner. 2004. Conditional models of identity
uncertainty with application to noun coreference. In
Neural Information Processing Systems (NIPS).
[McCallum et al999] Andrew McCallum, Kamal
Nigam, Jason Rennie, and Kristie Seymore. 1999.
A machine learning approach to building domain-
specific search engines. In International Joint
Conference on Artificial Intelligence (IJCAI).
[McCallum et al009] Andrew McCallum, Karl Schultz,
and Sameer Singh. 2009. FACTORIE: Probabilistic
programming via imperatively defined factor graphs.
In Neural Information Processing Systems (NIPS).
[Murray and Ghahramani2004] Iain Murray and Zoubin
Ghahramani. 2004. Bayesian learning in undirected
graphical models: Approximate MCMC algorithms.
In Uncertainty in Artificial Intelligence (UAI).
[Pasula et al003] H. Pasula, B. Marthi, B. Milch,
S. Russell, and I. Shpitser. 2003. Identity uncertainty
and citation matching. In Neural Information Process-
ing Systems (NIPS).
[Poon and Domingos2006] Hoifung Poon and Pedro
Domingos. 2006. Sound and efficient inference with
probabilistic and deterministic dependencies. In AAAI
Conference on Artificial Intelligence.
[Poon and Domingos2007] Hoifung Poon and Pedro
Domingos. 2007. Joint inference in informa-
tion extraction. In AAAI Conference on Artificial
Intelligence, pages 913?918.
[Poon et al008] Hoifung Poon, Pedro Domingos, and
Marc Sumner. 2008. A general method for reduc-
ing the complexity of relational inference and its ap-
plication to MCMC. In AAAI Conference on Artificial
Intelligence.
[Richardson and Domingos2006] Matthew Richardson
and Pedro Domingos. 2006. Markov logic networks.
Machine Learning, 62(1-2):107?136.
[Riedel and Smith2010] Sebastian Riedel and David A.
Smith. 2010. Relaxed marginal inference and its ap-
plication to dependency parsing. In North American
Chapter of the Association for Computational Linguis-
tics - Human Language Technologies (NAACL HLT),
pages 760?768.
[Singh et al009] Sameer Singh, Karl Schultz, and An-
drew McCallum. 2009. Bi-directional joint in-
ference for entity resolution and segmentation us-
ing imperatively-defined factor graphs. In Machine
Learning and Knowledge Discovery in Databases
(Lecture Notes in Computer Science) and European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML PKDD), pages 414?429.
[Singh et al010] Sameer Singh, Michael L. Wick, and
Andrew McCallum. 2010. Distantly labeling data for
large scale cross-document coreference. Computing
Research Repository (CoRR), abs/1005.4298.
[Singh et al011] Sameer Singh, Amarnag Subramanya,
Fernando Pereira, and Andrew McCallum. 2011.
1112
Large-scale cross-document coreference using dis-
tributed inference and hierarchical models. In Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL HLT).
[Sutton and McCallum2004] Charles Sutton and Andrew
McCallum. 2004. Collective segmentation and label-
ing of distant entities in information extraction. Tech-
nical Report TR#04-49, University of Massachusetts,
July.
[Sutton and McCallum2007] Charles Sutton and Andrew
McCallum. 2007. Improved dynamic schedules for
belief propagation. In Uncertainty in Artificial Intelli-
gence (UAI).
[Wick et al009] Michael Wick, Aron Culotta, Khasha-
yar Rohanimanesh, and Andrew McCallum. 2009.
An entity-based model for coreference resolution.
In SIAM International Conference on Data Mining
(SDM).
[Wick et al010] Michael Wick, Andrew McCallum, and
Gerome Miklau. 2010. Scalable probabilistic
databases with factor graphs and mcmc. International
Conference on Very Large Databases (VLDB), 3:794?
804, September.
[Wick et al011] Michael Wick, Khashayar Rohani-
manesh, Kedar Bellare, Aron Culotta, and Andrew
McCallum. 2011. Samplerank: Training factor graphs
with atomic gradients. In International Conference on
Machine Learning (ICML).
[Yao et al010] Limin Yao, Sebastian Riedel, and An-
drew McCallum. 2010. Collective cross-document
relation extraction without labelled data. In Empirical
Methods in Natural Language Processing (EMNLP).
1113
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379?388,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Discriminative Hierarchical Model for Fast Coreference at Large Scale
Michael Wick
University of Massachsetts
140 Governor?s Drive
Amherst, MA
mwick@cs.umass.edu
Sameer Singh
University of Massachusetts
140 Governor?s Drive
Amherst, MA
sameer@cs.umass.edu
Andrew McCallum
University of Massachusetts
140 Governor?s Drive
Amherst, MA
mccallum@cs.umass.edu
Abstract
Methods that measure compatibility between
mention pairs are currently the dominant ap-
proach to coreference. However, they suffer
from a number of drawbacks including diffi-
culties scaling to large numbers of mentions
and limited representational power. As these
drawbacks become increasingly restrictive,
the need to replace the pairwise approaches
with a more expressive, highly scalable al-
ternative is becoming urgent. In this paper
we propose a novel discriminative hierarchical
model that recursively partitions entities into
trees of latent sub-entities. These trees suc-
cinctly summarize the mentions providing a
highly compact, information-rich structure for
reasoning about entities and coreference un-
certainty at massive scales. We demonstrate
that the hierarchical model is several orders
of magnitude faster than pairwise, allowing us
to perform coreference on six million author
mentions in under four hours on a single CPU.
1 Introduction
Coreference resolution, the task of clustering men-
tions into partitions representing their underlying
real-world entities, is fundamental for high-level in-
formation extraction and data integration, including
semantic search, question answering, and knowl-
edge base construction. For example, coreference
is vital for determining author publication lists in
bibliographic knowledge bases such as CiteSeer and
Google Scholar, where the repository must know
if the ?R. Hamming? who authored ?Error detect-
ing and error correcting codes? is the same? ?R.
Hamming? who authored ?The unreasonable effec-
tiveness of mathematics.? Features of the mentions
(e.g., bags-of-words in titles, contextual snippets
and co-author lists) provide evidence for resolving
such entities.
Over the years, various machine learning tech-
niques have been applied to different variations of
the coreference problem. A commonality in many
of these approaches is that they model the prob-
lem of entity coreference as a collection of deci-
sions between mention pairs (Bagga and Baldwin,
1999; Soon et al, 2001; McCallum and Wellner,
2004; Singla and Domingos, 2005; Bengston and
Roth, 2008). That is, coreference is solved by an-
swering a quadratic number of questions of the form
?does mention A refer to the same entity as mention
B?? with a compatibility function that indicates how
likely A and B are coreferent. While these models
have been successful in some domains, they also ex-
hibit several undesirable characteristics. The first is
that pairwise models lack the expressivity required
to represent aggregate properties of the entities. Re-
cent work has shown that these entity-level prop-
erties allow systems to correct coreference errors
made from myopic pairwise decisions (Ng, 2005;
Culotta et al, 2007; Yang et al, 2008; Rahman and
Ng, 2009; Wick et al, 2009), and can even provide
a strong signal for unsupervised coreference (Bhat-
tacharya and Getoor, 2006; Haghighi and Klein,
2007; Haghighi and Klein, 2010).
A second problem, that has received significantly
less attention in the literature, is that the pair-
wise coreference models scale poorly to large col-
lections of mentions especially when the expected
379
Name:,Jamie,Callan,Ins(tu(ons:-CMU,LTI.,Topics:{WWW,,IR,,SIGIR},
Name:Jamie,Callan,Ins(tu(ons:,Topics:-IR,
Name:,J.,Callan,Ins(tu(ons:-CMU,LTI,Topics:-WWW,
Name:,J.,Callan,Ins(tu(ons:-LTI,Topics:-WWW,
Name:,James,Callan,Ins(tu(ons:-CMU,Topics:{WWW,,IR,,largeIscale},
Coref?-
Jamie,Callan,Topics:-IR,
J.,Callan,Inst:-LTI, J.,Callan,Topic:-WWW,
J.,Callan,Inst:-CMU,Jamie,Callan,Topics:-IR, J.,Callan,Inst:-CMU, James,Callan,Topics:-WWW,Inst:CMU,
J.,Callan,Topics:-IR,Inst:-CMU,
J.,Callan,Topics:-LIS,
Figure 1: Discriminative hierarchical factor graph for coreference: Latent entity nodes (white boxes)
summarize subtrees. Pairwise factors (black squares) measure compatibilities between child and parent
nodes, avoiding quadratic blow-up. Corresponding decision variables (open circles) indicate whether one
node is the child of another. Mentions (gray boxes) are leaves. Deciding whether to merge these two entities
requires evaluating just a single factor (red square), corresponding to the new child-parent relationship.
number of mentions in each entity cluster is also
large. Current systems cope with this by either
dividing the data into blocks to reduce the search
space (Herna?ndez and Stolfo, 1995; McCallum et
al., 2000; Bilenko et al, 2006), using fixed heuris-
tics to greedily compress the mentions (Ravin and
Kazi, 1999; Rao et al, 2010), employing special-
ized Markov chain Monte Carlo procedures (Milch
et al, 2006; Richardson and Domingos, 2006; Singh
et al, 2010), or introducing shallow hierarchies of
sub-entities for MCMC block moves and super-
entities for adaptive distributed inference (Singh et
al., 2011). However, while these methods help man-
age the search space for medium-scale data, eval-
uating each coreference decision in many of these
systems still scales linearly with the number of men-
tions in an entity, resulting in prohibitive computa-
tional costs associated with large datasets. This scal-
ing with the number of mentions per entity seems
particularly wasteful because although it is common
for an entity to be referenced by a large number
of mentions, many of these coreferent mentions are
highly similar to each other. For example, in author
coreference the two most common strings that refer
to Richard Hamming might have the form ?R. Ham-
ming? and ?Richard Hamming.? In newswire coref-
erence, a prominent entity like Barack Obama may
have millions of ?Obama? mentions (many occur-
ring in similar semantic contexts). Deciding whether
a mention belongs to this entity need not involve
comparisons to all contextually similar ?Obama?
mentions; rather we prefer a more compact repre-
sentation in order to efficiently reason about them.
In this paper we propose a novel hierarchical dis-
criminative factor graph for coreference resolution
that recursively structures each entity as a tree of la-
tent sub-entities with mentions at the leaves. Our
hierarchical model avoids the aforementioned prob-
lems of the pairwise approach: not only can it jointly
reason about attributes of entire entities (using the
power of discriminative conditional random fields),
but it is also able to scale to datasets with enor-
mous numbers of mentions because scoring enti-
ties does not require computing a quadratic number
of compatibility functions. The key insight is that
each node in the tree functions as a highly compact
information-rich summary of its children. Thus, a
small handful of upper-level nodes may summarize
millions of mentions (for example, a single node
may summarize all contextually similar ?R. Ham-
ming? mentions). Although inferring the structure
of the entities requires reasoning over a larger state-
space, the latent trees are actually beneficial to in-
ference (as shown for shallow trees in Singh et
al. (2011)), resulting in rapid progress toward high
probability regions, and mirroring known benefits
of auxiliary variable methods in statistical physics
(such as Swendsen and Wang (1987)). Moreover,
380
each step of inference is computationally efficient
because evaluating the cost of attaching (or detach-
ing) sub-trees requires computing just a single com-
patibility function (as seen in Figure 1). Further,
our hierarchical approach provides a number of ad-
ditional advantages. First, the recursive nature of the
tree (arbitrary depth and width) allows the model to
adapt to different types of data and effectively com-
press entities of different scales (e.g., entities with
more mentions may require a deeper hierarchy to
compress). Second, the model contains compatibil-
ity functions at all levels of the tree enabling it to si-
multaneously reason at multiple granularities of en-
tity compression. Third, the trees can provide split
points for finer-grained entities by placing contex-
tually similar mentions under the same subtree. Fi-
nally, if memory is limited, redundant mentions can
be pruned by replacing subtrees with their roots.
Empirically, we demonstrate that our model is
several orders of magnitude faster than a pairwise
model, allowing us to perform efficient coreference
on nearly six million author mentions in under four
hours using a single CPU.
2 Background: Pairwise Coreference
Coreference is the problem of clustering mentions
such that mentions in the same set refer to the same
real-world entity; it is also known as entity disam-
biguation, record linkage, and de-duplication. For
example, in author coreference, each mention might
be represented as a record extracted from the author
field of a textual citation or BibTeX record. The
mention record may contain attributes for the first,
middle, and last name of the author, as well as con-
textual information occurring in the citation string,
co-authors, titles, topics, and institutions. The goal
is to cluster these mention records into sets, each
containing all the mentions of the author to which
they refer; we use this task as a running pedagogical
example.
Let M be the space of observed mention records;
then the traditional pairwise coreference approach
scores candidate coreference solutions with a com-
patibility function ? : M ? M ? < that mea-
sures how likely it is that the two mentions re-
fer to the same entity.1 In discriminative log-
1We can also include an incompatibility function for when
linear models, the function ? takes the form of
weights ? on features ?(mi,mj), i.e., ?(mi,mj) =
exp (? ? ?(mi,mj)). For example, in author coref-
erence, the feature functions ? might test whether
the name fields for two author mentions are string
identical, or compute cosine similarity between the
two mentions? bags-of-words, each representing a
mention?s context. The corresponding real-valued
weights ? determine the impact of these features on
the overall pairwise score.
Coreference can be solved by introducing a set of
binary coreference decision variables for each men-
tion pair and predicting a setting to their values that
maximizes the sum of pairwise compatibility func-
tions. While it is possible to independently make
pairwise decisions and enforce transitivity post hoc,
this can lead to poor accuracy because the decisions
are tightly coupled. For higher accuracy, a graphi-
cal model such as a conditional random field (CRF)
is constructed from the compatibility functions to
jointly reason about the pairwise decisions (McCal-
lum and Wellner, 2004). We now describe the pair-
wise CRF for coreference as a factor graph.
2.1 Pairwise Conditional Random Field
Each mention mi ? M is an observed variable, and
for each mention pair (mi,mj) we have a binary
coreference decision variable yij whose value de-
termines whether mi and mj refer to the same en-
tity (i.e., 1 means they are coreferent and 0 means
they are not coreferent). The pairwise compatibility
functions become the factors in the graphical model.
Each factor examines the properties of its mention
pair as well as the setting to the coreference decision
variable and outputs a score indicating how likely
the setting of that coreference variable is. The joint
probability distribution over all possible settings to
the coreference decision variables (y) is given as a
product of all the pairwise compatibility factors:
Pr(y|m) ?
n?
i=1
n?
j=1
?(mi,mj , yij) (1)
Given the pairwise CRF, the problem of coreference
is then solved by searching for the setting of the
coreference decision variables that has the highest
probability according to Equation 1 subject to the
the mentions are not coreferent, e.g., ? :M?M?{0, 1} ? <
381
Jamie,Callan, Jamie,Callan,
J.,Callan,
J.,Callan, J.,Callan,
J.,Callan, Jamie,Callan, Jamie,Callan,
v,Jamie,Callan,
J.,Callan,
v,v,v,
J.,Callan,J.,Callan, J.,Callan,
J.,Callan,
Jamie,Callan,
Figure 2: Pairwise model on six mentions: Open
circles are the binary coreference decision variables,
shaded circles are the observed mentions, and the
black boxes are the factors of the graphical model
that encode the pairwise compatibility functions.
constraint that the setting to the coreference vari-
ables obey transitivity;2 this is the maximum proba-
bility estimate (MPE) setting. However, the solution
to this problem is intractable, and even approximate
inference methods such as loopy belief propagation
can be difficult due to the cubic number of determin-
istic transitivity constraints.
2.2 Approximate Inference
An approximate inference framework that has suc-
cessfully been used for coreference models is
Metropolis-Hastings (MH) (Milch et al (2006), Cu-
lotta and McCallum (2006), Poon and Domingos
(2007), amongst others), a Markov chain Monte
Carlo algorithm traditionally used for marginal in-
ference, but which can also be tuned for MPE in-
ference. MH is a flexible framework for specify-
ing customized local-search transition functions and
provides a principled way of deciding which local
search moves to accept. A proposal function q takes
the current coreference hypothesis and proposes a
new hypothesis by modifying a subset of the de-
cision variables. The proposed change is accepted
with probability ?:
? = min
(
1,
P r(y?)
Pr(y)
q(y|y?)
q(y?|y)
)
(2)
2We say that a full assignment to the coreference variables
y obeys transitivity if ? ijk yij = 1 ? yjk = 1 =? yik = 1
When using MH for MPE inference, the second term
q(y|y?)/q(y?|y) is optional, and usually omitted.
Moves that reduce model score m y be accepted and
an optional temperature can be used for annealing.
The primary advantages of MH for coreference are
(1) only the compatibility functions of the changed
decision variables need to be evaluated to ccept a
move, and (2) the proposal function can enforce the
transitivity constraint by exploring only variable set-
tings that result in valid coreference partitionings.
A commonly used propos l distribution for coref-
erence is the following: (1) randomly select two
mentions (mi,mj), (2) if the mentions (mi,mj) are
in the same entity cluster according to y then move
one mention into a singleton cluster (by setting the
necessary decision variables to 0), otherwise, move
mention mi so it is in the same cluster as mj (by
setting the necessary decision variables). Typically,
MH is employed by first initializing to a singleton
configuration (all entities have one mention), and
then executing the MH for a certain number of steps
(or until the predicted coreference hypothesis stops
changing).
This proposal distribution always moves a sin-
gle mention m from some entity ei to another en-
tity ej and thus the configuration y and y? only dif-
fer by the setting of decision variables governing to
which entity m refers. In order to guarantee transi-
tivity and a valid coreference equivalence relation,
we must properly remove m from ei by untethering
m from each mention in ei (this requires computing
|ei| ? 1 pairwise factors). Similarly?again, for the
sake of transitivity?in order to complete the move
into ej we must coref m to each mention in ej (this
requires computing |ej | pairwise factors). Clearly,
all the other coreference decision variables are in-
dependent and so their corresponding factors can-
cel because they yield the same scores under y and
y?. Thus, evaluating each proposal for the pairwise
model scales linearly with the number of mentions
assigned to the entities, requiring the evaluation of
2(|ei|+ |ej | ? 1) compatibility functions (factors).
3 Hierarchical Coreference
Instead of only capturing a single coreference clus-
tering between mention pairs, we can imagine mul-
tiple levels of coreference decisions over different
382
granularities. For example, mentions of an author
may be further partitioned into semantically similar
sets, such that mentions from each set have topically
similar papers. This partitioning can be recursive,
i.e., each of these sets can be further partitioned, cap-
turing candidate splits for an entity that can facilitate
inference. In this section, we describe a model that
captures arbitrarily deep hierarchies over such lay-
ers of coreference decisions, enabling efficient in-
ference and rich entity representations.
3.1 Discriminative Hierarchical Model
In contrast to the pairwise model, where each en-
tity is a flat cluster of mentions, our proposed model
structures each entity recursively as a tree. The
leaves of the tree are the observed mentions with
a set of attribute values. Each internal node of the
tree is latent and contains a set of unobserved at-
tributes; recursively, these node records summarize
the attributes of their child nodes (see Figure 1), for
example, they may aggregate the bags of context
words of the children. The root of each tree repre-
sents the entire entity, with the leaves containing its
mentions. Formally, the coreference decision vari-
ables in the hierarchical model no longer represent
pairwise decisions directly. Instead, a decision vari-
able yri,rj = 1 indicates that node-record rj is the
parent of node-record ri. We say a node-record ex-
ists if either it is a mention, has a parent, or has at
least one child. Let R be the set of all existing node
records, let rp denote the parent for node r, that is
yr,rp = 1, and ?r? 6= rp, yr,r? = 0. As we describe
in more detail later, the structure of the tree and the
values of the unobserved attributes are determined
during inference.
In order to represent our recursive model of coref-
erence, we include two types of factors: pairwise
factors ?pw that measure compatibility between a
child node-record and its parent, and unit-wise fac-
tors ?rw that measure compatibilities of the node-
records themselves. For efficiency we enforce that
parent-child factors only produce a non-zero score
when the corresponding decision variable is 1. The
unit-wise factors can examine compatibility of set-
tings to the attribute variables for a particular node
(for example, the set of topics may be too diverse
to represent just a single entity), as well as enforce
priors over the tree?s breadth and depth. Our recur-
sive hierarchical model defines the probability of a
configuration as:
Pr(y, R|m) ?
?
r?R
?rw(r)?pw(r, r
p) (3)
3.2 MCMC Inference for Hierarchical models
The state space of our hierarchical model is substan-
tially larger (theoretically infinite) than the pairwise
model due to the arbitrarily deep (and wide) latent
structure of the cluster trees. Inference must simul-
taneously determine the structure of the tree, the la-
tent node-record values, as well as the coreference
decisions themselves.
While this may seem daunting, the structures be-
ing inferred are actually beneficial to inference. In-
deed, despite the enlarged state space, inference
in the hierarchical model is substantially faster
than a pairwise model with a smaller state space.
One explanatory intuition comes from the statisti-
cal physics community: we can view the latent tree
as auxiliary variables in a data-augmentation sam-
pling scheme that guide MCMC through the state
space more efficiently. There is a large body of lit-
erature in the statistics community describing how
these auxiliary variables can lead to faster conver-
gence despite the enlarged state space (classic exam-
ples include Swendsen and Wang (1987) and slice
samplers (Neal, 2000)).
Further, evaluating each proposal during infer-
ence in the hierarchical model is substantially faster
than in the pairwise model. Indeed, we can replace
the linear number of factor evaluations (as in the
pairwise model) with a constant number of factor
evaluations for most proposals (for example, adding
a subtree requires re-evaluating only a single parent-
child factor between the subtree and the attachment
point, and a single node-wise factor).
Since inference must determine the structure of
the entity trees in addition to coreference, it is ad-
vantageous to consider multiple MH proposals per
sample. Therefore, we employ a modified variant
of MH that is similar to multi-try Metropolis (Liu
et al, 2000). Our modified MH algorithm makes k
proposals and samples one according to its model
ratio score (the first term in Equation 2) normalized
across all k. More specificaly, for each MH step, we
first randomly select two subtrees headed by node-
383
records ri and rj from the current coreference hy-
pothesis. If ri and rj are in different clusters, we
propose several alternate merge operations: (also in
Figure 3):
? Merge Left - merges the entire subtree of rj into
node ri by making rj a child of ri
?Merge Entity Left - merges rj with ri?s root
?Merge Left and Collapse - merges rj into ri then
performs a collapse on rj (see below).
? Merge Up - merges node ri with node rj by cre-
ating a new parent node-record variable rp with ri
and rj as the children. The attribute fields of rp are
selected from ri and rj .
Otherwise ri and rj are subtrees in the same entity
tree, then the following proposals are used instead:
? Split Right - Make the subtree rj the root of a new
entity by detaching it from its parent
? Collapse - If ri has a parent, then move ri?s chil-
dren to ri?s parent and then delete ri.
? Sample attribute - Pick a new value for an at-
tribute of ri from its children.
Computing the model ratio for all of coreference
proposals requires only a constant number of com-
patibility functions. On the other hand, evaluating
proposals in the pairwise model requires evaluat-
ing a number of compatibility functions equal to the
number of mentions in the clusters being modified.
Note that changes to the attribute values of the
node-record and collapsing still require evaluating
a linear number of factors, but this is only linear in
the number of child nodes, not linear in the number
of mentions referring to the entity. Further, attribute
values rarely change once the entities stabilize. Fi-
nally, we incrementally update bags during corefer-
ence to reflect the aggregates of their children.
4 Experiments: Author Coreference
Author coreference is a tremendously important
task, enabling improved search and mining of sci-
entific papers by researchers, funding agencies, and
governments. The problem is extremely difficult due
to the wide variations of names, limited contextual
evidence, misspellings, people with common names,
lack of standard citation formats, and large numbers
of mentions.
For this task we use a publicly available collec-
tion of 4,394 BibTeX files containing 817,193 en-
tries.3 We extract 1,322,985 author mentions, each
containing first, middle, last names, bags-of-words
of paper titles, topics in paper titles (by running la-
tent Dirichlet alocation (Blei et al, 2003)), and last
names of co-authors. In addition we include 2,833
mentions from the REXA dataset4 labeled for coref-
erence, in order to assess accuracy. We also include
?5 million mentions from DBLP.
4.1 Models and Inference
Due to the paucity of labeled training data, we did
not estimate parameters from data, but rather set
the compatibility functions manually by specifying
their log scores. The pairwise compatibility func-
tions punish a string difference in first, middle, and
last name, (?8); reward a match (+2); and reward
matching initials (+1). Additionally, we use the co-
sine similarity (shifted and scaled between ?4 and
4) between the bags-of-words containing title to-
kens, topics, and co-author last names. These com-
patibility functions define the scores of the factors
in the pairwise model and the parent-child factors
in the hierarchical model. Additionally, we include
priors over the model structure. We encourage each
node to have eight children using a per node factor
having score 1/(|number of children?8|+1), manage
tree depth by placing a cost on the creation of inter-
mediate tree nodes ?8 and encourage clustering by
placing a cost on the creation of root-level entities
?7. These weights were determined by just a few
hours of tuning on a development set.
We initialize the MCMC procedures to the single-
ton configuration (each entity consists of one men-
tion) for each model, and run the MH algorithm de-
scribed in Section 2.2 for the pairwise model and
multi-try MH (described in Section 3.2) for the hi-
erarchical model. We augment these samplers us-
ing canopies constructed by concatenating the first
initial and last name: that is, mentions are only
selected from within the same canopy (or block)
to reduce the search space (Bilenko et al, 2006).
During the course of MCMC inference, we record
the pairwise F1 scores of the labeled subset. The
source code for our model is available as part of the
FACTORIE package (McCallum et al, 2009, http:
3http://www.iesl.cs.umass.edu/data/bibtex
4http://www2.selu.edu/Academics/Faculty/
aculotta/data/rexa.html
384
!"#!$#
!%#
!"#!$# !"#!$# !$#
&"#
!"#!$#
&"#&"#
!"#$%&'()%)*' +*,-*'.*/' +*,-*'0"$)1'.*/' +*,-*'23' +*,-*'.*/'%"4'56&&%3(*'
!"#
!"#$%&'7)%)*'
&"#&$#
!'"#
&"#&$#
!'"#
73&#)8#-9)'
&$# !'"#
56&&%3(*'
Figure 3: Example coreference proposals for the case where ri and rj are initially in different clusters.
//factorie.cs.umass.edu/).
4.2 Comparison to Pairwise Model
In Figure 4a we plot the number of samples com-
pleted over time for a 145k subset of the data. Re-
call that we initialized to the singleton configuration
and that as the size of the entities grows, the cost of
evaluating the entities in MCMC becomes more ex-
pensive. The pairwise model struggles with the large
cluster sizes while the hierarchical model is hardly
affected. Even though the hierarchical model is eval-
uating up to four proposals for each sample, it is still
able to sample much faster than the pairwise model;
this is expected because the cost of evaluating a pro-
posal requires evaluating fewer factors. Next, we
plot coreference F1 accuracy over time and show in
Figure 5a that the prolific sampling rate of the hierar-
chical model results in faster coreference. Using the
plot, we can compare running times for any desired
level of accuracy. For example, on the 145k men-
tion dataset, at a 60% accuracy level the hierarchical
model is 19 times faster and at 90% accuracy it is
31 times faster. These performance improvements
are even more profound on larger datasets: the hi-
erarchical model achieves a 60% level of accuracy
72 times faster than the pairwise model on the 1.3
million mention dataset, reaching 90% in just 2,350
seconds. Note, however, that the hierarchical model
requires more samples to reach a similar level of ac-
curacy due to the larger state space (Figure 4b).
4.3 Large Scale Experiments
In order to demonstrate the scalability of the hierar-
chical model, we run it on nearly 5 million author
mentions from DBLP. In under two hours (6,700
seconds), we achieve an accuracy of 80%, and in
under three hours (10,600 seconds), we achieve an
accuracy of over 90%. Finally, we combine DBLP
with BibTeX data to produce a dataset with almost 6
million mentions (5,803,811). Our performance on
this dataset is similar to DBLP, taking just 13,500
seconds to reach a 90% accuracy.
5 Related Work
Singh et al (2011) introduce a hierarchical model
for coreference that treats entities as a two-tiered
structure, by introducing the concept of sub-entities
and super-entities. Super-entities reduce the search
space in order to propose fruitful jumps. Sub-
entities provide a tighter granularity of coreference
and can be used to perform larger block moves dur-
ing MCMC. However, the hierarchy is fixed and
shallow. In contrast, our model can be arbitrarily
deep and wide. Even more importantly, their model
has pairwise factors and suffers from the quadratic
curse, which they address by distributing inference.
The work of Rao et al (2010) uses streaming
clustering for large-scale coreference. However, the
greedy nature of the approach does not allow errors
to be revisited. Further, they compress entities by
averaging their mentions? features. We are able to
provide richer entity compression, the ability to re-
visit errors, and scale to larger data.
Our hierarchical model provides the advantages
of recently proposed entity-based coreference sys-
tems that are known to provide higher accuracy
(Haghighi and Klein, 2007; Culotta et al, 2007;
Yang et al, 2008; Wick et al, 2009; Haghighi and
Klein, 2010). However, these systems reason over a
single layer of entities and do not scale well.
Techniques such as lifted inference (Singla and
Domingos, 2008) for graphical models exploit re-
dundancy in the data, but typically do not achieve
any significant compression on coreference data be-
385
Samples versus Time
0 500 1,000 1,500 2,000
Running time (s)
0
50,000
100,000
150,000
200,000
250,000
300,000
350,000
400,000
Nu
mb
er o
f Sa
mp
les
Hierar Pairwise
(a) Sampling Performance
Accuracy versus Samples
0 50,000 100,000 150,000 200,000
Number of Samples
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
F1 
Acc
ura
cy
Hierar Pairwise
(b) Accuracy vs. samples (convergence accuracy as dashes)
Figure 4: Sampling Performance Plots for 145k mentions
Accuracy versus Time
0 250 500 750 1,000 1,250 1,500 1,750 2,000
Running time (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
F1 
Ac
cur
acy
Hierar Pairwise
(a) Accuracy vs. time (145k mentions)
Accuracy versus Time
0 10,000 20,000 30,000 40,000 50,000 60,000
Running time (s)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
F1 
Acc
ura
cy
Hierar Pairwise
(b) Accuracy vs. time (1.3 million mentions)
Figure 5: Runtime performance on two datasets
cause the observations usually violate any symmetry
assumptions. On the other hand, our model is able
to compress similar (but potentially different) obser-
vations together in order to make inference fast even
in the presence of asymmetric observed data.
6 Conclusion
In this paper we present a new hierarchical model
for large scale coreference and demonstrate it on
the problem of author disambiguation. Our model
recursively defines an entity as a summary of its
children nodes, allowing succinct representations of
millions of mentions. Indeed, inference in the hier-
archy is orders of magnitude faster than a pairwise
CRF, allowing us to infer accurate coreference on
six million mentions on one CPU in just 4 hours.
7 Acknowledgments
We would like to thank Veselin Stoyanov for his feed-
back. This work was supported in part by the CIIR, in
part by ARFL under prime contract #FA8650-10-C-7059,
in part by DARPA under AFRL prime contract #FA8750-
09-C-0181, and in part by IARPA via DoI/NBC contract
#D11PC20152. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation thereon.
Any opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
386
References
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: annotations, experiments, and ob-
servations. In Proceedings of the Workshop on Coref-
erence and its Applications, CorefApp ?99, pages 1?8,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
Dirichlet model for unsupervised entity resolution. In
SDM.
Mikhail Bilenko, Beena Kamath, and Raymond J.
Mooney. 2006. Adaptive blocking: Learning to scale
up record linkage. In Proceedings of the Sixth Interna-
tional Conference on Data Mining, ICDM ?06, pages
87?96, Washington, DC, USA. IEEE Computer Soci-
ety.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal on Machine
Learning Research, 3:993?1022.
Aron Culotta and Andrew McCallum. 2006. Prac-
tical Markov logic containing first-order quantifiers
with application to identity uncertainty. In Human
Language Technology Workshop on Computationally
Hard Problems and Joint Inference in Speech and Lan-
guage Processing (HLT/NAACL), June.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL HLT).
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 848?855.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Mauricio A. Herna?ndez and Salvatore J. Stolfo. 1995.
The merge/purge problem for large databases. In Pro-
ceedings of the 1995 ACM SIGMOD international
conference on Management of data, SIGMOD ?95,
pages 127?138, New York, NY, USA. ACM.
Jun S. Liu, Faming Liang, and Wing Hung Wong. 2000.
The multiple-try method and local optimization in
metropolis sampling. Journal of the American Statis-
tical Association, 96(449):121?134.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems (NIPS).
Andrew McCallum, Kamal Nigam, and Lyle Ungar.
2000. Efficient clustering of high-dimensional data
sets with application to reference matching. In In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 169?178.
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In Neural Informa-
tion Processing Systems (NIPS).
Brian Milch, Bhaskara Marthi, and Stuart Russell. 2006.
BLOG: Relational Modeling with Unknown Objects.
Ph.D. thesis, University of California, Berkeley.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31:705?767.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Annual Meeting of the Association for Computational
Linguistics (ACL).
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In AAAI Conference
on Artificial Intelligence, pages 913?918.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2 - Volume 2, EMNLP
?09, pages 968?977, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In International Conference on Computa-
tional Linguistics (COLING), pages 1050?1058, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Yael Ravin and Zunaid Kazi. 1999. Is Hillary Rodham
Clinton the president? disambiguating names across
documents. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 9?16.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sameer Singh, Michael L. Wick, and Andrew McCallum.
2010. Distantly labeling data for large scale cross-
document coreference. Computing Research Reposi-
tory (CoRR), abs/1005.4298.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2011. Large-scale cross-
document coreference using distributed inference and
hierarchical models. In Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT).
387
Parag Singla and Pedro Domingos. 2005. Discrimina-
tive training of Markov logic networks. In AAAI, Pitts-
burgh, PA.
Parag Singla and Pedro Domingos. 2008. Lifted first-
order belief propagation. In Proceedings of the 23rd
national conference on Artificial intelligence - Volume
2, AAAI?08, pages 1094?1099. AAAI Press.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. Comput. Linguist.,
27(4):521?544.
R.H. Swendsen and J.S. Wang. 1987. Nonuniversal crit-
ical dynamics in MC simulations. Phys. Rev. Lett.,
58(2):68?88.
Michael Wick, Aron Culotta, Khashayar Rohanimanesh,
and Andrew McCallum. 2009. An entity-based model
for coreference resolution. In SIAM International
Conference on Data Mining (SDM).
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In Association for Computational Linguistics,
pages 843?851.
388
