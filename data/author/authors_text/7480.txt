Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 345?352
Manchester, August 2008
Modeling Chinese Documents 
 with Topical Word-Character Models  
Wei Hu1           Nobuyuki Shimizu2 
1Department of Computer Science 
Shanghai Jiao Tong University 
Shanghai, China 200240 
{no_bit,hysheng} 
@sjtu.edu.cn 
Hiroshi Nakagawa2           Huanye Sheng1 
2Information Technology Center 
The University of Tokyo 
Tokyo, Japan 113-0033 
{shimizu, nakagawa} 
@r.dl.itc.u-tokyo.ac.jp 
Abstract 
As Chinese text is written without word 
boundaries, effectively recognizing Chi-
nese words is like recognizing colloca-
tions in English, substituting characters 
for words and words for collocations. 
However, existing topical models that in-
volve collocations have a common limi-
tation. Instead of directly assigning a top-
ic to a collocation, they take the topic of 
a word within the collocation as the topic 
of the whole collocation. This is unsatis-
factory for topical modeling of Chinese 
documents. Thus, we propose a topical 
word-character model (TWC), which al-
lows two distinct types of topics: word 
topic and character topic. We evaluated 
TWC both qualitatively and 
quantitatively to show that it is a power-
ful and a promising topic model.  
1 Introduction 
Topic models (Blei et al, 2003; Griffiths & 
Steyvers 2004, 2007) are a class of statistical 
models in which documents are expressed as 
mixtures of topics, where a topic is a probability 
distribution over words. A topic model is a gen-
erative model for documents: it specifies a prob-
abilistic procedure for generating documents. To 
make a new document, we choose a distribution 
over topics. Then, for each word in this docu-
ment, we randomly select a topic from the distri-
bution, and draw a word from the topic. Once we 
have a topic model, we can invert the generating 
process, inferring the set of topics that was re-
sponsible  for  generating  a  collection  of  docu- 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
ments.  
Although most topic models treat a document 
as a bag-of-words, the assumption has obvious 
shortcomings. Suppose there are many docu-
ments about art and musicals in New York. Then, 
we find a topic represented by words such as 
?art?, ?musical?, and ?New?, instead of getting 
?New York?. The bag-of-words assumption 
makes the topic model split a collocation?a 
phrase with meaning beyond the individual 
words?into individual words that have a differ-
ent meaning. One example of a collocation is the 
phrase ?white house?. In politics, it carries a spe-
cial meaning beyond a house that is white, 
whereas ?yellow house? does not.  
While it is reasonable for English, the equiva-
lent bag-of-characters assumption is especially 
troublesome for modeling Chinese documents, 
where almost all basic vocabularies are the equi-
valents of English collocations. In Chinese, some 
of the most commonly used couple-of-thousand 
characters are combined to make up a word, and 
no word boundary is given in the text. 
Effectively, Chinese words are like collocations 
in English. The difficulty is that there are over-
whelmingly more of them, enough to render a 
bag-of-character assumption unreasonable for 
Chinese. Therefore, a topical model for Chinese 
should be capable of detecting the boundary be-
tween two words, as well as assigning a topic to 
each word. 
While topic models for Chinese documents 
bear some similarity to collocation models in 
English, existing topical collocation discovery 
models, such as the LDA (latent Dirichelt alloca-
tion) Collocation model (LDACOL) (Griffiths et 
al., 2007) and the topical N-gram model (TNG) 
(Wang et al, 2007), do not directly assign a topic 
to a collocation. These models find the bounda-
ries of phrases and assign a topic to each word. 
The problem is in the next step?the topic of the 
collocation is exactly the same as one of the 
words. This is like saying that the topic of ?white 
345
house? is the same as either that of ?white? or 
?house?. We propose a new topical model, the 
topical word-character model (TWC), which 
aims to overcome these limitations.  
We evaluated the model both quantitatively 
and qualitatively. For the quantitative analysis, 
we compared the performance of TWC and TNG 
using a standard measure perplexity. For the 
qualitative analysis, we evaluated TWC?s ability 
to discover Chinese words and assign topics in 
comparison with TNG. 
The rest of the paper is organized as follows. 
Section 2 reviews topic models that aim to in-
clude collocations explicitly in the model and 
analyzes their limitations. Section 3 presents our 
new model TWC. Section 4 gives details of our 
consideration on inference for TWC. Section 5 
presents our qualitative and quantitative experi-
ments. Section 6 concludes with a summary and 
briefly mentions future work.  
2 Topic Models for Collocation Discov-
ery 
Since Chinese word discovery is similar to 
English collocation discovery, we first review 
some related topic models for collocation 
discovery. 
Although collocation discovery has long been 
studied, most methods are based on frequency or 
variance. LDACOL is an attempt to model collo-
cations in a topical scheme. Starting from the 
LDA topic model, LDACOL introduces special 
random variables xG . Variable xi = 1 implies that 
the  corresponding  word  wi  and  previous  word 
wi-1 belong to the same phrase, while xi = 0 im-
plies otherwise. Thus, LDACOL can decide the 
length of a phrase dynamically.  
TNG is a powerful generalization of LDACOL. 
Its graphical model is shown in Figure 1. 
 
Figure 1: Topical n-gram model. 
The model is defined in terms of three sets of  
variables: a sequence of words w
G
, a sequence of 
topics z
G
, and a sequence of indicators x
G
. TNG 
assumes the following generative process for 
documents. 
1. For each document d, draw ?d ~ 
Dirichlet(?). 
2. For each topic z, draw ?z ~ Dirichlet(?). 
3. For each topic z and each word w, draw ?zw 
~ Dirichlet(?).  
4. For each topic z and each word w, draw ?zw 
~ Beta(?). 
5. For each word wd,,i in document d: 
(a) draw xd,,i ~ Bernoulli(
d ,i 1 d ,i 1z ,w
? ? ? ), 
(b) draw zd,,i ~ Discrete(?d), 
(c) draw wd,,i ~ Discrete(
d ,iz
? )  if xd,,i=0, 
 draw wd,,i ~ Discrete(
d ,i d ,i 1z ,w
? ? )  if xd,,i=1, 
where ?, ?, ? are Dirichlet priors and ? is a Beta 
prior, zd,i denotes the ith topic assignment in 
document d, wd,i denotes the ith word in document 
d, and xd,i denotes the indicator between wd,i-1 and 
wd,i. Note that the variable xd,i = 1 implies that 
word wd,i-1 and its neighbor wd,i belong to the 
same phrase, while xd,i = 0 implies otherwise. 
However, the topics assigned to them (zd,i-1 and 
zd,i) are not required to be identical to each other. 
To decide the topic of a phrase, we can simply 
take the first (or last) word?s topic or the most 
common topic in the phrase. The authors of TNG 
prefer to choose the last word?s topic as the 
phrase topic because the last noun in a colloca-
tion is usually the ?head noun? in English.  
However, this simple strategy may be ineffec-
tive when we apply TNG to Chinese documents. 
The topics of ???? (game) and ????? 
(tournament) should be represented by their last 
characters while those of ???? (farmer) and 
???? (agriculture) should be represented by 
their first characters. And occasionally, the topic 
of a Chinese word is not identical to any topic of 
its component characters. For example ???? 
(Bluetooth) is neither a color nor a tooth.  
To overcome the limitation of TNG, we must 
discard its underlying assumption: that the topic 
of a whole word is the same as the topic of at 
least one of its components.  
3 Modeling Word Topic and Character 
Topic 
This section describes our topical word-character 
model (TWC), which models two distinct types 
of topics: word topic and character topic. 
 
? ?? ? 
    
? ? 
?? 
    
??? 
??? 
??? 
???
???
???
zi+1 zi+2 zi zi-1 
xi+2 xi+3xi-1xi xi-1 
  wi+1 wi+2 wi wi-1 
346
3.1 Word topic and character topic 
To solve the problem associated with the ??
?? (Bluetooth) example, we need to distinguish 
between the topics of characters and words. 
Therefore, we introduce a new type of topic for 
words in addition to the topics assigned to char-
acters. When generating a Chinese character, we 
first draw a word topic and then choose a charac-
ter topic. A schematic description of this model 
is shown in Figure 2.  
 
Figure 2: Schematic description of modeling 
Chinese documents with character and word top-
ics.  
Here, we use random variables z
G
 and t
G
 to 
denote word and character topics, respectively. 
Note that the word topic and character topics 
have a hierarchical tree-like structure (upper 
layer in Figure 2), whereas character topics and 
characters form a hidden Markov model (HMM) 
(lower layer in Figure 2). 
3.2 Topical word-character model (TWC)  
There are some indicators in the upper-right 
corner of each character topic in Figure 2. They 
help us to tell whether the current character be-
longs to the same word as the previous one. Now 
the question left is how to probabilistically draw 
these indicators, i.e., how to determine the length 
of the Markov chain. 
There are two ways to set the values of the in-
dicators. One is similar to that applied in the hid-
den semi-Markov model (HSMM), which gener-
ates the duration of a segment from the state. Ac-
cordingly, we could first choose the length of a 
word from the distribution associated with the 
word topic and then assign 0 or 1 to each indica-
tor. The other method is to directly draw indica-
tors from the distribution associated with the 
previous character and topic, just as LDACOL 
and TNG do. The difference between these two 
methods is that the former determines the length 
of a word in advance while the latter increases 
the length dynamically.  
We  prefer the  second choice  because it takes 
into consideration a lot of context information. In 
fact, our experimental results indicate that it has 
better performance. 
The formal definition of our model with word 
and character topics is as follows.  
 
Figure 3: Topical word-character model.  
TWC has four sets of variables: a sequence of 
characters c
G
, a sequence of character topics t
G
, a 
sequence of word topics z
G
, and a sequence of 
indicators x
G
. A document is generated via the 
following procedure.  
1. For each document d, draw ?d ~ 
Dirichlet(?);  
2. For each word topic z, draw ?z ~ 
Dirichlet(?); 
3. For each word topic z and each character 
topic t, draw ?zt ~ Dirichlet(?);  
4. For each word topic z, each character topic t 
and each character c, draw ?ztc ~ Beta(?); 
5. For each character topic t, draw ?t ~ 
Dirichlet(?); 
6. For each character cd,,i in document d: 
(a) draw xd,,i ~ Bernoulli( ? ? ?d ,i 1 d ,i 1 d ,i 1z ,t ,c? ); 
(b) draw zd,,i ~ Discrete (?d)  if xd,,i=0; 
     zd,,i= zd,,i-1    if xd,,i=1; 
(c) draw td,,i ~ Discrete(
d ,iz
? ) if xd,,i=0; 
    draw td,,i ~ Discrete( ?d ,i d ,i 1z ,t? )  if xd,,i=1; 
(d) draw cd,,i ~ Discrete(
d ,it
? ). 
Here, ?, ?, ?, ? are Dirichlet priors and ? is a 
Beta prior, zd,i denotes the ith word topic assignment 
in document d, td,i denotes the ith character topic as-
signment in document d, cd,i denotes the ith character 
in document d, and xd,i denotes the indicator between 
cd,i-1 and cd,i. 
Note that compared with the schematic model 
in Figure 2, each character has its corresponding 
Character
 topic
Word
 topic
 
? 
 
   
z2 z1 
t13 t21 t12 t11 
c13 c21 c12 c11 
 
 
t22 
c22 
1 1 0 1 0 
? ?? ?
 
  
??
?? ? ? 
  
??? 
??? 
??? 
??? 
??? 
???
??? 
??? 
  
zi+1 zi+2 zizi-1
ti+1 ti+2 titi-1
ci+1 ci+2 cici-1
xi+2 xi+3 xi-1xixi-1
347
word topic in the TWC model. This is because 
we cannot decide how many words there will be 
in a document and how many characters there 
will be in a certain word in advance. In other 
words, the structure of the ideal model is not 
fixed. Therefore, we duplicate word topic vari-
ables for each character. 
4 Inference with TWC 
Many approximate inference techniques such as 
variational methods, expectation propagation, 
and Gibbs sampling can be applied to graphical 
models. We use Gibbs sampling to perform our 
Bayesian inference in TWC.  
Gibbs sampling is a simple and widely appli-
cable Markov chain Monte Carlo (MCMC) algo-
rithm. In a traditional procedure, variables are 
sequentially sampled from their distributions 
conditioned on all other variables in the model. 
An extension of the basic approach is to 
choose blocks of variables first and then sample 
jointly from the variables in each block in turn, 
conditioned on the remaining variables; this is 
called blocking Gibbs sampling.  
When sampling for TWC, we separate vari-
ables into three types of blocks in the following 
manner (as shown in Figure 4). 
1. character variables ti 
2. indicators xi, whose value is 1 after n itera-
tions 
3. word topics zi, zi+1, ?, zi+l-1 and indicator xi, 
satisfying xi=xi+l=1 and xj=0 (j from i to i+l-1) 
after n iterations 
 
Figure 4: Illustration of partition in a certain it-
eration.  
Note that variables , , ,? ? ? ?
G G GG
 and ?
G
are not 
sampled. This is because we can integrate them 
out according to their conjugate priors. We only 
need to sample variables z
G
, x
G
, and t
G
. 
Before discussing the inference of conditional 
probabilities, let us analyze our partition strategy 
in detail. We will explain the reasons for (1) 
sampling zi, zi+1, ?, zi+l-1 together and (2) sam-
pling z
G
 and xi together 
1. Why do we sample zi, zi+1, ?, zi+l-1 together? 
Assume that we draw zi, zi+1, ?, zi+l-1 one by 
one, and it is now time to sample zi+1 according 
to the conditional probability 
( )( | , , , , , , , , )i 1 i 1P z j z x t c ? ? ? ? ?+ ? += GG GG , 
where ( )i 1z? +
G
denotes a word topic except i 1z + . 
Recall step 6-b in the generative TWC model: it 
says ?If xd,,i=1, then zd,,i= zd,,i-1?, which implies  
( | , ) ( )i 1 i i 1 i 1 iP z z x 1 I z z+ + += = = . 
As this probability is a factorial of the target 
probability, it follows that   
( )( | , , , , , , , , )i 1 i 1P z j z x t c ? ? ? ? ? 0+ ? += =GG GG   
for all ij z? . In other words, zi+1 should be equal 
to zi and not change during sampling.  
It seems that step 6-b in the generative model 
causes the problem. But supposing that we do not 
set zi+1 to zi; it is still more reasonable to sample 
z
G
 together. According to our partition principle, 
xi, xi+1, ?, xi+l-1, xi+l is a continuous indicator 
sequence whose head and tail are both 0 and the 
rest are 1, which implies that character string ci, 
ci+1, ?, ci+l-1 forms a word and has the same 
word topic. Recall the schematic model in Figure 
2: the word topic and character topics have a 
tree-like structure and each word has only one 
word topic node. We add some auxiliary dupli-
cates just because the ideal model is not fixed. 
Therefore, it is natural to sample the word topic 
together with its duplicates.  
2. Why do we sample z
G
 and xi together? 
Let us consider the probability of converting xi 
from 0 to 1 in the current sampling iteration. As-
sume that the number of word topics is 3, zi-1=2, 
and 
( ... | ) / ( ) ,
( | ... ) / ( ) ,
i i l 1 i
i i i l 1
P z z j x 0 1 3 1 j 3
P x k z z 2 1 2 0 k 1
+ ?
+ ?
= = = = = ? ?
= = = = = ? ?
where other variables and priors are omitted. If 
we first sample z
G
 and next sample xi, then the 
probability of drawing 1 for xi is 1/6, according 
to the multiplication principle. If we sample z
G
 
and xi together, the probability of drawing 1 for xi 
is ( ... , )i i l 1 iP z z 2 x 1+ ?= = = = . 
Since 
( ... , )
( ... , )
( ... , ) ( )
1 3
i i l 1 i
k 0 j 1
3
i i l 1 i
j 1
i i l 1 i i 1
1 P z z j x k
P z z j x 0
z z 2 x 1 z 2
+ ?
= =
+ ?
=
+ ? ?
=
==
= = = = =
= = = = =
+ = = = = =
??
?  
and  
( ... , )i i l 1 iP z z 2 x 1+ ?= = = =  
  
1 0 1 1 0
    
???
???
???ti ti+2 ti-1 ti-2 
??? 
zi zi+1 zi-1 zi-2 zi+2 
ti+2 
??? 
???  
0 
 
348
( ... , )
( ... , ) ( )
i i l 1 i
i i l 1 i
P z z 2 x 0
P z z j x 0 1 j 3
+ ?
+ ?
= = = = =
= = = = = ? ? , 
we get  
( ... , ) / /i i l 1 iP z z 2 x 1 1 4 1 6+ ?= = = = = > . 
In conclusion, the model is more likely to form 
long words, if we sample z
G
and xi together. This 
is preferred because both TNG and TWC tend to 
produce shorter words than we would like.  
For each type of block, we need to work out 
the corresponding conditional probability. 
, ,
, ,
, , , ,
, ( : ) ,
( | , , , , , , , , )
( | , , , , , , , , )
( ,
| , , , , , , , , )
d i d i
d i d i
d i d i 1 d i l 1 d i
d i i l 1 d i
P t s z x t c ? ? ? ? ?
P x k z x t c ? ? ? ? ?
P z z z j x k
z x t c ? ? ? ? ?P
?
?
+ + ?
? + ? ?
=
=
= = = = =
GG GG
GG GG
"
GG GG
 
where ,d it ?
G
 denotes the character topic assign-
ments except td,i, ,d ix ?
G
 denotes the indicators ex-
cept xd,i, and , ( : )d i i l 1z ? + ?
G
denotes the word topic 
assignments except ,d jz
G
 (j from i to i+l-1). De-
tails of the derivation of these conditional prob-
abilities are provided in Appendix A.1. 
5 Experiments 
In this section, we discuss our evaluation of 
TWC in Chinese document modeling and Chi-
nese word and topic discovery.  
5.1 Modeling documents 
To evaluate the generalization performance of 
our model, we trained both TWC and TNG on a 
Chinese corpus and computed the perplexity of 
the held-out test set. Perplexity, which indicates 
the uncertainty in predicting a single character, is 
a standard measure of performance for statistical 
models of natural language. A lower perplexity 
score indicates better generalization performance. 
Formally, the perplexities for TWC and TNG 
are defined as follows. 
( )
? ? ? ??log ( | , , , , )
exp{ }
TWC test
D
d
d 1
D
d
d 1
perplexity D
p c ? ? ? ? ?
N
=
=
= ?
?
?
GG G G GG
, 
where Dtest is the testing data, D is the number of 
documents in Dtest, Nd is the number of characters 
in document d, ,d? z? is simply set to 1/Z (Z is 
number of word topics), and ? ? ??, , ,? ? ? ?
G G GG
are poste-
rior estimates provided by applying TWC to 
training data. Details of the parameter estimation 
for TWC are provided in Appendix A.2. 
( )TNG testperplexity D  
? ? ??log ( | , , , )
exp{ }
D
d
d 1
D
d
d 1
p c ? ? ? ?
N
=
=
= ?
?
?
GG G GG
, 
where Dtest, D, and Nd are the same as defined for 
the TWC perplexity, ,d? z? is simply set to 1/T (T is 
number of topics), and ? ??, ,? ? ?
G GG
 are posterior esti-
mates provided by applying TNG to training data.  
Now, the remaining question is how to work 
out the likelihood function in the definition of 
perplexity. The likelihood function can be ob-
tained by marginalizing latent variables, but the 
time complexity is exponential. Therefore, we 
propose an efficient method of computing the 
likelihood that is similar to the forward algo-
rithm for an HMM. Details of the forward ap-
proach to computing likelihood for TWC and 
TNG are provided in Appendix B.  
In our experiments, we used a subset of Chi-
nese corpus LDC2005T14. The dataset contains 
6000 documents with 4476 unique characters and 
2,454,616 characters. We evaluated both TWC 
and TNG using 10-fold cross validation. In each 
experiment, both models ran for 500 iterations on 
90% of the data and computed the complexity for 
the remaining 10% of the data. 
TWC used fixed Dirichlet (Beta) priors ?=1, 
?=1, ?=1, ?=0.1 and ?=0.01 while TNG used 
?=1, ?=0.01, ?=0.01, and ?=0.1. 
0
50
100
150
200
250
0 20 40 60 80 100
          No. of topics (TNG)
No. of charcter topics *  no. of word topics
Pe
rp
le
xi
ty
TNG TWC
Figure 5: Perplexity results with LDC2005T14 
corpora for TNG and TWC. 
The results of these computations are shown in 
Figure 5. Note that the abscissa for TWC is the 
number of word topics (Z) multiplied by the 
(TWC)
349
number of character topics (T), while the ab-
scissa for TNG is the number of topics (T). They 
both represent the number of partitions into 
which the model classifies characters. 
Chance performance results in a perplexity 
equal to the number of unique characters, which 
was 4476 in our experiments. Therefore, both 
TWC and TNG are competitive models. And the 
lower curve shows that TWC is much better than 
TNG. 
We also found that both perplexity curves in-
creased with the number of partitions. In other 
words, both models suffer from overfitting issues. 
This is because the documents in a test set are 
very likely to contain words that do not appear in 
any of the documents in the training set. Such 
words will have a very low probability, which is 
inversely proportional to the number of partitions. 
Therefore, the perplexity of TWC increased from 
7.3513 (Z*T=2*2) to 8.9953 (Z*T=10*10), while 
that of TNG increased from 20.3789 (T=5) to 
193.6065 (T=100).  
5.2 Chinese word and topic discovery 
As shown in the previous subsection, TWC is 
a competitive method for topically modeling 
Chinese documents. Next, we show its ability to 
extract Chinese words and topics in comparison 
with TNG.  
In our qualitative experiments, the task of 
Chinese word and topic discovery was addressed 
as a supervised learning problem, where a set of 
words with their topical assignments was given 
as a seed set. Each seed can be viewed as some 
constraints imposed on the TWC and TNG mod-
els. For example, suppose that ???? (teacher) 
together with its assignment ?education? is a 
seed. This assumption implies that the indicator 
between characters ??? and ??? is 1 and that 
the (word) topic for each character is ?education?.  
We make use of such constraints in a simple 
but effective way. In each sampling iteration, we 
first sample all variables as usual and then reset 
observed variables according to the constraints. 
We used 8000 Chinese documents in the Chi-
nese Gigaword corpus (LDC2005T14) provided 
by the Linguistic Data Consortium for our ex-
periments. The dataset contains 4651 unique 
characters and 3,295,810 characters.  
The number of word topics in TWC, the num-
ber of character topics in TWC, and the number 
of topics in TNG were all set to 15. Furthermore, 
16 seeds scattered in 4 distinct topics were given, 
as listed in Table 1 column ?seed?. Dirichlet 
(Beta)   priors  were  set  to  the  same  values   as 
described in the previous subsection.  
Word and topic assignments were extracted af-
ter running 300 Gibbs sampling iterations on the 
training corpus together with the seed set. For the 
TNG model, we took the first character?s topic as 
the topic of the word. We omitted one-character 
words and ranked the extracted words using the 
following formula 
( )
( )
i
15
i
i 1
occ W
occ W
=
?
,  
where occi(W) represents how many words were 
assigned to (word) topic i. The top-50 extracted 
words are presented in Table 1.  
We find found that both TWC and TNG could 
assemble many common words used in corre-
sponding topics. And the TWC model had ad-
vantages over the TNG model in the following 
three respects. 
First, TNG drew more words related to the 
seeds. In Table 1, highly related words are 
marked in pink (underline) and partly related 
words are marked in blue (italic). It is clear that 
the TWC column is more colorful than the TNG 
column. 
Secondly, we found that many words extracted 
by TNG had the same prefix. For example, con-
sider the topic ?agriculture?: there are 14 words 
marked with superscript 1 in Table 1. They all 
have the prefix ???. This is because we took the 
first character?s topic as the topic of the word. 
Although this strategy is beneficial in some cases, 
such as for words with prefix ???, it is detri-
mental in other cases. For example, ??? ? 
(sugar cane) and ???? (Gansu) have the same 
prefix and topic assignment, but the latter is a 
name of a province in China and is not related to 
agriculture. Similarly, even though the character 
string ???? does not form a Chinese word, this 
string ???? and ???? (Iran) are classified in 
the same cluster. Compared with TNG, TWC can 
also extract words whose topics are identical to 
the topic of any character. For example, the top-
ics ????? (freestyle swimming), ????? 
(medley swimming), and ??? ? (butterfly 
stroke) depend on their suffixes.  
Thirdly, although TNG stands for ?topical n-
gram model?, it infrequently draws words con-
taining more than two characters. On the other 
hand, the TWC model extracts many n-character 
words, such as ???????? (president of 
United States, George Bush), ??????? 
(individual medley), and  ?????? (four  per- 
350
Seeds TNG TWC 
??(football) 
??(player) 
??(match) 
??
(championship) 
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,?? 
??,?? 2,??,???,???,??,??,??,?
?,??,??,??,??,??,??,???,??,?
?,??,???,??,??,??,?? 2,??,???,
??,???,??? 2,?????,??,??,??,?
?,??,??,??,??,??,???,???,??,?
?,??,??,??,??,??,???,?? 
?? (foodstuff) 
?? (country) 
?? (farmer) 
?? (paddies) 
??,?? 1,??,?? 1,??,?? 1,?? 1,??
1,??,?? 1,?? 1,??,?? 1,?? 1,??,?
?,?? 1,??,?? 1,??,?? 1,?? 1,??,?
? 1,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
?? 
??,??,??,??,??,??,??,??,??,?
?,??,???,??,??,??,??,??,??,??
?,??,?? 2,??,??? 2,??,????,?? 2,
??,??,??,??,??? 2,??,??,??,??,
??,??,??? 2,??,??,??,?? 2,?? 2,?
?,????,??,??,??,??,?? 
?? (school) 
?? (teacher) 
?? (student) 
?? (education) 
??,??,??,??,??,??,??,??,?
?,??,??,??,???,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,?? 
??,??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,??,
??,????,??,???,??,??,??,??,?
?,??,??,???,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,??,
??,??,??,?? 
?? (war) 
?? (solider) 
?? (general) 
?? (weapon) 
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,???,??,??,??,??? 
??,??,??,??,???,??,??,??,??,?
?????,??,????,????? 2,??? 2,?
??,????,??,??,????,??,????,?
?,??,???,??,??,??,??,????,??,
???,??? 2,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,??,
??,?? 
Table 1: Top-50 words extracted by TWC and TNG.  
cent). This is partly due to our sampling strategy, 
discussed in subsection 4.1, which increases the 
probability of forming long words. 
We also found that some extracted character 
strings were very close to real Chinese words. 
For instance, ???? is a substring of ????? 
(tournament); ????? is a suffix of ????
?? (Chinese player), ?????? (American 
player), and ?????? (French player); and 
????? is a substring of  ?????? (100 
thousand kilograms) and ?????? (50 thou-
sand kilograms). (Such substrings are marked 
with superscript 2 in Table 1.) We believe that 
this result occurred because the training corpus 
was not large enough and that TWC will achieve 
better performance with a large dataset. 
6 Conclusion and Future Work 
In this paper, we presented a topical word-
character (TWC) model, which models two dis-
tinct types of topics: word topic and character 
topic. The experimental results show that TWC 
is a powerful approach to modeling Chinese 
documents according to the standard evaluation 
measure of perplexity. We also demonstrated 
TWC?s ability to detect words and assign topics. 
Since TWC is a straightforward improvement 
that removes the limitations of existing topical 
collocation models, we expect that its application 
to English collocation will also result in higher 
performance.  
Appendix A.1 Gibbs Sampling Derivation 
for TWC 
Symbols used here are defined as follows.  
C is the number of unique characters, T is the num-
ber of character topics, and Z is the number of word 
topics.  
Nd denotes the number of characters in a document 
d. 
( )I ?  is an indicator function, taking the value 1 
when its argument is true, and 0 otherwise. 
qd,z,0 represents how words are assigned to topic z in 
document d; pz,t,c,k represents how many times an indi-
cator is k given the previous character c, the previous 
character topic t, and the previous word topic z; nz,t,0 
represents how many times a character topic is t given 
a word topic z and the corresponding indicator 0; 
mz,v,t,1 represents how many times a character topic is t 
given a word topic z, the previous character topic v, 
and the corresponding indicator 1; and rt,c represents 
how many times character c is assigned to character 
topic t. 
'
'
', ' ', ' ', '
, ,
' ', ' ', ' ', ' '
' ' '
', ', ' ', ' , ,
' ' ' ' '
'
( | , , , , , , , , )
( | ) ( | , , )
( | ) ( | )
(
d
d
d i 1 d i 1 d i 1
d i d i
ND D
d d i d i d i 1 d
d 1 d 1 i 1
NZ T C D
z t c d i z t c
z 1 t 1 c 1 d 1 i 1
z
P t s z x t c ? ? ? ? ?
P ? ? P z x z ? d?
P ? ? P x ? d?
P ?
? ? ?
?
?
= = =
= = = = =
=
? ? ? ?
? ?
?
? ???
??? ???
GG GG
G G G
G G G
G '
', '', ' ', ' ', '
' ' ' ' '
| ) ( | ) ( | , ,
d
d i
NZ Z T D
z t d i d i z
z 1 z 1 t 1 d 1 i 1
? P ? ? P t x ?
= = = = =
? ?? ?? ???? GG
 
351
'', ' ', ' ', '
', ', ', '
', ', ' ', ', '
,
, ,, ,
, ' ', '
' ' '
' '
' ' ' ' '
) ( | ) ( | )
?( )
( ) ( )
?( )
?( )
?( )
d
d i d i 1 d i
z t c x
z t c z t c
d i 1
z s cd i d i
NT D
z t t d i t
t 1 d 1 i 1
Z T C 2 2
px ? 1 x
2
z 1 t 1 c 1 x 1 x 1
x
C
t
? d? d? P ? ? P c ? d?
2?
? ?
?
C?
? d?
?
?
+
= = =
?
= = = = =
? ? ? ? ?
? ? ? ?
? ?
? ???
??? ? ??
G G G GG G
G
', ' ,
' '
', ',
,', ', ',
, ,
' '
' ' '
' '
' '
' ' ' ' '
' '
', ' ', '
' ' ,
( ) ( )
?( ) ?( )
( ) ( )
?( ) ?( )
( ) ( )
t c d i
t t s
z t 0
d iz t v 1
d i d i 1
T C C
r cc ? 1 c
1 c 1 c 1
Z T T Z T
nt ? 1 t
z zT T
z 1 t 1 t 1 z 1 t 1
sT T
zmv ? 1 v
z t z t
v 1 v 1 z t
? ? ? d?
T? T?
? ?
? ?
?
? ?
? ?
?
= = =
?
= = = = =
?
= =
? ? ?
? ? ? ?
? ?
? ? ??
? ? ? ????
? ?
G
,
,, , , ,
, ,, ,
, ,
,
,
, ,
,
,*,, , , ,
, , ,, , ,* ,*
,
, ,*,
d i
d id i d i d i d i
d i d i 1d i d i
d i d i 1
d i
s
d i
z s 0
d i
z 0z s c x s c
z t s 1z s c s
d i
z t 1
x 0
d? d?
x 1
n ?
x 0
n T?p ? r ?
m ?p 2? r C?
x 1
m T?
?
?
? =? ? ?? =??
+? =? ++ + ?? ? ? ? ++ + ? =? +?
G G
  
Similarly, 
,
, , ,
, , ,
, ,
,
, , ,
, ,
, ,
, ,
, , ,
,*,
, , ,*
, ,
, ,
,*,
, , ,
, ,*,
( | , , , , , , , , )
( )
d i
d i 1 d i 1 d i 1
d i 1 d i 1 d i 1
d i d i
d i
d i d i 1 d i
d i d i 1
d i d i
d z 0
z t c k
d 0
z t c
d i d i 1
z t 0
z 0
z t t 1
z t
P x k z x t c ? ? ? ? ?
q ?
p ?k 0
q Z?
p 2?
I z z k 1
n ?
k 0
n T?
m ?
m
? ? ?
? ? ?
?
?
?
?
=
+? +=? +? ? ?? +? = =?
+
+?
=+
GG GG
1
k 1
T?
????? =? +?
 
, , ,
, , ,
, , ,
, , , , , ( : ) ,
, ,
, , ,
,*,
, , ,*
,
, , ,
( , | , , ,
, , , , , )
( )
d i 1 d i 1 d i 1
d i 1 d i 1 d i 1
d i u 2 d i u 2 d i u 1
d i d i 1 d i l 1 d i d i i l 1 d i
d j 0
z t c k
d 0
z t c
d i 1
j t c x
j
P z z z j x k z x t
c ? ? ? ? ?
q ?
k 0 p ?
q Z?
p 2?
I z j k
P
1
p ?
p
? ? ?
? ? ?
+ ? + ? + ?
+ + ? ? + ? ?
?
= = = = =
+? = +? +? ? ??
?
+? = =?
+
GGG"
G
, ,
, , ,
,
, ,
,
, , ,
, , ,* , ,*
, ,
,*,
, , ,
, ,*,
( )d i u 2 d i u 1
d i u 2 d i u 2 d i u 2
d i
d i 1 d i
d i 1
l 1 l
j t t 1
u 2 u 2t c j t 1
j t 0
j 0
j t t 1
j t 1
m ?
2? m T?
n ?
k 0
n T?
m ?
k 1
m T?
+ ? + ?
+ ? + ? + ?
?
?
+
= =
?
+? ?+ +
+? =? +?? +? =? +?
? ?
 
Appendix A.2 Parameter estimation for 
TWC 
After each Gibbs sampling iteration, we obtain pos-
terior estimates ? ? ??, , ,? ? ? ?
G G GG
 and r by 
, , , , ,
, , , ,
,*, , , ,*
, , , , ,
, , ,
, ,*, ,*,
,
,
,*
? ?
??
?
d z 0 z t c k
d z z t c k
d 0 z t c
z v t 1 z t 0
z v t z t
z v 1 z 0
t c
t c
t
q ? p ?
? ?
q Z? p 2?
m ? n ?
? ?
m T? n T?
r ?
?
r C?
+ += =+ +
+ += =+ +
+= +
, 
where the symbols are the same as those defined in 
Appendix A.1. These values correspond to the predic-
tive distribution over new word topics, new indicators, 
new character topics, and new characters. 
Appendix B. Likelihood Function Deriva-
tion for TWC and TNG 
To compute the likelihood function for TWC, a qua-
ternion function gi is defined as follows: (formula has 
a broken character) 
, , , , ,
, ,
( , , , ) ( , ,..., , , ,
? ? ? ??, | , , , , )
i d 1 d 2 d i d i d i
d i 1 d i
g r s u v P c c c z r x s
t u t v ? ? ? ? ??=====
= =
= ======

G G G GG . 
Then, it is clear that 
? ? ? ??( | , , , , ) ( , , , )
d
Z 1 T T
d N
r 1 s 0 u 1 v 1
P c ? ? ? ? ? g r s u v
= = = =
=????GG G G GG , 
where Z is the number of word topics and T is the 
number of character topics. The function gi can be 
rewritten in a recursive manner.  
,
,
,, ,
,
( , , , )
?( , , , )
? ?( , , , ) ( , , , )
? ?
?( )
d 1
d i 1
d i
1
cr v
1 d r v
Z 1 T
cs
i 1 i j u c v
j 1 k 0 l 1
vr
rd
v
r u
g r 1 u v 0
1
g r 0 u v ? ? ?
T
g r s u v g j k l u ? ?
s 0??
s 1?I r j
+
+
= = =
=
= ? ? ?
= ? ?
? =?? ?? ==??
===========
???  
Similarly we can define function hi to help compute 
the likelihood for TNG. (formula has a broken charac-
ter) 
,
,
, ,
,
, ,
,
,
? ? ??( , ) ( ,..., , , | , , , )
? ? ??( | , , , ) ( , )
( , )
? ?( , )
???( , ) ( , )
?
d
d 1
d i 1
d i d i 1
d i
i d 1 d i i i
Z 1
d N
r 1 s 0
1
cr
1 d r
c
Z 1
rs r
i 1 i j c d c
j 1 k 0 r c
h r s P c c z r x s ? ? ? ?
P c ? ? ? ? h r s
h r 1 0
h r 0 ? ?
? s 0
h r s h j k ? ?
s 1?
+
+
= =
+
= =
= =
=
=
= ?
? =?= ? ? ? ? =??
??
??
G G GG
GG G GG
 
References 
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent 
Dirichlet alocation. Journal of Machine Learning 
Research, 3:993?1022. 
Griffiths, T. L. and Steyvers, M. 2004. Finding Scien-
tific Topics. Proceedings of the National Academy 
of Sciences, 101 (suppl. 1), 5228-5235. 
Steyvers, M. and Griffiths, T. L. 2007. Probabilistic 
topic models. Latent Semantic Analysis: A Road to 
Meaning. Laurence Erlbaum. 
Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B. T. 
2007. Topics in Semantic Representation Psycho-
logical Review, 114(2), 211-244.  
Wang, X., McCallum, A., and Wei, X. 2007. Topical 
N-grams: Phrase and Topic Discovery, with an 
Application to Information Retrieval. Proceedings 
of the 7th IEEE International Conference on Data 
Mining (ICDM 2007).  
352
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 793?800
Manchester, August 2008
Metric Learning for Synonym Acquisition
Nobuyuki Shimizu
Information Technology Center
University of Tokyo
shimizu@r.dl.itc.u-tokyo.ac.jp
Masato Hagiwara
Graduate School of Information Science
Nagoya University
hagiwara@kl.i.is.nagoya-u.ac.jp
Yasuhiro Ogawa and Katsuhiko Toyama
Graduate School of Information Science
Nagoya University
{yasuhiro,toyama}@kl.i.is.nagoya-u.ac.jp
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
n3@dl.itc.u-tokyo.ac.jp
Abstract
The distance or similarity metric plays an
important role in many natural language
processing (NLP) tasks. Previous stud-
ies have demonstrated the effectiveness of
a number of metrics such as the Jaccard
coefficient, especially in synonym acqui-
sition. While the existing metrics per-
form quite well, to further improve perfor-
mance, we propose the use of a supervised
machine learning algorithm that fine-tunes
them. Given the known instances of sim-
ilar or dissimilar words, we estimated the
parameters of the Mahalanobis distance.
We compared a number of metrics in our
experiments, and the results show that the
proposed metric has a higher mean average
precision than other metrics.
1 Introduction
Accurately estimating the semantic distance be-
tween words in context has applications for
machine translation, information retrieval (IR),
speech recognition, and text categorization (Bu-
danitsky and Hirst, 2006), and it is becoming
clear that a combination of corpus statistics can be
used with a dictionary, thesaurus, or other knowl-
edge source such as WordNet or Wikipedia, to in-
crease the accuracy of semantic distance estima-
tion (Mohammad and Hirst, 2006). Although com-
piling such resources is labor intensive and achiev-
ing wide coverage is difficult, these resources to
some extent explicitly capture semantic structures
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of concepts and words. In contrast, corpus statis-
tics achieve wide coverage, but the semantic struc-
ture of a concept is only implicitly represented in
the context. Assuming that two words are semanti-
cally closer if they occur in similar contexts, statis-
tics on the contexts of words can be gathered and
compared for similarity, by using a metric such as
the Jaccard coefficient.
Our proposal is to extend and fine-tune the latter
approach with the training data obtained from the
former. We apply metric learning to this task. Al-
though still in their infancy, distance metric learn-
ing methods have undergone rapid development in
the field of machine learning. In a setting simi-
lar to semi-supervised clustering, where known in-
stances of similar or dissimilar objects are given,
a metric such as the Mahalanobis distance can be
learned from a few data points and tailored to fit a
particular purpose. Although classification meth-
ods such as logistic regression now play impor-
tant roles in natural language processing, the use
of metric learning has yet to be explored.
Since popular current methods for synonym ac-
quisition require no statistical learning, it seems
that supervised machine learning should easily
outperform them. Unfortunately, there are obsta-
cles to overcome. Since metric learning algorithms
usually learn the parameters of a Mahalanobis dis-
tance, the number of parameters is quadratic to the
number of features. They learn how two features
should interact to produce the final metric. While
traditional metrics forgo examining of the interac-
tions entirely, in applying metrics such as Jaccard
coefficient, it is not uncommon nowadays to use
more than 10,000 features, a number that a typical
metric learner is incapable of processing. Thus we
have two options: one is to find the most impor-
tant features and model the interactions between
793
them, and the other is simply to use a large number
of features. We experimentally examined the two
options and found that metric learning is useful in
synonym acquisition, despite it utilizing fewer fea-
tures than traditional methods.
The remainder of this paper is organized as fol-
lows: in section 2, we review prior work on syn-
onym acquisition and metric learning. In section
3, we introduce the Mahalanobis distance metric
and a learning algorithm based on this metric. In
section 4 and 5, we explain the experimental set-
tings and propose the use of normalization to make
the Mahalanobis distances work in practice, and
then in section 6, we discuss issues we encountered
when applying this metric to synonym acquisition.
We conclude in section 7.
2 Prior Work
As this paper is based on two different lines of re-
search, we first review the work in synonym acqui-
sition, and then review the work in generic metric
learning. To the best of the authors? knowledge,
none of the metric learning algorithms have been
applied to automatic synonym acquisition.
Synonym relation is important lexical knowl-
edge for many natural language processing
tasks including automatic thesaurus construction
(Croach and Yang, 1992; Grefenstette, 1994) and
IR (Jing and Croft, 1994). Various methods (Hin-
dle, 1990; Lin, 1998) of automatically acquiring
synonyms have been proposed. They are usu-
ally based on the distributional hypothesis (Har-
ris, 1985), which states that semantically simi-
lar words share similar contexts, and they can be
roughly viewed as the combinations of two steps:
context extraction and similarity calculation. The
former extracts useful features from the contexts of
words, such as surrounding words or dependency
structure. The latter calculates how semantically
similar two given words are based on similarity or
distance metrics.
Many studies (Lee, 1999; Curran and Moens,
2002; Weeds et al, 2004) have investigated
similarity calculation, and a variety of dis-
tance/similarity measures have already been com-
pared and discussed. Weeds et al?s work is espe-
cially useful because it investigated the character-
istics of metrics based on a few criteria such as
the relative frequency of acquired synonyms and
clarified the correlation between word frequency,
distributional generality, and semantic generality.
However, all of the existing research conducted
only a posteriori comparison, and as Weeds et al
pointed out, there is no one best measure for all ap-
plications. Therefore, the metrics must be tailored
to applications, even to corpora and other settings.
We next review the prior work in generic metric
learning. Most previous metric learning methods
learn the parameters of the Mahalanobis distance.
Although the algorithms proposed in earlier work
(Xing et al, 2002; Weinberger et al, 2005; Glober-
son and Roweis, 2005) were shown to yield excel-
lent classification performance, these algorithms
all have worse than cubic computational complex-
ity in the dimensionality of the data. Because of
the high dimensionality of our objects, we opted
for information-theoretic metric learning proposed
by (Davis et al, 2007). This algorithm only uses
an operation quadratic in the dimensionality of the
data.
Other work on learning Mahalanobis metrics in-
cludes online metric learning (Shalev-Shwartz et
al., 2004), locally-adaptive discriminative methods
(Hastie and Tibshirani, 1996), and learning from
relative comparisons (Schutz and Joahims, 2003).
Non-Mahalanobis-based metric learning methods
have also been proposed, though they seem to suf-
fer from suboptimal performance, non-convexity,
or computational complexity. Examples include
neighborhood component analysis (Goldberger et
al., 2004).
3 Metric Learning
3.1 Problem Formulation
To set the context for metric learning, we first de-
scribe the objects whose distances from one an-
other we would like to know. As noted above re-
garding the distributional hypothesis, our object is
the context of a target word. To represent the con-
text, we use a sparse vector in Rd. Each dimension
of an input vector represents a feature of the con-
text, and its value corresponds to the strength of
the association. The vectors of two target words
represent their contexts as points in multidimen-
sional feature-space. A suitable metric (for exam-
ple, Euclidean) defines the distance between the
two points, thereby estimating the semantic dis-
tance between the target words.
Given points x
i
, x
j
? R
d
, the (squared) Ma-
halanobis distance between them is parameter-
ized by a positive definite matrix A as follows
d
A
(x
i
, x
j
) = (x
i
? x
j
)
?
A(x
i
? x
j
). The Ma-
794
halanobis distance is a straightforward extension
of the standard Euclidean distance. If we let A
be the identity matrix, the Mahalanobis distance
reduces to the Euclidean distance. Our objective
is to obtain the positive definite matrix A that pa-
rameterizes the Mahalanobis distance, so that the
distance between the vectors of two synonymous
words is small, and the distance between the vec-
tors of two dissimilar words is large. Stated more
formally, the Mahalanobis distance between two
similar points must be smaller than a given upper
bound, i.e., d
A
(x
i
, x
j
) ? u for a relatively small
value of u. Similarly, two points are dissimilar if
d
A
(x
i
, x
j
) ? l for sufficiently large l.
As we discuss below, we were able to use
the Euclidean distance to acquire synonyms quite
well. Therefore, we would like the positive definite
matrix A of the Mahalanobis distance to be close to
the identity matrix I . This keeps the Mahalanobis
distance similar to the Euclidean distance, which
would help to prevent overfitting the data. To op-
timize the matrix, we follow the information theo-
retic metric learning approach described in (Davis
et al, 2007). We summarize the problem formula-
tion advocated by this approach in this section and
the learning algorithm in the next section.
To define the closeness between A and I , we
use a simple bijection (up to a scaling function)
from the set of Mahalanobis distances to the set
of equal mean multivariate Gaussian distributions.
Without loss of generalization, let the equal mean
be ?. Then given a Mahalanobis distance pa-
rameterized by A, the corresponding Gaussian is
p(x;A) =
1
Z
exp(?
1
2
d
A
(x, ?)) where Z is the
normalizing factor. This enables us to measure
the distance between two Mahalanobis distances
with the Kullback-Leibler (KL) divergence of two
Gaussians:
KL(p(x; I)||p(x;A)) =
?
p(x, I) log
(
p(x; I)
p(x;A)
)
dx.
Given pairs of similar points S and pairs of dis-
similar points D, the optimization problem is:
min
A
KL(p(x; I)||p(x;A))
subject to d
A
(x
i
, x
j
) ? u (i, j) ? S
d
A
(x
i
, x
j
) ? l (i, j) ? D
3.2 Learning Algorithm
(Davis and Dhillon, 2006) has shown that the
KL divergence between two multivariate Gaus-
sians can be expressed as the convex combination
of a Mahalanobis distance between mean vectors
and the LogDet divergence between the covariance
matrices. The LogDet divergence equals
D
ld
(A,A
0
) = tr(AA
?1
0
)? log det(AA
?1
0
)? n
for n by n matrices A,A
0
. If we assume the means
of the Gaussians to be the same, we have
KL(p(x;A
0
||p(x,A)) =
1
2
D
ld
(A,A
0
)
The optimization problem can be restated as
min
A0
D
ld
(A, I)
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? u (i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? l (i, j) ? D
We then incorporate slack variables into the for-
mulation to guarantee the existence of a feasible
solution for A. The optimization problem be-
comes:
min
A0
D
ld
(A, I) + ?D
ld
(diag(?), diag(?
0
))
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? D
where c(i, j) is the index of the (i, j)-th constraint
and ? is a vector of slack variables whose compo-
nents are initialized to u for similarity constraints
and l for dissimilarity constraints. The tradeoff
between satisfying the constraints and minimiz-
ing D
ld
(A, I) is controlled by the parameter ?.
To solve this optimization problem, the algorithm
shown in Algorithm 3.1 repeatedly projects the
current solution onto a single constraint.
This completes the summary of (Davis et al,
2007).
4 Experimental Settings
In this section, we describe the experimental set-
tings including the preprocessing of data and fea-
tures, creation of the query word sets, and settings
of the cross validation.
4.1 Features
We used a dependency structure as the context for
words because it is the most widely used and one
of the best performing contextual information in
the past studies (Ruge, 1997; Lin, 1998). As the
extraction of an accurate and comprehensive de-
pendency structure is in itself a complicated task,
the sophisticated parser RASP Toolkit 2 (Briscoe
et al, 2006) was utilized to extract this kind of
word relation.
Let N(w, c) be the raw cooccurrence count of
word w and context c, the grammatical relation
795
Algorithm
3.1: INFORMATION THEORETIC METRIC LEARNING
Input :
X(d by n matrix), I(identity matrix)
S(set of similar pairs),D(set of dissimilar pairs)
?(slack parameter), c(constraint index function)
u, l(distance thresholds)
Output :
A(Mahalanobis matrix)
A := I
?
ij
:= 0
?
c(i,j)
:= u for (i, j) ? S; otherwise, ?
c(i,j)
:= l
repeat
Pick a constraint (i, j) ? S or (i, j) ? D
p := (x
i
? x
j
)
?
A(x
i
? x
j
)
? := 1 if (i, j) ? S,?1 otherwise.
? := min(?
ij
,
?
2
(
1
p
?
?
?
c(i,j)
))
? := ??/(1? ???
c(i,j)
)
?
c(i,j)
:= ??
c(i,j)
/(? + ???
c(i,j)
)
?
ij
:= ?
ij
? ?
A := A + ?A(x
i
? x
j
)(x
i
? x
j
)
?
A
until convergence
return (A)
in which w occurs. These raw counts were ob-
tained from New York Times articles (July 1994)
extracted from English Gigaword 1. The section
consists of 7,593 documents and approx. 5 million
words. As discussed below, we limited the vocab-
ulary to the nouns in the Longman Defining Vo-
cabulary (LDV) 2. The features were constructed
by weighting them using pointwise mutual infor-
mation: wgt(w, c) = PMI(w, c) = log P (w,c)
P (w)P (c)
.
Co-occurrence data constructed this way can
yield more than 10,000 context types, rendering
metric learning impractical. As the applications
of feature selection reduce the performance of the
baseline metrics, we tested them in two different
settings: with and without feature selection. To
mitigate this problem, we applied a feature selec-
tion technique to reduce the feature dimensional-
ity. We selected features using two approaches.
The first approach is a simple frequency cutoff, ap-
plied as a pre-processing to filter out words and
contexts with low frequency and to reduce com-
putational cost. Specifically, all words w such
that
?
c
N(w, c) < ?
f
and contexts c such that
?
w
N(w, c) < ?
f
, with ?
f
= 5, are removed from
the co-occurrence data.
The second approach is feature selection by con-
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
2http://www.cs.utexas.edu/users/kbarker/working notes/
ldoce-vocab.html
text importance (Hagiwara et al, 2008). First, the
context importance score for each context type is
calculated, and then the least important context
types are eliminated, until a desired numbers of
them remains. To measure the context importance
score, we used the number of unique words the
context co-occurs with: df(c) = |{w|N(w, c) >
0}|. We adopted this context selection criterion
on the assumption that the contexts shared by
many words should be informative, and the syn-
onym acquisition performance based on normal
distributional similarity calculation retains its orig-
inal level of performance until up to almost 90%
of context types are eliminated (Hagiwara et al,
2008). In our experiment, we selected features
rather aggressively, finally using only 10% of the
original contexts. These feature reduction oper-
ations reduced the dimensionality to a figure as
small as 1,281, while keeping the performance loss
at a minimum.
4.2 Similarity and Distance Functions
We compared seven similarity/distance functions
in our experiments: cosine similarity, Euclidean
distance, Manhattan distance, Jaccard coeffi-
cient, vector-based Jaccard coefficient (Jaccardv),
Jensen-Shannon Divergence (JS) and skew diver-
gence (SD99). We first define some notations. Let
C(w) be the set of context types that co-occur with
word w, i.e., C(w) = {c|N(w, c) > 0}, and w
i
be
the feature vector corresponding to word w, i.e.,
w
i
= [wgt(w
i
, c
1
) ... wgt(w
i
, c
M
)]
?
. The first
three, the cosine, Euclidean and Manhattan dis-
tance, are vector-based metrics.
cosine similarity
w
1
?w
2
||w
1
|| ? ||w
2
||
Euclidean distance
?
?
c?C(w
1
)?C(w
2
)
(wgt(w
1
, c)? wgt(w
2
, c))
2
Manhattan distance
?
c?C(w
1
)?C(w
2
)
|wgt(w
1
, c)? wgt(w
2
, c)|
Jaccard coefficient
?
c?C(w
1
)?C(w
2
)
min(wgt(w
1
, c),wgt(w
2
, c))
?
c?C(w
1
)?C(w
2
)
max(wgt(w
1
, c),wgt(w
2
, c))
,
796
vector-based Jaccard coefficient (Jaccardv)
w
i
?w
j
||w
i
|| + ||w
j
|| ?w
i
?w
j
.
Jensen-Shannon divergence (JS)
1
2
{KL(p
1
||m) + KL(p
2
||m)}, m = p
1
+ p
2
.
JS and SD99 are based on the KL divergence, so
the vectors must be normalized to form a probabil-
ity distribution. For notational convenience, we let
p
i
be the probability distribution representation of
feature vector w
i
, i.e., p
i
(c) = N(w
i
, c)/N(w
i
).
While the KL divergence suffers from the so-called
zero-frequency problem, a symmetric version of
the KL divergence called the Jensen-Shannon di-
vergence naturally avoids it.
skew divergence (SD99)
KL(p
1
||?p
2
+ (1? ?)p
1
).
As proposed by (Lee, 2001), the skew diver-
gence also avoids the zero-frequency problem by
mixing the original distribution with the target dis-
tribution. Parameter ? is set to 0.99.
4.3 Query Word Set and Cross Validation
To formalize the experiments, we must prepare a
set of query words for which synonyms are known
in advance. We chose the Longman Defining
Vocabulary (LDV) as the candidate set of query
words. For each word in the LDV, we consulted
three existing thesauri: Roget?s Thesaurus (Ro-
get, 1995), Collins COBUILD Thesaurus (Collins,
2002), and WordNet (Fellbaum, 1998). Each LDV
word was looked up as a noun to obtain the union
of synonyms. After removing words marked ?id-
iom?, ?informal? or ?slang? and phrases com-
prised of two or more words, this union was used
as the reference set of query words. LDV words for
which no noun synonyms were found in any of the
reference thesauri were omitted. From the remain-
ing 771 LDV words, there were 231 words that had
five or more synonyms in the combined thesaurus.
We selected these 231 words to be the query words
and distributed them into five partitions so as to
conduct five-fold cross validation. Four partitions
were used in training, and the remaining partition
was used in testing. For each fold, we created
the training set from four partitions as follows; for
each query word in the partitions, we randomly se-
lected five synonymous words and added the pairs
of query words and synonymous words to S, the
set of similar pairs. Similarly, five pairs of query
words and dissimilar words were randomly added
to D, the set of dissimilar pairs. The training set
for each fold consisted of S and D. Since a learner
trained on an imbalanced dataset may not learn
to discriminate enough between classes, we sam-
pled dissimilar pairs to create an evenly distributed
training dataset.
To make the evaluation realistic, we used a dif-
ferent method to create the test set: we paired each
query word with each of the 771 remaining words
to form the test set. Thus, in each fold, the training
set had an equal number of positive and negative
pairs, while in the test set, negative pairs outnum-
bered the positive pairs. While this is not a typical
setting for cross validation, it renders the evalua-
tion more realistic since an automatic synonym ac-
quisition system in operation must be able to pick
a few synonyms from a large number of dissimilar
words.
The meta-parameters of the metric learning
model were simply set u = 1, l = 2 and ? = 1.
Each training set consisted of 1,850 pairs, and the
test set consisted of 34,684 pairs. Since we con-
ducted five-fold cross validation, the reported per-
formance in this paper is actually a summary over
different folds.
4.4 Evaluation Measures
We used an evaluation program for KDD Cup
2004 (Caruana et al, 2004) called Perf to measure
the effectiveness of the metrics in acquiring syn-
onyms. To use the program, we used the following
formula to convert each distance metric to a simi-
larity metric. s(x
i
, x
j
) = 1/(1 + exp(d(x
i
, x
j
))).
Below, we summarize the three measures we
used: Mean Average Precision, TOP1, and Aver-
age Rank of Last Synonym.
Mean Average Precision (APR)
Perf implements a definition of average preci-
sion sometimes called ?expected precision?. Perf
calculates the precision at every recall where it is
defined. For each of these recall values, Perf finds
the threshold that produces the maximum preci-
sion, and takes the average over all of the recall
values greater than 0. Average precision is mea-
sured on each query, and then the mean of each
query?s average precision is used as the final met-
ric. A mean average precision of 1.0 indicates per-
fect prediction. The lowest possible mean average
797
precision is 0.0.
Average Rank of Last Synonym (RKL)
As in other evaluation measures, synonym can-
didates are sorted by predicted similarity, and this
metric measures how far down the sorted cases we
must go to find the last true synonym. A rank of
1 indicates that the last synonym is placed in the
top position. Given a query word, the highest ob-
tainable rank is N if there are N synonyms in the
corpus. The lower this measure is the better. Aver-
age ranks near 771 indicate poor performance.
TOP1
In each query, synonym candidates are sorted by
predicted similarity. If the word that ranks at the
top (highest similarity to the query word) is a true
synonym of the query word, Perf scores a 1 for
that query, and 0 otherwise. If there are ties, Perf
scores 0 unless all of the tied cases are synonyms.
TOP1 score ranges from 1.0 to 0.0. To achieve 1.0,
perfect TOP1 prediction, a similarity metric must
place a true synonym at the top of the sorted list
in every query. In the next section, we report the
mean of each query?s TOP1.
5 Results
The evaluations of the metrics are listed in Table
1. The figure on the left side of ? represents the
performance with 1,281 features, and that on the
right side with 12,812 features. Of all the met-
rics in Table 1, only the Mahalanobis L2 is trained
with the previously presented metric learning al-
gorithm. Thus, the values for the Mahalanobis
L2 are produced by the five-fold cross validation,
while the rest are given by the straight application
of the metrics discussed in Section 4.2 to the same
dataset. Strictly speaking, this is not a fair com-
parison, since we ought to compare a supervised
learning with a supervised learning. However, our
baseline is not the simple Euclidean distance; it
is the Jaccard coefficient and cosine similarity, a
handcrafted, best performing metric for synonym
acquisition, with 10 times as many features.
The computational resources required to obtain
the Mahalanobis L2 results were as follows: in the
training phase, each fold of cross validation took
about 80 iterations (less than one week) to con-
verge on a Xeon 5160 3.0GHz. The time required
to use the learned distance was a few hours at most.
At first, we were unable to perform competi-
tively with the Euclidean distance. As seen in Ta-
ble 1, the TOP1 measure of the Euclidean distance
is only 1.732%. This indicates that the likelihood
of finding the first item on the ranked list to be a
true synonym is 1.732%. The vector-based Jac-
card coefficient performs much better than the Eu-
clidean distance, placing a true synonym at the top
of the list 30.736% of the time.
Table 2 shows the Top 10 Words for Query
?branch?. The results for the Euclidean distance
rank ?hut? and other dissimilar words highly. This
is because the norm of such vectors is small, and in
a high dimensional space, the sparse vectors near
the origin are relatively close to many other sparse
vectors. To overcome this problem, we normal-
ized the input vectors by the L2 norm x? = x/||x||
This normalization enables the Euclidean distance
to perform very much like the cosine similarity,
since the Euclidean distance between points on a
sphere acts like the angle between the vectors. Sur-
prisingly, normalization by L2 did not affect other
metrics all that much; while the performances of
some metrics improved slightly, the L2 normaliza-
tion lowered that of the Jaccardv metric.
Once we learned the normalization trick, the
learned Mahalanobis distance consistently outper-
formed all other metrics, including the ones with
10 times more features, in all three evaluation
measures, achieving an APR of 18.66%, RKL of
545.09 and TOP1 of 45.455%.
6 Discussion
Examining the learned Mahalanobis matrix re-
vealed interesting features. The matrix essentially
shows the covariance between features. While it
was not as heavily weighted as the diagonal ele-
ments, we found that its positive non-diagonal el-
ements were quite interesting. They indicate that
some of the useful features for finding synonyms
are correlated and somewhat interchangeable. The
example includes a pair of features, (dobj begin
*) and (dobj end *). It was a pleasant surprise to
see that one implies the other. Among the diag-
onal elements of the matrix, one of the heaviest
features was being the direct object of ?by?. This
indicates that being the object of the preposition
?by? is a good indicator that two words are simi-
lar. A closer inspection of the NYT corpus showed
that this preposition overwhelmingly takes a per-
son or organization as its object, indicating that
words with this feature belong to the same class
of a person or organization. Similarly, the class
798
Metric APR RKL TOP1
Cosine 0.1184 ? 0.1324 580.27 ? 579.00 0.2987 ? 0.3160
Euclidean 0.0229 ? 0.0173 662.74 ? 695.71 0.0173 ? 0.0000
Euclidean L2 0.1182 ? 0.1324 580.30 ? 578.99 0.2943 ? 0.3160
Jaccard 0.1120 ? 0.1264 580.76 ? 579.51 0.2684 ? 0.2943
Jaccard L2 0.1113 ? 0.1324 580.29 ? 570.88 0.2640 ? 0.2987
Jaccardv 0.1189 ? 0.1318 580.50 ? 580.19 0.3073 ? 0.3030
Jaccardv L2 0.1184 ? 0.1254 580.27 ? 570.00 0.2987 ? 0.3160
JS 0.0199 ? 0.0170 681.97 ? 700.53 0.0129 ? 0.0000
JS L2 0.0229 ? 0.0173 679.21 ? 699.00 0.0303 ? 0.0086
Manhattan 0.0181 ? 0.0168 687.73 ? 701.47 0.0043 ? 0.0000
Manhattan L2 0.0185 ? 0.0170 686.56 ? 701.11 0.0043 ? 0.0086
SD99 0.0324 ? 0.1039 640.71 ? 588.16 0.0173 ? 0.2640
SD99 L2 0.0334 ? 0.1117 633.32 ? 586.78 0.0216 ? 0.2900
Mahalanobis L2 0.1866 545.09 0.4545
Table 1: Evaluation of Various Metrics, as Number of Features Increase from 1,281 to 12,812
Cosine Euclidean Euclidean L2 Jaccard Jaccardv Mahalanobis L2
1 (*) office hut (*) office (*) office (*) office (*) division
2 area wild area border area group
3 (*) division polish (*) division area (*) division (*) office
4 border thirst border plant border line
5 group hollow group (*) division group period
6 organization shout organization mouth organization organization
7 store fold store store store (*) department
8 mouth dear mouth circle mouth charge
9 plant hate plant stop plant world
10 home wake home track home body
(*) = a true synonym
Table 2: Top 10 Words for Query ?branch?
of words that ?to? and ?within?, take as an objects
were clear from the corpus: ?to? takes a person
or place, ?within? takes duration of time 3. Other
heavy features includes being the object of ?write?
or ?about?. While not obvious, we postulate that
having these words as a part of the context indi-
cates that a word is an event of some type.
7 Conclusion
We applied metric learning to automatic synonym
acquisition for the first time, and our experiments
showed that the learned metric significantly out-
performs existing similarity metrics. This outcome
indicates that while we must resort to feature se-
lection to apply metric learning, the performance
gain from the supervised learning is enough to off-
set the disadvantage and justify its usage in some
applications. This leads us to think that a com-
bination of the learned metric with unsupervised
metrics with even more features may produces the
best results. We also discussed interesting features
found in the learned Mahalanobis matrix. Since
3Interestingly, we note that not all prepositions were as
heavy: ?beyond? and ?without? were relatively light among
the diagonal elements. In the NYT corpus, the class of words
they take was not as clear as, for example, ?by?.
metric learning is known to boost clustering per-
formance in a semi-supervised clustering setting,
we believe these automatically identified features
would be helpful in assigning a target word to a
word class.
References
T. Briscoe, J. Carroll and R. Watson. 2006. The Sec-
ond Release of the RASP System. Proc. of the COL-
ING/ACL 2006 Interactive Presentation Sessions,
77?80.
T. Briscoe, J. Carroll, J. Graham and A. Copestake,
2002. Relational evaluation schemes. Proc. of the
Beyond PARSEVAL Workshop at the Third Interna-
tional Conference on Language Resources and Eval-
uation, 4?8.
A. Budanitsky and G. Hirst. 2006. Evaluat-
ing WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
R. Caruana, T. Jachims and L. Backstrom. 2004. KDD-
Cup 2004: results and analysis ACM SIGKDD Ex-
plorations Newslatter, 6(2):95?108.
C. J. Croach and B. Yang. 1992. Experiments in au-
tomatic statistical thesaurus construction. the 15th
799
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
77?88.
J. R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Workshop on Un-
supervised Lexical Acquisition. Proc. of the ACL
SIGLEX, 231?238.
J. V. Davis and I. S. Dhillon. 2006. Differential En-
tropic Clustering of Multivariate Gaussians. Ad-
vances in Neural Information Processing Systems
(NIPS).
J. V. Davis, B. Kulis, P. Jain, S. Sra and I. S. Dhillon.
2007. Information Theoretic Metric Learning. Proc.
of the International Conference on Machine Learn-
ing (ICML).
A. Globerson and S. Roweis. 2005. Metric Learning by
Collapsing Classes. Advances in Neural Information
Processing Systems (NIPS).
J. Goldberger, S. Roweis, G. Hinton and R. Salakhut-
dinov. 2004. Neighbourhood Component Analysis.
Advances in Neural Information Processing Systems
(NIPS).
G. Grefenstette. 1994. Explorations in Automatic The-
suarus Discovery. Kluwer Academic Publisher.
M. Hagiwara, Y. Ogawa, and K. Toyama. 2008. Con-
text Feature Selection for Distributional Similarity.
Proc. of IJCNLP-08, 553?560.
Z. Harris. 1985. Distributional Structure. Jerrold
J. Katz (ed.) The Philosophy of Linguistics. Oxford
University Press. 26?47.
T. Hastie and R. Tibshirani. 1996. Discriminant adap-
tive nearest neighbor classification. Pattern Analysis
and Machine Intelligence, 18, 607?616.
D. Hindle. 1990. Noun classification from predicate-
argument structures. Proc. of the ACL, 268?275.
J. J. Jiang and D. W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxon-
omy. Proceedings of International Conference on
Research on Computational Linguistics (ROCLING
X), Taiwan.
Y. Jing and B. Croft. 1994. An Association The-
saurus for Information Retrieval. Proc. of Recherche
d?Informations Assiste?e par Ordinateur (RIAO),
146?160.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. Proc. of COLING/ACL 1998, 786?
774.
L. Lee. 1999. Measures of distributional similarity.
Proc. of the ACL, 23?32
L. Lee. 2001. On the Effectiveness of the Skew Diver-
gence for Statistical Language Analysis. Artificial
Intelligence and Statistics 2001, 65?72.
S. Mohammad and G. Hirst. 2006. Distributional mea-
sures of concept-distance: A task-oriented evalua-
tion. Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Sydney, Australia.
P. Resnik. 1995. Using information content to evaluate
semantic similarity. Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-95), 448?453, Montreal, Canada.
G. Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. Foun-
dations of Computer Science: Potential - Theory -
Cognition, LNCS, Volume 1337, 499?506, Springer
Verlag, Berlin, Germany.
S. Shalev-Shwartz, Y. Singer and A. Y. Ng. 2004. On-
line and Batch Learning of Pseudo-Metrics. Proc. of
the International Conference on Machine Learning
(ICML).
M. Schutz and T. Joachims. 2003. Learning a Dis-
tance Metric from Relative Comparisons. Advances
in Neural Information Processing Systems (NIPS)..
J. Weeds, D. Weir and D. McCarthy. 2004. Character-
ising Measures of Lexical Distributional Similarity.
Proc. of COLING 2004, 1015?1021.
K. Q. Weinberger, J. Blitzer and L. K. Saul. 2005.
Distance Metric Learning for Large Margin Nearest
Neighbor Classification. Advances in Neural Infor-
mation Processing Systems (NIPS).
E. P. Xing, A. Y. Ng, M. Jordan and S. Russell 2002.
Distance metric learning with application to cluster-
ing with sideinformation. Advances in Neural Infor-
mation Processing Systems (NIPS).
Y. Yang and J. O. Pedersen. 1997. A Comparative
Study on Feature Selection in Text Categorization.
Proc. of the International Conference on Machine
Learning (ICML), 412?420.
800
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1166?1169,
Prague, June 2007. c?2007 Association for Computational Linguistics
Structural Correspondence Learning for Dependency Parsing
Nobuyuki Shimizu
Information Technology Center
University of Tokyo
Tokyo, Japan
shimizu@r.dl.itc.u-tokyo.ac.jp
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
Tokyo, Japan
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
Following (Blitzer et al, 2006), we present
an application of structural correspondence
learning to non-projective dependency pars-
ing (McDonald et al, 2005). To induce the
correspondences among dependency edges
from different domains, we looked at ev-
ery two tokens in a sentence and examined
whether or not there is a preposition, a de-
terminer or a helping verb between them.
Three binary linear classifiers were trained
to predict the existence of a preposition,
etc, on unlabeled data and we used singu-
lar value decomposition to induce new fea-
tures. During the training, the parser was
trained with these additional features in ad-
dition to these described in (McDonald et
al., 2005). We discriminatively trained our
parser in an on-line fashion using a vari-
ant of the voted perceptron (Collins, 2002;
Collins and Roark, 2004; Crammer and
Singer, 2003).
1 Introduction
We have recently seen growing popularity of depen-
dency parsing. It is no longer rare to see dependency
relations used as features, in tasks such as machine
translation (Ding and Palmer, 2005) and relation ex-
traction (Bunescu and Mooney, 2005). However,
there is one factor that prevents the use of depen-
dency parsing: sparseness of annotated corpora out-
side Wall Street Journal. In many situations we need
to parse sentences from a target domain with no la-
beled data, which is a different distribution from a
source domain where plentiful labeled training data
is available.
In this paper, we investigate the effectiveness of
structural correspondence learning (SCL) (Blitzer
et al, 2006) in the domain adaptation task given by
the CoNLL 2007. They hypothesize that a model
trained in the source domain using this common fea-
ture representation will generalize better to the tar-
get domain, and focus on using unlabeled data from
both the source and target domains to learn a com-
mon feature representation that is meaningful across
both domains.
The paper is structured as follows: in section
2, we review the decoding and learning aspects of
(McDonald et al, 2005), in section 3, structural cor-
respondence learning applied to dependency pars-
ing, and in section 4, we describe the experiments
and the features needed for the CoNLL 2006 shared
task.
2 Non-Projective Dependency Parsing
2.1 Dependency Structure
Let us define x to be a generic sequence of input to-
kens together with their POS tags and other morpho-
logical features, and y to be a generic dependency
structure, that is, a set of edges for x.
A labeled edge is a tuple ?DEPREL, i ? j? where
i is the start point of the edge, j is the end point, and
DEPREL is the label of the edge. The token at i is
the head of the token at j.
Table 1 shows our formulation of a structured pre-
diction problem. Given x, the input tokens and their
features (column 2 and 3, Table 1), the task is to pre-
1166
Index Token POS Labeled Edge
1 John NN ?SUBJ, 2 ? 1?
2 saw VBD ?PRED, 0 ? 2?
3 a DT ?DET, 4 ? 3?
4 dog NN ?OBJ, 2 ? 4?
5 yesterday RB ?ADJU, 2 ? 5?
6 which WDT ?MODWH, 7 ? 6?
7 was VBD ?MODPRED, 4 ? 7?
8 a DT ?DET, 10 ? 8?
9 Yorkshire NN ?MODN, 10 ? 9?
10 Terrier NN ?OBJ, 7 ? 10?
11 . . ?., 10 ? 11?
Table 1: Example Edges
dict y, the set of labeled edges (column 4, Table 1).
In this paper we use the common method of fac-
toring the score of the dependency structure as the
sum of the scores of all the labeled edges. A de-
pendency structure is characterized by its labeled
edges, and for each labeled edge, we have features
and corresponding weights. The score of a depen-
dency structure is the sum of these weights.
For example, let us say we would like to find the
score of the labeled edge ?OBJ, 2 ? 4?. This is the
edge going to the 4th token ?dog? in Table 1. The
features for this edge could be:
? There is an edge starting at saw, with the POS tag VBD,
and the distance between the head and the child is 2. (
head = wordj , headPOS = posj , dist(i, j) = |i? j| )
? There is an edge ending at dog, with the POS tag NN,
and the distance between the head and the child is 2. (
child = wordi, childPOS = posi, dist(i, j) = |i? j| )
In the upcoming section, we explain a decoding
algorithm for the dependency structures, and later
we give a method for learning the weight vector used
in the decoding.
2.2 Maximum Spanning Tree Algorithm
As in (McDonald et al, 2005), we use Chu-Liu-
Edmonds (CLE) algorithm (Chu and Liu, 1965; Ed-
monds, 1967) for decoding. CLE finds the Maxi-
mum Spanning Tree in a directed graph. The follow-
ing is a summary given in (McDonald et al, 2005).
Informally, the algorithm has each vertex in the
graph greedily select the incoming edge with high-
est weight.
Note that the edge is coming from the parent to
the child. That is, given a child node wordj , we are
finding the parent, or the head wordi such that the
edge (i, j) has the highest weight among all i, i 6= j.
If a tree results, then this must be the maximum
spanning tree. If not, there must be a cycle. The
procedure identifies a cycle and contracts it into a
single vertex and recalculates edge weights going
into and out of the cycle. It can be shown that a
maximum spanning tree on the contracted graph is
equivalent to a maximum spanning tree in the orig-
inal graph (Leonidas, 2003). Hence the algorithm
can recursively call itself on the new graph.
2.3 Online Learning
Again following (McDonald et al, 2005), we have
used the single best MIRA (Crammer and Singer,
2003), which is a ?margin aware? variant of percep-
tron (Collins, 2002; Collins and Roark, 2004) for
structured prediction. In short, the update is exe-
cuted when the decoder fails to predict the correct
parse, and we compare the correct parse yt and the
incorrect parse y? suggested by the decoding algo-
rithm. The weights of the features in y? will be low-
ered, and the weights of the features in yt will be
increased accordingly.
3 Domain Adaptation
Following (Blitzer et al, 2006), we present an appli-
cation of structural correspondence learning (SCL)
to non-projective dependency parsing (McDonald
et al, 2005). SCL is a method for adapting a clas-
sifier learned in a source domain to a target domain.
We assume that both domains have unlabeled data,
but only the source domain has labeled training data.
SCL works as follows: 1. Define a set of pivot
features on the unlabeled data from both domains. 2.
Use these pivot features to learn a mapping from the
original feature spaces of both domains to a shared,
low-dimensional real-valued feature space. A high
inner product in this new space indicates a high de-
gree of correspondence. 3. Use both the transformed
and original features from the source domain. 4.
Again using both the transformed and original fea-
tures, test the samples from the target domain. If we
learned a good mapping, then the effectiveness of
the classifier in the source domain should transfer to
the target domain.
To induce the correspondences among depen-
dency edges in the source domain and the target
domain, we looked at every two tokens in a sen-
tence and examined whether or not there is a prepo-
sition, a determiner or a helping verb between them.
Although no edge is present in unlabeled data, the
1167
presence of a preposition indicates that this edge be-
tween the tokens, if existed, will not be a noun mod-
ifier (in English corpus, this label is NMOD). Thus,
this induced feature should correlate with the label
of an edge candidate. We postulate that the label of
an edge candidate, if known, may allow the super-
vised learner to choose the correct edge among the
edge candidates in the target domain.
In the first step, we chose the presence of a prepo-
sition, a determiner or a helping verb between tokens
as pivot features. Then three binary linear classifiers
were trained to predict the existence of a preposi-
tion (prep), determiner (det) and helping verb (hv)
on unlabeled data and obtained a weight vector for
each classifier.
classifierprep(e) = sign(wprep?(e))
classifierdet(e) = sign(wdet?(e))
classifierhv(e) = sign(whv?(e))
The input to the above classifiers is an edge e in-
stead of a whole sentence x. ? is a mapping from
an edge to a feature vector. Since POS tags were
not available in unlabeled data, for pivot predictors,
we took the subset of the features given by an edge.
The features for pivot predictors are listed in Table 2.
The reminder of the features are the same as ones
used in (McDonald et al, 2005).
Using each weight vector as a column, we created
a weight matrix. W = [wprep|wdet|whv ]. And run a
singular value decomposition to induce a lower di-
mensional feature space. W = U?V . We then took
the transpose of the resulting unitary matrix, U?
which maps the original data to the space spanned
by the principal components, and applied it to the
feature vector of every potential edge. The origi-
nal feature vector is
(
fsubset
freminder
)
. We argument the
feature vector with the additional feature induced by
U?. The augmented feature vectors
( fsubset
freminder
U?fsubset
)
were used throughout the training and testing of the
dependency parser.
4 Experiments
Our experiments were conducted on CoNLL-2007
shared task domain adaptation track (Nivre et al,
2007) using treebanks (Marcus et al, 1993; Johans-
son and Nugues, 2007; Kulick et al, 2004).
Given an edge ?DEPREL, i, j?
head?1 = wordi?1
head = wordi
head+1 = wordi+1
child?1 = wordj?1
child = wordj
child+1 = wordj+1
Table 2: Binary Features for Pivot Predictors
4.1 Dependency Relation
The CLE algorithm works on a directed graph with
unlabeled edges. Since the CoNLL shared task
requires the labeling of edges, as a preprocessing
stage, we created a directed complete graph. Then
we labeled each edge with the highest scoring de-
pendency relation. This complete graph was given
to the CLE algorithm and the edge labels were never
altered in the course of finding the maximum span-
ning tree.
4.2 Features
The features we used for pivot predictors to classify
each edge ?DEPREL, i, j? are shown in Table 2. The
index i is the position of the parent and j is that of
the child.
wordj = the word token at the position j.
posj = the coarse part-of-speech at j.
No other features were used beyond the combina-
tions of the word token in Table 2.
The hardware used was an Intel CPU at 3.0 Ghz
with 32 GB of memory, and the software was writ-
ten in C++. While more iterations should help, due
to the time constraints, we were unable to complete
more training. The parser required a few days to
train.
5 Results
Unfortunately, we have discovered a bug in our
codes after submitting our results for the blind tests,
and the reported results in (Nivre et al, 2007) were
not representative of our approach. The current re-
sults (closed class) are shown in Table 3.
For the explanations of Labeled Attachment
Score, Unlabeled Attachment Score and Label Ac-
curacy, the readers are suggested to refer to the
shared task introductory paper (Nivre et al, 2007).
WSJ represents the application of the parser without
SCL to the source domain test set, and WSJ-SCL
the parser with SCL to the same test set. Similarily
1168
Domain LAS UAS Label Accuracy
WSJ 83.01% ? 83.43% 86.43% ? 86.81% 88.77% ? 88.99%
WSJ-SCL 83.43% ? 83.59% 86.87% ? 86.93% 88.75% ? 89.01%
Chem 74.75% ? 75.18% 80.74% ? 81.24% 82.34% ? 82.70%
Chem-SCL 75.04% ? 74.91% 81.02% ? 80.82% 82.18% ? 82.18%
Table 3: Labeled Attachment Score, Unlabeled Attachment Score and Label Accuracy
Chem and Chem-SCL represents the application of
the parser without SCL and with SCL to the source
domain test set respectively. We did batch learn-
ing by running the online algorithm 4 times. An
arrow ? indicates how the results after 2nd itera-
tion changed at the end of 4th iteration. Contrary
to our expectations, we seem to see SCL overfitting
to the source domain WSJ in this experiment. Due
to the lack of POS tags in unlabeled data, our fea-
ture set for pivot predictors uses tokens extensively
unlike that for the dependency parser. Since tokens
are not as abstract as POS tags, we suspect induced
features may have caused overfitting.
6 Conclusion
We presented an application of structural correspon-
dence learning to non-projective dependency pars-
ing. Effectiveness of SCL for domain adaptation is
mixed in this experiment perhaps due to the mis-
match between feature sets. Future work includes
use of more sophisticated features such as POS and
other morphological features, possibly a joint do-
main adaptation of POS tagging and dependency
parsing for unlabeled data as well as re-examination
of pivot features.
References
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of Empirical Methods in Natural Language Pro-
cessing (EMNLP).
R. Bunescu and R. Mooney. 2005. A shortest path de-
pendency kernel for relation extraction. In Proc. of
the Joint Conf. on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. In Science Sinica, page
14:13961400.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. of the 42rd Annual
Meeting of the ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of Empirical Methods
in Natural Language Processing (EMNLP).
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. In JMLR.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. of the 43rd Annual Meeting of the ACL.
J. Edmonds. 1967. Optimum branchings. In Journal of
Research of the National Bureau of Standards, page
71B:233240.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
G. Leonidas. 2003. Arborescence optimization problems
solvable by edmonds algorithm. In Theoretical Com-
puter Science, page 301:427 437.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Joint Conf. on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing (HLT/EMNLP).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
1169
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 603?611,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Deterministic shift-reduce parsing for unification-based grammars by 
using default unification 
 
 
Takashi Ninomiya 
Information Technology Center 
University of Tokyo, Japan 
ninomi@r.dl.itc.u-tokyo.ac.jp 
Takuya Matsuzaki 
Department of Computer Science 
University of Tokyo, Japan 
matuzaki@is.s.u-tokyo.ac.jp 
  
Nobuyuki Shimizu 
Information Technology Center 
University of Tokyo, Japan 
shimizu@r.dl.itc.u-tokyo.ac.jp 
Hiroshi Nakagawa 
Information Technology Center 
University of Tokyo, Japan 
nakagawa@dl.itc.u-tokyo.ac.jp
 
 
Abstract 
Many parsing techniques including pa-
rameter estimation assume the use of a 
packed parse forest for efficient and ac-
curate parsing.  However, they have sev-
eral inherent problems deriving from the 
restriction of locality in the packed parse 
forest.  Deterministic parsing is one of 
solutions that can achieve simple and fast 
parsing without the mechanisms of the 
packed parse forest by accurately choos-
ing search paths.  We propose (i) deter-
ministic shift-reduce parsing for unifica-
tion-based grammars, and (ii) best-first 
shift-reduce parsing with beam threshold-
ing for unification-based grammars.  De-
terministic parsing cannot simply be ap-
plied to unification-based grammar pars-
ing, which often fails because of its hard 
constraints.  Therefore, it is developed by 
using default unification, which almost 
always succeeds in unification by over-
writing inconsistent constraints in gram-
mars. 
1 Introduction 
Over the last few decades, probabilistic unifica-
tion-based grammar parsing has been investi-
gated intensively.  Previous studies (Abney, 
1997; Johnson et al, 1999; Kaplan et al, 2004; 
Malouf and van Noord, 2004; Miyao and Tsujii, 
2005; Riezler et al, 2000) defined a probabilistic 
model of unification-based grammars, including 
head-driven phrase structure grammar (HPSG), 
lexical functional grammar (LFG) and combina-
tory categorial grammar (CCG), as a maximum 
entropy model (Berger et al, 1996).  Geman and 
Johnson (Geman and Johnson, 2002) and Miyao 
and Tsujii (Miyao and Tsujii, 2002) proposed a 
feature forest, which is a dynamic programming 
algorithm for estimating the probabilities of all 
possible parse candidates.  A feature forest can 
estimate the model parameters without unpack-
ing the parse forest, i.e., the chart and its edges.  
Feature forests have been used successfully 
for probabilistic HPSG and CCG (Clark and Cur-
ran, 2004b; Miyao and Tsujii, 2005), and its 
parsing is empirically known to be fast and accu-
rate, especially with supertagging (Clark and 
Curran, 2004a; Ninomiya et al, 2007; Ninomiya 
et al, 2006).  Both estimation and parsing with 
the packed parse forest, however, have several 
inherent problems deriving from the restriction 
of locality.  First, feature functions can be de-
fined only for local structures, which limit the 
parser?s performance.  This is because parsers 
segment parse trees into constituents and factor 
equivalent constituents into a single constituent 
(edge) in a chart to avoid the same calculation.  
This also means that the semantic structures must 
be segmented.  This is a crucial problem when 
we think of designing semantic structures other 
than predicate argument structures, e.g., syn-
chronous grammars for machine translation.  The 
size of the constituents will be exponential if the 
semantic structures are not segmented.  Lastly, 
we need delayed evaluation for evaluating fea-
ture functions.  The application of feature func-
tions must be delayed until all the values in the 
603
segmented constituents are instantiated.  This is 
because values in parse trees can propagate any-
where throughout the parse tree by unification.  
For example, values may propagate from the root 
node to terminal nodes, and the final form of the 
terminal nodes is unknown until the parser fi-
nishes constructing the whole parse tree.  Conse-
quently, the design of grammars, semantic struc-
tures, and feature functions becomes complex.  
To solve the problem of locality, several ap-
proaches, such as reranking (Charniak and John-
son, 2005), shift-reduce parsing (Yamada and 
Matsumoto, 2003), search optimization learning 
(Daum? and Marcu, 2005) and sampling me-
thods (Malouf and van Noord, 2004; Nakagawa, 
2007), were studied. 
In this paper, we investigate shift-reduce pars-
ing approach for unification-based grammars 
without the mechanisms of the packed parse for-
est.  Shift-reduce parsing for CFG and dependen-
cy parsing have recently been studied (Nivre and 
Scholz, 2004; Ratnaparkhi, 1997; Sagae and La-
vie, 2005, 2006; Yamada and Matsumoto, 2003), 
through approaches based essentially on deter-
ministic parsing.  These techniques, however, 
cannot simply be applied to unification-based 
grammar parsing because it can fail as a result of 
its hard constraints in the grammar.  Therefore, 
in this study, we propose deterministic parsing 
for unification-based grammars by using default 
unification, which almost always succeeds in 
unification by overwriting inconsistent con-
straints in the grammars.  We further pursue 
best-first shift-reduce parsing for unification-
based grammars. 
Sections 2 and 3 explain unification-based 
grammars and default unification, respectively.  
Shift-reduce parsing for unification-based gram-
mars is presented in Section 4.  Section 5 dis-
cusses our experiments, and Section 6 concludes 
the paper. 
2 Unification-based grammars 
A unification-based grammar is defined as a pair 
consisting of a set of lexical entries and a set of 
phrase-structure rules.  The lexical entries ex-
press word-specific characteristics, while the 
phrase-structure rules describe constructions of 
constituents in parse trees.  Both the phrase-
structure rules and the lexical entries are 
represented by feature structures (Carpenter, 
1992), and constraints in the grammar are forced 
by unification.  Among the phrase-structure rules, 
a binary rule is a partial function: ? ? ? ? ? , 
where ? is the set of all possible feature struc-
tures.  The binary rule takes two partial parse 
trees as daughters and returns a larger partial 
parse tree that consists of the daughters and their 
mother.  A unary rule is a partial function: 
? ? ?, which corresponds to a unary branch. 
In the experiments, we used an HPSG (Pollard 
and Sag, 1994), which is one of the sophisticated 
unification-based grammars in linguistics.  Gen-
erally, an HPSG has a small number of phrase-
structure rules and a large number of lexical en-
tries.  Figure 1 shows an example of HPSG pars-
ing of the sentence, ?Spring has come.?  The up-
per part of the figure shows a partial parse tree 
for ?has come,? which is obtained by unifying 
each of the lexical entries for ?has? and ?come? 
with a daughter feature structure of the head-
complement rule.  Larger partial parse trees are 
obtained by repeatedly applying phrase-structure 
rules to lexical/phrasal partial parse trees.  Final-
ly, the parse result is output as a parse tree that 
dominates the sentence. 
3 Default unification 
Default unification was originally investigated in 
a series of studies of lexical semantics, in order 
to deal with default inheritance in a lexicon.  It is 
also desirable, however, for robust processing, 
because (i) it almost always succeeds and (ii) a 
feature structure is relaxed such that the amount 
of information is maximized (Ninomiya et al, 
2002).  In our experiments, we tested a simpli-
fied version of Copestake?s default unification.  
Before explaining it, we first explain Carpenter?s 
 
Figure 1: Example of HPSG parsing. 
 
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
HEAD  noun
SUBJ  <      SUBJ  <>     >
COMPS <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
head-comp
Spring has come
1
1 12
2
HEAD  verb
SUBJ  <>
COMPS <>
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
subject-head
head-comp
Spring has come
1
1 11 2
2
604
two definitions of default unification (Carpenter, 
1993). 
 
(Credulous Default Unification) 
? ?? ? ? =  ?? ? ????
? ? ? is maximal such
that ? ? ? ?is defined ? 
 
(Skeptical Default Unification) 
? ?? ? ? =  ?(? ?
?
? ?) 
 
?  is called a strict feature structure, whose in-
formation must not be lost, and ? is called a de-
fault feature structure, whose information can be 
lost but as little as possible so that ? and ? can 
be unified. 
Credulous default unification is greedy, in that 
it tries to maximize the amount of information 
from the default feature structure, but it results in 
a set of feature structures.  Skeptical default un-
ification simply generalizes the set of feature 
structures resulting from credulous default unifi-
cation.  Skeptical default unification thus leads to 
a unique result so that the default information 
that can be found in every result of credulous 
default unification remains.  The following is an 
example of skeptical default unification: 
 
[F: ?]  ?? ? ?
F: 1 ?
G: 1
H: ?
? =  ???
F: ?
G: ?
H: ?
? , ?
F: 1 ?
G: 1
H: ?
?? = ?
F: ?
G: ?
H: ?
?. 
 
Copestake mentioned that the problem with 
Carpenter?s default unification is its time com-
plexity (Copestake, 1993).  Carpenter?s default 
unification takes exponential time to find the op-
timal answer, because it requires checking the 
unifiability of the power set of constraints in a 
default feature structure.  Copestake thus pro-
posed another definition of default unification, as 
follows. Let ??(?) be a function that returns a 
set of path values in ?, and let ??(?) be a func-
tion that returns a set of path equations, i.e., in-
formation about structure sharing in ?. 
 
(Copestake?s default unification) 
? ?? ? ? =  ? ? ? ???
? ? ??(?)and there is no ?? ? ??(?)
such that ? ? ??is defined and
? ? ? ? ??is not defined
?, 
where ? = ? ? ???(?). 
 
Copestake?s default unification works effi-
ciently because all path equations in the default 
feature structure are unified with the strict fea-
ture structures, and because the unifiability of 
path values is checked one by one for each node 
in the result of unifying the path equations.  The 
implementation is almost the same as that of 
normal unification, but each node of a feature 
structure has a set of values marked as ?strict? or 
?default.?  When types are involved, however, it 
is not easy to find unifiable path values in the 
default feature structure.  Therefore, we imple-
mented a more simply typed version of Corpes-
take?s default unification. 
Figure 2 shows the algorithm by which we 
implemented the simply typed version.  First, 
each node is marked as ?strict? if it belongs to a 
strict feature structure and as ?default? otherwise. 
The marked strict and default feature structures 
procedure forced_unification(p, q) 
   queue := {?p, q?}; 
   while( queue is not empty ) 
      ?p, q? := shift(queue); 
      p := deref(p); q := deref(q); 
      if p ? q 
         ?(p) ?  ?(p) ? ?(q); 
         ?(q) ? ptr(p); 
         forall f ? feat(p)? feat(q) 
            if f ? feat(p) ? f ? feat(q) 
               queue := queue ? ??(f, p), ?(f, q)?; 
            if f ? feat(p) ? f ? feat(q) 
               ?(f, p) ?  ?(f, q); 
procedure mark(p, m) 
   p := deref(p); 
   if p has not been visited 
      ?(p) := {??(p),m?}; 
      forall f ? feat(p) 
         mark(?(f, p), m); 
procedure collapse_defaults(p) 
   p := deref(p); 
   if p has not been visited 
      ts := ?; td := ?; 
      forall ?t, ??????? ? ?(p) 
         ts := ts ? t; 
      forall ?t, ???????? ? ?(p) 
         td := td ? t; 
      if ts is not defined 
         return false; 
      if ts ? td is defined 
         ?(p) := ts ? td; 
      else 
         ?(p) := ts; 
      forall f ? feat(p) 
         collapse_defaults(?(f, p)); 
procedure default_unification(p, q) 
   mark(p, ??????); 
   mark(q, ???????); 
   forced_unification(p, q); 
   collapse_defaults(p); 
 
?(p) is (i) a single type, (ii) a pointer, or (iii) a set of pairs of 
types and markers in the feature structure node p. 
A marker indicates that the types in a feature structure node 
originally belong to the strict feature structures or the default 
feature structures. 
A pointer indicates that the node has been unified with other 
nodes and it points the unified node.  A function deref tra-
verses pointer nodes until it reaches to non-pointer node. 
?(f, p) returns a feature structure node which is reached by 
following a feature f from p. 
 
Figure 2: Algorithm for the simply typed ver-
sion of Corpestake?s default unification. 
605
are unified, whereas the types in the feature 
structure nodes are not unified but merged as a 
set of types.  Then, all types marked as ?strict? 
are unified into one type for each node.  If this 
fails, the default unification also returns unifica-
tion failure as its result.  Finally, each node is 
assigned a single type, which is the result of type 
unification for all types marked as both ?default? 
and ?strict? if it succeeds or all types marked 
only as ?strict? otherwise. 
4 Shift-reduce parsing for unification-
based grammars 
Non-deterministic shift-reduce parsing for unifi-
cation-based grammars has been studied by Bris-
coe and Carroll (Briscoe and Carroll, 1993).  
Their algorithm works non-deterministically with 
the mechanism of the packed parse forest, and 
hence it has the problem of locality in the packed 
parse forest.  This section explains our shift-
reduce parsing algorithms, which are based on 
deterministic shift-reduce CFG parsing (Sagae 
and Lavie, 2005) and best-first shift-reduce CFG 
parsing (Sagae and Lavie, 2006).  Sagae?s parser 
selects the most probable shift/reduce actions and 
non-terminal symbols without assuming explicit 
CFG rules.  Therefore, his parser can proceed 
deterministically without failure.  However, in 
the case of unification-based grammars, a deter-
ministic parser can fail as a result of its hard con-
straints in the grammar.  We propose two new 
shift-reduce parsing approaches for unification-
based grammars: deterministic shift-reduce pars-
ing and shift-reduce parsing by backtracking and 
beam search.  The major difference between our 
algorithm and Sagae?s algorithm is that we use 
default unification.  First, we explain the deter-
ministic shift-reduce parsing algorithm, and then 
we explain the shift-reduce parsing with back-
tracking and beam search. 
4.1 Deterministic shift-reduce parsing for 
unification-based grammars 
The deterministic shift-reduce parsing algorithm 
for unification-based grammars mainly compris-
es two data structures: a stack S, and a queue W.  
Items in S are partial parse trees, including a lex-
ical entry and a parse tree that dominates the 
whole input sentence.  Items in W are words and 
POSs in the input sentence.  The algorithm de-
fines two types of parser actions, shift and reduce, 
as follows. 
? Shift: A shift action removes the first item 
(a word and a POS) from W.  Then, one 
lexical entry is selected from among the 
candidate lexical entries for the item.  Fi-
nally, the selected lexical entry is put on 
the top of the stack. 
Common features: Sw(i), Sp(i), Shw(i), Shp(i), Snw(i), Snp(i), 
Ssy(i), Shsy(i), Snsy(i), wi-1, wi,wi+1, pi-2, pi-1, pi, pi+1, 
pi+2, pi+3 
Binary reduce features: d, c, spl, syl, hwl, hpl, hll, spr, syr, 
hwr, hpr, hlr 
Unary reduce features: sy, hw, hp, hl 
 
Sw(i) ? head word of i-th item from the top of the stack 
Sp(i) ? head POS of i-th item from the top of the stack 
Shw(i) ? head word of the head daughter of i-th item from the 
top of the stack 
Shp(i) ? head POS of the head daughter of i-th item from the 
top of the stack 
Snw(i) ? head word of the non-head daughter of i-th item 
from the top of the stack 
Snp(i) ? head POS of the non-head daughter of i-th item from 
the top of the stack 
Ssy(i) ? symbol of phrase category of the i-th item from the 
top of the stack 
Shsy(i) ? symbol of phrase category of the head daughter of 
the i-th item from the top of the stack 
Snsy(i) ? symbol of phrase category of the non-head daughter 
of the i-th item from the top of the stack 
d ? distance between head words of daughters 
c ? whether a comma exists between daughters and/or inside 
daughter phrases 
sp ? the number of words dominated by the phrase 
sy ? symbol of phrase category 
hw ? head word 
hp ? head POS 
hl ? head lexical entry 
 
Figure 3: Feature templates. 
Shift Features 
  [Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [wi-1, wi] [wi, wi+1] [pi-1, 
wi] [pi, wi] [pi+1, wi] [pi, pi+1, pi+2, pi+3] [pi-2, pi-1, pi] 
[pi-1, pi, pi+1] [pi, pi+1, pi+2] [pi-2, pi-1] [pi-1, pi] [pi, 
pi+1] [pi+1, pi+2] 
 
Binary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [d,c,hw,hp,hl] [d,c,hw,hp] [d, 
c, hw, hl] [d, c, sy, hw] [c, sp, hw, hp, hl] [c, sp, hw, hp] [c, 
sp, hw,hl] [c, sp, sy, hw] [d, c, hp, hl] [d, c, hp] [d, c, hl] [d, 
c, sy] [c, sp, hp, hl] [c, sp, hp] [c, sp, hl] [c, sp, sy] 
 
Unary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [hw, hp, hl] [hw, hp] [hw, hl] 
[sy, hw] [hp, hl] [hp] [hl] [sy]
 
Figure 4: Combinations of feature templates. 
606
? Binary Reduce: A binary reduce action 
removes two items from the top of the 
stack.  Then, partial parse trees are derived 
by applying binary rules to the first re-
moved item and the second removed item 
as a right daughter and left daughter, re-
spectively.  Among the candidate partial 
parse trees, one is selected and put on the 
top of the stack. 
? Unary Reduce: A unary reduce action re-
moves one item from the top of the stack.  
Then, partial parse trees are derived by 
applying unary rules to the removed item.  
Among the candidate partial parse trees, 
one is selected and put on the top of the 
stack. 
Parsing fails if there is no candidate for selec-
tion (i.e., a dead end).  Parsing is considered suc-
cessfully finished when W is empty and S has 
only one item which satisfies the sentential con-
dition: the category is verb and the subcategori-
zation frame is empty.  Parsing is considered a 
non-sentential success when W is empty and S 
has only one item but it does not satisfy the sen-
tential condition. 
In our experiments, we used a maximum en-
tropy classifier to choose the parser?s action.  
Figure 3 lists the feature templates for the clas-
sifier, and Figure 4 lists the combinations of fea-
ture templates.  Many of these features were tak-
en from those listed in (Ninomiya et al, 2007), 
(Miyao and Tsujii, 2005) and (Sagae and Lavie, 
2005), including global features defined over the 
information in the stack, which cannot be used in 
parsing with the packed parse forest.  The fea-
tures for selecting shift actions are the same as 
the features used in the supertagger (Ninomiya et 
al., 2007).  Our shift-reduce parsers can be re-
garded as an extension of the supertagger. 
The deterministic parsing can fail because of 
its grammar?s hard constraints.  So, we use de-
fault unification, which almost always succeeds 
in unification.  We assume that a head daughter 
(or, an important daughter) is determined for 
each binary rule in the unification-based gram-
mar.   Default unification is used in the binary 
rule application in the same way as used in Ni-
nomiya?s offline robust parsing (Ninomiya et al, 
2002), in which a binary rule unified with the 
head daughter is the strict feature structure and 
the non-head daughter is the default feature 
structure, i.e.,  (? ? ?) ?? ??, where R is a bi-
nary rule, H is a head daughter and NH is a non-
head daughter.  In the experiments, we used the 
simply typed version of Copestake?s default un-
ification in the binary rule application1.  Note 
that default unification was always used instead 
of normal unification in both training and evalua-
tion in the case of the parsers using default unifi-
cation.  Although Copestake?s default unification 
almost always succeeds, the binary rule applica-
tion can fail if the binary rule cannot be unified 
with the head daughter, or inconsistency is 
caused by path equations in the default feature 
structures.  If the rule application fails for all the 
binary rules, backtracking or beam search can be 
used for its recovery as explained in Section 4.2.  
In the experiments, we had no failure in the bi-
nary rule application with default unification. 
4.2 Shift-reduce parsing by backtracking 
and beam-search 
Another approach for recovering from the pars-
ing failure is backtracking.  When parsing fails 
or ends with non-sentential success, the parser?s 
state goes back to some old state (backtracking), 
and it chooses the second best action and tries 
parsing again.  The old state is selected so as to 
minimize the difference in the probabilities for 
selecting the best candidate and the second best 
candidate.  We define a maximum number of 
backtracking steps while parsing a sentence.  
Backtracking repeats until parsing finishes with 
sentential success or reaches the maximum num-
ber of backtracking steps.  If parsing fails to find 
a parse tree, the best continuous partial parse 
trees are output for evaluation. 
From the viewpoint of search algorithms, pars-
ing with backtracking is a sort of depth-first 
search algorithms.  Another possibility is to use 
the best-first search algorithm.  The best-first 
parser has a state priority queue, and each state 
consists of a tree stack and a word queue, which 
are the same stack and queue explained in the 
shift-reduce parsing algorithm.  Parsing proceeds 
by applying shift-reduce actions to the best state 
in the state queue.  First, the best state is re-
                                                 
1 We also implemented Ninomiya?s default unification, 
which can weaken path equation constraints.  In the prelim-
inary experiments, we tested binary rule application given 
as (? ? ?) ?? ?? with Copestake?s default unification, 
(? ? ?) ?? ?? with Ninomiya?s default unification, and 
(? ? ??) ?? ? with Ninomiya?s default unification.  How-
ever, there was no significant difference of F-score among 
these three methods.  So, in the main experiments, we only 
tested (? ? ?) ?? ?? with Copestake?s default unification 
because this method is simple and stable. 
607
moved from the state queue, and then shift-
reduce actions are applied to the state.  The new-
ly generated states as results of the shift-reduce 
actions are put on the queue.  This process re-
peats until it generates a state satisfying the sen-
tential condition.  We define the probability of a 
parsing state as the product of the probabilities of 
selecting actions that have been taken to reach 
the state.  We regard the state probability as the 
objective function in the best-first search algo-
rithm, i.e., the state with the highest probabilities 
is always chosen in the algorithm.  However, the 
best-first algorithm with this objective function 
searches like the breadth-first search, and hence, 
parsing is very slow or cannot be processed in a 
reasonable time.  So, we introduce beam thre-
sholding to the best-first algorithm.  The search 
space is pruned by only adding a new state to the 
state queue if its probability is greater than 1/b of 
the probability of the best state in the states that 
has had the same number of shift-reduce actions.  
In what follows, we call this algorithm beam 
search parsing. 
In the experiments, we tested both backtrack-
ing and beam search with/without default unifi-
cation.  Note that, the beam search parsing for 
unification-based grammars is very slow com-
pared to the shift-reduce CFG parsing with beam 
search.  This is because we have to copy parse 
trees, which consist of a large feature structures, 
in every step of searching to keep many states on 
the state queue.  In the case of backtracking, co-
pying is not necessary. 
5 Experiments 
We evaluated the speed and accuracy of parsing 
with Enju 2.3?, an HPSG for English (Miyao and 
Tsujii, 2005).  The lexicon for the grammar was 
extracted from Sections 02-21 of the Penn Tree-
bank (39,832 sentences).  The grammar consisted 
of 2,302 lexical entries for 11,187 words.  Two 
probabilistic classifiers for selecting shift-reduce 
actions were trained using the same portion of 
the treebank.  One is trained using normal unifi-
cation, and the other is trained using default un-
ification. 
We measured the accuracy of the predicate ar-
gument relation output of the parser.  A predi-
cate-argument relation is defined as a tuple 
??, ??, ?, ???, where ? is the predicate type (e.g., 
  Section 23 (Gold POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non- 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 87.26 86.50 86.88 604 - - - - - 
(Ninomiya et al, 2007) 89.78 89.28 89.53 234 - - - - - 
Ours 
det 76.45 82.00 79.13 122 0 - 867 35 1514 
det+du 87.78 87.45 87.61 256 0 - 0 117 2299 
back40 81.93 85.31 83.59 519 18986 - 386 23 2007 
back10 + du 87.79 87.46 87.62 267 574 - 0 45 2371 
beam(7.4) 86.17 87.77 86.96 510 - 226 369 30 2017 
beam(20.1)+du 88.67 88.79 88.48 457 - 205 0 16 2400 
beam(403.4) 89.98 89.92 89.95 10246 - 2822 71 14 2331 
           
  Section 23 (Auto POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 84.96 84.25 84.60 674 - - - - - 
(Ninomiya et al, 2007) 87.28 87.05 87.17 260 - - - - - 
(Matsuzaki et al, 2007)  86.93 86.47 86.70 30 - - - - - 
(Sagae et al, 2007)  88.50 88.00 88.20 - - - - - - 
Ours 
det 74.13 80.02 76.96 127 0 - 909 31 1476 
det+du 85.93 85.72 85.82 252 0 - 0 124 2292 
back40 78.71 82.86 80.73 568 21068 - 438 27 1951 
back10 + du 85.96 85.75 85.85 270 589 - 0 46 2370 
beam(7.4) 83.84 85.82 84.82 544 - 234 421 33 1962 
beam(20.1)+du 86.59 86.36 86.48 550 - 222 0 21 2395 
beam(403.4) 87.70 87.86 87.78 16822 - 4553 89 16 2311 
 
Table 1: Experimental results for Section 23. 
608
adjective, intransitive verb), ?? is the head word 
of the predicate, ? is the argument label (MOD-
ARG, ARG1, ?, ARG4), and ??  is the head 
word of the argument.  The labeled precision 
(LP) / labeled recall (LR) is the ratio of tuples 
correctly identified by the parser, and the labeled 
F-score (LF) is the harmonic mean of the LP and 
LR. This evaluation scheme was the same one 
used in previous evaluations of lexicalized 
grammars (Clark and Curran, 2004b; Hocken-
maier, 2003; Miyao and Tsujii, 2005).  The expe-
riments were conducted on an Intel Xeon 5160 
server with 3.0-GHz CPUs. Section 22 of the 
Penn Treebank was used as the development set, 
and the performance was evaluated using sen-
tences of ? 100 words in Section 23.  The LP, 
LR, and LF were evaluated for Section 23. 
Table 1 lists the results of parsing for Section 
23.  In the table, ?Avg. time? is the average pars-
ing time for the tested sentences.  ?# of backtrack? 
is the total number of backtracking steps that oc-
curred during parsing.  ?Avg. # of states? is the 
average number of states for the tested sentences.  
?# of dead end? is the number of sentences for 
which parsing failed.  ?# of non-sentential suc-
cess? is the number of sentences for which pars-
ing succeeded but did not generate a parse tree 
satisfying the sentential condition.  ?det? means 
the deterministic shift-reduce parsing proposed 
in this paper.  ?back?? means shift-reduce pars-
ing with backtracking at most ? times for each 
sentence.  ?du? indicates that default unification 
was used.  ?beam?? means best-first shift-reduce 
parsing with beam threshold ?.  The upper half 
of the table gives the results obtained using gold 
POSs, while the lower half gives the results ob-
tained using an automatic POS tagger.  The max-
imum number of backtracking steps and the 
beam threshold were determined by observing 
the performance for the development set (Section 
22) such that the LF was maximized with a pars-
ing time of less than 500 ms/sentence (except 
?beam(403.4)?). The performance of 
?beam(403.4)? was evaluated to see the limit of 
the performance of the beam-search parsing. 
Deterministic parsing without default unifica-
tion achieved accuracy with an LF of around 
79.1% (Section 23, gold POS).  With backtrack-
ing, the LF increased to 83.6%.  Figure 5 shows 
the relation between LF and parsing time for the 
development set (Section 22, gold POS).  As 
seen in the figure, the LF increased as the parsing 
time increased.  The increase in LF for determi-
nistic parsing without default unification, how-
ever, seems to have saturated around 83.3%.  
Table 1 also shows that deterministic parsing 
with default unification achieved higher accuracy, 
with an LF of around 87.6% (Section 23, gold 
POS), without backtracking.  Default unification 
is effective: it ran faster and achieved higher ac-
curacy than deterministic parsing with normal 
unification.  The beam-search parsing without 
default unification achieved high accuracy, with 
an LF of around 87.0%, but is still worse than 
deterministic parsing with default unification.  
However, with default unification, it achieved 
the best performance, with an LF of around 
88.5%, in the settings of parsing time less than 
500ms/sentence for Section 22. 
For comparison with previous studies using 
the packed parse forest, the performances of 
Miyao?s parser, Ninomiya?s parser, Matsuzaki?s 
parser and Sagae?s parser are also listed in Table 
1.  Miyao?s parser is based on a probabilistic 
model estimated only by a feature forest.  Nino-
miya?s parser is a mixture of the feature forest 
 
Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS). 
 
82.00%
83.00%
84.00%
85.00%
86.00%
87.00%
88.00%
89.00%
90.00%
0 1 2 3 4 5 6 7 8
LF
Avg. parsing time (s/sentence)
back
back+du
beam
beam+du
609
and an HPSG supertagger.  Matsuzaki?s parser 
uses an HPSG supertagger and CFG filtering.  
Sagae?s parser is a hybrid parser with a shallow 
dependency parser.  Though parsing without the 
packed parse forest is disadvantageous to the 
parsing with the packed parse forest in terms of 
search space complexity, our model achieved 
higher accuracy than Miyao?s parser. 
?beam(403.4)? in Table 1 and ?beam? in Fig-
ure 5 show possibilities of beam-search parsing.  
?beam(403.4)? was very slow, but the accuracy 
was higher than any other parsers except Sagae?s 
parser. 
Table 2 shows the behaviors of default unifi-
cation for ?det+du.?  The table shows the 20 
most frequent path values that were overwritten 
by default unification in Section 22.  In most of 
the cases, the overwritten path values were in the 
selection features, i.e., subcategorization frames 
(COMPS:, SUBJ:, SPR:, CONJ:) and modifiee 
specification (MOD:).  The column of ?Default 
type? indicates the default types which were 
overwritten by the strict types in the column of 
?Strict type,? and the last column is the frequency 
of overwriting.  ?cons? means a non-empty list, 
and ?nil? means an empty list.  In most of the 
cases, modifiee and subcategorization frames 
were changed from empty to non-empty and vice 
versa.  From the table, overwriting of head in-
formation was also observed, e.g., ?noun? was 
changed to ?verb.? 
6 Conclusion and Future Work 
We have presented shift-reduce parsing approach 
for unification-based grammars, based on deter-
ministic shift-reduce parsing.  First, we presented 
deterministic parsing for unification-based 
grammars.  Deterministic parsing was difficult in 
the framework of unification-based grammar 
parsing, which often fails because of its hard 
constraints.  We introduced default unification to 
avoid the parsing failure.  Our experimental re-
sults have demonstrated the effectiveness of de-
terministic parsing with default unification.  The 
experiments revealed that deterministic parsing 
with default unification achieved high accuracy, 
with a labeled F-score (LF) of 87.6% for Section 
23 of the Penn Treebank with gold POSs.  
Second, we also presented the best-first parsing 
with beam search for unification-based gram-
mars.  The best-first parsing with beam search 
achieved the best accuracy, with an LF of 87.0%, 
in the settings without default unification.  De-
fault unification further increased LF from 
87.0% to 88.5%.  By widening the beam width, 
the best-first parsing achieved an LF of 90.0%. 
References 
Abney, Steven P. 1997. Stochastic Attribute-Value 
Grammars. Computational Linguistics, 23(4), 597-
618. 
Path Strict 
type 
Default 
type 
Freq
SYNSEM:LOCAL:CAT:HEAD:MOD: cons nil 434
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD:MOD: cons nil 237
SYNSEM:LOCAL:CAT:VAL:SUBJ: nil cons 231
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: nil cons 125
SYNSEM:LOCAL:CAT:HEAD: verb noun 110
SYNSEM:LOCAL:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:hd:LOCAL:CAT: 
HEAD:MOD: 
cons nil 101
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:
hd:LOCAL:CAT:HEAD:MOD: 
cons nil 96
SYNSEM:LOCAL:CAT:HEAD:MOD: nil cons 92
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: verb noun 91
SYNSEM:LOCAL:CAT:VAL:SUBJ: cons nil 79
SYNSEM:LOCAL:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD: nominal verb 75
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:HEAD:MOD: cons nil 74
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:HEAD:MOD: cons nil 69
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:COMPS:hd:LOCAL:CAT:HEAD: nominal verb 63
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: cons nil 63
? ? ? ?
Total   10,598
 
Table 2: Path values overwritten by default unification in Section 22. 
610
Berger, Adam, Stephen Della Pietra, and Vincent Del-
la Pietra. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational Lin-
guistics, 22(1), 39-71. 
Briscoe, Ted and John Carroll. 1993. Generalized 
probabilistic LR-Parsing of natural language (cor-
pora) with unification-based grammars. Computa-
tional Linguistics, 19(1), 25-59. 
Carpenter, Bob. 1992. The Logic of Typed Feature 
Structures: Cambridge University Press. 
Carpenter, Bob. 1993. Skeptical and Credulous De-
fault Unification with Applications to Templates 
and Inheritance. In Inheritance, Defaults, and the 
Lexicon. Cambridge: Cambridge University Press. 
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative 
Reranking. In proc. of ACL'05, pp. 173-180. 
Clark, Stephen and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG 
parsing. In proc. of COLING-04, pp. 282-288. 
Clark, Stephen and James R. Curran. 2004b. Parsing 
the WSJ using CCG and log-linear models. In proc. 
of ACL'04, pp. 104-111. 
Copestake, Ann. 1993. Defaults in Lexical Represen-
tation. In Inheritance, Defaults, and the Lexicon. 
Cambridge: Cambridge University Press. 
Daum?, Hal III and Daniel Marcu. 2005. Learning as 
Search Optimization: Approximate Large Margin 
Methods for Structured Prediction. In proc. of 
ICML 2005. 
Geman, Stuart and Mark Johnson. 2002. Dynamic 
programming for parsing and estimation of sto-
chastic unification-based grammars. In proc. of 
ACL'02, pp. 279-286. 
Hockenmaier, Julia. 2003. Parsing with Generative 
Models of Predicate-Argument Structure. In proc. 
of ACL'03, pp. 359-366. 
Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi 
Chi, and Stefan Riezler. 1999. Estimators for Sto-
chastic ``Unification-Based'' Grammars. In proc. of 
ACL '99, pp. 535-541. 
Kaplan, R. M., S. Riezler, T. H. King, J. T. Maxwell 
III, and A. Vasserman. 2004. Speed and accuracy 
in shallow and deep stochastic parsing. In proc. of 
HLT/NAACL'04. 
Malouf, Robert and Gertjan van Noord. 2004. Wide 
Coverage Parsing with Stochastic Attribute Value 
Grammars. In proc. of IJCNLP-04 Workshop 
``Beyond Shallow Analyses''. 
Matsuzaki, Takuya, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2007. Efficient HPSG Parsing with Supertag-
ging and CFG-filtering. In proc. of IJCAI 2007, pp. 
1671-1676. 
Miyao, Yusuke and Jun'ichi Tsujii. 2002. Maximum 
Entropy Estimation for Feature Forests. In proc. of 
HLT 2002, pp. 292-297. 
Miyao, Yusuke and Jun'ichi Tsujii. 2005. Probabilistic 
disambiguation models for wide-coverage HPSG 
parsing. In proc. of ACL'05, pp. 83-90. 
Nakagawa, Tetsuji. 2007. Multilingual dependency 
parsing using global features. In proc. of the 
CoNLL Shared Task Session of EMNLP-CoNLL 
2007, pp. 915-932. 
Ninomiya, Takashi, Takuya Matsuzaki, Yusuke 
Miyao, and Jun'ichi Tsujii. 2007. A log-linear 
model with an n-gram reference distribution for ac-
curate HPSG parsing. In proc. of IWPT 2007, pp. 
60-68. 
Ninomiya, Takashi, Takuya Matsuzaki, Yoshimasa 
Tsuruoka, Yusuke Miyao, and Jun'ichi Tsujii. 2006. 
Extremely Lexicalized Models for Accurate and 
Fast HPSG Parsing. In proc. of EMNLP 2006, pp. 
155-163. 
Ninomiya, Takashi, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2002. Lenient Default Unification for Robust 
Processing within Unification Based Grammar 
Formalisms. In proc. of COLING 2002, pp. 744-
750. 
Nivre, Joakim and Mario Scholz. 2004. Deterministic 
dependency parsing of English text. In proc. of 
COLING 2004, pp. 64-70. 
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar: University of Chicago 
Press. 
Ratnaparkhi, Adwait. 1997. A linear observed time 
statistical parser based on maximum entropy mod-
els. In proc. of EMNLP'97. 
Riezler, Stefan, Detlef Prescher, Jonas Kuhn, and 
Mark Johnson. 2000. Lexicalized Stochastic Mod-
eling of Constraint-Based Grammars using Log-
Linear Measures and EM Training. In proc. of 
ACL'00, pp. 480-487. 
Sagae, Kenji and Alon Lavie. 2005. A classifier-based 
parser with linear run-time complexity. In proc. of 
IWPT 2005. 
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In proc. of COL-
ING/ACL on Main conference poster sessions, pp. 
691-698. 
Sagae, Kenji, Yusuke Miyao, and Jun'ichi Tsujii. 
2007. HPSG parsing with shallow dependency 
constraints. In proc. of ACL 2007, pp. 624-631. 
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector 
Machines. In proc. of IWPT-2003. 
 
611
HITIQA: Towards Analytical Question Answering 
Sharon Small1, Tomek Strzalkowski1, Ting Liu1, Sean Ryan1, Robert Salkin1,  
Nobuyuki Shimizu1, Paul Kantor2, Diane Kelly2, Robert Rittman2, Nina Wacholder2 
 
1The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tomek,tl7612,seanryan, 
rs6021,ns3202}@albany.edu 
2Rutgers University 
4 Huntington Street 
New Brunswick, NJ 08904 
{kantor,diane,hitiqa, 
wacholder}@scils.rutgers.edu
 
Abstract 
In this paper we describe the analytic 
question answering system HITIQA (High-
Quality Interactive Question Answering) 
which has been developed over the last 2 years 
as an advanced research tool for information 
analysts. HITIQA is an interactive open-
domain question answering technology 
designed to allow analysts to pose complex 
exploratory questions in natural language and 
obtain relevant information units to prepare 
their briefing reports. The system uses novel 
data-driven semantics to conduct a 
clarification dialogue with the user that 
explores the scope and the context of the 
desired answer space. The system has 
undergone extensive hands-on evaluations by 
a group of intelligence analysts. This 
evaluation validated the overall approach in 
HITIQA but also exposed limitations of the 
current prototype.  
1 Introduction 
Our objective in HITIQA is to allow the user to 
submit exploratory, analytical questions, such as 
?What has been Russia?s reaction to the U.S. 
bombing of Kosovo?? The distinguishing property 
of such questions is that one cannot generally 
anticipate what might constitute the answer. While 
certain types of things may be expected (e.g., 
diplomatic statements), the answer is heavily 
conditioned by what information is in fact 
available on the topic. From a practical viewpoint, 
analytical questions are often underspecified, thus 
casting a broad net on a space of possible answers. 
Questions posed by professional analysts are 
aimed to probe the available data along certain 
dimensions. The results of these probes determine 
follow up questions, if necessary. Furthermore, at 
any stage clarifications may be needed to adjust 
the scope and intent of each question. Figure 1 
shows a fragment of an analytical session with 
HITIQA; note that these questions are not aimed at 
factoids, despite their simple form. 
User: What is the history of the nuclear arms 
program linking Iraq and other countries in the 
region? 
HITIQA: [responses and clarifications] 
User: Who financed the nuclear arms program 
in Iraq? 
HITIQA:? 
User: Has Iraq been able to import uranium? 
HITIQA:? 
User: What type of debt does exist between Iraq 
and her trading partners in the region? 
FIGURE 1: A fragment of an analyst?s session 
with HITIQA 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in 
the state of the art of automated question 
answering.  In this paper we focus on three aspects 
of our work: 
1. Question Semantics: how the system 
?understands? user requests 
2. Human-Computer Dialogue: how the user and 
the system negotiate this understanding 
3. User Evaluations and Results 
2 Factoid vs. Analytical QA 
There are significant differences between 
factoid, or fact-finding, and analytical question 
answering.  A factoid question is normally 
understood to seek a piece of information that 
would make a corresponding statement true (i.e., it 
becomes a fact): ?How many states are in the 
U.S.?? / ?There are X states in the U.S.? In this 
sense, a factoid question usually has just one 
correct answer that can generally be judged for its 
truthfulness with respect to some information 
source.  
As noted by Harabagiu et al (1999), factoid 
questions display a distinctive ?answer type?, 
which is the type of the information item needed 
for the answer, e.g., ?person? or ?country?, etc. 
Most existing factoid QA systems deduct this 
expected answer type from the form of the 
question using a finite list of possible answer 
types. For example, ?Who was the first man in 
space? expects a ?person? as the answer type. This 
is generally a very good strategy that has been 
exploited successfully in a number of automated 
QA systems, especially in the context of TREC 
QA1 evaluations. Given the excellent results posted 
by the best systems and an adequate performance 
attained even by some entry-level system, we 
believe that the process of factoid question 
answering is now fairly well understood 
(Harabagiu et al, 2002; Hovy et al, 2000; Prager 
at al., 2001, Wu et al, 2003). 
   In contrast to a factoid question, an analytical 
question has a virtually unlimited variety of 
syntactic forms with only a loose connection 
between their syntax and the expected answer. 
Given the many possible forms of analytical 
questions, it would be counter-productive to 
restrict them to a predefined number of 
question/answer types. Therefore, the formation of 
an answer in analytical QA should instead be 
guided by the user?s intended interest expressed in 
the question, as well as through any follow up 
dialogue with the system. This clearly involves 
user's intentions (the speech acts) and how they 
evolve with respect to the overall information 
strategy they are pursuing. 
In this paper we argue that the semantics 
(though not necessarily the intent) of an analytical 
question is more likely to be deduced from the 
information that is considered relevant to the 
question than through a detailed analysis of its 
particular form. We noted that the questions 
analysts ask, while clearly part of a strategy, are 
generally quite flexible and ?forgiving?, in the 
sense that there is always a strong possibility that 
the answer may not arrive in the expected form, 
and thus a change of strategy, and even the initial 
expectations, may be warranted. This suggests 
strongly that a solution to analytic QA must 
involve a dialogue that combines information 
seeking and problem solving strategies. 
3 Document Retrieval 
HITIQA works with unstructured text data, 
which means that a document retrieval step is 
required to detect any information that may be 
relevant to the user question. It has to be noted that 
determining ?relevant? information is not the same 
as finding an answer; indeed we can use relatively 
simple information retrieval methods (keyword 
matching, etc.) to obtain perhaps 200 ?relevant? 
                                                     
1 TREC QA is the annual Question Answering evaluation 
sponsored by the U.S. National Institute of Standards and Technology 
www.trec.nist.gov 
documents from a database. This gives us an initial 
information space to work on in order to determine 
the scope and complexity of the answer, but we are 
nowhere near the answer yet. The current version 
of HITIQA uses the INQUERY system (Callan et 
al., 1992), although we have also used SMART 
(Buckley, 1985) and other IR systems (such as 
Google).   
4 Text Framing 
In HITIQA we use a text framing technique to 
delineate the gap between the possible meaning of 
the user?s question and the system ?understanding? 
of this question. We can approximate the meaning 
of the question by extracting references to known 
concepts in it, including named entities. The 
information retrieved from the database may well 
lead to other interpretations of the question, and we 
need to determine which of these are ?correct?.  
The framing process imposes a partial structure 
on the text passages that allows the system to 
systematically compare different passages against 
each other and against the question. Framing is not 
attempting to capture the entire meaning of the 
passage; it needs to be just sufficient enough to 
communicate with the user about the differences in 
their question and the returned text. In particular, 
the framing process may uncover topics or aspects 
within the answer space which the user has not 
explicitly asked for, and thus may be unaware of 
their existence. If these topics or aspects align 
closely with the user?s question, (i.e., matching 
many of the salient attributes) we may want to 
make the user aware of them and let him/her 
decide if they should be included in the answer.   
Frames are built from the retrieved data, after 
clustering it into several topical groups. Passages 
are clustered using a combination of hierarchical 
clustering and n-bin classification (Hardy et al, 
2002a). Each cluster represents a topic theme 
within the retrieved set: usually an alternative or 
complimentary interpretation of the user?s 
question. Since clusters are built out of small text 
passages, we initially associate a frame with each 
passage that serves as a seed of a cluster. We 
subsequently merge passages and their associated 
frames to arrive at one or more combined frames 
for the cluster. 
HITIQA starts text framing by building a 
general frame on the seed passages of the clusters 
and any of the top N (currently N=10) scored 
passages that are not already in a cluster. The 
general frame represents an event or a relation 
involving any number of entities, which make up 
the frame?s attributes, such as LOCATION, PERSON, 
ORGANIZATION, DATE, etc. Attributes are extracted 
from text passages by BBN?s Identifinder, which 
tags 24 types of named entities. The event/relation 
itself could be pretty much anything, e.g., accident, 
pollution, trade, etc. and it is captured into the 
TOPIC attribute from the central verb or noun 
phrase of the passage. In the general frame, 
attributes have no assigned roles; they are loosely 
grouped around the TOPIC (Figure 2).  
We have also defined three slightly more 
specialized typed frames by assigning roles to 
selected attributes in the general frame. These 
three ?specialized? frames are: (1) a Transfer 
frame with three roles including FROM, TO and 
OBJECT; (2) a two-role Relation frame with AGENT 
and OBJECT roles; and (3) an one-role Property 
frame. These typed frames represent certain 
generic events/relationships, which then map into 
more specific event types in each domain. Other 
frame types may be defined if needed, but we do 
not anticipate there will be more than a handful all 
together.2 For example, another 3-role frame may 
be State-Change frame with AGENT, OBJECT and 
INSTRUMENT roles, etc.3  
FRAME TYPE: General 
TOPIC: imported 
LOCATION: Iraq, France, Israel 
ORGANIZATION: IAEA [missed: Nukem] 
PERSON: Leonard Spector 
WEAPON: uranium, nuclear bomb 
DATES: 1981, 30 November 1990, .. 
FIGURE 2: A general frame obtained from the 
text passage in Figure 3 (not all attributes shown). 
 
Where the general frame is little more than just 
a ?bag of attributes?, the typed frames capture 
some internal structure of an event, but only to the 
extent required to enable an efficient dialogue with 
the user. Typed frames are ?triggered? by 
appearance of specific words in text, for example 
the word export may trigger a Transfer frame. A 
single text passage may invoke one or more typed 
frames, or none at all. When no typed frame is 
invoked, the general frame is used as default. If a 
typed frame is invoked, HITIQA will attempt to 
identify the roles, e.g. FROM, TO, OBJECT, etc. This 
is done by mapping general frame attributes 
selected from text onto the typed attributes in the 
frames. In any given domain, e.g., weapon non-
proliferation, both the trigger words and the role 
identification rules can be specialized from a 
                                                     
2 Scalability is certainly an outstanding issue here, and we are 
working on effective frame acquisition methods, which is outside of 
the scope of this paper. While classifications such as (Levin, 1993) or 
FrameNet (Fillmore, 2001) are relevant, we are currently aiming at a 
less detailed system. 
3 A more detailed discussion of possible frame types is beyond the 
scope of the current paper. 
training corpus of typical documents and 
questions. For example, the role-id rules rely both 
on syntactic cues and the expected entity types, 
which are domain adaptable.  
Domain adaptation is desirable for obtaining 
more focused dialogue, but it is not necessary for 
HITIQA to work. We used both setups under 
different conditions: the generic frames were used 
with TREC document collection to measure impact 
of IR precision on QA accuracy (Small et al, 
2004). The domain-adapted frames were used for 
sessions with intelligence analysts working with 
the WMD Domain (see below). Currently, the 
adaptation process includes manual tuning 
followed by corpus bootstrapping using an 
unsupervised learning method (Strzalkowski & 
Wang, 1996). We generally rely on BBN?s 
Identifinder for extraction of basic entities, and use 
bootstrapping to define additional entity types as 
well as to assign roles to attributes. 
The version of HITIQA reported here and used 
by analysts during the evaluation has been adapted 
to the Weapons of Mass Destruction Non-
Proliferation domain (WMD domain, henceforth).  
Figure 3 contains an example passage from this 
data set. In the WMD domain, the typed frames 
were mapped onto WMDTransfer 3-role frame, 
and two 2-role frames WMDTreaty  and 
WMDDevelop. Adapting the frames to the WMD 
domain required very minimal modification, such 
as adding the WEAPON entity to augment the 
Identifinder entity set, generating a list of 
international weapon control treaties, etc. 
The Bush Administration claimed that Iraq was 
within one year of producing a nuclear bomb. On 
30 November 1990... Leonard Spector said that 
Iraq possesses 200 tons of natural uranium 
imported and smuggled from several countries. 
Iraq possesses a few working centrifuges and the 
blueprints to build them. Iraq imported centrifuge 
materials from Nukem of the FRG and from other 
sources. One decade ago, Iraq imported 27 pounds 
of weapons-grade uranium from France, for Osirak 
nuclear research center. In 1981, Israel destroyed 
the Osirak nuclear reactor. In November 1990, the 
IAEA inspected Iraq and found all material 
accounted for....  
FIGURE 3: A text passage from the WMD 
domain data    
 
HITIQA frames define top-down constraints on 
how to interpret a given text passage, which is 
quite different from MUC4 template filling task 
                                                     
4 MUC, the Message Understanding Conference, funded by 
DARPA, involved the evaluation of information extraction systems 
applied to a common task. 
(Humphreys et al, 1998). What we?re trying to do 
here is to ?fit? a frame over a text passage. This 
also means that multiple frames can be associated 
with a text passage, or to be exact, with a cluster of 
passages. Since most of the passages that undergo 
the framing process are part of some cluster of 
very similar passages, the added redundancy helps 
to reinforce the most salient features for extraction. 
This makes the framing process potentially less 
error-prone than MUC-style template filling. 
A very similar framing process is applied to the 
user?s question, resulting in one or more Goal 
frames, which are subsequently compared to the 
data frames obtained from retrieved text passages. 
A Goal frame can be a general frame or any of the 
typed frames. Goal frames generated from the 
question, ?Has Iraq been able to import 
uranium?? are shown in Figures 4 and 5. 
FRAME TYPE: General 
TOPIC: import 
WEAPON:  uranium 
LOCATION: Iraq 
FIGURE 4: A general goal frame from the Iraq 
question 
The frame in Figure 4 is simply a General 
frame which is invoked first. HITIQA then 
discovers that TOPIC=import denotes a Transfer-
event in the WMD domain, so it creates a 
WMDTransfer frame that replaces the general 
frame. This new frame, shown in Figure 5, has 
three role attributes TRF_TO, TRF_FROM and 
TRF_OBJECT, plus the relation type (TRF_TYPE). 
Each role attribute is defined over an underlying 
general frame attribute (given in parentheses), 
which are used to compare frames of different 
types.  The role-id rules rely both on syntactic cues 
and the expected entity types, which are domain 
adaptable. 
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): import 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION, ORGANIZATION): ? 
TRF_OBJECT (WEAPON): uranium 
FIGURE 5: A typed goal frame from the Iraq 
question 
HITIQA automatically judges a particular data 
frame as relevant, and subsequently the 
corresponding segment of text as relevant, by 
comparison to the Goal frame. The data frames are 
scored based on the number of conflicts found with 
the Goal frame. The conflicts are mismatches on 
values of corresponding attributes, specifically 
when the data frame attribute list does not contain 
any of the entities in the corresponding Goal 
Frame attribute list.  If a data frame is found to 
have no conflicts, it is given the highest relevance 
rank, and a conflict score of zero.   
All other data frames are scored with an 
increasing value based on the number of conflicts, 
score 1 for frames with one conflict with the Goal 
frame, score 2 for two conflicts etc. Frames that 
conflict with all information found in the query are 
given the score 99 indicating the lowest rank. 
Currently, frames with a conflict score 99 are 
excluded from further processing as outliers. The 
frame in Figure 6 is scored as relevant to the user?s 
query and included in the answer space. 
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): imported 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION): France 
TRF_OBJECT (WEAPON): uranium 
CONFLICT SCORE: 0 
FIGURE 6: A typed frame obtained from the 
text passage in Figure 3, in response to the Iraq 
question 
5 Enabling Dialogue with the User 
Framed information allows HITIQA to 
automatically judge text passages as fully or 
partially relevant and to conduct a meaningful 
dialogue with the user about their content. The 
purpose of the dialogue is to help the user navigate 
the answer space and to negotiate more precisely 
what information he or she is seeking. The main 
principle here is that the dialogue is primarily 
content oriented. Thus, it is okay to ask the user 
whether information about the AIDS conference in 
Cape Town should be included in the answer to a 
question about combating AIDS in Africa. 
However, the user should never be asked if a 
particular keyword is useful or not, or if a 
document is relevant or not.  
Our approach to dialogue in HITIQA is 
modeled to some degree upon the mixed-initiative 
dialogue management adopted in the AMITIES 
project (Hardy et al, 2002b). The main advantage 
of the AMITIES model is its reliance on data-
driven semantics which allows for spontaneous 
and mixed initiative dialogue to occur. By contrast, 
the major approaches to implementation of 
dialogue systems to date rely on systems of 
functional transitions that make the resulting 
system much less flexible. In the grammar-based 
approach, which is prevalent in commercial 
systems, such as in various telephony products, as 
well as in practically oriented research prototypes 
(e.g., DARPA Communicator; Seneff and Polifoni, 
2000; Ferguson and Allen, 1998), a complete 
dialogue transition graph is designed to guide the 
conversation and predict user responses, which is 
suitable for closed domains only. In the statistical 
variation of this approach, a transition graph is 
derived from a large body of annotated 
conversations (e.g., Walker, 2000; Litman and Pan, 
2002). This latter approach is facilitated through a 
dialogue annotation process, e.g., using Dialogue 
Act Markup in Several Layers (DAMSL) (Allen 
and Core, 1997), which is a system of functional 
dialogue acts.  
Nonetheless, an efficient, spontaneous dialogue 
cannot be designed on a purely functional layer. 
Therefore, here we are primarily interested in the 
semantic layer, that is, the information exchange 
and information building effects of a conversation. 
In order to properly understand a dialogue, both 
semantic and functional layers need to be 
considered. In this paper we are concentrating 
exclusively on the semantic layer. 
6 Clarification Dialogue 
The clarification dialogue is when the user and 
the system negotiate the information task that 
needs to be performed. Data frames with a conflict 
score of 0 form the initial kernel answer space and 
HITIQA proceeds by generating an answer from 
this space. Depending upon the presence of other 
frames outside of this set, the system may initiate a 
dialogue with the user. When the Goal frame is a 
general frame HITIQA first initiates a clarification 
dialogue on existing general data frames that have 
one conflict. All of these 1-conflict general frames 
are first grouped on their common conflict 
attribute. HITIQA begins asking the user questions 
on these near-miss frame groups, with the largest 
group first. The groups must be at least groups of 
size N, where N is a user controlled setting.  This 
setting restricts of all HITIQA?s generated 
dialogue. HITIQA then check for the existence of 
any data frames that are one of the three typed 
frames. Clarification dialogue will be initiated on 
these, when all of their general attributes agree 
with the general attributes of the Goal frame 
respectively. Alternatively, if the Goal frame is one 
of the three type specific frames, a clarification 
dialogue is first initiated on groups of one conflict 
data frames that are the same type as the Goal 
frame. The clarification dialogue will then 
continue to the remaining two type specific frames 
if any exist, and finally on to any General data 
frames. 
A 1-conflict frame has only a single attribute 
mismatch with the Goal frame. This could be a 
mismatch on any of the general frame attributes, 
for example, LOCATION, ORGANIZATION, TIME, 
etc., or in one of the role-assigned attributes, TO, 
FROM, OBJECT, etc.  A special case arises when the 
conflict occurs on the TOPIC attribute, which 
indicated the event type. Since all other attributes 
match, we may be looking at potentially different 
events (though similar type events) involving the 
same entities, occurring at the same location or 
time. The purpose of the clarification dialogue in 
this case is to probe which of these additional 
events may be of interest to the user.  
Another special case arises when the Goal 
frame has a different value of the TYPE attribute 
than a data frame. The purpose of the clarification 
dialogue here is to see if the user wishes to expand 
the answer space to include events of a different 
type. This situation is illustrated in the exchange 
shown in Figure 7.  
The clarification question asked by HITIQA in 
Figure 7 was obtained by comparing the Goal 
frame (Figure 5) to a partly matching frame 
(Figure 8) generated from another text passage. 
We note first that the Goal frame for this example 
is WMDTransfer type, while the data frame in 
Figure 8 is WMDDevelop type. Nonetheless, both 
frames match on their general-frame attributes 
WEAPON and LOCATION. Therefore, HITIQA asks 
the user if it should expand the answer space to 
include development of uranium in Iraq as well. 
User: ?Has Iraq been able to import uranium?? 
HITIQA: IRAQ REPORTED TO HAVE 
IMPORTED MISSILES AND URANIUM  
Supporting passage:  
...that Iraq possesses 200 tons of natural 
uranium imported and smuggled from several 
countries. Iraq possesses a few working 
centrifuges and the blueprints to build them. Iraq 
imported centrifuge materials from Nukem of the 
FRG and from other sources. One decade ago, 
Iraq imported 27 pounds of weapons-grade 
uranium from France, for Osirak nuclear research 
center... 
HITIQA: ?Are you also interested in 
background information on the uranium 
development program in Iraq?? 
User: ? 
 
FIGURE 7:  The clarification dialogue detail 
During the dialogue, as new information is 
obtained from the user, the Goal frame is updated 
and the scores of all the data frames are 
reevaluated.  If the user responds the equivalent of 
?yes? to the system clarification question in the 
dialogue in Figure 7, a corresponding 
WMDDevelop frame will be added to the set of 
active Goal frames and all WMDDevelop frames 
obtained from text passages will be re-scored for 
possible inclusion in the answer. 
FRAME TYPE: WMDDevelop    
DEV_TYPE (TOPIC): development, produced 
DEV_OBJ (WEAPON): nuc. weapons, uranium 
DEV_AGENT (LOCATION): Iraq, Tuwaitha 
CONFLICT SCORE: 2 
Conflicts with FRAME_TYPE and TOPIC  
FIGURE 8: A 2-conflict frame against the 
Iraq/uranium question that generated the dialogue 
in Figure 7. 
The user may end the dialogue at any point using 
the generated answer given the current state of the 
frames. Currently, the answer is simply composed 
of text passages from the zero conflict frames. In 
addition, HITIQA will generate a ?headline? for 
the text passages in the answer space.  This is done 
using a combination of text templates and simple 
grammar rules applied to the attributes of the 
passage frame. Figure 7 shows a portion of the 
answer generated by HITIQA for the Iraq query. 
7 HITIQA Preliminary Evaluations 
We have evaluated HITIQA in a series of 
workshops with professional analysts in order to 
obtain an in-depth and comprehensive assessment 
of the system usability and performance. In 
addition to evaluating our research progress, the 
purpose of these workshops was to test several 
evaluation instruments to see if they can be 
meaningfully applied to a complex information 
system such as HITIQA. 
     For the participating analysts, the primary 
activity at these workshops involved preparation of 
reports in response to ?scenarios? ? complex 
questions that often encompass multiple sub-
questions, aspects and hypotheses. For example, in 
one scenario, analysts were asked ti locate 
information about the al Qaeda terorist group: its 
membership, sources of funding and activities. In 
another scenario, the analysts were requested to 
find information on the chemical weapon Sarin. 
Figure 9 shows one of the analytical scenarios used 
in these workshops. We prepared a database of 
over 1GByte of text documents; it included articles 
from the Center for Non-proliferation (CNS) data 
collected for the AQUAINT program and similar 
data retrieved from the web using Google. The 
analysts? task was to prepare a report ?as much like 
what you would do in your normal work 
environment as possible.? Over the six days of the 
workshops, each analyst prepared five such reports 
in sessions of one to three hours. Each session 
involved multiple questions posed to the system, as 
well as clarification dialogue, visual browsing and 
report construction. Figure 10 shows an abridged 
transcript from another analytical session with 
HITIQA.  
 Figure 9: A scenario level analytic task  
One of our primary concerns was to design 
tasks that were similar in scope and difficulty to 
those that the analysts are used to performing at 
work and to ensure that they felt comfortable using 
the system. 5 questions in the scenario evaluation 
dealt with this issue; for example, one question 
asked how the scenarios compared in difficulty 
with the tasks the analysts normally perform at 
work. The mean score for these five questions was 
3.75 on a 5 point scale (five is the best score). The 
lowest score (M=2.88) was received on the 
question ?How did the scenario compare in 
difficulty to tasks that you normally perform at 
work??; this slightly above average rating of 
difficulty of the tasks was quite satisfactory for our 
purposes.  
    In the final evaluation, analysts were asked to 
rate their agreement with statements such as 
?Having HITIQA helps me find important 
information? (score 4.50), ?Having Hitiqa at work 
would help me find information faster than I can 
currently find it? (score 4.33), and ?Hitiqa would 
be a useful addition to the tools that I already have 
at work? (score 4.25). The mean normalized score 
for the combined final evaluation of Workshop I 
was 3.75 on the 5 point scale; this means that the 
system received many more ratings of 4 and 5 than 
of 1 and 2. Comments made by the analysts in the 
group discussion and in the individual interviews 
confirmed that analysts liked the interactive 
dialogue and were very pleased with the results. 
For example, one analyst said ?I learned more 
about Sarin gas in 30 minutes than I probably 
would have at work in a half a day.? As desired, 
the analysts also made many suggestions for 
improving the interface and the interoperation of 
The department chief has requested a report by the 
close of business today on the nuclear arms program in 
Iraq and how it was influenced by the neighboring 
countries. List the extent of the nuclear program in each 
involved country including funding, capabilities, quantity, 
etc. Your report should also include key figures in Iraq 
nuclear program as well as in other countries in the region, 
and,any travels that these key figures have made to other 
countries in regards to a nuclear program, any weapons 
that have been used in the past by either country, any 
purchases or trades that have been made relevant to 
weapons of mass destruction (possibly oil trade, etc.), any 
ingredients and chemicals that have been used, any 
potential weapons that could be under development, 
countries that are involved or have close ties to Iraq or her 
trade partners, possible locations of development sites, and 
possible companies or organizations that these countries 
work with for their nuclear arms program. Add any other 
information relating to the Iraqi Nuclear Arms Programs.  
the visual and text display. For a research system 
undergoing its first rigorous evaluation, these 
results are very satisfactory ? they support the 
value of the design of the HITIQA system, 
including the interactive mode and the visual 
display and encourage us to move forward with 
this approach. 
 FIGURE 10: Fragment of an analytical session 
8 Future work 
The AQUAINT Program has entered its second 
phase in May 2004. Over the next 2 years our 
focus will be on augmenting HITIQA to provide 
more advanced dialogue capabilities, including 
problem solving dialogue related to hypothesis 
formation and verification. This implies building 
up system?s knowledge acquisition capabilities by 
exploiting diverse data sources, including 
structured databases and the internet. 
9 Acknowledgements 
This paper is based on work supported in part by 
the Advanced Research and Development Activity 
(ARDA)?s Advanced Question Answering for 
Intelligence (AQUAINT) Program. Special thanks 
to Heather McCallum-Bayliss and John Rogers for 
helping to arrange the analyst workshops. 
Additional thanks for Google for extending their 
license for this experiment, to Ralph Weischedel of 
BBN/Verizon for the use of IdentiFinder, to Chuck 
Messenger and Peter LaMonica for assistance in 
development of the analytical scenarios, and to 
Bruce Croft at University of Massachusetts for the 
use of INQUERY system. 
References  
Allen, J. and M. Core. 1997. Draft of DAMSL:  
Dialog Act Markup in Several Layers. 
www.cs.rochester.edu/research/cisd.  
Buckley, C. 1985. Implementation of the Smart 
information retrieval system. TR85-686, 
Computer Science, Cornell University. 
Ferguson, G. and J. Allen. 1998. TRIPS: An 
Intelligent Integrated Problem-Solving Assistant. 
AAAI-98 Conf., pp. 567-573. 
Fillmore, C. & C. F. Baker. 2001. Frame semantics 
for text understanding. WordNet Workshop at 
NAACL. 
Hardy, H., et al 2002a. Cross-Document 
Summarization by Concept Classification. 
Proceedings of SIGIR, Tampere, Finland. 
Hardy, H., et al 2002b.  Multi-layer Dialogue 
Annotation for Automated Multilingual 
Customer Service. ISLE Workshop, UK. 
Harabagiu, S., et. al. 2002. Answering Complex, 
List and Context questions with LCC?s Question 
Answering Server.   TREC-10. 
Hovy, E., et al 2000. Question Answering in 
Webclopedia. Notebook. Proceedings of Text 
Retrieval Conference TREC-9. 
Humphreys, R. et al 1998. Description of the 
LaSIE-II System as Used for MUC-7. Proc. of  
7th Message Under. Conf. (MUC-7.). 
Levin, B. 1993. English Verb Class and 
Alternations: A Preliminary Investigation. 
Chicago: University of Chicago Press. 
Litman, Diane J. and Shimei Pan. 2002. Designing 
and Evaluating an Adaptive Spoken Dialogue 
System. User Modeling and User-Adapted 
Interaction. Vol. 12, No. 2/3, pp. 111-137. 
Prager, J. et al 2003. In Question-Answering Two 
Heads are Better Than One. Proceedings of 
HLT-NAACL 2003, pp 24-31.  
Seneff, S. and J. Polifroni. 2000. Dialogue 
Management in the MERCURY Flight 
Reservation System. ANLP-NAACL 2000. 
Small et al 2004. A Data Driven Approach to 
Interactive Question Answering. In M. Maybury 
(ed). Future Directions in Automated Question 
Answering. MIT Press (to appear). 
Strzalkowski, T and J. Wang. 1996. A self-learning 
Universal Concept Spotter. Proceedings of 
COLING-96, pp. 931-936. 
Walker, M. A. 2002. An Application of 
Reinforcement Learning to Dialogue Strategy 
Selection in a Spoken Dialogue System for 
Email. Journal of AI Research, vol 12., pp. 387-
416. 
Wu, M. et al 2003. Question Answering by 
Pattern Matching, Web-Proofing, Semantic 
Form Proofing. TREC-12.Notebook. 
 
User: What is the status of South Africa's chemical, 
biological, and nuclear programs?  
 Clarification Dialogue: 1 minute 
 Studying Answer Panel: 60 minutes  
Copying 24 passages to report 
 Visual Panel Browsing: 5 minutes 
User: Has South Africa provided CBW material or 
assistance to any other countries?  
 Clarification Dialogue: 1 minute 
 Studying Answer Panel: 26 minutes 
 Copying 6 passages to report 
 Visual Panel browsing: 1 minute 
 Adding 1 passage to report 
User: How was South Africa's CBW program 
financed?  
 Clarification Dialogue: 40 seconds 
 Studying Answer Panel: 11 minutes 
 Copying 3 passages to report 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 763?770,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exact Decoding for Jointly Labeling and Chunking Sequences
Nobuyuki Shimizu
Department of Computer Science
State University of New York at Albany
Albany, NY 12222, USA
nobuyuki@shimizu.name
Andrew Haas
Department of Computer Science
State University of New York at Albany
Albany, NY 12222 USA
haas@cs.albany.edu
Abstract
There are two decoding algorithms essen-
tial to the area of natural language pro-
cessing. One is the Viterbi algorithm
for linear-chain models, such as HMMs
or CRFs. The other is the CKY algo-
rithm for probabilistic context free gram-
mars. However, tasks such as noun phrase
chunking and relation extraction seem to
fall between the two, neither of them be-
ing the best fit. Ideally we would like to
model entities and relations, with two lay-
ers of labels. We present a tractable algo-
rithm for exact inference over two layers
of labels and chunks with time complexity
O(n2), and provide empirical results com-
paring our model with linear-chain mod-
els.
1 Introduction
The Viterbi algorithm and the CKY algorithms are
two decoding algorithms essential to the area of nat-
ural language processing. The former models a lin-
ear chain of labels such as part of speech tags, and
the latter models a parse tree. Both are used to ex-
tract the best prediction from the model (Manning
and Schutze, 1999).
However, some tasks seem to fall between the
two, having more than one layer but flatter than the
trees created by parsers. For example, in relation
extraction, we have entities in one layer and rela-
tions between entities as another layer. Another task
is shallow parsing. We may want to model part-of-
speech tags and noun/verb chunks at the same time,
since performing simultaneous labeling may result
in increased joint accuracy by sharing information
between the two layers of labels.
To apply the Viterbi decoder to such tasks, we
need two models, one for each layer. We must feed
the output of one layer to the next layer. In such an
approach, errors in earlier processing nearly always
accumulate and produce erroneous results at the end.
If we use CKY, we usually end up flattening the out-
put tree to obtain the desired output. This seems like
a round-about way of modeling two layers.
There are previous attempts at modeling two
layer labeling. Dynamic Conditional Random Fields
(DCRFs) by (McCallum et al 2003; Sutton et al
2004) is one such attempt, however, exact inference
is in general intractable for these models and the
authors were forced to settle for approximate infer-
ence.
Our contribution is a novel model for two layer
labeling, for which exact decoding is tractable. Our
experiments show that our use of label-chunk struc-
tures results in significantly better performance over
cascaded CRFs, and that the model is a promising
alternative to DCRFs.
The paper is organaized a follows: In Section 2
and 3, we describe the model and present the de-
coding algorithm. Section 4 describes the learning
methods applicable to our model and the baseline
models. In Section 5 and 6, we describe the experi-
ments and the results.
763
Token POS NP
U.K. JADJ B
base NOUN I
rates NOUN I
are VERB O
at OTHER O
their OTHER B
highest JADJ I
level NOUN I
in OTHER O
eight OTHER B
years NOUN I
. OTHER O
Table 1: Example with POS and NP tags
2 Model for Joint Labeling and Chunking
Consider the task of finding noun chunks. The noun
chunk extends from the beginning of a noun phrase
to the head noun, excluding postmodifiers (which
are difficult to attach correctly). Table 1 shows a
sentence labeled with POS tags and segmented into
noun chunks. B marks the first word of a noun
chunk, I the other words in a noun chunk, and O
the words that are not in a noun chunk. Note that
we collapsed the 45 different POS labels into 5 la-
bels, following (McCallum et al 2003). All differ-
ent types of adjectives are labeled as JADJ.
Each word carries two tags. Given the first layer,
our aim is to present a model that can predict the
second and third layers of tags at the same time.
Assume we have n training samples, {(xi, yi)}ni=1,
where xi is a sequence of input tokens and yi is a
label-chunk structure for xi. In this example, the
first column contains the tokens xi and the second
and third columns together represent the label-chunk
structures yi. We will present an efficient exact de-
coding for this structure.
The label-chunk structure, shown in Table 2, is a
representation of the two layers of tags. The tuples
in Table 2 are called parts. If the token at index r
carries a POS tag P and a chunk tag C , the first layer
includes part ?C,P, r?. This part is called a node.
If the tokens at index r ? 1 and r are in the same
chunk, and C is the label of that chunk, the first layer
also includes part ?C,P0, P, r?1, r? (where P0 and
P are the POS tags of the tokens at r ? 1 and r
Token First Layer (POS) Second Layer (NP)
U.K. ?I, JADJ, 0?
?I, JADJ, NOUN, 0, 1?
base ?I, NOUN, 1?
?I, NOUN, NOUN, 1, 2?
rates ?I, NOUN, 2? ?I, 0, 2?
?I,O, 2, 3?
are ?O, VERB, 3?
?O, VERB, OTHER, 3, 4?
at ?O, OTHER, 4? ?O, 3, 4?
?O, I, 4, 5?
their ?I, OTHER, 5?
?I, OTHER, JADJ, 5, 6?
highest ?I, JADJ, 6?
?I, JADJ, NOUN, 6, 7?
level ?I, NOUN, 7? ?I, 5, 7?
?I,O, 7, 8?
in ?O, OTHER, 8? ?O, 8, 8?
?O, I, 8, 9?
eight ?I, OTHER, 9?
?I, OTHER, NOUN, 9, 10?
years ?I, NOUN, 10? ?I, 9, 10?
?I,O, 10, 11?
. ?O, OTHER, 11? ?O, 11, 11?
Table 2: Example Parts
respectively). This part is called a transition. If a
chunk tagged C extends from the token at q to the
token at r inclusive, the second layer includes part
?C, q, r?. This part is a chunk node. And if the token
at q?1 is the last token in a chunk tagged C0, while
the token at q is the first token of a chunk tagged C ,
the second layer includes part ?C0, C, q?1, q?. This
part is a chunk transition.
In this paper we use the common method of fac-
toring the score of the label-chunk structure as the
sum of the scores of all the parts. Each part in a
label-chunk structure can be lexicalized, and gives
rise to several features. For each feature, we have a
corresponding weight. If we sum up the weights for
these features, we have the score for the part, and if
we sum up the scores of the parts, we have the score
for the label-chunk structure.
Suppose we would like to score a pair (xi, yi) in
the training set, and it happens to be the one shown
in Table 2. To begin, let?s say we would like to find
the features for the part ?I,NOUN, 7? of POS node
type (1st Layer). This is the NOUN tag on the sev-
enth token ?level? in Table 2. By default, the POS
node type generates the following binary feature.
? Is there a token labeled with ?NOUN? in a
chunk labeled with ?I??
764
Now, to have more features, we can lexicalize POS
node type. Suppose we use xr to lexicalize POS
node ?C,P, r?, then we have the following binary
feature, as it is ?I,NOUN, 7? and xi7 = ?level?.
? Is there a token ?level? labeled with ?NOUN?
in a chunk labeled with ?I??
We can also use xr?1 and xr to lexicalize the parts
of POS node type.
? Is there a token ?level? labeled with ?NOUN?
in a chunk labeled with ?I? that?s preceded by
?highest??
This way, we have a complete specification of the
feature set given the part type, lexicalization for each
part type and the training set. Let us define f a
boolean feature vector function such that each di-
mension of f(xi, yi) contains 1 if the pair (xi, yi)
has the feature, 0 otherwise. Now define a real-
valued weight vector w with the same dimension
as f . To represent the score of the pair (xi, yi), we
write s(xi, yi) = w?f(xi, yi) We could also have
w?f(xi, {p}) where p just a single part, in which
case we just write s(p).
Assuming an appropriate feature representation
as well as a weight vector w, we would like to
find the highest scoring label-chunk structure y =
argmaxy?(w?f(x, y?)) given an input sentence x.
In the upcoming section, we present a decoding
algorithm for the label-chunk structures, and later
we give a method for learning the weight vector used
in the decoding.
3 Decoding
The decoding algorithm is shown in Figure 1. The
idea is to use two tables for dynamic programming:
label table and chunk table.
Suppose we are examining the current position
r, and would like to consider extending the chunk
[q, r? 1] to [q, r]. We need to know the chunk tag C
for [q, r? 1] and the last POS tag P0 at index r? 1.
The array entry label table[q][r ? 1] keeps track of
this information.
Then we examine how the current chunk is con-
nected with the previous chunk. The array entry
chunk table[q][C0] keeps track of the score of the
best label-chunk structure from 0 up to the index q
that has the ending chunk tag C0. Now checking
the chunk transition from C0 to C at the index q is
simple, and we can record the score of this chunk to
chunk table[r][C], so that the next chunk starting at
r can use this information.
In short, we are executing two Viterbi algorithms
on the first and second layer at the same time. One
extends [q, r ? 1] to [q, r], considering the node in-
dexed by r (first layer). The other extends [0, q] to
[0, r], considering the node indexed by [q, r] (sec-
ond layer). The dynamic programming table for the
first layer is kept in the label table (r ? 1 and P0
are used in the Viterbi algorithm for this layer) and
that for the second layer in the chunk table (q and
C0 used). The algorithm returns the best score of
the label-chunk structure.
To recover the structure, we simply need to main-
tain back pointers to the items that gave rise to the
each item in the dynamic programming table. This
is just like maintaining back pointers in the Viterbi
algorithm for sequences, or the CKY algorithm for
parsing.
The pseudo-code shows that the run-time com-
plexity of the decoding algorithm is O(n2) unlike
that of CFG parsing, O(n3). Thus the algorithm per-
forms better on long sentences. On the other hand,
the constant is c2p2 where c is the number of chunk
tags and p is the number of POS tags.
4 Learning
4.1 Voted Perceptron
In the CKY and Viterbi decoders, we use the
forward-backward or inside-outside algorithm to
find the marginal probabilities. Since we don?t yet
have the inference algorithm to find the marginal
probabilities of the parts of a label-chunk structure,
we use an online learning algorithm to train the
model. Despite this restriction, the voted percep-
tron is known for its performance (Sha and Pereira,
2003).
The voted perceptron we use is the adaptation of
(Freund and Schapire, 1999) to the structured set-
ting. Algorithm 4.1 shows the pseudo code for the
training, and the function update(wk, xi, yi, y?) re-
turns wk ? f(xi, y?) + f(xi, yi) .
Given a training set {(xiyi)}ni=1 and the epoch
number T, Algorithm 4.1 will return a list of
765
Algorithm 3.1: DECODE(the scoring function s(p))
score := 0;
for q := index start to index end
for length := 1 to index end ? q
r := q + length;
for each Chunk Tag C
for each Chunk Tag C0
for each POS Tag P
for each POS Tag P0
score := 0;
if (length > 1)
#Add the score of the transition from r-2 to r-1. (1st Layer, POS)
score := score + s(?C,P0, P, r ? 2, r ? 1?) + label table[q][r ? 1][C][P0];
#Add the score of the node at r-1. (1st Layer, POS)
score := score + s(?C,P, r ? 1?);
if (score >= label table[q][r][C][P ])
label table[q][r][C][P ] := score;
#Add the score of the chunk node at [q,r-1]. (2nd Layer, NP)
score := score + s(?C, q, r ? 1?);
if (index start < q)
#Add the score of the chunk transition from q-1 to q. (2nd Layer, NP)
score := score + s(?C0, C, q ? 1, q?) + chunk table[q][C0];
if (score >= chunk table[r][C])
chunk table[r][C] := score;
end for
end for
end for
end for
end for
end for
score := 0;
for each C in chunk tags
if (chunk table[index end][C] >= score)
score := chunk table[index end][C];
last symbol := C;
end for
return (score)
Note: Since the scoring function s(p) is defined as w?f(xi, {p}), the input sequence xi and the weight
vector w are also the inputs to the algorithm.
Figure 1: Decoding Algorithm
766
weighted perceptrons {(w1, c1), ..(wk, ck)}. The fi-
nal model V uses the weight vector
w =
?k
j=1(cjwj)
Tn
(Collins, 2002).
Algorithm 4.1: TRAIN(T, {(xi, yi)}ni=1)
k := 0;
w1 := 0;
c1 := 0;
for t := 1 to T
for i := 1 to n
y? := argmaxy(w?k f(y, xi))
if (y? = yi)
ck := ck + 1;
else
wk+1 := update(wk, xi, yi, y?);
ck+1 := 1;
k := k + 1;
ck := ck + 1;
end for
end for
return ({(w1, c1), ..(wk, ck)})
Algorithm 4.2: UPDATE1(wk, xi, yi, y?)
return (wk ? f(xi, y?) + f(xi, yi))
Algorithm 4.3: UPDATE2(wk, xi, yi, y?)
? = max(0,min( li(y
?)?s(xi,yi)+s(xi,y?)
?fi(yi)?fi(y?)?2
, 1));
return (wk ? ?f(xi, y?) + ?f(xi, yi))
4.2 Max Margin
4.2.1 Sequential Minimum Optimization
A max margin method minimizes the regularized
empirical risk function with the hard (penalized)
margin
min
w
1
2?w?
2?
?
i
(s(xi, yi)?max
y
(s(xi, y)?li(y)))
li finds the loss for y with respect to yi, and it is as-
sumed that the function is decomposable just as y is
decomposable to the parts. This equation is equiva-
lent to
minw 12?w?2 + C
?
i ?i
?i, y, s(xi, yi) + ?i ? s(xi, y) ? li(y)
After taking the Lagrange dual formation, we have
max
??0
?12?
?
i,y
?i(y)(f(xi, yi)? f(xi, y))?2 +
?
i,y
?i(y)li(y)
such that
?
y
?i(y) = C
and
w =
?
i,y
?i(y)(f(xi, yi) ? f(xi, y)) (1)
This quadratic program can be optimized by bi-
coordinate descent, known as Sequential Minimum
Optimization. Given an example i and two label-
chunk structures y? and y??,
d = li(y
?) ? li(y??) ? (s(xi, y??) ? s(xi, y?))
?fi(y??) ? fi(y?)?2
(2)
? = max(??i(y?),min(d, ?i(y??))
The updated values are : ?i(y?) := ?i(y?) + ? and
?i(y??) := ?i(y??) ? ?.
Using the equation (1), any increase in ? can be
translated to w. For a naive SMO, this update is
executed for each training sample i, for all pairs of
possible parses y? and y?? for xi. See (Taskar and
Klein, 2005; Zhang, 2001; Jaakkola et al 2000).
Here is where we differ from (Taskar et al 2004).
We choose y?? to be the correct parse yi, and y?
to be the best runner-up. After setting the ini-
tial weights using yi, we also set ?i(yi) = 1 and
?i(y?) = 0. Although these alphas are not correct,
as optimization nears the end, the margin is wider;
?i(yi) and ?i(y?) gets closer to 1 and 0 respec-
tively. Given this approximation, we can compute ?.
Then, the function update(wk, xi, yi, y?) will return
wk??f(xi, y?)+?f(xi, yi) and we have reduced the
SMO to the perceptron weight update.
4.2.2 Margin Infused Relaxed Algorithm
We can think of maximizing the margin in terms
of extending the Margin Infused Relaxed Algorithm
(MIRA) (Crammer and Singer, 2003; Crammer et
al, 2003) to learning with structured outputs. (Mc-
Donald et al 2005) presents this approach for de-
pendency parsing.
In particuler, Single-best MIRA (McDonald et
al, 2005) uses only the single margin constraint for
the runner up y? with the highest score. The result-
ing online update would be wk+1 with the following
767
condition: min?wk+1 ? wk? such that s(xi, yi) ?
s(xi, y?) ? li(y?) where y? = argmaxys(xi, y).
Incidentally, the equation (2) for d above when
?i(yi) = 1 and ?i(y?) = 0 solves this minimization
problem as well, and the weight update is the same
as the SMO case.
4.2.3 Conditional Random Fields
Instead of minimizing the regularized empirical
risk function with the hard (penalized) margin, con-
ditional random fields try to minimize the same with
the negative log loss:
min
w
1
2?w?
2 ?
?
i
(s(xi, yi) ? log(
?
y
s(xi, y)))
Usually, CRFs use marginal probabilities of parts to
do the optimization. Since we have not yet come
up with the algorithm to compute marginals for a
label-chunk structure, the training methods for CRFs
is not applicable to our purpose. However, on se-
quence labeling tasks CRFs have shown very good
performance (Lafferty et al 2001; Sha and Pereira,
2003), and we will use them for the baseline com-
parison.
5 Experiments
5.1 Task: Base Noun Phrase Chunking
The data for the training and evaluation comes from
the CoNLL 2000 shared task (Tjong Kim Sang and
Buchholz, 2000), which is a portion of the Wall
Street Journal.
We consider each sentence to be a training in-
stance xi, with single words as tokens.
The shared task data have a standard training set
of 8936 sentences and a test set of 2012 sentences.
For the training, we used the first 447 sentences from
the standard training set, and our evaluation was
done on the standard test set of the 2012 sentences.
Let us define the set D to be the first 447 samples
from the standard training set .
There are 45 different POS labels, and the three
NP labels: begin-phrase, inside-phrase, and other.
(Ramshaw and Marcus, 1995) To reduce the infer-
ence time, following (McCallum et al 2003), we
collapsed the 45 different POS labels contained in
the original data. The rules for collapsing the POS
labels are listed in the Table 3.
Original Collapsed
all different types of nouns NOUN
all different types of verbs VERB
all different types of adjectives JADJ
all different types of adverbs RBP
the remaining POS labels OTHER
Table 3: Rules for collapsing POS tags
Token POS Collapsed Chunk NP
U.K. JJ JADJ B-NP B
base NN NOUN I-NP I
rates NNS NOUN I-NP I
are VBP VERB B-VP O
at IN OTHER B-PP O
their PRP$ OTHER B-NP B
highest JJS JADJ I-NP I
level NN NOUN I-NP I
in IN OTHER B-PP O
eight CD OTHER B-NP B
years NNS NOUN I-NP I
. . OTHER O O
Table 4: Example with POS and NP labels, before
and after collapsing the labels.
We present two experiments: one comparing
our label-chunk model with a cascaded linear-chain
model and a simple linear-chain model, and one
comparing different learning algorithms.
The cascaded linear-chain model uses one linear-
chain model to predict POS tags, and another linear-
chain model to predict NP labels, using the POS tags
predicted by the first model as a feature.
More specifically, we trained a POS-tagger using
the training set D. We then used the learned model
and replaced the POS labels of the test set with the
labels predicted by the learned model. The linear-
chain NP chunker was again trained on D and eval-
uated on this new test set with POS supplied by the
earlier processing. Note that the new test set has ex-
actly the same word tokens and noun chunks as the
original test set.
5.2 Systems
5.2.1 POS Tagger and NP Chunker
There are three versions of POS taggers and NP
chunkers: CRF, VP, MMVP. For CRF, L-BFGS,
a quasi-Newton optimization method was used for
the training, and the implementation we used is
CRF++ (Kudo, 2005). VP uses voted perceptron,
and MMVP uses max margin update for the voted
perceptron. For the voted perceptron, we used aver-
768
if xq matches then tq is
[A-Z][a-z]+ CAPITAL
[A-Z] CAP ONE
[A-Z]+ CAP ALL
[A-Z]+[a-z]+[A-Z]+[a-z] CAP MIX
.*[0-9].* NUMBER
Table 5: Rules to create tq for each token xq
First Layer (POS)
Node ?C,P, r? Trans. ?C,P0, P, r ? 1, r?
xr?1 xr?1
xr xr
xr+1
tr
Second Layer (NP)
Node ?C, q, r? Trans. ?C0, C, q ? 1, q?
xq xq?1
xq?1 xq
xr
xr+1
Table 6: Lexicalized Features for Joint Models
aging of the weights suggested by (Collins, 2002).
The features are exactly the same for all three sys-
tems.
5.2.2 Cascaded Models
For each CRF, VP, MMVP, the output of a POS
tagger was used as a feature for the NP chunker.
The feeds always consist of a POS tagger and NP
chunker of the same kind, thus we have CRF+CRF,
VP+VP, and MMVP+MMVP.
5.2.3 Joint Models
Since CRF requires the computation of marginals
for each part, we were not able to use the learning
method. VP and MMVP were used to train the label-
chunk structures with the features explained in the
following section.
5.3 Features
First, as a preprocessing step, for each word token
xq, feature tq was created with the rule in Table 5,
and included in the input files. This feature is in-
cluded in x along with the word tokens. The feature
tells us whether the token is capitalized, and whether
digits occur in the token. No outside resources such
as a list of names or a gazetteer were used.
Table 6 shows the lexicalized features for the joint
labeling and chunking. For the first iteration of train-
ing, the weights for the lexicalized features were not
POS tagging POS NP F1
CRF 91.56% N/A N/A
VP 90.55% N/A N/A
MMVP 90.02% N/A N/A
NP chunking POS NP F1
CRF given 94.44% 87.52%
VP given 94.28% 86.96%
MMVP given 94.17% 86.79%
Both POS & NP POS NP F1
CRF + CRF above 90.16% 79.08%
VP + VP above 89.21% 76.26%
MMVP + MMVP above 88.95% 75.28%
VP Joint 88.42% 90.60% 79.69%
MMVP Joint 88.69% 90.84% 80.34%
Table 7: Performance
updated. The intention is to have more weights on
the unlexicalized features, so that when lexical fea-
ture is not found, unlexicalized features could pro-
vide useful information and avoid overfitting, much
as back-off probabilities do.
6 Result
We evaluated the performance of the systems using
three measures: POS accuracy, NP accuracy, and F1
measure on NP. These figures show how errors ac-
cumulate as the systems are chained together. For
the statistical significance testing, we have used pair-
samples t test, and for the joint labeling and chunk-
ing task, everything was found to be statistically sig-
nificant except for CRF + CRF vs VP Joint.
One can see that the systems with joint label-
ing and chunking models perform much better than
the cascaded models. Surprisingly, the perceptron
update motivated by the max margin principle per-
formed significantly worse than the simple percep-
tron update for linear-chain models but performed
better on joint labeling and chunking.
Although joint labeling and chunking model takes
longer time per sample because of the time complex-
ity of decoding, the number of iteration needed to
achieve the best result is very low compared to other
systems. The CPU time required to run 10 iterations
of MMVP is 112 minutes.
7 Conclusion
We have presented the decoding algorithm for label-
chunk structure and showed its effectiveness in find-
ing two layers of information, POS tags and NP
chunks. This algorithm has a place between the
769
POS tagging Iterations
VP 30
MMVP 40
CRF 126
NP chunking Iterations
VP 70
MMVP 50
CRF 101
Both POS & NP Iterations
VP 10
MMVP 10
Table 8: Iterations needed for the result
Viterbi algorithm for linear-chain models and the
CKY algorithm for parsing, and the time complex-
ity is O(n2). The use of our label-chunk structure
significantly boosted the performance over cascaded
CRFs despite the online learning algorithms used to
train the system, and shows itself as a promising al-
ternative to cascaded models, and possibly dynamic
conditional random fields for modeling two layers of
tags. Further work includes applying the algorithm
to relation extraction, and devising an effective algo-
rithm to find the marginal probabilities of parts.
References
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. of Empirical Methods
in Natural Language Processing (EMNLP)
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2003. Online passive aggressive algorithms. In Ad-
vances in Neural Information Processing Systems 15
K. Crammer, R. McDonald, and F. Pereira. 2004. New
large margin algorithms for structured prediction. In
Learning with Structured Outputs Workshop (NIPS)
Y. Freund and R. Schapire 1999. Large Margin Classi-
fication using the Perceptron Algorithm. In Machine
Learning, 37(3):277-296.
T.S. Jaakkola, M. Diekhans, and D. Haussler. 2000. A
discriminative framework for detecting remote protein
homologies. Journal of Computational Biology
T. Kudo 2005. CRF++: Yet Another CRF toolkit. Avail-
able at http://chasen.org/?taku/software/CRF++/
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of the
18th International Conference on Machine Learning
(ICML)
F. Peng and A. McCallum. 2004. Accurate Informa-
tion Extraction from Research Papers using Condi-
tional Random Fields. In Proc. of the Human Lan-
guage Technology Conf. (HLT)
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of the Human Language
Technology Conf. (HLT)
C. Manning and H. Schutze. 1999. Foundations of Sta-
tistical Natural Language Processing MIT Press.
A. McCallum, K. Rohanimanesh and C. Sutton. 2003.
Dynamic Conditional Random Fields for Jointly La-
beling Multiple Sequences. In Proc. of Workshop on
Syntax, Semantics, Statistics. (NIPS)
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of the 43rd Annual Meeting of the ACL
L. Ramshaw and M. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proc. of Third
Workshop on Very Large Corpora. ACL
C. Sutton, K. Rohanimanesh and A. McCallum. 2004.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting Se-
quence Data. In Proc. of the 21st International Con-
ference on Machine Learning (ICML)
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning 2004. Max Margin Parsing. In Proc. of
Empirical Methods in Natural Language Processing
(EMNLP)
B. Taskar and D. Klein. 2005. Max-Margin Methods for
NLP: Estimation, Structure, and Applications Avail-
able at http://www.cs.berkeley.edu/?taskar/pubs/max-
margin-acl05-tutorial.pdf
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In
Proc. of the 4th Conf. on Computational Natural Lan-
guage Learning (CoNLL)
T. Zhang. 2001. Regularized winnow methods. In Ad-
vances in Neural Information Processing Systems 13
770
Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 31?36,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Discourse Segmentation and Labeling for Route Instructions
Nobuyuki Shimizu
Department of Computer Science
State University of New York at Albany
Albany, NY 12222, USA
nobuyuki@shimizu.name
Abstract
In order to build a simulated robot that
accepts instructions in unconstrained nat-
ural language, a corpus of 427 route in-
structions was collected from human sub-
jects in the office navigation domain. The
instructions were segmented by the steps
in the actual route and labeled with the
action taken in each step. This flat
formulation reduced the problem to an
IE/Segmentation task, to which we applied
Conditional Random Fields. We com-
pared the performance of CRFs with a set
of hand-written rules. The result showed
that CRFs perform better with a 73.7%
success rate.
1 Introduction
To have seamless interactions with computers, ad-
vances in task-oriented deep semantic understand-
ing are of utmost importance. The examples in-
clude tutoring, dialogue systems and the one de-
scribed in this paper, a natural language interface
to mobile robots. Compared to more typical text
processing tasks on newspapers for which we at-
tempt shallow understandings and broad coverage,
for these domains vocabulary is limited and very
strong domain knowledge is available. Despite
this, deeper understanding of unrestricted natural
language instructions poses a real challenge, due
to the incredibly rich structures and creative ex-
pressions that people use. For example,
?Just head straight through the hallway
ignoring the rooms to the left and right
of you, but while going straight your go-
ing to eventually see a room facing you,
which is north, enter it.?
?Head straight. continue straight past
the first three doors until you hit a cor-
ner. On that corner there are two doors,
one straight ahead of you and one on the
right. Turn right and enter the room to
the right and stop within.?
These utterances are taken from an office navi-
gation corpus collected from undergrad volunteers
at SUNY/Albany. There is a good deal of variety.
Previous efforts in this domain include the clas-
sic SHRDLU program by Winograd (1972), us-
ing a simulated robot, and the more ambitious IBL
(Instruction-based Learning for Mobile Robots)
project (Lauria et al 2001) which tried to inte-
grate vision, voice recognition, natural language
understanding and robotics. This group has yet to
publish performance statistics. In this paper we
will focus on the application of machine learning
to the understanding of written route instructions,
and on testing by following the instructions in a
simulated office environment.
2 Task
2.1 Input and Output
Three inputs are required for the task:
? Directions for reaching an office, written in
unrestricted English.
? A description of the building we are traveling
through.
? The agent?s initial position and orientation.
The output is the location of the office the direc-
tions aim to reach.
31
2.2 Corpus Collection
In an experiment to collect the corpus, (Haas,
1995) created a simulated office building modeled
after the actual computer science department at
SUNY/Albany. This environment was set up like
a popular first person shooter game such as Doom,
and the subject saw a demonstration of the route
he/she was asked to describe. The subject wrote
directions and sent them to the experimenter, who
sat at another computer in the next room. The
experimenter tried to follow the directions; if he
reaches the right destination, the subject got $1.
This process took place 10 times for each subject;
instructions that the experimenter could not fol-
low correctly were not added to the corpus. In this
manner, they were able to elicit 427 route instruc-
tions from the subject pool of 44 undergraduate
students.
2.3 Abstract Map
To simplify the learning task, the map of our
computer science department was abstracted to a
graph. Imagine a track running down the halls of
the virtual building, with branches into the office
doors. The nodes of the graph are the intersec-
tions, the edges are the pieces of track between
them. We assume this map can either be prepared
ahead of time, or dynamically created as a result of
solving Simultaneous Localization and Mapping
(SLAM) problem in robotics (Montemerlo et al
2003).
2.4 System Components
Since it is difficult to jump ahead and learn the
whole input-output association as described in the
task section, we will break down the system into
two components.
Front End:
RouteInstruction? ActionList
Back End:
ActionList?Map? Start? Goal
The front-end is an information extraction sys-
tem, where the system extracts how one should
move from a route instruction. The back-end is a
reasoning system which takes a sequence of moves
and finds the destination in the map. We will first
describe the front-end, and then show how to inte-
grate the back-end to it.
One possibility is to keep the semantic repre-
sentation close to the surface structure, including
under-specification and ambiguity, and leaving the
back-end to resolve the ambiguity. We will pursue
a different route. The disambiguation will be done
in the front-end; the representation that it passes
to the back-end will be unambiguous, describing
at most one path through the building. The task
of the back-end is simply to check the sequence
of moves the front-end produced against the map
and see if there is a path leading to a point in the
map or not. The reason for this is two fold. One is
to have a minimal annotation scheme for the cor-
pus, and the other is to enable the learning of the
whole task including the disambiguation as an IE
problem.
3 Semantic Analysis
Note that in this paper, given an instruction, one
step in the instruction corresponds to one action
shown to the subject, one episode of action detec-
tion and tracking, and one segment of the text.
In order to annotate unambiguously, we need to
detect and track both landmarks and actions. A
landmark is a hallway or a door, and an action
is a sequence of a few moves one will make with
respect to a specific landmark.
The moves one can make in this map are:
(M1). Advancing to x,
(M2). Turning left/right to face x, and
(M3). Entering x.
Here, x is a landmark. Note that all three moves
have to do with the same landmark, and two or
three moves on the same landmark constitute one
action. An action is ambiguous until x is filled
with an unambiguous landmark. The following is
a made-up example in which each move in an ac-
tion is mentioned explicitly.
a. ?Go down the hallway to the second
door on the right. Turn right. Enter the
door.?
But you could break it down even further.
b. ?Go down the hallway. You will see
two doors on the right. Turn right and
enter the second.?
One can add any amount of extra information to an
instruction and make it longer, which people seem
to do. However, we see the following as well.
c. ?Enter the second door on the right.?
In one sentence, this sample contains the advance,
the turn and the entering. In the corpus, the norm
32
is to assume the move (M1) when an expression
indicating the move (M2) is present. Similarly, an
expression of move (M3) often implicitly assumes
the move (M1) and (M2). However, in some cases
they are explicitly stated, and when this happens,
the action that involves the same landmark must
be tracked across the sentences.
Since all three samples result in the same action,
for the back-end it is best not to differentiate the
three. In order to do this, actions must be tracked
just like landmarks in the corpus.
The following two samples illustrate the need to
track actions.
d. ?Go down the hallway until you see
two doors. Turn right and enter the sec-
ond door on the right.?
In this case, there is only one action in the instruc-
tion, and ?turn right? belongs to the action ?ad-
vance to the second door on the right, and then
turn right to face it, and then enter it.?
e. ?Proceed to the first hallway on the
right. Turn right and enter the second
door on the right.?
There are two actions in this instruction. The first
is ?advance to the first hallway on the right, and
then turn right to face the hallway.? The phrase
?turn right? belongs to this first action. The second
action is the same as the one in the example (d).
Unless we can differentiate between the two, the
execution of the unnecessary turn results in failure
when following the instructions in the case (d).
This illustrates the need to track actions across
a few sentences. In the last example, it is impor-
tant to realize that ?turn right? has something to do
with a door, so that it means ?turn right to face a
door?. Furthermore, since ?enter the second door
on the right? contains ?turning right to face a door?
in its semantics as well, they can be thought of as
the same action. Thus, the critical feature required
in the annotation scheme is to track actions and
landmarks.
The simplest annotation scheme that can show
how actions are tracked across the sentences is
to segment the instruction into different episodes
of action detection and tracking. Note that each
episode corresponds to exactly one action shown
to the subject during the experiment. The annota-
tion is based on the semantics, not on the the men-
tions of moves or landmarks. Since each segment
Token Node Part Transition Part
make ?B-GHL1, 0? ?B-GHL1, I-GHL1, 0, 1?
left ?I-GHL1, 1? ?I-GHL1, I-GHL1, 1, 2?
, ?I-GHL1, 2? ?I-GHL1, B-EDR1, 2, 3?
first ?B-EDR1, 3? ?B-EDR1, I-EDR1, 3, 4?
door ?I-EDR1, 4? ?I-EDR1, I-EDR1, 4, 5?
on ?I-EDR1, 5? ?I-EDR1, I-EDR1, 5, 6?
the ?I-EDR1, 6? ?I-EDR1, I-EDR1, 6, 7?
right ?I-EDR1, 7?
Table 1: Example Parts: linear-chain CRFs
involves exactly one landmark, we can label the
segment with an action and a specific landmark.
For example,
GHR1 := ?advance to the first hallway on the
right, then turn right to face it.?
EDR2 := ?advance to the second door on the
right, then turn right to face it, then enter it.?
GHLZ := ?advance to the hallway on the left at
the end of the hallway, then turn left to face it.?
EDSZ := ?advance to the door straight ahead of
you, then enter it.?
Note that GH=go-hall, ED=enter-door,
R1=first-right, LZ=left-at-end, SZ=ahead-of-you.
The total number of possible actions is 15.
This way, we can reduce the front-end task into
a sequence of tagging tasks, much like the noun
phrase chunking in the CoNLL-2000 shared task
(Tjong Kim Sang and Buchholz, 2000). Given
a sequence of input tokens that forms a route in-
struction, a sequence of output labels, with each
label matching an input token was prepared. We
annotated with the BIO tagging scheme used in
syntactic chunkers (Ramshaw and Marcus, 1995).
make B-GHL1
left I-GHL1
, I-GHL1
first B-EDR1
door I-EDR1
on I-EDR1
the I-EDR1
right I-EDR1
4 Systems
4.1 System 1: CRFs
4.1.1 Model: A Linear-Chain Undirected
Graphical Model
From the output labels, we create the parts in a
linear-chain undirected graph (Table 1). Our use
of term part is based on (Bartlett et al 2004).
For each pair (xi, yi) in the training set, xi is
the token (in the first column, Table 1), and yi
33
Transition Node
?L0, L, j ? 1, j? ?L, j?
no lexicalization no lexicalization
xj?4
xj?3
xj?2
xj?1
xj
xj+1
xj+2
xj+3
xj?1, xj
xj+0, xj+1
Table 2: Features
is the part (in the second and third column, Ta-
ble 1). There are two kinds of parts: node and
transition. A node part tells us the position and
the label, ?B-GHL1, 0?, ?I-GHL1, 1?, and so on. A
transition part encodes a transition. For example,
between tokens 0 and 1 there is a transition from
tag B-GHL1 to I-GHL1. The part that describes
this transition is: ?B-GHL1, I-GHL1, 0, 1?.
We factor the score of this linear node-transition
structure as the sum of the scores of all the parts in
y, where the score of a part is again the sum of the
feature weights for that part.
To score a pair (xi, yi) in the training set, we
take each part in yi and check the features associ-
ated with it via lexicalization. For example, a part
?I-GHL1, 1? could give rise to binary features such
as,
? Does (xi, yi) contain a label ?I-GHL1?? (No
Lexicalization)
? Does (xi, yi) contain a token ?left? labeled
with ?I-GHL1?? (Lexicalized by x1)
? Does (xi, yi) contain a token ?left? labeled
with ?I-GHL1? that?s preceded by ?make??
(Lexicalized by x0, x1)
and so on. The features used in this experiment are
listed in Table 2.
If a feature is present, the feature weight is
added. The sum of the weights of all the parts
is the score of the pair (xi, yi). To represent
this summation, we write s(xi, yi) = w?f(xi, yi)
where f represents the feature vector and w is the
weight vector. We could also have w?f(xi, {p})
where p is a single part, in which case we just write
s(p).
Assuming an appropriate feature representation
as well as a weight vector w, we would like to find
the highest scoring y = argmaxy?(w?k f(y?, x))
given an input sequence x. We next present a ver-
sion of this decoding algorithm that returns the
best y consistent with the map.
4.1.2 Decoding: the Viterbi Algorithm and
Inferring the Path in the Map
The action labels are unambiguous; given the
current position, the map, and the action label,
there is only one position one can go to. This back-
end computation can be integrated into the Viterbi
algorithm. The function ?go? takes a pair of (ac-
tion label, start position) and returns the end posi-
tion or null if the action cannot be executed at the
start position according to the map. The algorithm
chooses the best among the label sequences with a
legal path in the map, as required by the condition
(cost > bestc ? end 6= null). Once the model
is trained, we can then use the modified version of
the Viterbi algorithm (Algorithm 4.1) to find the
destination in the map.
Algorithm 4.1: DECODE PATH(x, n, start, go)
for each label y1
node[0][y1].cost? s(?y1, 0?)
node[0][y1].end? start;
for j ? 1 to n? 1
for each label yj+1
bestc? ??;
end? null;
for each label yj
cost? node[j][yj ].cost
+s(?yj, yj+1, j, j + 1?)
+s(?yj+1, j + 1?);
end? node[j][yj ].end;
if (yj 6= yj+1)
end? go(yj+1, end);
if (cost > bestc ? end 6= null)
bestc? cost;
if (bestc 6= ??)
node[j + 1][yj+1].cost? bestc;
node[j + 1][yj+1].end? end;
bestc? ??;
end? null;
for each label yn
if (node[j][yn].cost > bestc)
bestc? node[j][yn].cost;
end? node[j][yn].end;
return (bestc, end)
34
4.1.3 Learning: Conditional Random Fields
Given the above problem formulation, we
trained the linear-chain undirected graphical
model as Conditional Random Fields (Lafferty et
al, 2001; Sha and Pereira, 2003), one of the best
performing chunkers. We assume the probability
of seeing y given x is
P (y|x) = exp(s(x, y))?
y? exp(s(x, y?))
where y? is all possible labeling for x , Now, given
a training set T = {(xiyi)}mi=1, We can learn
the weights by maximizing the log-likelihood,
?
i logP (yi|xi). A detailed description of CRFs
can be found in (Lafferty et al 2001; Sha and
Pereira, 2003; Malouf, 2002; Peng and McCallum,
2004). We used an implementation called CRF++
which can be found in (Kudo, 2005)
4.2 System 2: Baseline
Suppose we have clean data and there is no need to
track an action across sentences or phrases. Then,
the properties of an action are mentioned exactly
once for each episode.
For example, in ?go straight and make the first
left you can, then go into the first door on the right
side and stop? , LEFT and FIRST occur exactly
once for the first action, and FIRST, DOOR and
RIGHT are found exactly once in the next action.
In a case like that, the following baseline algo-
rithm should work well.
? Find all the mentions of LEFT/RIGHT,
? For each occurrence of LEFT/RIGHT, look
for an ordinal number, LAST, or END (= end
of the hallway) nearby,
? Also, for each LEFT/RIGHT, look for a men-
tion of DOOR. If DOOR is mentioned, the
action is about entering a door.
? If DOOR is not mentioned around
LEFT/RIGHT, then the action is about
going to a hallway by default,
? If DOOR is mentioned at the end of an in-
struction without LEFT/RIGHT, then the ac-
tion is to go straight into the room.
? Put the sequence of action labels together ac-
cording to the mentions collected.
count average length
GHL1 128 8.5
GHL2 4 7.7
GHLZ 36 14.4
GHR1 175 10.8
GHR2 5 15.8
GHRZ 42 13.6
EDL1 98 10.5
EDL2 81 12.3
EDL3 24 13.9
EDLZ 28 13.7
EDR1 69 10.4
EDR2 55 12.9
EDR3 6 13.0
EDRZ 11 16.4
EDSZ 55 16.2
Table 3: Steps found in the dataset
In this case, all that?s required is a dictionary of
how a word maps to a concept such as DOOR. In
this corpus, ?door?, ?office?, ?room?, ?doorway?
and their plural forms map to DOOR, and the or-
dinal number 1 will be represented by ?first? and
?1st?, and so on.
5 Dataset
As noted, we have 427 route instructions, and the
average number of steps was 1.86 steps per in-
struction. We had 189 cases in which a sentence
boundary was found in the middle of a step. Ta-
ble 3 shows how often action steps occurred in the
corpus and average length of the segments.
One thing we noticed is that somehow people do
not use a short phrase to say the equivalent of ?en-
ter the door straight ahead of you?, as seen by the
average length of EDSZ. Also, it is more common
to say the equivalent of ?take a right at the end of
the hallway? than that of ?go to the second hallway
on the right?, as seen by the count of GHR2 and
GHRZ. The distribution is highly skewed; there
are a lot more GHL1 than GHL2.
6 Results
We evaluated the performance of the systems us-
ing three measures: overlap match, exact match,
and instruction follow through, using 6-fold cross-
valiadation on 427 samples. Only the action
chunks were considered for exact match and over-
lap match. Overlap match is a lenient measure
that considers a segmentation or labeling to be cor-
35
Exact Match Recall Precision F-1
CRFs 66.0% 67.0% 66.5%
Overlap Match Recall Precision F-1
Baseline 62.8% 49.9% 55.6%
CRFs 85.7% 87.0% 86.3%
Instruction Follow Through success rate
Baseline 39.5%
CRFs 73.7%
Table 4: Recall, Precision, F-1 and Success Rate
rect if it overlaps with any of the annotated labels.
Instruction follow through is the success rate for
reaching the destination, and the most important
measure of the performance in this domain. Since
the baseline algorithm does not identify the token
labeled with B-prefix, no exact match comparison
is made. The result (Table 4) shows that CRFs per-
form better with a 73.7% success rate.
7 Future Work
More complex models capable of representing
landmarks and actions separately may be applica-
ble to this domain, and it remains to be seen if such
models will perform better. Also, some form of
co-reference resolution or more sophisticated ac-
tion tracking should also be considered.
Acknowledgement
We thank Dr. Andrew Haas for introducing us to
the problem, collecting the corpus and being very
supportive in general.
References
P. Bartlett, M. Collins, B. Taskar and D. McAllester.
2004. Exponentiated gradient algorithms for large-
margin structured classification. In Advances in
Neural Information Processing Systems (NIPS)
A. Haas 1995. Testing a Simulated Robot that Follows
Directions. unpublished
T. Kudo 2005. CRF++: Yet An-
other CRF toolkit. Available at
http://chasen.org/?taku/software/CRF++/
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of International Conference on Machine Learn-
ing .
R. Malouf. 2002. A Comparison of Algorithms for
Maximum Entropy Parameter Estimation. In Pro-
ceedings of Conference of Computational Natural
Language Learning
F. Peng and A. McCallum. 2004. Accurate Informa-
tion Extraction from Research Papers using Condi-
tional Random Fields. In Proceedings of Human
Language Technology Conference .
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of Human
Language Technology Conference .
S. Lauria, G. Bugmann, T. Kyriacou, J. Bos, and E.
Klein. 2001. Personal Robot Training via Natural-
Language Instructions. IEEE Intelligent Systems,
16:3, pp. 38-45.
C. Manning and H. Schutze. 1999. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
M. Montemerlo, S. Thrun, D. Koller, and B. Wegbreit.
2003. FastSLAM 2.0: An improved particle fil-
tering algorithm for simultaneous localization and
mapping that provably converges. In Proceedings of
the International Joint Conference on Artificial In-
telligence (IJCAI).
L. Ramshaw and M. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proceedings
of Third Workshop on Very Large Corpora. ACL
E. F. Tjong Kim Sang and S. Buchholz. 2000. In-
troduction to the CoNLL-2000 shared task: Chunk-
ing. In Proceedings of Conference of Computational
Natural Language Learning .
T. Winograd. 1972. Understanding Natural Language.
Academic Press.
36
HITIQA:  An Interactive Question Answering System  
A Preliminary Report 
 
Sharon Small, Ting Liu, Nobuyuki Shimizu, and Tomek Strzalkowski  
 
ILS Institute 
The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tl7612,ns3203,tomek}@albany.edu 
 
 
Abstract
HITIQA is an interactive question answering 
technology designed to allow intelligence analysts 
and other users of information systems to pose 
questions in natural language and obtain relevant 
answers, or the assistance they require in order to 
perform their tasks. Our objective in HITIQA is to 
allow the user to submit exploratory, analytical, 
non-factual questions, such as ?What has been 
Russia?s reaction to U.S. bombing of Kosovo?? 
The distinguishing property of such questions is 
that one cannot generally anticipate what might 
constitute the answer. While certain types of things 
may be expected (e.g., diplomatic statements), the 
answer is heavily conditioned by what information 
is in fact available on the topic. From a practical 
viewpoint, analytical questions are often under-
specified, thus casting a broad net on a space of 
possible answers. Therefore, clarification dialogue 
is often needed to negotiate with the user the exact 
scope and intent of the question. 
 
1   Introduction 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in 
the state of the art of automated question answer-
ing.  In this paper we focus on two aspects of our 
work: 
1. Question Semantics: how the system ?un-
derstands? user requests. 
2. Human-Computer Dialogue: how the user 
and the system negotiate this understand-
ing. 
     We will also discuss very preliminary evalua-
tion results from a series of pilot tests of the system 
conducted by intelligence analysts via a remote 
internet link.  
   
2   Factual vs. Analytical 
The objective in HITIQA is to allow the user to 
submit and obtain answers to exploratory, analyti-
cal, non-factual questions.  There are very signifi-
cant differences between factual, or fact-finding, 
and analytical question answering. A factual ques-
tion seeks pieces of information that would make a 
corresponding statement true (i.e., they become 
facts): ?How many states are in the U.S.?? / ?There 
are X states in the U.S.? In this sense, a factual 
question usually has just one correct answer that 
can generally, be judged for its truthfulness. By 
contrast, an analytical question is when the ?truth? 
of the answer is more a matter of opinion and may 
depend upon the context in which the question is 
asked. Answers to analytical questions are rarely 
unilateral, indeed, a mere ?correct? answer may 
have limited value, and in some cases may not 
even be determinate (?Which college is the best??, 
?How do I stop my baby?s crying??). Instead, an-
swers to analytical questions are often judged as 
helpful, or useful, or satisfactory, etc. ?Technically 
correct? answers (e.g., ?feed the baby milk?) may 
be considered as irrelevant or at best unresponsive.   
     The distinction between factual and analytical 
questions depends primarily on the intention of the 
person who is asking, however, the form of a ques-
tion is often indicative of which of the two classes 
it is more likely to belong to.  Factual questions 
can be classified into a number of syntactic formats 
(?question typology?) that aids in automatic proc-
essing. 
     Factual questions display a fairly distinctive 
?answer type?, which is the type of the information 
piece needed to fulfill the statement.  Recent auto-
mated systems for answering factual questions  
deduct  this expected answer type from the form of 
the question and a finite list of possible answer 
 types. For example, ?Who was the first man in 
space? expects a ?person? as the answer, while 
?How long was the Titanic?? expects some length 
measure as an answer, probably in yards and feet, 
or meters.  This is generally a very good strategy, 
that has been exploited successfully in a number of 
automated QA systems that appeared in recent 
years, especially in the context of TREC QA1 
evaluations (Harabagiu et al, 2000; Hovy et al, 
2000; Prager at al., 2001).     
     This process is not easily applied to analytical 
questions. This is because the type of an answer for 
analytical questions cannot always be anticipated 
due to their inherently exploratory character.  In 
contrast to a factual question, an analytical ques-
tion has an unlimited variety of syntactic forms 
with only a loose connection between their syntax 
and the expected answer.  Given the unlimited po-
tential of the formation of analytical questions, it 
would be counter-productive to restrict them to a 
limited number of question/answer types. Even 
finding a non-strictly factual answer to an other-
wise simple question about Titanic length (e.g., 
?two football fields?) would push the limits of the 
answer-typing approach. Therefore, the formation 
of an answer should instead be guided by the top-
ics the user is interested in, as recognized in the 
query and/or through the interactive dialogue, 
rather than by a single type as inferred from the 
query in a factual system.   
     This paper argues that the semantics of an ana-
lytical question is more likely to be deduced from 
the information that is considered relevant to the 
question than through a detailed analysis of their 
particular form. While this may sound circular, it 
needs not be. Determining ?relevant? information 
is not the same as finding an answer; indeed we 
can use relatively simple information retrieval 
methods (keyword matching, etc.) to obtain per-
haps 50 or 100 ?relevant? documents from a data-
base. This gives us an initial answer space to work 
on in order to determine the scope and complexity 
of the answer. In our project, we use structured 
templates, which we call frames to map out the 
content of pre-retrieved documents, and subse-
quently to delineate the possible meaning of the 
question (Section 6). 
                                                 
1 TREC QA is the annual Question Answering evalua-
tion sponsored by the U.S. National Institute of Stan-
dards and Technology www.trec.nist.gov. 
 
3   Document Retrieval 
When the user poses a question to a system sitting 
atop a huge database of unstructured data (text 
files), the first order of business is to reduce that 
pile to perhaps a handful of documents where the 
answer is likely to be found. This means, most of-
ten, document retrieval, using fast but non-exact 
selection methods.  Questions are tokenized and 
sent to a document retrieval engine, such as Smart 
(Buckley, 1985) or InQuery (Callan et al, 1992).  
Noun phrases and verb phrases are extracted from 
the question to give us a list of potential topics that 
the user may be interested in.   
    In the experiments with the HITIQA prototype, 
see Figure 1, we are retrieving the top fifty docu-
ments from three gigabytes of newswire 
(AQUAINT corpus plus web-harvested docu-
ments).  
 
Document 
Retrieval
Document 
Retrieval
Build
Frames
Build
Frames
Process
Frames
Process
Frames
Dialogue
Manager
Dialogue
anager
Segment/
Filter
Segment/
Filter
Cluster
Paragraphs
Cluster
Paragraphs
Answer
Generator
Answer
Generator
answer
Tokenized 
question
top 50 
documents
distinct 
paragraphs
clusters
framed text 
segments
candidate 
answer topics
relevant text 
segments
system 
clarification 
question/
user response
DB
Gate
Wordnet
Figure 1: HITIQA preliminary architecture 
 
4   Data Driven Semantics of Questions 
The set of documents and text passages returned 
from the initial search is not just a random subset 
of the database. Depending upon the quality (recall 
and precision) of the text retrieval system avail-
 able, this set can be considered as a first stab at 
understanding the user?s question by the machine.  
Again, given the available resources, this is the 
best the system can do under the circumstances. 
Therefore, we may as well consider this collection 
of retrieved texts (the Retrieved Set) as the mean-
ing of the question as understood by the system. 
This is a fair assessment: the better our search ca-
pabilities, the closer this set would be to what the 
user may accept as an answer to the question.  
     We can do better, however. We can perform 
automatic analysis of the retrieved set, attempting 
to uncover if it is a fairly homogenous bunch (i.e., 
all texts have very similar content), or whether 
there are a number of diverse topics represented 
there, somehow tied together by a common thread. 
In the former case, we may be reasonably confi-
dent that we have the answer, modulo the retriev-
able information. In the latter case, we know that 
the question is more complex than the user may 
have intended, and a negotiation process is needed. 
     We can do better still. We can measure how 
well each of the topical groups within the retrieved 
set is ?matching up? against the question. This is 
accomplished through a framing process described 
later in this paper. The outcome of the framing 
process is twofold: firstly, the alternative interpre-
tations of the question are ranked within 3 broad 
categories: on-target, near-misses and outliers. 
Secondly, salient concepts and attributes for each 
topical group are extracted into topic frames. This 
enables the system to conduct a meaningful dia-
logue with the user, a dialogue which is wholly 
content oriented, and thus entirely data driven.  
ON-TARGET
OUTLIERS
NEAR-MISSES
 
Figure 2: Answer Space Topology.  The goal of interac-
tive QA it to optimize the ON-TARGET middle zone. 
 
5   Clustering 
We use n-gram-based clustering of text passages 
and concept extraction  to uncover the main topics, 
themes and entities in this set.  
     Retrieved documents are first broken into natu-
rally occurring paragraphs.  Duplicate paragraphs 
are filtered out and the remaining passages are 
clustered using a combination of hierarchical clus-
tering and n-bin classification (details of the clus-
tering algorithm can be found in Hardy et al, 
2002a).  Typically three to six clusters are gener-
ated out of the top 50 documents, which may yield 
as many as 1000 passages.  Each cluster represents 
a topic theme within the retrieved set: usually an 
alternative or complimentary interpretation of the 
user?s question. 
    A list of topic labels is assigned to each cluster. 
A topic label may come from one of two places:  
First, the texts in the cluster are compared against 
the list of key phrases extracted from the user?s 
query.  For each match found, the matching phrase 
is used as a topic label for the cluster. If a match 
with the key phrases from the question cannot be 
obtained, Wordnet is consulted to see if a common 
ancestor can be found. For example, ?rifle? and 
?machine gun? are kinds of ?weaponry? in Word-
net, which allows an indirect match between a 
question about weapon inspectors and a text re-
porting a discovery by the authorities of a cache of 
?rifles? and ?machine guns?.  
 
6   Framing 
In HITIQA we use a text framing technique to de-
lineate the gap between the meaning of the user?s 
question and the system ?understanding? of this 
question. The framing is an attempt to impose a 
partial structure on the text that would allow the 
system to systematically compare different text 
pieces against each other and against the question, 
and also to communicate with the user about this. 
In particular, the framing process may uncover 
topics and themes within the retrieved set which 
the user has not explicitly asked for, and thus may 
be unaware of their existence. Nonetheless these 
may carry important information ? the NEAR-
MISSES in Figure 2. 
     In the current version of the system, frames are 
fairly generic templates, consisting of a small 
number of attributes, such as LOCATION, PERSON, 
COUNTRY, ORGANIZATION, etc.  Future versions of 
HITIQA will add domain specialized frames, for 
example, we are currently constructing frames for 
the Weapons Non-proliferation Domain. Most of 
the frame attributes are defined in advance, how-
 ever, dynamic frame expansion is also possible. 
Each of the attributes in a frame is equipped with 
an extractor function which specializes in locating 
and extracting instances of this attribute in the run-
ning text. The extractors are implemented using 
information extraction utilities which form the ker-
nel of Sheffield?s GATE2 system.  We have modi-
fied GATE to separate organizations into compa-
nies and other organizations, and we have also ex-
panded by adding new concepts such as industries.  
Therefore, the framing process resembles strongly 
the template filling task in information extraction 
(cf. MUC3 evaluations), with one significant ex-
ception: while the MUC task was to fill in a tem-
plate using potentially any amount of source text 
(Humphreys et al, 1998), the framing is essentially 
an inverse process. In framing, potentially multiple 
frames can be associated with a small chunk of text 
(a passage or a short paragraph). Furthermore, this 
chunk of text is part of a cluster of very similar text 
chunks that further reinforce some of the most sali-
ent features of these texts. This makes the frame 
filling a significantly less error-prone task ? our 
experience has been far more positive than the 
MUC evaluation results may indicate. This is be-
cause, rather than trying to find the most appropri-
ate values for attributes from among many poten-
tial candidates, we in essence fit the frames over 
small passages4.  
Therefore, data frames are built from the re-
trieved data, after clustering it into several topical 
groups. Since clusters are built out of small text 
passages, we associate a frame with each passage 
that serves as a seed of a cluster. We subsequently 
merge passages, and their associated frames when-
ever anaphoric and other cohesive links are de-
tected.   
     A very similar process is applied to the user?s 
question, resulting in a Goal Frame which can be 
subsequently compared to the data frames obtained 
from retrieved data. For example, the Goal Frame 
generated from the question, ?How has pollution in 
the Black Sea affected the fishing industry, and 
                                                 
2 GATE is Generalized Architecture for Text Engineering, an 
information extraction system developed at the University of 
Sheffield (Cunningham, 2000). 
3 MUC, the Message Understanding Conference, funded by 
ARPA, involved the evaluation of information extraction sys-
tems applied to a common task. 
4 We should note that selecting the right frame type for a pas-
sage is an important pre-condition to ?understanding?. 
what are the sources of this pollution?? is shown 
in Figure 3 below. 
 
TOPIC:[pollution, industry, sources] 
LOCATION: [Black Sea] 
INDUSTRY:[fishing] 
Figure 3: HITIQA generated Goal Frame 
 
            
TOPIC: pollution 
SUB-TOPIC: [sources] 
LOCATION: [Black Sea] 
INDUSTRY :[fisheries, tourism] 
TEXT: [In a period of only three decades (1960's-1980's), 
the Black Sea has suffered the catastrophic degradation 
of a major part of its natural resources. Particularly acute 
problems have arisen as a result of pollution (notably 
from nutrients, fecal material, solid waste and oil), a 
catastrophic decline in commercial fish stocks, a severe 
decrease in tourism and an uncoordinated approach to-
wards coastal zone management. Increased loads of nutri-
ents from rivers and coastal sources caused an overpro-
duction of phytoplankton leading to extensive eutrophica-
tion and often extremely low dissolved oxygen concentra-
tions. The entire ecosystem began to collapse. This prob-
lem, coupled with pollution and irrational exploitation of 
fish stocks, started a sharp decline in fisheries resources.] 
RELEVANCE: Matches on all elements found in goalframe  
Figure 4: A HITIQA generated data frame.  Words in 
bold were used to fill the Frame. 
 
     The data frames are then compared to the Goal 
Frame. We pay particular attention to matching the 
topic attributes, before any other attributes are con-
sidered. If there is an exact match between a Goal 
Frame topic and the text being used to build the 
data frame, then this becomes the data frame?s 
topic as well.  If more than one match is found, the 
subsequent matches become the sub-topics of the 
data frame. On the other hand, if no match is pos-
sible against the Goal Frame topic, we choose the 
topic from the list of the Wordnet generated hy-
pernyms. An example data frame generated from 
the text retrieved in response to the query about the 
Black Sea is shown in Figure 4. After the initial 
framing is done, frames judged to be related to the 
same concept or event, are merged together and 
values of their attributes are combined. 
 
7   Judging Frame Relevance 
We judge a particular data frame as relevant, and 
subsequently the corresponding segment of text as 
relevant, by comparison to the Goal Frame. The 
 data frames are scored based on the number of 
conflicts found between them and the Goal Frame. 
The conflicts are mismatches on values of corre-
sponding attributes. If a data frame is found to 
have no conflicts, it is given the highest relevance 
rank, and a conflict score of zero.  All other data 
frames are scored with an incrementing conflict 
value, one for frames with one conflict with the 
Goal Frame, two for two conflicts etc.  Frames that 
conflict with all information found in the query are 
given a score of 99 indicating the lowest relevancy 
rank.  Currently, frames with a conflict score of 99  
are excluded from further processing. The frame in 
Figure 4 is scored as fully relevant to the question 
(0 conflicts). 
 
8   Enabling Dialogue with the User 
Framed information allows HITIQA to automati-
cally judge some text as relevant and to conduct a 
meaningful dialogue with the user as needed on 
other text. The purpose of the dialogue is to help 
the user to navigate the answer space and to solicit 
from the user more details as to what information 
he or she is seeking. The main principle here is that 
the dialogue is at the information semantic level, 
not at the information organization level. Thus, it is 
okay to ask the user whether information about the 
AIDS conference in Cape Town should be in-
cluded in the answer to a question about combating 
AIDS in Africa. However, the user should never be 
asked if a particular keyword is useful or not, or if 
a document is relevant or not. We have developed 
a 3-pronged strategy: 
1. Narrowing dialogue: ask questions that 
would allow the system to reduce the size 
of the answer set.  
2. Expanding dialogue: ask questions that 
would allow the system to decide if the an-
swer set needs to be expanded by informa-
tion just outside of it (near-misses). 
3. Fact seeking dialogue: allow the user to 
ask questions seeking additional facts and 
specific examples, or similar situations. 
Of the above, we have thus far implemented the 
first two options as part of the preliminary clarifi-
cation dialogue. The clarification dialogue is when 
the user and the system negotiate the task that 
needs to be performed. We can call this a ?triaging 
stage?, as opposed to the actual problem solving 
stage (point 3 above). In practice, these two stages 
are not necessarily separated and may be overlap-
ping throughout the entire interaction. Nonetheless, 
these two have decidedly distinct character and 
require different dialogue strategies on the part of 
the system. 
     Our approach to dialogue in HITIQA is mod-
eled to some degree upon the mixed-initiative dia-
logue management adopted in the AMITIES pro-
ject (Hardy et al, 2002b). The main advantage of 
the AMITIES model is its reliance on data-driven 
semantics which allows for spontaneous and mixed 
initiative dialogue to occur.  
     By contrast, the major approaches to implemen-
tation of dialogue systems to date rely on systems 
of functional transitions that make the resulting 
system much less flexible. In the grammar-based 
approach, which is prevalent in commercial sys-
tems, such as in various telephony products, as 
well as in practically oriented research prototypes5, 
(e.g., DARPA, 2002; Seneff and Polifoni, 2000; 
Ferguson and Allen, 1998) a complete dialogue 
transition graph is designed to guide the conversa-
tion and predict user responses, which is suitable 
for closed domains only. In the statistical variation 
of this approach, a transition graph is derived from 
a large body of annotated conversations (e.g., 
Walker, 2000; Litman and Pan, 2002). This latter 
approach is facilitated through a dialogue annota-
tion process, e.g., using Dialogue Act Markup in 
Several Layers (DAMSL) (Allen and Core, 1997), 
which is a system of functional dialogue acts.  
     Nonetheless, an efficient, spontaneous dialogue 
cannot be designed on a purely functional layer. 
Therefore, here we are primarily interested in the 
semantic layer, that is, the information exchange 
and information building effects of a conversation. 
In order to properly understand a dialogue, both 
semantic and functional layers need to be consid-
ered. In this paper we are concentrating exclusively 
on the semantic layer. 
 
9   Clarification Dialogue 
Data frames with a conflict score of zero form the 
initial kernel answer space. Depending upon the 
size of this set and the presence of other frames 
outside of it, the system either proceeds to generate 
the answer or initiates a dialogue with the user. For 
                                                 
5 A notable exception is CU Communicator developed 
at University of Colorado (Ward and Pellom, 1999) 
 example, if the answer space appears too large or 
varied, e.g. consists of many different topics, the 
system may ask the user how to narrow it. Alterna-
tively, the presence of large groups of texts frames 
with near-miss frames assigned to them (i.e., 
frames with 1 or 2 conflicts with the Goal Frame) 
may indicate that the answer space is actually lar-
ger, and the user will be consulted about a  possi-
ble broadening of the question. Currently, we only 
initiate a clarification dialogue for 1-conflict 
frames.  
     A 1-conflict frame has only a single attribute 
mismatch with the Goal Frame. This could be a 
mismatch on any attribute, for example, LOCA-
TION, or ORGANIZATION, or TIME, etc. A special 
case arises when the conflict occurs on the TOPIC 
attribute.  Since all other attributes match, we may 
be looking at potentially different events or situa-
tions involving the same entities, or occurring at 
the same location or time. The purpose of the clari-
fication dialogue in this case is to probe which of 
these topics may be of interest to the user.  This is 
illustrated in the exchange below recorded during 
an evaluation session with an intelligence analyst: 
 
User: ?Who is Elizardo Sanchez?? 
HITIQA: ?Are you interested in seeing information 
about civil rights as it is related to Elizardo Sanchez? 
 
ONE-CONFLICT FRAME 
TOPIC: civil_rights  
SUB-TOPIC: []  
LOCATION: [Cuba, Cuba, Cuba]  
COMPANIES: []  
PEOPLE: [Sanchez, Sanchez]  
DOCUMENT DATE: [2000, 1, 11]  
SOURCE: HAVANA (AP) ?  
TEXT: [``I consider that the situation for civil and political 
rights in Cuba has worsened over the past year... owing to 
that Cuba continues to be the only closed society in this hemi-
sphere,'' Sanchez said. ``There have been no significant re-
lease of prisoners, the number of people sanctioned or proc-
essed for political motives increased. Sanchez, who himself 
spent many years in Cuban prisons, is among the communist 
island's best known opposition activists. The commission he 
heads issues a report on civil rights every six months, along 
with a list of people it considers to be imprisoned for political 
motives. ] 
Figure 5: One of the Frames that were used in generat-
ing Sanchez  dialogue.  Words in bold were used to fill 
the Frame. 
 
    In order to understand what happened here, we 
need to note first that the Goal Frame for the user 
question does not have any specific value assigned 
to its TOPIC attribute. This of course is as we would 
expect it: the question does not give us a hint as to 
what information we need to look for or may be 
hoping to find about Sanchez. This also means that 
all the text frames obtained from the retrieved set 
for this question will have at least one conflict, 
near-misses. One such text frame is shown in Fig-
ure 5: its topic is ?civil rights? and it about San-
chez. HITIQA thus asks if ?civil rights? is a topic 
of interest to the user. If the user responds posi-
tively, this topic will be added to the answer space.    
     The above dialogue strategy is applicable to 
other attribute mismatch cases, and produces intel-
ligent-sounding responses from the system. During 
the dialogue, as new information is obtained from 
the user, the Goal Frame is updated and the scores 
of all the data frames are reevaluated. The system 
may interpret the new information as a positive or 
negative. Positives are added to the Goal Frame. 
Negatives are stored in a Negative-Goal Frame and 
will also be used in the re-scoring of the data 
frames, possibly causing conflict scores to in-
crease. The Negative-Goal Frame is created when 
HITIQA receives a negative response from the 
user. The Negative-Goal Frame includes informa-
tion that HITIQA has identified as being of no in-
terest to the user.  If the user responds the equiva-
lent of ?yes? to the system clarification question  in 
the Sanchez dialogue, civil_rights will be added to 
the topic list in the Goal Frame and all one-conflict 
frames with a civil_rights topic will be re-scored to 
Zero conflicts, two-conflict frames with 
civil_rights as a topic will be rescored to one, etc.  
If the user responds ?no?, the Negative-Goal 
Frame will be generated and all frames with 
civil_rights as a topic will be rescored to 99 in or-
der to remove them from further processing. 
     The clarification dialogue will continue on the 
topic level until all the significant sets of NEAR-
MISS frames are either included in the answer 
space (through user broadening the scope of the 
question that removes the initial conflicts) or dis-
missed as not relevant. When HITIQA reaches this 
point it will re-evaluate the data frames in its an-
swer space.  If there are too many answer frames 
now (more than a pre-determined upper threshold), 
the dialogue manager will offer to the user to nar-
row the question using another frame attribute. If 
the size of the new answer space is still too small 
(i.e., there are many unresolved near-miss  frames), 
 the dialogue manager will suggest to the user ways 
of further broadening the question, thus making 
more data frames relevant, or possibly retrieving 
new documents by adding terms acquired through  
the clarification dialogue.  When the number of 
frames is within the acceptable range, HITIQA will 
generate the answer using the text from the frames 
in the current answer space.  The user may end the 
dialogue at any point and have an answer gener-
ated given the current state of the frames. 
 
9.1   Narrowing Dialogue 
HITIQA attempts to reduce the number of frames 
judged to be relevant through a Narrowing Dia-
logue. This is done when the answer space con-
tains too many elements to form a succinct answer. 
This typically happens when the initial question 
turns out to be too vague or unspecific, with re-
spect to the available data. 
 
9.2   Broadening Dialogue 
As explained before, the system may attempt to 
increase the number of frames judged relevant 
through a Broadening Dialogue (BD), whenever 
the answer space appears too narrow, i.e., contains 
too few zero-conflict frames.  We are conducting 
further experiments to define this situation more 
precisely. Currently, the BD will only occur if 
there are one-conflict frames, or near misses. 
Broadening questions can be asked about any of 
the attributes which have values in the Goal Frame. 
 
10   Answer Generation 
Currently, the answer is simply composed of text 
passages from the zero conflict frames. The text of 
these frames are ordered by date and outputted to 
the user.  Typically the answer to these analytical 
type questions will require many pages of informa-
tion.  Example 1 below shows the first portion of 
the answer generated by HITIQA for the Black Sea 
query. Current work is focusing on answer genera-
tion. 
 
2002:  
The Black Sea is widely recognized as one of the re-
gional seas most damaged by human activity. Almost 
one third of the entire land area of continental Europe 
drains into this sea? major European rivers, the Da-
nube, Dnieper and Don, discharge into this sea while its 
only connection to the world's oceans is the narrow 
Bosphorus Strait. The Bosphorus is as little as 70 me-
ters deep and 700 meters wide but the depth of the 
Black Sea itself exceeds two kilometers in places. Con-
taminants and nutrients enter the Black Sea via river 
run-off mainly and by direct discharge from land-based 
sources. The management of the Black Sea itself is the 
shared responsibility of the six coastal countries: Bul-
garia, Georgia, Romania, Russian Federation, Turkey, 
and Ukraine? 
Example 1: Partial answer generated by HITIQA to the 
Black Sea query. 
 
11   Evaluations 
We have just completed the first round of a pilot 
evaluation for testing the interactive dialogue com-
ponent of HITIQA. The purpose of this first stage 
of evaluation is to determine what kind of dialogue 
is acceptable/tolerable to the user and whether an 
efficient navigation though the answer space is 
possible.  HITIQA was blindly tested by two dif-
ferent analysts on eleven different topics.  Five 
different groups participated, but no analyst tested 
more than one system, as system comparison was 
not a goal.  The analysts were given complete free-
dom in forming their queries and responses to 
HITIQA?s questions.  They were only provided 
with descriptions of the eleven topics the systems 
would be tested on.  The analysts were given 15 
minutes for each topic to arrive at what they be-
lieved to be an acceptable answer. During testing a 
Wizard (human) was allowed to intervene if 
HITIQA generated a dialogue question/response 
that was felt inappropriate. The Wizard was able to 
override the system and send a Wizard generated 
question/response to the analyst.  The HITIQA 
Wizard intervened an average of 13% of the time. 
     These results are for information purposes only 
as it was not a formal evaluation.  HITIQA earned 
an average score of 5.8 from both Analysts for dia-
logue, where 1 was ?extremely dissatisfied? and 7 
was ?completely satisfied?.  The highest score pos-
sible was a 7 for each dialogue.  The Analysts were 
asked to grade each scenario for success or failure.  
We divide the failures from both analysts into three 
categories: 
1) the user gives up on the system for the 
given scenario(9%) 
2) the 15 minute time limit was up(13%) 
3) the data was not in the database(9%) 
HITIQA had a 63% success rate for Analyst 1 and 
a 73% success rate for Analyst 2. It is unclear how 
 these results should be interpreted, if at all, as the 
evaluation was a mere pilot, mostly to test the me-
chanics of the setup. We know only that a human 
Wizard equipped with all necessary information 
can easily achieve 100% success in this test. What 
is still needed is a baseline performance, perhaps 
based on using an ordinary keyword-based search 
engine.  
12   Future Work 
This paper describes a work in progress. We ex-
pect that the initial specification of content frame 
will evolve as we subject the initial system to more 
demanding evaluations. Currently, the frames are 
not topically specialized, and this appears the most 
logical next refinement, i.e., develop several (10-
30) types of frames covering different classes of 
events, from politics to medicine to science to in-
ternational economics, etc. This is expected to in-
crease the accuracy of the dialogue as is the inter-
active visualization which is also under develop-
ment. Answer generation will involve fusion of 
information on the frame level, and is currently in 
an initial phase of implementation. 
Acknowledgements 
This paper is based on work supported by the Advanced 
Research and Development Activity (ARDA)?s Ad-
vanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 2002-
H790400-000. 
References  
J. Allen.  and Core. 1997. Draft of DAMSL:  Dialog Act 
Markup in Several Layers. 
http://www.cs.rochester.edu/research/cisd/ resources/damsl/    
Bagga, A., T. Strzalkowski, and G.B. Wise. 2000. PartsID: A 
Dialog-Based System for Identifying Parts for Medical 
Systems. Proc. of the ANLP-NAACL-2.  
Chris Buckley. May 1985. Implementation of the Smart in-
formation retrieval system. Technical Report TR85-686, 
Department of Computer Science, Cornell University, 
Ithaca, NY.  
James P. Callan, W. Bruce Croft, Stephen M. Harding 1992. 
The INQUERY Retrieval System.  Proc. of DEXA-92, 3rd 
International Conference on Database and Expert Systems 
Applications. 78-83.  
Cunningham, H., D. Maynard, K. Bontcheva, V. Tablan and 
Y. Wilks. 2000 Experience of using GATE for NLP R&D. 
In Coling 2000 Workshop on Using Toolsets and Architec-
tures To Build NLP Systems.  
DARPA Communicator Program. 2002. 
http://www.darpa.mil/iao/communicator   
 Grinstein, G.G., Levkowitz, H., Pickett, R.M., Smith, S. 1993. 
?Visualization alternatives: non-pixel based images,? 
Proc. of IS&T 46th Annual Conf. 132-133.  
George Ferguson and James Allen. 1998. "TRIPS: An Intelli-
gent Integrated Problem-Solving Assistant," in Proc. of 
the Fifteenth National Conference on Artificial Intelli-
gence (AAAI-98), Madison, WI. 567-573.  
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, B. Wise and 
X. Zhang 2002a. Cross-Document Summarization by 
Concept Classification. Proceedings of SIGIR-2002, Tam-
pere, Finland.  
H. Hardy, K. Baker, L. Devillers, L. Lamel, S. Rosset, T. 
Strzalkowski, C. Ursu and N. Webb.  2002b.  Multi-layer 
Dialogue Annotation for Automated Multilingual Cus-
tomer Service. ISLE Workshop, Edinburgh, Scotland.  
Harabagiu, S., M. Pasca and S. Maiorano. 2000. Experiments 
with Open-Domain Textual Question Answering. In Proc. 
of COLING-2000. 292-298.  
Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, 
H. Cunningham, Y. Wilks. 1998. Description of the 
LaSIE-II System as Used for MUC-7. In Proceedings of 
the Seventh Message Understanding Conference (MUC-
7.)  
Judith Hochberg, Nanda Kambhatla and Salim Roukos. 2002. 
A Flexible Framework for Developing Mixed-Initiative 
Dialog Systems. Proc. of 3rd SIGDIAL Workshop on Dis-
course and Dialogue, Philadelphia.  
Hovy, E., L. Gerber, U. Hermjakob, M. Junk, C-Y. Lin. 2000. 
Question Answering in Webclopedia. Notebook Proceed-
ings of Text Retrieval Conference (TREC-9).  
Johnston, M., Ehlen, P., Bangalore, S., Walker., M., Stent, A., 
Maloor, P., and Whittaker, S. 2002. MATCH: An Archi-
tecture for Multimodal Dialogue Systems. In Meeting of 
the Association for Computational Linguistics , 2002.  
Diane J. Litman and Shimei Pan. Designing and Evaluating an 
Adaptive Spoken Dialogue System. 2002. User Modeling 
and User-Adapted Interaction. 12(2/3):111-137.  
Miller, G.A. 1995. WordNet: A Lexical Database. Comm. of 
the ACM, 38(11):39-41. 
John Prager, Dragomir R. Radev, and Krzysztof Czuba. An-
swering what-is questions by virtual annotation. In Human 
Language Technology Conference, Demonstrations Sec-
tion, San Diego, CA, 2001.  
S. Seneff and J. Polifroni, ``Dialogue Management in the 
MERCURY Flight Reservation System,'' Proc. ANLP-
NAACL 2000, Satellite Workshop, 1-6, Seattle, WA, 2000.   
Marilyn A. Walker. An Application of Reinforcement Learn-
ing to Dialogue Strategy Selection in a Spoken Dialogue 
System for Email . Journal of Artificial Intelligence Re-
search.12:387-416.  
W. Ward and B. Pellom.  1999.  The CU Communicator Sys-
tem.  IEEE ASRU. 341-344.  
HITIQA: Scenario Based Question Answering 
Sharon Small, Tomek Strzalkowski, Tracy Janack, Ting Liu,  
Sean Ryan, Robert Salkin, Nobuyuki Shimizu 
The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tomek,tj5550,tl7612,seanryan,rs6021,ns3203}@albany.edu 
 
Paul Kantor, Diane Kelly, Robert Rittman, Nina Wacholder 
Rutgers University 
New Brunswick, New Jersey 08903 
{kantor, nina, diane, rritt}@scils.rutgers.edu 
 
Boris Yamrom 
Lehman College of the City University of New York 
Bronx, New York 10468 
byamrom@lehman.cuny.edy 
 
 
 
Abstract 
In this paper we describe some preliminary 
results of qualitative evaluation of the answer-
ing system HITIQA (High-Quality Interactive 
Question Answering) which has been devel-
oped over the last 2 years as an advanced re-
search tool for information analysts. HITIQA 
is an interactive open-domain question an-
swering technology designed to allow analysts 
to pose complex exploratory questions in natu-
ral language and obtain relevant information 
units to prepare their briefing reports in order 
to satisfy a given scenario. The system uses 
novel data-driven semantics to conduct a clari-
fication dialogue with the user that explores 
the scope and the context of the desired answer 
space. The system has undergone extensive 
hands-on evaluations by a group of intelli-
gence analysts representing various foreign in-
telligence services. This evaluation validated 
the overall approach in HITIQA but also ex-
posed limitations of the current prototype.  
1   Introduction 
Our objective in HITIQA is to allow the user to 
submit exploratory, analytical questions, such as ?What 
has been Russia?s reaction to U.S. bombing of Kos-
ovo?? The distinguishing property of such questions is 
that one cannot generally anticipate what might consti-
tute the answer. While certain types of things may be 
expected (e.g., diplomatic statements), the answer is 
heavily conditioned by what information is in fact avail-
able on the topic, background knowledge of the user, 
context in the scenario, intended audience, etc. From a 
practical viewpoint, analytical questions are often un-
derspecified, thus casting a broad net on a space of pos-
sible answers. Therefore, clarification dialogue is often 
needed to negotiate with the user the exact scope and 
intent of the question, and clarify whether similar topics 
found might also be of interest to the user in order to 
complete their scenario report. This paper will present 
results from a series of evaluations conducted in a series 
of workshops with the intended end users of HITIQA 
(professional intelligence analysts) using the system to 
solve realistic analytic problems. 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in the 
state of the art of automated question answering.  In this 
paper we focus on our approach to analytical question 
answering in order to produce a report in response to a 
given scenario.  We also report on the user evaluations 
we conducted and their results with respect to our 
unique approach. 
2   Analytical QA Scenarios 
Analytical scenarios are information task directives 
assigned to analysts to support a larger foreign policy 
process. Scenarios thus contain the information need 
specifications at various levels of detail,  the type, for-
mat and timing of the response required (an intelligence 
report) as well as the primary recipient of the report 
(e.g., the Secretary of State). A hypothetical, but realis-
tic scenario is shown in Figure 1 below. This scenario, 
along with several others like it, was used in evaluating 
 HITIQA performance and fitness for supporting the 
analytical process.  
As can be readily assessed from the directives in 
Figure 1, scenarios are not merely tough questions; they 
are far too complex to be considered as a single question 
at all. It is equally clear that no simple answer can be 
expected and that preparing a report would mean find-
ing answers to a series of interlocking questions or vari-
ous granularities.  
 
Scenario: The al-Qaida Terrorist Group 
 
As an employee of the Central Intelligence Agency, your pro-
fession entails knowledge of the al-Qaida terrorist group.  
Your division chief has ordered a detailed report on the al-
Qaida Terrorist Group due in three weeks. Provide as much 
information as possible on this militant organization. Eventu-
ally, this report should present information regarding the most 
essential concerns, including who are the key figures involved 
with al-Qaida along with other organizations, countries, and 
members that are affiliated, any trades that al-Qaida has made 
with organizations or countries, what facilities they possess, 
where they receive their financial support, what capabilities 
they have (CBW program, other weapons, etc.) and how have 
they acquired them, what is their possible future activity, how 
their training program operates, who their new members are. 
Also, include any other relevant information to your report as 
you see fit.  
 FIGURE 1: Scenario used during user evaluations 
  
We have organized a series of usability evaluations 
with active duty intelligence analysts to find out how 
they approach the problem of solving a scenario. The 
prerequisites for this were are follows: 
1. A robust, broadly functional analytical QA sys-
tem capable of sustaining realistic analytic 
tasks. 
2. A realistic corpus of ?raw intelligence? in form 
of varying quality and verity new-like reports. 
3. A set of realistic, average complexity analytic 
tasks or scenarios to be used. 
HITIQA has been developed over the past two years as 
an open-ended highly flexible interactive QA system to 
allow just this type of evaluation. The system supports a 
variety of information gathering functions without 
straight jacketing the user into any particular mode or 
interaction style. The system does not produce cut and 
dry ?answers?; instead it allows the analysts to build the 
answers the way they want them. While this open-
endedness may seem like unfinished business, we be-
lieve that further development must take into account 
the needs of analysts if they were ever to adopt this 
technology in their work. 
Our main hypothesis is that analysts employ a range 
of strategies to find the required information and that 
these strategies depend significantly upon the nature of 
the task and the progress the analyst is making on the 
task, in addition to individual differences between ana-
lysts. Our experience with interactive systems also indi-
cated that real users are unlikely to follow any single 
information exploration strategy, but instead would use 
multiple, parallel, even overlapping approaches in order 
to maximize the returns and their confidence in the re-
sults. As a corollary we may expect that the scenario 
tasks are unlikely to be systematically decomposed into 
a series of smaller tasks ahead of actual search. In other 
words, the analytical process is a dialogue, not a se-
quence of commands. Moreover, questions actually 
submitted to the system during the analytical process 
seldom seek just the exact answer, instead they are often 
considered as ?light beams? through the data: focusing 
on the answer but also illuminating adjacent, related 
information which may prove just as valuable.  
AFRL, NIST, CNS and ARDA collaborated in the 
development of scenarios used in our evaluation ses-
sions.  
3   Data Driven Semantics of Questions 
When the user poses a question to a system having 
access to a huge database of unstructured data (text 
files), we need to first reduce the big pile to perhaps a 
handful of documents where the answer is likely to be 
found. The easiest way to do it is to convert the question 
into a search query (by removing stopwords and stem-
ming and tokenizing other words) and submitting this 
query to a fast but non-exact document retrieval system, 
e.g.,   Smart (Buckley, 1985) or InQuery (Callan et al, 
1992), or if you are on the web, Google, etc.   
In the current prototype of HITIQA, we use a com-
bination of Google and InQuery to retrieve the top 50 to 
200 documents from a large document database, con-
sisting of several smaller collections such as newspaper 
stories, documents from the Center of Nonproliferation 
Studies, as well as web mined files.  The retrieved 
documents are then broken down into passages, mostly 
exploiting the naturally occurring paragraph structure of 
the original sources. 
The set of text passages returned from the initial 
search is the first (very crude) approximation of the An-
swer Space for the user?s first question. In order to de-
termine what this answer space consists of we perform 
automatic analysis (a combination of hierarchical clus-
tering and classification) to uncover if what we got is a 
fairly homogenous collection (i.e., all texts have very 
similar content), or whether there are a number of di-
verse topics or aspects represented in there, somehow 
tied together by a common thread. In the former case, 
we may be reasonably confident that we have the an-
swer, modulo the retrievable information. In the latter 
case, we know that the question is more complex than 
the user may have intended, and a negotiation process is 
needed to clarify topics of interest for the scenario re-
port. 
 The next step is to measure how well each of the as-
pects within the answer space is ?matching up? against 
the original question. This is accomplished through the 
framing process described later in this paper. The out-
come of the framing process is twofold: first, the alter-
native interpretations of the question are ranked within 3 
broad categories: on-target, near-misses and outliers. 
Second, salient concepts and attributes for each topi-
cal/aspectual group are extracted into topic frames. This 
enables the system to conduct a meaningful dialogue 
with the user, a dialogue which is wholly content ori-
ented, and entirely data driven.  
4   Partial structuring of text data 
In HITIQA we use a text framing technique to de-
lineate the gap between the meaning of the user?s ques-
tion and the system ?understanding? of this question. 
The framing is an attempt to impose a partial structure 
on the text that would allow the system to systemati-
cally compare different text pieces against each other 
and against the question, and also to communicate with 
the user about this. In particular, the framing process 
may uncover topics or aspects within the answer space 
which the user has not explicitly asked for, and thus 
may be unaware of their existence.  This approach is 
particularly beneficial to the needs of the scenario prob-
lem, where these similar aspects frequently are needed 
in completely ?answering? the scenario, with the sce-
nario report.   
In the current version of HITIQA, frames are pre-
defined structures representing various event types. We 
started with the General frame, which can represent any 
event or relation involving any number of entities such 
as people, locations, organizations, time, and so forth.  
In a specialized domain, or if the user interests are 
known to be limited to a particular set of topics, we de-
fine domain-specific frames. Current HITIQA prototype 
has three broad domain-specific frames, related to the 
Weapon of Mass Destruction proliferation domain 
(which was one of the domains of interest to our users). 
These frames are: WMDTransfer, WMDDevelop, 
WMDTreaty, and of course we keep the General frame.  
Obviously, these three frames do not cover the domain 
represented by our data set; they merely capture the 
most commonly occurring types of events. All frames 
contain a small number of core attributes, such as LO-
CATION, PERSON, COUNTRY, ORGANIZATION, ETC., which 
are extracted using BBN?s Identifinder software, which 
extracts 24 types of entities.  Domain-specific frames 
add event specific attributes, which may require extract-
ing additional items from text, or assigning roles to ex-
isting attributes, or both.  For example, WMDTransfer?s 
attributes TRANSFER_TO and TRANSFER_FROM define 
roles of some COUNTRY or ORGANIZATION, while the 
TRANSFER_TYPE attribute scans the text for keywords 
that may indicate the type of transfer, e.g., export, sale, 
etc.  
HITIQA creates a Goal frame for the user?s ques-
tion, which can be subsequently compared to the data 
frames obtained from retrieved data. A Goal frame can 
be a General frame or any of the domain specific frames 
available in HITIQA.  For example, the Goal frame 
generated from the question, ?Where does al-Qaida 
have training facilities?? is a General frame as shown in 
Figure 2.  This was the first question generated by one 
of our analysts during the first evaluation while working 
on the al-Qaida scenario shown in Figure 1. 
 
FRAME TYPE: General 
TOPIC: training facilities 
ORGANIZATION: al-Qaida 
FIGURE 2: HITIQA generated General-type Goal frame from 
the al-Qaida training facilities question 
 
FRAME TYPE: General 
CONFLICT SCORE: 1 
TRANSFER TYPE: provided 
TRANSFER TO: al-Qaida 
TRANSFER FROM: Iraq 
TOPIC: provided 
SUB-TOPIC: imported 
LOCATION: Iraq 
PEOPLE: Abu Musab al-Zarqawi, Bush, George 
Tenet, Saddam Hussein 
ORGANIZATION:CIA, Administration, al-Qaida 
DOCUMENT: web_283330  
PARAGRAPHS:  ["CIA chief George Tenet seems to 
have gone a long way to back the Bush Administrations dec-
larations that the long split between Islamic fundamentalist 
terrorist organizations like Al-Qaida and secular Iraqi ruler 
Saddam Hussein is healed.   
He has testified that the CIA has evidence of Iraqi provid-
ing Al Qaida with training in forgery and bomb making and of 
providing two, Al Qaida associates with training in gas and 
poisons. He said also that Iraq is harboring senior members 
of a terrorist network led by Abu Musab al-Zarqawi, a close 
Al Qaida associate. "]  
RELEVANCE:  Conflict: [Topic] 
FIGURE 3: A HITIQA generated data frame and the un-
derlying text passage. Words in bold were used to fill the 
Frame.   
 
HTIQA automatically judges a particular data frame 
as relevant, and subsequently the corresponding seg-
ment of text as relevant, by comparison to the Goal 
frame. The data frames are scored based on the number 
of conflicts found between them and the Goal frame. 
The conflicts are mismatches on values of correspond-
ing attributes. If a data frame is found to have no con-
flicts, it is given the highest relevance rank, and a con-
flict score of zero.  All other data frames are scored with 
 a decreasing value based on the number of conflicts, 
negative one for frames with one conflict with the Goal 
frame, negative two for two conflicts etc.  Frames that 
conflict with all information found in the question are 
given a score of -99 indicating the lowest relevancy 
rank.  Currently, frames with a conflict score of -99 are 
excluded from further processing as outliers. The frame 
in Figure 2 is scored as a near miss and will generate 
dialogue, where the user will decide whether or not it 
should be included in the answer space. 
5   Clarification Dialogue 
Data frames with a conflict score of zero form the 
initial kernel answer space. Depending upon the pres-
ence of other frames outside of this set, the system ei-
ther proceeds to generate the answer or initiates a dia-
logue with the user.  HITIQA begins asking the user 
questions on these near-miss frame groups, with the 
largest group first.  The groups must be at least groups 
of size N, where N is a user controlled setting.  This 
setting restricts all of HITIQA?s generated dialogue.   
A one conflict frame has only a single attribute 
mismatch with the Goal frame. This could be a mis-
match on any of the General attributes, for example, 
LOCATION, or ORGANIZATION, or TIME, etc., or in one of 
the domain specific attributes, TRANSFER_TO, or TRANS-
FER_TYPE, etc.  A special case arises when the conflict 
occurs on the TOPIC attribute.  Since all other attributes 
match, we may be looking at potentially different events 
or situations involving the same entities, or occurring at 
the same location or time. The purpose of the clarifica-
tion dialogue in this case is to probe which of these top-
ics may be of interest to the user.  Another special case 
arises when the Goal frame is of a different type than a 
data frame.  The purpose of the clarification dialogue in 
this case is to expand the user?s answer space into a 
different but possibly related event.  A combination of 
both of these cases is illustrated in the exchange in Fig-
ure 4 below.   
User: ?Where does al-Qaida have training facili-
ties?? 
HITIQA: ?Do you want to see material on the trans-
fer of weapons and intelligence to al-Qaida?? 
FIGURE 4: Dialogue generated by HITIQA for the al-Qaida 
training facilities question 
 
In order to understand what happened here, we need 
to note first that the Goal frame for this example is a 
General Frame, from Figure 2.  One of the data frames 
that caused this dialogue to be generated is shown in 
Figure 3 above.  While this frame is of a different frame 
type than the Goal frame, namely WMD Transfer, it 
matches on all of the General attributes except TOPIC, so 
HITIQA asks the user if they would like to expand their 
answer space to this other domain, namely to include 
the transfer of weapons involving this organization as 
well.   
 
ANSWER REPORT:  
 
The New York Times said the Mindanao had become the 
training center for the Jemaah Islamiah network, believed by 
many Western governments to be affiliated to the al-Qaida 
movement of Osama bin Laden 
DocName: A-web_283305 ParaId: 2  
 
? 
IRAQ REPORTED TO HAVE PROVIDED MATERIALS 
TO AL QAIDA  
2003  
[CIA chief George Tenet seems to have gone a long way to 
back the Bush Administrations declarations that the long split 
between Islamic fundamentalist terrorist organizations like Al 
Qiada and secular Iraqi ruler Saddam Hussein is healed. 
DocName: A-web_283330 ParaId: 6  
He has testified that the CIA has evidence of Iraqi providing 
Al Qaida with training in forgery and bomb making and of 
providing two, Al Qaida associates with training in gas and 
poisons. He said also that Iraq is harboring senior members of 
a terrorist network led by Abu Musab al-Zarqawi, a close Al 
Qaida associate. The Bush Administration and the press has 
carelessly shorthanded this to mean, a senior Al Qaida mem-
ber, ignoring the real ambiguities that surround the true nature 
of that association, and whether Zarqawi shares Al Qaidas 
ends, or is receiving anything more than lodging inside Iraq. ] 
DocName: A-web_283330 ParaId: 7  
FIGURE 5: Partial answer generated by HITIQA to the al-
Qaida training facilities question 
 
During the dialogue, as new information is obtained 
from the user, the Goal frame is updated and the scores 
of all the data frames are reevaluated. The system may 
interpret the new information as a positive or negative. 
Positives are added to the Goal frame. Negatives are 
stored in a Negative-Goal frame and will also be used in 
the re-scoring of the data frames, possibly causing con-
flict scores to increase. If the user responds the equiva-
lent of ?yes? to the system clarification question in Fig-
ure 4, a corresponding WMD Transfer frame would be 
added to the Goal frame and all WMD Transfer frames 
will be re-scored.  If the user responds ?no?, the Nega-
tive-Goal frame will be generated and all WMD Trans-
fer frames will be rescored to 99 in order to remove 
them from further processing.  The user may end the 
dialogue, at any point and have an answer generated 
given the current state of the frames.   
Currently, the answer is simply composed of text 
passages from the zero conflict frames. In addition, 
HITIQA will generate a ?headline? for the text passages 
in all the Frames in the answer space.  This is done us-
ing grammar rules and the attributes of a frame.  Figure 
 5 shows a portion of the answer generated by HITIQA 
for the al-Qaida training facilities question. 
 
6   HITIQA Interface 
There are two distinct ways for the user to interact 
with HITIQA to explore their answer space.  The An-
swer Panel displays the user?s current answer at any 
given time during the interaction for a single question.  
Through this panel the user can read the paragraphs that 
are currently in their answer.  There are links on this 
panel so the user is able to view the full original source 
document from which the passage(s) were extracted. 
 The Visual panel offers the user an alternative to 
reading text by providing a tool for visually browsing 
the entire answer space.  Figure 6 shows a typical view 
of the visualization panel. The spheres are representa-
tive of single frames and groups of frames.  The user?s 
attention may be drawn to particular frames by the color 
coding or the attribute spikes.  The colors represent the 
frame?s score, so the user can quickly see what is in 
their answer, blue, and what is not, all other colors.  The 
attribute spikes may also be used as a navigation tool.  
The active attribute is chosen by the user through radio 
buttons. The current active attribute in Figure 6, is Lo-
cation.  This displays all instances of locations men-
tioned in the corresponding text. 
 
 
        Figure 6: Frame Level Display 
 
The underlying text that was used to build the frame 
may be displayed in the lower right hand window.  In 
this text display window there is a hyperlink that takes 
the user directly to the full source document. The user is 
able to interact with this panel by adding and removing 
information from their generated answer. Moving from 
the visualization to the textual dialogue, the generated 
answer, and back is seamless in a sense that any 
changes to the frame scores in one modality are imme-
diately accessible to the user in another modality. Users 
can add and remove frames from the answer space and 
HITIQA will always seamlessly pickup a new dialogue 
or generate a new answer.  
 
7   HITIQA Qualitative Evaluations 
In order to assess our progress thus far, and to also 
develop metrics to guide future evaluation, we invited a 
group of analysts employed by the US government to 
participate in two three-day workshops held in Septem-
ber and October 2003.  
The two basic objectives of the workshops were: 
1. To perform a realistic assessment of the useful-
ness and usability of HITIQA as an end-to-end system, 
from the information seeker's initial questions to com-
pletion of a draft report.  
2. To develop metrics to compare the answers ob-
tained by different analysts and evaluate the quality of 
the support that HITIQA provides.     
Each of these objectives entails a particular chal-
lenge. Performing a realistic assessment of HITIQA is 
difficult because many of the resources that the analysts 
use, as well as the reports they produce, are classified 
and therefore inaccessible to researchers.  
Assessing the quality of the support that the system 
provides is not easy because analytical questions rarely 
have a single right answer. It is not obvious how to de-
fine, for example, the precision of the system. We there-
fore conducted an 'information unit' exercise, whose 
purpose was to determine whether the analysts could 
identify information building blocks in their reports, so 
that we could compare and contrast different reports.  
To obtain an adequate supply of appropriate text 
data to support extensive question answering sessions 
(1, 2, 3 and 4 hours long), we prepared a new corpus of 
approximately 1.2 Gbytes. This new corpus consists of 
the reports from the Center for Non-Proliferation Stud-
ies (CNS) collected for the AQUAINT Program, aug-
mented with a much larger collection of texts on similar 
subject matter mined from the web using Google1. The 
final corpus proved to be sufficient to support about 
three hours of use of HITIQA to ?solve? each of the 
scenarios. 
The first day of the first workshop was devoted to 
training, including a two-part proficiency test. HITIQA 
is a fairly complex system, that includes multiple layers 
of data processing and user interaction, and it was criti-
cal that the users are sufficiently ?fluent? if we were to 
measure their productivity. The analysts' primary task 
on the second day was preparation of reports in re-
sponse to the scenarios. 
                                                 
1 Google has kindly agreed to temporarily extend our 
usage license so we could collect the data over a short 
time. 
  The third day was devoted to quantitative and quali-
tative evaluation, discussed later. In addition, we asked 
the analysts to score each others reports, as well as to 
identify key information units in them. These informa-
tion units could be later compared across different re-
ports in order to determine their completeness.  
8   Workshop Results 
The results of the quantitative evaluations strongly vali-
date the approach that we have taken. These conclusions 
are confirmed by analysts comments gleaned both from 
the formal qualitative assessment and from informal 
discussion. As one analyst said, ?the system as it stands 
now, in my mind, gave me enough information to try to 
put together a 80% solution but ?I don't think you're 
ever gonna reach that 100% state.? At the same time, we 
learned a great deal about how analysts work. 
It is important to determine the realism of the sce-
narios used during the workshop relative to the analysts? 
current work tasks in order for any results to be mean-
ingful. Each analyst was asked a series of five questions 
such as, ?How realistic was the scenario?  In other 
words did it resemble tasks you could imagine perform-
ing at work?? These 5 questions were all relative to the 
realism and difficulty of the scenario tasks.  Analysts 
used a scale of 1 to 5 based on their agreement with the 
statements, where 5 was complete agreement.  Our 
mean score was 3.84, indicating our scenarios were real-
istic and of about average difficulty when compared to 
the work they normally perform.   
We have classified the type of passages that an ana-
lyst copied to their report into two categories, answer 
passages and additional information passages, see Fig-
ure 7 below.  The answer passages either exactly an-
swered the user?s initial question or supplied supporting 
information.  The additional passages do not answer the 
original question posed, but may have been added to the 
answer through dialogue, or through the user?s explora-
tion of document links offered.  This could be a piece of 
information needed to satisfy some other aspect of the 
scenario that they had not asked about yet, or possibly a 
topic the user had not even considered but found rele-
vant when it was presented to them. As can be seen 
there was a very large amount of ?additional? informa-
tion that the user copied to their report.  The amounts 
reported here are the averages for all of the analysts for 
both workshops.  This supports our hypothesis that ana-
lysts seldom seek just the exact answer, but they are 
also looking at adjacent, related information, much of 
which they retain for their report.  Note that there were a 
small number of passages that contained a combination 
of answer and additional information; these were added 
to answer.   
 
Average Number of Passages Copied to Report
2.83
13.63
1.54
5.06
0.00
2.00
4.00
6.00
8.00
10.00
12.00
14.00
16.00
answ er additional
Passage Type
N
um
be
r o
f P
as
sa
ge
s
copied f rom link
copied f rom answ er
 Figure 7: Average Number of Passages Copied 
 
Total Passages Copied and Viewed: Analyst 2
37
8 16
28 27
230
50
242
352
152
16 11 4 7 4
49
27 26 34
44
0
50
100
150
200
250
300
350
400
1 2 3 4 5
Scenario
N
um
be
r o
f P
as
sa
ge
s
passages copied from links
passages viewed from links
passages copied from answer
passages viewed on answer
 
 
Figure 8: Number of Passages Copied Vs. Those Viewed 
 
We should now establish the number of passages 
copied versus those viewed, relative to links and the 
answer.  Figure 8 above shows the total number of pas-
sages copied versus the total number of passages 
viewed.  It is seen that many more passages need to be 
viewed through full document links before a useful pas-
sage is found.  In comparison a much smaller number of 
answer passages need to be viewed from the Answer 
panel in order to find useful passages.   
All of the analysts? sessions were recorded using 
Camtasia.  Figure 9 shows an annotation created for a 
typical session.  Analysts were observed to utilize a 
range of varying strategies as they worked different 
scenarios and even while working different queries of 
the same scenario.  Figure 10 shows the statistics for 
each Analyst?s use of HITIQA while working on the 
scenarios during the two workshops (note that Analyst-4 
was only able to attend the first workshop and Analyst-1 
did not create a report for Scenario 2).  Some of the 
variations in strategies among the analysts while work-
ing the same scenario are quite striking.  For example, 
Scenario 4 was  worked quite  differently  by  Analyst-1  
 versus Analyst-2.  While Analyst-1 spent almost all of 
his/her  time in  the Visual Panel, Analyst-2 spent virtu-
ally all of his/her time in the Answer panel.  Analyst-1 
produced his/her report copying 52 paragraphs while 
Analyst 2 copied only 35.  There are also large varia-
tions in the number of questions asked for the same sce-
nario.  Examine scenario 5, where Analyst-3 asked a 
total of 11 questions and Analyst-2 only needed to ask 2 
questions.  Relative to this, Analyst-3, who asked a 
much larger number of questions, copied only 28 pas-
sages, whereas Analyst-2 copied 31.  These variations, 
as stated earlier in the paper, could be due to the nature 
of the task, the progress the analyst is making on the 
task, in addition to individual differences between ana-
lysts. For example, the difference in the number of 
questions asked between Analyst-2 and Analyst-3 for 
scenario 5 may be due to difference in search strategies 
employed, but may also reflect the amount of back-
ground knowledge of the topic.   
 
      
        FIGURE 9: Fragment of an analytical session 
 
Variation of Strategies: Analyst 1
8
0
5 5 5
18
0
20
52
4248
0
41.5
86
60.5
12
0 1
6.67
26
1
10
100
1 2 3 4 5
Scenario
Variation of Strategies: Analyst 2
5
4
3 3
2
53
19 20
35 3135 29
47
5
17
61
20
33
100
72
1
10
100
1 2 3 4 5
Scenario
 
101 115
Variation of Strategies: Analyst 3
12
3
4
6
11
58
17
25 24
28
21
7
19
70
54
65
1
26
1
10
100
1 2 3 4 5
Scenario
 
Variation of Strategies: Analyst 4
2
3
0 0 0
35
0 0 0
37
0 0 0 0
36 40
0 0 0
34
1
10
100
1 2 3 4 5
Scenario
# questions asked
# passages copied
time in visual
time in answer
 
Figure 10: Varying Strategies Employed 
User: What is the status of South Africa's chemical, 
biological, and nuclear programs?  
          Clarification Dialogue: 1 minute 
? 6 questions generated by HITIQA 
? replied ?Yes? to 5 and ?No? to 1 
? 5+ passages added to answer 
           Studying Answer Panel: 60 minutes  
? Copying 24 passages to report 
? 10 from Answer 
? 14 from Links to Full Document 
? Visual Panel Browsing: 5 minutes 
? Nothing copied 
User: Has South Africa provided CBW material or 
assistance to any other countries?  
          Clarification Dialogue: 1 minute 
? 5 questions generated by HITIQA 
? replied ?Yes? to 2 and ?No? to 3 
? 2+ passages added to answer 
           Studying Answer Panel: 26 minutes 
? Copying 6 passages to report 
? 6 from Links to Full Document 
            Visual Panel browsing: 1 minute 
? Copying 1 passage to report 
? 1 from Links to Full Document 
User: How was South Africa's CBW program fi-
nanced?  
         Clarification Dialogue: 40 seconds 
? 7 questions generated by HITIQA 
? replied ?Yes? to 3 and ?No? to 4 
? 3+ passages added to answer 
            Studying Answer Panel: 11 minutes 
? Copying 3 passages to report 
? 1 from Answer 
      2 from Links to full Document 
  
There is, however, some consistency across the ana-
lysts in the amount of information retained per scenario. 
The charts are drawn in logarithmic scale, but it should 
be visible that scenarios 2 and 3 produced less interac-
tion and required less information to fulfill than scenar-
ios 4 and 5. It is also visible that scenario 1 required 
more questions to be asked and more exploration to be 
done in visual panel than other scenarios. 
Finally, it is important to provide some metric re-
garding the user?s overall satisfaction with their use of 
HITIQA.  At the end of each workshop Analysts were 
given a series of 17 questions, such as ?HITIQA helps 
me find important information?, shown in Figure 11, to 
assess their overall experience with the system.  Many 
of these questions were designed for the user to com-
pare HITIQA to the current tools they are using for this 
type of task.   Analysts again used a scale of 1 to 5 
based on their agreement with the statements.  The re-
sults were then converted, where 5 would always denote 
the best, and are shown in Figure 11 below.  It is impor-
tant to note that we scored highly overall, but addition-
ally we scored highly in the majority of questions rela-
tive to comparison of their current tools.  For example, 
for Question 14: ?Having HITIQA at work would help 
me find information faster than I can currently find it?, 
our mean score was 3.83.  
 
3.7215702092Total
3.00311117
4.141616
2.8614215
3.8314114
3.1632113
4.1424112
4.0024111
4.00710
3.71529
3.29258
3.143317
3.711516
4.43345
4.293314
4.14163
3.7114112
3.71611
ScoreScore5Score4Score3Score2Score1Question
MeanFrequency of Analyst's Scores of Overall Workshop I & II
1                  2                 3                 4    5 score
frequency
 
      FIGURE 11: Final Evaluation Results, Workshop 1 & 2 
 
In summary, the results from these two evaluations 
indicate that HITIQA, in its current state, is already 
competitive with the tools that the analysts are currently 
using in their work, supporting our overall approach to 
Analytical Question Answering.  HTIQA provides the 
user with a tool to find the passages needed to complete 
a report for a given scenario.  While working on a sce-
nario HITIQA has been shown to provide information 
which exactly answers the user?s question, and addi-
tionally HITIQA?s method brings to light other related 
information that the analyst retains in order to complete 
their report. 
 
Acknowledgements 
This paper is based on work supported by the Advanced 
Research and Development Activity (ARDA)?s Advanced 
Question Answering for Intelligence (AQUAINT) Program 
under contract number 2002-H790400-000. 
References  
Allen, J. and M. Core. 1997. Draft of DAMSL:  Dialog Act Markup in 
Several Layers. www.cs.rochester.edu/research/cisd/   
Baeza-Yates and Ribeiro-Neto. 1999. Modern Information Retrieval. 
Addison Wesley. 
Chris Buckley. 1985. Implementation of the Smart information re-
trieval system. Technical Report TR85-686, Department of Com-
puter Science, Cornell University, Ithaca, NY. 
Ferguson, George and James Allen. 1998. TRIPS: An Intelligent Inte-
grated Problem-Solving Assistant, in Proceedings of the 15th 
AAAI Conference (AAAI-98), Madison, WI, pp. 567-573. 
Hardy, H., N. Shimizu, T. Strzalkowski, L. Ting, B. Wise and X. 
Zhang. 2002a. Cross-Document Summarization by Concept Clas-
sification. Proceedings of SIGIR, Tampere, Finland. 
Hardy, H., K. Baker, L. Devillers, L. Lamel, S. Rosset, T. 
Strzalkowski, C. Ursu and N. Webb. 2002b.  Multi-layer Dialogue 
Annotation for Automated Multilingual Customer Service. ISLE 
Workshop, Edinburgh, Scotland. 
Harabagiu, S., et. al. 2002. Answering Complex, List and Context 
questions with LCC?s Question Answering Server.   In Proceedings 
of Text Retrieval Conference (TREC-10). 
Hovy, E., L. Gerber, U. Hermjakob, M. Junk, C-Y. Lin. 2000. Ques-
tion Answering in Webclopedia. Notebook. Proceedings of Text 
Retrieval Conference (TREC-9). 
Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, H. 
Cunningham, Y. Wilks. 1998. Description of the LaSIE-II System 
as Used for MUC-7. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7.) 
Litman, Diane J. and Shimei Pan. 2002. Designing and Evaluating an 
Adaptive Spoken Dialogue System. User Modeling and User-
Adapted Interaction. Vol. 12, No. 2/3, pp. 111-137. 
Seneff, S. and J. Polifroni. 2000. Dialogue Management in the MER-
CURY Flight Reservation System. Proc. ANLP-NAACL 2000, 
Satellite Workshop, pp. 1-6, Seattle, WA. 
Small, Sharon, Nobuyuki Shimizu, Tomek Strzalkowski and Liu Ting 
(2003). HITIQA: A Data Driven Approach to Interactive Question 
Answering: A Preliminary Report. AAAI Spring Symposium on 
New Directions in Question Answering, Stanford University, 
March 24-26, 2003. pp. 94?104. 
Tang, Rong, K.B. Ng, Tomek Strzalkowski and Paul Kantor (2003). 
Automatic Prediction of Information Quality in News Documents. 
Proceedings of HLT-NAACL 2003, Edmonton, May 27-June 1 
Walker, Marilyn A. 2002. An Application of Reinforcement Learning 
to Dialogue Strategy Selection in a Spoken Dialogue System for 
Email . Journal of AI Research, vol 12., pp. 387-416. 
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 236?240, New York City, June 2006. c?2006 Association for Computational Linguistics
Maximum Spanning Tree Algorithm for Non-projective Labeled
Dependency Parsing
Nobuyuki Shimizu
Dept. of Computer Science
State University of New York at Albany
Albany, NY, 12222, USA
shimizu@cs.albany.edu
Abstract
Following (McDonald et al, 2005), we
present an application of a maximum
spanning tree algorithm for a directed
graph to non-projective labeled depen-
dency parsing. Using a variant of the
voted perceptron (Collins, 2002; Collins
and Roark, 2004; Crammer and Singer,
2003), we discriminatively trained our
parser in an on-line fashion. After just one
epoch of training, we were generally able
to attain average results in the CoNLL
2006 Shared Task.
1 Introduction
Recently, we have seen dependency parsing grow
more popular. It is not rare to see dependency re-
lations used as features, in tasks such as relation ex-
traction (Bunescu and Mooney, 2005) and machine
translation (Ding and Palmer, 2005). Although En-
glish dependency relations are mostly projective, in
other languages with more flexible word order, such
as Czech, non-projective dependencies are more fre-
quent. There are generally two methods for learn-
ing non-projective dependencies. You could map a
non-projective dependency tree to a projective one,
learn and predict the tree, then bring it back to the
non-projective dependency tree (Nivre and Nilsson,
2005). Non-projective dependency parsing can also
be represented as search for a maximum spanning
tree in a directed graph, and this technique has been
shown to perform well in Czech (McDonald et al,
2005). In this paper, we investigate the effective-
ness of (McDonald et al, 2005) in the various lan-
guages given by the CoNLL 2006 shared task for
non-projective labeled dependency parsing.
The paper is structured as follows: in section 2
and 3, we review the decoding and learning aspects
of (McDonald et al, 2005), and in section 4, we de-
scribe the extension of the algorithm and the features
needed for the CoNLL 2006 shared task.
2 Non-Projective Dependency Parsing
2.1 Dependency Structure
Let us define x to be a generic sequence of input to-
kens together with their POS tags and other morpho-
logical features, and y to be a generic dependency
structure, that is, a set of edges for x. We use the
terminology in (Taskar et al, 2004) for a generic
structured output prediction, and define a part.
A part represents an edge together with its label.
A part is a tuple ?DEPREL, i, j? where i is the start
point of the edge, j is the end point, and DEPREL is
the label of the edge. The token at i is the head of
the token at j.
Table 1 shows our formulation of building a non-
projective dependency tree as a prediction problem.
The task is to predict y, the set of parts (column 3,
Table 1), given x, the input tokens and their features
(column 1 and 2, Table 1).
In this paper we use the common method of fac-
toring the score of the dependency structure as the
sum of the scores of all the parts.
A dependency structure is characterized by its
features, and for each feature, we have a correspond-
236
Token POS Edge Part
John NN ?SUBJ, 2, 1?
saw VBD ?PRED, 0, 2?
a DT ?DET, 4, 3?
dog NN ?OBJ, 2, 4?
yesterday RB ?ADJU, 2, 5?
which WDT ?MODWH, 7, 6?
was VBD ?MODPRED, 4, 7?
a DT ?DET, 10, 8?
Yorkshire NN ?MODN, 10, 9?
Terrier NN ?OBJ, 7, 10?
. . ?., 10, 11?
Table 1: Example Parts
ing weight. The score of a dependency structure
is the sum of these weights. Now, the dependency
structures are factored by the parts, so that each fea-
ture is some type of a specialization of a part. Each
part in a dependency structure maps to several fea-
tures. If we sum up the weights for these features,
we have the score for the part, and if we sum up the
scores of the parts, we have the score for the depen-
dency structure.
For example, let us say we would like to find the
score of the part ?OBJ, 2, 4?. This is the edge going
to the 4th token ?dog? in Table 1. Suppose there are
two features for this part.
? There is an edge labeled with ?OBJ? that points
to the right. ( = DEPREL, dir(i, j) )
? There is an edge labeled with ?OBJ? starting at
the token ?saw? which points to the right. ( =
DEPREL, dir(i, j), wordi )
If a statement is never true during the training, the
weight for it will be 0. Otherwise there will be a
positive weight value. The score will be the sum of
all the weights of the features given by the part.
In the upcoming section, we explain a decoding
algorithm for the dependency structures, and later
we give a method for learning the weight vector used
in the decoding.
2.2 Maximum Spanning Tree Algorithm
As in (McDonald et al, 2005), the decoding algo-
rithm we used is the Chu-Liu-Edmonds (CLE) al-
gorithm (Chu and Liu, 1965; Edmonds, 1967) for
finding the Maximum Spanning Tree in a directed
graph. The following is a nice summary by (Mc-
Donald et al, 2005).
Informally, the algorithm has each vertex
in the graph greedily select the incoming
edge with highest weight.
Note that the edge is coming from the parent to the
child. This means that given a child node wordj , we
are finding the parent, or the head wordi such that
the edge (i, j) has the highest weight among all i,
i 6= j.
If a tree results, then this must be the max-
imum spanning tree. If not, there must be
a cycle. The procedure identifies a cycle
and contracts it into a single vertex and
recalculates edge weights going into and
out of the cycle. It can be shown that a
maximum spanning tree on the contracted
graph is equivalent to a maximum span-
ning tree in the original graph (Leonidas,
2003). Hence the algorithm can recur-
sively call itself on the new graph.
3 Online Learning
Again following (McDonald et al, 2005), we have
used the single best MIRA (Crammer and Singer,
2003), which is a variant of the voted perceptron
(Collins, 2002; Collins and Roark, 2004) for struc-
tured prediction. In short, the update is executed
when the decoder fails to predict the correct parse,
and we compare the correct parse yt and the incor-
rect parse y? suggested by the decoding algorithm.
The weights of the features in y? will be lowered, and
the weights of the features in yt will be increased ac-
cordingly.
4 Experiments
Our experiments were conducted on CoNLL-X
shared task, with various datasets (Hajic? et al, 2004;
Simov et al, 2005; Simov and Osenova, 2003; Chen
et al, 2003; Bo?hmova? et al, 2003; Kromann, 2003;
van der Beek et al, 2002; Brants et al, 2002;
Kawata and Bartels, 2000; Afonso et al, 2002;
Dz?eroski et al, 2006; Civit Torruella and Mart?? An-
ton??n, 2002; Nilsson et al, 2005; Oflazer et al,
2003; Atalay et al, 2003) .
4.1 Dependency Relation
The CLE algorithm works on a directed graph with
unlabeled edges. Since the CoNLL-X shared task
237
Given a part ?DEPREL, i, j?
DEPREL, dir(i, j)
DEPREL, dir(i, j), wordi
DEPREL, dir(i, j), posi
DEPREL, dir(i, j), wordj
DEPREL, dir(i, j), posj
DEPREL, dir(i, j), wordi, posi
DEPREL, dir(i, j), wordj , posj
DEPREL, dir(i, j), wordi?1
DEPREL, dir(i, j), posi?1
DEPREL, dir(i, j), wordi?1, posi?1
DEPREL, dir(i, j), wordj?1
DEPREL, dir(i, j), posj?1
DEPREL, dir(i, j), wordj?1, posj?1
DEPREL, dir(i, j), wordi+1
DEPREL, dir(i, j), posi+1
DEPREL, dir(i, j), wordi+1, posi+1
DEPREL, dir(i, j), wordj+1
DEPREL, dir(i, j), posj+1
DEPREL, dir(i, j), wordj+1, posj+1
DEPREL, dir(i, j), posi?2
DEPREL, dir(i, j), posi+2
DEPREL, dir(i, j), distance = |j ? i|
additional features
DEPREL, dir(i, j), wordi, wordj
DEPREL, dir(i, j), posi+1, posi, posi+1
DEPREL, dir(i, j), posi+1, wordi, posi+1
DEPREL, dir(i, j), wordi, posi, posj
DEPREL, dir(i, j), posi, wordj , posj
Table 2: Binary Features for Each Part
requires the labeling of edges, as a preprocessing
stage, we created a directed complete graph with-
out multi-edges, that is, given two distinct nodes i
and j, exactly two edges exist between them, one
from i to j, and the other from j to i. There is no
self-pointing edge. Then we labeled each edge with
the highest scoring dependency relation. This com-
plete graph was given to the CLE algorithm and the
edge labels were never altered in the course of find-
ing the maximum spanning tree. The result is the
non-projective dependency tree with labeled edges.
4.2 Features
The features we used to score each part (edge)
?DEPREL, i, j? are shown in Table 2. The index i
is the position of the parent and j is that of the child.
wordj = the word token at the position j.
posj = the coarse part-of-speech at j.
dir(i, j) = R if i < j, and L otherwise.
No other features were used beyond the combina-
tions of the CPOS tag and the word token in Table 2.
We have evaluated our parser on Arabic, Danish,
Slovene, Spanish, Turkish and Swedish, and used
the ?additional features? listed in Table 2 for all lan-
guages except for Danish and Swedish. The reason
for this is simply that the model with the additional
features did not fit in the 4 GB of memory used in
the training.
Although we could do batch learning by running
the online algorithm multiple times, we run the on-
line algorithm just once. The hardware used is an
Intel Pentinum D at 3.0 Ghz with 4 GB of memory,
and the software was written in C++. The training
time required was Arabic 204 min, Slovene 87 min,
Spanish 413 min, Swedish 1192 min, Turkish 410
min, Danish 381 min.
5 Results
The results are shown in Table 3. Although our fea-
ture set is very simple, the results were around the
averages. We will do error analysis of three notable
languages: Arabic, Swedish and Turkish.
5.1 Arabic
Of 4990 words in the test set, 800 are prepositions.
The prepositions are the most frequently found to-
kens after nouns in this set. On the other hand,
our head attachment error was 44% for prepositions.
Given the relatively large number of prepositions
found in the test set, it is important to get the prepo-
sition attachment right to achieve a higher mark in
this language. The obvious solution is to have a fea-
ture that connects the head of a preposition to the
child of the preposition. However, such a feature
effects the edge based factoring and the decoding al-
gorithm, and we will be forced to modify the MST
algorithm in some ways.
5.2 Swedish
Due to the memory constraint on the computer, we
did not use the additional features for Swedish and
our feature heavily relied on the CPOS tag. At the
same time, we have noticed that relatively higher
performance of our parser compared to the average
coincides with the bigger tag set for CPOS for this
corpus. This suggests that we should be using more
fine grained POS in other languages.
5.3 Turkish
The difficulty with parsing Turkish stems from the
large unlabeled attachment error rate on the nouns
238
Language LAS AV SD
Arabic 62.83% 59.92% 6.53
Danish 75.81% 78.31% 5.45
Slovene 64.57% 65.61% 6.78
Spanish 73.17% 73.52% 8.41
Swedish 79.49% 76.44% 6.46
Turkish 54.23% 55.95% 7.71
Language UAS AV SD
Arabic 74.27% 73.48% 4.94
Danish 81.72% 84.52% 4.29
Slovene 74.88% 76.53% 4.67
Spanish 77.58% 77.76% 7.81
Swedish 86.62% 84.21% 5.45
Turkish 68.77% 69.35% 5.51
Table 3: Labeled and Unlabeled Attachment Score
(39%). Since the nouns are the most frequently oc-
curring words in the test set (2209 out of 5021 to-
tal), this seems to make Turkish the most challeng-
ing language for any system in the shared task. On
the average, there are 1.8 or so verbs per sentence,
and nouns have a difficult time attaching to the cor-
rect verb or postposition. This, we think, indicates
that there are morphological features or word order-
ing features that we really need in order to disam-
biguate them.
6 Future Work
As well as making use of fine-grained POS tags and
other morphological features, given the error analy-
sis on Arabic, we would like to add features that are
dependent on two or more edges.
6.1 Bottom-Up Non-Projective Parsing
In order to incorporate features which depend on
other edges, we propose Bottom-Up Non-Projective
Parsing. It is often the case that dependency rela-
tions can be ordered by how close one relation is to
the root of dependency tree. For example, the de-
pendency relation between a determiner and a noun
should be decided before that between a preposition
and a noun, and that of a verb and a preposition, and
so on. We can use this information to do bottom-up
parsing.
Suppose all words have a POS tag assigned to
them, and every edge labeled with a dependency re-
lation is attached to a specific POS tag at the end
point. Also assume that there is an ordering of POS
tags such that the edge going to the POS tag needs
be decided before other edges. For example, (1) de-
terminer, (2) noun, (3) preposition, (4) verb would
be one such ordering. We propose the following al-
gorithm:
? Assume we have tokens as nodes in a graph and no edges
are present at first. For example, we have tokens ?I?,
?ate?, ?with?, ?a?, ?spoon?, and no edges between them.
? Take the POS tag that needs to be decided next. Find all
edges that go to each token labeled with this POS tag,
and put them in the graph. For example, if the POS is
noun, put edges from ?ate? to ?I?, from ?ate? to ?spoon?,
from ?with? to ?I?, from ?with? to ?spoon?, from ?I? to
?spoon?, and from ?spoon? to ?I?.
? Run the CLE algorithm on this graph. This selects the
highest incoming edge to each token with the POS tag we
are looking at, and remove cycles if any are present.
? Take the resulting forests and for each edge, bring the in-
formation on the child node to the parent node. For ex-
ample, if this time POS was noun, and there is an edge to
a preposition ?with? from a noun ?spoon?, then ?spoon?
is absorbed by ?with?. Note that since no remaining de-
pendency relation will attach to ?spoon?, we can safely
ignore ?spoon? from now on.
? Go back and repeat until no POS is remaining and we
have a dependency tree. Now in the next round, when
deciding the score of the edge from ?ate? to ?with?, we
can use the all information at the token ?with?, including
?spoon?.
7 Conclusion
We have extended non-projective unlabeled de-
pendency parsing (McDonald et al, 2005) to a
very simple non-projective labeled dependency and
showed that the parser performs reasonably well
with small number of features and just one itera-
tion of training. Based on the analysis of the Ara-
bic parsing results, we have proposed a bottom-
up non-projective labeled dependency parsing algo-
rithm that allows us to use features dependent on
more than one edge, with very little disadvantage
compared to the original algorithm.
References
A. Abeille?, editor. 2003. Treebanks: Building and Us-
ing Parsed Corpora, volume 20 of Text, Speech and
Language Technology. Kluwer Academic Publishers,
Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Flo-
resta sinta?(c)tica?: a treebank for Portuguese. In Proc.
of the Third Intern. Conf. on Language Resources and
Evaluation (LREC), pages 1698?1703.
239
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annota-
tion process in the Turkish treebank. In Proc. of the 4th
Intern. Workshop on Linguistically Interpreteted Cor-
pora (LINC).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
R. Bunescu and R. Mooney. 2005. A shortest path de-
pendency kernel for relation extraction. In Proc. of
the Joint Conf. on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. In Science Sinica, page
14:13961400.
M. Civit Torruella and Ma A. Mart?? Anton??n. 2002. De-
sign principles for a Spanish treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. of the 42rd Annual
Meeting of the ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of Empirical Methods
in Natural Language Processing (EMNLP).
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. In JMLR.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. of the 43rd Annual Meeting of the ACL.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. ?Zabokrtsky, and A. ?Zele. 2006. Towards a Slovene
dependency treebank. In Proc. of the Fifth Intern.
Conf. on Language Resources and Evaluation (LREC).
J. Edmonds. 1967. Optimum branchings. In Journal of
Research of the National Bureau of Standards, page
71B:233240.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese treebank in VERBMOBIL. Verbmobil-
Report 240, Seminar fu?r Sprachwissenschaft, Univer-
sita?t Tu?bingen.
M. T. Kromann. 2003. The Danish dependency treebank
and the underlying linguistic theory. In Proc. of the
Second Workshop on Treebanks and Linguistic Theo-
ries (TLT).
G. Leonidas. 2003. Arborescence optimization problems
solvable by edmonds algorithm. In Theoretical Com-
puter Science, page 301:427 437.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Joint Conf. on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing (HLT/EMNLP).
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from an-
tiquity. In Proc. of the NODALIDA Special Session on
Treebanks.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of the 43rd Annual Meeting of
the ACL.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15.
K. Simov and P. Osenova. 2003. Practical annotation
scheme for an HPSG treebank of Bulgarian. In Proc.
of the 4th Intern. Workshop on Linguistically Inter-
preteted Corpora (LINC), pages 17?24.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov.
2005. Design and implementation of the Bulgarian
HPSG-based treebank. In Journal of Research on Lan-
guage and Computation ? Special Issue, pages 495?
522. Kluwer Academic Publishers.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of
Empirical Methods in Natural Language Processing
(EMNLP).
L. van der Beek, G. Bouma, R. Malouf, and G. van No-
ord. 2002. The Alpino dependency treebank. In Com-
putational Linguistics in the Netherlands (CLIN).
240
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 138?143,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Features for Detecting Hedge Cues
Nobuyuki Shimizu
Information Technology Center
The University of Tokyo
shimizu@r.dl.itc.u-tokyo.ac.jp
Hiroshi Nakagawa
Information Technology Center
The University of Tokyo
n3@dl.itc.u-tokyo.ac.jp
Abstract
We present a sequential labeling approach
to hedge cue detection submitted to the bi-
ological portion of task 1 for the CoNLL-
2010 shared task. Our main approach is
as follows. We make use of partial syntac-
tic information together with features ob-
tained from the unlabeled corpus, and con-
vert the task into one of sequential BIO-
tagging. If a cue is found, a sentence is
classified as uncertain and certain other-
wise. To examine a large number of fea-
ture combinations, we employ a genetic al-
gorithm. While some features obtained by
this method are difficult to interpret, they
were shown to improve the performance of
the final system.
1 Introduction
Research on automatically extracting factual in-
formation from biomedical texts has become pop-
ular in recent years. Since these texts are abundant
with hypotheses postulated by researchers, one
hurdle that an information extraction system must
overcome is to be able to determine whether or not
the information is part of a hypothesis or a factual
statement. Thus, detecting hedge cues that indi-
cate the uncertainty of the statement is an impor-
tant subtask of information extraction (IE). Hedge
cues include words such as ?may?, ?might?, ?ap-
pear?, ?suggest?, ?putative? and ?or?. They also
includes phrases such as ?. . .raising an intriguing
question that. . .? As these expressions are sparsely
scattered throughout the texts, it is not easy to gen-
eralize results of machine learning from a training
set to a test set. Furthermore, simply finding the
expressions listed above does not guarantee that
a sentence contains a hedge. Their function as a
hedge cue depends on the surrounding context.
The primary objective of the CoNLL-2010
shared task (Farkas et al, 2010) is to detect hedge
cues and their scopes as are present in biomedi-
cal texts. In this paper, we focus on the biological
portion of task 1, and present a sequential labeling
approach to hedge cue detection. The following
summarizes the steps we took to achieve this goal.
Similarly to previous work in hedge cue detec-
tion (Morante and Daelemans, 2009), we first con-
vert the task into a sequential labeling task based
on the BIO scheme, where each word in a hedge
cue is labeled as B-CUE, I-CUE, or O, indicating
respectively the labeled word is at the beginning
of a cue, inside of a cue, or outside of a hedge
cue; this is similar to the tagging scheme from
the CoNLL-2001 shared task. We then prepared
features, and fed the training data to a sequential
labeling system, a discriminative Markov model
much like Conditional Random Fields (CRF), with
the difference being that the model parameters are
tuned using Bayes Point Machines (BPM), and
then compared our model against an equivalent
CRF model. To convert the result of sequential
labeling to sentence classification, we simply used
the presence of a hedge cue, i.e. if a cue is found, a
sentence is classified as uncertain and certain oth-
erwise.
To prepare features, we ran the GENIA tag-
ger to add partial syntactic parse and named en-
tity information. We also applied Porter?s stem-
mer (Jones and Willet, 1997) to each word in the
corpus. For each stem, we acquired the distribu-
tion of surrounding words from the unlabeled cor-
pus, and calculated the similarity between these
distributions and the distribution of hedge cues in
the training corpus. Given a stem and its similari-
ties to different hedge cues, we took the maximum
similarity and discretized it. All these features are
passed on to a sequential labeling system. Using
these base features, we then evaluated the effects
of feature combinations by repeatedly training the
system and selecting feature combinations that in-
creased the performance on a heldout set. To au-
138
tomate this process, we employed a genetic algo-
rithm.
The contribution of this paper is two-fold. First,
we describe our system, outlined above, that we
submitted to the CoNLL-2010 shared task in more
detail. Second, we analyze the effects of partic-
ular choices we made when building our system,
especially the feature combinations and learning
methods.
The rest of this paper is organized as follows.
In Section 2, we detail how the task of sequential
labeling is formalized in terms of linear classifi-
cation, and explain the Viterbi algorithm required
for prediction. We next present several algorithms
for optimizing the weight vector in a linear classi-
fier in Section 3. We then detail the complete list
of feature templates we used for the task of hedge
cue detection in Section 4. In order to evaluate the
effects of feature templates, in Section 5, we re-
move each feature template and find that several
feature templates overfit the training set. We fi-
nally conclude with Section 6.
2 Sequential Labeling
We discriminatively train a Markov model us-
ing Bayes Point Machines (BPM). We will first
explain linear classification, and then apply a
Markov assumption to the classification formal-
ism. Then we will move on to BPM. Note that
we assume all features are binary in this and up-
coming sections as it is sufficient for the task at
hand.
In the setting of sequential labeling, given the
input sequence x = (x1, x2, x3, ...xn), a system
is asked to produce the output sequence y =
(y1, y2, y3, ...yn). Considering that y is a class,
sequential labeling is simply a classification with
a very large number of classes. Assuming that the
problem is one of linear classification, we may cre-
ate a binary feature vector ?(x) for an input x and
have a weight vector wy of the same dimension
for each class y. We choose a class y that has the
highest dot product between the input vector and
the weight vector for the class y. For binary classi-
fication, this process is very simple: compare two
dot product values. Learning is therefore reduced
to specifying the weight vectors.
To follow the standard notations in sequential
labeling, let weight vectors wy be stacked into
one large vector w, and let ?(x,y) be a binary
feature vector such that w>?(x,y) is equal to
w>y ?(x). Classification is to choose y such that
y = argmaxy?(w>?(x,y?)).
Unfortunately, a large number of classes created
out of sequences makes the problem intractable,
so the Markov assumption factorizes y into a se-
quence of labels, such that a label yi is affected
only by the label before and after it (yi?1 and yi+1
respectively) in the sequence. Each structure, or
label y is now associated with a set of the parts
parts(y) such that y can be recomposed from the
parts. In the case of sequential labeling, parts con-
sist of states yi and transitions yi ? yi+1 between
neighboring labels. We assume that the feature
vector for an entire structure y decomposes into
a sum over feature vectors for individual parts as
follows: ?(x,y) =?r?parts(y) ?(x, r). Note that
we have overloaded the symbol ? to apply to either
a structure y or its parts r.
The Markov assumption for factoring labels lets
us use the Viterbi algorithm (much like a Hidden
Markov Model) in order to find
y = argmaxy? (w>?(x,y?))
= argmaxy? (
?n
j=1w
>?(x, y?j)
+
?n?1
j=1 w
>?(x, y?j ? y?j+1)).
3 Optimization
We now turn to the optimization of the weight pa-
rameter w. We compare three approaches ? Per-
ceptron, Bayes Point Machines and Conditional
Random Fields, using our c++ library for struc-
tured output prediction 1.
Perceptron is an online update scheme that
leaves the weights unchanged when the predicted
output matches the target, and changes them when
it does not. The update is:
wk := wk ? ?(xi,y) + ?(xi,yi).
Despite its seemingly simple update scheme, per-
ceptron is known for its effectiveness and perfor-
mance (Collins, 2002).
Conditional Random Fields (CRF) is a condi-
tional model
P (y|x) = 1Zx exp(w
>?(x,y))
where w is the weight for each feature and Zx is a
normalization constant for each x.
Zx =
?
y
exp(w>?(x,y))
1Available at http://soplib.sourceforge.net/
139
for structured output prediction. To fit the weight
vector w using the training set {(xi,yi)}ni=1, we
use a standard gradient-descent method to find the
weight vector that maximizes the log likelihood?n
i logP (yi|xi) (Sha and Pereira, 2003). To
avoid overfitting, the log likelihood is often pe-
nalized with a spherical Gaussian weight prior:?n
i logP (yi|xi) ? C||w||2 . We also evaluated thispenalized version, varying the trade-off parameter
C.
Bayes Point Machines (BPM) for structured
prediction (Corston-Oliver et al, 2006) is an en-
semble learning algorithm that attempts to set the
weight w to be the Bayes Point which approxi-
mates to Bayesian inference for linear classifiers.
Assuming a uniform prior distribution over w, we
revise our belief of w after observing the training
data and produce a posterior distribution. We cre-
ate the final wbpm for classification using a poste-
rior distribution as follows:
wbpm = Ep(w|D)[w] =
|V (D)|?
i=1
p(wi|D)wi
where p(w|D) is the posterior distribution of the
weights given the data D and Ep(w|D) is the ex-
pectation taken with respect to this distribution.
V (D) is the version space, which is the set of
weightswi that classify the training data correctly,
and |V (D)| is the size of the version space. In
practice, to explore the version space of weights
consistent with the training data, BPM trains a few
different perceptrons (Collins, 2002) by shuffling
the samples. The approximation of Bayes Point
wbpm is the average of these perceptron weights:
wbpm = Ep(w|D)[w] ?
K?
k=1
1
Kwk.
The pseudocode of the algorithm is shown in Al-
gorithm 3.1. We see that the inner loop is simply
a perceptron algorithm.
4 Features
4.1 Base Features
For each sentence x, we have state features, rep-
resented by a binary vector ?(x, y?j) and transition
features, again a binary vector ?(x, y?j ? y?j+1).
For transition features, we do not utilize lexical-
ized features. Thus, each dimension of ?(x, y?j ?
Algorithm
3.1: BPM(K,T, {(xi,yi)}ni=1)
wbpm := 0;
for k := 1 to K
Randomly shuffle the sequential order of
samples {(xi,yi)}ni=1
wk := 0;
for t := 1 to T # Perceptron iterations
for i := 1 to n # Iterate shuffled samples
y := argmaxy?(w>k ?(xi,y?))if (y 6= yi)
wk := wk ? ?(xi,y) + ?(xi,yi);
wbpm := wbpm + 1Kwk;return (wbpm)
y?j+1) is an indicator function that tests a com-
bination of labels, for example, O?B-CUE, B-
CUE?I-CUE or I-CUE?O.
For state features ?(x, y?j), the indicator func-
tion for each dimension tests a combination of
y?j and lexical features obtained from x =
(x1, x2, x3, ...xn). We now list the base lexical
features that were considered for this experiment.
F 0 a token, which is usually a word. As a part of
preprocessing, words in each input sentence
are tokenized using the GENIA tagger 2. This
tokenization coincides with Penn Treebank
style tokenization 3.
We add a subscript to indicate the position. F 0j is
exactly the input token xj . From xj , we also create
other lexical features such as F 1j , F 2j , F 3j , and so
on.
F 1 the token in lower case, with digits replaced
by the symbol #.
F 2 1 if the letters in the token are all capitalized,
0 otherwise.
F 3 1 if the token contains a digit, 0 otherwise.
F 4 1 if the token contains an uppercase letter, 0
otherwise.
F 5 1 if the token contains a hyphen, 0 otherwise.
2Available at: http:// www-tsujii.is.s.u-tokyo.ac.jp/ GE-
NIA/ tagger/
3A tokenizer is available at: http:// www.cis.upenn.edu/
treebank/ tokenization.html
140
F 6 first letter in the token.
F 7 first two letters in the token.
F 8 first three letters in the token.
F 9 last letter in the token.
F 10 last two letters in the token.
F 11 last three letters in the token.
The features F 0 to F 11 are known to be useful
for POS tagging. We postulated that since most
frequent hedge cues tend not to be nouns, these
features might help identify them.
The following three features are obtained by
running the GENIA tagger.
F 12 a part of speech.
F 13 a CoNLL-2000 style shallow parse. For ex-
ample, B-NP or I-NP indicates that the token
is a part of a base noun phrase, B-VP or I-VP
indicates that it is part of a verb phrase.
F 14 named entity, especially a protein name.
F 15 a word stem by Porter?s stemmer 4. Porter?s
stemmer removes common morphological
and inflectional endings from words in En-
glish. It is often used as part of an informa-
tion retrieval system.
Upon later inspection, it seems that Porter?s
stemmer may be too aggressive in stemming
words. The word putative, for example, after be-
ing processed by the stemmer, becomes simply put
(which is clearly erroneous).
The last nine types of features utilize the unla-
beled corpus for the biological portion of shared
task 1, provided by the shared task organizers.
For each stem, we acquire a histogram of sur-
rounding words, with a window size of 3, from
the unlabeled corpus. Each histogram is repre-
sented as a vector; the similarity between his-
tograms was then computed. The similarity met-
ric we used is called the Tanimoto coefficient, also
called extended/vector-based Jaccard coefficient.
vi ? vj
||vi|| + ||vj || ? vi ? vj
It is based on the dot product of two vectors and
reduces to Jaccard coefficient for binary features.
4Available at: http://tartarus.org/ martin/PorterStemmer/
This metric is known to perform quite well for
near-synonym discovery (Hagiwara et al, 2008).
Given a stem and its similarities to different hedge
cues, we took the maximum similarity and dis-
cretized it.
F 16 1 if similarity is bigger than 0.9, 0 otherwise.
...
F 19 1 if similarity is bigger than 0.6, 0 otherwise.
...
F 24 1 if similarity is bigger than 0.1, 0 otherwise.
This concludes the base features we considered.
4.2 Combinations of Base Features
In order to discover combinations of base features,
we implemented a genetic algorithm (Goldberg,
1989). It is an adaptive heuristic search algorithm
based on the evolutionary ideas of natural selec-
tion and genetics. After splitting the training set
into three partitions, given the first partition as the
training set, the fitness is measured by the score
of predicting the second partition. We removed
the feature sets that did not score high, and intro-
duced mutations ? new feature sets ? as replace-
ments. After several generations, surviving fea-
ture sets performed quite well. To avoid over fit-
ting, occasionally feature sets were evaluated on
the third partition, and we finally chose the feature
set according to this partition.
The features of the submitted system are listed
in Table 1. Note that Table 1 shows the dimensions
of the feature vector that evaluate to 1 given x and
y?j . The actual feature vector is created by instan-
tiating all the combinations in the table using the
training set.
Surprisingly, our genetic algorithm removed
features F 10 and F 11, the last two/three let-
ters in a token. It also removed the POS in-
formation F 12, but kept the sequence of POS
tags F 12j?1, F 12j , F 12j+1, F 12j+2, F 12j+3. The reason for
longer sequences is due to our heuristics for muta-
tions. Occasionally, we allowed the genetic algo-
rithm to insert a longer sequence of feature com-
binations at once. One other notable observation
is that shallow parses and NEs are removed. Be-
tween the various thresholds from F 16 to F 24,
it only kept F 19, discovering 0.6 as a similarity
threshold.
141
State ?(x, y?j)
y?j
y?j , F 0j?2
y?j , F 0j?1
y?j , F 0j
y?j , F 0j , F 19j
y?j , F 0j?1, F 0j , F 0j+1, F 0j+2, F 0j+3, F 0j+4 ?(1)
y?j , F 0j+1
y?j , F 0j+2
y?j , F 1j
y?j , F 2j ?(2)
y?j , F 3j
y?j , F 4j
y?j , F 4j?2, F 4j?1, F 4j , F 4j+1, F 4j+2
y?j , F 5j
y?j , F 5j , F 7j?1
y?j , F 6j
y?j , F 7j
y?j , F 8j
y?j , F 9j?1, F 9j , F 9j+1, F 9j+2, F 9j+3
y?j , F 12j?1, F 12j , F 12j+1, F 12j+2, F 12j+3
y?j , F 15j , F 15j+1, F 15j+2, F 15j+3
y?j , F 19j?2, F 19j?1, F 19j , F 19j+1, F 19j+2
Table 1: Features for Sequential Labeling
5 Experiments
In order to examine the effects of learning parame-
ters, we conducted experiments on the test data af-
ter it was released to the participants of the shared
task.
While BPM has two parameters, K and T , we
fixed T = 5 and varied K, the number of percep-
trons. As increasing the number of perceptrons re-
sults in more thorough exploration of the version
space V (D), we expect that the performance of
the classifier would improve as K increases. Ta-
ble 2 shows how the number of perceptrons affects
the performance.
TP stands for True Positive, FP for False Pos-
itive, and FN for False Negative. The evaluation
metrics were precision P (the number of true pos-
K TP FP FN P (%) R (%) F1 (%)
10 641 80 149 88.90 81.14 84.84
20 644 79 146 89.07 81.52 85.13
30 644 80 146 88.95 81.52 85.07
40 645 81 145 88.84 81.65 85.09
50 645 80 145 88.97 81.65 85.15
Table 2: Effects of K in Bayes Point Machines
itives divided by the total number of elements la-
beled as belonging to the positive class) recall R
(the number of true positives divided by the to-
tal number of elements that actually belong to the
positive class) and their harmonic mean, the F1
score (F1 = 2PR/(P + R)). All figures in this
paper measure hedge cue detection performance at
the sentence classification level, not word/phrase
classification level. From the results, once the
number of perceptrons hits 20, the performance
stabilizes and does not seem to show any improve-
ment.
Next, in order to examine whether or not we
have overfitted to the training/heldout set, we re-
moved each row of Table 1 and reevaluated the
performance of the system. Reevaluation was
conducted on the labeled test set released by the
shared task organizers after our system?s output
had been initially evaluated. Thus, these figures
are comparable to the sentence classification re-
sults reported in Farkas et al (2010).
TP FP FN P (%) R (%) F1 (%)
1 647 79 143 89.12 81.90 85.36
2 647 80 143 89.00 81.90 85.30
1,2 647 81 143 88.87 81.90 85.24
Table 3: Effects of removing features (1) or (2), or
both
Table 3 shows the effect of removing (1), (2),
or both (1) and (2), showing that they overfit the
training data. Removing any other rows in Ta-
ble 1 resulted in decreased classification perfor-
mance. While there are other large combination
features such as ones involving F 4, F 9, F 12, F 15
and F 19, we find that they do help improving the
performance of the classifier. Since these fea-
tures seem unintuitive to the authors, it is likely
that they would not have been found without the
genetic algorithm we employed. Error analysis
shows that inclusion of features involving F 9 af-
fects prediction of ?believe?, ?possible?, ?puta-
tive?, ?assumed?, ?seemed?, ?if?, ?presumably?,
?perhaps?, ?suggestion?, ?suppose? and ?intrigu-
ing?. However, as this feature template is unfolded
into a large number of features, we were unable to
obtain further linguistic insights.
In the following experiments, we used the cur-
rently best performing features, that is, all fea-
tures except (1) in Table 1, and trained the classi-
fiers using the formalism of Perceptron and Con-
ditional Random Fields besides Bayes Point Ma-
142
chines as we have been using. The results in Table
4 shows that BPM performs better than Percep-
tron or Conditional Random Fields. As the train-
ing time for BPM is better than CRF, our choice
of BPM helped us to run the genetic algorithm re-
peatedly as well. After several runs of empirical
tuning and tweaking, the hyper-parameters of the
algorithms were set as follows. Perceptron was
stopped at 40 iterations (T = 40). For BPM, we
fixed T = 5 and K = 20. For Conditional Ran-
dom Fields, we compared the penalized version
with C = 1 and the unpenalized version (C = 0).
The results in Table 4 is that of the unpenalized
version, as it performed better than the penalized
version.
Perceptron
TP FP FN P (%) R (%) F1 (%)
671 128 119 83.98 84.94 84.46
Conditional Random Fields
TP FP FN P (%) R (%) F1 (%)
643 78 147 89.18 81.39 85.11
Bayes Point Machines
TP FP FN P (%) R (%) F1 (%)
647 79 143 89.12 81.90 85.36
Table 4: Performance of different optimization
strategies
6 Conclusion
To tackle the hedge cue detection problem posed
by the CoNLL-2010 shared task, we utilized a
classifier for sequential labeling following previ-
ous work (Morante and Daelemans, 2009). An
essential part of this task is to discover the fea-
tures that allow us to predict unseen hedge expres-
sions. As hedge cue detection is semantic rather
than syntactic in nature, useful features such as
word stems tend to be specific to each word and
hard to generalize. However, by using a genetic al-
gorithm to examine a large number of feature com-
binations, we were able to find many features with
a wide context window of up to 5 words. While
some features are found to overfit, our analysis
shows that a number of these features are success-
fully applied to the test data yielding good general-
ized performance. Furthermore, we compared dif-
ferent optimization schemes for structured output
prediction using our c++ library, freely available
for download and use. We find that Bayes Point
Machines have a good trade-off between perfor-
mance and training speed, justifying our repeated
usage of BPM in the genetic algorithm for feature
selection.
Acknowledgments
The authors would like to thank the reviewers for
their comments. This research was supported by
the Information Technology Center through their
grant to the first author. We would also like to
thank Mr. Ono, Mr. Yonetsuji and Mr. Yamada
for their contributions to the library.
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of Empirical Methods in Natural Language Process-
ing (EMNLP).
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency pars-
ing using bayes point machines. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 160?167, June.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden. ACL.
David E. Goldberg. 1989. Genetic Algorithms
in Search, Optimization, and Machine Learning.
Addison-Wesley Professional.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2008. Context feature selection for distri-
butional similarity. In Proceedings of IJCNLP-08.
Karen Spa?rk Jones and Peter Willet. 1997. Readings
in Information Retrieval. Morgan Kaufmann.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts.
In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 28?36.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceed-
ings of the Human Language Technology Confer-
ence (HLT).
143
