Proceedings of the ACL-HLT 2011 System Demonstrations, pages 38?43,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
An ERP-based Brain-Computer Interface for text entry
using Rapid Serial Visual Presentation and Language Modeling
K.E. Hild?, U. Orhan?, D. Erdogmus?, B. Roark?, B. Oken?, S. Purwar?, H. Nezamfar?, M. Fried-Oken?
?Oregon Health and Science University ?Cognitive Systems Lab, Northeastern University
{hildk,roarkb,oken,friedm}@ohsu.edu {orhan,erdogmus,purwar,nezamfar}@ece.neu.edu
Abstract
Event related potentials (ERP) corresponding
to stimuli in electroencephalography (EEG)
can be used to detect the intent of a per-
son for brain computer interfaces (BCI). This
paradigm is widely used to build letter-by-
letter text input systems using BCI. Neverthe-
less using a BCI-typewriter depending only on
EEG responses will not be sufficiently accu-
rate for single-trial operation in general, and
existing systems utilize many-trial schemes to
achieve accuracy at the cost of speed. Hence
incorporation of a language model based prior
or additional evidence is vital to improve accu-
racy and speed. In this demonstration we will
present a BCI system for typing that integrates
a stochastic language model with ERP classifi-
cation to achieve speedups, via the rapid serial
visual presentation (RSVP) paradigm.
1 Introduction
There exist a considerable number of people with se-
vere motor and speech disabilities. Brain computer
interfaces (BCI) are a potential technology to create
a novel communication environment for this popula-
tion, especially persons with completely paralyzed
voluntary muscles (Wolpaw, 2007; Pfurtscheller et
al., 2000). One possible application of BCI is typ-
ing systems; specifically, those BCI systems that
use electroencephalography (EEG) have been in-
creasingly studied in the recent decades to enable
the selection of letters for expressive language gen-
eration (Wolpaw, 2007; Pfurtscheller et al, 2000;
Treder and Blankertz, 2010). However, the use of
noninvasive techniques for letter-by-letter systems
lacks efficiency due to low signal to noise ratio and
variability of background brain activity. Therefore
current BCI-spellers suffer from low symbol rates
and researchers have turned to various hierarchi-
cal symbol trees to achieve system speedups (Serby
et al, 2005; Wolpaw et al, 2002; Treder and
Blankertz, 2010). Slow throughput greatly dimin-
ishes the practical usability of such systems. In-
corporation of a language model, which predicts
the next letter using the previous letters, into the
decision-making process can greatly affect the per-
formance of these systems by improving the accu-
racy and speed.
As opposed to the matrix layout of the popu-
lar P300-Speller (Wolpaw, 2007), shown in Fig-
ure 1, or the hexagonal two-level hierarchy of the
Berlin BCI (Treder and Blankertz, 2010), we uti-
lize another well-established paradigm: rapid se-
rial visual presentation (RSVP), shown in Figure
2. This paradigm relies on presenting one stimu-
lus at a time at the focal point of the screen. The
sequence of stimuli are presented at relatively high
speeds, each subsequent stimulus replacing the pre-
vious one, while the subject tries to perform men-
tal target matching between the intended symbol and
the presented stimuli. EEG responses corresponding
to the visual stimuli are classified using regularized
discriminant analysis (RDA) applied to stimulus-
locked temporal features from multiple channels.
The RSVP interface is of particular utility for the
most impaired users, including those suffering from
locked-in syndrome (LIS). Locked-in syndrome can
result from traumatic brain injury, such as a brain-
stem stroke1, or from neurodegenerative diseases
such as amyotrophic lateral sclerosis (ALS or Lou
Gehrig?s disease). The condition is characterized by
near total paralysis, though the individuals are cog-
nitively intact. While vision is retained, the motor
control impairments extend to eye movements. Of-
ten the only reliable movement that can be made by
1Brain stem stroke was the cause of LIS for Jean-Dominique
Bauby, who dictated his memoir The Diving Bell and the But-
terfly via eyeblinks (Bauby, 1997).
M
G
 
 A FEC
_9765
3 4Y 1Z
XWUTS
RQON
H
B
LKI
V
8
P
J
2
D
Figure 1: Spelling grid such as that used for the P300
speller (Farwell and Donchin, 1988). ? ? denotes space.
38
Figure 2: RSVP scanning interface.
an individual is a particular muscle twitch or single
eye blink, if that. Such users have lost the voluntary
motor control sufficient for such an interface. Rely-
ing on extensive visual scanning or complex gestu-
ral feedback from the user renders a typing interface
difficult or impossible to use for the most impaired
users. Simpler interactions via brain-computer in-
terfaces (BCI) hold much promise for effective text
communication for these most impaired users. Yet
these simple interfaces have yet to take full advan-
tage of language models to ease or speed typing.
In this demonstration, we will present a language-
model enabled interface that is appropriate for the
most impaired users.
In addition, the RSVP paradigm provides some
useful interface flexibility relative to the grid-based
paradigm. First, it allows for auditory rather than
visual scanning, for use by the visually impaired
or when visual access is inconvenient, such as in
face-to-face communication. Auditory scanning is
less straightforward when using a grid. Second,
multi-character substrings can be scanned in RSVP,
whereas the kind of dynamic re-organization of a
grid that would be required to support this can be
very confusing. Finally, language model integration
with RSVP is relatively straightforward, as we shall
demonstrate. See Roark et al (2010) for methods
integrating language modeling into grid scanning.
2 RSVP based BCI and ERP Classification
RSVP is an experimental psychophysics technique
in which visual stimulus sequences are displayed
on a screen over time on a fixed focal area and
in rapid succession. The Matrix-P300-Speller used
by Wadsworth and Graz groups (especially g.tec,
Austria) opts for a spatially distributed presentation
of possible symbols, highlighting them in different
orders and combinations to elicit P300 responses.
Berlin BCI?s recent variation utilizes a 2-layer tree
structure where the subject chooses among six units
(symbols or sets of these) where the options are laid
out on the screen while the subject focuses on a cen-
tral focal area that uses an RSVP-like paradigm to
elicit P300 responses. Full screen awareness is re-
quired. In contrast, our approach is to distribute
the stimuli temporally and present one symbol at a
time using RSVP and seek a binary response to find
the desired letter, as shown in Figure 2. The latter
method has the advantage of not requiring the user
to look at different areas of the screen, which can be
an important factor for those with LIS.
Our RSVP paradigm utilizes stimulus sequences
consisting of the 26 letters in the English alphabet
plus symbols for space and backspace, presented in
a randomly ordered sequence. When the user sees
the target symbol, the brain generates an evoked re-
sponse potential (ERP) in the EEG; the most promi-
nent component of this ERP is the P300 wave, which
is a positive deflection in the scalp voltage primar-
ily in frontal areas and that generally occurs with a
latency of approximately 300 ms. This natural nov-
elty response of the brain, occurring when the user
detects a rare, sought-after target, allows us to make
binary decisions about the user?s intent.
The intent detection problem becomes a signal
classification problem when the EEG signals are
windowed in a stimulus-time-locked manner start-
ing at stimulus onset and extending for a sufficient
duration ? in this case 500ms. Consider Figure
3, which shows the trial-averaged temporal signals
from various EEG channels corresponding to tar-
get and non-target (distractor) symbols. This graph
shows a clear effect between 300 and 500 ms for the
target symbols that is not present for the distractor
symbols (the latter of which clearly shows a com-
ponent having a periodicity of 400 ms, which is ex-
pected in this case since a new image was presented
every 400 ms). Figure 4, on the other hand, shows
the magnitude of the trial and distractor responses at
channel Cz on a single-trial basis, rather than aver-
aged over all trials. The signals acquired from each
EEG channel are incorporated and classified to de-
termine the class label: ERP or non-ERP.
Our system functions as follows. First, each chan-
nel is band-pass filtered. Second, each channel is
temporally-windowed. Third, a linear dimension
reduction (using principal components analysis) is
learned using training data and is subsequently ap-
plied to the EEG data when the system is being
used. Fourth, the data vectors obtained for each
channel and a given stimulus are concatenated to
create the data matrix corresponding to the speci-
fied stimulus. Fifth, Regularized Discriminant Anal-
ysis (RDA) (Friedman, 1989), which estimates con-
ditional probability densities for each class using
39
Figure 3: Trial-averaged EEG data corresponding to the target
response (top) and distractor response (bottom) for a 1 second
window.
Kernel Density Estimation (KDE), is used to deter-
mine a purely EEG-based classification discriminant
score for each stimulus. Sixth, the conditional prob-
ability of each letter given the typed history is ob-
tained from the language model. Seventh, Bayesian
fusion (which assumes the EEG-based information
and the language model information are statistically
independent given the class label) is used to combine
the RDA discriminant score and the language model
score to generate an overall score, from which we
infer whether or not a given stimulus represents an
intended (target) letter.
RDA is a modified quadratic discriminant anal-
ysis (QDA) model. Assuming each class has a
multivariate normal distribution and assuming clas-
sification is made according to the comparison of
posterior distributions of the classes, the optimal
Bayes classifier resides within the QDA model fam-
ily. QDA depends on the inverse of the class co-
variance matrices, which are to be estimated from
training data. Hence, for small sample sizes and
high-dimensional data, singularities of these matri-
ces are problematic. RDA applies regularization and
shrinkage procedures to the class covariance matrix
Figure 4: Single-trial EEG data at channel Cz corresponding
to the target response (top) and distractor response (bottom) for
a 1 second window.
estimates in an attempt to minimize problems asso-
ciated with singularities. The shrinkage procedure
makes the class covariances closer to the overall data
covariance, and therefore to each other, thus mak-
ing the quadratic boundary more similar to a linear
boundary. Shrinkage is applied as
??c(?) = (1? ?)??c + ???, (1)
where ? is the shrinkage parameter, ??c is the class
covariance matrix estimated for class c ? {0, 1},
c = 0 corresponds to the non-target class, c = 1 cor-
responds to the target class, and ?? is the weighted
average of class covariance matrices. Regularization
is administered as
??c(?, ?) = (1? ?)??c(?) +
?
d
tr[??c(?)]I, (2)
where ? is the regularization parameter, tr[?] is the
trace function, and d is the dimension of the data
vector.
After carrying out the regularization and shrink-
age on the estimated covariance matrices, the
Bayesian classification rule (Duda et al, 2001) is
applied by comparing the log-likelihood ratio (using
40
Figure 5: Timing of stimulus sequence presentation
the posterior probability distributions) with a confi-
dence threshold. The confidence threshold can be
chosen so that the system incorporates the relative
risks or costs of making an error for each class. The
corresponding log-likelihood ratio is given by
?RDA(x) = log
fN (x; ??1, ??1(?, ?))p?i1
fN (x; ??0, ??0(?, ?))p?i0
, (3)
where ?c and p?ic are the estimates of the class means
and priors, respectively, x is the data vector to be
classified, and fN (x;?,?) is the pdf of a multivari-
ate normal distribution.
The set of visual stimuli (letters plus two ex-
tra symbols, in our case) can be shown multiple
times to achieve a higher classification accuracy for
the EEG-based classifier. The information obtained
from showing the visual stimuli multiple times can
easily be combined by assuming the trials are sta-
tistically independent, as is commonly assumed in
EEG-based spellers2. Figure 5 presents a diagram of
the timing of the presentation of stimuli. We define
a sequence to be a randomly-ordered set of all the
letters (and the space and backspace symbols). The
letters are randomly ordered for each sequence be-
cause the magnitude of the ERP, hence the quality of
the EEG-based classification, is commonly thought
to depend on how surprised the user is to find the
intended letter. Our system also has a user-defined
parameter by which we are able to limit the max-
imum number of sequences shown to the user be-
fore our system makes a decision on the (single) in-
tended letter. Thus we are able to operate in single-
trial or multi-trial mode. We use the term epoch to
denote all the sequences that are used by our sys-
tem to make a decision on a single, intended let-
2The typical number of repetitions of visual stimuli is on the
order of 8 or 16, although g.tec claims one subject is able to
achieve reliable operation with 2 trials (verbal communication).
ter. As can be seen in the timing diagram shown
in Figure 5, epoch k contains between 1 and Mk
sequences. This figure shows the onset of each se-
quence, each fixation image (which is shown at the
beginning of each sequence), and each letter using
narrow pulses. After each sequence is shown, the
cumulative (overall) score for all letters is computed.
The cumulative scores are non-negative and sum to
one (summing over the 28 symbols). If the num-
ber of sequences shown is less than the user-defined
limit and if the maximum cumulative score is less
than 0.9, then another randomly-ordered sequence is
shown to the user. Likewise, if either the maximum
number of sequences has already been shown or if
the maximum cumulative score equals or exceeds
0.9, then the associated symbol (for all symbols ex-
cept the backspace) is added to the end of the list
of previously-detected symbols, the user is able to
take a break of indefinite length, and then the system
continues with the next epoch. If the symbol hav-
ing the maximum cumulative score is the backspace
symbol, then the last item in the list of previously-
detected symbols is removed and, like before, the
user can take a break and then the system continues
with the next epoch.
3 Language Modeling
Language modeling is important for many text pro-
cessing applications, e.g., speech recognition or ma-
chine translation, as well as for the kind of typ-
ing application being investigated here (Roark et al,
2010). Typically, the prefix string (what has al-
ready been typed) is used to predict the next sym-
bol(s) to be typed. The next letters to be typed be-
come highly predictable in certain contexts, partic-
ularly word-internally. In applications where text
generation/typing speed is very slow, the impact
of language modeling can become much more sig-
nificant. BCI-spellers, including the RSVP Key-
board paradigm presented here, can be extremely
low-speed, letter-by-letter writing systems, and thus
can greatly benefit from the incorporation of proba-
bilistic letter predictions from an accurate language
model.
For the current study, all language models were
estimated from a one million sentence (210M char-
acter) sample of the NY Times portion of the English
Gigaword corpus. Models were character n-grams,
estimated via relative frequency estimation. Corpus
normalization and smoothing methods were as de-
scribed in Roark et al (2010). Most importantly for
41
Figure 6: Block diagram of system architecture.
this work, the corpus was case normalized, and we
used Witten-Bell smoothing for regularization.
4 System Architecture
Figure 6 shows a block diagram of our system. We
use a Quad-core, 2.53 GHz laptop, with system code
written in Labview, Matlab, and C. We also use
the Psychophysics Toolbox3 to preload the images
into the video card and to display the images at
precisely-defined temporal intervals. The type UB
g.USBamp EEG-signal amplifier, which is manufac-
tured by g.tec (Austria), has 24 bits of precision and
has 16 channels. We use a Butterworth bandpass fil-
ter of 0.5 to 60 Hz, a 60 Hz notch filter, a sampling
rate of 256 Hz, and we buffer the EEG data until we
have 8 samples of 16-channel EEG data, at which
point the data are transmitted to the laptop. We
use either g.BUTTERfly or g.LADYbird active elec-
trodes, a g.GAMMA cap, and the g.GAMMAsys ac-
tive electrode system.
The output of the amplifier is fed to the laptop via
a USB connection with a delay that is both highly
variable and unknown a priori. Consequently, we
are unable to rely on the laptop system clock in or-
der to synchronize the EEG data and the onset of
the visual stimuli. Instead, synchronization between
the EEG data and the visual stimuli is provided by
sending a parallel port trigger, via an express card-
to-parallel port adaptor, to one of the digital inputs
of the amplifier, which is then digitized along with
the EEG data. The parallel port to g.tec cable was
custom-built by Cortech Solutions, Inc. (Wilming-
ton, North Carolina, USA). The parallel port trigger
is sent immediately after the laptop monitor sends
the vertical retrace signal. The mean and the stan-
3http://psychtoolbox.org/wikka.php?wakka=HomePage
dard deviation of the delay needed to trigger the par-
allel port has been measured to be on the order of
tens of microseconds, which should be sufficiently
small for our purposes.
5 Results
Here we report data collected from 2 subjects, one
of whom is a LIS subject with very limited experi-
ence using our BCI system, and the other a healthy
subject with extensive experience using our BCI sys-
tem. The symbol duration was set to 400 ms, the
duty cycle was set to 50%, and the maximum num-
ber of sequences per trial was set to 6. Before test-
ing, the classifier of our system was trained on data
obtained as each subject viewed 50 symbols with 3
sequences per epoch (the classifier was trained once
for the LIS subject and once for the healthy sub-
ject). The healthy subject was specifically instructed
to neither move nor blink their eyes, to the extent
possible, while the symbols are being flashed on the
screen in front of them. Instead, they were to wait
until the rest period, which occurs after each epoch,
to move or to blink. The subjects were free to pro-
duce whatever text they wished. The only require-
ment given to them concerning the chosen text was
that they must not, at any point in the experiment,
change what they are planning to type and they must
correct all mistakes using the backspace symbol.
Figure 7 shows the results for the non-expert,
LIS subject. A total of 10 symbols were correctly
typed by this subject, who had chosen to spell,
?THE STEELERS ARE GOING TO ...?. Notice
that the number of sequences shown exceeds the
maximum value of 6 for 3 of the symbols. This
occurs when the specified letter is mistyped one or
more times. For example, for each mistyped non-
backspace symbol, a backspace is required to delete
42
T H E _ S T E E L E0 
5 
10
15
20
25
30
35
40
45
No. 
of se
quen
ces 
to re
ach 
conf
iden
ce th
resh
old
Mean = 144/10 = 14.4 (seq/desired symbol)Mean = 5.1 (seq/symbol)                  
Figure 7: Number of sequences to reach the confidence thresh-
old for the non-expert, LIS subject.
T H E _ L A K E R S _ A R E _ I N _ F I0 
5 
10
15
20
25
30
35
40
45
No. 
of se
quen
ces 
to re
ach 
conf
iden
ce th
resh
old
Mean = 28/20 = 1.4 (seq/desired symbol)Mean = 1.4 (seq/symbol)                
Figure 8: Number of sequences to reach the confidence thresh-
old for the expert, healthy subject.
the incorrect symbol. Likewise, if a backspace sym-
bol is detected although it was not the symbol that
the subject wished to type, then the correct symbol
must be retyped. As shown in the figure, the mean
number of sequences for each correctly-typed sym-
bol is 14.4 and the mean number of sequences per
symbol is 5.1 (the latter of which has a maximum
value of 6 in this case).
Figure 8 shows the result for the expert, healthy
subject. A total of 20 symbols were cor-
rectly typed by this subject, who had chosen to
spell, ?THE LAKERS ARE IN FIRST PLACE?.
The mean number of sequences for each correctly-
typed symbol for this subject is 1.4 and the mean
number of sequences per symbol is also 1.4. Notice
that in 15 out of 20 epochs the classifier was able to
detect the intended symbol on the first epoch, which
corresponds to a single-trial presentation of the sym-
bols, and no mistakes were made for any of the 20
symbols.
There are two obvious explanations as to why the
healthy subject performed better than the LIS sub-
ject. First, it is possible that the healthy subject was
using a non-neural signal, perhaps an electromyo-
graphic (EMG) signal stemming from an unintended
muscle movement occurring synchronously with the
target onset. Second, it is also possible that the LIS
subject needs more training in order to learn how
to control the system. We believe the second ex-
planation is correct and are currently taking steps
to make sure the LIS subject has additional time to
train on our system in hopes of resolving this ques-
tion quickly.
Acknowledgments
This work is supported by NSF under grants
ECCS0929576, ECCS0934506, IIS0934509,
IIS0914808, BCS1027724 and by NIH under grant
1R01DC009834-01. The opinions presented here
are those of the authors and do not necessarily
reflect the opinions of the funding agencies.
References
J.-D. Bauby. 1997. The Diving Bell and the Butterfly.
Knopf, New York.
R.O. Duda, P.E. Hart, and D.G. Stork. 2001. Pattern
classification. Citeseer.
L.A. Farwell and E. Donchin. 1988. Talking off the
top of your head: toward a mental prosthesis utiliz-
ing event-related brain potentials. Electroenceph Clin.
Neurophysiol., 70:510?523.
J.H. Friedman. 1989. Regularized discriminant analy-
sis. Journal of the American statistical association,
84(405):165?175.
G. Pfurtscheller, C. Neuper, C. Guger, W. Harkam,
H. Ramoser, A. Schlogl, B. Obermaier, and M. Pre-
genzer. 2000. Current trends in Graz brain-computer
interface (BCI) research. IEEE Transactions on Reha-
bilitation Engineering, 8(2):216?219.
B. Roark, J. de Villiers, C. Gibbons, and M. Fried-Oken.
2010. Scanning methods and language modeling for
binary switch typing. In Proceedings of the NAACL
HLT 2010 Workshop on Speech and Language Pro-
cessing for Assistive Technologies, pages 28?36.
H. Serby, E. Yom-Tov, and G.F. Inbar. 2005. An im-
proved P300-based brain-computer interface. Neural
Systems and Rehabilitation Engineering, IEEE Trans-
actions on, 13(1):89?98.
M.S. Treder and B. Blankertz. 2010. (C) overt atten-
tion and visual speller design in an ERP-based brain-
computer interface. Behavioral and Brain Functions,
6(1):28.
J.R. Wolpaw, N. Birbaumer, D.J. McFarland,
G. Pfurtscheller, and T.M. Vaughan. 2002. Brain-
computer interfaces for communication and control.
Clinical neurophysiology, 113(6):767?791.
J.R. Wolpaw. 2007. Brain?computer interfaces as new
brain output pathways. The Journal of Physiology,
579(3):613.
43
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 28?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Scanning methods and language modeling for binary switch typing
Brian Roark?, Jacques de Villiers?, Christopher Gibbons? and Melanie Fried-Oken?
?Center for Spoken Language Understanding ?Child Development & Rehabilitation Center
Oregon Health & Science University
{roark,jacques}@cslu.ogi.edu {gibbons,mfo}@ohsu.edu
Abstract
We present preliminary experiments of a
binary-switch, static-grid typing interface
making use of varying language model contri-
butions. Our motivation is to quantify the de-
gree to which language models can make the
simplest scanning interfaces ? such as show-
ing one symbol at a time rather than a scan-
ning a grid ? competitive in terms of typing
speed. We present a grid scanning method
making use of optimal Huffman binary codes,
and demonstrate the impact of higher order
language models on its performance. We also
investigate the scanning methods of highlight-
ing just one cell in a grid at any given time
or showing one symbol at a time without a
grid, and show that they yield commensurate
performance when using higher order n-gram
models, mainly due to lower error rate and a
lower rate of missed targets.
1 Introduction
Augmentative and Alternative Communication
(AAC) is a well-defined subfield of assistive tech-
nology, focused on methods that assist individuals
for whom conventional spoken or written communi-
cation approaches are difficult or impossible. Those
who cannot make use of standard keyboards for text
entry have a number of alternative text entry meth-
ods that permit typing. One of the most common of
these alternative text entry methods is the use of a
binary switch ? triggered by button-press, eye-blink
or even through event related potentials (ERP) such
as the P300 detected in EEG signals ? that allows
the individual to make a selection based on some
method for scanning through alternatives (Lesher et
al., 1998). Typing speed is a challenge, yet critically
important for usability. One common approach is
row/column scanning on a matrix of characters,
symbols or images (a ?spelling grid?), which allows
the user of a binary yes/no switch to select the row
and column of a target symbol, by simply indicating
?yes? (pressing a button or blinking an eye) when the
row or column of the target symbol is highlighted.
Figure 1 shows the 6?6 spelling grid used for the
P300 Speller (Farwell and Donchin, 1988).
For any given scanning method, the use of a bi-
nary switch to select from among a set of options
(letter, symbols, or images) amounts to the assign-
ment of binary codes to each symbol. For example,
the standard row/column scanning algorithm works
by scanning each row until a selection is made, then
scanning each column until a selection is made, and
returning the symbol at the selected row and column.
This can be formalized as follows:
1 for i = 1 to (# of rows) do
2 HIGHLIGHTROW(i)
3 if YESSWITCH
4 for j = 1 to (# of columns) do
5 HIGHLIGHTCOLUMN(j)
6 if YESSWITCH
7 return (i, j)
8 return (i, 0)
9 return (0, 0)
where the function YESSWITCH returns true if the
button is pressed (or whatever switch event counts as
a ?yes? response) within the parameterized latency.
If the function returns (0, 0) then nothing has been
selected, requiring rescanning. If the function re-
turns (i, 0) for i > 0, then row i has been selected,
but columns must be rescanned. Under this scanning
method, the binary code for the letter ?J? in the ma-
trix in Figure 1 is 010001; the letter ?T? is 000101.
The length of the binary code for a symbol is re-
M
G
 
 
A FEC
_9765
3 4Y 1Z
XWUTS
RQON
H
B
LKI
V
8
P
J
2
D
Figure 1: Spelling grid such as that used for the P300
speller (Farwell and Donchin, 1988). ? ? denotes space.
28
lated to the time required to type it. In the ma-
trix in Figure 1, the space character is in the bot-
tom right-hand corner, yielding the maximum binary
code length for that grid size (12), despite that, in
typical written English we would expect the space
character to be used about 20% of the time. A more
efficient strategy would be to place the space charac-
ter in the upper left-hand corner of the grid, leading
to the much shorter binary code ?11?.
Ordering symbols in a fixed grid so that frequent
symbols are located in the upper left-hand corner is
one method for making use of a statistical model of
the language so that likely symbols receive the short-
est codes. Such a language model, however, does
not take into account what has already been typed,
but rather assigns its code identically in all contexts.
In this paper we examine alternative fixed-grid scan-
ning methods that do take into account context in the
language models used to establish codes, i.e., the
codes in these methods vary in different contexts,
so that high probability symbols receive the short-
est codes and hence require the fewest keystrokes.
We show that n-gram language models can provide
a large improvement in typing speed.
Before presenting our methods and experimental
results, we next provide further background on alter-
native text entry methods, language modeling, and
binary coding based on language models.
2 Preliminaries and background
2.1 Alternative text entry
Of the ways in which AAC typing interfaces differ,
perhaps most relevant to the current paper is whether
the symbol positions are fixed or can move dynam-
ically, because such dynamic layouts facilitate in-
tegration of richer language models. For example,
if we re-calculate character probabilities after each
typed character, then we could re-arrange the char-
acters in the grid so that the most likely are placed
in the upper left-hand corner for row/column scan-
ning. Conventional wisdom, however, is that the
cognitive overhead of processing a different grid ar-
rangement after every character would slow down
typing more than the speedup due to the improved
binary coding (Baletsa et al, 1976; Lesher et al,
1998). The GazeTalk system (Hansen et al, 2003),
which presents the user with a 3?4 grid and captures
which cell the user?s gaze fixates upon, is an instance
of a dynamically changing grid. The cell layouts
are configurable, but typically one cell contains a set
of likely word completions; others are allocated to
space and backspace; and around half of the cells are
allocated to the most likely single character contin-
uation of the input string, based on language model
predictions. Hansen et al (2003) report that users
produced more words per minute with a static key-
board than with the predictive grid interface, illus-
trating the impact of the cognitive overhead that goes
along with this sort of scanning.
The likely word completions in the GazeTalk sys-
tem illustrates another common way in which lan-
guage modeling is integrated into AAC typing sys-
tems. Much of the language modeling research
within the context of AAC has been for word com-
pletion/prediction for keystroke reduction (Darragh
et al, 1990; Li and Hirst, 2005; Trost et al, 2005;
Trnka et al, 2006; Trnka et al, 2007; Wandmacher
and Antoine, 2007). The typical scenario for this is
allocating a region of the interface to contain a set of
suggested words that complete what the user has be-
gun typing. The expectation is to derive a keystroke
savings when the user selects one of the alternatives
rather than typing the rest of the letters. The cogni-
tive load of monitoring a list of possible completions
has made the claim that this speeds typing contro-
versial (Anson et al, 2004); yet some results have
shown this to speed typing under certain conditions
(Trnka et al, 2007).
One innovative language-model-driven AAC typ-
ing interface is Dasher (Ward et al, 2002), which
uses language models and arithmetic coding to
present alternative letter targets on the screen with
size relative to their likelihood given the history.
Users can type by continuous motion, such as eye
gaze or mouse cursor movement, targeting their cur-
sor at the intended letter and moving the cursor
from left-to-right through the interface, while its
movements are tracked. This is an extremely effec-
tive typing interface alternative to keyboards, pro-
vided the user has sufficient motor control to per-
form the required systematic visual scanning. The
most severely impaired users, such as those with
locked-in syndrome (LIS), have lost the voluntary
motor control sufficient for such an interface.
Relying on extensive visual scanning, such as that
required in dynamically reconfiguring spelling grids
or Dasher, or requiring complex gestural feedback
from the user renders a typing interface difficult or
impossible to use for those with the most severe im-
pairments. Indeed, even spelling grids like the P300
speller can be taxing as an interface for users. Re-
cent attempts to use the P300 speller as a typing
interface for locked-in individuals with ALS found
29
1 A? V  initialize A as symbol set V
2 k ? 1  initialize bit position k to 1
3 while |A| > 1 do
4 P ? {a ? A : a[k] = 1}
5 Q? {a ? A : a[k] = 0}
6 Highlight symbols in P
7 if selected then A? P
8 else A? Q
9 k ? k + 1
10 return a ? A  Only 1 element in A
Figure 2: Algorithm for binary code symbol selection
that the number of items in the grid caused prob-
lems for these patients, because of difficulty orient-
ing attention to specific locations in the spelling grid
(Sellers et al, 2003). This is another illustration of
the need to reduce the cognitive overhead of such in-
terfaces. Yet the success of classification of ERP in
a simpler task for this population indicates that the
P300 is a binary response mechanism of utility for
this task (Sellers and Donchin, 2006).
Simpler interactions via brain-computer inter-
faces (BCI) hold much promise for effective text
communication. Yet these simple interfaces have yet
to take full advantage of language models to ease or
speed typing. In this paper we will make use of a
static grid, or a single letter linear scanning inter-
face, yet scan in a way that allows for the use of
contextual language model probabilities when con-
structing the binary code for each symbol.
2.2 Binary codes for typing interfaces
Row/column scanning, as outlined in the previous
section, is not the only means by which the spelling
grid in Figure 1 can be used as a binary response
typing interface. Rather than highlighting full rows
or full columns, arbitrary subsets of letters could be
highlighted, and letter selection again driven by a
binary response mechanism. An algorithm to do this
is as follows. Assign a unique binary code to each
symbol in the symbol set V (letters in this case). For
each symbol a ? V , there are |a| bits in the code
representing the letter. Let a[k] be the kth bit of the
code for symbol a. We will assume that no symbol?s
binary code is a prefix of another symbol?s binary
code. Given such an assignment of binary codes to
the symbol set V , the algorithm in Figure 2 can be
used to select the target symbol in a spelling grid.
One key question in this paper is how to produce
such a binary code, which is how language models
can be included in scanning. Figure 3 shows two
different binary trees, which yield different binary
codes for six letters in a simple, artificial example.
Huffman:
 
1
 
 
011 0
1 0 1
0
0
000feac
b d
001110111
0110
Linear:
1
01
   
c
01d
b1
  
 
fe
a
0000000001
0001
001
1 0
01
01
0
Letter: a b c d e f
Probability: 0.15 0.25 0.18 0.2 0.12 0.1
Huffman bits: 3 2 3 2 3 3
Linear bits: 4 1 3 2 5 5
Figure 3: Two binary trees for encoding letters based on
letter probabilities: (1) Huffman coding; and (2) Linear
coding via a right-branching tree (right-linear). Expected
bits are 2.55 for Huffman and 2.89 for linear coding.
Huffman coding (Huffman, 1952) builds a binary
tree that minimizes the expected number of bits ac-
cording to the provided distribution. There is a lin-
ear complexity algorithm for building this tree given
a list of items sorted by descending probability.
Another type of binary code, which we will call a
linear code, provides a lot of flexibility in the kind of
interface that it allows, relative to the other methods
mentioned above. In this binary code, each itera-
tion of the WHILE loop in the Figure 2 algorithm
would have a set P on line 4 with exactly one mem-
ber. With such a code, the spelling grid in Figure
1 would highlight exactly one letter at a time for
selection. Alternately, symbols could be presented
one at a time with no grid, which we call rapid serial
visual presentation (RSVP, see Fig.7). Linear cod-
ing builds a simple right-linear tree (seen in Figure
3) that preserves the sorted order of the set, putting
higher probability symbols closer to the root of the
tree, thus obtaining shorter binary codes. Linear
coding can never produce codes with fewer expected
bits than Huffman coding, though the linear code
may reach the minimum under certain conditions.
The simplicity of an interface that presents a sin-
gle letter at a time may reduce user fatigue, and even
make typing feasible for users that cannot maintain
focus on a spelling grid. Additionally, single symbol
auditory presentation would be possible, for visually
impaired individuals, something that is not straight-
forwardly feasible with the sets of symbols that must
be presented when using Huffman codes.
2.3 Language modeling for typing interfaces
The current task is very similar to word prediction
work discussed in Section 2.1, except that the pre-
30
diction interface is the only means by which text
is input, rather than a separate window with com-
pletions being provided. In principle, the symbols
that are being predicted (hence typed) can be from
a vocabulary that includes multiple symbol strings
such as words. However, a key requirement in a
composition-based typing interface is an open vo-
cabulary ? the user should be able to type any word,
whether or not it is in some fixed vocabulary. In-
cluded in such a mechanism is the ability to repair:
delete symbols and re-type new ones. In contrast,
a word prediction component must be accompanied
by some additional mechanism in place for typing
words not in the vocabulary. The current problem is
to use symbol prediction for that core typing inter-
face, and this paper will focus on predicting single
ASCII and control characters, rather than multiple
character strings. The task is actually very similar
to the well known Shannon game (Shannon, 1950),
where text is guessed one character at a time.
Character prediction is done in the Dasher and
GazeTalk interfaces, as discussed in an earlier sec-
tion. There is also a letter prediction component to
the Sibyl/Sibylle interfaces (Schadle, 2004; Wand-
macher et al, 2008), alongside a separate word pre-
diction component. Interestingly, the letter predic-
tion component of Sibylle (Sibyletter) involves a lin-
ear scan of the letters, one at a time in order of proba-
bility (as determined by a 5-gram character language
model), rather than a row/column scanning of the
P300 speller. This approach was based on user feed-
back that the row/column scanning was a much more
tiring interface than the linear scan interface (Wand-
macher et al, 2008), which is consistent with the
results previously discussed on the difficulty of ALS
individuals with the P300 speller interface.
Language modeling for a typing interface task of
this sort is very different from other common lan-
guage modeling tasks. This is because, at each sym-
bol in the string, the already typed prefix string is
given ? there is no ambiguity in the prefix string,
modulo subsequent repairs. In contrast, in speech
recognition, machine translation, optical character
recognition or T9 style text input, the actual pre-
fix string is not known; rather, there is a distribu-
tion over possible prefix strings, and a global in-
ference procedure is required to find the best string
as a whole. For typing, once the symbol has been
produced and not repaired, the model predicting the
next symbol is given the true context. This has sev-
eral important ramifications for language modeling,
including the availability of supervised adaptation
data and the fact that the models trained with rel-
ative frequency estimation are both generative and
discriminative. See Roark (2009) for extensive dis-
cussion of these issues. Here we will consider n-
gram language models of various orders, estimated
via smoothed relative frequency estimation (see ?
3.1). The principal novelty in the current approach
is the principled incorporation of error probabilities
into the binary coding approaches, and the experi-
mental demonstration of how linear coding for grids
or RSVP interfaces compare to Huffman coding and
row/column scanning for grids.
3 Methods
3.1 Character-based language models
For this paper, we use character n-gram models.
Carpenter (2005) has an extensive comparison of
large scale character-based language models, and
we adopt smoothing methods from that paper. It
presents a version of Witten-Bell smoothing (Wit-
ten and Bell, 1991) with an optimized hyperparam-
eter K, which is shown to be as effective as Kneser-
Ney smoothing (Kneser and Ney, 1995) for higher
order n-grams. We refer readers to that paper for de-
tails on this standard n-gram language modeling ap-
proach. For the experimental results presented here,
we trained unigram and 8-gram models from the NY
Times portion of the English Gigaword corpus.
We performed extensive normalization of this
corpus, detailed in Roark (2009). We de-cased
the resulting corpus and selected sentences that
only included characters that would appear in
our 6?6 spelling grid. Those characters are:
the 26 letters of the English alphabet, the space
character, a delete symbol, comma, period, double
and single quote, dash, dollar sign, colon and
semi-colon. We used a 42 million character subset
of this corpus for training the model. Finally, we
appended to this corpus approximately 112 thou-
sand words from the CMU Pronouncing Dictionary
(www.speech.cs.cmu.edu/cgi-bin/cmudict),
which also contained only the symbols from the
grid. For hyper-parameter settings, we used a 100k
character development set. Our best performing
hyper-parameter for the Witten-Bell smoothing was
K = 15, which is comparable to optimal settings
found by Carpenter (2005) for 12-grams.
3.2 Binary codes
Given what has been typed so far, we can use a char-
acter n-gram language model to assign probabilities
31
Figure 4: Row/column scanning interface.
to all next symbols in the symbol set V . After sort-
ing the set in order of decreasing probability, we can
use these probabilities to build binary coding trees
for the set. Hence the binary code assigned to each
symbol in the symbol set differs depending on what
has been typed before. For Huffman coding, we
used the algorithm from Perelmouter and Birbaumer
(2000) that accounts for any probability of error in
following a branch of the tree, and builds the optimal
coding tree even when there is non-zero probability
of taking a branch in error. Either linear or Huffman
codes can be built from the language model proba-
bilities, and can then be used for a typing interface,
using the algorithm presented in Figure 2.
3.3 Scanning systems
For these experiments, we developed an interface
for controlled testing of typing performance under
a range of scanning methods. These include: (i)
row/column scanning, both auto scan (button press
selects) and step scan (lack of button press selects);
(ii) Scanning with a Huffman code, either derived
from a unigram language model, or from an 8-gram
language model; and (iii) Scanning with a linear
code, either on the 6?6 grid, or using RSVP, which
shows one symbol at a time. Each trial involved giv-
ing subjects a target phrase with instructions to type
the phrase exactly as displayed. All errors in typing
were required to be corrected by deleting (via?) the
incorrect symbol and re-typing the correct symbol.
Figure 4 shows our typing interface when config-
ured for row/column scanning. At the top of the
application window is the target string to be typed
by the subject (?we run the risk of failure?). Below
that is the buffer displaying what has already been
typed (?we run t?). Spaces between words must also
be typed ? they are represented by the underscore
character in the upper left-hand corner of the grid.
Spaces are treated like any other symbol in our lan-
guage model ? they must be typed, thus they are pre-
Figure 5: Error in row/column scanning interface.
dicted along with the other symbols. Figure 5 shows
how the display updates when an incorrect character
is typed. The errors are highlighted in red, followed
by the backarrow symbol to remind users to delete.
If a row has not been selected after a pass over all
rows, scanning begins again at the top. After row
selection, column scanning commences; if a column
is not selected after three passes from left-to-right
over the columns, then row scanning re-commences
at the following row. Hence, even if a wrong row is
selected, the correct symbol can still be typed.
Note that the spelling grid has been sorted in uni-
gram frequency order, so that the most frequent sym-
bols are in the upper left-hand corner. This same grid
is used in all grid scanning conditions, and provides
language modeling benefit to row/column scanning.
Figure 6 shows our typing interface when config-
ured for what we term Huffman scanning. In this
scanning mode, the highlighted subset is dictated by
the Huffman code, and is not necessarily contiguous.
Not requiring contiguity of highlighted symbols al-
lows the coding to vary with the context, thus allow-
ing use of an n-gram language model. As far as we
know, this is the first time that contiguity of high-
lighting is relaxed in a scanning interface to accom-
modate Huffman coding. Baljko and Tam (2006)
used Huffman coding for a grid scanning interface,
but using a unigram model and the grid layout was
selected to ensure that highlighted regions would al-
ways be contiguous, thus precluding n-grammodels.
In our Huffman scanning approach, when the se-
lected set includes just one character, it is typed. As
with row/column scanning, when the wrong charac-
ter is typed, the backarrow symbol must be chosen
to delete it. If an error is made in selection that does
not result in a typed character ? i.e., if the incorrectly
selected set has more than one member ? then we
need some mechanism for allowing the target sym-
bol to still be selected, much as we have a mecha-
32
Figure 6: Huffman scanning interface.
nism in row/column scanning for recovering if the
wrong row is selected. Section 3.4 details our novel
method for recalculating the binary codes based on
an error rate parameter. At no point in typing is any
character ruled out from being selected.
The grids shown in Figures 4-6 can be straightfor-
wardly used with linear coding as well, by simply
highlighting one cell at a time in descending proba-
bility order. Additionally, linear coding can be used
with an RSVP interface, shown in Figure 7, which
displays one character at a time.
Each interface needs a scan rate, specifying how
long to wait for a button press before advancing. The
scan rate for each condition was set for each individ-
ual during a training/calibration session (see ?4.1).
3.4 Errors in Huffman and Linear scanning
In this section we briefly detail how we account for
the probability of error in scanning with Huffman
and linear codes. The scanning interface takes a pa-
rameter p, which is the probability that, when a se-
lection is made, it is correct. Thus 1?p is the proba-
bility of an error. Recall that if a selection leads to a
single symbol, then that symbol is typed. Otherwise,
if a selection leads to a set with more than one sym-
bol, then all symbol probabilities (even those not in
the selected set) are updated based on the error prob-
ability and scanning continues. If a non-target (in-
correct) symbol is selected, the delete (backarrow)
symbol must be chosen to correct the error, after
which the typing interface returns to the previous
position. Three key questions must be answered in
such an approach: (1) how are symbol probabilities
updated after a keystroke, to reflect the probability
of error? (2) how is the probability of backarrow es-
timated? and (3) when the typing interface returns
to the previous position, where does it pick up the
scanning? Here we answer all three questions.
Consider the Huffman coding tree in Figure 3. If
the left-branch (?1?) is selected by the user, the prob-
ability that it was intended is p versus an error with
Figure 7: RSVP scanning interface.
probability 1?p. If the original probability of a sym-
bol is q, then the updated probability of the symbol
is pq if it starts with a ?1? and (1?p)q if it starts with
a ?0?. After updating the scores and re-normalizing
over the whole set, we can build a new binary cod-
ing tree. The user then selects a branch at the root
of the new tree. A symbol is finally selected when
the user selects a branch leading to a single symbol.
The same approach is used with a linear coding tree.
The probability of requiring the delete (backar-
row) character can be calculated directly from the
probability of keystroke error ? in fact, the probabil-
ity of backarrow is exactly the probability of error
1?p. To understand why this is the case, consider
that a non-target (incorrect) symbol can be chosen
according to the approach in the previous paragraph
only with a final keystroke error. Any keystroke
error that does not select a single symbol does not
eliminate the target symbol, it merely re-adjusts the
target symbol?s probability along with all other sym-
bols. Hence, no matter how many keystrokes have
been made, the probability that a selected symbol
was not the target symbol is simply the probability
that the last keystroke was in error, i.e., 1?p.
Finally, if backarrow is selected, the previous po-
sition is revisited, and the probabilities are reset as
though no prior selection had been made.
4 Empirical results
4.1 Subjects and scan rate calibration
We recruited 10 native English speakers between the
ages of 24 and 48 years, who had not used our typ-
ing interface, are not users of scanning interfaces
for typing, and have typical motor function. Each
subject participated in two sessions, one for training
and calibration of scan rates; and another for testing.
We use the phrase set from MacKenzie and Souko-
reff (2003) to evaluate typing performance. Of the
500 phrases in that set, 20 were randomly set aside
for testing, the other 480 available during training
and calibration phases. Five of the 20 evaluation
33
strings were used in this study. We used an Ablenet
Jellybean R? button as the binary switch. For these
trials, to estimate error rates in modeling, we fixed
p = 0.95, i.e., 5% error rate.
The scan rate for row/column scanning is typi-
cally different than for Huffman or linear scanning,
since row/column scanning methods allow for an-
ticipation: one can tell from the current highlight-
ing whether the desired row or column will be high-
lighted next. For the Huffman and linear scanning
approaches that we are investigating, that is not the
case: any cell can be highlighted (or symbol dis-
played) at any time, even multiple times in a row.
Hence the scan rate for these methods depends more
on reaction time than row/column scanning, where
anticipation allows for faster rates.
The scan rate also differs between the two
row/column scanning approaches (auto scan and
step scan), due to the differences in control needed
to advance scanning with a button press versus se-
lecting with a button press. We thus ran scan rate
calibration under three conditions: row/column step
scan; row/column auto scan; and Huffman scan-
ning, using a unigram language model. The Huff-
man scanning scan rate was then used for all of the
Huffman and linear scanning approaches.
Calibration involved two stages for each of the
three approaches, and the first stage of all three is
completed before running the second stage, thus fa-
miliarizing subjects with all interfaces prior to final
calibration. The first stage of calibration starts with
slow scan rate (1200 ms dwell time), then speeds up
the scan rate by reducing dwell time by 200 ms when
a target string is successfully typed. Success here
means that the string is correctly typed with less than
10% error rate. The subject gets three tries to type a
string successfully at a given scan rate, after which
they are judged to not be able to complete the task
at that rate. In the first stage, this stops the stage for
that method and the dwell time is recorded. In the
second stage, calibration starts at a dwell time 500
ms higher than where the subject failed in the first
stage, and the dwell time decreases by 100 ms in-
crements when target strings are successfully typed.
When subjects cannot complete the task at a dwell
time, the dwell time then increases at 50 ms incre-
ments until they can successfully type a target string.
Table 1 shows the mean (and std) scan rates (dwell
time) for each condition. Step scanning generally
had a slower scan rate than auto scanning, and Huff-
man scanning (unsurprisingly) was slowest.
4.2 Testing stage and results
In the testing stage of the protocol, there were
six conditions: (1) row/column step scan; (2)
row/column auto scan; (3) Huffman scanning with
codes derived from the unigram language model; (4)
Huffman scanning with codes derived from the 8-
gram language model; (5) Linear scanning on the
6?6 spelling grid with codes derived from the 8-
gram language model; and (6) RSVP single letter
presentation with codes derived from the 8-gram
language model. The ordering of the conditions for
each subject was randomized. In each condition, in-
structions were given (identical to instructions dur-
ing calibration phase), and the subjects typed prac-
tice phrases until they successfully reached error rate
criterion performance (10% error rate or lower), at
which point they were given the test phrases to type.
Recall that the task is to type the stimulus phrase
exactly as presented, hence the task is not com-
plete until the phrase has been correctly typed. To
avoid non-termination scenarios ? e.g., the subject
does not recognize that an error has occurred, what
the error is, or simply cannot recover from cascad-
ing errors ? the trial is stopped if the total errors
in typing the target phrase reach 20, and the sub-
ject is presented with the same target phrase to type
again from the beginning, i.e., the example is re-
set. Only 2 subjects in the experiment had a phrase
reset in this way (just one phrase each), both in
row/column scanning conditions. Of course, the
time and keystrokes spent typing prior to reset are
included in the statistics of the condition.
Table 1 shows the mean (and std) of several mea-
sures for the 10 subjects. Speed is reported in char-
acters per minute. Bits per character represents
the number of keypress and non-keypress (timeout)
events that were used to type the symbol. Note that
bits per character does not correlate perfectly with
speed, since a non-keypress bit due to a timeout
takes the full dwell time, while the time for a key-
press event may be less than that full time. For any
given symbol the bits may involve making an error,
followed by deleting the erroneous symbol and re-
typing the correct symbol. Alternately, the subject
may scan pass the target symbol, but still return to
type it correctly, resulting in extra keystrokes, i.e., a
longer binary code than optimal. In addition to the
mean and standard deviation of bits per character,
we present the optimal could be achieved with each
method. Finally we characterize the errors that are
made by subjects by the error rate, which is the num-
34
Scan rate (ms) Speed (cpm) Bits per character Error rate Long code rate
Scanning condition mean (std) mean (std) mean (std) opt. mean (std) mean (std)
row/column step scan 425 (116) 20.7 (3.6) 8.5 (2.6) 4.5 6.3 (5.1) 29.9 (19.0)
auto scan 310 (70) 19.1 (2.2) 8.4 (1.2) 4.5 5.4 (2.8) 33.8 (11.5)
Huffman unigram 475 (68) 12.5 (2.3) 8.4 (1.9) 4.4 4.4 (2.2) 39.2 (13.5)
8-gram 475 (68) 23.4 (3.7) 4.3 (1.1) 2.6 4.1 (2.2) 19.3 (14.2)
Linear grid 8-gram 475 (68) 23.2 (2.1) 4.2 (0.7) 3.4 2.4 (1.5) 5.0 (4.1)
RSVP 8-gram 475 (68) 20.3 (5.1) 6.1 (2.6) 3.4 7.7 (5.4) 5.2 (4.0)
Table 1: Typing results for 10 users on 5 test strings (total 31 words, 145 characters) under six conditions.
ber of incorrect symbols typed divided by the total
symbols typed. The long code rate is the percent-
age of correctly typed symbols for which a longer
than optimal code was used to type the symbol, by
making an erroneous selection that does not result in
typing the wrong symbol.
We also included a short survey, using a Likert
scale for responses, and mean scores are shown in
Table 2 for four questions: 1) I was fatigued by the
end of the trial; 2) I was stressed by the end of the
trial; 3) I liked this trial; and 4) I was frustrated by
this trial. The responses showed a consistent prefer-
ence for Huffman and linear grid conditions with an
8-gram language model over the other conditions.
Survey Row/Column Huffman Linear
Question step auto 1-grm 8-grm grid RSVP
Fatigued 3.2 2.4 3.6 2.0 2.4 2.8
Stressed 2.7 2.4 2.7 1.5 1.8 2.6
Liked it 2.2 3.3 2.3 4.2 3.8 3.2
Frustrated 3.2 1.7 3.1 1.7 1.7 2.3
Table 2: Mean Likert scores to survey questions
(5 = strongly agree; 1 = strongly disagree)
4.3 Discussion of results
While this is a preliminary study of just 10 sub-
jects, several things stand out from the results. First,
comparing the three methods using just unigram fre-
quencies to inform scanning (row/column and Huff-
man unigram), we can see that Huffman unigram
scanning is significantly slower than the other two,
mainly due to a slower scan rate with no real im-
provement in bits per character (real or optimal). All
three methods have a high rate of longer than opti-
mal codes, leading to nearly double the bits per char-
acter that would optimally be required.
Next, with the use of the 8-gram language model
in Huffman scanning, both the optimal bits per char-
acter and the difference between real and optimal are
reduced, leading to nearly double the speed. Inter-
estingly, use of the linear code on the grid leads to
fewer bits per character than Huffman scanning, de-
spite nearly 1 bit increase in optimal bits per charac-
ter, due to a decrease in error rate and a very large
decrease in long code rate. We speculate that this is
because highlighting a single cell at a time draws the
eye to that cell, making visual scanning easier.
Finally, despite using the same model, RSVP is
found to be slightly slower than the Huffman 8-
gram or Linear grid conditions, though commensu-
rate with the row/column scanning, mainly due to an
increase in error rate. Monitoring a single cell, rec-
ognizing symbol identity and pressing the switch is
apparently somewhat harder than finding the symbol
on a grid and waiting for the cell to light up.
5 Summary and future directions
We have presented methods for including language
modeling in simple scanning interfaces for typing,
and evaluated performance of novice subjects with
typical motor control. We found that language mod-
eling can make a very large difference in the us-
ability of the Huffman scanning condition. We also
found that, despite losing bits to optimal Huffman
coding, linear coding leads to commensurate typ-
ing speed versus Huffman coding presumably due
to lower cognitive overhead of scanning and thus
fewer mistakes. Finally, we found that RSVP was
somewhat slower than grid scanning with the same
language model and code.
This research is part of a program to make the
simplest scanning approaches as efficient as possi-
ble, so as to facilitate the use of binary switches for
individuals with the most severe impairments, in-
cluding ERP for locked-in subjects. While our sub-
jects in this study have shown slightly better perfor-
mance using a grid versus RSVP, these individuals
have no problem with visual scanning or fixation
on relatively small cells in the grid. It is encourag-
ing that subjects can achieve nearly the same perfor-
mance with an interface that simply displays an op-
tion and requests a yes or a no. We intend to run this
study with subjects with impairment, and are incor-
porating the interfaces with an ERP detection system
for use as a brain-computer interface.
35
Acknowledgments
This research was supported in part by NIH Grant
#1R01DC009834-01 and NSF Grant #IIS-0447214.
Any opinions, findings, conclusions or recommen-
dations expressed in this publication are those of the
authors and do not necessarily reflect the views of
the NSF or NIH.
References
D. Anson, P. Moist, M. Przywars, H. Wells, H. Saylor,
and H. Maxime. 2004. The effects of word com-
pletion and word prediction on typing rates using on-
screen keyboards. Assistive Technology, 18(2):146?
154.
G. Baletsa, R. Foulds, and W. Crochetiere. 1976. Design
parameters of an intelligent communication device. In
Proceedings of the 29th Annual Conference on Engi-
neering in Medicine and Biology, page 371.
M. Baljko and A. Tam. 2006. Indirect text entry using
one or two keys. In Proceedings of the Eigth Inter-
national ACM Conference on Assistive Technologies
(ASSETS), pages 18?25.
B. Carpenter. 2005. Scaling high-order character lan-
guage models to gigabytes. In Proceedings of the ACL
Workshop on Software, pages 86?99.
J.J. Darragh, I.H. Witten, and M.L. James. 1990. The
reactive keyboard: A predictive typing aid. Computer,
23(11):41?49.
L.A. Farwell and E. Donchin. 1988. Talking off the
top of your head: toward a mental prosthesis utiliz-
ing event-related brain potentials. Electroenceph Clin.
Neurophysiol., 70:510?523.
J.P. Hansen, A.S. Johansen, D.W. Hansen, K. Itoh, and
S. Mashino. 2003. Language technology in a pre-
dictive, restricted on-screen keyboard with ambiguous
layout for severely disabled people. In Proceedings of
EACL Workshop on Language Modeling for Text Entry
Methods.
D.A. Huffman. 1952. A method for the construction of
minimum redundancy codes. In Proceedings of the
IRE, volume 40(9), pages 1098?1101.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 181?184.
G.W. Lesher, B.J. Moulton, and D.J. Higginbotham.
1998. Techniques for augmenting scanning commu-
nication. Augmentative and Alternative Communica-
tion, 14:81?101.
J. Li and G. Hirst. 2005. Semantic knowledge in word
completion. In Proceedings of the 7th International
ACM Conference on Computers and Accessibility.
I.S. MacKenzie and R.W. Soukoreff. 2003. Phrase sets
for evaluating text entry techniques. In Proceedings of
the ACM Conference on Human Factors in Computing
Systems (CHI), pages 754?755.
J. Perelmouter and N. Birbaumer. 2000. A binary
spelling interface with random errors. IEEE Transac-
tions on Rehabilitation Engineering, 8(2):227?232.
B. Roark. 2009. Open vocabulary language modeling
for binary response typing interfaces. Technical
Report #CSLU-09-001, Center for Spoken Language
Processing, Oregon Health & Science University.
cslu.ogi.edu/publications/ps/roark09.pdf.
I. Schadle. 2004. Sibyl: AAC system using NLP tech-
niques. In Proceedings of the 9th International Con-
ference on Computers Helping People with Special
needs (ICCHP), pages 1109?1015.
E.W. Sellers and E. Donchin. 2006. A p300-based brain-
computer interface: initial tests by als patients. Clini-
cal Neuropsysiology, 117:538?548.
E.W. Sellers, G. Schalk, and E. Donchin. 2003. The
p300 as a typing tool: tests of brain-computer interface
with an als patient. Psychophysiology, 40:77.
C.E. Shannon. 1950. Prediction and entropy of printed
English. Bell System Technical Journal, 30:50?64.
K. Trnka, D. Yarrington, K.F. McCoy, and C. Pennington.
2006. Topic modeling in fringe word prediction for
AAC. In Proceedings of the International Conference
on Intelligent User Interfaces, pages 276?278.
K. Trnka, D. Yarrington, J. McCaw, K.F. McCoy, and
C. Pennington. 2007. The effects of word predic-
tion on communication rate for AAC. In Proceed-
ings of HLT-NAACL; Companion Volume, Short Pa-
pers, pages 173?176.
H. Trost, J. Matiasek, and M. Baroni. 2005. The lan-
guage component of the FASTY text prediction sys-
tem. Applied Artificial Intelligence, 19(8):743?781.
T. Wandmacher and J.Y. Antoine. 2007. Methods to in-
tegrate a language model with semantic information
for a word prediction component. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 506?513.
T. Wandmacher, J.Y. Antoine, F. Poirier, and J.P. De-
parte. 2008. Sibylle, an assistive communication sys-
tem adapting to the context and its user. ACM Transac-
tions on Accessible Computing (TACCESS), 1(1):6:1?
30.
D.J. Ward, A.F. Blackwell, and D.J.C. MacKay. 2002.
DASHER ? a data entry interface using continuous
gestures and language models. Human-Computer In-
teraction, 17(2-3):199?228.
I.H. Witten and T.C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Transactions on
Information Theory, 37(4):1085?1094.
36
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 22?31,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards technology-assisted co-construction with communication partners
Brian Roark
?
, Andrew Fowler
?
, Richard Sproat
?
, Christopher Gibbons
?
, Melanie Fried-Oken?
?Center for Spoken Language Understanding ?Child Development & Rehabilitation Center
Oregon Health & Science University
{roark,fowlera,sproatr}@cslu.ogi.edu {gibbons,mfo}@ohsu.edu
Abstract
In this paper, we examine the idea of
technology-assisted co-construction, where
the communication partner of an AAC user
can make guesses about the intended mes-
sages, which are included in the user?s word
completion/prediction interface. We run some
human trials to simulate this new interface
concept, with subjects predicting words as the
user?s intended message is being generated in
real time with specified typing speeds. Re-
sults indicate that people can provide substan-
tial keystroke savings by providing word com-
pletion or prediction, but that the savings are
not as high as n-gram language models. In-
terestingly, the language model and human
predictions are complementary in certain key
ways ? humans doing a better job in some
circumstances on contextually salient nouns.
We discuss implications of the enhanced co-
construction interface for real-time message
generation in AAC direct selection devices.
1 Introduction
Individuals who cannot use standard keyboards for
text entry because of physical disabilities have a
number of alternative text entry methods that per-
mit typing. Referred to as keyboard emulation
within augmentative and alternative communication
(AAC), there are many different access options for
the user, ranging from direct selection of letters with
any anatomical pointer (e.g., head, eyes) to use of a
binary switch ? triggered by button-press, eye-blink
or even through event related potentials (ERP) such
as the P300 detected in EEG signals. These options
allow the individual to indirectly select a symbol
based on some process for scanning through alter-
natives (Lesher et al, 1998). Typing speed is a chal-
lenge, yet is critically important for usability, and
as a result there is a significant line of research into
the utility of statistical language models for improv-
ing typing speed (McCoy et al, 2007; Koester and
Levine, 1996; Koester and Levine, 1997; Koester
and Levine, 1998). Methods of word, symbol,
phrase and message prediction via statistical lan-
guage models are widespread in both direct selec-
tion and scanning devices (Darragh et al, 1990; Li
and Hirst, 2005; Trost et al, 2005; Trnka et al,
2006; Trnka et al, 2007; Wandmacher and Antoine,
2007; Todman et al, 2008). To the extent that the
predictions are accurate, the number of keystrokes
required to type a message can be dramatically re-
duced, greatly speeding typing.
AAC devices for spontaneous and novel text gen-
eration are intended to empower the user of the sys-
tem, to place them in control of their own com-
munication, and reduce their reliance on others for
message formulation. As a result, all such devices
(much like standard personal computers) are built
for a single user, with a single keyboard and/or alter-
native input interface, which is driven by the user of
the system. The unilateral nature of these high tech-
nology solutions to AAC stands in contrast to com-
mon low technology solutions, which rely on collab-
oration between the individual formulating the mes-
sage and their communication partner. Many adults
with acquired neurological conditions rely on com-
munication partners for co-construction of messages
(Beukelman et al, 2007).
One key reason why low-tech co-construction
may be preferred to high-tech stand-alone AAC sys-
tem solutions is the resulting speed of communica-
tion. Whereas spoken language reaches more than
one hundred words per minute and an average speed
typist using standard touch typing will achieve ap-
proximately 35 words per minute, a user of an AAC
device will typically input text in the 3-10 words per
minute range. With a communication partner guess-
22
ing the intended message and requesting confirma-
tion, the communication rate can speed up dramati-
cally. For face-to-face communication ? a modality
that is currently very poorly served by AAC devices
? such a speedup is greatly preferred, despite any
potential authorship questions.
Consider the following low-tech scenario. Sandy
is locked-in, with just a single eye-blink serving to
provide binary yes/no feedback. Sandy?s commu-
nication partner, Kim, initiates communication by
verbally stepping through an imagined row/column
grid, first by number (to identify the row); then by
letter. In such a way, Sandy can indicate the first
desired symbol. Communication can continue in
this way until Kim has a good idea of the word that
Sandy intends and proposes the word. If Sandy says
yes, the word has been completed, much as auto-
matic word completion may occur within an AAC
device. But Kim doesn?t necessarily stop with word
completion; subsequent word prediction, phrase pre-
diction, in fact whole utterance prediction can fol-
low, driven by Kim?s intuitions derived from knowl-
edge of Sandy, true sensitivity to context, topic, so-
cial protocol, etc. It is no wonder that such methods
are often chosen over high-tech alternatives.
In this paper, we present some preliminary ideas
and experiments on an approach to providing tech-
nology support to this sort of co-construction during
typing. The core idea is to provide an enhanced in-
terface to the communication partner (Kim in the ex-
ample above), which does not allow them to directly
contribute to the message construction, but rather
to indirectly contribute, by predicting what they be-
lieve the individual will type next. Because most text
generation AAC devices typically already rely upon
symbol, word and phrase prediction from statistical
language models to speed text input, the predictions
of the conversation partner could be used to influ-
ence (or adapt) the language model. Such adaptation
could be as simple as assigning high probability to
words or symbols explicitly predicted by the com-
munication partner, or as complex as deriving the
topic or context from the partner?s predictions and
using that context to improve the model.
Statistical language models in AAC devices can
capture regularities in language, e.g., frequent word
collocations or phrases and names commonly used
by an individual. People, however, have access to
much more information than computational mod-
els, including rich knowledge of language, any rel-
evant contextual factors that may skew prediction,
familiarity with the AAC user, and extensive world
knowledge ? none of which can be easily included in
the kinds of simple statistical models that constitute
the current state of the art. People are typically quite
good at predicting what might come next in a sen-
tence, particularly if it is part of a larger discourse or
dialogue. Indeed, some of the earliest work looking
at statistical models of language established the en-
tropy of English by asking subjects to play a simple
language guessing game (Shannon, 1950). The so-
called ?Shannon game? starts with the subject guess-
ing the first letter of the text. Once they have guessed
correctly, it is uncovered, and the subject guesses
the next letter, and so on. A similar game could be
played with words instead of letters. The number of
guesses required is a measure of entropy in the lan-
guage. People are understandably very good at this
game, often correctly predicting symbols on the first
try for very long stretches of text. No purely com-
putational model can hope to match the contextual
sensitivity, partner familiarity, or world knowledge
that a human being brings to such a task.
A co-construction scenario differs from a Shan-
non game in terms of the time constraints under
which it operates. The communication partner in
such a scenario must offer completions and predic-
tions to the user in a way that actually speeds com-
munication relative to independent text generation.
Given an arbitrary amount of time, it is clear that
people have greater information at their disposal for
predicting subsequent content; what happens under
time constraints is less clear. Indeed, in this paper
we demonstrate that the time constraints put human
subjects at a strong disadvantage relative to language
models in the scenarios we simulated. While it is
far from clear that this disadvantage will also apply
in scenarios closer to the motivating example given
above, it is certainly the case that providing useful
input is a challenging task.
The principal benefit of technology-assisted co-
construction with communication partners is making
use of the partner?s knowledge of language and con-
text, as well as their familiarity with the AAC user
and the world, to yield better predictions of likely
continuations than are currently made by the kinds
23
of relatively uninformed (albeit state of the art) com-
putational language models. A secondary benefit is
that such an approach engages the conversation part-
ner in a high utility collaboration during the AAC
user?s turn, rather than simply sitting and waiting for
the reply to be produced. Lack of engagement is a
serious obstacle to successful conversation in AAC
(Hoag et al, 2004). The slow speed of AAC input is
itself a contributing factor to AAC user dissatisfac-
tion with face-to-face conversation, one of the most
critical modes of human social interaction, and the
one least served by current technology. Because of
the slow turnaround, the conversation partner tends
to lose focus and interest in the conversation, leading
to shorter and less satisfying exchanges than those
enjoyed by those using spoken language. A system
which leverages communication partner predictions
will more fully engage the conversation partner in
the process, rather than forcing them to wait for a
response with nothing to do.
Importantly, an enhanced interface such as that
proposed here provides predictive input from the
communication partner, but not direct compositional
input. The responsibility of selecting symbols and
words during text entry remains with the AAC user,
as the sole author of the text. In the preliminary
experiments presented later in the paper, we simu-
late a direct selection typing system with word pre-
diction, and measure the utility of human generated
word completions and predictions relative to n-gram
models. In such a scenario, n-gram predictions can
be replaced or augmented by human predictions.
This illustrates how easily technology assisted co-
construction with communication partners could po-
tentially be integrated into a user?s interface.
Despite the lack of speedup achieved versus n-
gram models in the results reported below, the po-
tential for capturing communication partner intu-
itions about AAC user intended utterances seems a
compelling topic for future research.
2 Background and Related Work
Over the past forty years, there has been a vast
array of technological solutions to aid AAC users
who present with severe speech and physical im-
pairments, from methods for generating possible
responses, to techniques for selecting among re-
sponses. The simplest methods to generate lan-
guage involve the use of pre-stored phrases, such as
?hello?, ?thank you?, ?I love you?, etc., which are
available on many AAC devices. Some studies have
indicated that use of such phrases improves the per-
ception of fluid communication (McCoy et al, 2007;
Hoag et al, 2008).
Prediction options vary in AAC devices, rang-
ing from letter-by-letter prediction ? see Higgin-
botham (1992) and Lesher et al (1998) for some
reviews ? to word-based prediction. Some systems
can be quite sophisticated, for example incorporat-
ing latent semantic analysis to aid in the better mod-
eling of discourse-level information (Wandmacher
and Antoine, 2007). The WebCrawler project in Jef-
frey Higginbotham?s lab uses topic-related wordlists
mined from the Web to populate a user?s AAC de-
vice with terminology that is likely to be of utility to
the current topic of conversation.
Going beyond word prediction, there has been
an increased interest in utterance-based approaches
(Todman et al, 2008), which extend prediction
from the character or word level to the level
of whole sentences. For example, systems like
FrameTalker/Contact (Higginbotham and Wilkins,
1999; Wilkins and Higginbotham, 2006) populate
the AAC device with pre-stored phrases that can be
organized in various ways. In a similar vein, re-
cent work reported in Wisenburn and Higginbotham
(2008; 2009) proposed a novel method that uses au-
tomatic speech recognition (ASR) on the speech of
the communication partner, extracts noun phrases
from the speech, and presents those noun phrases on
the AAC device, with frame sentences that the AAC
user can select. Thus if the communication partner
says ?Paris?, the AAC user will be able to select
from phrases like ?Tell me more about Paris? or ?I
want to talk about Paris?. This can speed up the con-
versation by providing topically-relevant responses.
Perhaps the most elaborate system of this kind is the
How Was School Today system (Reiter et al, 2009).
This system, which is geared towards children with
severe communication disabilities, uses data from
sensors, the Web, and other sources as input for a
natural language generation system. The system ac-
quires information about the child?s day in school:
which classes he or she attended, what activities
there were, information about visitors, food choices
at the cafeteria, and so forth. The data are then used
24
to generate natural language sentences, which are
converted to speech via a speech synthesizer. At the
end of the day, the child uses a menu to select sen-
tences that he or she wants the system to utter, and
thereby puts together a narrative that describes what
he/she did. The system allows for vastly more rapid
output than a system where the child constructs each
sentence from scratch.
Perhaps the closest work to what we are proposing
is the study of non-disabled adults in Cornish and
Higginbotham (No Date), where one of the adults
played the role of an AAC user, and the other a non-
disabled communication partner. The participants
completed a narrative, a map and a puzzle task. Of
interest was the relative amount of co-construction
of the other?s utterances by each partner, and in
particular its relation to which of the partners was
the one initiating the attempt to achieve a common
ground with the other speaker ? the ?grounded
contribution owner?. In all tasks both the commu-
nication partner and the AAC user co-constructed
each other?s contributions, but there was the great-
est asymmetry between the two users in the puzzle
task.
In what follows, we will first describe a prelim-
inary experiment of word completion for a simu-
lated AAC user, using sentences from the Enron
email corpus and the New York Times. We then
will present results for word completion and pre-
diction within the context of dialogs in the Switch-
board corpus. While we ultimately believe that
the potential for co-construction goes far beyond
simple word completion/prediction, these experi-
ments serve as a first indication of the challenges
to an enhanced technology-assisted interface for co-
construction with communication partners during
novel text generation.
3 Preliminary experiment
In this section, we present a preliminary experiment
to evaluate the potential utility of our technology-
assisted co-construction scenario. The experiment is
akin to a Shannon Game (Shannon, 1950), but with
a time limit for guesses imposed by the speed of typ-
ing. For the current experiment we chose 5 seconds
per keystroke as the simulated typing speed: target
sentences appeared one character at a time, every
five seconds. The subjects? task was to provide a
Figure 1: Preliminary experimental interface in terminal
window, with 4 predicted completions and cursor below
completion for the current word. If the correct word
is provided by the subject, it is selected by the sim-
ulated AAC user as the next keystroke.
For this preliminary experiment, we used a sim-
ple program running in the terminal window of a
Mac laptop. Figure 1 shows a screenshot from this
program in operation. The target string is displayed
at the top of the terminal window, one character at
a time, with the carat symbol showing white space
word boundaries. Predicted word completions are
made by typing with a standard qwerty keyboard;
and when the enter key is pressed, the word that has
been typed is aligned with the current incomplete
word. If it is consistent with the prefix of the word
that has been typed, it remains as a candidate for
completion. When the current five second interval
has passed, the set of accumulated predictions are
filtered to just those which are consistent with the
new letter that the user would have typed (e.g., ?i?
in Figure 1). If the correct word completion for the
target string is present, it is selected with the follow-
ing keystroke. Otherwise the following letter will
be typed (with the typical 5-second delay) and the
interface proceeds as before.
Three able-bodied, adult, literate subjects were
recruited for this initial experiment, and all three
completed trials with both Enron email and New
York Times target strings. The Enron data
comes from the Enron email dataset (http://www-
2.cs.cmu.edu/?enron/) and the NY Times data from
the English Gigaword corpus (LDC2007T07). Both
corpora were pre-processed to remove duplicate data
(e.g., spam or multiple recipient emails), tabular
data and other material that does not represent writ-
ten sentences. Details on this normalization can be
found in Roark (2009). Both corpora consist of writ-
ten sentences, one heavily edited (newspaper), the
other less formal (email); and both are large enough
to allow for robust statistical language modeling.
25
Ngram training Testing
Task sents words sents words chars
NYT 1.9M 35.6M 10 201 1199
Enron 0.6M 6.1M 10 102 528
Table 1: Statistics for each task of n-gram training corpus
size and test set size in terms of sentences, words and
characters (baseline keystrokes)
The two corpora were split into training and test-
ing sets, to allow for training of n-gram language
models to compare word completion performance.
To ensure fair comparison between n-gram and hu-
man word completion performance, no sentences in
the test sets were seen in the training data. From
each test corpus, we extracted sets of 10 contiguous
sentences at periodic intervals, to use as test or prac-
tice sets. Each subject used a 10 sentence practice
set from the NY Times to become familiar with the
task and interface; then performed the word com-
pletion task on one 10 sentence set from the NY
Times and one 10 sentence set from the Enron cor-
pus. Statistics of the training and test sets are given
in Table 1.
Language models were n-gram word-based mod-
els trained from the given corpora using Kneser-Ney
smoothing (Kneser and Ney, 1995). We performed
no pruning on the models.
We evaluate in terms of keystroke savings per-
centage. Let k be the baseline number of keystrokes
without word completion, which is the number of
characters in the sample, i.e., 1 keystroke per char-
acter. With a given word completion method, let c be
the number of keystrokes required to enter the text,
i.e., if the word completion method provides correct
words for selection, those will reduce the number of
keystrokes required1. Then keystroke savings per-
centage is 100?(k?c)/k, the percentage of original
keystrokes that were saved with word completion.
Table 2 shows the keystroke savings percentage on
our two tasks for three n-gram language models (un-
igram, bigram and trigram) and our three subjects.
It is clear from this table that the n-gram language
models are achieving much higher keystroke savings
than our three human subjects. Further, our three
subjects performed quite similarly, not only in com-
1Each word completion requires a selection keystroke, but
saves the keystrokes associated with the remaining characters
in the selected word.
N-gram Subject
Task 1g 2g 3g 1 2 3
NYT 47.4 54.5 56.0 36.5 32.0 32.9
Enron 54.4 61.4 64.4 34.5 32.0 34.1
Table 2: Keystroke savings percentage for test set across
models and subjects
parison with each other, but across the two tasks.
On the face of it, the relatively poor performance
of the human predictors might be surprising, given
that the original Shannon game was intended to es-
tablish a lower bound on the entropy of English. The
assumption has always been that people have better
language models than we can hope to learn automat-
ically. However, in contrast to the original Shannon
game, our predictions are carried out with a fairly
tight time limit, i.e., predictions need to be made
within a fairly short period in order to be made avail-
able to individuals for word completion. The time
limit within the current scenario is one factor that
seems to be putting the subjects at a disadvantage
compared to automated n-gram models on this task.
There are a couple of additional reasons why n-
gram models are performing better on these tasks.
First, they are specific domains with quite ample
training data for the language models. As the
amount of training data decreases ? which would
certainly be the case for individual AAC users ? the
efficacy of the n-gram models decrease. Second,
there is a 1-character advantage of n-gram models
relative to human predictions in this approach. To
see this point clearly, consider the position at the
start of the string. N-gram models can (for prac-
tical purposes) instantaneously provide predictions
for that word. But our subjects must begin typing
the words that they are predicting for this position
at the same time the individual is making their first
keystroke. Those predictions do not become opera-
tive until after that keystroke. Hence the time over-
head of prediction places a lag relative to what is
possible for the n-gram model. We will return to
this point in the discussion section at the end of the
paper.
There are some scenarios, however, where the
subjects did provide word completions prior to the
trigram language model in both domains. Interest-
ingly, a fairly large fraction of these words were
faster than n-gram for more than one of the three
26
NY Times Enron
company cranbury creditor hearing
creditors denied facility suggestions
foothill jamesway jamesways stairs
plan proposal sandler savings
stock stockholders warrants
Table 3: Words completed using subject suggestions with
fewer keystrokes than trigram model. Bold indicates
more than one subject was faster for that word.
subjects. Table 3 shows the list of these words for
our trials. These tended to be longer, open-class
words with high topical importance. In addition,
they tended to be words with common word pre-
fixes, which lead to higher confusability in the n-
gram model. Of course, common prefixes also lead
to higher confusability in our subjects, yet they ap-
pear to be able to leverage their superior context sen-
sitivity to yield effective disambiguation earlier than
the n-gram model in these cases.
Based on these results, we designed a second ex-
periment, with a few key changes from this prelim-
inary experiment, including an improved interface,
the ability to predict as well as complete, and a do-
main that is closer to a proposed model for this co-
construction task.
4 Switchboard experiment
Based on the preliminary experiment, we created a
new protocol and ran seven able-bodied, adult, lit-
erate subjects. We changed the interface and do-
main in ways that we believed would make a dif-
ference in the ability of subjects to compete with n-
gram models in keystroke savings. What remained
the same was the timing of the interface: characters
for target strings were displayed every five seconds.
Word completions were then evaluated for consis-
tency with what had been typed, and if the correct
word was present, the word was completed and re-
vealed, and typing continued.
Data Our primary motivating case for technology-
assisted co-construction comes from face-to-face di-
alog, yet the corpora from which target strings were
extracted in the preliminary experiments were from
large corpora of text produced under very different
conditions. One corpus that does represent a varied-
topic, conversational dialog scenario is the Switch-
board corpus (Godfrey et al, 1992), which contains
transcripts of both sides of telephone conversations.
The idea in using this data was to provide some num-
ber of utterances of dialog context (from the 10 pre-
vious dialog turns), and then ask subjects to provide
word completions for some number of subsequent
utterances.
While the Switchboard corpus does represent the
kind of conversational dialog we are interested in, it
is a spoken language corpus, yet we are modeling
written (typed) language. The difference between
written and spoken language does present something
of an issue for our task. To mitigate this mismatch
somewhat, we made use of the Switchboard section
of the Penn Treebank (Marcus et al, 1993), which
contains syntactic annotations of the Switchboard
transcripts, including explicit marking of disfluen-
cies (?EDITED? non-terminals in the treebank), in-
terjections or parentheticals such as ?I mean? or
?you know?. Using these syntactic annotations, we
produced edited transcripts that omit much of the
spoken language specific phenomena, thus provid-
ing a closer approximation to the kind of written di-
alogs we would like to simulate. In addition, we de-
cased the corpus and removed all characters except
the following: the 26 letters of the English alphabet,
the apostrophe, the space, and the dash.
Interface Figure 2 shows the graphical user inter-
face that was created for these trials. In the upper
box, ten utterances from the context of the dialog are
presented, with an indication of which speaker (A or
B) took the turn. Participants are asked to first read
this context and then press enter to begin the session.
Below this box, the current utterance is displayed,
along with which of the two participants is currently
producing the utterance. As in the previous experi-
ment, the string is displayed one character at a time
in this region. Below this is a text box where word
completions and predictions are entered. Finally, at
the bottom of the interface, Figure 2 shows two of
the five rows of current word completions (left col-
umn) and next word predictions (right column).
Perhaps the largest departure from the preliminary
experiment is the ability to not only complete the
current word but also to provide predictions about
the subsequent word. The subject uses a space de-
limiter to indicate whether predictions are for the
current word or for the subsequent word. Words
preceding a space are taken as current word com-
pletions; the first word after a space is taken as a
27
Figure 2: Experimental graphical user interface
subsequent word prediction. To just predict the sub-
sequent word, one can lead with a space, which re-
sults in no current word completion and whatever
comes after the space as next word prediction. Once
the current word is complete, any words on the sub-
sequent word prediction list are immediately shifted
to the word completion list. We limited current and
next word predictions to five.
We selected ten test dialogs, and subjects pro-
duced word completions and predictions for three
utterances per dialog, for a total of thirty utterances.
We selected the test dialogs to conform to the fol-
lowing characteristics:
1. Each group of three utterances was consecutive
and spoken by the same person.
2. Each utterance contained more than 15 charac-
ters of text.
3. Each group of three utterances began turn-
initially; the first of the three utterances was
always immediately after the other speaker in
the corpus had spoken at least two consecutive
utterances of 15 characters or more.
4. Each group of three utterances was far enough
into its respective conversation that there was
enough text to provide the ten lines of context
required above.
Language models used to contrast with human
performance on this task were trained separately for
every conversation in the test set. For each conver-
sation, Kneser-Ney smoothed n-gram models were
built using all other conversations in the normalized
Switchboard corpus. Thus no conversation is in its
own training data. Table 4 shows statistics of train-
ing and test sets.
Table 5 shows the results for n-gram models and
our seven subjects on this test. Despite the differ-
ences in the testing scenario from the preliminary
experiment, we can see that the results are very sim-
ilar to what was found in that experiment. Also sim-
ilar to the previous trial was the fact that a large per-
centage of tokens for which subjects provided faster
word completion than the trigram model were faster
for multiple subjects. Table 6 shows the nine words
that were completed faster by more than half of the
subjects than the trigram model. Thus, while there is
some individual variation in task performance, sub-
jects were fairly consistent in their ability to predict.
5 Discussion
In this paper we presented two experiments that
evaluated a new kind of technology-assisted co-
construction interface for communication partners
during time-constrained text generation. Results
Ngram training Testing
Task sents words sents words chars
SWBD 0.66M 3.7M 30 299 1501
Table 4: Statistics for the Switchboard task of n-gram
training corpus size and test set size in terms of utter-
ances, words and characters (baseline keystrokes)
28
N-gram Subject
Task 1g 2g 3g 1 2 3 4 5 6 7
Switchboard 51.0 59.0 60.0 28.7 33.1 28.4 28.6 34.1 31.8 32.5
Table 5: Keystroke savings percentage for Switchboard test set across models and subjects
applied can?t comes
every failure named
physics should supervisor
Table 6: Words completed in more than half of the
Switchboard trials using subject suggestions with fewer
keystrokes than trigram model.
from both experiments are negative, in terms of the
ability of our human subjects to speed up communi-
cation via word prediction under time constraints be-
yond what is achievable with n-gram language mod-
els. These results are somewhat surprising given
conventional wisdom about the superiority of hu-
man language models versus their simplified compu-
tational counterparts. One key reason driving the di-
vergence from conventional wisdom is the time con-
straint on production of predictions. Another is the
artificiality of the task and relative unfamiliarity of
the subjects with the individuals communicating.
While these results are negative, there are reasons
why they should not be taken as an indictment of
the approach as a whole, rather an indication of the
challenges faced by this task. First, we would stress
the fact that we have not yet tested the approach in a
situation where the user knows the speaker well, and
therefore can be presumed to have knowledge well
beyond general knowledge of English and general
topical knowledge. In future work we are planning
experiments based on interactions between people
who have a close relationship with each other. In
such a scenario, we can expect that humans would
have an advantage over statistical language models,
for which appropriate training data would not, in any
case, be available.
None of the domains that we evaluated were a per-
fect match to the application: the text data was not
dialog, and the dialogs were spoken rather than writ-
ten language. Further, the tasks that we evaluated in
this paper are quite rigid compared to what might
be considered acceptable in real use. For example,
our task required the prediction of a particular word
type, whereas in actual use synonyms or other ways
of phrasing the same information will likely be quite
acceptable to most AAC users. In such an applica-
tion, the task is not to facilitate production of a spe-
cific word string, rather production of an idea which
might be realized variously. We were interested in
the tasks reported here as a first step towards under-
standing the problem, and among the lessons learned
are the shortcomings of these very tasks.
Another take-away message relates to the util-
ity of the new interface itself. The subjects in
these trials had the difficult task of quickly pre-
dicting intended words; this is also a communica-
tion task that may be assisted. Providing access to
what n-gram models are predicting may allow the
communication partner to quickly select or winnow
down the options. Further, it is apparent that single
word completions or predictions is not where com-
munication partners are going to achieve order-of-
magnitude speedups in communication; rather such
speedups may be realized in facilitation of larger
phrase or whole utterance production, particularly
when the communication is between familiar part-
ners on known topics.
In summary, this paper presented preliminary re-
sults on the ability of human subjects to provide
word completion and prediction information to users
of AAC systems, through simulation of such a new
interface concept. While the subjects were not
able to match n-gram language models in terms
of keystroke reduction, we did see consistent per-
formance across many subjects and across several
domains, yielding real keystroke reductions on the
stimulus strings. Ultimately, the tasks were not as
representative of real co-construction scenarios a we
would have liked, but they serve to illustrate the
challenges of such an application.
Acknowledgments
This research was supported in part by NIH Grant
#1R01DC009834-01. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NIH.
29
References
D.R. Beukelman, S. Fager, L. Ball, and A. Dietz. 2007.
AAC for adults with acquired neurological conditions:
A review. Augmentative and Alternative Communica-
tion, 23(3):230?242.
Jennifer Cornish and Jeffrey Higginbotham. No Date.
Assessing AAC interaction III: Effect of task type
on co-construction & message repair. AAC-
RERC, available from http:aac-rerc.psu.
edu/_userfiles/asha3.pdf.
J.J. Darragh, I.H. Witten, and M.L. James. 1990. The
reactive keyboard: A predictive typing aid. Computer,
23(11):41?49.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
Switchboard: A telephone speech corpus for research
and develpment. In Proceedings of ICASSP, volume I,
pages 517?520.
D. Jeffery Higginbotham and David Wilkins. 1999.
Frametalker: A system and method for utilizing com-
munication frames in augmented communication tech-
nologies. US Patent No. 5,956,667.
D. Jeffery Higginbotham. 1992. Evaluation of keystroke
savings across five assistive communication technolo-
gies. Augmentative and Alternative Communication,
8:258?272.
Linda A. Hoag, Jan L. Bedrosian, Kathleen F. McCoy,
and Dallas Johnson. 2004. Informativeness and speed
of message delivery trade-offs in augmentative and
alternative communication. Journal of Speech, Lan-
guage, and Hearing Research, 47:1270?1285.
Linda A. Hoag, Jan L. Bedrosian, Kathleen F. Mc-
Coy, and Dallas Johnson. 2008. Hierarchy of
conversational rule violations involving utterance-
based augmentative and alternative communication
systems. Augmentative and Alternative Communica-
tion, 24(2):149?161.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 181?184.
Heidi H. Koester and Simon Levine. 1996. Ef-
fect of a word prediction feature on user perfor-
mance. Augmentative and Alternative Communica-
tion, 12(3):155?168.
Heidi H. Koester and Simon Levine. 1997. Keystroke-
level models for user performance with word predic-
tion. Augmentative and Alternative Communication,
13(4):239257.
Heidi H. Koester and Simon Levine. 1998. Model
simulations of user performance with word predic-
tion. Augmentative and Alternative Communication,
14(1):25?36.
G.W. Lesher, B.J. Moulton, and D.J. Higginbotham.
1998. Techniques for augmenting scanning commu-
nication. Augmentative and Alternative Communica-
tion, 14:81?101.
J. Li and G. Hirst. 2005. Semantic knowledge in word
completion. In Proceedings of the 7th International
ACM Conference on Computers and Accessibility.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Kathleen F. McCoy, Jan L. Bedrosian, Linda A. Hoag,
and Dallas E. Johnson. 2007. Brevity and speed of
message delivery trade-offs in augmentative and alter-
native communication. Augmentative and Alternative
Communication, 23(1):76?88.
Ehud Reiter, Ross Turner, Norman Alm, Rolf Black,
Martin Dempster, and Annalu Waller. 2009. Us-
ing NLG to help language-impaired users tell stories
and participate in social dialogues. In 12th European
Workshop on Natural Language Generation, pages 1?
8. Association for Computational Linguistics.
B. Roark. 2009. Open vocabulary language modeling
for binary response typing interfaces. Technical
Report #CSLU-09-001, Center for Spoken Language
Processing, Oregon Health & Science University.
cslu.ogi.edu/publications/ps/roark09.pdf.
C.E. Shannon. 1950. Prediction and entropy of printed
English. Bell System Technical Journal, 30:50?64.
John Todman, Norman Alm, D. Jeffery Higginbotham,
and Portia File. 2008. Whole utterance approaches in
AAC. Augmentative and Alternative Communication,
24(3):235?254.
K. Trnka, D. Yarrington, K.F. McCoy, and C. Pennington.
2006. Topic modeling in fringe word prediction for
AAC. In Proceedings of the International Conference
on Intelligent User Interfaces, pages 276?278.
K. Trnka, D. Yarrington, J. McCaw, K.F. McCoy, and
C. Pennington. 2007. The effects of word predic-
tion on communication rate for AAC. In Proceed-
ings of HLT-NAACL; Companion Volume, Short Pa-
pers, pages 173?176.
H. Trost, J. Matiasek, and M. Baroni. 2005. The lan-
guage component of the FASTY text prediction sys-
tem. Applied Artificial Intelligence, 19(8):743?781.
T. Wandmacher and J.Y. Antoine. 2007. Methods to in-
tegrate a language model with semantic information
for a word prediction component. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 506?513.
David Wilkins and D. Jeffery Higginbotham. 2006. The
short story of Frametalker: An interactive AAC de-
vice. Perspectives on Augmentative and Alternative
Communication, 15(1):18?21.
30
Bruce Wisenburn and D. Jeffery Higginbotham. 2008.
An AAC application using speaking partner speech
recognition to automatically produce contextually rel-
evant utterances: Objective results. Augmentative and
Alternative Communication, 24(2):100?109.
Bruce Wisenburn and D. Jeffery Higginbotham. 2009.
Participant evaluations of rate and communication ef-
ficacy of an AAC application using natural language
processing. Augmentative and Alternative Communi-
cation, 25(2):78?89.
31
