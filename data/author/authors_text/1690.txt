99
100
101
102
103
104
105
106
Interactive grammar development with WCDG
Kilian A. Foth Michael Daum Wolfgang Menzel
Natural Language Systems Group
Hamburg University
D-22527 Hamburg
Germany
{foth,micha,menzel}@nats.informatik.uni-hamburg.de
Abstract
The manual design of grammars for accurate natu-
ral language analysis is an iterative process; while
modelling decisions usually determine parser be-
haviour, evidence from analysing more or differ-
ent input can suggest unforeseen regularities, which
leads to a reformulation of rules, or even to a differ-
ent model of previously analysed phenomena. We
describe an implementation of Weighted Constraint
Dependency Grammar that supports the grammar
writer by providing display, automatic analysis, and
diagnosis of dependency analyses and allows the di-
rect exploration of alternative analyses and their sta-
tus under the current grammar.
1 Introduction
For parsing real-life natural language reliably, a
grammar is required that covers most syntactic
structures, but can also process input even if it
contains phenomena that the grammar writer has
not foreseen. Two fundamentally different ways
of reaching this goal have been employed various
times. One is to induce a probability model of the
target language from a corpus of existing analyses
and then compute the most probable structure for
new input, i.e. the one that under some judiciously
chosen measure is most similar to the previously
seen structures. The other way is to gather linguis-
tically motivated general rules and write a parsing
system that can only create structures adhering to
these rules.
Where an automatically induced grammar re-
quires large amounts of training material and the
development focuses on global changes to the prob-
ability model, a handwritten grammar could in prin-
ciple be developed without any corpus at all, but
considerable effort is needed to find and formu-
late the individual rules. If the formalism allows
the ranking of grammar rules, their relative impor-
tance must also be determined. This work is usu-
ally much more cyclical in character; after grammar
rules have been changed, intended and unforeseen
consequences of the change must be checked, and
further changes or entirely new rules are suggested
by the results.
We present a tool that allows a grammar writer to
develop and refine rules for natural language, parse
new input, or annotate corpora, all in the same envi-
ronment. Particular support is available for interac-
tive grammar development; the effect of individual
grammar rules is directly displayed, and the system
explicitly explains its parsing decisions in terms of
the rules written by the developer.
2 The WCDG parsing system
The WCDG formalism (Schro?der, 2002) describes
natural language exclusively as dependency struc-
ture, i.e. ordered, labelled pairs of words in the in-
put text. It performs natural language analysis under
the paradigm of constraint optimization, where the
analysis that best conforms to all rules of the gram-
mar is returned. The rules are explicit descriptions
of well-formed tree structures, allowing a modular
and fine-grained description of grammatical knowl-
edge. For instance, rules in a grammar of English
would state that subjects normally precede the finite
verb and objects follow it, while temporal NP can
either precede or follow it.
In general, these constraints are defeasible, since
many rules about language are not absolute, but
can be preempted by more important rules. The
strength of constraining information is controlled by
the grammar writer: fundamental rules that must al-
ways hold, principles of different import that have
to be weighed against each other, and general pref-
erences that only take effect when no other disam-
biguating knowledge is available can all be formu-
lated in a uniform way. In some cases preferences
can also be used for disambiguation by approximat-
ing information that is currently not available to the
system (e.g. knowledge on attachment preferences).
Even the very weak preferences have an influence
on the parsing process; apart from serving as tie-
breakers for structures where little context is avail-
able (e.g. with fragmentary input), they provide an
Figure 1: Display of a simplified feature hierarchy
initial direction for the constraint optimization pro-
cess even if they are eventually overruled. As a con-
sequence, even the best structure found usually in-
curs some minor constraint violations; as long as
the combined evidence of these default expectation
failures is small, the structure can be regarded as
perfectly grammatical.
The mechanism of constraint optimization si-
multaneously achieves robustness against extra-
grammatical and ungrammatical input. There-
fore WCDG allows for broad-coverage parsing with
high accuracy; it is possible to write a grammar
that is guaranteed to allow at least one structure for
any kind of input, while still preferring compliant
over deviant input wherever possible. This graceful
degradation under reduced input quality makes the
formalism suitable for applications where deviant
input is to be expected, e.g. second language learn-
ing. In this case the potential for error diagnosis
is also very valuable: if the best analysis that can
be found still violates an important constraint, this
directly indicates not only where an error occurred,
but also what might be wrong about the input.
3 XCDG: A Tool for Parsing and
Modelling
An implementation of constraint dependency gram-
mar exists that has the character of middleware to al-
low embedding the parsing functionality into other
natural language applications. The program XCDG
uses this functionality for a graphical tool for gram-
mar development.
In addition to providing an interface to a range
of different parsing algorithms, graphical display
of grammar elements and parsing results is possi-
ble; for instance, the hierarchical relations between
possible attributes of lexicon items can be shown.
See Figure 1 for an excerpt of the hierarchy of Ger-
man syntactical categories used; the terminals cor-
respond to those used the Stuttgart-Tu?bingen Tagset
of German (Schiller et al, 1999).
More importantly, mean and end results of pars-
ing runs can be displayed graphically. Dependency
structures are represented as trees, while additional
relations outside the syntax structure are shown as
arcs below the tree (see the referential relationship
REF in Figure 2). As well as end results, inter-
mediate structures found during parsing can be dis-
played. This is often helpful in understanding the
behaviour of the heuristic solution methods em-
ployed.
Together with the structural analysis, instances
of broken rules are displayed below the depen-
dency graph (ordered by decreasing weights), and
the dependencies that trigger the violation are high-
lighted on demand (in our case the PP-modification
between the preposition in and the infinite form
verkaufen). This allows the grammar writer to eas-
ily check whether or not a rule does in fact make the
distinction it is supposed to make. A unique iden-
tifier attached to each rule provides a link into the
grammar source file containing all constraint defi-
nitions. The unary constraint ?mod-Distanz? in
the example of Figure 2 is a fairly weak constraint
which penalizes attachments the stronger the more
distant a dependent is placed from its head. At-
taching the preposition to the preceding noun Bund
would be preferred by this constraint, since the dis-
tance is shorter. However, it would lead to a more
serious constraint violation because noun attach-
ments are generally dispreferred.
To facilitate such experimentation, the parse win-
dow doubles as a tree editor that allows structural,
lexical and label changes to be made to an analysis
by drag and drop. One important application of the
integrated parsing and editing tool is the creation of
large-scale dependency treebanks. With the ability
to save and load parsing results from disk, automat-
ically computed analyses can be checked and hand-
corrected where necessary and then saved as anno-
tations. With a parser that achieves a high perfor-
mance on unseen input, a throughput of over 100 an-
notations per hour has been achieved.
4 Grammar development with XCDG
The development of a parsing grammar based on
declarative constraints differs fundamentally from
that of a derivational grammar, because its rules for-
bid structures instead of licensing them: while a
context-free grammar without productions licenses
nothing, a constraint grammar without constraints
would allow everything. A new constraint must
therefore be written whenever two analyses of the
same string are possible under the existing con-
straints, but human judgement clearly prefers one
over the other.
Figure 2: Xcdg Tree Editor
Most often, new constraints are prompted by in-
spection of parsing results under the existing gram-
mar: if an analysis is computed to be grammati-
cal that clearly contradicts intuition, a rule must be
missing from the grammar. Conversely, if an error
is signalled where human judgement disagrees, the
relevant grammar rule must be wrong (or in need of
clarifying exceptions). In this way, continuous im-
provement of an existing grammar is possible.
XCDG supports this development style through
the feature of hypothetical evaluation. The tree dis-
play window does not only show the result returned
by the parser; the structure, labels and lexical selec-
tions can be changed manually, forcing the parser to
pretend that it returned a different analysis. Recall
that syntactic structures do not have to be specif-
ically allowed by grammar rules; therefore, every
conceivable combination of subordinations, labels
and lexical selections is admissible in principle, and
can be processed by XCDG, although its score will
be low if it contradicts many constraints.
After each such change to a parse tree, all con-
straints are automatically re-evaluated and the up-
dated grammar judgement is displayed. In this way
it can quickly be checked which of two alternative
structures is preferred by the grammar. This is use-
ful in several ways. First, when analysing pars-
ing errors it allows the grammar author to distin-
guish search errors from modelling errors: if the
intended structure is assigned a better score than the
one actually returned by the parser, a search error
occurred (usually due to limited processing time);
but if the computed structure does carry the higher
score, this indicates an error of judgement on the
part of the grammar writer, and the grammar needs
to be changed in some way if the phenomenon is to
be modelled adequately.
If a modelling error does occur, it must be be-
cause a constraint that rules against the intended
analysis has overruled those that should have se-
lected it. Since the display of broken constraints is
ordered by severity, it is immediately obvious which
of the grammar rules this is. The developer can
then decide whether to weaken that rule or extend
it so that it makes an exception for the current phe-
nomenon. It is also possible that the intended anal-
ysis really does conflict with a particular linguistic
principle, but in doing so follows a more important
one; in this case, this other rule must be found and
strengthened so that it will overrule the first one.
The other rule can likewise be found by re-creating
the original automatic analysis and see which of its
constraint violations needs to be given more weight,
or, alternatively, which entirely new rule must be
added to the grammar.
In the decision whether to add a new rule to a con-
straint grammar, it must be discovered under what
conditions a particular phenomenon occurs, so that
a generally relevant rule can be written. The posses-
sion of a large amount of analysed text is often use-
ful here to verify decisions based on mere introspec-
tion. Working together with an external program
to search for specific structures in large treebanks,
XCDG can display multiple sentences in stacked
widgets and highlight all instances of the same phe-
nomenon to help the grammar writer decide what
the relevant conditions are.
Using this tool, a comprehensive grammar of
modern German has been constructed (Foth, 2004)
that employs 750 handwritten well-formedness
rules, and has been used to annotate around 25,000
sentences with dependency structure. It achieves a
structural recall of 87.7% on sentences from the NE-
GRA corpus (Foth et al, submitted), but can be ap-
plied to texts of many other types, where structural
recall varies between 80?90%. To our knowledge,
no other system has been published that achieves
a comparable correctness for open-domain German
text. Parsing time is rather high due to the computa-
tional effort of multidimensional optimization; pro-
cessing time is usually measured in seconds rather
than milliseconds for each sentence.
5 Conclusions
We demonstrate a tool that lets the user parse, dis-
play and manipulate dependency structures accord-
ing to a variant of dependency grammar in a graph-
ical environment. We have found such an inte-
grated environment invaluable for the development
of precise and large grammars of natural language.
Compared to other approaches, c.f. (Kaplan and
Maxwell, 1996), the built-in WCDG parser pro-
vides a much better feedback by pinpointing possi-
ble reasons for the current grammar being unable to
produce the desired parsing result. This additional
information can then be immediately used in subse-
quent development cycles.
A similar tool, called Annotate, has been de-
scribed in (Brants and Plaehn, 2000). This tool
facilitates syntactic corpus annotation in a semi-
automatic way by using a part-of-speech tagger and
a parser running in the background. In compari-
son, Annotate is primarily used for corpus annota-
tion, whereas XCDG supports the development of
the parser itself also.
Due to its ability to always compute the single
best analysis of a sentence and to highlight possible
shortcomings of the grammar, the XCDG system
provides a useful framework in which human design
decisions on rules and weights can be effectively
combined with a corpus-driven evaluation of their
consequences. An alternative for a symbiotic coop-
eration in grammar development has been devised
by (Hockenmaier and Steedman, 2002), where a
skeleton of fairly general rule schemata is instan-
tiated and weighed by means of a treebank anno-
tation. Although the resulting grammar produced
highly competitive results, it nevertheless requires
a treebank being given in advance, while our ap-
proach also supports a simultaneous treebank com-
pilation.
References
Thorsten Brants and Oliver Plaehn. 2000. Interac-
tive corpus annotation. In Proc. 2nd Int. Conf.
on Language Resources and Engineering, LREC
2000, pages 453?459, Athens.
Kilian Foth, Michael Daum, and Wolfgang Men-
zel. submitted. A broad-coverage parser for Ger-
man based on defeasible constraints. In Proc. 7.
Konferenz zur Verarbeitung nat?urlicher Sprache,
KONVENS-2004, Wien, Austria.
Kilian A. Foth. 2004. Writing weighted constraints
for large dependency grammars. In Proc. Recent
Advances in Dependency Grammars, COLING
2004, Geneva, Switzerland.
Julia Hockenmaier and Mark Steedman. 2002.
Generative models for statistical parsing with
combinatory categorial grammar. In Proc. 40th
Annual Meeting of the ACL, ACL-2002, Philadel-
phia, PA.
Ronald M. Kaplan and John T. Maxwell. 1996.
LFG grammar writer?s workbench. Technical re-
port, Xerox PARC.
Anne Schiller, Simone Teufel, Christine Sto?ckert,
and Christine Thielen. 1999. Guidelines fu?r das
Tagging deutscher Textcorpora. Technical report,
Universita?t Stuttgart / Universita?t Tu?bingen.
Ingo Schro?der. 2002. Natural Language Parsing
with Graded Constraints. Ph.D. thesis, Depart-
ment of Informatics, Hamburg University, Ham-
burg, Germany.
Dynamic Dependency Parsing
Michael Daum
Natural Language Systems Group
Department of Computer Science
University of Hamburg
micha@nats.informatik.uni-hamburg.de
Abstract
The inherent robustness of a system might be an
important prerequisite for an incremental pars-
ing model to the effect that grammaticality re-
quirements on full sentences may be suspended
or allowed to be violated transiently. How-
ever, we present additional means that allow the
grammarian to model prefix-analyses by alter-
ing a grammar for non-incremental parsing in a
controlled way. This is done by introducing un-
derspecified dependency edges that model the
expected relation between already seen and yet
unseen words during parsing. Thus the basic
framework of weighted constraint dependency
parsing is extended by the notion of dynamic
dependency parsing.
1 Introduction
In an incremental mode of operation, a parser
works on a prefix of a prolonging utterance, try-
ing to compute prefix-analyses while having to
cope with a growing computational effort. This
situation gives rise at least to the following ques-
tions:
(1) Which provisions can be made to accept
prefix-analyses transiently given a model
of language that describes complete sen-
tences?
(2) How shall prefix-analyses look like?
(3) How can the complexity of incremental
parsing be bounded?
We will introduce underspecified dependency
edges, called nonspec dependency edges, to the
framework of weighted constraint dependency
grammar (WCDG) (Schro?der, 2002). These are
used to encode an expected function of a word
already seen but not yet integrated into the rest
of the parse tree during incremental parsing.
In WCDG, parse trees are annotated by
constraint violations that pinpoint deviations
from grammatical requirements or preferences.
Hence weighted constraints are a means to de-
scribe a graded grammaticality discretion by de-
scribing the inherent ?costs? of accepting an im-
perfect parse tree. Thus parsing follows princi-
ples of economy when repairing constraint vio-
lations as long as reducing costs any further is
justified by its effort.
The following sections revise the basic ideas
of applying constraint optimization to natural
language parsing and extend it to dynamic de-
pendency parsing.
2 From static to dynamic constraint
satisfaction
We begin by describing the standard constraint
satisfaction problem (CSP), then extend it in
two different directions commonly found in the
literature: (a) to constraint optimization prob-
lems (COP) and (b) to dynamic constraint sat-
isfaction problems (DynCSP) aggregating both
to dynamic constraint optimization problems
(DynCOP) which is motivated by our current
application to incremental parsing1.
2.1 Constraint Satisfaction
Constraint satisfaction is defined as being the
problem of finding consistent values for a fixed
set of variables given all constraints between
those values. Formally, a constraint satisfac-
tion problem (CSP) can be viewed as a triple
(X, D, C) where X = {x1, . . . , xn} is a fi-
nite set of variables with respective domains
D = {D1, . . . , Dn}, and a set of constraints
C = {C1, . . . , Ct}. A constraint Ci is defined as
a relation defined on a subset of variables, called
1Note that we don?t use the common abbreviations
for dynamic constraint satisfaction problems DCSP in fa-
vor of DynCSP in order to distinguish if from distributed
constraint satisfaction problems which are called DCSPs
also. Likewise we use DynCOP instead of DCOP, the
latter of which is commonly known as distributed con-
straint optimization problems.
the scope, restricting their simultaneous assign-
ment. Constraints defined on one variable are
called unary ; constraints on two variables are
binary. We call unary and binary constraints lo-
cal constraints as their scope is very restricted.
Constraints of wider scope are classified non-
local. Especially those involving a full scope
over all variables are called context constraints.
The ?local knowledge? of a CSP is encoded in
a constraint network (CN) consisting of nodes
bundling all values of a variable consistent with
all unary constraints. The edges of a CN de-
pict binary constraints between the connected
variables. So a CN is a compact representation
(of a superset) of all possible instantiations. A
solution of a CSP is a complete instantiation of
variables ?x1, . . . , xn? with values ?di1 , . . . , din?
with dik ? Dk found in a CN that is consistent
with all constraints.
Principles of processing CSPs have been de-
veloped in (Montanari, 1974), (Waltz, 1975)
and (Mackworth, 1977).
2.2 Constraint Optimization
In many problem cases no complete instantia-
tion exists that satisfies all constraints: either
we get stuck by solving only a part of the prob-
lem or constraints need to be considered defea-
sible for a certain penalty. Thus finding a so-
lution becomes a constraint optimization prob-
lem (COP). A COP is denoted as a quadruple
(X, D, C, f), where (X, D, C) is a CSP and f
is a cost function on (partial) variable instan-
tiations. f might be computed by multiply-
ing the penalties of all violated constraints. A
solution of a COP is a complete instantiation,
where f(?di1 , . . . , din?) is optimal. This term
becomes zero if the penalty of at least one vio-
lated constraint is zero. These constraints are
called hard, those with a penalty greater zero
are called soft.
An more precise formulation of COPs (also
called partial constraint satisfaction problems),
can be found in (Freuder and Wallace, 1989).
2.3 Dynamic Constraint Satisfaction
The traditional CSP and COP framework is
only applicable to static problems, where the
number of variables, the values in their domains
and the constraints are all known in advance. In
a dynamically changing environment these as-
sumptions don?t hold any more as new variables,
new values or new constraints become available
over time. A dynamic constraint satisfaction
problem (DynCSP) is construed as a series of
CSPs P0, P1, . . . that change periodically over
time by loss of gain of values, variables or con-
straints (Pi+1 = Pi + ?Pi+1). For each problem
change ?Pi+1 we try to find a solution change
?Si+1 such that Si+1 = Si + ?Si+1 is a solution
to Pi+1. The legitimate hope is that this is more
efficient than solving Pi+1 the naive way from
scratch whenever things change.
This notation is consistent with previous ones
found in in (Dechter and Dechter, 1988) and
(Wire?n, 1993).
2.4 Dynamic Constraint Optimization
Most notions of DynCSPs in the literature are
an extension of the classical CSP that use hard
constraints exclusively. To model the aimed
application of incremental parsing however, we
still like to use weighted constraints. There-
fore we define dynamic constraint optimization
problems (DynCOP) the same way DynCSPs
were defined on the basis of CSPs as a series
of COPs P0, P1, . . . that change over time. In
addition to changing variables, values and con-
straints we are concerned with changes of the
cost function as well. In particular, variable in-
stantiations evaluated formerly might now be
judged differently. As this could entail seri-
ous computational problems we try to keep
changes in the cost function monotonic, that
is re-evaluation shall only give lower penalties
than before, i.e. instantiations that become in-
consistent once don?t get consistent later on
again.
3 Basic Dependency Parsing
Using constraint satisfaction techniques for nat-
ural language parsing was introduced first in
(Maruyama, 1990) by defining a constraint de-
pendency grammar (CDG) that maps nicely on
the notion of a CSP. A CDG is a quadruple
(?, R, L, C), where ? is a lexicon of known
words, R is a set of roles of a word. A role rep-
resents a level of language like ?SYN? or ?SEM?.
L is a set of labels for each role (e.g. {?SUBJ?,
?OBJ?}, {?AGENT?,?PATIENT?}), and C is a
constraint grammar consisting of atomic logical
formulas. Now, the only thing that is left in
order to match a CDGs to a CSPs is to define
variables and their possible values. For each
word of an utterance and for each role we al-
locate one variable that can take values of the
form ei,j = ?r, wi, l, wj? with r ? R, wi, wj ? ?
and l ? L. ei,j is called the dependency edge
between word form wi and wj labeled with l on
the description level r. A dependency edge of
the form ei,root is called the root edge. Hence
a dependency tree of an utterance of length n
is a set of dependency edges s = {ei,j | i ?
{1, . . . , n} , j ? {1, . . . , n} ? {root} , i 6= j}.
From this point on parsing natural language
has become a matter of constraint processing
as can be found in the CSP literature (Dechter,
2001).
4 Weighted Dependency Parsing
In (Schro?der, 2002) the foundations of depen-
dency parsing have been carried over to COPs
using weighted constraint dependency grammars
(WCDG), a framework to model language using
all-quantified logical formulas on dependency
structures. Penalties for constraint violations
aren?t necessarily static once, but can be lexi-
calized or computed arithmetically on the ba-
sis of the structure under consideration. The
following constraints are rather typical once re-
stricting the properties of subject edges:
{X:SYN} : SUBJ-init : 0.0 :
X.label = SUBJ ->
( X@cat = NN | X@cat = NE |
X@cat = FM | X@cat = PPER ) &
X^cat = VVFIN;
{X:SYN} : SUBJ-dist : 2.9 / X.length :
X.label = SUBJ -> X.length < 3;
Both constraints have a scope of one depen-
dency edge on the syntax level ({X:SYN}). The
constraint SUBJ-init is a hard constraint stat-
ing that every dependency edge labeled SUBJ
shall have a nominal modifier and a finite
verb as its modifiee. The second constraint
SUBJ-dist is a soft one, such as every edge
with label SUBJ attached more than two words
away induces a penalty calculated by the term
2.9 / X.length. Note, that the maximal edge
length in SUBJ-dist is quite arbitrary and
should be extracted from a corpus automati-
cally as well as the grade of increasing penal-
ization. A realistic grammar consists of about
500 such handwritten constraints like the cur-
rently developed grammar for German (Daum
et al, 2003).
The notation used for constraints in this pa-
per is expressing valid formulas interpretable by
the WCDG constraint system. The following
definitions explain some of the primitives that
are part of the constraint language:
? X is a variable for a dependency edge of the
form ei,j = ?r, wi, l, wj?,
REF
SEM SYN
semantic
constraints syntaxconstraints
reference
constraints
syntax?semantic
constraints
syntax?
reference
constraints
semantic?
reference
constraints
Lexicon
Chunker Tagger
Ontology
Figure 1: Architecture of WCDG
? X@word (X^word) refers to the word form
wi ? ? (wj ? ?)
? X@id (X@id) refers to the position i (j)
? X.label refers to the label l ? L
? X@cat (X^cat) refers to the POS-tag of the
modifier (modifiee)
? root(X^id) ? true iff wj = root
? X.length is defined as |i ? j|.
A complete definition can be found in (Schro?der
et al, 1999).
Figure (1) outlines the overall architecture of
the system consisting of a lexicon component,
ontologies and other external shallow parsing
components, all of which are accessed via con-
straints affecting the internal constraint net-
work as far as variables are in their scope. While
a static parsing model injects all variables into
the ring in Figure (1) once and then waits for
all constraints to let the variable instantiations
settle in a state of equilibrium, a dynamic opti-
mization process will add and remove variables
from the current scope repeatedly.
5 Dynamic Dependency Parsing
As indicated, the basic parsing model in WCDG
is a two stage process: first building up a con-
straint network given an utterance and second
constructing an optimal dependency parse. In a
dynamic system like an incremental dependency
parser these two steps are repeated in a loop
while consuming all bits from the input that
complete a sentence over time. In principle, the
problem of converting the static parsing model
into a dynamic one should only be a question
of repetitive updating the constraint network in
a subtle way. Additionally, information about
the internal state of the ?constraint optimizer?
itself, which is not stored in the constraint net,
shall not get lost during consecutive iterations
as it (a) might participate in the update heuris-
tics of the first phase and (b) the parsing effort
during all previous loops might affect the cur-
rent computation substantially. We will come
back to this argument in Section 8.
Basically, changes to the constraint network
are only allowed after the parser has emitted a
parse tree. This is acceptable if the parser it-
self is interruptible providing the best parse tree
found so far. An interrupt may occur either
from ?outside? or from ?inside? by the parser it-
self taking into account the number of pending
new words not yet added. So it either may in-
tegrate a new word as soon as it arrives or wait
until further hypotheses have been checked. As
transformation based parsing has strong any-
time properties, these heuristics can be imple-
mented as part of a termination criterion be-
tween increments easily.
6 Modeling expectations using
nonspec
6.1 Motivation
Analyzing sentence prefixes with a static parser,
that i.e. is not aware of the sentence being a pre-
fix, will yield at least a penalty for a fragmen-
tary representation. To get such a result at all,
the parser must allow partial parse trees. The
constraints S-init and frag illustrate modeling
of normal and fragmentary dependency trees.
{X:SYN} : S-init : 0.0 :
X.label = S -> root(X^id) &
(X@cat = VVFIN | X@cat = VMFIN |
X@cat = VAFIN | ... );
{X:SYN} : frag : 0.001 :
root(X^id) -> X.label = S;
Constraint S-init restricts all edges with la-
bel S to be finite verbs pointing to root . But if
some dependency edge is pointing to root and
is not labeled with S then constraint frag is vi-
olated and induces a penalty of 0.001. So every
fragment in sentence (2a) that can not be inte-
grated into the rest of the dependency tree will
increase the penalty of the structure by three or-
ders of magnitude. A constraint optimizer will
try to avoid an analysis with an overall penalty
of at least 1?12 and will search for another struc-
ture better than that. Modeling language in
a way that (2a) in fact turns out as the op-
timal solution is therefore difficult. Moreover,
the computational effort could be avoided if a
partial tree is promised to be integrated later
with fewer costs.
The only way to prevent a violation of frag
in WCDG is either by temporarily switching it
off completely or, preferably, by replacing the
root attachment with a nonspec dependency as
shown in (2b), thereby preventing the prerequi-
sites of frag in the first place while remaining
relevant for ?normal? dependency edges.
A prefix-analysis like (2a) might turn out to
be cognitively implausible as well, as humans
expect a proper NP-head to appear as long as
no other evidence forces an adjective to be nom-
inalized. Such a thesis can be modeled using
nonspec dependency edges.
6.2 Definition
We now extend the original definition of
WCDG, so that a dependency tree is devised as
s = {ei,j | i ? {1, . . . , n} ? {?}, j ? {1, . . . , n} ?
{root , ?}, i 6= j}. We will use the notation w?
to denote any unseen word. A dependency edge
modifying w? is written as ei,?, and an edge of
the form e?,i denotes a dependency edge of w?
modifying an already seen word. ei,? and e?,i
are called nonspec dependency edges.
Selective changes to the semantics of the con-
straint language have been made to accom-
plish nonspec dependency edges. So given two
edges ei1,i2 = ?r, wi1 , l?, wi2? and ej1,j2 =
the big blue bouncing
DET ADJ ADJ ADJ
the big blue bouncing w?
DET
ADJ
ADJ
ADJ
(a)
(b)
(c)
the big blue bouncing ball w?
DET ADJ ADJ
ADJ SUBJ
Figure 2: Example sentence prefix
?r, wj1 , l??, wj2? with X ? ei1,i2 and Y ? ej1,j2 :
? X^id = Y^id ? false iff wi2 6= wj2 ? ? ?
wi2 = wj2 = w?, and true otherwise
? X.length ? |i1 ? i2| iff wi1 , wi2 ? ?, and
n+1 iff wi2 = w?, (n: length of the current
sentence prefix)
? X^cat = ?POS ? tag? ? false iff wi2 = w?
? nonspec(X^id) ? true iff wi2 = w?
? spec(X^id) ? true iff wi2 ? ?
6.3 Properties
Although every nonspec dependency in Figure
(2b) points to the same word w?, two nonspec
dependency edges are not taken to be connected
at the top (X^id = Y^id ? false) as we don?t
know yet whether wi and wj will be modifying
the same word in the future.
In general, the above extension to the con-
straint language is reasonable enough to fit into
the notion of static parsing, that is a grammar
tailored for incremental parsing can still be used
for static parsing. An unpleasant consequence
of nonspec is, that more error-cases might occur
in an already existing constraint grammar for
static parsing that was not written with nonspec
dependency edges in mind. Therefore we intro-
duced guard-predicates nonspec() and spec()
that complete those guard-predicates already
part of the language (e.g. root() and exists()).
These must be added by the grammarian to pre-
vent logical error-cases in a constraint formula.
Looking back at the constraints we?ve dis-
cussed so far, the constraints SUBJ-init and
S-init have to be adjusted to become nonspec-
aware because referring to the POS-tag is not
possible if the dependency edge under consider-
ation is of the form ei,? or e?,i. Thus a prefix-
analysis like (2c) is inducing a hard violation
of SUBJ-init. We have to rewrite SUBJ-init to
allow (2c) as follows:
{X:SYN} : SUBJ-init : 0.0 :
X.label = SUBJ ->
( nonspec(X@id) |
X@cat = NN | X@cat = NE |
X@cat = FM | X@cat = PPER) &
( nonspec(X^id) | X^cat = VVFIN );
When all constraints have been checked for
logical errors due to a possible nonspec depen-
dency edge, performance of the modified gram-
mar will not have changed for static parsing but
will accept nonspec dependency edges.
Using the nonspec() predicate, we are able to
write constraints that are triggered by nonspec
edges only being not pertinent for ?normal?
edges. For example we like to penalize nonspec
dependency edges the older they become dur-
ing the incremental course and thereby allow a
cheaper structure to take over seamlessly. This
can easily be achieved with a constraint like
nonspec-dist, similar to SUBJ-dist:
{X:SYN} : nonspec-dist : 1.9 / X.length :
nonspec(X^id) -> X.length < 2;
The effect of nonspec-dist is, that a certain
amount of penalty is caused by ?SYN, the, DET,
w?? and ?SYN, big, ADJ, w?? in (2b). Figure (2c)
illustrates the desired prefix-analysis in the next
loop when nonspec edges become pricey due to
their increased attachment length. In a real-
life constraint grammar (2c) will be optimal ba-
sically because the head of the NP occurred,
therefore overruling every alternative nonspec
dependency edges that crosses the head. The
latter alternative structure will either cause a
projectivity violation with all other non-head
components of the NP that are still linked to the
head or cause an alternative head to be elected
when becoming available.
7 Dynamic Constraint Networks
nonspec dependency edges play an important
role when updating a constraint network to re-
flect the problem change ?Pi . Maintaining the
constraint network in the first phase is crucial
for the overall performance as a more sophisti-
cated strategy to prune edges might compensate
computational effort in the second phase.
Figure (3) illustrates a sentence of three
words being processed one word wi per time-
point ti as follows:
1. for each edge e of the form ej,? or e?,j , (j <
i) recompute penalty f(?e?). If its penalty
drops below ?, then remove e. Otherwise
derive edge e? on the basis of e
2. add new edges ei,? and e?,i to the CN as far
as f(?ei,??) < ? and f(?e?,i?) < ?
3. remove each edge e from the CN if it?s lo-
cal penalty is lower than the penalty of the
best parse so far.
The parameter ? is a penalty threshold that
determines the amount of nonspec edges being
pruned. Any remaining nonspec edge indicates
where the constraint network remains extensible
w1 w?
e1,?
e?,1
t1
w1 w2 w?
e1,?
e1,2
e?,1
e2,1
e2,?
e?,2
t2
w1 w2 w3 w?
e1,?
e1,2
e1,3
e?,1
e2,1
e3,1
e2,?
e2,3
e?,2
e3,2
e3,?
e?,3
t3
Figure 3: Incremental update of a constraint network
and provides an upper estimate of any future
edge derived from it. This holds only if some
prerequisites of monotony are guaranteed:
? The penalty of a parse will always be lower
than each of the penalties on its depen-
dency edges (guaranteed by the multiplica-
tive cost function).
? Each nonspec edge must have a penalty
that is an upper boundary of each depen-
dency edge that will be derived from it:
f(?e?,i1?) >= f(?ei2,i1?) and
f(?ei1,??) >= f(?ei1,i2?) with (i1 < i2).
Only then will pruning of nonspec depen-
dency edges be correct.
? As a consequence the overall penalties of
prefix-analyses degrade monotonically over
time: f(si) >= f(si+1)
Note, that the given strategy to update the
constraint network does not take the struc-
ture of the previous prefix-analysis into account
but only works on the basis of the complete
constraint network. Nevertheless, the previous
parse tree is used as a starting point for the
next optimization step, so that near-by parse
trees will be constructed within a few transfor-
mation steps using the alternatives licensed by
the constraint network.
8 The Optimizer
So far we discussed the first phase of a dynamic
dependency parser building up a series of prob-
lems P0, P1, . . . changing Pi using ?Pi+1 in terms
of maintaining a dynamic constraint network.
In the second phase ?the optimizer? tries to ac-
commodate to those changes by constructing
Si+1 on the basis of Si and Pi+1.
WCDG offers a decent set of methods to com-
pute the second phase, one of which implements
a guided local search (Daum and Menzel, 2002).
The key idea of GLS is to add a heuristics
sitting on top of a local search procedure by in-
troducing weights for each possible dependency
edge in the constraint network. Initially being
zero, weights are increased steadily if a local
search settles in a local optimum. By augment-
ing the cost function f with these extra weights,
further transformations are initiated along the
gradient of f . Thus every weight of a depen-
dency edge resembles an custom-tailored con-
straint whose penalty is learned during search.
The question now to be asked is, how weights
acquired during the incremental course of pars-
ing influence GLS. The interesting property
is that the weights of dependency edges inte-
grated earlier will always tend to be higher than
weights of most recently introduced dependency
edges as a matter of saturation. Thus keeping
old weights will prevent GLS from changing old
dependency edges and encourage transforming
newer dependency edges first. Old dependency
edges will not be transformed until more re-
cent constraint violations have been removed or
old structures are strongly deprecated recently.
This is a desirable behavior as it stabilizes for-
mer dependency structures with no extra provi-
sions to the base mechanism. Transformations
will be focused on the most recently added de-
pendency edges. This approach is comparable
to a simulated annealing heuristics where trans-
formations are getting more infrequent due to a
declining ?temperature?.
Another very successful implementation of
?the optimizer? in WCDG is called Frobbing
(Foth et al, 2000) which is a transformation
based parsing technique similar to taboo search.
One interesting feature of Frobbing is its ability
to estimate an upper boundary of the penalty of
any structure using a certain dependency edge
and a certain word form. In an incremental
parsing mode the penalty limit of a nonspec de-
pendency edge will then be an estimate of any
structure derived from it and thereby provide a
good heuristics to prune nonspec edges falling
beyond ? during the maintenance of the con-
straint network.
9 Conclusion
Incremental parsing using weighted constraint
optimization has been classified as a special case
of dynamic dependency parsing.
The idea of nonspec dependency edges has
been described as a means of expressing ex-
pectations during the incremental process. We
have argued that (a) nonspec dependency edges
are more adequate to model prefix-analyses and
(b) offer a computational advantage compared
to a parser that models the special situation of
a sentence prefix only by means of violated con-
straints.
While completing the notion of dynamic de-
pendency parsing, we assessed the consequences
of an incremental parsing mode to the most
commonly used optimization methods used in
WCDG.
Further research will need to add the notion
of DynCSP to the WCDG system as well as
an adaption and completion of an existing con-
straint grammar. This will allow an in-depth
evaluation of dynamic dependency parsing with
and without nonspec dependency edges given
the optimization methods currently available.
Experiments will be conducted to acquire pars-
ing times per increment that are then compared
to human reading times.
Acknowledgments
This research has been partially supported by
Deutsche Forschungsgemeinschaft under grant
Me 1472/4-1.
References
Michael Daum and Wolfgang Menzel. 2002.
Parsing natural language using guided local
search. In F. van Harmelen, editor, Proc. 15th
European Conference on Artificial Intelli-
gence, Amsterdam. IOS Press.
Michael Daum, Kilian Foth, and Wolfgang Men-
zel. 2003. Constraint based integration of
deep and shallow parsing techniques. In Pro-
ceedings 11th Conference of the European
Chapter of the ACL, Budapest, Hungary.
Rina Dechter and Avi Dechter. 1988. Be-
lief Maintenance in Dynamic Constraint Net-
works. In 7th Annual Conference of the
American Association of Artificial Intelli-
gence, pages 37?42.
Rina Dechter. 2001. Constraint Processing.
Morgan Kaufmann, September.
Kilian Foth, Wolfgang Menzel, and Ingo
Schro?der. 2000. A transformation-based
parsing technique with anytime properties.
In Proc. 4th International Workshop on Pars-
ing Technologies, pages 89?100, Trento, Italy.
Eugene C. Freuder and Richard J. Wallace.
1989. Partial constraint satisfaction. In
Proc. 11th International Joint Conference on
Artificial Intelligence (IJCAI-89), volume 58,
pages 278?283, Detroit, Michigan, USA.
A. K. Mackworth. 1977. Consistency in net-
works of relations. Artificial Intelligence.
8:99-118.
Hiroshi Maruyama. 1990. Structure disam-
biguation with constraint propagation. In
Proc. the 28th Annual Meeting of the ACL,
pages 31?38, Pittsburgh.
U. Montanari. 1974. Networks of constraints:
Fundamental properties and applications to
picture processing. Inform. Sci., 7:95-132.
Ingo Schro?der, Kilian A. Foth, and Michael
Schulz. 1999. [X]cdg Benutzerhandbuch.
Technical Report Dawai-HH-13, Universita?t
Hamburg.
Ingo Schro?der. 2002. Natural Language Parsing
with Graded Constraints. Ph.D. thesis, Dept.
of Computer Science, University of Hamburg,
Germany.
David Waltz. 1975. Understanding line draw-
ings of scenes with shadows. In P. H. Win-
ston, editor, The Psychology of Computer Vi-
sion. McGraw?Hill, New York.
Mats Wire?n. 1993. Bounded incremental pars-
ing. In Proc. 6th Twente Workshop on
Language Technology, pages 145?156, En-
schede/Netherlands.
