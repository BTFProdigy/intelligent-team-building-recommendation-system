Inheritance and the CCG Lexicon
Mark McConville
Institute for Communicating and Collaborative Systems
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, Scotland
Mark.McConville@ed.ac.uk
Abstract
I propose a uniform approach to the elim-
ination of redundancy in CCG lexicons,
where grammars incorporate inheritance
hierarchies of lexical types, defined over
a simple, feature-based category descrip-
tion language. The resulting formalism is
partially ?constraint-based?, in that the cat-
egory notation is interpreted against an un-
derlying set of tree-like feature structures.
I argue that this version of CCG subsumes
a number of other proposed category no-
tations devised to allow for the construc-
tion of more efficient lexicons. The for-
malism retains desirable properties such
as tractability and strong competence, and
provides a way of approaching the prob-
lem of how to generalise CCG lexicons
which have been automatically induced
from treebanks.
1 The CCG formalism
In its most basic conception, a CCG over alpha-
bet ? of terminal symbols is an ordered triple
?A,S, L?, where A is an alphabet of saturated cat-
egory symbols, S is a distinguished element of A,
and L is a lexicon, i.e. a mapping from ? to cate-
gories over A. The set of categories over alphabet
A is the closure of A under the binary infix con-
nectives / and \ and the associated ?modalities? of
Baldridge (2002). For example, assuming the sat-
urated category symbols ?S? and ?NP?, here is a
simple CCG lexicon (modalities omitted):
John ` NP(1)
Mary ` NP
loves ` (S\NP)/NP
The combinatory projection of a CCG lexicon is
its closure under a finite set of resource-sensitive
combinatory operations such as forward applica-
tion (2), backward application (3), forward type
raising (4), and forward composition (5):
X/Y Y ? X(2)
Y X\Y ? X(3)
X ? Y/(Y \X)(4)
X/Y Y/Z ? X/Z(5)
CCG ?A,S, L? over alphabet ? generates string
s ? ?? just in case ?s, S? is in the combinatory
projection of lexicon L. The derivation in Figure
1 shows that CCG (1) generates the sentence John
loves Mary, assuming that ?S? is the distinguished
symbol, and where >T, >B and > denote in-
stances of forward raising, forward composition
and forward application respectively:
John loves Mary
NP (S\NP)/NP NP
>T
S/(S\NP)
>B
S/NP
>
S
Figure 1: A CCG derivation
2 Lexical redundancy in CCG
CCG has many advantages both as a theory of
human linguistic competence and as a tool for
practical natural language processing applications
(Steedman, 2000). However, in many cases de-
velopment has been hindered by the absence of
an agreed uniform approach to eliminating redun-
dancy in CCG lexicons. This poses a particular
problem for a radically lexicalised formalism such
as CCG, where it is customary to handle bounded
1
dependency constructions such as case, agreement
and binding by means of multiple lexical cate-
gory assignments. Take for example, the language
schematised in Table 1. This fragment of English,
though small, exemplifies certain non-trivial as-
pects of case and number agreement:
John John
he loves me
the girl you
girls him
I us
you love them
we the girl
they girls
girls girls
Table 1: A fragment of English
The simplest CCG lexicon for this fragment is pre-
sented in Table 2:
John ` NPsgsbj, NPobj
girl ` Nsg
s ` Npl\Nsg, NPplsbj\Nsg, NPobj\Nsg
the ` NPsgsbj/Nsg, NPobj/Nsg,
NPplsbj/Npl, NPobj/Npl
I,we, they ` NPplsbj
me, us, them, him ` NPobj
you ` NPplsbj, NPobj
he ` NPsgsbj
love ` (S\NPplsbj)/NPobj
s ` ((S\NPsgsbj)/NPobj)\((S\NP
pl
sbj)/NPobj)
Table 2: A CCG lexicon
This lexicon exhibits a number of multiple cate-
gory assignments: (a) the proper noun John and
the second person pronoun you are each assigned
to two categories, one for each case distinction;
(b) the plural suffix -s is assigned to three cate-
gories, depending on both the case and ?bar level?
of the resulting nominal; and (c) the definite arti-
cle the is assigned to four categories, one for each
combination of case and number agreement dis-
tinctions. Since in each of these three cases there
is no pretheoretical ambiguity involved, it is clear
that this lexicon violates the following efficiency-
motivated ideal on human language lexicons, in
the Chomskyan sense of locus of non-systematic
information:
ideal of functionality a lexicon is ideally a func-
tion from morphemes to category labels, modulo
genuine ambiguity
Another efficiency-motivated ideal which the
CCG lexicon in Table 2 can be argued to violate
is the following:
ideal of atomicity a lexicon is a mapping from
morphemes ideally to atomic category labels
In the above example, the transitive verb love is
mapped to the decidedly non-atomic category la-
bel (S\NPplsbj)/NPobj. Lexicons which violate the
criteria of functionality and atomicity are not just
inefficient in terms of storage space and develop-
ment time. They also fail to capture linguistically
significant generalisations about the behaviour of
the relevant words or morphemes.
The functionality and atomicity of a CCG lexi-
con can be easily quantified. The functionality ra-
tio of the lexicon in Table 2, with 22 lexical entries
for 14 distinct morphemes, is 2214 = 1.6. The atom-
icity ratio is calculated by dividing the number of
saturated category symbol-tokens by the number
of lexical entries, i.e. 3622 = 1.6.
Various, more or less ad hoc generalisations of
the basic CCG category notation have been pro-
posed with a view to eliminating these kinds of
lexical redundancy. One area of interest has in-
volved the nature of the saturated category sym-
bols themselves. Bozsahin (2002) presents a ver-
sion of CCG where saturated category symbols
are modified by unary modalities annotated with
morphosyntactic features. The features are them-
selves ordered according to a language-particular
join semi-lattice. This technique, along with the
insistence that lexicons of agglutinating languages
are necessarily morphemic, allows generalisations
involving the morphological structure of nouns
and verbs in Turkish to be captured in an elegant,
non-redundant format. Erkan (2003) generalises
this approach, modelling saturated category labels
as typed feature structures, constrained by under-
specified feature structure descriptions in the usual
manner.
Hoffman (1995) resolves other violations of the
ideal of functionality in CCG lexicons for lan-
guages with ?local scrambling? constructions by
means of ?multiset? notation for unsaturated cat-
egories, where scope and direction of arguments
can be underspecified. For example, a multiset
category label like S{\NPsbj, \NPobj} is to be un-
derstood as incorporating both (S\NPsbj)\NPobj
and (S\NPobj)\NPsbj.
Computational implementations of the CCG
formalism, including successive versions of the
2
Grok/OpenCCG system1, have generally dealt
with violations of the ideal of atomicity by allow-
ing for the definition of macro-style abbreviations
for unsaturated categories, e.g. using the macro
?TV? as an abbreviation for (S\NPsbj)/NPobj.
One final point of note involves the project re-
ported in Beavers (2004), who implements CCG
within the LKB system, i.e. as an application of
the Typed Feature Structure Grammar formalism
of Copestake (2002), with the full apparatus of un-
restricted typed feature structures, default inheri-
tance hierarchies, and lexical rules.
3 Type-hierarchical CCG
One of the aims of the project reported here has
been to take a bottom-up approach to the prob-
lem of redundancy in CCG lexicons, adding just
enough formal machinery to allow the relevant
generalisations to be formulated, whilst retaining a
restrictive theory of human linguistic competence
which satisfies the ?strong competence? require-
ment, i.e. the competence grammar and the pro-
cessing grammar are identical.
I start with a generalisation of the CCG for-
malism where the alphabet of saturated category
symbols is organised into a ?type hierarchy? in
the sense of Carpenter (1992), i.e. a weak order
?A,vA?, where A is an alphabet of types, vA is
the ?subsumption? ordering on A (with a least ele-
ment), and every subset of A with an upper bound
has a least upper bound. An example type hi-
erarchy is in Figure 2, where for example types
?Nomsg? and ?NP? are compatible since they have
a non-empty set of upper bounds, the least upper
bound (or ?unifier?) being ?NPsg?.
NPsgsbj NP
pl
sbj NP
sg
obj NP
pl
obj
QQ
Q
QQ
Q
PPPP
PP



NPsbj NPobj NPsg NPpl Nsg Npl
## LL HHH
H
PPPP
PPP
  PPPP
P
HHH
H
  
NP Nomsg Nompl N


   @@ PPPP
PPNomS
!!!! PPPP
top
Figure 2: Type hierarchy of saturated categories
A type-hierarchical CCG (T-CCG) over alpha-
bet ? is an ordered 4-tuple ?A,vA, S, L?, where
1http://openccg.sourceforge.net
?A,vA? is a type hierarchy of saturated category
symbols, S is a distinguished element of A, and
lexicon L is a mapping from ? to categories over
A. Given an appropriate vA-compatibility rela-
tion on the categories over A, the combinatory
projection of T-CCG ?A,vA, S, L? can again be
defined as the closure of L under the CCG com-
binatory operations, assuming that variable Y in
the type raising rule (4) is restricted to maximally
specified categories.
The T-CCG lexicon in Table 3, in tandem with
the type hierarchy in Figure 2, generates the frag-
ment of English in Table 1:
John ` NPsg
girl ` Nsg
s ` Nompl\Nsg
the ` NPsg/Nsg, NPpl/Npl
I,we, they ` NPplsbj
me, us, them ` NPplobj
you ` NPpl
he ` NPsgsbj
him ` NPsgobj
love ` (S\NPplsbj)/NPobj
s ` ((S\NPsgsbj)/NPobj)\((S\NP
pl
sbj)/NPobj)
Table 3: A T-CCG lexicon
Using this lexicon, the sentence girls love John is
derived as in Figure 3:
girl s love John
Nsg Nompl\Nsg (S\NPplsbj)/NPobj NPsg
<
Nompl
>T
S/(S\Nompl)
>B
S/NPobj
>
S
Figure 3: A T-CCG derivation
The T-CCG lexicon in Table 3 comes closer to sat-
isfying the ideal of functionality than does the lex-
icon in Table 2. While the latter has a functionality
ratio of 1.6, the former?s is 1614 = 1.1.
This improved functionality ratio results from
the underspecification of saturated category sym-
bols inherent in the subsumption relation. For ex-
ample, whereas the proper noun John is assigned
to two distinct categories in the lexicon in Table
2, in the T-CCG lexicon it is assigned to a sin-
gle non-maximal type ?NPsg? which subsumes the
two maximal types ?NPsgsbj? and ?NP
sg
obj?. In other
3
words, the phenomenon of case syncretism in En-
glish proper nouns is captured by having a general
singular noun phrase type, which subsumes a plu-
rality of case distinctions.
The T-CCG formalism is equivalent to the ?mor-
phosyntactic CCG? formalism of Bozsahin (2002),
where features are ordered in a join semi-lattice.
Any generalisation which can be expressed in a
morphosyntactic CCG can also be expressed in a
T-CCG, since any lattice of morphosyntactic fea-
tures can be converted into a type hierarchy. In
addition, T-CCG is equivalent to the formalism
described in Erkan (2003), where saturated cat-
egories are modelled as typed feature structures.
Any lexicon from either of these formalisms can
be translated into a T-CCG lexicon whose func-
tionality ratio is either equivalent or lower.
4 Inheritance-driven CCG
A second generalisation of the CCG formalism in-
volves adding a second alphabet of non-terminals,
in this case a set of ?lexical types?. The lexical
types are organised into an ?inheritance hierarchy?,
constrained by expressions of a simple feature-
based category description language, inspired by
previous attempts to integrate categorial grammars
and unification-based grammars, e.g. Uszkoreit
(1986) and Zeevat et al (1987).
4.1 Simple category descriptions
The set of simple category descriptions over al-
phabet A of saturated category symbols is defined
as the smallest set ? such that:
1. A ? ?
2. for all ? ? {f, b}, (SLASH ?) ? ?
3. for all ? ? ?, (ARG ?) ? ?
4. for all ? ? ?, (RES ?) ? ?
Note that category descriptions may be infinitely
embedded, in which case they are considered to
be right-associative, e.g. RES ARG RES SLASH f.
A simple category description like (SLASH f) or
(SLASH b) denotes the set of all expressions which
seek their argument to the right/left. A description
of the form (ARG ?) denotes the set of expressions
which take an argument of category ?, and one
like (RES ?) denotes the set of expressions which
combine with an argument to yield an expression
of category ?.
Complex category descriptions are simply sets
of simple category descriptions, where the as-
sumed semantics is simply that of conjunction.
4.2 Lexical inheritance hierarchies
Lexical inheritance hierarchies (Flickinger, 1987)
are type hierarchies where each type is associated
with a set of expressions drawn from some cate-
gory description language ?. Formally, they are
ordered triples ?B,vB, b?, where ?B,vB? is a
type hierarchy, and b is a function fromB to ?(?).
An example lexical inheritance hierarchy over
the set of category descriptions over the alpha-
bet of saturated category symbols in Table 2 is
presented in Figure 4. The intuition underlying
these (monotonic) inheritance hierarchies is that
instances of a type must satisfy all the constraints
associated with that type, as well as all the con-
straints it inherits from its supertypes.
verbpl
RES ARG Nompl
 
verbsg
RES ARG Nomsg
detsg
ARG NomsgRES Nomsg



detpl
ARG NomplRES Nompl
BB
BB
suffixsg
ARG verbplRES verbsg

suffixpl
ARG NsgRES Nompl
CC
CC
verb
SLASH f
ARG NPobjRES SLASH b
RES ARG NPsbjRES RES S

det
SLASH f
ARG N
RES NP
suffix
SLASH b
HHH
HHH
top
Figure 4: A lexical inheritance hierarchy
This example hierarchy is a single inheritance hi-
erarchy, since every lexical type has no more than
one immediate supertype. However, multiple in-
heritance hierarchies are also allowed, where a
given type can inherit constraints from two super-
types, neither of which subsumes the other.
4.3 I-CCGs
An inheritance-driven CCG (I-CCG) over alpha-
bet ? is an ordered 7-tuple ?A,vA, B,vB, b,
S, L?, where ?A,vA? is a type hierarchy of sat-
urated category symbols, ?B,vB, b? is an inheri-
tance hierarchy of lexical types over the set of cat-
egory descriptions overA?B, S is a distinguished
symbol inA, and lexicon L is a function from ? to
A ? B. Given an appropriate vA,B-compatibility
relation on the categories overA?B, the combina-
tory projection of I-CCG ?A,vA, B,vB, b, S, L?
can again be defined as the closure of L under the
4
CCG combinatory operations.
The I-CCG lexicon in Table 4, along with the
type hierarchy of saturated category symbols in
Figure 2 and the inheritance hierarchy of lexical
types in Figure 4, generates the fragment of En-
glish in Table 1. Using this lexicon, the sentence
John ` NPsg
girl ` Nsg
s ` suffix
the ` det
I,we, they ` NPplsbj
me, us, them ` NPplobj
you ` NPpl
he ` NPsgsbj
him ` NPsgobj
love ` verbpl
Table 4: An I-CCG lexicon
girls love John is derived as in Figure 5, where
derivational steps involve ?cache-ing out? sets of
constraints from lexical types.
girl s love John
Nsg suffix verbpl NPsg
SLASH b RES ARG Nompl
suffixpl verb
ARG Nsg SLASH f
RES Nompl ARG NPobj
< RES SLASH b
Nompl RES ARG NPsbj
>T RES RES S
RES S
SLASH f
ARG RES S
ARG ARG Nompl
ARG SLASH b
>B
RES S
ARG NPobj
SLASH f
>
S
Figure 5: An I-CCG derivation
This derivation relies on a version of the CCG
combinatory rules defined in terms of the I-CCG
category description language. For example, for-
ward application is expressed as follows ? for all
compex category descriptions ? and ? such that
(SLASH b) 6? ?, and {? | (ARG ?) ? ?} ? ? is
compatible, the following is a valid inference:
? ?
>
{? | (RES ?) ? ?}
The functionality ratio of the I-CCG lexicon in Ta-
ble 4 is 1414 = 1 and the atomicity ratio is 1414 = 1.
In other words, the lexicon is maximally non-
redundant, since all the linguistically significant
generalisations are encodable within the lexical in-
heritance hierarchy.
The optimal atomicity ratio of the I-CCG lexi-
con is a direct result of the introduction of lexical
types. In the T-CCG lexicon in Table 3, the transi-
tive verb love was assigned to a non-atomically la-
belled category (S\NPplsbj)/NPobj. In the I-CCG?s
inheritance hierarchy in Figure 4, there is a lexical
type ?verbpl? which inherits six constraints whose
conjunction picks out exactly the same category.
It is with this atomic label that the verb is paired
in the I-CCG lexicon in Table 4.
The lexical inheritance hierarchy also has a role
to play in constructing lexicons with optimal func-
tionality ratios. The T-CCG lexicon in Table 3
assigned the definite article to two distinct cate-
gories, one for each grammatical number distinc-
tion. The I-CCG utilises the disjunction inherent
in inheritance hierarchies to give each of these a
common supertype ?det?, which is associated with
the properties all determiners share.
Finally, the I-CCG formalism can be argued
to subsume the multiset category notation of
Hoffman (1995), in the sense that every mul-
tiset CCG lexicon can be converted into an I-
CCG lexicon with an equivalent or better func-
tionality ratio. Recall that Hoffman uses gener-
alised category notation like S{\NPsbj, \NPobj}
to subsume two standard CCG category labels
(S\NPsbj)\NPobj and (S\NPobj)\NPsbj. Again it
should be clear that this is just another way of
representing disjunction in a categorial lexicon,
and can be straightforwardly converted into a lexi-
cal inheritance hierarchy over I-CCG category de-
scriptions.
5 Semantics of the category notation
In the categorial grammar tradition initiated by
Lambek (1958), the standard way of providing a
semantics for category notation defines the deno-
tation of a category description as a set of strings
of terminal symbols. Thus, assuming an alphabet
? and a denotation function [[. . .]] from the sat-
urated category symbols to ?(?), the denotata of
unsaturated category descriptions can be defined
as follows, assuming that the underlying logic is
simply that of string concatenation:
[[?/?]] = {s | ?s? ? [[?]], ss? ? [[?]]}(6)
[[?\?]] = {s | ?s? ? [[?]], s?s ? [[?]]}
This suggests an obvious way of interpreting the
I-CCG category notation defined above. Let?s
5
start by assuming that, given some I-CCG ?A,vA,
B,vB, b, S, L? over alphabet ?, there is a deno-
tation function [[. . .]] from the maximal types in
the hierarchy of saturated categories ?A,vA? to
?(?). For all non-maximal saturated category
symbols ? in A, the denotation of ? is then the
set of all strings in any of ??s subcategories, i.e.
[[?]] = ??vA?[[?]]. The denotata of the simple
category descriptions can be defined by universal
quantification over the set of simple category de-
scriptions ?:
? [[SLASH f]] = ??,???[[?/?]]
? [[SLASH b]] = ??,???[[?\?]]
? [[ARG ?]] = ????[[?/?]] ? [[?\?]]
? [[RES ?]] = ????[[?/?]] ? [[?\?]]
This just leaves the simple descrip-
tions which consist of a type in the
lexical inheritance hierarchy ?B,vB,
b?. If we define the constraint set of some
lexical type ? ? B as the set ? of all category
descriptions either associated with or inherited
by ?, then the denotation of ? is defined as
?
???[[?]].
Unfortunately, this approach to interpreting I-
CCG category descriptions is insufficient, since
the logic underlying CCG is not simply the logic
of string concatenation, i.e. CCG allows a limited
degree of permutation by dint of the crossed com-
position and substitution operations. In fact, there
appears to be no categorial type logic, in the sense
of Moortgat (1997), for which the CCG combi-
natory operations provide a sound and complete
derivation system, even in the resource-sensitive
system of Baldridge (2002). An alternative ap-
proach involves interpreting I-CCG category de-
scriptions against totally well-typed, sort-resolved
feature structures, as in the HPSG formalism of
Pollard and Sag (1994).
Given some type hierarchy ?A,vA? of saturated
category symbols and some lexical inheritance hi-
erarchy ?B,vB, b?, we define a class of ?category
models?, i.e. binary trees where every leaf node
carries a maximal saturated category symbol in A,
every non-leaf node carries a directional slash, and
every branch is labelled as either a ?result? or an
?argument?. In addition, nodes are optionally la-
belled with maximal lexical types from B. Note
that since only maximal types are permitted in a
model, they are by definition sort-resolved. As-
suming the hierarchies in Tables 2 and 4, an ex-
ample category model is given in Figure 6, where
arcs by convention point downwards:
S
  
R
NPplsbj
@@
A
\

R
NPsgobj
QQ
Q A
/ : verbpl
Figure 6: A category model
Given some inheritance hierarchy ?B,vB, b? of
lexical types, not all category models whose nodes
are labelled with maximal types from B are ?well-
typed?. In fact, this property is restricted to those
models where, if node n carries lexical type ?,
then every category description in the constraint
set of ? is satisfied from n. Note that the root
of the model in Figure 6 carries the lexical type
?verbpl?. Since all six constraints inherited by this
type in Figure 4 are satisfied from the root, and
since no other lexical types appear in the model,
we can state that the model is well-typed.
In sum, given an appropriate satisfaction rela-
tion between well-typed category models and I-
CCG category descriptions, along with a definition
of the CCG combinatory operations in terms of
category models, it is possible to provide a formal
interpretation of the language of I-CCG category
descriptions, in the same way as unification-based
formalisms like HPSG ground attribute-value no-
tation in terms of underlying totally well-typed,
sort-resolved feature structure models. Such a se-
mantics is necessary in order to prove the correct-
ness of eventual I-CCG implementations.
6 Extending the description language
The I-CCG formalism described here involves a
generalisation of the CCG category notation to in-
corporate the concept of lexical inheritance. The
primary motivation for this concerns the ideal of
non-redundant encoding of lexical information in
human language grammars, so that all kinds of lin-
guistically significant generalisation can be cap-
tured somewhere in the grammar. In order to fulfil
this goal, the simple category description language
defined above will need to be extended somewhat.
For example, imagine that we want to specify the
6
set of all expressions which take an NPobj argu-
ment, but not necessarily as their first argument,
i.e. the set of all ?transitive? expressions:
ARG NPobj(7)
? RES ARG NPobj
? RES RES ARG NPobj
? . . .
It should be clear that this category is not finitely
specifiable using the I-CCG category notation.
One way to allow such generalisations to be
made involves incorporating the ? modal itera-
tion operator used in Propositional Dynamic Logic
(Harel, 1984) to denote an unbounded number
of arc traversals in a Kripke structure. In other
words, category description (RES* ?) is satisfied
from node n in a model just in case some finite se-
quence of result arcs leads from n to a node where
? is satisfied. In this way, the set of expressions
taking an NPobj argument is specified by means of
the category description RES* ARG NPobj.
7 Computational aspects
At least as far as the I-CCG category notation de-
fined in section 4.1 is concerned, it is a straight-
forward task to take the standard CKY approach
to parsing with CCGs (Steedman, 2000), and gen-
eralise it to take a functional, atomic I-CCG lex-
icon and ?cache out? the inherited constraints on-
line. As long as the inheritance hierarchy is non-
recursive and can thus be theoretically cached out
into a finite lexicon, the parsing problem remains
worst-case polynomial.
In addition, the I-CCG formalism satisfies
the ?strong competence? requirement of Bresnan
(1982), according to which the grammar used by
or implicit in the human sentence processor is
the competence grammar itself. In other words,
although the result of cache-ing out particularly
common lexical entries will undoubtedly be part
of a statistically optimised parser, it is not essen-
tial to the tractability of the formalism.
One obvious practical problem for which the
work reported here provides at least the germ of
a solution involves the question of how to gener-
alise CCG lexicons which have been automatically
induced from treebanks (Hockenmaier, 2003). To
take a concrete example, Cakici (2005) induces a
wide coverage CCG lexicon from a 6000 sentence
dependency treebank of Turkish. Since Turkish is
a pro-drop language, every transitive verb belongs
to both categories (S\NPsbj)\NPobj and S\NPobj.
However, data sparsity means that the automati-
cally induced lexicon assigns only a small minor-
ity of transitive verbs to both classes. One possi-
ble way of resolving this problem would involve
translating the automatically induced lexicon into
sets of fully specified I-CCG category descrip-
tions, generating an inheritance hierarchy of lex-
ical types from this lexicon (Sporleder, 2004), and
applying some more precise version of the follow-
ing heuristic: if a critical mass of words in the au-
tomatically induced lexicon belong to both CCG
categories X and Y , then in the derived I-CCG
lexicon assign all words belonging to either X or
Y to the lexical type which functions as the great-
est lower bound of X and Y in the lexical inheri-
tance hierarchy.
8 Acknowledgements
The author is indebted to the following people for
providing feedback on various drafts of this paper:
Mark Steedman, Cem Bozsahin, Jason Baldridge,
and three anonymous EACL reviewers.
References
Baldridge, J. (2002). Lexically Specified Deriva-
tional Control in Combinatory Categorial
Grammar. PhD thesis, University of Edinburgh.
Beavers, J. (2004). Type-inheritance Combina-
tory Categorial Grammar. In Proceedings of
the 20th International Conference on Compu-
tational Linguistics, University of Geneva.
Bozsahin, C. (2002). The combinatory morphemic
lexicon. Computational Linguistics, 28(2):145?
186.
Bresnan, J., editor (1982). The Mental Represen-
tation of Grammatical Relations. MIT Press,
Cambridge MA.
Cakici, R. (2005). Automatic induction of a CCG
grammar for Turkish. In Proceedings of the Stu-
dent Research Workshop, 43rd Annual Meeting
of the Association for Computational Linguis-
tics, University of Michigan, pages 73?78.
Carpenter, B. (1992). The Logic of Typed Fea-
ture Structures. Cambridge Tracts in Theoret-
ical Computer Science. Cambridge University
Press.
Copestake, A. (2002). Implementing Typed Fea-
ture Structure Grammars. CSLI Publications,
Stanford CA.
7
Erkan, G. (2003). A Type System for Combina-
tory Categorial Grammar. Master?s thesis, Mid-
dle East Technical University, Ankara.
Flickinger, D. P. (1987). Lexical Rules in the Hi-
erarchical Lexicon. PhD thesis, Stanford Uni-
versity.
Harel, D. (1984). Dynamic logic. In Gabbay, D.
and Guenthner, F., editors, Handbook of Philo-
sophical Logic, Volume 2, pages 497?604. Rei-
del, Dordrecht.
Hockenmaier, J. (2003). Data and Models for
Statistical Parsing with Combinatory Catego-
rial Grammar. PhD thesis, University of Ed-
inburgh.
Hoffman, B. (1995). The Computational Analy-
sis of the Syntax and Interpretation of ?Free?
Word Order in Turkish. PhD thesis, University
of Pennsylvania.
Lambek, J. (1958). The Mathematics of Sentence
Structure. American Mathematical Monthly,
65:154?170.
Moortgat, M. (1997). Categorial type logics. In
van Benthem, J. and ter Meulen, A., editors,
Handbook of Logic and Language, pages 93?
177. North Holland, Amsterdam, NL.
Pollard, C. J. and Sag, I. A. (1994). Head-Driven
Phrase Structure Grammar. The University of
Chicago Press.
Sporleder, C. (2004). Discovering Lexical Gener-
alisations: A Supervised Machine Learning Ap-
proach to Inheritance Hierarchy Construction.
PhD thesis, University of Edinburgh.
Steedman, M. (2000). The Syntactic Process. MIT
Press, Cambridge MA.
Uszkoreit, H. (1986). Categorial Unification
Grammars. In Proceedings of the 11th Inter-
national Conference on Computational Linguis-
tics, Bonn, pages 187?194.
Zeevat, H., Klein, E., and Calder, J. (1987). Uni-
fication Categorial Grammar. In Haddock, N.,
Klein, E., and Morrill, G., editors, Categorial
Grammar, Unification Grammar and Parsing,
Working Papers in Cognitive Science. Centre
for Cognitive Science, University of Edinburgh.
8
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 112?119,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Extracting a verb lexicon for deep parsing from FrameNet
Mark McConville and Myroslava O. Dzikovska
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, Scotland
{Mark.McConville,M.Dzikovska}@ed.ac.uk
Abstract
We examine the feasibility of harvesting
a wide-coverage lexicon of English verbs
from the FrameNet semantically annotated
corpus, intended for use in a practical natural
language understanding (NLU) system. We
identify a range of constructions for which
current annotation practice leads to prob-
lems in deriving appropriate lexical entries,
for example imperatives, passives and con-
trol, and discuss potential solutions.
1 Introduction
Although the lexicon is the primary source of infor-
mation in lexicalised formalisms such as HPSG or
CCG, constructing one manually is a highly labour-
intensive task. Syntactic lexicons have been derived
from other resources ? the LinGO ERG lexicon
(Copestake and Flickinger, 2000) contains entries
extracted from ComLex (Grishman et al, 1994),
and Hockenmaier and Steedman (2002) acquire a
CCG lexicon from the Penn Treebank. However,
one thing these resources lack is information on how
the syntactic subcategorisation frames correspond to
meaning.
The output representation of many ?deep? wide
coverage parsers is therefore limited with respect to
argument structure ? sense distinctions are strictly
determined by syntactic generalisations, and are
not always consistent. For example, in the logi-
cal form produced by the LinGO ERG grammar,
the verb end can have one of two senses depend-
ing on its subcategorisation frame: end v 1 rel
or end v cause rel, corresponding to the cel-
ebrations ended and the storm ended the celebra-
tions respectively. Yet a very similar verb, stop, has
a single sense, stop v 1 rel, for both the cele-
brations stopped and the storm stopped the celebra-
tions. There is no direct connection between these
different verbs in the ERG lexicon, even though
they are intuitively related and are listed as belong-
ing to the same or related word classes in semantic
lexicons/ontologies such as VerbNet (Kipper et al,
2000) and FrameNet (Baker et al, 1998).
If the output of a deep parser is to be used with
a knowledge representation and reasoning compo-
nent, for example in a dialogue system, then we need
a more consistent set of word senses, linked by spec-
ified semantic relations. In this paper, we investi-
gate how straightforward it is to harvest a compu-
tational lexicon containing this kind of information
from FrameNet, a semantically annotated corpus of
English. In addition, we consider how the FrameNet
annotation system could be made more transparent
for lexical harvesting.
Section 2 introduces the FrameNet corpus, and
section 3 discusses the lexical information required
by frame-based NLU systems, with particular em-
phasis on linking syntactic and semantic structure.
Section 4 presents the algorithm which converts the
FrameNet corpus into a frame-based lexicon, and
evaluates the kinds of entries harvested in this way.
We then discuss a number of sets of entries which
are inappropriate for inclusion in a frame-based lex-
icon: (a) ?subjectless? entries; (b) entries derived
from passive verbs; (c) entries subcategorising for
modifiers; and (d) entries involving ?control? verbs.
112
2 FrameNet
FrameNet1 is a corpus of English sentences an-
notated with both syntactic and semantic informa-
tion. Underlying the corpus is an ontology of
795 ?frames? (or semantic types), each of which
is associated with a set of ?frame elements? (or
semantic roles). To take a simple example, the
Apply heat frame describes a situation involving
frame elements such as a COOK, some FOOD, and
a HEATING INSTRUMENT. Each frame is, in addi-
tion, associated with a set of ?lexical units? which
are understood as evoking it. For example, the
Apply heat frame is evoked by such verbs as
bake, blanch, boil, broil, brown, simmer, steam, etc.
The FrameNet corpus proper consists of 139,439
sentences (mainly drawn from the British National
Corpus), each of which has been hand-annotated
with respect to a particular target word in the sen-
tence. Take the following example: Matilde fried
the catfish in a heavy iron skillet. The process of an-
notating this sentence runs as follows: (a) identify a
target word for the annotation, for example the main
verb fried; (b) identify the semantic frame which is
evoked by the target word in this particular sentence
? in this case the relevant frame is Apply heat;
(c) identify the sentential constituents which realise
each frame element associated with the frame, i.e.:
[COOK Matilde] [Apply heat fried] [FOOD the
catfish] [HEATING INSTR in a heavy iron skillet]
Finally, some basic syntactic information about the
target word and the constituents realising the vari-
ous frame elements is also added: (a) the part-of-
speech of the target word (e.g. V, N, A, PREP); (b)
the syntactic category of each constituent realising a
frame element (e.g. NP, PP, VPto, Sfin); and (c)
the syntactic role, with respect to the target word,
of each constituent realising a frame element (e.g.
Ext, Obj, Dep). Thus, each sentence in the corpus
can be seen to be annotated on at least three inde-
pendent ?layers?, as exemplified in Figure 1.
3 Frame-based NLU
The core of any frame-based NLU system is a parser
which produces domain-independent semantic rep-
1The version of FrameNet discussed in this paper is
FrameNet II release 1.3 from 22 August 2006.
resentations like the following, for the sentence John
billed the champagne to my account:
?
?
?
?
?
commerce-pay
AGENT John
THEME champagne
SOURCE
[
account
OWNER me
]
?
?
?
?
?
Deep parsers/grammars such as the ERG, OpenCCG
(White, 2006) and TRIPS (Dzikovska, 2004) pro-
duce more sophisticated representations with scop-
ing and referential information, but still contain a
frame-based representation as their core. The lex-
ical entries necessary for constructing such repre-
sentations specify information about orthography,
part-of-speech, semantic type and subcategorisation
properties, including a mapping between a syntactic
subcategorisation frame and the semantic frame.
An example of a TRIPS lexical entry is presented
in Figure 2, representing the entry for the verb bill
as used in the sentence discussed above. Note that
for each subcategorised argument the syntactic role,
syntactic category, and semantic role are specified.
Much the same kind of information is included in
ERG and OpenCCG lexical entries.
When constructing a computational lexicon, there
are a number of issues to take into account, sev-
eral of which are pertinent to the following discus-
sion. Firstly, computational lexicons typically list
only the ?canonical? subcategorisation frames, cor-
responding to a declarative sentence whose main
verb is in the active voice, as in Figure 1. Other vari-
ations, such as passive forms, imperatives and dative
alternations are generated automatically, for exam-
ple by lexical rules. Secondly, parsers that build se-
mantic representations typically make a distinction
between ?complements? and ?modifiers?. Comple-
ments are those dependents whose meaning is com-
pletely determined by the verb, for example the PP
on him in the sentence Mary relied on him, and are
thus listed in lexical entries. Modifiers, on the other
hand, are generally not specified in verb entries ?
although they may be associated with the underlying
verb frame, their meaning is determined indepen-
dently, usually by the preposition, such as the time
adverbial next week in I will see him next week.
Finally, for deep parsers, knowledge about which
argument of a matrix verb ?controls? the implicit
113
Matilde fried the catfish in a heavy iron skillet
target Apply heat
frame element COOK FOOD HEATING INSTR
syntactic category NP V NP PP
syntactic role Ext Obj Dep
Figure 1: A FrameNet annotated sentence
?
?
?
?
?
?
?
?
?
?
?
?
?
?
ORTH ?bill?
SYNCAT v
SEMTYPE
?
?
commerce-pay
ASPECT bounded
TIME-SPAN atomic
?
?
ARGS
?
?
?
SYNROLE subj
SYNCAT np
SEMROLE agent
?
?,
?
?
SYNROLE obj
SYNCAT np
SEMROLE theme
?
?,
?
?
?
?
SYNROLE comp
SYNCAT
[
pp
PTYPE to
]
SEMROLE source
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: A TRIPS lexical entry
subject of an embedded complement verb phrase is
necessary in order to to build the correct semantic
form. In a unification parser such as TRIPS, control
is usually represented by a relation of token-identity
(i.e. feature structure reentrancy) between the sub-
ject or object of a control verb and the subject of a
verbal complement.
4 Harvesting a computational lexicon from
FrameNet
In order to harvest a computational lexicon from the
FrameNet corpus, we took each of the 60,309 an-
notated sentences whose target word is a verb and
derived a lexical entry directly from the annotated
information. For example, from the sentence in Fig-
ure 1, the lexical entry in Figure 3 is derived.2
In order to remove duplicate entries, we made two
assumptions: (a) the value of the ARGS feature is a
set of arguments, rather than, say, a list or multiset;
and (b) two arguments are identical just in case they
specify the same syntactic role and semantic role.
These assumptions prevent a range of inappropriate
entries from being created, for example entries de-
2Our original plan was to use the automatically generated
?lexical entry? files included with the most recent FrameNet re-
lease as a basis for deep parsing. However, these entries contain
so many inappropriate subcategorisation frames, of the types
discussed in this paper, that we decided to start from scratch
with the corpus annotations.
rived from sentences involving a ?split? argument,
both parts of which are annotated independently in
FrameNet, e.g. [Ext Serious concern] arose [Ext
about his motives]. A second group of inappropri-
ate entries which are thus avoided are those deriving
from relative clause constructions, where the rela-
tive pronoun and its antecedent are also annotated
separately:
[Ext Perp The two boys] [Ext Perp who] ab-
ducted [Obj Victim James Bulger] are likely to
have been his murderers
Finally, assuming that the arguments constitute a set
means that entries derived from sentences involving
both canonical3 and non-canonical word order are
treated as equivalent. The kinds of construction im-
plicated here include ?quotative inversion? (e.g. ?Or
Electric Ladyland,? added Bob), and leftwards ex-
traction of objects and dependents, for example:
Are there [Obj any places] [Ext you] want to praise
[Dep for their special facilities]?
In this paper we are mainly interested in extract-
ing the possible syntax-semantics mappings from
FrameNet, rather than the precise details of their rel-
ative ordering. Since dependents in the harvested
3The canonical word order in English involves a pre-verbal
subject, with all other dependents following the verb.
114
??
?
?
?
?
?
ORTH ?fry?
SYNCAT V
SEMTYPE Apply heat
ARGS
?
?
?
SYNROLE Ext
SYNCAT NP
SEMROLE Cook
?
?
?
?
SYNROLE Obj
SYNCAT NP
SEMROLE Food
?
?,
?
?
SYNROLE Dep
SYNCAT PP
SEMROLE Heating Instr
?
?
?
?
?
?
?
?
?
?
Figure 3: The lexical entry derived from Figure 1
lexicon are fully specified for semantic role, syn-
tactic category and syntactic role, post-verbal con-
stituent ordering can-be regulated extra-lexically by
means of precedence rules. For example, for the
TRIPS and LFG formalisms, there is a straightfor-
ward correspondence between their native syntactic
role specifications and the FrameNet syntactic roles.
After duplicate entries were removed from the re-
sulting lexicon, we were left with 26,022 distinct
entries. The harvested lexicon incorporated 2,002
distinct orthographic forms, 358 distinct frames,
and 2,661 distinct orthography-frame pairs, giving
a functionality ratio (average number of lexical en-
tries per orthography-type pair) of 9.8.
Next, we evaluated a random sample of the de-
rived lexical entries by hand. The aim here was to
identify general classes of the harvested verb entries
which are not appropriate for inclusion in a frame-
based verb lexicon, and which would need to be
identified and fixed in some way. The main groups
identified were: (a) entries with no Ext argument
(section 4.1); (b) entries derived from verbs in the
passive voice (section 4.2); (c) entries which subcat-
egorise for modifiers (section 4.3); and (d) entries
for control verbs (section 4.4).
4.1 Subjectless entries
The harvested lexicon contains 2,201 entries (i.e.
9% of the total) which were derived from sentences
which do not contain an argument labelled with the
Ext syntactic role, in contravention of the gener-
ally accepted constraint on English verbs that they
always have a subject.
Three main groups of sentences are implicated
here: (a) those featuring imperative uses of the tar-
get verb, e.g. Always moisturise exposed skin with
an effective emollient like E45; (b) those featuring
other non-finite forms of the target verb whose un-
derstood subject is not controlled by (or even coref-
erential with) some other constituent in the sentence,
e.g. Being accused of not having a sense of humour
is a terrible insult; and (c) those involving a non-
referential subject it, for example It is raining heav-
ily or It is to be regretted that the owner should have
cut down the trees. In FrameNet annotations, non-
referential subjects are not identified on the syntactic
role annotation layer, and this makes it more difficult
to harvest appropriate lexical entries for these verbs
from the corpus.
These entries are easy to locate in the harvested
lexicon, but more difficult to repair. Typically one
would want to discard the entries generated from
(a) and (b) as they will be derived automatically in
the grammar, but keep the entries generated from (c)
while adding a non-referential it as a subject.
Although the FrameNet policy is to annotate the
(a) and (b) sentences as having a ?non-overt? real-
isation of the relevant frame element, this is con-
fined to the frame element annotation layer itself,
with the syntactic role and syntactic category lay-
ers containing no clues whatsoever about understood
subjects. One rather roundabout way of differentiat-
ing between these cases would involve attempting to
identify the syntactic category and semantic role of
the missing Ext argument by looking at other en-
tries with the same orthography and semantic type.
However, this whole problem could be avoided if
understood and expletive subjects were identified on
the syntactic layers in FrameNet annotations.
4.2 ?Passive? entries
Many entries in the harvested lexicon were derived
from sentences where the target verb is used in the
passive voice, for example:
[Ext NP Victim The men] had allegedly been ab-
ducted [Dep PP Perp by Mrs Mandela?s body-
115
guards] [Dep PP Time in 1988]
As discussed above, computational lexicons do not
usually list the kinds of lexical entry derived directly
from such sentences. Thus, it is necessary to identify
and correct or remove them.
In FrameNet annotated sentences, the voice of tar-
get verbs is not marked explicitly.4 We applied the
following simple diagnostic to identify ?passive? en-
tries: (a) there is an Ext argument realising frame
element e; and (b) there is some other entry with the
same orthographic form and semantic frame, which
has an Obj argument realising frame element e.
Initially we applied this diagnostic to the entries
in the harvested lexicon together with a part-of-
speech tag filter. The current FrameNet release in-
cludes standard POS-tag information for each word
in each annotated sentence. We considered only
those lexical entries derived from sentences whose
target verb is tagged as a ?past-participle? form (i.e.
VVN). This technique identified 4,160 entries in the
harvested lexicon (i.e. 16% of the total) as being
?passive?. A random sample of 10% of these was
examined and no false positives were found.
The diagnostic test was then repeated on the re-
maining lexical entries, this time without the POS-
tag filter. This was deemed necessary in order to
pick up false negatives caused by the POS-tagger
having assigned the wrong tag to a passive target
verb (generally the past tense form tag VVD). This
test identified a further 1007 entries as ?passive? (4%
of the total entries). As well as mis-tagged instances
of normal passives, this test picked up a further three
classes of entry derived from target verbs appearing
in passive-related constructions. The first of these
involves cases where the target verb is in the com-
plement of a ?raising adjective? (e.g. tough, difficult,
easy, impossible), for example:
[Ext NP Goal Both planning and control] are dif-
ficult to achieve [Dep PP Circs in this form of
production]
The current FrameNet annotation guidelines (Rup-
penhofer et al, 2006) state that the extracted object
in these cases should be tagged as Obj. However,
in practice, the majority of these instances appear to
have been tagged as Ext.
4Whilst there are dedicated subcorpora containing only pas-
sive targets, it is not the case that all passive targets are in these.
The second group of passive-related entries in-
volve verbs in the need -ing construction5 , e.g.:
[Ext NP Content Many private problems] need
airing [Dep PP Medium in the family]
The third group involved sentences where the target
verb is used in the ?middle? construction:
[Ext Experiencer You] frighten [Dep
Manner easily]
Again, linguistically-motivated grammars generally
treat these three constructions in the rule component
rather than the lexicon. Thus, the lexical entries de-
rived from these sentences need to be located and
repaired, perhaps by comparison with other entries.
Of the 1007 lexical entries identified by the sec-
ond, weaker form of the passive test, 224 (i.e. 22%)
turn out to be false positives. The vast majority
of these involve verbs implicated in the causative-
inchoative alternation (e.g. John?s back arched vs.
John arched his back). The official FrameNet pol-
icy is to distinguish between frames encoding a
change-of-state and those encoding the causation
of such a change, for example Amalgamation
versus Cause to amalgamate, Motion versus
Cause motion etc. In each case, the two frames
are linked by the Causative of frame relation.
Most of the false positives are the result of a fail-
ure to consistently apply this principle in annotation
practice, for example where no causative counterpart
has been defined for a particular inchoative frame,
or where an inchoative target has been assigned to a
causative frame, or a causative target to an inchoa-
tive frame. For example, 94 of the false positives
are accounted for simply by the lack of a causative
counterpart for the Body movement frame, mean-
ing that both inchoative and causative uses of verbs
like arch, flutter and wiggle have all been assigned
to the same frame.
For reasons of data sparsity, it is expected that the
approach to identifying passive uses of target verbs
discussed here will result in false negatives, since it
relies on there being at least one corresponding ac-
tive use in the corpus. We checked a random sample
of 400 of the remaining entries in the harvested lex-
icon and found nine false negatives, suggesting that
5Alternatively merit -ing, bear -ing etc.
116
the test successfully identifies 91% of those lexical
entries derived from passive uses of target verbs.
4.3 Modifiers
General linguistic theory makes a distinction be-
tween two kinds of non-subject dependent of a verb,
depending on the notional ?closeness? of the seman-
tic relation ? complements vs. modifiers. Take for
example the following sentence:
[Ext Performer She]?s [Dep Time currently]
starring [Dep Performance in The Cemetery
Club] [Dep Place at the Wyvern Theatre]
Of the three constituents annotated here as Dep,
only one is an complement (the Performance);
the Time and Place dependents are modifiers.
Frame-based NLU systems do not generally list
modifiers in the argument structure of a verb?s lexi-
cal entry. Thus, we need to find a means of identify-
ing those dependents in the harvested lexicon which
are actually modifiers.
The FrameNet ontology provides some informa-
tion to help differentiate complements and modi-
fiers. A frame element can be marked as Core,
signifying that it ?instantiates a conceptually nec-
essary component of a frame, while making the
frame unique and different from other frames?. The
annotation guidelines state that the distinction be-
tween Core and non-Core frame elements cov-
ers ?the semantic spirit? of the distinction between
complements and modifiers. Thus, for example,
obligatory dependents are always Core, as are:
(a) those which, when omitted, receive a definite
interpretation (e.g. the Goal in John arrived);
and (b) those whose semantics cannot be predicted
from their form. In the Performers and roles
frame used in the example above, the Performer
and Performance frame elements are marked as
Core, whilst Time and Place are not.
However, it is not clear that the notion of on-
tological ?coreness? used in FrameNet corresponds
well with the intuitive distinction between syntactic
complements and modifiers. This is exemplified by
the existence of numerous constituents in the corpus
which have been marked as direct objects, despite
invoking non-Core frame elements, for example:
[Agent I] ripped [Subregion the top]
[Patient from my packet of cigarettes]
The relevant frame here is Damaging, where the
Subregion frame element is marked as non-
Core, based on examples like John ripped his
trousers [below the knee]. Thus in this case, the
decision to retain all senses of the verb rip within
the same frame has led to a situation where seman-
tic and syntactic coreness have become dislocated.
Thus, although the Core vs. non-Core property on
frame elements does yield a certain amount of in-
formation about which arguments are complements
and which are modifiers, greater care needs to be
taken when assigning different subcategorisation al-
ternants to the same frame. For example, it would
have been more convenient to have assigned the verb
rip in the above example to the Removing frame,
where the direct object would then be assigned the
Core frame element Theme.
In the example discussed above, FrameNet does
provide syntactic role information (Obj) allowing
us to infer that a non-Core role is a complement
rather than a modifier. Where the syntactic role is
simply marked as Dep however, it is not possible
to make the decision without recourse to other lexi-
cal resources (e.g. ComLex). Since different parsers
may utilise different criteria for distinguishing com-
plements from modifiers, it might be better to post-
pone this task to the syntactic alignment module.
4.4 Control verbs
Unification-based parsers generally handle the dis-
tinction between subject (John promised Mary to
go) and object (John persuaded Mary to go) con-
trol verbs in the lexicon, using coindexation of the
subject/object of the control verb and the understood
subject of the embedded verb. The parser can use
this lexical information to assign the correct refer-
ent to the understood subject in a sentence like John
asked Mary to go:
?
?
?
?
?
?
command
AGENT John
THEME Mary 1
EFFECT
[
motion
THEME 1
]
?
?
?
?
?
?
Control verbs are annotated in FrameNet in the fol-
lowing manner:
Perhaps [Ext NP Speaker we] can persuade
[Obj NP Addressee Tammuz] [Dep VPto
117
Content to entertain him]
The lexical entries for transitive control verbs that
we can harvest directly from these annotations thus
fail to identify whether it is the subject or the direct
object which controls the understood subject of the
embedded verb.
We attempted to automatically distinguish subject
from object control in FrameNet by looking for the
annotated sentences that contain independently an-
notated argument structures for both the control verb
and embedded verb. For example, let?s assume the
following annotation also exists in the corpus:
Perhaps we can persuade [Ext NP Agent Tam-
muz] to entertain [Obj NP Experiencer him]
We can then use the fact that it is the object of the
control verb which is coextensive with the Ext of
the embedded verb to successfully identify persuade
as an object-control verb.
The problem with this approach is data sparsity.
The harvested lexicon contains 135 distinct verbs
which subcategorise for both a direct object and
a controlled VP complement. In a random sam-
ple of ten of these none of the annotated sentences
had been annotated independently from the perspec-
tive of the governed verb. As the proportion of the
FrameNet corpus which involves annotation of run-
ning text, rather than cherry-picked example sen-
tences, increases, we would expect this to improve.6
5 Implementation and discussion
The revised version of the harvested lexicon con-
tains 9,019 entries for 2,626 orthography-frame
pairs, yielding a functionality ratio of 3.4.
This lexicon still requires a certain amount of
cleaning up. For example, the verb accompany is
assigned to a number of distinct lexical entries de-
pending on the semantic role associated with the PP
complement (i.e. Goal, Path or Source). Cases
like this, where the role name is determined by the
particular choice of preposition, could be handled
outside the lexicon. Alternatively, it may be possible
to use the ?core set? feature of the FrameNet ontol-
ogy (which groups together roles that are judged to
6An alternative approach would be to consult an external
lexical resource, e.g. the LinGO ERG lexicon, which has good
coverage of control verbs.
be equivalent in some sense) to locate this kind of re-
dundancy. Other problems involve sentences where
a possessive determiner has been annotated as the
subject of a verb, e.g. It was [his] intention to aid
Larsen, resulting in numerous spurious entries.
The harvested lexical entries are encoded ac-
cording to a framework-independent XML schema,
which we developed with the aim of deriving lexi-
cons for use with a diverse range of parsers. At the
moment, several additional steps are required to con-
vert the entries we extracted into a format suitable
for a particular parser.
Firstly, the syntactic categories used by FrameNet
and the target lexicon have to be reconciled. While
basic constituent types such as noun and adjective
phrases do not change between the theories, small
differences may still exist. For example, the TRIPS
parser classifies all wh-clauses such as what he did
in I saw what he did and What he did was good as
noun phrases, the LinGO ERG grammar interprets
them as either noun phrases or clauses depending on
the context, and FrameNet annotation classifies all
of them as clauses. The alignment, however, should
be relative straightforward as there is, in general,
good agreement on the basic syntactic categories.7
Secondly, the information relevant to constituent
ordering may need to be derived, as discussed in
Section 4. Finally, the more abstract features such as
control have to be converted into feature structures
appropriate for the unification parsers. Our schema
incorporates the possibility for embedded category
structure, as in the treatment of control verbs in CCG
and HPSG where the verbal complement is an ?un-
saturated? category. We plan to use our schema
as a platform for deriving richer lexical represen-
tations from the ?flatter? entries harvested directly
from FrameNet.
As part of our future work, we expect to create
generic algorithms that help automate these steps. In
particular, we plan to include a domain-independent
set of constituent categories and syntactic role la-
bels, and add algorithms that convert between a lin-
ear ordering and a set of functional labels, for exam-
ple (Crabbe? et al, 2006). We also plan to develop
algorithms to import information from other seman-
7http://www.cl.cam.ac.uk/users/alk23/classes/Classes2.txt
contains a list of mappings between three different deep parsers
and ComLex subcategorisation frames
118
tic lexicons such as VerbNet into the same schema.
Currently, we have implemented an algorithm for
converting the harvested entries into the TRIPS lex-
icon format, resulting in a 6133 entry verb lexicon
involving 2654 distinct orthography-type pairs. This
lexicon has been successfully used with the TRIPS
parser, but additional work remains to be done be-
fore the conversion process is complete. For exam-
ple, we need a more sophisticated approach to re-
solving the complement-modifier distinction, along
with a means of integrating the FrameNet semantic
types with the TRIPS ontology so the parser can use
selectional restrictions to disambiguate.
The discussion in this paper has been mainly fo-
cused on extracting entries for a deep lexicons us-
ing frame-based NLU, but similar issues have been
faced also by the developers of shallow semantic
parsers from semantically annotated corpora. For
example, Gildea and Jurafsky (2002) found that
identifying passives was important in training a se-
mantic role classifier from FrameNet, using a parser
trained on the Penn Treebank along with a set of
templates to distinguish passive constructions from
active ones. Similarly, Chen and Rambow (2003)
argue that the kind of deep linguistic features we
harvest from FrameNet is beneficial for the success-
ful assignment of PropBank roles to constituents, in
this case using TAGs generated from PropBank to
generate the relevant features. From this perspec-
tive, our harvested lexicon can be seen as providing a
?cleaned-up?, filtered version of FrameNet for train-
ing semantic interpreters. It may also be utilised to
provide information for a separate lexical interpreta-
tion and disambiguation module to be built on top of
a syntactic parser.
6 Conclusion
We have developed both a procedure and a
framework-independent representation schema for
harvesting lexical information for deep NLP systems
from the FrameNet semantically annotated corpus.
In examining the feasibility of this approach to in-
creasing lexical coverage, we have identified a num-
ber of constructions for which current FrameNet an-
notation practice leads to problems in deriving ap-
propriate lexical entries, for example imperatives,
passives and control.
7 Acknowledgements
The work reported here was supported by grants
N000140510043 and N000140510048 from the Of-
fice of Naval Research.
References
C. F. Baker, C. Fillmore, and J. B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
COLING-ACL?98, Montreal, pages 86?90.
J. Chen and O. Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic
arguments. In Proceedings of EMNLP?03, Sapporo,
Japan.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of LREC?00, Athens, Greece, pages 591?600.
B. Crabbe?, M. O. Dzikovska, W. de Beaumont, and
M. Swift. 2006. Increasing the coverage of a domain
independent dialogue lexicon with VerbNet. In Pro-
ceedings of ScaNaLU?06, New York City.
M. O. Dzikovska. 2004. A Practical Semantic Repre-
sentation for Natural Language Parsing. Ph.D. thesis,
University of Rochester, Rochester NY.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
R. Grishman, C. MacLeod, and A. Meyers. 1994. Com-
lex syntax: Building a computational lexicon. In Pro-
ceedings of COLING?94, Kyoto, Japan, pages 268?
272.
J. Hockenmaier and M. Steedman. 2002. Acquiring
Compact Lexicalized Grammars from a Cleaner Tree-
bank. In Proceedings of LREC?02, Las Palmas, Spain.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of AAAI?00, Austin TX.
J. Ruppenhofer, M. Ellsworth, M. R. L. Petruck, C. R.
Johnson, and J. Scheffczyk, 2006. FrameNet II: Ex-
tended Theory and Practice. The Berkeley FrameNet
Project, August.
M. White. 2006. Efficient realization of coordinate struc-
tures in Combinatory Categorial Grammar. Research
on Language and Computation, 4(1):39?75.
119
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 51?58
Manchester, August 2008
?Deep? Grammatical Relations for Semantic Interpretation
Mark McConville and Myroslava O. Dzikovska
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
Informatics Forum, 10 Crichton Street, Edinburgh, EH8 9AB, Scotland
{Mark.McConville,M.Dzikovska}@ed.ac.uk
Abstract
In this paper, we evaluate five distinct sys-
tems of labelled grammatical dependency
against the kind of input we require for se-
mantic interpretation, in particular for the
deep semantic interpreter underlying a tu-
torial dialogue system. We focus on the
following linguistic phenomena: passive,
control and raising, noun modifiers, and
meaningful vs. non-meaningful preposi-
tions. We conclude that no one system
provides all the features that we require,
although each such feature is contained
within at least one of the competing sys-
tems.
1 Introduction
The aim of the work reported in this paper is to
evaluate the extent to which proposed systems of
grammatical relations (GRs) reflect the kinds of
deep linguistic knowledge required for semantic
interpretation, in particular for deriving semantic
representations suitable for domain reasoning in
dialogue systems.
Grammatical relations either produced by or ex-
tracted from the output of wide-coverage syntactic
parsers are currently used as input to shallow se-
mantic parsers, which identify semantic relations
that exist between predicators (typically verbs) and
their dependents (Gildea and Jurafsky, 2002; Erk
and Pad?o, 2006). Predicate-argument structure
identified in this way can then be used in tasks like
information extraction (Surdeanu et al, 2003) and
question answering (Kaisser and Webber, 2007).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
However, wide-coverage stochastic parsers are
only rarely used in dialogue systems. Tradi-
tionally, interpretation modules of dialogue sys-
tems utilise specialised parsers and semantic in-
terpreters handcrafted to a small domain (Seneff,
1992; Chang et al, 2002), or wide coverage deep
parsers (Allen et al, 2007; Jordan et al, 2006;
Wolska and Kruijff-Korbayov?a, 2003; Callaway et
al., 2007; Kay et al, 1994). Unlike in information
retrieval and question answering tasks, the system
often needs to be connected to a knowledge base
which represents the state of the world, and must
be able to convert user utterances into knowledge
base queries. In addition to identifying predicate-
argument relationships, such systems need to sup-
port a variety of tasks, for example resolution of
pronouns and anaphors, and interpreting negation,
quantification, tense and modality.
While deep parsers produce precise seman-
tic representations appropriate for such reason-
ing, they suffer from robustness problems. Wide-
coverage dependency parsers could potentially
provide a more robust alternative, provided that
their output is easy to convert into semantic rep-
resentations for reasoning.
Section 2 introduces the kind of deep linguis-
tic processing application which motivates our ap-
proach to grammatical relations. Section 3 de-
fines some underlying principles behind the kind
of ?deep? GR systemwe have in mind. The remain-
der of the paper discusses a number of linguistic
phenomena in detail, and evaluates how well vari-
ous systems of GR representation from the depen-
dency parsing literature capture the kind of linguis-
tic insights required for interface with reasoning?
passive (section 4), raising and control (section 5),
noun modification (section 6) and syntactic versus
semantic prepositions (section 7).
51
2 Motivation
As an example application that requires deep pars-
ing consider a tutorial dialogue system that inter-
prets students? answers to factual questions (e.g.
Which bulbs will be lit in this circuit?) as well
as explanation questions (e.g. Explain your rea-
soning!). It has been argued previously (Wolska
and Kruijff-Korbayov?a, 2004; Ros?e et al, 2003)
that tutorial dialogue systems require deep under-
standing of student explanations, which can have
significantly more complex structure than database
queries in the information-seeking domain. In our
application, if a student is asked for an explana-
tion, his or her input has to be passed through the
domain knowledge base to verify its factual cor-
rectness, and a separate process verifies that all
relations mentioned in the explanation are correct
and relevant. For example, imagine that the stu-
dent says the following:
(1) The bulbs in circuits 1 and 3 will be lit
because they are in closed paths with the
batteries.
Here, the system has to verify two things: (a) that
the facts are correct (bulbs in circuits 1 and 3 will
be lit, and each of those bulbs is in a closed path
with a battery); and (b) that the reason is valid ?
being in a closed path with a battery is a necessary
and sufficient condition for a bulb to be lit.
This task is particularly interesting because it
combines characteristics of deep and shallow inter-
pretation tasks. On the one hand, the fact-checking
mechanism requires a connection to the database.
Thus, both pronouns and definite noun phrases
need to be resolved to the objects they represent in
the knowledge base, and first-order logic formulas
representing utterance content need to be checked
against the system knowledge. This task is simi-
lar to natural language interfaces to databases, or
knowledge acquisition interfaces that convert lan-
guage into knowledge base statements (Yeh et al,
2005). On the other hand, with respect to rea-
son checking, human tutors have indicated that
they would accept an answer simply if a student
produces the key concepts and relations between
them, even if the answer is not strictly logically
equivalent to the ideal answer (Dzikovska et al,
2008). Human tutors tend to be especially lenient
if a student is asked a generic question, like What
is the definition of voltage?, which does not refer
to specific objects in the knowledge base. Thus, a
simpler matching mechanism is used to check the
reasons, making this task more similar to an infor-
mation retrieval task requiring shallower process-
ing, i.e. that the predicate-argument relations are
retrieved correctly (though negation still remains
important).
Thus, while a specific task is used to motivate
our evaluation, the conclusions would be applica-
ble to a variety of systems, including both deep and
shallow semantic interpreters.
For the purposes of this evaluation, we discuss
features of grammatical representation relevant to
two subtasks critical for the system: (a) identify-
ing predicate-argument structure; and (b) resolving
anaphora.
The extraction of predicate-argument relations
is a common requirement for both shallow and
deep semantic tasks. For example, for the stu-
dent input in example (1) we may expect some-
thing like:
1
(2) (LightBulb b1) (LightBulb b2)
(lit b1 true) (lit b2 true)
(Path P3) (closed P3 true)
(contains P3 b1) (Path P4)
(closed P4 true) (contains P4 b2)
Resolving anaphora, on the other hand, is par-
ticularly important for the kind of deep seman-
tic processing used in dialogue systems. Implicit
in the above representation is the fact that the
definite noun phrase the bulbs in circuits 1 and
3 was resolved to domain constants b1 and b3,
and indefinite references to paths were replaced by
Skolem constants P3 and P4. The reference reso-
lution process requires detailed knowledge of noun
phrase structure, including information about re-
strictive modification, and this is the second focus
of our evaluation.
Ideally, we would like a dependency parser to
produce grammatical relations that can be con-
verted into such semantic representations with
minimal effort, thus minimising the number of spe-
cific rules used to convert individual relations. We
discuss the principles underlying such representa-
tions in more detail in the next section.
1
We used a simplified representation of quantifiers that as-
sumes no scope ambiguity and uses skolem constants to rep-
resent existential quantification. This is sufficient for our par-
ticular application. In general, a more sophisticated quantifier
representation would be necessary, for example that proposed
in Copestake et al (2005) or Bos and Oka (2002), but we
leave the relevant evaluation for future work.
52
3 Deep grammatical relations
We formulated four principles for deep grammati-
cal relations representation.
Firstly, grammatical relations should, whenever
possible, reflect relations between the predicators
(i.e. content words as opposed to function words)
in a sentence. In addition, the same relation should
correspond to the same role assignment. For exam-
ple, the deep GRs in passive constructions should
be the same as those in the active equivalents
(see section 4), and the analysis of a control verb
construction like John persuaded Mary to dance
should make it clear that there is a ?subject? GR
from dance to Mary similar to that in the implied
sentence Mary danced (see section 5).
Secondly, a GR should, whenever possible, ap-
pear only if there is a an explicit selectional restric-
tion link between the words. For example, in a
raising verb construction like John expects Mary to
dance, there should be noGR from the raising verb
expects to its object Mary (see section 5). Also,
where a preposition functions strictly as a syntac-
tic role marker, as in the construction John relies
on Mary, it should have no place in the GR anal-
ysis; rather there should be a direct link from the
verb to the embedded noun phrase (see section 7).
Thirdly, the GRs should preserve evidence of
syntactic modification to enable reference resolu-
tion. To understand why this is important, take the
following two examples:
(3) The lit bulb is in a closed path.
The bulb in a closed path is lit.
From a pure predicate-argument structure perspec-
tive, these two sentences share exactly the same
deep GRs:
2
(4) ext(lit,bulb)
ext(in-closed-path,bulb)
However, from the perspective of reference resolu-
tion, the two sentences are very different. For the
first example, this process involves first finding the
lit bulb and then verifying that it is in a closed path,
whereas for the second we need to find the bulb in
a closed path and verify that it is lit. This differ-
ence can be captured by assigning the following
additional deep GRs to the first example:
2
The representation is simplified for reasons of exposition.
The GRs should be interpreted as follows: ext denotes the
external argument of an adjective or preposition, ncmod a
non-clausal restrictive modifier, and det the determiner of a
noun.
(5) det(bulb,the)
ncmod(bulb,lit)
And the following GRs are added to the analysis
of the second example:
(6) det(bulb,the)
ncmod(bulb,in-closed-path)
Now the two analyses are formally distinct: (a) the
first is rooted at predicate in a closed path and the
second at lit; and (b) the definite external argument
the bulb takes scope over the modifier lit in the first
but over in a closed path in the second. Noun mod-
ification is discussed in section 6.
Finally, the set of grammatical relations should
make it easy to identify and separate out con-
structions which are largely dependent on seman-
tic/world knowledge, such as N-N modification, so
that separate models and evaluations can be con-
ducted as necessary.
4 Passive
The shared task dataset contains numerous passive
participles, most of which can be classified into the
following four groups depending on how the par-
ticiple is used: (a) complement of passive auxiliary
e.g. Tax induction is activated by the RelA subunit;
(b) complement of raising verb e.g. The adminis-
tration doesn?t seem moved by the arguments; (c)
nominal postmodifier e.g. the genes involved in T-
cell growth; and (d) nominal premodifier e.g. the
proposed rules.
In all these cases, our system for deep gram-
matical relation annotation requires: (a) that
there is a relation from the passive partici-
ple to the deep object; and (b) that this rela-
tion be the same as in the corresponding ac-
tive declarative construction, so that predicate-
argument structure can be straightforwardly de-
rived. Thus, for example, the analysis of Tax in-
duction is activated by the RelA subunit will con-
tain the GR dobj(activated,induction),
and that of the proposed rules will include
dobj(proposed,rules), where dobj is the
relation between a transitive verb and its (deep) di-
rect object.
We evaluated five GR-based output formats ac-
cording to these two features. The results are pre-
sented in Table 1, where for each representation
format (the rows) and each usage class of pas-
sive participles (the columns), we provide the GR
which goes from the participle to its deep object,
53
complement of complement of nominal nominal
passive auxiliary raising verb postmodifier premodifier active
HPSG ARG2 (of verb arg12)
RASP ncsubj:obj dobj
CCGBank Spss\NP N/N S\NP/[NP]
Stanford nsubjpass - dobj
PARC subj - obj
Table 1: Representation of deep objects in passive and active
if such a GR exists.
3
The five GR representations
compared are:
HPSG predicate-argument structures extracted
from the University of Tokyo HPSG Treebank
(Miyao, 2006)
RASP grammatical relations as output by the
RASP parser (Briscoe et al, 2006)
CCGBank predicate-argument dependencies ex-
tracted from CCGBank (Hockenmaier and
Steedman, 2007)
Stanford grammatical relations output by the
Stanford Parser (de Marneffe et al, 2006)
PARC dependency structures used in the annota-
tion of DepBank (King et al, 2003)
The first four columns in Table 1 represent, for
each of the four uses of passive participles listed
above, the grammatical relation, if any, which typ-
ically joins a passive participle to its deep object.
The rightmost column presents the label used for
this relation in equivalent active clauses. Adjacent
columns have been collapsed where the same GR
is used for both uses. The ideal system would have
the same GR listed in each of the five columns.
The grammatical relations used in the Stan-
ford, PARC and RASP systems are atomic labels
like subj, obj etc, although the latter system
does allow for a limited range of composite GRs
like ncsubj:obj (a non-clausal surface subject
which realises a deep object). In the HPSG sys-
tem, verbal subjects and objects are represented
as ARG1 and ARG2 respectively of strict transi-
tive verb type verb arg12. Finally, the GRs as-
sumed in CCGBank consist of a lexical category
(e.g. the strict transitive verb category S\NP/NP)
with one argument emphasised. I assume the
3
The relations presented for HPSG and CCG are those for
passive participle of strict transitive verbs.
following notational convenience for those cate-
gories which contain specify more than one argu-
ment ? the emphasised argument is surrounded
by square brackets. Thus, subject and object of a
strict transitive verb are denoted S\[NP]/NP and
S\NP/[NP] respectively.
With respect to Table 1, note that: (a) in the
CCGbank dependency representation, although
prenominal passive participles are linked to their
deep object (i.e. the modified noun), this relation
is just one of generic noun premodification (i.e.
N/N) and is thus irrelevant to the kind of predicate-
argument relation we are interested in; (b) in the
PARC and Stanford dependency representations,
there is no GR from noun-modifying passive par-
ticiples to their deep objects, just generic modifica-
tion relations in the opposite direction; and (c) in
PARC, passive participles are themselves marked
as being passive, thus allowing a subsequent inter-
pretation module to normalise the deep grammati-
cal relations if desired.
If we are interested in a system of deep gram-
matical role annotation which allows for the rep-
resentation of normalised GRs for passive partici-
ples in all their uses, then the HPSG Treebank for-
mat is more appropriate than the other schemes,
since it uniformly uses deep GRs for both ac-
tive and passive verb constructions. The RASP
representation comes a close second, only requir-
ing a small amount of postprocessing to convert
ncsubj:obj relations into dobj ones. In addi-
tion, both the CCGBank and the Stanford notation
distinguish two kinds of surface subject ? those
which realise deep subjects, and those which re-
alise passivised deep objects.
5 Control
The shared task dataset contains a number of in-
finitives or participles which are dependents of
non-auxiliary verbs or adjectives (rather than be-
ing nounmodifiers for example). Most of these can
54
complements adjuncts raising
HPSG 3 3 5
RASP 3 3 5
CCGbank 3 3 5
Stanford 3 5 3
PARC 5 5 5
Table 2: Representation of controlled subjects and
raising
be partitioned into the following three classes: (a)
complements of subject control verbs e.g. The ac-
cumulation of nuclear c-Rel acts to inhibit its own
continued production; (b) complements of subject
raising verbs e.g. The administration seems moved
by arguments that . . . ; and (c) subject controlled
adjuncts e.g. Alex de Castro has stopped by to slip
six cards to the Great Man Himself.
In all these cases, our deep grammatical role an-
notation requires that there be a subject relation
(or an object relation in the case of a passive par-
ticiple) from the infinitive/participle to the surface
subject (or surface object in the case of object con-
trol) of the controlling verb/adjective. For exam-
ple, the analysis of Tax acts indirectly by induc-
ing the action of various host transcription fac-
tors will contain both the GRs sbj(acts,Tax)
and sbj(inducing,Tax). In addition, we also
want to distinguish ?raising? verbs and adjectives
from control structures. Thus, in the analysis of
The administration seems moved by arguments
that . . . , we want a (deep) object relation from
moved to administration, but we don?t want any
relation from seems to administration.
We again evaluated the various GR-based output
formats according to these features. The results are
presented in Table 2, where for each representation
format (the rows) we determine: (a) whether a verb
with an understood subject which is a complement
of the matrix verb is linked directly to its relevant
subject (column 1); (b) whether a verb with an un-
derstood subject which is a controlled adjunct of
the matrix verb is linked directly to its relevant
subject (column 2); and (c) whether raising verbs
are non-linked to their surface subjects (column
3). Note that the Stanford dependency represen-
tation is the only format which distinguishes be-
tween raising and control. This distinction is made
both structurally and in terms of the name assigned
to the relevant dependent ? controlled subjects
are distinguished from all other subjects (includ-
ing raised ones) by having the label xsubj rather
than just nsubj.
4
The ideal GR representation format would have
a tick in each of the three columns in Table 2. It is
clear that no single representation covers all of our
desiderata for a deep grammatical relation treat-
ment of control/raising, but each feature we require
is provided by at least one format.
6 Nominal modifiers
The dataset contains numerous prenominal modi-
fiers
5
, subdivided into the following three groups:
(a) attributive adjectives e.g. a few notable excep-
tions; (b) verb participles e.g. the proposed rules;
and (c) nouns e.g. a car salesman.
In order to ensure an adequate representation of
basic predicate-argument structure, our system of
deep grammatical annotation first of all requires
that, from each prenominal adjective or verb, there
is an appropriate relation to the modified noun, of
the same type as in the corresponding predicative
usage. For example, assuming that He proposed
the rules has a direct object relation from proposed
to rules, the same relation should occur in the anal-
ysis of the proposed rules. Similarly, if The excep-
tions are notable is analysed as having an external
argument relation from notable to exceptions, then
the same should happen in the case of a few no-
table exceptions. However, this does not appear to
hold for prenominal nouns, since the relation be-
tween the two is not simply one of predication ?
a car salesman is not a salesman who ?is? a car,
but rather a salesman who is ?associated? with cars
in some way. Thus we would not want the same
relation to be used here.
6
Secondly, in order to ensure a straightforward
interface with reference resolution, we need a
modification relation going in the opposite direc-
4
We have judged that CCGBank does not make the rele-
vant distinction between raising and control verbs based on
the dependency representations contained in the shared task
dataset. For example, for the example sentence The adminis-
tration seemmoved by the fact that . . . , a CCG subject relation
is specified from the raising verb seem to its surface subject
administration.
5
We focus on prenominal modifiers in order to keep the
exposition simple. Similar remarks are valid for postnominal
restrictive modifiers as well.
6
Presumably the same goes for attributive adjectives
which lack corresponding predicative uses, e.g. the former
president.
55
tion, from the modified noun to each (restrictive)
modifier, as argued in section 2. Thus, a complete
GR representation of a noun phrase like notable
exceptions would be cyclical, for example:
(7) ext(notable,exceptions)
ncmod(exceptions,notable)
We evaluated the various GR-based output formats
according to these desiderata. The results are pre-
sented in Table 3. For each annotation scheme (the
rows), we first present the relation (if any) which
goes from the modified noun to each kind of pre-
modifier (adjective, verb participle and noun re-
spectively).
7
Themiddle three columns contain the
relation (if any) which goes to the noun from each
kind of modifier. Finally, the last three columns
give the corresponding predicative relation used in
the annotation scheme, for example in construc-
tions like The exceptions are notable, He proposed
the rules, or Herbie is a car. Where it is un-
clear whether a particular format encodes the re-
lation between a predicative noun and its subject,
we mark this as ??? in the last column.
Ideally, what we want is a representation where:
(a) there is a GR in all nine columns (with the pos-
sible exception of the ?noun modifier to noun? one
(column 6)); (b) the corresponding relations in the
middle and righthand sections are identical, except
for ?noun modifier to noun? (column 6) and ?pred-
icative noun? (the last column) which should be
distinct, since the relation between a noun modifier
and its head noun is not simply one of predication.
It is clear that no one representation is perfect,
though every feature we require is present in at
least one representation system. Note in particu-
lar that the HPSG, PARC and Stanford systems are
acyclic ? the former only has ?modifier to noun?
links, while the latter two only have ?noun to mod-
ifier? ones. The RASP format is cyclic, at least for
prenominal participles ? in the proposed rules,
there is a modifier relation from rules to proposed,
as well as a deep object relation from proposed to
rules, the same relation that would be found in the
corresponding predicative the rules were proposed.
Note finally that the PARC and Stanford repre-
sentations distinguish between prenominal adjec-
tives and nouns, in terms of the name of the rele-
vant modifier GR. This corresponds well with our
7
Note that the N/N links in the CCG representation actu-
ally go from the modifier to the noun. However, they have
been included in the set of ?noun to modifier? relations since
they are formally modifier categories (i.e. of the form X/X).
preference for a GR system where we can evalu-
ate modules of N-N disambiguation (e.g. luxury
car salesman) in isolation from other aspects of
prenominal structure.
7 Prepositions
All five grammatical relations formats treat prepo-
sition phrases in pretty much the same way: (a)
there is a GR link from the head of which the PP
is a complement or modifier to the preposition it-
self (the HPSG representation has this link going
in the opposite direction for PP modifiers, but the
principle is the same); and (b) there is a link from
the preposition to its complement NP. For example,
the noun phrase experts in Congress is annotated as
follows:
(8) ncmod(experts,in)
dobj(in,Congress)
The only PPs which have been handled differently
are agentive by-PPs of passive participles, which
are either normalised or treated using a special,
construction-specific GR.
Note however that all prepositions are not equal
when it comes down to representing the predicate-
argument structure of a sentence. In a nutshell,
some prepositions are predicators (e.g. experts
in Congress) whereas others are simply syntactic
role markers (e.g. a workout of the Suns). Ide-
ally, we would want a GR system which marks
this distinction, for example by annotating pred-
icator prepositions as lexical heads and ignoring
role-marking prepositions altogether. The only
GR scheme which attempts to make this distinc-
tion is the PARC system, which has a ptype fea-
ture for every preposition with two possible val-
ues, semantic and non-semantic. However,
this does not appear to have been annotated consis-
tently in the PARC dataset ? the only examples of
non-semantic prepositions are agentive by-PPs of
passive participles.
8 Conclusion
We have proposed a set of principles for devel-
oping a grammatical relation annotation system
for use with both shallow and deep semantic in-
terpretation systems, in particular a tutorial dia-
logue system. We then evaluated five different GR
schemes from the dependency parsing literature
based on how well they handle a number of ?deep?
syntactic phenomena implied by these principles,
56
noun to modifier modifier to noun predicative
A V N A V N A V N
RASP ncmod - ncsubj etc - - ncsubj etc -
HPSG - a arg1 v arg1 etc n arg1 a arg1 v arg1 etc n arg1
CCG N/N - N/N - S\NP etc - Sadj\NP S\NP etc ?
PARC adjunct mod - subj subj ?
Stanf amod nn - nsubj nsubj ?
Table 3: Representation of prenominal modifiers
i.e. passive, control and raising, noun modifica-
tion, and meaningful vs. non-meaningful prepo-
sitions. We conclude that none of the proposed
GR annotation schemes contains everything we re-
quire for deep semantic processing, although each
of the features/distinctions we included in our list
of desiderata is provided by at least one system.
Many of the deep syntactic phenomena dis-
cussed here are known issues for shallow seman-
tic tasks like semantic role labelling. For exam-
ple, passive constructions are a recognised source
of noise in semantic role labelling systems (Gildea
and Jurafsky, 2002), and resolving controlled sub-
jects provides more data for training models of se-
lectional restrictions, which are known to be useful
features for role labelling. More generally, Chen
and Rambow (2003) demonstrate that a focus on
?deep? syntactic features results in a more accurate
stochastic semantic role labeller than using surface
information alone.
Note also that the deep grammatical role rep-
resentation proposed here is meant to be ?theory-
neutral?, in the sense that it was not influenced by
any one of the competing grammar formalisms to
the exclusion of the others. Indeed, it should be
a straightforward task to write a grammar using
either the HPSG, LFG, CCG or RASP-style un-
derlying formalism which can produce an output
representation consisting of deep relations, con-
structed in a purely compositional manner. Indeed,
the syntactic phenomena discussed in this paper
are those which form the basis of numerous in-
troductory textbooks on English generative syntax
(Haegeman, 1994; Sag and Wasow, 1999; Bres-
nan, 2000). In addition, the phenomena which
form the basis of the analysis in this paper were
among those which had been the focus of a sig-
nificant amount of attention in the development
of the semantic interpretation system underlying
our domain-independent tutorial dialogue system.
Other issues which were considered, but for which
we lack space to discuss in detail include: (a) ex-
pletive pronouns should be ignored, i.e. the subject
pronouns in ?impersonal? verb constructions like It
is raining or It?s great that John loves Mary should
not be seen as the target of deep grammatical re-
lations; (b) unbounded dependencies should be re-
solved, i.e. in the relative clause the woman Bill
thinks John loves there should be an object relation
between the embedded verb loves and its extracted
object woman; (c) restrictive and non-restrictive
modification (including apposition) should be dis-
tinguished, since the latter is not relevant for refer-
ence resolution; and (d) certain subsentential con-
junctions need to be compiled out (for examples
like electronic, computer and building products).
Finally, we recognise that, in many cases, it is
possible to transform parser representations into
our desired format. For example, if the parser out-
put tells us that a given verb form is a passive
participle, we can use this information to remap
the surface relations, thus retrieving the underlying
predicate-argument structure. However, we pre-
fer a system where this kind of post-processing
is not needed. Reasons for this include the in-
creased potential for error in a system relying on
post-processing rules, as well as the need to have
both detailed documentation for how each parser
output format handles particular constructions, as
well as a comprehensive mapping schema between
representations. Having a community standard for
GR-based parser output is an essential element of
future parsing technology, and to be practically
useful in a range of semantic interpretation tasks,
this standard should involve ?deep? syntactic dis-
tinctions of the kind discussed in this paper.
9 Acknowledgements
The work reported here was supported by grants
N00014-08-1-0179 and N00014-08-1-0043 from
the Office of Naval Research.
57
References
Allen, James, Myroslava Dzikovska, Mehdi Manshadi,
and Mary Swift. 2007. Deep linguistic processing
for spoken dialogue systems. In Proceedings of the
ACL?07 Workshop on Deep Linguistic Processing.
Bos, Johan and Tetsushi Oka. 2002. An inference-
based approach to dialogue system design. In Pro-
ceedings of COLING?02.
Bresnan, Joan. 2000. Lexical-Functional Syntax. Basil
Blackwell.
Briscoe, Ted, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL?06 Interactive Presen-
tation Sessions.
Callaway, Charles B., Myroslava Dzikovska, Elaine
Farrow, Manuel Marques-Pita, Colin Matheson, and
Johanna D. Moore. 2007. The Beetle and BeeDiff
tutoring systems. In Proceedings of SLaTE?07.
Chang, N., J. Feldman, R. Porzel, and K. Sanders.
2002. Scaling cognitive linguistics: Formalisms
for language understanding. In Proceedings of
ScaNaLU?02.
Chen, John and Owen Rambow. 2003. Use of deep
linguistic features for the recognition and labeling of
semantic arguments. In Proceedings of EMNLP?03.
Copestake, Ann, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal Recursion Semantics:
An Introduction. Research on Language and Com-
putation, 3:281?332.
de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC?06.
Dzikovska, Myroslava O., Gwendolyn E. Campbell,
Charles B. Callaway, Natalie B. Steinhauser, Elaine
Farrow, Johanna D. Moore, Leslie A. Butler, and
Colin Matheson. 2008. Diagnosing natural lan-
guage answers to support adaptive tutoring. In Pro-
ceedings of FLAIRS?08 special track on Intelligent
Tutoring Systems.
Erk, Katrin and Sebastian Pad?o. 2006. SHAL-
MANESER - a toolchain for shallow semantic pars-
ing. In Proceedings of LREC?06.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3).
Haegeman, Liliane. 1994. Introduction to Government
and Binding Theory. Basil Blackwell, 2nd edition
edition.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Jordan, Pamela, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of FLAIRS?06.
Kaisser, Michael and Bonnie Webber. 2007. Question
answering based on semantic roles. In Proceedings
of the ACL?07 Workshop on Deep Linguistic Pro-
cessing.
Kay, Martin, Jean Mark Gawron, and Peter Norvig.
1994. Verbmobil: A Translation System for Face-
To-Face Dialog. CSLI Press, Stanford, CA.
King, Tracy Holloway, Richard Crouch, Stefan Rie-
zler, Mary Dalrymple, and Ronald M. Kaplan. 2003.
The PARC 700 dependency bank. In Proceedings of
EACL?03.
Miyao, Yusuke. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Devel-
opment and Feature Forest Model. Ph.D. thesis, Uni-
versity of Tokyo.
Ros?e, C. P., D. Bhembe, S. Siler, R. Srivastava, and
K. VanLehn. 2003. The role of why questions in ef-
fective human tutoring. In Proceedings of AIED?03.
Sag, Ivan A. and Thomas Wasow. 1999. Syntactic The-
ory: A Formal Introduction. CSLI.
Seneff, Stephanie. 1992. TINA: A natural language
system for spoken language applications. Computa-
tional Linguistics, 18(1).
Surdeanu, Mihai, Sanda M. Harabagiu, John Williams,
and Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceedings
of ACL?03.
Wolska, Magdalena and Ivana Kruijff-Korbayov?a.
2003. Issues in the interpretation of input in mathe-
matical dialogs. In Duchier, Denys, editor, Prospects
and advances in the syntax/semantics interface.
Lorraine-Saarland Workshop Series proceedings.
Wolska, Magdalena and Ivana Kruijff-Korbayov?a.
2004. Analysis of mixed natural and symbolic lan-
guage input in mathematical dialogs. In Proceedings
of ACL?04.
Yeh, Peter Z., Bruce Porter, and Ken Barker. 2005.
Matching utterances to rich knowledge structures to
acquire a model of the speaker?s goal. In Proceed-
ings of K-CAP?05.
58
