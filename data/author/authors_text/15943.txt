Proceedings of the EACL 2012 Student Research Workshop, pages 46?54,
Avignon, France, 26 April 2012. c?2012 Association for Computational Linguistics
Yet Another Language Identifier
Martin Majlis?
Charles University in Prague
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
majlis@ufal.mff.cuni.cz
Abstract
Language identification of written text has
been studied for several decades. Despite
this fact, most of the research is focused
on a few most spoken languages, whereas
the minor ones are ignored. The identi-
fication of a larger number of languages
brings new difficulties that do not occur
for a few languages. These difficulties are
causing decreased accuracy. The objective
of this paper is to investigate the sources
of such degradation. In order to isolate
the impact of individual factors, 5 differ-
ent algorithms and 3 different number of
languages are used. The Support Vector
Machine algorithm achieved an accuracy of
98% for 90 languages and the YALI algo-
rithm based on a scoring function had an
accuracy of 95.4%. The YALI algorithm
has slightly lower accuracy but classifies
around 17 times faster and its training is
more than 4000 times faster.
Three different data sets with various num-
ber of languages and sample sizes were pre-
pared to overcome the lack of standardized
data sets. These data sets are now publicly
available.
1 Introduction
The task of language identification has been stud-
ied for several decades, but most of the literature
is about identifying spoken language1. This is
mainly because language identification of written
form is considered an easier task, because it does
not contain such variability as the spoken form,
such as dialects or emotions.
1http://speech.inesc.pt/?dcaseiro/
html/bibliografia.html
Language identification is used in many NLP
tasks and in some of them simple rules2 are of-
ten good enough. But for many other applica-
tions, such as web crawling, question answering
or multilingual documents processing, more so-
phisticated approaches need to be used.
This paper first discusses previous work in Sec-
tion 2, and then presents possible hypothesis for
decreased accuracy when a larger number of lan-
guages is identified in Section 3. Data used for
experiments is described in Section 4, along with
methods used in experiments for language iden-
tification in Section 5. Results for all methods
as well as comparison with other systems is pre-
sented in Section 6.
2 Related Work
The methods used in language identification have
changed significantly during the last decades. In
the late sixties, Gold (1967) examined language
identification as a task in automata theory. In the
seventies, Leonard and Doddington (1974) was
able to recognize five different languages, and in
the eighties, Beesley (1988) suggested using cryp-
toanalytic techniques.
Later on, Cavnar and Trenkle (1994) intro-
duced their algorithm with a sliding window over
a set of characters. A list of the 300 most com-
mon n-grams for n in 1..5 is created during train-
ing for each training document. To classify a new
document, they constructed a list of the 300 most
common n-grams and compared n-grams position
with the testing lists. The list with the least dif-
ferences is the most similar one and new doc-
ument is likely to be written in same language.
2http://en.wikipedia.org/wiki/
Wikipedia:Language_recognition_chart
46
They classified 3478 samples in 14 languages
from a newsgroup and reported an achieved accu-
racy of 99.8%. This influenced many researches
that were trying different heuristics for selecting
n-grams, such as Martins and Silva (2005) which
achieved an accuracy of 91.25% for 12 languages,
or Hayati (2004) with 93.9% for 11 languages.
Sibun and Reynar (1996) introduced a method
for language detection based on relative entropy, a
popular measure also known as Kullback-Leibler
distance. Relative entropy is a useful measure
of the similarity between probability distributions.
She used texts in 18 languages from the European
Corpus Initiative CD-ROM. She achieved a 100%
accuracy for bigrams.
In recent years, standard classification tech-
niques such as support vector machines also be-
came popular and many researchers used them
Kruengkrai et al(2005) or Baldwin and Lui
(2010) for identifying languages.
Nowadays, language recognition is considered
as an elementary NLP task3 which can be used
for educational purposes. McNamee (2005) used
single documents for each language from project
Gutenberg in 10 European languages. He prepro-
cessed the training documents ? the texts were
lower-cased, accent marks were retained. Then,
he computed a so-called profile of each language.
Each profile consisted of a percentage of the train-
ing data attributed to each observed word. For
testing, he used 1000 sentences per language from
the Euro-parliament collection. To classify a new
document, the same preprocessing was done and
inner product based on the words in the document
and the 1000 most common words in each lan-
guage was computed. Performance varied from
80.0% for Portuguese to 99.5% for German.
Some researches such as Hughes et al(2006)
or Grothe et al(2008) focused in their papers
on the comparison of different approaches to lan-
guage identification and also proposed new goals
in that field, such as as minority languages or lan-
guages written non-Roman script.
Most of the researches in the past identified
mostly up to twenty languages but in recent
years, language identification of minority lan-
guages became the focus of Baldwin and Lui
(2010), Choong et al(2011), and Majlis? (2012).
All of them observed that the task became much
3http://alias-i.com/lingpipe/demos/
tutorial/langid/read-me.html
harder for larger numbers of languages and accu-
racy of the system dropped.
3 Hypothesis
The accuracy degradation with a larger number of
languages in the language identification system
may have many reasons. This section discusses
these reasons and suggests how to isolate them.
In some hypotheses, charts involving data from
the W2C Wiki Corpus are used, which are intro-
duced in Section 4.
3.1 Training Data Size
In many NLP applications, size of the available
training data influences overall performance of
the system, as was shown by Halevy et al(2009).
To investigate the influence of training data
size, we decided to use two different sizes of train-
ing data ? 1 MB and 4 MB. If the drop in accu-
racy is caused by the lack of training data, then
all methods used on 4 MB should outperform the
same methods used on 1 MB of data.
3.2 Language Diversity
The increasing number of languages recognised
by the system decreases language diversity. This
may be another reason for the observed drop
in the accuracy. We used information about
language classes from the Ethnologue website
(Lewis, 2009). The number of different language
classes is depicted in Figure 1. Class 1 represents
the most distinguishable classes, such as Indo-
European vs. Japonic, while Class 2 represents
finer classification, such as Indo-European, Ger-
manic vs. Indo-European, Italic.
 0
 5
 10
 15
 20
 25
 30
 10  20  30  40  50  60  70  80  90
Lan
gua
ge 
Fam
ilies
Languages
Class 1Class 2
Figure 1: Language diversity on Wikipedia. Lan-
guages are sorted according to their text corpus size.
The first 52 languages belong to 15 different
Class 1 classes and the number of classes does not
47
change until the 77th language, when the Swahili
language from class Niger-Congo appears.
3.3 Scalability
Another issue with increasing number of lan-
guages is the scalability of used methods. There
are several pitfalls for machine learning algo-
rithms ? a) many languages may require many
features which may lead to failures caused by
curse-of-dimensionality, b) differences in lan-
guages may shrink, so the classifier will be forced
to learn minor differences and will lose its abil-
ity to generalise, and become overfitted, and c)
the classifier may internally use only binary clas-
sifiers which may lead up to quadratic complexity
(Dimitriadou et al 2011).
4 Data Sets
For our experiments, we decided to use the W2C
Wiki Corpus (Majlis?, 2012) which contains arti-
cles from Wikipedia. The total size of all texts
was 8 GB and available material for various lan-
guages differed significantly, as is displayed in
Figure 2.
 0
 50
 100
 150
 200
 250
 300
 350
 400
 450
 10  20  30  40  50  60  70  80  90
Siz
e in
 MB
Language
W2C Wiki Corpus - Size in MB
Figure 2: Available data in the W2C Wiki Corpus.
Languages are sorted according to their size in the cor-
pus.
We used this corpus to prepare 3 different data
sets. We used one of them for testing hypothesis
presented in the previous section and the remain-
ing two for comparison with other systems. These
data sets contain samples of length approximately
30, 140, and 1000 bytes. The sample of length 30
represents image caption or book title, the sample
of length 140 represents tweet or user comment,
and sample of length 1000 represents newspaper
article.
All datasets are available at http://ufal.
mff.cuni.cz/?majlis/yali/.
4.1 Long
The main purpose of this data set (yali-dataset-
long) was testing hypothesis described in the pre-
vious section.
To investigate the drop, we intended to cover
around 100 languages, but the amount of available
data limited us. For example, the 80th language
has 12 MB, whereas the 90th has 6 MB and tbe
100th has only 1 MB of text. To investigate the
hypothesis of the influence of training data size,
we decided to build a 1 MB and 4 MB corpus for
each language, where the 1 MB corpus is a subset
of the 4 MB one.
Then, we divided the corpus for each language
into chunks with 1000 bytes of text, so we gained
1000 and 4000 chunks respectively. These chunks
were divided into training and testing sets in a
90:10 ratio, thus we had 900 and 3600 train-
ing chunks, respectively, and 100 and 400 testing
chunks respectively.
To reduce the risk that the training and testing
are influenced by the position from which they
were taken (the beginning or the end of the cor-
pus), we decided to use every 10th sentence as a
testing one and use the remaining ones for train-
ing.
Then, we created an n-gram for n in 1..4 fre-
quency list for each language, each corpus size.
From each frequency list, we preserved only the
first m = 100 most frequent n-grams. For exam-
ple, from the raw frequency list ? a: 5, b: 3, c: 1,
d: 1, and m = 2, frequency list a: 5, b: 3 would
be created. We used this n-grams as features for
testing classifiers.
4.2 Small
The second data set (yali-dataset-small ) was pre-
pared for comparison with Google Translate4
(GT). The GT is paid service capable of recog-
nizing 50 different languages. This data set con-
tains 50 samples of lengths 30 and 140 for 48 lan-
guages, so it contains 4,800 samples in total.
4.3 Standard
The purpose of the third data sets is compari-
son with other systems for language identifica-
tion. This data set contains 700 samples of length
30, 140, and 1000 for 90 languages, so it contains
in total 189,000 samples.
4http://translate.google.com
48
Size L\N 1 2 3 4
30 177 1361 2075 2422
1MB 60 182 1741 3183 4145
90 186 1964 3943 5682
30 176 1359 2079 2418
4MB 60 182 1755 3184 4125
90 187 1998 3977 5719
Table 1: The number of unique N-grams in corpus
Size with L languages. (D(Size,L,n))
5 Methods
To investigate the influence of the language di-
versity, we decided to use 3 different language
counts ? 30, 60, and 90 languages sorted ac-
cording to their raw text size. For each cor-
pus size (cS ? {1000, 4000}), language
count (lC ? {30, 60, 90}), and n-gram size
(n ? {1, 2, 3, 4}) we constructed a separate dic-
tionary D(cS,lC,n) containing the first 100 most
frequent n-grams for each language. The number
of items in each dictionary is displayed in Table 1
and visualised for 1 MB corpus in Figure 3.
The dictionary sizes for 4 MB corpora were
slightly higher when compared to 1 MB corpora,
but surprisingly for 30 languages it was mostly
opposite.
 0
 1000
 2000
 3000
 4000
 5000
 6000
 20  40  60  80  100  120
Un
iqu
e n
-gra
ms
Languages (lC)
n=1
n=2
n=3
n=4
Figure 3: The number of unique n-grams in the dic-
tionary D(1000,lC,n). Languages are sorted according
to their text corpus size.
Then, we converted all texts into matri-
ces in the following way. For each cor-
pus size (cS ? {1000, 4000}), language
count (lC ? {30, 60, 90}), and n-gram size
(n ? {1, 2, 3, 4}) we constructed a training ma-
trix Tr(cS,lC,n) and a testing matrix Te(cS,lC,n),
where element on Tr(cS,lC,n)i,j represents the num-
ber of occurrences of j-th n-gram from dic-
tionary D(cS,lC,n) in training sample i, and
Tr(cS,lC,n)i,0 represents language of that sample.
The training matrix Tr(cS,lC,n) has dimension
(0.9 ? cS ? lC) ? (1 + | D(cS,lC,n) |)
and the testing matrix Te(cS,lC,n) has dimension
(0.1 ? cS ? lC)? (1 + | D(cS,lC,n) |).
For investigating the scalability of the differ-
ent approaches to language identification, we de-
cided to use five different methods. Three of them
were based on standard classification algorithms
and two of them were based on scoring function.
For experimenting with the classification algo-
rithms, we used R (2009) environment which con-
tains many packages with machine learning algo-
rithms5, and for scoring functions we used Perl.
5.1 Support Vector Machine
The Suport Vector Machine (SVM) is a state of
the art algorithm for classification. Hornik et al
(2006) compared four different implementations
and concluded that Dimitriadou et al(2011) im-
plementation available in the package e1071 is the
fastest one. We used SVM with sigmoid kernel,
cost of constraints violation set to 10, and termi-
nation criterion set to 0.01.
5.2 Naive Bayes
The Naive Bayes classifier (NB) is a simple prob-
abilistic classifier. We used Dimitriadou et al
(2011) implementation from the package e1071
with default arguments.
5.3 Regression Tree
Regression trees are implemented by Therneau et
al. (2010) in the package rpart. We used it with
default arguments.
5.4 W2C
The W2C algorithm is the same as was used by
Majlis? (2011). From the frequency list, probabil-
ity is computed for each n-gram, which is used as
a score in classification. The language with the
highest score is the winning one. For example,
from the raw frequency list ? a: 5, b: 3, c: 1, d: 1,
and m=2, the frequency list a: 5; b: 3, and com-
puted scores ? a: 0.5, b: 0.3 would be created.
5http://cran.r-project.org/web/views/
MachineLearning.html
49
5.5 Yet Another Language Identifier
The Yet Another Language Identifier (YALI) al-
gorithm is based on the W2C algorithm with two
small modifications. The first is modification in
n-gram score computation. The n-gram score is
not based on its probability in raw data, but rather
on its probability in the preserved frequency list.
So for the numbers used in the W2C example, we
would receive scores ? a: 0.625, b: 0.375. The
second modification is using rather byte n-grams
instead of character n-grams.
6 Results & Discussion
At the beginning we used only data set yali-
dataset-long to investigate the influence of vari-
ous set-ups.
The accuracy of all experiments is presented
in Table 2, and visualised in Figure 4 and Fig-
ure 5. These experiments also revealed that algo-
rithms are strong in different situations. All clas-
sification techniques outperform all scoring func-
tions on short n-grams and small amount of lan-
guages. However, with increasing n-gram length,
their accuracy stagnated or even dropped. The in-
creased number of languages is unmanageable for
NB a RPART classifiers and their accuracy sig-
nificantly decreased. On the other hand, the ac-
curacy of scoring functions does not decrease so
much with additional languages. The accuracy of
the W2C algorithm decreased when greater train-
ing corpora was used or more languages were
classified, whereas the YALI algorithm did not
have these problems, but moreover its accuracy
increased with greater training corpus.
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 1  2  3  4
Acc
ura
cy
N-Gram
SVMNBRPARTW2CYALI
Figure 4: Accuracy for 90 languages and 1 MB cor-
pus with respect to n-gram length.
 60
 65
 70
 75
 80
 85
 90
 95
 100
 30  60  90
Acc
ura
cy
Language Count
SVMn=2NBn=1RPARTn=1W2Cn=4YALIn=4
Figure 5: Accuracy for 1 MB corpus and the best
n-gram length with respect to the number of languages.
The highest accuracy for all language
amounts ? 30, 60, 90 was achieved by the
SVM with accuracies of 100%, 99%, and 98.5%,
respectively, followed by the YALI algorithm
with accuracies of 99.9%, 96.8%, and 95.4%
respectively.
From the obtained results, it is possible to no-
tice that 1 MB of text is sufficient for training lan-
guage identifiers, but some algorithms achieved
higher accuracy with more training material.
Our next focus was on the scalability of the
used algorithms. Time required for training is pre-
sented in Table 3, and visualised in Figures 6 and
7.
The training of scoring functions required only
loading dictionaries and therefore is extremely
fast, whereas training classifiers required compli-
cated computations. The scoring functions did not
have any advantages, because all algorithms had
to load all training examples, segment them, ex-
tract the most common n-grams, build dictionar-
ies, and convert text to matrices as was described
in Section 5.
 1
 10
 100
 1000
 10000
 100000
 1  2  3  4
Tra
inin
g T
ime
 (s)
N-Gram
SVMNBRPARTW2CYALI
Figure 6: Training time for 90 languages and 1 MB
corpus with respect to n-gram length.
50
N-Gram L 1 2 3 4
Method S 1MB 4MB 1MB 4MB 1MB 4MB 1MB 4MB
30 96.3% 96.7% 100.0% 99.9% 100.0% 99.9% 99.9% 99.9%
SVM 60 91.5% 92.3% 98.5% 98.5% 99.0% 99.0% 98.6% 98.5%
90 90.8% 91.6% 98.0% 98.0% 98.5% - 98.3% -
30 91.8% 94.2% 91.3% 90.9% 82.2% 93.3% 32.1% 59.9%
NB 60 78.7% 84.8% 70.6% 68.2% 71.7% 77.6% 25.7% 34.0%
90 75.4% 82.7% 68.8% 66.5% 64.3% 71.0% 18.4% 17.5%
30 97.3% 96.7% 98.8% 98.6% 98.4% 97.8% 97.7% 97.4%
RPART 60 90.2% 91.2% 67.3% 72.0% 67.2% 68.8% 65.5% 74.6%
90 64.3% 55.9% 39.7% 39.6% 43.0% 44.0% 38.5% 39.6%
30 38.0% 38.6% 89.9% 91.0% 96.2% 96.5% 97.9% 98.1%
W2C 60 34.7% 30.9% 83.0% 81.7% 86.0% 84.9% 89.1% 82.0%
90 34.7% 30.9% 77.8% 77.6% 84.9% 83.4% 87.8% 82.7%
30 38.0% 38.6% 96.7% 96.2% 99.6% 99.5% 99.9% 99.8%
YALI 60 35.0% 31.2% 86.1% 86.1% 95.7% 96.4% 96.8% 97.4%
90 34.9% 31.1% 86.8% 87.8% 95.0% 95.6% 95.4% 96.1%
Table 2: Accuracy of classifiers for various corpora sizes, n-gram lengths, and language counts.
 1
 10
 100
 1000
 10000
 100000
 30  60  90
Tra
inin
g T
ime
 (s)
Language Count
SVMn=2NBn=1RPARTn=1W2Cn=4YALIn=4
Figure 7: Training time for 1 MB corpus and the best
n-gram length with respect to the number of languages.
Time required for training increased dramat-
ically for SVM and RPART algorithms when
the number of languages or the corpora size in-
creased. It is possible to use the SVM only with
unigrams or bigrams, because training on trigrams
required 12 times more time for 60 languages
compared with 30 languages. The SVM also had
problems with increasing corpora sizes, because it
took almost 10-times more time when the corpus
size increased 4 times. Scoring functions scaled
well and were by far the fastest ones. We ter-
minated training the SVM on trigrams and quad-
grams for 90 languages after 5 days of computa-
tion.
Finally, we also measured time required for
classifying all testing examples. The results are
in Table 4, and visualised in Figure 8 and Fig-
ure 6. Times displayed in the table and charts rep-
resents the number of seconds needed for classi-
fying 1000 chunks.
 0.1
 1
 10
 100
 1000
 10000
 1  2  3  4
Pre
dic
tion
 Tim
e (s
/100
0 ch
unk
s)
N-Gram
SVMNBRPARTW2CYALI
Figure 8: Prediction time for 90 languages and 1 MB
corpus with respect to n-gram length.
 0
 10
 20
 30
 40
 50
 60
 70
 30  60  90
Pre
dic
tion
 Tim
e (s
/100
0 ch
unk
s)
Language Count
SVMn=2NBn=1RPARTn=1W2Cn=4YALIn=4
Prediction time for 1 MB corpus and the best n-gram
length with respect to the number of languages.
The RPART algorithm was the fastest classifier
followed by both scoring functions, whereas NB
was the slowest one. All algorithms with 4 times
more data achieved slightly higher accuracy, but
their training took 4 times longer, with the ex-
ception of the SVM which took at least 10 times
longer. The SVM algorithm is the least scalable
51
N-Gram L 1 2 3 4
Method S 1MB 4MB 1MB 4MB 1MB 4MB 1MB 4MB
30 215 1858 663 1774 627 7976 655 3587
SVM 60 1499 13653 7981 87260 7512 44288 26943 207123
90 2544 24841 12698 267824 76693 - 27964 -
30 5 19 27 83 40 144 54 394
NB 60 9 32 76 255 142 515 363 1187
90 12 56 188 683 298 1061 672 2245
30 44 189 144 946 267 1275 369 1360
RPART 60 162 1332 736 3447 1270 11114 2583 7493
90 351 1810 1578 7647 5139 23413 6736 17659
30 1 1 0 1 0 1 1 1
W2C 60 1 2 1 2 2 1 2 2
90 1 1 1 1 3 1 2 1
30 1 1 1 1 1 1 1 1
YALI 60 2 2 2 2 2 2 2 0
90 2 1 2 1 3 1 3 2
Table 3: Training Time
Method 30 60 90
Acc 100.0% 98.5% 98.0%
SVM Tre 663 7981 12698
n=2 Pre 10.3 66.2 64.1
Acc 91.8% 78.7% 75.4%
NB Tre 5 9 12
n=1 Pre 13.0 18.2 22.2
Acc 97.3% 90.2% 64.3%
RPART Tre 44 162 351
n=1 Pre 0.1 0.2 0.1
Acc 97.9% 89.1% 87.8%
W2C Tre 1 2 2
n=4 Pre 1.3 2.8 12.3
Acc 99.9% 96.8% 95.4%
YALI Tre 1 2 3
n=4 Pre 1.3 2.7 3.6
Table 5: Comparison of classifiers with best param-
eters. Label Acc represents accuracy, Tre represents
training time in seconds, and Pre represents prediction
time for 1000 chunks in seconds.
algorithm of all the examined ? all the rest re-
quired proportionally more time for training and
prediction when the greater training corpus was
used or more languages were classified.
The comparison of all methods is presented in
Table 5. For each model we selected the n-grams
size with the best trade-off between accuracy and
time required for training and prediction. The two
most accurate algorithms are SVM and YALI. The
SVM achieved the highest accuracy for all lan-
guages but its training took around 4000 times
longer and classification was around 17 times
slower than the YALI.
In the next step we evaluated the YALI algo-
rithm for various size of selected n-grams. These
Languages
Size 30 140 1000
100 64.9% 85.7 % 93.8 %
200 68.7% 87.3 % 93.9 %
400 71.7% 88.0 % 94.0 %
800 73.7% 88.5 % 94.0 %
1600 75.0% 88.8% 94.0%
Table 6: Effect of the number of selected 4-grams on
accuracy.
experiments were evaluated on the data set yali-
dataset-standard. Achieved results are presented
in Table 6. The number of used n-grams increased
the accuracy for short samples from 64.9% to
75.0% but it had no effect on long samples.
As the last step in evaluation we decided to
compare the YALI with Google Translate (GT),
which also provides language identification for 50
languages through their API.6 For comparison we
used data set yali-dataset-small which contains 50
samples of length 30 and 140 for each language
(4800 samples in total). Achieved results are pre-
sented in Table 7. The GT and the YALI per-
form comparably well on samples of length 30 on
which they achieved accuracy 93.6% and 93.1%
respectively, but on samples of length 140 GT
with accuracy 97.3% outperformed YALI with ac-
curacy 94.8%.
7 Conclusions & Future Work
In this paper we compared 5 different algorithms
for language identification ? three based on the
6http://code.google.com/apis/language/
translate/v2/using_rest.html
52
N-Gram L 1 2 3 4
Method S 1MB 4MB 1MB 4MB 1MB 4MB 1MB 4MB
30 3.7 7.3 10.3 6.8 9.0 31.8 9.3 13.8
SVM 60 13.3 30.1 66.2 189.7 59.8 92.8 236.7 375.2
90 16.1 36.7 64.1 381.4 414.9 - 133.4 -
30 13.0 13.6 75.3 77.1 132.7 147.9 186.0 349.7
NB 60 18.2 18.8 155.3 162.0 291.5 297.4 860.3 676.0
90 22.2 24.7 318.1 251.9 546.3 469.3 1172.8 1177.8
30 0.1 0.1 0.3 0.1 0.1 0.2 0.7 0.2
RPART 60 0.2 0.1 0.2 0.0 0.2 0.4 0.8 0.2
90 0.1 0.1 0.2 0.1 0.4 0.3 1.2 0.3
30 0.7 0.8 1.7 1.6 3.3 1.5 1.3 2.2
W2C 60 1.3 1.3 2.2 2.4 2.7 2.5 2.8 2.9
90 2.1 1.8 4.0 3.2 4.4 3.8 12.3 5.8
30 0.7 0.8 1.0 1.2 2.0 1.9 1.3 2.2
YALI 60 1.3 1.5 1.8 2.2 2.5 2.2 2.7 2.5
90 2.2 1.8 2.7 2.9 4.4 3.5 3.6 3.7
Table 4: Prediction Time
Text Length
30 140
System
Google 93.6% 97.3%
YALI 93.1% 94.8%
Table 7: Comparison of Google Translate and YALI
on 48 languages.
standard classification algorithms (Support Vec-
tor Machine (SVM), Naive Bayes (NB), and Re-
gression Tree (RPART)) and two based on scoring
functions. For investigating the influence of the
amount of training data we constructed two cor-
pora from the Wikipedia with 90 languages. To
investigate the influence of number if identified
languages we created three sets with 30, 60, and
90 languages. We also measured time required for
training and classification.
Our experiments revealed that the standard
classification algorithms requires at most bi-
grams while the scoring ones required quad-
grams. We also showed that Regression Trees and
Naive Bayes are not suitable for language identifi-
cation because they achieved accuracy 64.3% and
75.4% respectively.
The best classifier for language identification
was the SVM algorithm which achieved accuracy
98% for 90 languages but its training took 4200
times more and its classification was 16 times
slower than the YALI algorithm with accuracy
95.4%. This YALI algorithm has also potential
for increasing accuracy and number of recognized
languages because it scales well.
We also showed that the YALI algorithm is
comparable with the Google Translate system.
Both systems achieved accuracy 93% for sam-
ples of length 30. On samples of length 140
Google Translate with accuracy 97.3% outper-
formed YALI with accuracy 94.8%.
All data sets as well as source codes are
available at http://ufal.mff.cuni.cz/
?majlis/yali/.
In the future we would like to focus on using
described techniques not only on recognizing lan-
guages but also on recognizing character encod-
ings which is directly applicable for web crawl-
ing.
Acknowledgments
The research has been supported by the grant
Khresmoi (FP7-ICT-2010-6-257528 of the EU
and 7E11042 of the Czech Republic).
References
[Baldwin and Lui2010] Timothy Baldwin and Marco
Lui. 2010. Language identification: the long and
the short of the matter. Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pp. 229?237.
[Beesley1988] Kenneth R. Beesley. 1988. Lan-
guage identifier: A computer program for automatic
natural-language identification of on-line text. Lan-
guages at Crossroads: Proceedings of the 29th An-
nual Conferenceof the American Translators Asso-
ciation, 12-16 October 1988, pp. 47-54.
[Cavnar and Trenkle1994] William B. Cavnar and John
M. Trenkle. 1994. N-gram-based text categoriza-
53
tion. In Proceedings of Symposium on Document
Analysis and Information Retrieval.
[Choong et al011] Chew Yew Choong, Yoshiki
Mikami, and Robin Lee Nagano. 2011. Language
Identification of Web Pages Based on Improved N
gram Algorithm. IJCSI, issue 8, volume 3.
[Dimitriadou et al011] Evgenia Dimitriadou, Kurt
Hornik, Friedrich Leisch, David Meyer, and and
Andreas Weingessel 2011. e1071: Misc Func-
tions of the Department of Statistics (e1071), TU
Wien. R package version 1.5-27. http://CRAN.
R-project.org/package=e1071.
[Gold1967] E. Mark Gold. 1967. Language iden-
tification in the limit. Information and Control,
5:447474.
[Grothe et al008] Lena Grothe, Ernesto William De
Luca, and Andreas Nrnberger. 2008. A Com-
parative Study on Language Identification Meth-
ods. Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08). Mar-
rakech, 980-985.
[Halevy et al009] Alon Halevy, Peter Norvig, and
Fernando Pereira. 2009. The unreasonable effec-
tiveness of data. IEEE Intelligent Systems, 24:8?
12.
[Hayati 2004] Katia Hayati. 2004. Language Iden-
tification on the World Wide Web. Master The-
sis, University of California, Santa Cruz. http:
//lily-field.net/work/masters.pdf.
[Hornik et al006] Kurt Hornik, Alexandros Karat-
zoglou, and David Meyer. 2006. Support Vec-
tor Machines in R. Journal of Statistical Software
2006., 15.
[Hughes et al006] Baden Hughes, Timothy Bald-
win, Steven Bird, Jeremy Nicholson, and Andrew
Mackinlay. 2006. Reconsidering language identifi-
cation for written language resources. Proceedings
of LREC2006, 485?488.
[Kruengkrai et al005] Canasai Kruengkrai, Prapass
Srichaivattana, Virach Sornlertlamvanich, and Hi-
toshi Isahara. 2005. Language identification based
on string kernels. In Proceedings of the 5th Interna-
tional Symposium on Communications and Infor-
mation Technologies (ISCIT2005), pages 896899,
Beijing, China.
[Leonard and Doddington1974] Gary R. Leonard and
George R. Doddington. 1974. Automatic language
identification. Technical report RADC-TR-74-200,
Air Force Rome Air Development Center.
[Lewis2009] M. Paul Lewis. 2009. Ethnologue: Lan-
guages of the World, Sixteenth edition. Dallas,
Tex.: SIL International. Online version: http:
//www.ethnologue.com/
[McNamee2005] Paul McNamee. 2005. Language
identification: a solved problem suitable for under-
graduate instruction. J. Comput. Small Coll, vol-
ume: 20, issue: 3, February 2005, 94?101. Consor-
tium for Computing Sciences in Colleges, USA.
[Majlis?2012] Martin Majlis?, Zdene?k Z?abokrtsky?.
2012. Language Richness of the Web. In Proceed-
ings of the Eight International Language Resources
and Evaluation (LREC?12), Istanbul, Turkey, May
2012.
[Majlis?2011] Martin Majlis?. 2011. Large Multilin-
gual Corpus. Mater Thesis, Charles University in
Prague.
[Martins and Silva2005] Bruno Martins and Ma?rio J.
Silva. 2005. Language identification in web pages.
Proceedings of the 2005 ACM symposium on Ap-
plied computing, SAC ?05, 764?768. ACM, New
York, NY, USA. http://doi.acm.org/10.
1145/1066677.1066852.
[R2009] R Development Core Team. 2009. R: A Lan-
guage and Environment for Statistical Computing.
R Foundation for Statistical Computing. ISBN 3-
900051-07-0. http://www.R-project.org,
[Sibun and Reynar1996] Penelope Sibun and Jeffrey C.
Reynar. 1996. Language identification: Examining
the issues. In Proceedings of the 5th Symposium on
Document Analysis and Information Retrieval.
[Therneau et al010] Terry M. Therneau, Beth Atkin-
son, and R port by Brian Ripley. 2010.
rpart: Recursive Partitioning. R package ver-
sion 3.1-48. http://CRAN.R-project.
org/package=rpart.
54
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267?274,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Formemes in English-Czech Deep Syntactic MT ?
Ondr?ej Du?ek, Zdene?k ?abokrtsk?, Martin Popel,
Martin Majli?, Michal Nov?k, and David Marec?ek
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?me?st? 25, Prague
{odusek,zabokrtsky,popel,majlis,mnovak,marecek}@ufal.mff.cuni.cz
Abstract
One of the most notable recent improve-
ments of the TectoMT English-to-Czech trans-
lation is a systematic and theoretically sup-
ported revision of formemes?the annotation
of morpho-syntactic features of content words
in deep dependency syntactic structures based
on the Prague tectogrammatics theory. Our
modifications aim at reducing data sparsity,
increasing consistency across languages and
widening the usage area of this markup.
Formemes can be used not only in MT, but in
various other NLP tasks.
1 Introduction
The cornerstone of the TectoMT tree-to-tree ma-
chine translation system is the deep-syntactic lan-
guage representation following the Prague tec-
togrammatics theory (Sgall et al, 1986), and its ap-
plication in the Prague Dependency Treebank (PDT)
2.01 (Hajic? et al, 2006), where each sentence is
analyzed to a dependency tree whose nodes corre-
spond to content words. Each node has a number
of attributes, but the most important (and difficult)
for the transfer phase are lemma?lexical informa-
tion, and formeme?surface morpho-syntactic infor-
? This research has been supported by the grants
FP7-ICT-2009-4-247762 (FAUST), FP7-ICT-2009-4-249119
(Metanet), LH12093 (Kontakt II), DF12P01OVV022 (NAKI),
201/09/H057 (Czech Science Foundation), GAUK 116310, and
SVV 265 314. This work has been using language resources de-
veloped and/or stored and/or distributed by the LINDAT-Clarin
project of the Ministry of Education of the Czech Republic
(project LM2010013).
1http://ufal.mff.cuni.cz/pdt2.0
mation, including selected auxiliary words (Pt?c?ek
and ?abokrtsk?, 2006; ?abokrtsk? et al, 2008).
This paper focuses on formemes?their definition
and recent improvements of the annotation, which
has been thoroughly revised in the course of prepa-
ration of the CzEng 1.0 parallel corpus (Bojar et al,
2012b), whose utilization in TectoMT along with the
new formemes version has brought the greatest ben-
efit to our English-Czech MT system in the recent
year. However, the area of possible application of
formemes is not limited to MT only or to the lan-
guage pair used in our system; the underlying ideas
are language-independent.
We summarize the development of morpho-
syntactic annotations related to formemes (Sec-
tion 2), provide an overview of the whole TectoMT
system (Section 3), then describe the formeme an-
notation (Section 4) and our recent improvements
(Section 5), as well as experimental applications, in-
cluding English-Czech MT (Section 6). The main
asset of the formeme revision is a first systematic re-
organization of the existing practical aid, providing
it with a solid theoretical base, but still bearing its
intended applications in mind.
2 Related Work
Numerous theoretical approaches had been made
to morpho-syntactic description, mainly within va-
lency lexicons, starting probably with the work by
Helbig and Schenkel (1969). Perhaps the best one
for Czech is PDT-VALLEX (Hajic? et al, 2003), list-
ing all possible subtrees corresponding to valency
arguments (Ure?ov?, 2009). ?abokrtsk? (2005)
gives an overview of works in this field.
267
This kind of information has been most exploited
in structural MT systems, employing semantic re-
lations (Menezes and Richardson, 2001) or surface
tree substructures (Quirk et al, 2005; Marcu et al,
2006). Formemes, originally developed for Natural
Language Generation (NLG) (Pt?c?ek and ?abokrt-
sk?, 2006), have been successfully applied to MT
within the TectoMT system. Our revision of for-
meme annotation aims to improve the MT perfor-
mance, keeping other possible applications in mind.
3 The TectoMT English-Czech Machine
Translation System
The TectoMT system is a structural machine trans-
lation system with deep transfer, first introduced
by ?abokrtsk? et al (2008). It currently supports
English-to-Czech translation. Its analysis stage
follows the Prague tectogrammatics theory (Sgall,
1967; Sgall et al, 1986), proceeding over two layers
of structural description, from shallow (analytical)
to deep (tectogrammatical) (see Section 3.1).
The transfer phase of the system is based on Max-
imum Entropy context-sensitive translation models
(Marec?ek et al, 2010) and Hidden Tree Markov
Models (?abokrtsk? and Popel, 2009). It is factor-
ized into three subtasks: lemma, formeme and gram-
matemes translation (see Sections 3.2 and 3.3).
The subsequent generation phase consists of rule-
based components that gradually change the deep
target language representation into a shallow one,
which is then converted to text (cf. Section 6.1).
The version of TectoMT submitted to WMT122
builds upon the WMT11 version. Several rule-based
components were slightly refined. However, most of
the effort was devoted to creating a better and bigger
parallel treebank?CzEng 1.03 (Bojar et al, 2012b),
and re-training the statistical components on this re-
source. Apart from bigger size and improved filter-
ing, one of the main differences between CzEng 0.9
(Bojar and ?abokrtsk?, 2009) (used in WMT11) and
CzEng 1.0 (used in WMT12) is the revised annota-
tion of formemes.
2http://www.statmt.org/wmt12
3http://ufal.mff.cuni.cz/czeng
3.1 Layers of structural analysis
There are two distinct structural layers used in the
TectoMT system:
? Analytical layer. A surface syntax layer, which
includes all tokens of the sentence, organized
into a labeled dependency tree. The labels cor-
respond to surface syntax functions.
? Tectogrammatical layer. A deep syntax/se-
mantic layer describing the linguistic meaning
of the sentence. Its dependency trees include
only content words as nodes, assigning to each
of them a deep lemma (t-lemma), a semantic
role label (functor), and other deep linguistic
features (grammatemes), such as semantic part-
of-speech, person, tense or modality.
The analytical layer can be obtained using differ-
ent dependency parsers (Popel et al, 2011); the tec-
togrammatical representation is then created by rule-
based modules from the analytical trees.
In contrast to the original PDT annotation,
the TectoMT tectogrammatical layer also includes
formemes describing the surface morpho-syntactic
realization of the nodes (cf. also Section 3.3).
3.2 Transfer: Translation Factorization and
Symmetry
Using the tectogrammatical representation in struc-
tural MT allows separating the problem of translat-
ing a sentence into relatively independent simpler
subtasks: lemma, functors, and grammatemes trans-
lation (Bojar et al, 2009; ?abokrtsk?, 2010). Since
topology changes to deep syntax trees are rare in MT
transfer, each of these three subtasks allows a vir-
tually symmetric source-target one-to-one mapping,
thus simplifying the initial n-to-m mapping of word
phrases or surface subtrees.
?abokrtsk? et al (2008) obviated the need for
transfer via functors (i.e. semantic role detection)
by applying a formeme transfer instead. While
formeme values are much simpler to obtain by au-
tomatic processing, this approach preserved the ad-
vantage of symmetric one-to-one value translation.
Moreover, translations of a given source morpho-
syntactic construction usually follow a limited num-
ber of patterns in the target language regardless of
268
their semantic functions, e.g. a finite clause will
most often be translated as a finite clause.
3.3 Motivation for the Introduction of
Formemes
Surface-oriented formemes have been introduced
into the semantics-oriented tectogrammatical layer,
as it proves beneficial to combine the deep syntax
trees, smaller in size and more consistent across lan-
guages, with the surface morphology and syntax to
provide for a straightforward transition to the surface
level (?abokrtsk?, 2010).
The three-fold factorization of the transfer phase
(see Section 3.2) helps address the data sparsity is-
sue faced by today?s MT systems. As the translation
of lemmas and their morpho-syntactic forms is sepa-
rated, combinations unseen in the training data may
appear on the output.
To further reduce data sparsity, only minimal in-
formation needed to reconstruct the surface form is
stored in formemes; morphological categories deriv-
able from elsewhere, i.e. morphological agreement
or grammatemes, are discarded.
4 Czech and English Formemes in
TectoMT
A formeme is a concise description of relevant
morpho-syntactic features of a node in a tectogram-
matical tree (deep syntactic tree whose nodes usu-
ally correspond to content words). The general
shape of revised Czech and English formemes, as
implemented within the Treex4 NLP framework
(Popel and ?abokrtsk?, 2010) for the TectoMT sys-
tem, consists of three main parts:
1. Syntactic part-of-speech.5 The number of syn-
tactic parts-of-speech is very low, as only con-
tent words are used on the deep layer and the
categories of pronouns and numerals have been
divided under nouns and adjectives accord-
ing to syntactic behavior (?evc??kov?-Raz?mov?
and ?abokrtsk?, 2006). The possible values are
v for verbs, n for nouns, adj for adjectives,
and adv for adverbs.
4http://ufal.mff.cuni.cz/treex/,
https://metacpan.org/module/Treex
5Cf. Section 5.2 for details.
2. Subordinate conjunction/preposition. Applies
only to formemes of prepositional phrases and
subordinate clauses introduced by a conjunc-
tion and contains the respective conjunction or
preposition; e.g. if, on or in_case_of.
3. Form. This part represents the morpho-
syntactic form of the node in question and de-
pends on the part-of-speech (see Table 1).
The two or three parts are concatenated into
a human-readable string to facilitate usage in
hand-written rules as well as statistical systems
(?abokrtsk?, 2010), producing values such as
v:inf, v:if+fin or n:into+X. Formeme val-
ues of nodes corresponding to uninflected words are
atomic.
Formemes are detected by rule-based modules op-
erating on deep and surface trees. Example deep
syntax trees annotated with formemes are shown in
Fig. 1. A listing of all possible formeme values is
given in Table 1.
Verbal formemes remain quite consistent in both
languages, except for the greater range of forms in
English (Czech uses adjectives or nouns instead of
gerunds and verbal attributes). Nominal formemes
differ more significantly: Czech is a free-word order
language with rich morphology, where declension
is important to syntactic relations?case is therefore
included in formemes. As English makes its syntac-
tic relations visible rather with word-order than with
morphology, English formemes indicate the syntac-
tic position instead. The same holds for adjecti-
val complements to verbs. Posession is expressed
mostly using nouns in English and adjectives in
Czech, which is also reflected in formemes.
5 Recent Markup Improvements
Our following markup innovations address several
issues found in the previous version and aim to adapt
the range of values more accurately to the intended
applications.
5.1 General Form Changes
The relevant preposition and subordinate conjunc-
tion nodes had been selected based on their depen-
dency labels; we use a simple part-of-speech tag fil-
ter instead in order to minimize the influence of pars-
ing errors and capture more complex prepositions,
269
Figure 1: An example English and Czech deep sentence structure annotated with formemes (in typewriter font).
Formeme Language Definition
v:(P+)fin both Verbs as heads of finite clauses
v:rc both Verbs as heads of relative clauses
v:(P+)inf both Infinitive clauses; typically with the particle to in English?
v:(P+)ger EN Gerunds, e.g. I like reading (v:ger), but I am tired of arguing (v:of+ger).
v:attr EN Present or past participles (i.e. -ing or -ed forms) in the attributive syntactic
position, e.g. Striking (v:attr) teachers hate bored (v:attr) students.
n:[1..7] CS Bare nouns; the numbers indicate morphological case?
n:X CS Bare nouns that cannot be inflected
n:subj EN Nouns in the subject position (i.e. in front of the main verb of the clause)
n:obj EN Nouns in the object position (i.e. following the verb with no preposition)
n:obj1, n:obj2 EN Nouns in the object position; distinguishing the two objects of ditransitive
verbs (e.g. give, consider)
n:adv EN Nouns in an adverbial position, e.g. The sales went up by 1 % last month
n:P+X EN Prepositional phrases
n:P+[1..7] CS Prepositional phrases; the preposition surface form is combined with the re-
quired case?
n:attr both Nominal attributes, e.g. insurance company or president Smith in English
and prezident Smith in Czech
n:poss EN English possessive pronouns and nouns with the ?s suffix
adj:attr both Adjectival attributes (Czech inflection forms need not be stored thanks to
congruency with the parent noun)
adj:compl EN Direct adjectival complements to verbs
adj:[1..7] CS Direct adjectival complements to verbs (morphological case must be stored
in Czech, as it is determined by valency)
adj:poss CS Czech possesive adjectives and pronouns; a counterpart to English n:poss
adv both Adverbs (not inflected, can take no prepositions etc.)
x both Coordinating conjunctions, other uninflected words
drop both Deep tree nodes which do not appear on the surface (e.g. pro-drop pronouns)
?I.e. infinitives as head of clauses, not infinitives as parts of compound verb forms with finite auxiliary verbs.
?Numbers are traditionally used to mark morphological case in Czech; 1 stands for nominative, 2 for genitive etc.
?Since many prepositions may govern multiple cases in Czech, the case number is necessary.
Table 1: A listing of all possible formeme values, indicating their usage in Czech, English or both languages. ?P+?
denotes the (lowercased) surface form of a preposition or a subordinate conjunction. Round brackets denote optional
parts, square brackets denote a set of alternatives.
270
e.g. in case of. Our revision also allows combining
prepositions with all English gerunds and infinitives,
preventing a loss of important data.
We also use the lowercased surface form in the
middle formeme part instead of lemmas to allow for
a more straightforward surface form generation.
5.2 Introducing Syntactic Part-of-Speech
Formemes originally contained the semantic part-of-
speech (sempos) (Raz?mov? and ?abokrtsk?, 2006)
as their first part. We replaced it with a syntac-
tic part-of-speech (syntpos), since it proved compli-
cated to assign sempos reliably by a rule-based mod-
ule and morpho-syntactic behavior is more relevant
to formemes than semantics.
The syntpos is assigned in two steps:
1. A preliminary syntpos is selected, using our
categorization based on the part-of-speech tag
and lemma.
2. The final syntpos is selected according to the
syntactic position of the node, addressing nom-
inal usage of adjectives and cardinal numerals
(see Sections 5.4 and 5.5).
5.3 Capturing Czech Nominal Attributes
Detecting the attributive usage of nouns is straight-
forward for English, where any noun depending di-
rectly on another noun is considered an attribute.
In Czech, one needs to distinguish case-congruent
attributes from others that have a fixed case. We
aimed at assigning the n:attr formeme only in the
former case and thus replaced the original method
based on word order with a less error-prone one
based on congruency and named entity recognition.
5.4 Numerals: Distinguishing Usage and
Correcting Czech Case
The new formemes now distinguish adjectival and
nominal usage of cardinal numerals (cf. also Sec-
tion 5.2), e.g. the number in 5 potatoes is now as-
signed the adj:attr formeme, whereas Apollo 11
is given n:attr. The new situation is analogous
in Czech, with nominal usages of numerals having
their morphological case marked in formemes.
To reduce data sparsity in the new formemes ver-
sion, we counter the inconsistent syntactic behavior
of Czech cardinal numerals, where 1-4 behave like
The word ban?n is in genitive (n:2), but would have an ac-
cusative (n:4) form if the numeral behaved like an adjective.
Figure 2: Case correction with numerals in Czech.
adjectives but other numerals behave like nouns and
shift their semantically governing noun to the po-
sition of a genitive attribute. An example of this
change is given in Fig. 2.
5.5 Adjectives: Nominal Usage and Case
The new formemes address the usage of adjectives
in the syntactic position of nouns (cf. Section 5.2),
which occurs only rarely, thus preventing sparse val-
ues, namely in these syntactic positions:
? The subject. We replaced the originally as-
signed adj:compl value, which was impos-
sible to tell from adjectival objects, with the
formeme a noun would have in the same po-
sition, e.g. in the sentence Many of them were
late, the subject many is assigned n:subj.
? Prepositional phrases. Syntactic behavior of
adjectives is identical to nouns here; we thus
assign them the formeme values a noun would
receive in the same position, e.g. n:of+X in-
stead of adj:of+X in He is one of the best at
school.
In Czech, we detect nominal usage of adjectives
in verbal direct objects as well, employing large-
coverage valency lexicons (Lopatkov? et al, 2008;
Hajic? et al, 2003).
Instead of assigning the compl value in Czech,
our formemes revision includes the case of adjecti-
val complements, which depends on the valency of
the respective verb.
5.6 Mutual Information Across Languages
The changes described above have been motivated
not only by theoretical linguistic description of the
languages in question, but also by the intended us-
age within the TectoMT translation system. Instead
271
of retraining the translation model after each change,
we devised a simpler and faster estimate to measure
the asset of our innovations: using Mutual Informa-
tion (MI) (Manning and Sch?tze, 1999, p. 66) of
formemes in Czech and English trees.
We expect that an inter-language MI increase will
lead to lower noise in formeme-to-formeme transla-
tion dictionary (Bojar et al, 2009, cf. Section 3.2),
thus achieving higher MT output quality.
Using the analysis pipeline from CzEng1.0, we
measured the inter-language MI on sentences from
the Prague Czech-English Dependency Treebank
(PCEDT) 2.0 (Bojar et al, 2012a). The overall re-
sults show an MI increase from 1.598 to 1.687 (Bo-
jar et al, 2012b). Several proposed markup changes
have been discarded as they led to an inter-language
MI drop; e.g. removing the v:rc relative clause
formeme or merging the v:attr and adj:attr
values in English.
6 Experimental Usage
We list here our experiments with the newly de-
veloped annotation: an NLG experiment aimed at
assessing the impact of formemes on the synthesis
phase of the TectoMT system, and the usage in the
English-Czech MT as a whole.
6.1 Czech Synthesis
The synthesis phase of the TectoMT system relies
heavily on the information included in formemes, as
its rule-based blocks use solely formemes and gram-
mar rules to gradually change a deep tree node into
a surface subtree.
To directly measure the suitability of our changes
for the synthesis stage of the TectoMT system, we
used a Czech-to-Czech round trip?deep analysis of
Czech PDT 2.0 development set sentences using the
CzEng 1.0 pipeline (Bojar et al, 2012b), followed
directly by the synthesis part of the TectoMT sys-
tem. The results were evaluated using the BLEU
metric (Papineni et al, 2002) with the original sen-
tences as reference; they indicate a higher suitability
of the new formemes for deep Czech synthesis (see
Table 2).
6.2 English-Czech Machine Translation
To measure the influence of the presented formeme
revision on the translation quality, we compared
Version BLEU
Original formemes 0.6818
Revised formemes 0.7092
Table 2: A comparison of formeme versions in Czech-to-
Czech round trip.
Version BLEU
Original formemes 0.1190
Revised formemes 0.1199
Table 3: A comparison of formeme versions in English-
to-Czech TectoMT translation on the WMT12 test set.
two translation scenarios?one using the origi-
nal formemes and the second using the revised
formemes in the formeme-to-formeme translation
model. Due to time reasons, we were able to
train both translation models only on 1/2 of the
CzEng 1.0 training data.
The results in Table 3 demonstrate a slight6 BLEU
gain when using the revised formemes version. The
gain is expected to be greater if several rule-based
modules of the transfer phase are adapted to the re-
visions.
7 Conclusion and Further Work
We have presented a systematic and theoretically
supported revision of a surface morpho-syntactic
markup within a deep dependency annotation sce-
nario, designed to facilitate the TectoMT transfer
phase. Our first practical experiments proved the
merits of our innovations in the tasks of Czech syn-
thesis and deep structural MT as a whole. We have
also experimented with formemes in the functor as-
signment (semantic role labelling) task and gained
moderate improvements (ca. 1-1.5% accuracy).
In future, we intend to tune the rule-based parts
of our MT transfer for the new version of formemes
and examine further possibilities of data sparsity re-
duction (e.g. by merging synonymous formemes).
We are also planning to create formeme annotation
modules for further languages to widen the range of
language pairs used in the TectoMT system.
6Significant at 90% level using pairwise bootstrap resam-
pling test (Koehn, 2004).
272
References
O. Bojar and Z. ?abokrtsk?. 2009. CzEng 0.9: Large
Parallel Treebank with Rich Annotation. Prague Bul-
letin of Mathematical Linguistics, 92.
O. Bojar, D. Marec?ek, V. Nov?k, M. Popel, J. Pt?c?ek,
J. Rou?, and Z. ?abokrtsk?. 2009. English-Czech MT
in 2008. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 125?129. As-
sociation for Computational Linguistics.
O. Bojar, J. Hajic?, E. Hajic?ov?, J. Panevov?, P. Sgall,
S. Cinkov?, E. Fuc??kov?, M. Mikulov?, P. Pajas,
J. Popelka, J. Semeck?, J. ?indlerov?, J. ?te?p?nek,
J. Toman, Z. Ure?ov?, and Z. ?abokrtsk?. 2012a.
Announcing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of LREC 2012, Istanbul,
Turkey, May. ELRA, European Language Resources
Association. In print.
O. Bojar, Z. ?abokrtsk?, O. Du?ek, P. Galu?c??kov?,
M. Majli?, D. Marec?ek, J. Mar??k, M. Nov?k,
M. Popel, and A. Tamchyna. 2012b. The Joy of Par-
allelism with CzEng 1.0. In Proceedings of LREC
2012, Istanbul, Turkey, May. ELRA, European Lan-
guage Resources Association. In print.
J. Hajic?, J. Panevov?, Z. Ure?ov?, A. B?mov?,
V. Kol?rov?, and P. Pajas. 2003. PDT-VALLEX: Cre-
ating a large-coverage valency lexicon for treebank an-
notation. In Proceedings of The Second Workshop on
Treebanks and Linguistic Theories, volume 9, pages
57?68.
J. Hajic?, J. Panevov?, E. Hajic?ov?, P. Sgall, P. Pajas,
J. ?te?p?nek, J. Havelka, M. Mikulov?, Z. ?abokrtsk?,
and M. ?evc??kov?-Raz?mov?. 2006. Prague Depen-
dency Treebank 2.0. CD-ROM LDC2006T01, LDC,
Philadelphia.
G. Helbig and W. Schenkel. 1969. W?rterbuch zur
Valenz und Distribution deutscher Verben. VEB Bib-
liographisches Institut, Leipzig.
P. Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain.
M. Lopatkov?, Z. ?abokrtsk?, V. Kettnerov?, and
K. Skwarska. 2008. Valenc?n? slovn?k c?esk?ch sloves.
Karolinum, Prague.
C.D. Manning and H. Sch?tze. 1999. Foundations of
statistical natural language processing. MIT Press.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntacti-
fied target language phrases. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 44?52. Association for
Computational Linguistics.
D. Marec?ek, M. Popel, and Z. ?abokrtsk?. 2010. Maxi-
mum entropy translation model in dependency-based
MT framework. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and Met-
rics (MATR), pages 201?206. Association for Compu-
tational Linguistics.
A. Menezes and S. D. Richardson. 2001. A best-first
alignment algorithm for automatic extraction of trans-
fer mappings from bilingual corpora. In Proceed-
ings of the workshop on Data-driven methods in ma-
chine translation - Volume 14, DMMT ?01, pages 1?8,
Stroudsburg, PA. Association for Computational Lin-
guistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meet-
ing on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
M. Popel and Z. ?abokrtsk?. 2010. TectoMT: modular
NLP framework. Advances in Natural Language Pro-
cessing, pages 293?304.
M. Popel, D. Marec?ek, N. Green, and Z. ?abokrtsk?.
2011. Influence of parser choice on dependency-based
MT. In Chris Callison-Burch, Philipp Koehn, Christof
Monz, and Omar Zaidan, editors, Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 433?439, Edinburgh, UK. Association for Com-
putational Linguistics.
J. Pt?c?ek and Z. ?abokrtsk?. 2006. Synthesis of
Czech sentences from tectogrammatical trees. In Text,
Speech and Dialogue, pages 221?228. Springer.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 271?279. Association for Computational
Linguistics.
M. Raz?mov? and Z. ?abokrtsk?. 2006. Annotation
of grammatemes in the Prague Dependency Treebank
2.0. In Proceedings of the LREC 2006 Workshop on
Annotation Science, pages 12?19.
M. ?evc??kov?-Raz?mov? and Z. ?abokrtsk?. 2006. Sys-
tematic parameterized description of pro-forms in the
Prague Dependency Treebank 2.0. In J. Hajic? and
J. Nivre, editors, Proceedings of the Fifth Workshop on
Treebanks and Linguistic Theories (TLT), pages 175?
186, Prague.
P. Sgall, E. Hajic?ov?, J. Panevov?, and J. Mey. 1986. The
meaning of the sentence in its semantic and pragmatic
aspects. Springer.
P. Sgall. 1967. Generativn? popis jazyka a c?esk? dekli-
nace. Academia, Prague.
Z. Ure?ov?. 2009. Building the PDT-VALLEX valency
lexicon. In On-line proceedings of the fifth Corpus
Linguistics Conference. University of Liverpool.
273
Z. ?abokrtsk?, J. Pt?c?ek, and P. Pajas. 2008. Tec-
toMT: highly modular MT system with tectogrammat-
ics used as transfer layer. In Proceedings of the Third
Workshop on Statistical Machine Translation, StatMT
?08, pages 167?170, Stroudsburg, PA. Association for
Computational Linguistics.
Z. ?abokrtsk?. 2005. Valency Lexicon of Czech Verbs.
Ph.D. thesis, Charles University in Prague.
Z. ?abokrtsk?. 2010. From Treebanking to Machine
Translation. Habilitation thesis, Charles University in
Prague.
Z. ?abokrtsk? and M. Popel. 2009. Hidden Markov
Tree Model in Dependency-based Machine Transla-
tion. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 145?148, Suntec, Singa-
pore.
274
