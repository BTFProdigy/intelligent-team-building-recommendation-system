Data-Driven Strategies for an Automated Dialogue System 
Hilda HARDY, Tomek 
STRZALKOWSKI, Min WU 
ILS Institute 
University at Albany, SUNY 
1400 Washington Ave., SS262 
Albany, NY  12222   USA 
hhardy|tomek|minwu@ 
cs.albany.edu  
Cristian URSU, Nick WEBB 
Department of Computer Science 
University of Sheffield 
Regent Court, 211 Portobello St. 
Sheffield  S1 4DP   UK 
c.ursu@sheffield.ac.uk, 
n.webb@dcs.shef.ac.uk 
Alan BIERMANN, R. Bryce 
INOUYE, Ashley MCKENZIE 
Department of Computer Science 
Duke University 
P.O. Box 90129, Levine Science 
Research Center, D101  
Durham, NC  27708   USA 
awb|rbi|armckenz@cs.duke.edu 
 
Abstract 
We present a prototype natural-language 
problem-solving application for a financial 
services call center, developed as part of the 
Amiti?s multilingual human-computer 
dialogue project. Our automated dialogue 
system, based on empirical evidence from real 
call-center conversations, features a data-
driven approach that allows for mixed 
system/customer initiative and spontaneous 
conversation. Preliminary evaluation results 
indicate efficient dialogues and high user 
satisfaction, with performance comparable to 
or better than that of current conversational 
travel information systems. 
1 Introduction 
Recently there has been a great deal of interest in 
improving natural-language human-computer 
conversation. Automatic speech recognition 
continues to improve, and dialogue management 
techniques have progressed beyond menu-driven 
prompts and restricted customer responses. Yet 
few researchers have made use of a large body of 
human-human telephone calls, on which to form 
the basis of a data-driven automated system.  
The Amiti?s project seeks to develop novel 
technologies for building empirically induced 
dialogue processors to support multilingual 
human-computer interaction, and to integrate these 
technologies into systems for accessing 
information and services (http://www.dcs.shef.ac. 
uk/nlp/amities). Sponsored jointly by the European 
Commission and the US Defense Advanced 
Research Projects Agency, the Amiti?s Consortium 
includes partners in both the EU and the US, as 
well as financial call centers in the UK and France. 
A large corpus of recorded, transcribed 
telephone conversations between real agents and 
customers gives us a unique opportunity to analyze 
and incorporate features of human-human 
dialogues into our automated system. (Generic 
names and numbers were substituted for all 
personal details in the transcriptions.) This corpus 
spans two different application areas: software 
support and (a much smaller size) customer 
banking. The banking corpus of several hundred 
calls has been collected first and it forms the basis 
of our initial multilingual triaging application, 
implemented for English, French and German 
(Hardy et al, 2003a); as well as our prototype 
automatic financial services system, presented in 
this paper, which completes a variety of tasks in 
English. The much larger software support corpus 
(10,000 calls in English and French) is still being 
collected and processed and will be used to 
develop the next Amiti?s prototype. 
We observe that for interactions with structured 
data ? whether these data consist of flight 
information, spare parts, or customer account 
information ? domain knowledge need not be built 
ahead of time. Rather, methods for handling the 
data can arise from the way the data are organized. 
Once we know the basic data structures, the 
transactions, and the protocol to be followed (e.g., 
establish caller?s identity before exchanging 
sensitive information); we need only build 
dialogue models for handling various 
conversational situations, in order to implement a 
dialogue system. For our corpus, we have used a 
modified DAMSL tag set (Allen and Core, 1997) 
to capture the functional layer of the dialogues, and 
a frame-based semantic scheme to record the 
semantic layer (Hardy et al, 2003b). The ?frames? 
or transactions in our domain are common 
customer-service tasks: VerifyId, ChangeAddress, 
InquireBalance, Lost/StolenCard and Make 
Payment. (In this context ?task? and ?transaction? 
are synonymous.) Each frame is associated with 
attributes or slots that must be filled with values in 
no particular order during the course of the 
dialogue; for example, account number, name, 
payment amount, etc. 
2 Related Work 
Relevant human-computer dialogue research 
efforts include the TRAINS project and the 
DARPA Communicator program. 
The classic TRAINS natural-language dialogue 
project (Allen et al, 1995) is a plan-based system 
which requires a detailed model of the domain and 
therefore cannot be used for a wide-ranging 
application such as financial services. 
The US DARPA Communicator program has 
been instrumental in bringing about practical 
implementations of spoken dialogue systems. 
Systems developed under this program include 
CMU?s script-based dialogue manager, in which 
the travel itinerary is a hierarchical composition of 
frames (Xu and Rudnicky, 2000). The AT&T 
mixed-initiative system uses a sequential decision 
process model, based on concepts of dialog state 
and dialog actions (Levin et al, 2000). MIT?s 
Mercury flight reservation system uses a dialogue 
control strategy based on a set of ordered rules as a 
mechanism to manage complex interactions 
(Seneff and Polifroni, 2000). CU?s dialogue 
manager is event-driven, using a set of hierarchical 
forms with prompts associated with fields in the 
forms. Decisions are based not on scripts but on 
current context (Ward and Pellom, 1999). 
Our data-driven strategy is similar in spirit to 
that of CU. We take a statistical approach, in 
which a large body of transcribed, annotated 
conversations forms the basis for task 
identification, dialogue act recognition, and form 
filling for task completion.  
3 System Architecture and Components 
The Amiti?s system uses the Galaxy 
Communicator Software Infrastructure (Seneff et 
al., 1998). Galaxy is a distributed, message-based, 
hub-and-spoke infrastructure, optimized for spoken 
dialogue systems. 
  
 
Figure 1. Amiti?s System Architecture 
 
Components in the Amiti?s system (Figure 1) 
include a telephony server, automatic speech 
recognizer, natural language understanding unit, 
dialogue manager, database interface server, 
response generator, and text-to-speech conversion. 
3.1 Audio Components 
Audio components for the Amiti?s system are 
provided by LIMSI. Because acoustic models have 
not yet been trained, the current demonstrator 
system uses a Nuance ASR engine and TTS 
Vocalizer.  
To enhance ASR performance, we integrated 
static GSL (Grammar Specification Language) 
grammar classes provided by Nuance for 
recognizing several high-frequency items: 
numbers, dates, money amounts, names and yes-no 
statements. 
Training data for the recognizer were collected 
both from our corpus of human-human dialogues 
and from dialogues gathered using a text-based 
version of the human-computer system. Using this 
version we collected around 100 dialogues and 
annotated important domain-specific information, 
as in this example: ?Hi my name is [fname ; 
David] [lname ; Oconnor] and my account number 
is [account ; 278 one nine five].? 
Next we replaced these annotated entities with 
grammar classes. We also utilized utterances from 
the Amiti?s banking corpus (Hardy et al, 2002) in 
which the customer specifies his/her desired task, 
as well as utterances which constitute common, 
domain-independent speech acts such as 
acceptances, rejections, and indications of non-
understanding. These were also used for training 
the task identifier and the dialogue act classifier 
(Section 3.3.2). The training corpus for the 
recognizer consists of 1744 utterances totaling 
around 10,000 words. 
Using tools supplied by Nuance for building 
recognition packages, we created two speech 
recognition components: a British model in the UK 
and an American model at two US sites. 
For the text to speech synthesizer we used 
Nuance?s Vocalizer 3.0, which supports multiple 
languages and accents. We integrated the 
Vocalizer and the ASR using Nuance?s speech and 
telephony API into a Galaxy-compliant server 
accessible over a telephone line. 
3.2 Natural Language Understanding 
The goal of the language understanding 
component is to take the word string output of the 
ASR module, and identify key semantic concepts 
relating to the target domain. This is a specialized 
kind of information extraction application, and as 
such, we have adapted existing IE technology to 
this task.  
Hub 
Speech 
Recognition 
Dialogue 
Manager Database 
Server 
Nat?l Language 
Understanding 
Telephony 
Server 
Response      
Generation 
Customer 
Database 
Text-to-speech
Conversion 
We have used a modified version of the ANNIE 
engine (A Nearly-New IE system; Cunningham et 
al., 2002; Maynard, 2003). ANNIE is distributed as 
the default built-in IE component of the GATE 
framework (Cunningham et al, 2002). GATE is a 
pure Java-based architecture developed over the 
past eight years in the University of Sheffield 
Natural Language Processing group. ANNIE has 
been used for many language processing 
applications, in a number of languages both 
European and non-European. This versatility 
makes it an attractive proposition for use in a 
multilingual speech processing project. 
ANNIE includes customizable components 
necessary to complete the IE task ? tokenizer, 
gazetteer, sentence splitter, part of speech tagger 
and a named entity recognizer based on a powerful 
engine named JAPE (Java Annotation Pattern 
Engine; Cunningham et al, 2000). 
Given an utterance from the user, the NLU unit 
produces both a list of tokens for detecting 
dialogue acts, an important research goal inside 
this project, and a frame with the possible named 
entities specified by our application. We are 
interested particularly in account numbers, credit 
card numbers, person names, dates, amounts of 
money, locations, addresses and telephone 
numbers.  
In order to recognize these, we have updated the 
gazetteer, which works by explicit look-up tables 
of potential candidates, and modified the rules of 
the transducer engine, which attempts to match 
new instances of named entities based on local 
grammatical context. There are some significant 
differences between the kind of prose text more 
typically associated with information extraction, 
and the kind of text we are expecting to encounter. 
Current models of IE rely heavily on punctuation 
as well as certain orthographic information, such as 
capitalized words indicating the presence of a 
name, company or location. We have access to 
neither of these in the output of the ASR engine, 
and so had to retune our processors to data which 
reflected that. 
In addition, we created new processing 
resources, such as those required to spot number 
units and translate them into textual representations 
of numerical values; for example, to take ?twenty 
thousand one hundred and fourteen pounds?, and 
produce ??20,114?. The ability to do this is of 
course vital for the performance of the system. 
If none of the main entities can be identified 
from the token string, we create a list of possible 
fallback entities, in the hope that partial matching 
would help narrow the search space. 
For instance, if a six-digit account number is not 
identified, then the incomplete number recognized 
in the utterance is used as a fallback entity and sent 
to the database server for partial matching. 
Our robust IE techniques have proved 
invaluable to the efficiency and spontaneity of our 
data-driven dialogue system. In a single utterance 
the user is free to supply several values for 
attributes, prompted or unprompted, allowing tasks 
to be completed with fewer dialogue turns. 
3.3 Dialogue Manager 
The dialogue manager identifies the goals of the 
conversation and performs interactions to achieve 
those goals. Several ?Frame Agents?, implemented 
within the dialogue manager, handle tasks such as 
verifying the customer?s identity, identifying the 
customer?s desired transaction, and executing those 
transactions. These range from a simple balance 
inquiry to the more complex change of address and 
debit-card payment. The structure of the dialogue 
manager is illustrated in Figure 2. 
Rather than depending on a script for the 
progression of the dialogue, the dialogue manager 
takes a data-driven approach, allowing the caller to 
take the initiative. Completing a task depends on 
identifying that task and filling values in frames, 
but this may be done in a variety of ways: one at a 
time, or several at once, and in any order. 
For example, if the customer identifies himself 
or herself before stating the transaction, or even if 
he or she provides several pieces of information in 
one utterance?transaction, name, account number, 
payment amount?the dialogue manager is flexible 
enough to move ahead after these variations. 
Prompts for attributes, if needed, are not restricted 
to one at a time, but they are usually combined in 
the way human agents request them; for example, 
city and county, expiration date and issue number, 
birthdate and telephone number. 
 
 
 
Figure 2. Amiti?s Dialogue Manager 
If the system fails to obtain the necessary values 
from the user, reprompts are used, but no more 
than once for any single attribute. For the customer 
verification task, different attributes may be 
 
 
 
 
 
 
 
 
 Response Decision 
Input:  
from NLU via 
Hub (token string, 
language id, 
named entities) 
Task infoExternal files, 
domain-specific
Dialogue Act 
Classifier 
Frame Agent 
Task ID 
Frame Agent 
Verify-Caller 
Frame Agent 
DB Server 
Customer 
Database
 
 
 
 
 
 
Task Execution 
Frame Agents via Hub 
Dialogue History 
requested. If the system fails even after reprompts, 
it will gracefully give up with an explanation such 
as, ?I?m sorry, we have not been able to obtain the 
information necessary to update your address in 
our records. Please hold while I transfer you to a 
customer service representative.? 
3.3.1 Task ID Frame Agent 
For task identification, the Amiti?s team has 
made use of the data collected in over 500 
conversations from a British call center, recorded, 
transcribed, and annotated. Adapting a vector-
based approach reported by Chu-Carroll and 
Carpenter (1999), the Task ID Frame Agent is 
domain-independent and automatically trained. 
Tasks are represented as vectors of terms, built 
from the utterances requesting them. Some 
examples of labeled utterances are: ?Erm I'd like to 
cancel the account cover premium that's on my, 
appeared on my statement? [CancelInsurance] and 
?Erm just to report a lost card please? 
[Lost/StolenCard].   
The training process proceeds as follows: 
1. Begin with corpus of transcribed, annotated 
calls. 
2. Document creation: For each transaction, collect 
raw text of callers? queries. Yield: one 
?document? for each transaction (about 14 of 
these in our corpus). 
3. Text processing: Remove stopwords, stem 
content words, weight terms by frequency. 
Yield: one ?document vector? for each task. 
4. Compare queries and documents: Create ?query 
vectors.? Obtain a cosine similarity score for 
each query/document pair. Yield: cosine 
scores/routing values for each query/document 
pair. 
5. Obtain coefficients for scoring: Use binary 
logistic regression. Yield: a set of coefficients 
for each task. 
Next, the Task ID Frame Agent is tested on 
unseen utterances or queries: 
1. Begin with one or more user queries. 
2. Text processing: Remove stopwords, stem 
content words, weight terms (constant weights). 
Yield: ?query vectors?. 
3. Compare each query with each document. 
Yield: cosine similarity scores. 
4. Compute confidence scores (use training 
coefficients). Yield: confidence scores, 
representing the system?s confidence that the 
queries indicate the user?s choice of a particular 
transaction. 
Tests performed over the entire corpus, 80% of 
which was used for training and 20% for testing, 
resulted in a classification accuracy rate of 85% 
(correct task is one of the system?s top 2 choices). 
The accuracy rate rises to 93% when we eliminate 
confusing or lengthy utterances, such as requests 
for information about payments, statements, and 
general questions about a customer?s account. 
These can be difficult even for human annotators 
to classify. 
3.3.2 Dialogue Act Classifier 
The purpose of the DA Classifier Frame Agent 
is to identify a caller?s utterance as one or more 
domain-independent dialogue acts. These include 
Accept, Reject, Non-understanding, Opening, 
Closing, Backchannel, and Expression. Clearly, it 
is useful for a dialogue system to be able to 
identify accurately the various ways a person may 
say ?yes?, ?no?, or ?what did you say?? As with 
the task identifier, we have trained the DA 
classifier on our corpus of transcribed, labeled 
human-human calls, and we have used vector-
based classification techniques. Two differences 
from the task identifier are 1) an utterance may 
have multiple correct classifications, and 2) a 
different stoplist is necessary. Here we can filter 
out the usual stops, including speech dysfluencies, 
proper names, number words, and words with 
digits; but we need to include words such as yeah, 
uh-huh, hi, ok, thanks, pardon and sorry.  
Some examples of DA classification results are 
shown in Figure 3. For sure, ok, the classifier 
returns the categories Backchannel, Expression and 
Accept. If the dialogue manager is looking for 
either Accept or Reject, it can ignore Backchannel 
and Expression in order to detect the correct 
classification. In the case of certainly not, the first 
word has a strong tendency toward Accept, though 
both together constitute a Reject act.  
 
Text: ?sure, okay? Text: ?certainly not?
Categories returned: Backchannel, 
Expression, Accept 
Categories returned:
Reject, Accept 
Expression
Closing
Accept
Back.
0
0.2
0.4
0.6
0.8
1
Top four cosine scores
Expression
Accept Closing
Back.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Confidence scores
Reject
Reject-part
Accept Expression
0
0.1
0.2
0.3
0.4
0.5
0.6
Top four cosine scores
Reject
Accept Expression
Reject-part
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Confidence scores
Figure 3. DA Classification examples 
 
Our classifier performs well if the utterance is 
short and falls into one of the selected categories 
(86% accuracy on the British data); and it has the 
advantages of automatic training, domain 
independence, and the ability to capture a great 
variety of expressions. However, it can be 
inaccurate when applied to longer utterances, and it 
is not yet equipped to handle domain-specific 
assertions, questions, or queries about a 
transaction. 
3.4 Database Manager 
Our system identifies users by matching 
information provided by the caller against a 
database of user information. It assumes that the 
speech recognizer will make errors when the caller 
attempts to identify himself. Therefore perfect 
matches with the database entries will be rare. 
Consequently, for each record in the database, we 
attach a measure of the probability that the record 
is the target record. Initially, these measures are 
estimates of the probability that this individual will 
call. When additional identifying information 
arrives, the system updates these probabilities 
using Bayes? rule. 
Thus, the system might begin with a uniform 
probability estimate across all database records. If 
the user identifies herself with a name recognized 
by the machine as ?Smith?, the system will 
appropriately increment the probabilities of all 
entries with the name ?Smith? and all entries that 
are known to be confused with ?Smith? in 
proportion to their observed rate of substitution. Of 
course, all records not observed to be so 
confusable would similarly have their probabilities 
decreased by Bayes? rule. When enough 
information has come in to raise the probability for 
some record above a threshold (in our system 0.99 
probability), the system assumes that the caller has 
been correctly identified. The designer may choose 
to include a verification dialog, but our decision 
was to minimize such interactions to shorten the 
calls.  
Our error-correcting database system receives 
tokens with an identification of what field each 
token should represent. The system processes the 
tokens serially. Each represents an observation 
made by the speech recognizer. To process a token, 
the system examines each record in the database 
and updates the probability that the record is the 
target record using Bayes? rule: 
 
 
  
where rec is the event where the record under 
consideration is the target record.  
As is common in Bayes? rule calculations, the 
denominator P(obs) is treated as a scaling factor, 
and is not calculated explicitly. All probabilities 
are renormalized at the end of the update of all of 
the records. P(rec) is the previous estimate of the 
probability that the record is the target record. 
P(obs|rec) is the probability that the recognizer 
returned the observation that it did given that the 
target record is the current record under 
examination. For some of the fields, such as the 
account number and telephone number, the user 
responses consist of digits. We collected data on 
the probability that the speech recognition system 
we are using mistook one digit for another and 
calculated the values for P(obs|rec) from the data. 
For fields involving place names and personal 
names, the probabilities were estimated.  
Once a record has been selected (by virtue of its 
probability being greater than the threshold) the 
system compares the individual fields of the record 
with values obtained by the speech recognizer. If 
the values differ greatly, as measured by their 
Levenshtein distance, the system returns the field 
name to the dialogue manager as a candidate for 
additional verification. If no record meets the 
threshold probability criterion, the system returns 
the most probable record to the dialogue manager, 
along with the fields which have the greatest 
Levenshtein distance between the recognized and 
actual values, as candidates for reprompting.  
Our database contains 100 entries for the system 
tests described in this paper. We describe the 
system in a more demanding environment with one 
million records in Inouye et al (2004). In that 
project, we required all information to be entered 
by spelling the items out so that the vocabulary 
was limited to the alphabet plus the ten digits. In 
the current project, with fewer names to deal with, 
we allowed the complete vocabulary of the 
domain: names, streets, counties, and so forth.  
3.5 Response Generator 
Our current English-only system preserves the 
language-independent features of our original tri-
lingual generator, storing all language- and 
domain-specific information in separate text files. 
It is a template-based system, easily modified and 
extended. The generator constructs utterances 
according to the dialogue manager?s specification 
of one or more speech acts (prompt, request, 
confirm, respond, inform, backchannel, accept, 
reject), repetition numbers, and optional lists of 
attributes, values, and/or the person?s name. As far 
as possible, we modeled utterances after the 
human-human dialogues. 
For a more natural-sounding system, we 
collected variations of the utterances, which the 
generator selects at random. Requests, for 
example, may take one of twelve possible forms: 
Request, part 1 of 2: 
Can you just confirm | Can I have | Can I take | 
What is | What?s | May I have 
)(
)()|()|(
obsP
recPrecobsPobsrecP ?=
Request, part 2 of 2: 
[list of attributes], [person name]? | [list of 
attributes], please? 
Offers to close or continue the dialogue are 
similarly varied: 
Closing offer, part 1 of 2: 
Is there anything else | Anything else | Is there 
anything else at all 
Closing offer, part 2 of 2: 
I can do for you today? | I can help you with 
today? | I can do for you? | I can help you with? | 
you need today? | you need? 
4 Preliminary Evaluation 
Ten native speakers of English, 6 female and 4 
male, were asked to participate in a preliminary in-
lab system evaluation (half in the UK and half in 
the US). The Amiti?s system developers were not 
among these volunteers. Each made 9 phone calls 
to the system from behind a closed door, according 
to scenarios designed to test various customer 
identities as well as single or multiple tasks. After 
each call, participants filled out a questionnaire to 
register their degree of satisfaction with aspects of 
the interaction. 
Overall call success was 70%, with 98% 
successful completions for the VerifyId and 96% 
for the CheckBalance subtasks (Figure 4). 
?Failures? were not system crashes but simulated 
transfers to a human agent. There were 5 user 
terminations. 
Average word error rates were 17% for calls that 
were successfully completed, and 22% for failed 
calls. Word error rate by user ranged from 11% to 
26%. 
 
0.70
0.98 0.96
0.88 0.90
0.57
0.85
0.00
0.20
0.40
0.60
0.80
1.00
1.20
Ca
ll S
uc
ce
ss
Ve
rify
Id
Ch
ec
kB
ala
nc
e
Lo
stC
ar
d
Ma
ke
Pa
ym
en
t
Ch
an
ge
Ad
dr
es
s
Fin
ish
Di
alo
gu
e
 
Figure 4. Task Completion Rates 
Call duration was found to reflect the 
complexity of each scenario, where complexity is 
defined as the number of ?concepts? needed to 
complete each task. The following items are 
judged to be concepts: task identification; values 
such as first name, last name, house number, street 
and phone number; and positive or negative 
responses such as whether a new card is desired. 
Figures 5 and 6 illustrate the relationship between 
length of call and task complexity. It should be 
noted that customer verification, a task performed 
in every dialogue, requires a minimum of 3 
personal details to be verified against a database 
record, but may require more in the case of 
recognition errors. 
The overall average number of turns per 
dialogue was 18.28. The user spoke an average of 
6.89 words per turn and the system 11.42. 
User satisfaction for each call was assessed by 
way of a questionnaire containing five statements. 
These covered the clarity of the instructions, ease 
of doing the task, how well the system understands 
the caller, how well the system works, and the 
caller?s enjoyment of the system. Participants rated 
each on a five-point Likert scale. Summed results 
showed an average score of 20.45 over all users 
(range 5?25; higher = stronger agreement). 
 
0
2
4
6
8
10
12
1 2 3 4 5 6 7 8 9
Scenario
Nu
m
be
r o
f c
on
ce
pt
s
 
85.0
135.8
119.3
88.3
106.7 114.0
48.4
109.3
127.1
0.0
20.0
40.0
60.0
80.0
100.0
120.0
140.0
160.0
1 2 3 4 5 6 7 8 9
Figures 5 and 6. Scenario Complexity (top) and 
Average Call Duration by Scenario (in seconds) 
Although user satisfaction was high, we were 
more interested in identifying the major problems 
for the callers. Users were often frustrated by 
recognition failures and/or unsuccessful attempts 
to capture values such as a new street address, 
county, or phone number. Sometimes the system 
failed to determine that the user was finished. 
Because the system is designed to be non-
restrictive in accepting users? input, 
misidentifications were common. One user spoke 
for many by saying, ?There were times when the 
system would ask me to repeat information I had 
not yet given to it.? We plan to revise our strategy 
so that we are a little more cautious in our 
reprompts. Occasionally, the system misidentified 
the user?s desired task and had difficulty backing 
off gracefully and starting the correct task. We are 
working on improving our recovery strategies for 
these cases. 
 The following transcription of an interaction 
with our system illustrates the efficiency made 
possible when the data supplied by the caller drives 
the conversation: 
AMITI?S:  Hello, this is Am-ih-tee-eh! What can 
I do for you? 
USER:  yes this is Stella Lang. My card number 
is three one six seven one four. Can you tell me the 
balance on my account please? 
AMITI?S:  Can you just confirm the city and 
county? 
USER:  uh yes it?s London, Middlesex 
AMITI?S:  I?m sorry, I didn?t understand what 
you said.  Can you repeat, please? 
USER:  London, Middlesex 
AMITI?S:  All right, your balance is 1753 
pounds and 20 pence. Anything else I can do for 
you? 
USER:  yes I'd like to make a payment of fifty 
three pounds with my debit card please 
AMITI?S:  Can I have the debit card number, 
Miss Lang? 
USER:  sure, it's two eight seven four six six four 
nine five eight nine five three eight nine one, and it 
expires October two thousand six 
AMITI?S:  What's the issue number, please? 
USER:  one 
AMITI?S:  Ok, Miss Lang, I'll send in the 
payment amount of 53 pounds. Did I get that right? 
USER:  yes, what's my new balance please? 
AMITI?S:  Ok, that's gone through. Your 
payment will be credited overnight and will reflect 
on your next statement. Your balance is 1700 
pounds and 20 pence. Anything else I can do for 
you today? 
USER:  no, that's it thanks 
AMITI?S:  Thanks very much for your call, 
goodbye. 
USER:  bye 
5 Discussion, Future Work 
The preliminary evaluation reported here 
indicates promise for an automated dialogue 
system such as ours, which incorporates robust 
techniques for information extraction, record 
matching, task identification, dialogue act 
classification, and an overall data-driven strategy. 
Task duration and number of turns per dialogue 
both appear to indicate greater efficiency and 
corresponding user satisfaction than many other 
similar systems. In the DARPA Communicator 
evaluation, for example, between 60 and 79 calls 
were made to each of 8 participating sites (Walker, 
et al, 2001, 2002). A sample scenario for a 
domestic round-trip flight contained 8 concepts 
(airline, departure city, state, date, etc.). The 
average duration for such a call was over 300 
seconds; whereas our overall average was 104 
seconds. ASR accuracy rates in 2001 were about 
60% and 75%, for airline itineraries not completed 
and completed; and task completion rates were 
56%. Our average number of user words per turn, 
6.89, is also higher than that reported for 
Communicator systems. This number seems to 
reflect lengthier responses to open prompts, 
responses to system requests for multiple 
attributes, and greater user initiative. 
We plan to port the system to a new domain: 
from telephone banking to information-technology 
support. As part of this effort we are again 
collecting data from real human-human calls. For 
advanced speech recognition, we hope to train our 
ASR on new acoustic data. We also plan to expand 
our dialogue act classification so that the system 
can recognize more types of acts, and to improve 
our classification reliability.  
6 Acknowledgements 
This paper is based on work supported in part by 
the European Commission under the 5th 
Framework IST/HLT Programme, and by the US 
Defense Advanced Research Projects Agency. 
References 
J. Allen and M. Core. 1997. Draft of DAMSL: 
Dialog Act Markup in Several Layers. 
http://www.cs.rochester.edu/research/cisd/resour
ces/damsl/. 
J. Allen, L. K. Schubert, G. Ferguson, P. Heeman, 
Ch. L. Hwang, T. Kato, M. Light, N. G. Martin, 
B. W. Miller, M. Poesio, and D. R. Traum. 
1995. The TRAINS Project: A Case Study in 
Building a Conversational Planning Agent. 
Journal of Experimental and Theoretical AI, 7 
(1995), 7?48. 
Amiti?s, http://www.dcs.shef.ac.uk/nlp/amities.  
J. Chu-Carroll and B. Carpenter. 1999. Vector-
Based Natural Language Call Routing. 
Computational Linguistics, 25 (3): 361?388. 
H. Cunningham, D. Maynard, K. Bontcheva, V. 
Tablan. 2002. GATE: A Framework and 
Graphical Development Environment for Robust 
NLP Tools and Applications. Proceedings of the 
40th Anniversary Meeting of the Association for 
Computational Linguistics (ACL'02), 
Philadelphia, Pennsylvania. 
H. Cunningham and D. Maynard and V. Tablan. 
2000. JAPE: a Java Annotation Patterns Engine 
(Second Edition). Technical report CS--00--10, 
University of Sheffield, Department of 
Computer Science.  
DARPA, 
http://www.darpa.mil/iao/Communicator.htm. 
H. Hardy, K. Baker, L. Devillers, L. Lamel, S. 
Rosset, T. Strzalkowski, C. Ursu and N. Webb. 
2002. Multi-Layer Dialogue Annotation for 
Automated Multilingual Customer Service. 
Proceedings of the ISLE Workshop on Dialogue 
Tagging for Multi-Modal Human Computer 
Interaction, Edinburgh, Scotland. 
H. Hardy, T. Strzalkowski and M. Wu. 2003a. 
Dialogue Management for an Automated 
Multilingual Call Center. Research Directions in 
Dialogue Processing, Proceedings of the HLT-
NAACL 2003 Workshop, Edmonton, Alberta, 
Canada. 
H. Hardy, K. Baker, H. Bonneau-Maynard, L. 
Devillers, S. Rosset and T. Strzalkowski. 2003b. 
Semantic and Dialogic Annotation for 
Automated Multilingual Customer Service. 
Eurospeech 2003, Geneva, Switzerland. 
R. B. Inouye, A. Biermann and A. Mckenzie. 
2004. Caller Identification from Spelled-Out 
Personal Data Using a Database for Error 
Correction. Duke University Internal Report. 
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, 
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. 
Lee, A. Pokrovsky, M. Rahim, P. Ruscitti, and 
M. Walker. 2000. The AT&T-DARPA 
Communicator Mixed-Initiative Spoken Dialog 
System. ICSLP 2000. 
D. Maynard. 2003. Multi-Source and Multilingual 
Information Extraction. Expert Update. 
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, 
and V. Zue. 1998. Galaxy-II: A Reference 
Architecture for Conversational System 
Development. ICSLP 98, Sydney, Australia. 
S. Seneff and J. Polifroni. 2000. Dialogue 
Management in the Mercury Flight Reservation 
System. Satellite Dialogue Workshop, ANLP-
NAACL, Seattle, Washington. 
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. 
Garofolo, L. Hirschman, A. Le, S. Lee, S. 
Narayanan, K. Papineni, B. Pellom, J. Polifroni, 
A. Potamianos, P. Prabhu, A. Rudnicky, G. 
Sanders, S. Seneff, D. Stallard and S. Whittaker. 
2001. DARPA Communicator Dialog Travel 
Planning Systems: The June 2000 Data 
Collection. Eurospeech 2001. 
M. Walker, A. Rudnicky, J. Aberdeen, E. Bratt, J. 
Garofolo, H. Hastie, A. Le, B. Pellom, A. 
Potamianos, R. Passonneau, R. Prasad, S. 
Roukos, G. Sanders, S. Seneff and D. Stallard. 
2002. DARPA Communicator Evaluation: 
Progress from 2000 to 2001. ICSLP 2002. 
W. Ward and B. Pellom. 1999. The CU 
Communicator System. IEEE ASRU, pp. 341?
344. 
W. Xu and A. Rudnicky. 2000. Task-based Dialog 
Management Using an Agenda. ANLP/NAACL 
Workshop on Conversational Systems, pp. 42?
47. 
 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1169?1176,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Utilizing Co-Occurrence of Answers in Question Answering   
 
 
 
Abstract 
In this paper, we discuss how to utilize 
the co-occurrence of answers in building 
an automatic question answering system 
that answers a series of questions on a 
specific topic in a batch mode. Experi-
ments show that the answers to the many 
of the questions in the series usually have 
a high degree of co-occurrence in rele-
vant document passages. This feature 
sometimes can?t be easily utilized in an 
automatic QA system which processes 
questions independently. However it can 
be utilized in a QA system that processes 
questions in a batch mode. We have used 
our pervious TREC QA system as base-
line and augmented it with new answer 
clustering and co-occurrence maximiza-
tion components to build the batch QA 
system. The experiment results show that 
the QA system running under the batch 
mode get significant performance im-
provement over our baseline TREC QA 
system.   
1 Introduction 
Question answering of a series of questions on 
one topic has gained more and more research 
interest in the recent years. The current TREC 
QA test set contains factoid and list questions 
grouped into different series, where each series 
has the target of a definition associated with it 
(Overview of the TREC 2004 Question Answer-
ing Track, Voorhees 2005). Usually, the target is 
also called ?topic? by QA researchers. One of the 
restrictions of TREC QA is that ?questions 
within a series must be processed in order, with-
out looking ahead.? That is, systems are allowed 
to use answers to earlier questions to help answer 
later questions in the same series, but can not use 
later questions to help answer earlier questions. 
This requirement models the dialogue discourse 
between the user and the QA system. However 
our experiments on interactive QA system show 
that some impatient QA users will throw a bunch 
of questions to the system and waiting for the 
answers returned in all. This prompted us to con-
sider building a QA system which can accept as 
many questions as possible from users once in all 
and utilizing the relations between these ques-
tions to help find answers. We would also like to 
know the performance difference between the 
QA system processing the question series in an 
order and the QA system processing the question 
series as a whole. We call the second type of QA 
system as batch QA system to avoid the ambigu-
ity in the following description in this paper.  
  What kind of relations between questions 
could be utilized is a key problem in building the 
batch QA system. By observing the test ques-
tions of TREC QA, we found that the questions 
given under the same topic are not independent 
at all. Figure-1 shows a series of three questions 
proposed under the topic ?Russian submarine 
Kursk Sinks? and some relevant passages to this 
topic found in the TREC data set. These passages 
contain answers not to just one but to two or 
three of the questions. This indicates that the an-
swers to these questions have high co-occurrence.  
In an automatic QA system which processes 
the questions independently, the answers to the 
questions may or may not always be extracted 
due to algorithmic limitations or noisy informa-
tion around the correct answer. However in 
building a batch QA system, the inter-
dependence between the answers could be util-
ized to help to filter out the noisy information 
and pinpoint the correct answer for each question 
in the series.  
 
 
Min Wu1 and Tomek Strzalkowski1,2  
1 ILS Institute, University at Albany, State University of New York 
1400 Washington Ave SS261, Albany NY, 12222 
2Institute of Computer Science, Polish Academy of Sciences 
minwu@cs.albany.edu, tomek@csc.albany.edu  
1169
 
We will discuss later in this paper how to util-
ize the co-occurrence of answers to a series of 
questions in building a batch QA system. The 
remainder of this paper is organized as follows. 
In the next section, we review the current tech-
niques used in building an automatic QA system. 
Section 3 introduces the answers co-occurrence 
and how to cluster questions by the co-
occurrence of their answers. Section 4.1 de-
scribes our TREC QA system and section 4.2 
describes how to build a batch QA system by 
augmenting the TREC QA system with question 
clustering and answer co-occurrence maximiza-
tion. Section 4.3 describes the experiments and 
explains the experimental results. Finally we 
conclude with the discussion of future work.   
2 Related Work 
During recent years, many automatic QA sys-
tems have been developed and the techniques 
used in these systems cover logic inference, syn-
tactic relation analysis, information extraction 
and proximity search, some systems also utilize 
pre-compiled knowledge base and external 
online knowledge resource.  
The LCC system (Moldovan & Rus, 2001; 
Harabagiu et al 2004) uses a logic prover to se-
lect answer from related passages. With the aid 
of extended WordNet and knowledge base, the 
text terms are converted to logical forms that can 
be proved to match the question logical forms. 
The IBM?s PIQUANT system (Chu-Carroll et al 
2003; Prager et al 2004) adopts a QA-by-
Dossier-with-Constraints approach, which util-
izes the natural constraints between the answer to 
the main question and the answers to the auxil-
iary questions. Syntactic dependency matching 
has also been applied in many QA systems (Cui 
et al 2005; Katz and Lin 2003). The syntactic 
dependency relations of a candidate sentence are 
matched against the syntactic dependency rela-
tions in the question in order to decide if the can-
didate sentence contains the answer. Although 
surface text pattern matching is a comparatively 
simple method, it is very efficient for simple fac-
toid questions and is used by many QA systems 
(Hovy et al2001; Soubbotin, M. and S. Soub-
botin 2003). As a powerful web search engine 
and external online knowledge resource, Google 
has been widely adopted in QA systems (Hovy et 
al 2001; Cui 2005) as a tool to help passage re-
trieval and answer validation. 
Current QA systems mentioned above and 
represented at TREC have been developed to 
answer one question at the time. This may par-
tially be an artifact of the earlier TREC QA 
evaluations which used large sets of independent 
questions. It may also partially reflect the inten-
tion of the current TREC QA Track that the 
question series introduced in TREC QA 2004 
(Voorhees 2005) simulate an interaction with a 
human, thus expected to arrive one at a time. 
The co-occurrence of answers of a series of 
highly related questions has not yet been fully 
utilized in current automatic QA systems partici-
pating TREC. In this situation, we think it 
worthwhile to find out whether a series of highly 
related questions on a specific topic such as the 
TREC QA test questions can be answered to-
gether in a batch mode by utilizing the co-
occurrences of the answers and how much it will 
help improve the QA system performance.    
3 Answer Co-Occurrence and Question 
Clustering 
Many QA systems utilize the co-occurrence of 
question terms in passage retrieval (Cui 2005). 
Topic Russian submarine Kursk sinks 
 
1. When did the submarine sink?   August 12 
2. How many crewmen were lost in the disaster?   118 
3. In what sea did the submarine sink?    Barents Sea 
 
Some Related Passages 
 
Russian officials have speculated that the Kursk col-
lided with another vessel in the Barents Sea, and usu-
ally blame an unspecified foreign submarine. All 118 
officers and sailors aboard were killed. 
 
The Russian governmental commission on the acci-
dent of the submarine Kursk sinking in the Barents 
Sea on August 12 has rejected 11 original explana-
tions for the disaster. 
 
.... as the same one carried aboard the nuclear subma-
rine Kursk, which sank in the Barents Sea on Aug. 12, 
killing all 118 crewmen aboard. 
 
The navy said Saturday that most of the 118-man 
crew died Aug. 12 when a huge explosion .... 
 
Chief of Staff of the Russian Northern Fleet Mikhail 
Motsak Monday officially confirmed the deaths of 
118 crewmen on board the Kursk nuclear submarine 
that went to the bottom of the Barents Sea on August 
12. 
Figure-1 Questions and Related Passages 
1170
Some QA systems utilize the co-occurrence of 
question terms and answer terms in answer vali-
dation. These methods are based on the assump-
tion that the co-occurrences of question terms 
and answer terms are relatively higher than the 
co-occurrences of other terms. Usually the co-
occurrence are measured by pointwise mutual 
information between terms.  
During the development of our TREC QA sys-
tem, we found the answers of some questions in 
a series have higher co-occurrence. For example, 
in a series of questions on a topic of disaster 
event, the answers to questions such as ?when 
the event occurred?, ?where the event occurred? 
and ?how many were injured in the event? have 
high co-occurrence in relatively short passages. 
Also, in a series of questions on a topic of some 
person, the answers to questions such as ?when 
did he die?, ?where did he die? and ?how did he 
die? have high co-occurrence. To utilize this an-
swers co-occurrence effectively in a batch QA 
system, we need to know which questions are 
expected to have higher answers co-occurrence 
and cluster these questions to maximize the an-
swers co-occurrence among the questions in the 
cluster.  
Currently, the topics used in TREC QA test 
questions fall into four categories: ?Person?, 
?Organization?, ?Event? and ?Things?. The topic 
can be viewed as an object and the series of 
questions can be viewed as asking for the attrib-
utes of the object. In this point of view, to find 
out which questions have higher answers co-
occurrence is to find out which attributes of the 
object (topic) have high co-occurrence. 
We started with three categories of TREC QA 
topics: ?Event?, ?Person? and ?Organization?. 
For ?Event? topic category, we divided it into 
two sub-categories: ?Disaster Event? and ?Sport 
Event?. From the 2004 & 2005 TREC QA test 
questions, we manually collected frequently 
asked questions on each topic category and 
mapped these questions to the corresponding 
attributes of the topic. We focused on frequently 
asked questions because these questions are eas-
ier to be classified and thus served as a good 
starting point for our work. However for this 
technique to scale in the future, we are expecting 
to integrate automatic topic model detection into 
the system. For topic category ?Person?, the at-
tributes and corresponding named entity (NE) 
tags list as follows.   
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
For each topic category, we collected 20 sam-
ple topics as well as the corresponding attributes 
information about these topics. The sample topic 
?Rocky Marciano? and the attributes are listed as 
follows: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
From each attribute of the sample topic, an 
appropriate question can be formulated and rele-
vant passages about this question were retrieved 
from TREC data (AQUAINT Data) and the web. 
A topic-related passages collection was formed 
by the relevant passages of questions on all at-
tributes under the topic. Among the topic-related 
passages, the pointwise mutual information (PMI) 
of attribute values were calculated which conse-
quently formed a symmetric mutual information 
matrix. The PMI of two attribute values x and y 
was calculated by the following equation. 
)()(
),(log),(
ypxp
yxpyxPMI =  
All the mutual information matrixes under the 
topic category were added up and averaged in 
order to get one mutual information matrix 
which reflects the general co-occurrence rela-
Attribute      Attribute Value                        
 
Birth Date   September 1, 1923         
Birth Place  Brockton, MA                                                 
Death Date  August 31, 1969    
Death Place  Iowa  
Death Reason  airplane crash 
Death Age   45 
Buried Place  Fort Lauderdale, FL 
Nationality   American                      
Occupation  heavyweight champion boxer            
Father    Pierino Marchegiano 
Mother    Pasqualena Marchegiano 
Wife   Barbara Cousins 
Children   Mary Ann, Rocco Kevin 
No. of Children  two 
Real Name  Rocco Francis Marchegiano 
Nick Name  none 
Affiliation   none                         
Education   none  
Attribute                                     Attribute?s NE tag 
 
Birth Date   Date 
Birth Place  Location 
Death Date  Date 
Death Place  Location 
Death Reason  Disease, Accident 
Death Age   Number 
Nationality  Nationality 
Occupation  Occupation 
Father   Person 
Mother   Person 
Wife   Person 
Children   Person 
Number of Children  Number 
Real Name  Person, Other 
Nick Name  Person, Other 
Affiliation   Organization 
Education   Organization 
1171
tions between attributes under the topic category. 
We clustered the attributes by their mutual in-
formation value. Our clustering strategy was to 
cluster attributes whose pointwise mutual infor-
mation is greater than a threshold ?. We choose ? 
as equal to 60% of the maximum value in the 
matrix. 
The operations described above were auto-
matically carried out by our carefully designed 
training system. The clusters learned for each 
topic category is listed as follows. 
The reason for the clustering of attributes of 
topic category is for the convenience of building 
a batch QA system. When a batch QA system is 
processing a series of questions under a topic, 
some of the questions in the series are mapped to 
the attributes of the topic and thus grouped to-
gether according to the attribute clusters. Then 
questions in the same group are processed to-
gether to obtain a maximum of answers co-
occurrence. More details are given in section 4.2. 
4 Experiment  Setup and Evaluation 
4.1 Baseline System 
The baseline system is an automatic IE-driven 
(Information Extraction) QA system. We call it 
IE-driven because the main techniques used in 
the baseline system: surface pattern matching 
and N-gram proximity search need to be applied 
to NE-tagged (Named Entity) passages. The sys-
tem architecture is illustrated in Figure-2. The 
components indicated by dash lines are not in-
cluded in the baseline system and they are added 
to the baseline system to build a batch QA sys-
tem. As shown in the figure with light color, the 
two components are question classification and 
co-occurrence maximization. Both our baseline 
system and batch QA system didn?t utilize any 
pre-compiled knowledge base. 
In the question analysis component, questions 
are classified by their syntactic structure and an-
swer target. The answer targets are classified as 
named entity types. The retrieved documents are 
segmented into passages and filtered by topic 
keywords, question keywords and answer target. 
The answer selection methods we used are 
surface text pattern matching and n-gram prox-
imity search. We build a pattern learning system 
to automatically extract answer patterns from the 
TREC data and the web. These answer patterns 
are scored by their frequency, sorted by question 
type and represented as regular expressions with 
terms of ?NP?, ?VP?, ?VPN?, ?ADVP?, ?be?, 
?in?, ?of?, ?on?, ?by?, ?at?, ?which?, ?when?, 
?where?, ?who?, ?,?, ?-?, ?(?. Some sample an-
swer patterns of question type ?when_be_np_vp? 
are listed as follows. 
 
 
 
 
 
 
 
 
 
When applying these answer patterns to ex-
tract answer from candidate passages, the terms 
such as ?NP?, ?VP?, ?VPN?, ?ADVP? and ?be? 
are replaced with the corresponding question 
terms. The replaced patterns can be matched di-
rectly to the candidate passages and answer can-
didate be extracted.  
Some similar proximity search methods have 
been applied in document and passage retrieval 
in the previous research. We applied n-gram 
proximity search to answer questions whose an-
swers can?t be extracted by surface text pattern 
matching. Around every named entity in the fil-
tered candidate passages, question terms as well 
as topic terms are matched as n-grams. A ques-
tion term is tokenized by word. We matched the 
longest possible sequence of tokenized word 
within the 100 word sliding window around the 
named entity. Once a sequence is matched, the 
corresponding word tokens are removed from the 
ADVP1 VP in <Date>([^<>]+?)<\/Date>                             
NP1.{1,15}VP.{1,30} in <Date>([^<>]+?)<\/Date>                          
NP1.{1,30} be VP in <Date>([^<>]+?)<\/Date>                             
NP1, which be VP in <Date>([^<>]+?)<\/Date>                             
VP NP1.{1,15} at .{1,15}<Date>([^<>]+?)<\/Date>                         
ADVP1.{1,80}NP1.{1,80}<Date>([^<>]+?)<\/Date>                           
NP1, VP in <Date>([^<>]+?)<\/Date>                              
NP1 of <Date>([^<>]+?)<\/Date>                           
NP1 be VP in <Date>([^<>]+?)<\/Date>           
?Person? Topic  
 
Cluster1: Birth Date; Birth Place 
Cluster2a: Death Date; Death Place;  
   Death Reason; Death Age 
Cluster2b: Death Date; Birth Date 
Cluster3: Father; Mother 
Cluster4: Wife; Children; Number of Children 
Cluster5: Nationality; Occupation 
 
?Disaster Event? Topic  
 
Cluster1: Event Date; Event Location; Event Casualty;  
Cluster2:  Organization Involved, Person Involved 
 
?Sport Event? Topic 
 
Cluster1: Winner; Winning Score 
Cluster2: Location, Date 
 
?Organization? Topic 
 
Cluster1: Founded Date; Founded Location; Founder 
Cluster2: Headquarters; Number of Members 
1172
token list and the same searching and matching is 
repeated until the token list is empty or no se-
quence of tokenized word can be matched. The 
named entity is scored by the average weighted 
distance score of question terms and topic terms. 
Let Num(ti...tj) denotes the number of all 
matched n-grams, d(E, ti...tj) denotes the word 
distance between the named entity and the 
matched n-gram, W1(ti...tj) denotes the topic 
weight of the matched n-gram, W2(ti...tj) denotes 
the length weight of the matched n-gram. If ti...tj 
contains topic terms or question verb phrase, 0.5 
is assigned to W1, otherwise 1.0 is assigned. The 
value assigned to length weight W2 is deter-
mined by ?, the ratio value of matched n-gram 
length to question term length. How to assign W2 
is illustrated as follows.  
The weighted distance score D(E,QTerm) of 
the question term and the final score S(E) of the 
named entity are calculated by the following 
equations. 
)...(
)...(2
)...(1)...,(
),( ...
ji
tt ji
jiji
ttNum
ttW
ttWttEd
QTermED ji
? ?
=  
N
QTermED
ES
N
i
i?
=
),(
)(  
4.2 Batch QA System 
The batch QA system is built from the base-
line system and two added components: question 
classification and co-occurrence maximization. 
In a batch QA system, questions are classified 
before they are syntactically and semantically 
analyzed. The classification process consists of 
two steps: topic categorization and question 
mapping.  Firstly the topic of the series questions 
is classified into appropriate topic category and 
then the questions can be mapped to the corre-
sponding attribute and clustered according to the 
mapped attributes. Since the attributes of topic 
category is collected from frequently asked ques-
tions, there are some questions in the question 
series which can?t be mapped to any attribute. 
These unmapped questions are processed indi-
vidually.    
The topic categorization is done by a Na?ve 
Bayes classifier which employs features such as 
stemmed question terms and named entities in 
the question. The training data is a collection of 
85 question series labeled as one of four topic 
categories: ?Person?, ?Disaster Event?, ?Sport 
Event? and ?Organization?. The mapping of 
question to topic attribute is an example-based 
syntactic pattern matching and keywords match-
ing.   
 The questions grouped together are processed 
as a question cluster. After the processing of an-
swer selection and ranking, each question in the 
cluster gets top 10 scored candidate answers 
which forms an answer vector A(a1, ?, a10). 
 W2(ti...tj)=0.4   if ?<0.4; 
 W2(ti...tj)=0.6       if 0.4? ?? 0.6;  
W2(ti...tj)=0.8        if ?>0.6; 
W2(ti...tj)= 0.9      if ?>0.75. 
Answers 
 
Syntactic Chunking 
Type Categorization 
Query Generation 
Target Classification  
Questions Document 
Retrieval 
 Passage Filtering 
Surface Text Pattern Matching 
  N-Gram Proximity Search 
          Answer Ranking 
Pattern Files 
 
Tagged Corpus 
(AQUAINT 
/Web) 
 Question 
Clustering 
Co-occurrence 
Maximization 
Figure-2  Baseline QA System & Batch QA System (dashed lines and light colored component) 
1173
Suppose there are n questions in the cluster, the 
task of answer co-occurrence maximization is to 
retrieve a combination of n answers which has 
maximum pointwise mutual information (PMI). 
This combination is assumed to be the answers to 
the questions in the cluster. 
There are a total of 10n possible combinations 
among all the candidate answers. If the PMI of 
every combination should be calculated, it is 
computationally inefficient. Also, some combi-
nations containing noisy information may have 
higher co-occurrence than the correct answer 
combination. For example, the correct answers 
combination to questions showed in figure-1 is 
?August 12; 118; Barents Sea?. However, there 
is also a combination of ?Aug. 12, two; U.S.? 
which has higher pointwise mutual information 
due to the frequently occurred noisy information 
of ?two U.S. submarines? and ?two explosions in 
the area Aug. 12 at the time?.  
To reduce this negative effect brought by the 
noisy information, we started from the highest 
scored answer and put it in the final answer list. 
Then we added the answers one by one to the 
final answer list. The added answer has the high-
est PMI with the answers in the final answer list. 
It is important here to choose the first answer 
added to the final answer list correctly. Other-
wise, the following added answers will be nega-
tively affected. So in our batch QA system, a 
correct answer should be scored highest among 
all the answer candidates of the questions in the 
cluster. Although this can?t be always achieved, 
it can be approximated by setting higher thresh-
old both in passage scoring and answer ranking. 
However, in the baseline system, passages are 
not scored. They are equally processed because 
we wanted to retrieve as many answer candidates 
as possible and answer candidates are ranked by 
their matching score and redundancy score.   
4.3 Performance Evaluation 
The data corpus we used is TREC QA data 
(AQUAINT Corpus). The test questions are 
TREC QA 2004 and TREC QA 2005 questions. 
Each topic is followed with a series of factoid 
questions. The number of questions selected 
from TREC 2004 collection is 230 and the num-
ber of question series is 65. The number of ques-
tions selected from TREC 2005 collection is 362 
and the number of question series is 75.  
    We performed 4 different experiments: (1). 
Baseline system. (2). Batch QA system (Baseline 
system with co-occurrence maximization). (3). 
Baseline system with web supporting.  (4). Batch 
QA with web supporting. We introduced web 
supporting into the experiments because usually 
the information on the web tends to share more 
co-occurrence and redundancy which is also 
proved by our results.   
    Compared between the baseline system and 
batch system, the experiment results show that 
the overall accuracy score has been improved 
from 0.34 to 0.39 on TREC 2004 test questions 
and from 0.31 to 0.37 on TREC 2005 test ques-
tions. Compared between the baseline system 
and batch system with web supporting, the accu-
racy score can be improved up to 0.498.  We also 
noticed that the average number of questions un-
der each topic in TREC 2004 test questions is 
3.538, which is significantly lower than the 
4.8267 average in TREC 2005 questions series. 
This may explain why the improvement we ob-
tained on TREC2004 data is not as significant as 
the improvement obtained on TREC 2005 ques-
tions. 
The accuracy score of each TREC2005 ques-
tion series is also calculated. Figure3-4 shows the 
comparisons between 4 different experiment 
methods. We also calculate the number of ques-
tion series with accuracy increased, unchanged 
and decreased. It is also shown in the following 
table. (?+? means number of question series with 
accuracy increased, ?=? unchanged and ?-? de-
creased.)   
 
TREC2005 Question Series 
(75 question series) 
    + - = 
Baseline + Co-occurrence 25 5 45 
Baseline + Web 40 2 33 
Baseline + Co-occurrence + 
Web 
49 2 24 
Accuracy Comparison on Different 
Methods
0
0.1
0.2
0.3
0.4
0.5
0.6
1 2 3 4
TREC2004 TREC2005
1174
Some question series get unchanged accuracy 
because the questions can?t be clustered accord-
ing to our clustering template so that it can?t util-
ize the co-occurrence of answers in the cluster. 
Some question series get decreased accuracy be-
cause the questions because the noisy informa-
tion had even higher co-occurrence, the error 
occurred during the question clustering and the 
answers didn?t show any co-relations in the re-
trieved passages at all. A deep and further error 
analysis is necessary for this answer co-
occurrence maximization technique to be applied 
topic independently.   
 
5 Discussion and Future Work 
We have demonstrated that in a QA system, 
answering a series of inter-related questions can 
be improved by grouping the questions by ex-
pected co-occurrence of answers in text. The im-
provement can be made without exploiting the 
pre-compiled knowledge base. 
Although our system can cluster frequently 
asked questions on topics of ?Events?, ?Persons? 
and ?Organizations?, there are still some highly 
related questions which can?t be clustered by our 
method. Here are some examples.  
 
 
 
 
 
 
 
 
To cluster these questions, we plan to utilize 
event detection techniques and set up an event 
topic ?Carlos the Jackal captured? during the 
answering process, which will make it easier to 
cluster ?When was the Carlos the Jackal cap-
tured?? and ?Where was the Carlos the Jackal 
captured?? 
Can this answers co-occurrence maximization 
approach be applied to improve QA performance 
Topic Carlos the Jackal 
1. When was he captured?  
2. Where was he captured?    
 
Topic boxer Floyd Patterson 
1. When did he win the title? 
2. How old was he when he won the title? 
3. Who did he beat to win the title? 
 
Accuracy on TREC2005 Test Questions
0
0.2
0.4
0.6
0.8
1
1.2
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70 73
question series
ac
cu
ra
cy
baseline baseline+co_occurrence baseline+w eb baseline+w eb+co_occurrence
Accuracy on TREC2004 Test Questions
0
0.2
0.4
0.6
0.8
1
1.2
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64
question series
ac
cu
ra
cy
baseline baseline+co_occurrence baseline+w eb baseline+w eb+co_occurrence
Figure 3-4 Comparison of TREC2004/2005 Question Series Accuracy 
1175
on single questions (i.e. 1-series)? As suggested 
in the reference paper (Chu-Carrol and Prager), 
we may be able to add related (unasked) ques-
tions to form a cluster around the single question. 
Another open issue is what kind of effect will 
this technique bring to answering series of ?list? 
questions, i.e., where each question expects a list 
of items as answer.  As we know that the an-
swers of some ?list? questions have pretty high 
co-occurrence while others don?t have co-
occurrence at all. Future work involves experi-
ments conducted on these aspects.   
Acknowledgement 
The Authors wish to thank BBN for the use of 
NE tagging software IdentiFinder, CIIR at 
University of Massachusetts for the use of 
Inquery search engine, Stanford University NLP 
group for the use of Stanford parser. Thanks also 
to the anonymous reviewers for their helpful 
comments. 
References  
Chu-Carrol, J., J. Prager, C. Welty, K. Czuba and 
D. Ferrucci. ?A Multi-Strategy and Multi-
Source Approach to Question Answering?, In 
Proceedings of the 11th TREC, 2003. 
Cui, H., K. Li, R. Sun, T.-S. Chua and M.-Y. 
Kan. ?National University of Singapore at the 
TREC 13 Question Answering Main Task?. In 
Proceedings of the 13th TREC, 2005. 
Han, K.-S., H. Chung, S.-B. Kim, Y.-I. Song, J.-
Y. Lee, and H.-C. Rim. ?Korea University 
Question Answering System at TREC 2004?. 
In Proceedings of the 13th TREC, 2005.  
Harabagiu, S., D. Moldovan, C. Clark, M. Bow-
den, J. Williams and J. Bensley. ?Answer 
Mining by Combining Extraction Techniques 
with Abductive Reasoning?. In Proceedings of 
12th TREC, 2004.  
Hovy, E. L. Gerber, U. Hermjakob, M. Junk and 
C.-Y. Lin. ?Question Answering  in Webclo-
pedia?. In Proceedings of the 9th TREC, 2001.  
Lin, J., D. Quan, V. Sinha, K. Bakshi, D. Huynh, 
B. Katz and D. R. Karger. ?The Role of Con-
text in Question Answering Systems?. In CHI 
2003. 
Katz, B. and J. Lin. ?Selectively Using Relations 
to Improve Precision in Question Answering?. 
In Proceedings of the EACL-2003 Workshop 
on Natural Language Processing for Question 
Answering.  2003. 
Moldovan, D. and V. Rus. ?Logical Form Trans-
formation of WordNet and its Applicability to 
Question Answering?. In Proceedings of the 
ACL, 2001. 
Monz. C. ?Minimal Span Weighting Retrieval 
for Question Answering? In Proceedings of 
the SIGIR Workshop on Information Retrieval 
for Question Answering. 2004.  
Prager, J., E. Brown, A. Coden and D. Radev. 
?Question-Answering by Predictive Annota-
tion?. In Proceedings of SIGIR 2000, pp. 184-
191. 2000. 
Prager, J., J. Chu-Carroll and K. Czuba. ?Ques-
tion Answering Using Constraint Satisfaction: 
QA-By-Dossier-With-Constraints?. In Pro-
ceedings of the 42nd ACL. 2004. 
Ravichandran, D. and E. Hovy.  ?Learning Sur-
face   Text Patterns for a Question Answering 
System?. In Proceedings of 40th ACL. 2002. 
Soubbotin, M. and S. Soubbotin. ?Patterns of 
Potential Answer Expressions as Clues to the 
Right Answers?. In Proceedings of 11th TREC. 
2003.  
Voorhees, E. ?Using Question Series to Evaluate 
Question Answering System Effectiveness?. 
In Proceedings of HLT 2005. 2005.  
1176
Dialogue Management for an Automated Multilingual Call Center 
Hilda Hardy, Tomek Strzalkowski and Min Wu 
Institute for Informatics, Logics and Security Studies 
University at Albany, Albany, NY  12222 
and  
The AMITIES Consortium1 
hardyh,tomek,minwu@cs.albany.edu 
 
                                                          
1 
2 
1 The AMITIES consortium members include University of Sheffield, CNRS-LIMSI, Duke University, SUNY Albany, 
VESCYS, and Viel et Cie. 
 
Introduction 
The AMITI?S project (Automated Multilingual Interac-
tion with Information and Services) has been estab-
lished under joint funding from the European 
Commission?s 5th Framework Program and the U.S. 
DARPA to develop the next generation of empirically-
induced human-computer interaction capabilities in 
spoken language. One of the central goals of this project 
is to create a dialogue management system capable of 
engaging the user in human-like conversation within a 
specific domain. The domain we selected is telephone-
based customer service where the system has access to 
an appropriate information database to support callers? 
information needs. Our objective is to automate at least 
some of the more mundane human functions in cus-
tomer service call centers, but do so in a manner that is 
maximally responsive to the customer. This practically 
eliminates all prompt or menu based voice response 
systems used at commercial call centers today. 
Exploiting the corpus of hundreds (and soon to be 
thousands) of annotated dialogues, recorded at Euro-
pean financial call centers, we have developed a call 
triaging prototype for financial services domain. This 
demonstrator system handles the initial portion of a cus-
tomer call: identifying the customer (based on a sample 
customer database) and determining the reason the cus-
tomer is calling (based on a subset of transactions han-
dled at the call center). Our approach to dialogue act 
semantics allows for mixed system/customer initiative 
and spontaneous conversation to occur. We are cur-
rently extending this prototype beyond its triage role to 
negotiate and execute the transactions requested by the 
customers, ranging from simple address changes to 
more complex account payment transactions. 
The aim of AMITIES project is to build a large-
scale, empirical system using data-driven design, de-
rived from actual and purposeful (i.e., not acted or con-
trived) human-to-human dialogues. This proves to be a 
lengthy and complicated process due to a variety of le-
gal constraints we need to overcome to obtain real data 
in sufficient quantities. We have devoted a considerable 
effort to this issue, which only now is beginning to 
bring results. The prototype described here has not been 
empirically validated yet. 
Dialogue with Information and Services 
The key concept underlying AMITIES dialogue man-
ager is the notion of dialogue with data. The prevalent 
type of dialogue in a call center environment is informa-
tion seeking/information access, which displays specific 
characteristics that can be exploited in the design of an 
automated system. In a human-operated call center, an 
operator mediates between the caller and a variety of 
data sources: information about customers, products, 
regulations, etc. Much of this data is in a structured 
form, usually a relational database (accounts informa-
tion), while some may remain in an unstructured form 
(e.g., text memos, flyers, regulations manuals.) The ob-
jective of an automated call center is to obtain a natu-
rally interactive mediation, between the caller and the 
information which is as close to a human-human dia-
logue as possible. 
This automated call center scenario applies to many 
customer service situations, including the following: 
? Financial services (AMITIES primary domain) 
? Product support 
? Travel reservations 
where the objective is to locate, insert or update a single 
(or several) data object in a structured data base. At a 
more abstract level, the call center of the type described 
here can be characterized as an Interaction with Struc-
tured Data (ISD). ISD consists of the following compo-
nents: 
1. Data structure, which defines the set of basic enti-
ties (accounts, spare parts, flights) and their attrib-
utes (account number, part size, destination city, 
etc.) as well as methods for identifying references 
to these attributes in user statements. 
2. List of basic transactions supported by the service 
(account payment, address change, locating a 
flight) along with methods to detect references to 
these transactions. 
3. Dialogue models for handling various conversa-
tional situations in human-like fashion (e.g., re-
sponding to requests, emotions, indecision) and 
consistent with the character of the service (polite, 
helpful, caring). 
4. Optional dialogue meta-strategy as required to ad-
dress privacy and security concerns (e.g., positive 
caller identification must precede exchange of any 
sensitive information.) 
The components 1, 2 and 4 can be built using limited 
amount of static data about the service and are to a large 
degree domain-independent or domain-adaptable. These 
components are sufficient to design basic mixed-
initiative dialogue capabilities, as explained further in 
the following section. Although the dialogue may not 
feel very ?natural? it will be quite efficient, giving the 
user a broad initiative to conduct it as they wish. Dia-
logue models (component #3) are required to create an 
illusion of naturalness and these can only be derived 
from large corpora of actual call center conversations. 
Large corpora of real conversations are also needed to 
develop speech and prosody models. 
We have built a prototype caller triaging dialogue 
management which has been incorporated in the first 
AMITIES demonstrator. The system is based on Galaxy 
Communicator architecture (Seneff et al, 1998) in a 
standard configuration shown in Figure 1. The DM can 
handle dialogues in 3 European languages, and can ad-
ditionally switch from one language to another in mid-
conversation. 
 
Figure 1.  AMITI?S System Architecture 
3 Dialogue Manager/Frame Router 
In this section we explain some key principles of de-
signing an interactive dialogue with Structured Data 
(ISD). The overall strategy is to locate an item or items 
in the database that meet a number of specific condi-
tions, for example, the most convenient flight, the 
caller?s bank account, etc. This overall objective is bro-
ken down into a set of sub-goals some of which may 
need to be satisfied to achieve the objective. The role of 
ISD dialogue is to chart a path through the sub-goals in 
such as way that: 
1. the objective is achieved 
2. any partial constraints on the order or selection of 
the sub-goals are met, and 
3. the most efficient route is chosen. 
The dialogue manager identifies the goal of the con-
versation and performs interactions to achieve that goal. 
The overall mechanism works by filling attribute values 
in frames representing transactions and the sub-goals. 
Spontaneous conversation works in this environment, 
because values may be filled in any order, or several 
values may be supplied in one turn. As attribute values 
in the frames are filled, the need for dialogue decreases. 
The system sets key milestones or goals to be 
reached by gathering sufficient information from the 
customer, but these milestones may be approached by a 
variety of different paths. If the customer?s last name is 
misrecognized, for example, or if there are multiple 
database records returned, the system will ask for a dif-
ferent attribute, such as the address or postal code. Re-
prompts are used when necessary, but no more than 
once for any single attribute. The process continues un-
til a unique (e.g., bank account) or best (e.g., a flight) 
record is identified. Thus the dialogue system has flexi-
bility to deal with user input arriving in any order or 
form and the input that is not completely captured, 
without getting stuck on any single attribute. The paths 
to the key milestones, and even the order of the mile-
stones, may be seen as a series of hidden transitions. 
This means exact progression of the dialogue is never 
pre-set or can be known in advance ? a major advance 
over system-driven prompts.   
In order to keep the dialogue manager language- and 
domain-independent, mechanisms were created to store 
the language-specific and task-specific information in 
separate modules, to be loaded as needed. These are 
illustrated in Figure 2. 
 
 
Figure 2.  Dialogue Manager Structure 
Hub 
Speech  
Recognition 
Dialogue 
Manager 
Database 
Server 
Text-to-speech 
Conversion 
Nat?l Language 
Understanding 
French 
German 
Telephony 
Server 
Response       
Generation 
French 
German 
Customer 
Database
Engl. 
  (English)
  Keyword profile 
  Prompt sequence
  Task2 ? 
 
 
 
 Response Decision 
User ID 
Frame Router
Input:  
from NLU via Hub 
Task Properties File 
User Properties File 
 Task ID 
Frame Router
Task1 
account_number,
fname, lname, 
post_code, ? 
In Fig. 2, the transaction identification module 
(TaskID Frame Router) matches the incoming user ut-
terances to identify which transaction is being invoked. 
If multiple transactions are matched, their representa-
tions (frames) are ranked in the likelihood order. Each 
frame consists of a keyword profile (a list of salient 
terms derived from human-human dialogues) and a 
prompt sequence, which is a list of attributes whose 
values must be collected in order to complete the trans-
action. This prompt sequence is passed to the Dialogue 
Manager which then handles it in a manner consistent 
with the adopted dialogue model. Task identification 
succeeds even with high speech recognition error rates, 
provided the keywords are detected accurately.2 Ties (or 
near ties) are resolved through clarification dialogue by 
determining the value of some contrasting attribute, 
which is dynamically selected for each situation. The 
User ID Frame Router operates analogously, with each 
frame representing a different user (customer).  
Two simple dialogues with AMITIES are below: 
 
AMITI?S:  Amiti?s, how can I help, je vous ecoute, was kann ich 
f?r Sie tun? 
USER: Bonjour, je viens de quitter la r?gion parisienne, je voulais 
vous donner l'adresse de ma nouvelle r?sidence. 
AMITI?S:  Pouvez-vous me donner votre nom et votre ancien 
code postal, s'il vous pla?t? 
USER: madame Cecile Brun, j'habite ? Paris dans le ll?me code 
postal 38200 
AMITI?S:  Merci, Mme Brun.  Alors je vais avoir quelques 
renseignements ? vous demander pour mettre ? jour 
votre adresse. 
 
AMITI?S:  Amiti?s, how can I help, je vous ecoute, was kann ich 
f?r Sie tun? 
USER: Hi, I'm Margaret Roberts, post code CO4 3SQ 
AMITI?S:  How can I help, Mrs. Roberts? 
USER: Can I get my card replaced?  My old one was stolen. 
AMITI?S:  Thank you, Mrs. Roberts.  I'll just ask you some ques-
tions and have a new card sent to you. 
4 
                                                          
Dialogue Annotations 
Working with the real call center dialogues required 
us to develop a new method for dialogue annotation. 
The DAMSL functional annotation scheme had to be 
modified and further augmented with semantic annota-
tion. To do so, we have created dialogue act taxonomy 
appropriate for ISD dialogues. To capture the semantics, 
we used a domain-independent framework populated 
with domain-specific lists. Furthermore, to facilitate 
speedy annotation, we have designed a new flexible, 
annotation tool, XDMLTool, and annotated several 
hundred French and English dialogues using it. 
In order to annotate semantic information with 
XDMLTool, the user makes entries for a particular turn 
or turn segment in a semantic table on the user interface. 
Transactions such as MAKEPYMNT or CHANGEADDR are 
selected and their attributes appear in combo-boxes on 
the GUI. If necessary, the user may type in new labels. 
To fill a value for an attribute, text from the displayed 
dialogue may be copied into a table cell. 
2 While different combinations of keywords may invoke a 
transaction frame, this process is robust because the selection 
of transactions is limited to those known to the system.  
For example, the following exchange, part of a 
VERIFYID transaction, would be labeled with the attrib-
utes Name and PostCode. The values John Smith and 
AB1 1CD would be tagged for the answer. 
A: Your full name and postcode please? 
C: Yes it's err John Smith AB1 1CD 
The new annotation scheme reflects our approach to 
dialogue design ? we hope it will help us to automati-
cally derive appropriate dialogue strategies for novel 
ISD situations, and beyond.3                                                                          
Acknowledgments 
This paper is based on work supported in part by the 
European Commission under the 5th Framework 
IST/HLT Programme, and by the U.S. Defense Ad-
vanced Research Projects Agency. 
References 
J. Allen and M. Core.  1997.  Draft of DAMSL: Dialog 
Act Markup in Several Layers.  http://www.cs. roches-
ter. edu/research/cisd/resources/damsl/. 
J. Allen, et al 1995.  The TRAINS Project:  A Case 
Study in Building a Conversational Planning Agent.  
Journal of Experimental and Theoretical AI, 7, 7?48. 
AMITI?S, http://www.dcs.shef.ac.uk/nlp/amities/. 
A. Bagga, T. Strzalkowski and G. B. Wise.  2000.  Parts 
ID : A Dialogue-Based System for Finding Parts for 
Medical Systems.  In Proc. of ANLP-2000. 
J. Chu-Carroll and B. Carpenter.  1999.  Vector-Based 
Natural Language Call Routing.  Computational 
Linguistics, 25 (3): 361?388. 
DARPA, http://www.darpa.mil/iao/Communicator.htm.  
L. Devillers, S. Rosset, H. Maynard and L. Lamel.  May 
2002.  Annotations for Dynamic Diagnosis of the 
Dialog State.  In Proc. of LREC, Las Palmas. 
R. Gaizauskas et al 1996.  GATE :  An Environment to 
Support Research and Development in Natural 
Language Engineering.  In Proc. Of 8th IEEE Int. 
Conf. on Tools with AI, Toulouse, France. 
A. L. Gorin, G. Riccardi and J. Wright.  1997.  How 
May I Help You?  Speech Comm., 23 (1/2): 113?127. 
S. Seneff, E et al  1998.  Galaxy-II:  A Reference Ar-
chitecture for Conversational System Development.  
In Proc. of ICSLP 98, Sydney, Australia. 
                                                          
3 Some preliminary results of dialogue structure analysis are 
available but we lack space to include them in this note. 
